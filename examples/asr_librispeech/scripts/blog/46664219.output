/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
train_data_folder: aphasia
test_data_folder: aphasia
use_peft: true
seed: 
debug: 
Is test_run? 
freeze_encoder: true
Is save_embedding? false
projector_transfer_learning: true
transfer_data_folder: librispeech-100
llm_inference_config: repetition_penalty
eval_ckpt: best
----------
----------
Final identifier: aphasia_wavlm_llama32_1b_linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_23834_loss_1.1383870840072632



----- Transfer Learning Information -----
Resume Epoch: 1
Resume Step: 0
Train Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/train.jsonl
Validation Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/validation.jsonl
Test Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/test.jsonl
Identifier: aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100
Output Directory: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100
----------------------------------------
----------------------------------------
Resume epoch: 1
Resume step: 0
[2025-02-13 19:18:05][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-13 19:18:05][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 19:18:05][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100'}
[2025-02-13 19:18:05][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-13_19-18-04.txt', 'log_interval': 5}
[2025-02-13 19:18:33][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 19:18:38][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 19:18:38][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 19:18:38][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 19:18:38][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 19:18:47][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 19:18:47][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 19:18:47][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 19:18:47][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 19:18:47][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 19:18:47][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 19:18:47][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 19:18:47][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_23834_loss_1.1383870840072632/model.pt
[2025-02-13 19:18:47][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 19:18:47][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 19:18:49][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 19:18:54][root][INFO] - --> Training Set Length = 28539
[2025-02-13 19:18:54][root][INFO] - --> Validation Set Length = 2703
[2025-02-13 19:18:54][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 19:18:54][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 19:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:57][root][INFO] - Training Epoch: 1/2, step 0/7134 completed (loss: 1.1579816341400146, acc: 0.75)
[2025-02-13 19:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58][root][INFO] - Training Epoch: 1/2, step 1/7134 completed (loss: 0.9910687208175659, acc: 0.7770700454711914)
[2025-02-13 19:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58][root][INFO] - Training Epoch: 1/2, step 2/7134 completed (loss: 0.869597852230072, acc: 0.8011363744735718)
[2025-02-13 19:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:59][root][INFO] - Training Epoch: 1/2, step 3/7134 completed (loss: 1.0732008218765259, acc: 0.7790697813034058)
[2025-02-13 19:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:59][root][INFO] - Training Epoch: 1/2, step 4/7134 completed (loss: 1.2136540412902832, acc: 0.7358490824699402)
[2025-02-13 19:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00][root][INFO] - Training Epoch: 1/2, step 5/7134 completed (loss: 0.9124829769134521, acc: 0.7932960987091064)
[2025-02-13 19:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00][root][INFO] - Training Epoch: 1/2, step 6/7134 completed (loss: 0.7168205976486206, acc: 0.8450704216957092)
[2025-02-13 19:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00][root][INFO] - Training Epoch: 1/2, step 7/7134 completed (loss: 1.1315159797668457, acc: 0.7795698642730713)
[2025-02-13 19:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01][root][INFO] - Training Epoch: 1/2, step 8/7134 completed (loss: 1.100317358970642, acc: 0.75)
[2025-02-13 19:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01][root][INFO] - Training Epoch: 1/2, step 9/7134 completed (loss: 0.6515158414840698, acc: 0.8675496578216553)
[2025-02-13 19:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02][root][INFO] - Training Epoch: 1/2, step 10/7134 completed (loss: 1.399608850479126, acc: 0.715976357460022)
[2025-02-13 19:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02][root][INFO] - Training Epoch: 1/2, step 11/7134 completed (loss: 1.3747658729553223, acc: 0.6916666626930237)
[2025-02-13 19:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02][root][INFO] - Training Epoch: 1/2, step 12/7134 completed (loss: 0.9549150466918945, acc: 0.7976878881454468)
[2025-02-13 19:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:03][root][INFO] - Training Epoch: 1/2, step 13/7134 completed (loss: 1.0423942804336548, acc: 0.7640449404716492)
[2025-02-13 19:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:03][root][INFO] - Training Epoch: 1/2, step 14/7134 completed (loss: 0.8915500044822693, acc: 0.8040540814399719)
[2025-02-13 19:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04][root][INFO] - Training Epoch: 1/2, step 15/7134 completed (loss: 0.8156634569168091, acc: 0.8333333134651184)
[2025-02-13 19:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04][root][INFO] - Training Epoch: 1/2, step 16/7134 completed (loss: 0.501194179058075, acc: 0.8859649300575256)
[2025-02-13 19:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04][root][INFO] - Training Epoch: 1/2, step 17/7134 completed (loss: 1.0562118291854858, acc: 0.8085106611251831)
[2025-02-13 19:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05][root][INFO] - Training Epoch: 1/2, step 18/7134 completed (loss: 0.7087686061859131, acc: 0.849397599697113)
[2025-02-13 19:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05][root][INFO] - Training Epoch: 1/2, step 19/7134 completed (loss: 1.1565136909484863, acc: 0.7514451146125793)
[2025-02-13 19:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06][root][INFO] - Training Epoch: 1/2, step 20/7134 completed (loss: 0.8011123538017273, acc: 0.8068181872367859)
[2025-02-13 19:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06][root][INFO] - Training Epoch: 1/2, step 21/7134 completed (loss: 1.0967758893966675, acc: 0.7727272510528564)
[2025-02-13 19:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07][root][INFO] - Training Epoch: 1/2, step 22/7134 completed (loss: 0.6382497549057007, acc: 0.8650306463241577)
[2025-02-13 19:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07][root][INFO] - Training Epoch: 1/2, step 23/7134 completed (loss: 0.7131471037864685, acc: 0.8531073331832886)
[2025-02-13 19:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07][root][INFO] - Training Epoch: 1/2, step 24/7134 completed (loss: 1.0184439420700073, acc: 0.8055555820465088)
[2025-02-13 19:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08][root][INFO] - Training Epoch: 1/2, step 25/7134 completed (loss: 0.7238438129425049, acc: 0.8520709872245789)
[2025-02-13 19:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08][root][INFO] - Training Epoch: 1/2, step 26/7134 completed (loss: 0.7843343019485474, acc: 0.8220859169960022)
[2025-02-13 19:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09][root][INFO] - Training Epoch: 1/2, step 27/7134 completed (loss: 0.6212082505226135, acc: 0.8461538553237915)
[2025-02-13 19:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09][root][INFO] - Training Epoch: 1/2, step 28/7134 completed (loss: 1.154974341392517, acc: 0.7614213228225708)
[2025-02-13 19:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10][root][INFO] - Training Epoch: 1/2, step 29/7134 completed (loss: 0.9886778593063354, acc: 0.7692307829856873)
[2025-02-13 19:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10][root][INFO] - Training Epoch: 1/2, step 30/7134 completed (loss: 1.330649733543396, acc: 0.7101449370384216)
[2025-02-13 19:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10][root][INFO] - Training Epoch: 1/2, step 31/7134 completed (loss: 1.0220288038253784, acc: 0.7860465049743652)
[2025-02-13 19:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11][root][INFO] - Training Epoch: 1/2, step 32/7134 completed (loss: 1.480767846107483, acc: 0.739130437374115)
[2025-02-13 19:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11][root][INFO] - Training Epoch: 1/2, step 33/7134 completed (loss: 0.9624863862991333, acc: 0.800000011920929)
[2025-02-13 19:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12][root][INFO] - Training Epoch: 1/2, step 34/7134 completed (loss: 0.8747957944869995, acc: 0.8142856955528259)
[2025-02-13 19:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12][root][INFO] - Training Epoch: 1/2, step 35/7134 completed (loss: 1.006600260734558, acc: 0.8117647171020508)
[2025-02-13 19:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:13][root][INFO] - Training Epoch: 1/2, step 36/7134 completed (loss: 0.9815825819969177, acc: 0.8110598921775818)
[2025-02-13 19:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:13][root][INFO] - Training Epoch: 1/2, step 37/7134 completed (loss: 1.112215518951416, acc: 0.7671957612037659)
[2025-02-13 19:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14][root][INFO] - Training Epoch: 1/2, step 38/7134 completed (loss: 0.7136710286140442, acc: 0.8301886916160583)
[2025-02-13 19:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14][root][INFO] - Training Epoch: 1/2, step 39/7134 completed (loss: 0.5405465364456177, acc: 0.8654970526695251)
[2025-02-13 19:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14][root][INFO] - Training Epoch: 1/2, step 40/7134 completed (loss: 0.8778253197669983, acc: 0.8303571343421936)
[2025-02-13 19:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15][root][INFO] - Training Epoch: 1/2, step 41/7134 completed (loss: 0.7530835866928101, acc: 0.8723404407501221)
[2025-02-13 19:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15][root][INFO] - Training Epoch: 1/2, step 42/7134 completed (loss: 0.5869245529174805, acc: 0.8765432238578796)
[2025-02-13 19:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:16][root][INFO] - Training Epoch: 1/2, step 43/7134 completed (loss: 0.5194288492202759, acc: 0.8882681727409363)
[2025-02-13 19:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:16][root][INFO] - Training Epoch: 1/2, step 44/7134 completed (loss: 0.7175498008728027, acc: 0.8395721912384033)
[2025-02-13 19:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17][root][INFO] - Training Epoch: 1/2, step 45/7134 completed (loss: 0.7169306874275208, acc: 0.8537735939025879)
[2025-02-13 19:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17][root][INFO] - Training Epoch: 1/2, step 46/7134 completed (loss: 1.0077418088912964, acc: 0.7848837375640869)
[2025-02-13 19:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17][root][INFO] - Training Epoch: 1/2, step 47/7134 completed (loss: 0.80377197265625, acc: 0.8379888534545898)
[2025-02-13 19:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18][root][INFO] - Training Epoch: 1/2, step 48/7134 completed (loss: 0.621318519115448, acc: 0.8309178948402405)
[2025-02-13 19:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18][root][INFO] - Training Epoch: 1/2, step 49/7134 completed (loss: 0.8958303928375244, acc: 0.8294117450714111)
[2025-02-13 19:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19][root][INFO] - Training Epoch: 1/2, step 50/7134 completed (loss: 0.7388230562210083, acc: 0.8106796145439148)
[2025-02-13 19:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19][root][INFO] - Training Epoch: 1/2, step 51/7134 completed (loss: 0.566402018070221, acc: 0.8571428656578064)
[2025-02-13 19:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19][root][INFO] - Training Epoch: 1/2, step 52/7134 completed (loss: 0.4844636619091034, acc: 0.887417197227478)
[2025-02-13 19:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:20][root][INFO] - Training Epoch: 1/2, step 53/7134 completed (loss: 0.5961623191833496, acc: 0.8540540337562561)
[2025-02-13 19:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:20][root][INFO] - Training Epoch: 1/2, step 54/7134 completed (loss: 0.7771532535552979, acc: 0.8428571224212646)
[2025-02-13 19:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21][root][INFO] - Training Epoch: 1/2, step 55/7134 completed (loss: 0.9448931217193604, acc: 0.7880434989929199)
[2025-02-13 19:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21][root][INFO] - Training Epoch: 1/2, step 56/7134 completed (loss: 0.5293661952018738, acc: 0.8769230842590332)
[2025-02-13 19:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22][root][INFO] - Training Epoch: 1/2, step 57/7134 completed (loss: 1.2808226346969604, acc: 0.7692307829856873)
[2025-02-13 19:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22][root][INFO] - Training Epoch: 1/2, step 58/7134 completed (loss: 0.9779337644577026, acc: 0.7821229100227356)
[2025-02-13 19:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22][root][INFO] - Training Epoch: 1/2, step 59/7134 completed (loss: 1.1221625804901123, acc: 0.7914438247680664)
[2025-02-13 19:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23][root][INFO] - Training Epoch: 1/2, step 60/7134 completed (loss: 0.9963467717170715, acc: 0.7721518874168396)
[2025-02-13 19:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23][root][INFO] - Training Epoch: 1/2, step 61/7134 completed (loss: 0.8901520371437073, acc: 0.832402229309082)
[2025-02-13 19:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:24][root][INFO] - Training Epoch: 1/2, step 62/7134 completed (loss: 1.282824158668518, acc: 0.7303370833396912)
[2025-02-13 19:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:24][root][INFO] - Training Epoch: 1/2, step 63/7134 completed (loss: 0.9094303846359253, acc: 0.8131868243217468)
[2025-02-13 19:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25][root][INFO] - Training Epoch: 1/2, step 64/7134 completed (loss: 0.8790978193283081, acc: 0.7941176295280457)
[2025-02-13 19:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25][root][INFO] - Training Epoch: 1/2, step 65/7134 completed (loss: 1.5793614387512207, acc: 0.7234042286872864)
[2025-02-13 19:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25][root][INFO] - Training Epoch: 1/2, step 66/7134 completed (loss: 1.321089506149292, acc: 0.7651515007019043)
[2025-02-13 19:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26][root][INFO] - Training Epoch: 1/2, step 67/7134 completed (loss: 1.1522371768951416, acc: 0.7471264600753784)
[2025-02-13 19:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26][root][INFO] - Training Epoch: 1/2, step 68/7134 completed (loss: 1.342912197113037, acc: 0.7303370833396912)
[2025-02-13 19:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27][root][INFO] - Training Epoch: 1/2, step 69/7134 completed (loss: 1.2383785247802734, acc: 0.7830687761306763)
[2025-02-13 19:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27][root][INFO] - Training Epoch: 1/2, step 70/7134 completed (loss: 1.010359525680542, acc: 0.8046875)
[2025-02-13 19:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27][root][INFO] - Training Epoch: 1/2, step 71/7134 completed (loss: 0.7492308616638184, acc: 0.8363636136054993)
[2025-02-13 19:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28][root][INFO] - Training Epoch: 1/2, step 72/7134 completed (loss: 1.0655746459960938, acc: 0.7869822382926941)
[2025-02-13 19:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28][root][INFO] - Training Epoch: 1/2, step 73/7134 completed (loss: 1.4042980670928955, acc: 0.7514451146125793)
[2025-02-13 19:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29][root][INFO] - Training Epoch: 1/2, step 74/7134 completed (loss: 0.9244768023490906, acc: 0.8720930218696594)
[2025-02-13 19:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29][root][INFO] - Training Epoch: 1/2, step 75/7134 completed (loss: 1.4599946737289429, acc: 0.7083333134651184)
[2025-02-13 19:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29][root][INFO] - Training Epoch: 1/2, step 76/7134 completed (loss: 1.1663358211517334, acc: 0.760869562625885)
[2025-02-13 19:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30][root][INFO] - Training Epoch: 1/2, step 77/7134 completed (loss: 0.8865079879760742, acc: 0.8342541456222534)
[2025-02-13 19:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30][root][INFO] - Training Epoch: 1/2, step 78/7134 completed (loss: 1.0785491466522217, acc: 0.7748344540596008)
[2025-02-13 19:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31][root][INFO] - Training Epoch: 1/2, step 79/7134 completed (loss: 1.1482975482940674, acc: 0.7733333110809326)
[2025-02-13 19:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31][root][INFO] - Training Epoch: 1/2, step 80/7134 completed (loss: 0.5009597539901733, acc: 0.9044944047927856)
[2025-02-13 19:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32][root][INFO] - Training Epoch: 1/2, step 81/7134 completed (loss: 1.1123602390289307, acc: 0.7513227462768555)
[2025-02-13 19:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32][root][INFO] - Training Epoch: 1/2, step 82/7134 completed (loss: 0.8429518342018127, acc: 0.7891566157341003)
[2025-02-13 19:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32][root][INFO] - Training Epoch: 1/2, step 83/7134 completed (loss: 1.2330173254013062, acc: 0.75)
[2025-02-13 19:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33][root][INFO] - Training Epoch: 1/2, step 84/7134 completed (loss: 0.803117036819458, acc: 0.8057553768157959)
[2025-02-13 19:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33][root][INFO] - Training Epoch: 1/2, step 85/7134 completed (loss: 1.0804805755615234, acc: 0.8100000023841858)
[2025-02-13 19:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34][root][INFO] - Training Epoch: 1/2, step 86/7134 completed (loss: 0.9000405073165894, acc: 0.8125)
[2025-02-13 19:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34][root][INFO] - Training Epoch: 1/2, step 87/7134 completed (loss: 1.1110221147537231, acc: 0.7077922224998474)
[2025-02-13 19:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34][root][INFO] - Training Epoch: 1/2, step 88/7134 completed (loss: 0.9409828782081604, acc: 0.7710843086242676)
[2025-02-13 19:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35][root][INFO] - Training Epoch: 1/2, step 89/7134 completed (loss: 0.696330189704895, acc: 0.8571428656578064)
[2025-02-13 19:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35][root][INFO] - Training Epoch: 1/2, step 90/7134 completed (loss: 0.8703500628471375, acc: 0.8067227005958557)
[2025-02-13 19:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36][root][INFO] - Training Epoch: 1/2, step 91/7134 completed (loss: 0.9515179991722107, acc: 0.7685950398445129)
[2025-02-13 19:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36][root][INFO] - Training Epoch: 1/2, step 92/7134 completed (loss: 0.936954140663147, acc: 0.798701286315918)
[2025-02-13 19:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37][root][INFO] - Training Epoch: 1/2, step 93/7134 completed (loss: 0.8667973875999451, acc: 0.7771428823471069)
[2025-02-13 19:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37][root][INFO] - Training Epoch: 1/2, step 94/7134 completed (loss: 0.9593728184700012, acc: 0.7924528121948242)
[2025-02-13 19:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37][root][INFO] - Training Epoch: 1/2, step 95/7134 completed (loss: 0.9003345370292664, acc: 0.8048780560493469)
[2025-02-13 19:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38][root][INFO] - Training Epoch: 1/2, step 96/7134 completed (loss: 0.5697740316390991, acc: 0.8691588640213013)
[2025-02-13 19:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38][root][INFO] - Training Epoch: 1/2, step 97/7134 completed (loss: 1.7939740419387817, acc: 0.6666666865348816)
[2025-02-13 19:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39][root][INFO] - Training Epoch: 1/2, step 98/7134 completed (loss: 1.1117596626281738, acc: 0.7253521084785461)
[2025-02-13 19:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39][root][INFO] - Training Epoch: 1/2, step 99/7134 completed (loss: 0.6780874133110046, acc: 0.8384615182876587)
[2025-02-13 19:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39][root][INFO] - Training Epoch: 1/2, step 100/7134 completed (loss: 0.84760981798172, acc: 0.8121547102928162)
[2025-02-13 19:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40][root][INFO] - Training Epoch: 1/2, step 101/7134 completed (loss: 0.9260473251342773, acc: 0.7890625)
[2025-02-13 19:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40][root][INFO] - Training Epoch: 1/2, step 102/7134 completed (loss: 0.6025024056434631, acc: 0.868852436542511)
[2025-02-13 19:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:41][root][INFO] - Training Epoch: 1/2, step 103/7134 completed (loss: 0.8670310378074646, acc: 0.7876712083816528)
[2025-02-13 19:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:41][root][INFO] - Training Epoch: 1/2, step 104/7134 completed (loss: 0.5588876605033875, acc: 0.8666666746139526)
[2025-02-13 19:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42][root][INFO] - Training Epoch: 1/2, step 105/7134 completed (loss: 0.49645182490348816, acc: 0.858208954334259)
[2025-02-13 19:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42][root][INFO] - Training Epoch: 1/2, step 106/7134 completed (loss: 0.5822944641113281, acc: 0.8633540272712708)
[2025-02-13 19:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42][root][INFO] - Training Epoch: 1/2, step 107/7134 completed (loss: 1.2932029962539673, acc: 0.7441860437393188)
[2025-02-13 19:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43][root][INFO] - Training Epoch: 1/2, step 108/7134 completed (loss: 0.9547482132911682, acc: 0.7621621489524841)
[2025-02-13 19:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43][root][INFO] - Training Epoch: 1/2, step 109/7134 completed (loss: 0.775134265422821, acc: 0.8492063283920288)
[2025-02-13 19:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44][root][INFO] - Training Epoch: 1/2, step 110/7134 completed (loss: 0.8062340021133423, acc: 0.8156424760818481)
[2025-02-13 19:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44][root][INFO] - Training Epoch: 1/2, step 111/7134 completed (loss: 0.6842694282531738, acc: 0.8409090638160706)
[2025-02-13 19:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45][root][INFO] - Training Epoch: 1/2, step 112/7134 completed (loss: 0.601958155632019, acc: 0.8472222089767456)
[2025-02-13 19:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45][root][INFO] - Training Epoch: 1/2, step 113/7134 completed (loss: 0.4423055946826935, acc: 0.8715083599090576)
[2025-02-13 19:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45][root][INFO] - Training Epoch: 1/2, step 114/7134 completed (loss: 0.789219081401825, acc: 0.8285714387893677)
[2025-02-13 19:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:46][root][INFO] - Training Epoch: 1/2, step 115/7134 completed (loss: 1.1812604665756226, acc: 0.7227723002433777)
[2025-02-13 19:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:46][root][INFO] - Training Epoch: 1/2, step 116/7134 completed (loss: 1.1696827411651611, acc: 0.7241379022598267)
[2025-02-13 19:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47][root][INFO] - Training Epoch: 1/2, step 117/7134 completed (loss: 0.7612439393997192, acc: 0.8121212124824524)
[2025-02-13 19:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47][root][INFO] - Training Epoch: 1/2, step 118/7134 completed (loss: 1.0100853443145752, acc: 0.7611111402511597)
[2025-02-13 19:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48][root][INFO] - Training Epoch: 1/2, step 119/7134 completed (loss: 0.860557496547699, acc: 0.8085106611251831)
[2025-02-13 19:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48][root][INFO] - Training Epoch: 1/2, step 120/7134 completed (loss: 0.5947567224502563, acc: 0.8333333134651184)
[2025-02-13 19:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49][root][INFO] - Training Epoch: 1/2, step 121/7134 completed (loss: 0.6744710206985474, acc: 0.8491619825363159)
[2025-02-13 19:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49][root][INFO] - Training Epoch: 1/2, step 122/7134 completed (loss: 0.6922733783721924, acc: 0.8791208863258362)
[2025-02-13 19:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49][root][INFO] - Training Epoch: 1/2, step 123/7134 completed (loss: 0.7027183175086975, acc: 0.8553459048271179)
[2025-02-13 19:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:50][root][INFO] - Training Epoch: 1/2, step 124/7134 completed (loss: 0.7347831130027771, acc: 0.8402777910232544)
[2025-02-13 19:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:50][root][INFO] - Training Epoch: 1/2, step 125/7134 completed (loss: 0.8031057715415955, acc: 0.8186274766921997)
[2025-02-13 19:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51][root][INFO] - Training Epoch: 1/2, step 126/7134 completed (loss: 0.8545796275138855, acc: 0.835106372833252)
[2025-02-13 19:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51][root][INFO] - Training Epoch: 1/2, step 127/7134 completed (loss: 0.875531017780304, acc: 0.8541666865348816)
[2025-02-13 19:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52][root][INFO] - Training Epoch: 1/2, step 128/7134 completed (loss: 0.8856180906295776, acc: 0.8086124658584595)
[2025-02-13 19:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52][root][INFO] - Training Epoch: 1/2, step 129/7134 completed (loss: 1.0045360326766968, acc: 0.8100558519363403)
[2025-02-13 19:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52][root][INFO] - Training Epoch: 1/2, step 130/7134 completed (loss: 0.6231512427330017, acc: 0.8529411554336548)
[2025-02-13 19:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53][root][INFO] - Training Epoch: 1/2, step 131/7134 completed (loss: 0.48057737946510315, acc: 0.8947368264198303)
[2025-02-13 19:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53][root][INFO] - Training Epoch: 1/2, step 132/7134 completed (loss: 0.6082344055175781, acc: 0.8364779949188232)
[2025-02-13 19:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54][root][INFO] - Training Epoch: 1/2, step 133/7134 completed (loss: 0.7347307205200195, acc: 0.8161764740943909)
[2025-02-13 19:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54][root][INFO] - Training Epoch: 1/2, step 134/7134 completed (loss: 0.5117580890655518, acc: 0.8796992301940918)
[2025-02-13 19:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55][root][INFO] - Training Epoch: 1/2, step 135/7134 completed (loss: 0.7366487979888916, acc: 0.8571428656578064)
[2025-02-13 19:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55][root][INFO] - Training Epoch: 1/2, step 136/7134 completed (loss: 0.5809131860733032, acc: 0.864130437374115)
[2025-02-13 19:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55][root][INFO] - Training Epoch: 1/2, step 137/7134 completed (loss: 0.7578109502792358, acc: 0.8542713522911072)
[2025-02-13 19:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:56][root][INFO] - Training Epoch: 1/2, step 138/7134 completed (loss: 0.9140452742576599, acc: 0.7956989407539368)
[2025-02-13 19:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:56][root][INFO] - Training Epoch: 1/2, step 139/7134 completed (loss: 0.8554503917694092, acc: 0.835106372833252)
[2025-02-13 19:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57][root][INFO] - Training Epoch: 1/2, step 140/7134 completed (loss: 0.5818794965744019, acc: 0.8588957190513611)
[2025-02-13 19:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57][root][INFO] - Training Epoch: 1/2, step 141/7134 completed (loss: 0.6185181140899658, acc: 0.8700000047683716)
[2025-02-13 19:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58][root][INFO] - Training Epoch: 1/2, step 142/7134 completed (loss: 0.6379004716873169, acc: 0.8757061958312988)
[2025-02-13 19:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58][root][INFO] - Training Epoch: 1/2, step 143/7134 completed (loss: 0.6955274939537048, acc: 0.8287292718887329)
[2025-02-13 19:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58][root][INFO] - Training Epoch: 1/2, step 144/7134 completed (loss: 0.9844940900802612, acc: 0.7905405163764954)
[2025-02-13 19:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59][root][INFO] - Training Epoch: 1/2, step 145/7134 completed (loss: 1.2179694175720215, acc: 0.7461140155792236)
[2025-02-13 19:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59][root][INFO] - Training Epoch: 1/2, step 146/7134 completed (loss: 0.799367368221283, acc: 0.7740113139152527)
[2025-02-13 19:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00][root][INFO] - Training Epoch: 1/2, step 147/7134 completed (loss: 0.9879950284957886, acc: 0.7931034564971924)
[2025-02-13 19:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00][root][INFO] - Training Epoch: 1/2, step 148/7134 completed (loss: 1.3570102453231812, acc: 0.7096773982048035)
[2025-02-13 19:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00][root][INFO] - Training Epoch: 1/2, step 149/7134 completed (loss: 1.0037477016448975, acc: 0.7918781638145447)
[2025-02-13 19:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01][root][INFO] - Training Epoch: 1/2, step 150/7134 completed (loss: 1.0729939937591553, acc: 0.7801046967506409)
[2025-02-13 19:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01][root][INFO] - Training Epoch: 1/2, step 151/7134 completed (loss: 0.6699702739715576, acc: 0.8602150678634644)
[2025-02-13 19:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02][root][INFO] - Training Epoch: 1/2, step 152/7134 completed (loss: 0.6391641497612, acc: 0.8708133697509766)
[2025-02-13 19:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02][root][INFO] - Training Epoch: 1/2, step 153/7134 completed (loss: 1.0689494609832764, acc: 0.7569060921669006)
[2025-02-13 19:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02][root][INFO] - Training Epoch: 1/2, step 154/7134 completed (loss: 0.8476163744926453, acc: 0.8300653696060181)
[2025-02-13 19:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:03][root][INFO] - Training Epoch: 1/2, step 155/7134 completed (loss: 0.8555500507354736, acc: 0.7978141903877258)
[2025-02-13 19:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:03][root][INFO] - Training Epoch: 1/2, step 156/7134 completed (loss: 0.6803544759750366, acc: 0.8472906351089478)
[2025-02-13 19:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04][root][INFO] - Training Epoch: 1/2, step 157/7134 completed (loss: 0.8224169015884399, acc: 0.8148148059844971)
[2025-02-13 19:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04][root][INFO] - Training Epoch: 1/2, step 158/7134 completed (loss: 0.5046972632408142, acc: 0.8947368264198303)
[2025-02-13 19:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04][root][INFO] - Training Epoch: 1/2, step 159/7134 completed (loss: 0.7799127101898193, acc: 0.8150289058685303)
[2025-02-13 19:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05][root][INFO] - Training Epoch: 1/2, step 160/7134 completed (loss: 0.6775964498519897, acc: 0.8305084705352783)
[2025-02-13 19:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05][root][INFO] - Training Epoch: 1/2, step 161/7134 completed (loss: 0.49929025769233704, acc: 0.9054054021835327)
[2025-02-13 19:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06][root][INFO] - Training Epoch: 1/2, step 162/7134 completed (loss: 0.5975907444953918, acc: 0.8641975522041321)
[2025-02-13 19:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06][root][INFO] - Training Epoch: 1/2, step 163/7134 completed (loss: 0.6014242768287659, acc: 0.8273381590843201)
[2025-02-13 19:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06][root][INFO] - Training Epoch: 1/2, step 164/7134 completed (loss: 0.6982408165931702, acc: 0.8383838534355164)
[2025-02-13 19:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07][root][INFO] - Training Epoch: 1/2, step 165/7134 completed (loss: 0.5590691566467285, acc: 0.8600000143051147)
[2025-02-13 19:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07][root][INFO] - Training Epoch: 1/2, step 166/7134 completed (loss: 0.4240877330303192, acc: 0.8552631735801697)
[2025-02-13 19:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08][root][INFO] - Training Epoch: 1/2, step 167/7134 completed (loss: 2.751243829727173, acc: 0.5722891688346863)
[2025-02-13 19:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08][root][INFO] - Training Epoch: 1/2, step 168/7134 completed (loss: 5.030994415283203, acc: 0.2818181812763214)
[2025-02-13 19:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08][root][INFO] - Training Epoch: 1/2, step 169/7134 completed (loss: 4.151239395141602, acc: 0.421875)
[2025-02-13 19:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09][root][INFO] - Training Epoch: 1/2, step 170/7134 completed (loss: 3.2056634426116943, acc: 0.5297619104385376)
[2025-02-13 19:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09][root][INFO] - Training Epoch: 1/2, step 171/7134 completed (loss: 3.547682762145996, acc: 0.48514851927757263)
[2025-02-13 19:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:10][root][INFO] - Training Epoch: 1/2, step 172/7134 completed (loss: 1.1904696226119995, acc: 0.7286821603775024)
[2025-02-13 19:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:10][root][INFO] - Training Epoch: 1/2, step 173/7134 completed (loss: 1.237549901008606, acc: 0.7120000123977661)
[2025-02-13 19:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11][root][INFO] - Training Epoch: 1/2, step 174/7134 completed (loss: 1.8226652145385742, acc: 0.6842105388641357)
[2025-02-13 19:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11][root][INFO] - Training Epoch: 1/2, step 175/7134 completed (loss: 1.5166947841644287, acc: 0.6754966974258423)
[2025-02-13 19:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11][root][INFO] - Training Epoch: 1/2, step 176/7134 completed (loss: 1.306869626045227, acc: 0.7431694269180298)
[2025-02-13 19:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12][root][INFO] - Training Epoch: 1/2, step 177/7134 completed (loss: 1.5592082738876343, acc: 0.682634711265564)
[2025-02-13 19:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12][root][INFO] - Training Epoch: 1/2, step 178/7134 completed (loss: 1.0962125062942505, acc: 0.8051947951316833)
[2025-02-13 19:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13][root][INFO] - Training Epoch: 1/2, step 179/7134 completed (loss: 0.9239317178726196, acc: 0.7883211970329285)
[2025-02-13 19:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13][root][INFO] - Training Epoch: 1/2, step 180/7134 completed (loss: 0.7014711499214172, acc: 0.8322147727012634)
[2025-02-13 19:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13][root][INFO] - Training Epoch: 1/2, step 181/7134 completed (loss: 0.5781344175338745, acc: 0.8413792848587036)
[2025-02-13 19:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14][root][INFO] - Training Epoch: 1/2, step 182/7134 completed (loss: 0.6298597455024719, acc: 0.850931704044342)
[2025-02-13 19:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14][root][INFO] - Training Epoch: 1/2, step 183/7134 completed (loss: 0.49698859453201294, acc: 0.8904109597206116)
[2025-02-13 19:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:15][root][INFO] - Training Epoch: 1/2, step 184/7134 completed (loss: 0.5998672842979431, acc: 0.84375)
[2025-02-13 19:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:15][root][INFO] - Training Epoch: 1/2, step 185/7134 completed (loss: 0.25335296988487244, acc: 0.9523809552192688)
[2025-02-13 19:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16][root][INFO] - Training Epoch: 1/2, step 186/7134 completed (loss: 0.4880503714084625, acc: 0.89552241563797)
[2025-02-13 19:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16][root][INFO] - Training Epoch: 1/2, step 187/7134 completed (loss: 0.5018497705459595, acc: 0.8758170008659363)
[2025-02-13 19:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16][root][INFO] - Training Epoch: 1/2, step 188/7134 completed (loss: 0.7950764894485474, acc: 0.8353658318519592)
[2025-02-13 19:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17][root][INFO] - Training Epoch: 1/2, step 189/7134 completed (loss: 0.5788471698760986, acc: 0.8524590134620667)
[2025-02-13 19:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17][root][INFO] - Training Epoch: 1/2, step 190/7134 completed (loss: 0.8349127173423767, acc: 0.7985074520111084)
[2025-02-13 19:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18][root][INFO] - Training Epoch: 1/2, step 191/7134 completed (loss: 0.6507360935211182, acc: 0.8557692170143127)
[2025-02-13 19:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18][root][INFO] - Training Epoch: 1/2, step 192/7134 completed (loss: 0.4672664701938629, acc: 0.8720930218696594)
[2025-02-13 19:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18][root][INFO] - Training Epoch: 1/2, step 193/7134 completed (loss: 0.826694667339325, acc: 0.7904191613197327)
[2025-02-13 19:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19][root][INFO] - Training Epoch: 1/2, step 194/7134 completed (loss: 1.0126128196716309, acc: 0.7861635088920593)
[2025-02-13 19:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19][root][INFO] - Training Epoch: 1/2, step 195/7134 completed (loss: 0.7287052869796753, acc: 0.8125)
[2025-02-13 19:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20][root][INFO] - Training Epoch: 1/2, step 196/7134 completed (loss: 0.9241830706596375, acc: 0.7932960987091064)
[2025-02-13 19:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20][root][INFO] - Training Epoch: 1/2, step 197/7134 completed (loss: 0.6251688003540039, acc: 0.8716216087341309)
[2025-02-13 19:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20][root][INFO] - Training Epoch: 1/2, step 198/7134 completed (loss: 0.639306902885437, acc: 0.8471337556838989)
[2025-02-13 19:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:21][root][INFO] - Training Epoch: 1/2, step 199/7134 completed (loss: 0.9990988373756409, acc: 0.7941176295280457)
[2025-02-13 19:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:21][root][INFO] - Training Epoch: 1/2, step 200/7134 completed (loss: 0.5394032001495361, acc: 0.884353756904602)
[2025-02-13 19:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22][root][INFO] - Training Epoch: 1/2, step 201/7134 completed (loss: 0.35694390535354614, acc: 0.8936170339584351)
[2025-02-13 19:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22][root][INFO] - Training Epoch: 1/2, step 202/7134 completed (loss: 0.4802814722061157, acc: 0.9032257795333862)
[2025-02-13 19:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23][root][INFO] - Training Epoch: 1/2, step 203/7134 completed (loss: 0.716172456741333, acc: 0.8461538553237915)
[2025-02-13 19:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23][root][INFO] - Training Epoch: 1/2, step 204/7134 completed (loss: 0.751494288444519, acc: 0.8113207817077637)
[2025-02-13 19:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24][root][INFO] - Training Epoch: 1/2, step 205/7134 completed (loss: 0.9522903561592102, acc: 0.7900000214576721)
[2025-02-13 19:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24][root][INFO] - Training Epoch: 1/2, step 206/7134 completed (loss: 0.34913691878318787, acc: 0.8958333134651184)
[2025-02-13 19:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24][root][INFO] - Training Epoch: 1/2, step 207/7134 completed (loss: 0.6829363107681274, acc: 0.8435373902320862)
[2025-02-13 19:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:25][root][INFO] - Training Epoch: 1/2, step 208/7134 completed (loss: 1.2712219953536987, acc: 0.7642857432365417)
[2025-02-13 19:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:25][root][INFO] - Training Epoch: 1/2, step 209/7134 completed (loss: 1.3640512228012085, acc: 0.6703910827636719)
[2025-02-13 19:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26][root][INFO] - Training Epoch: 1/2, step 210/7134 completed (loss: 0.8964428305625916, acc: 0.8281938433647156)
[2025-02-13 19:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26][root][INFO] - Training Epoch: 1/2, step 211/7134 completed (loss: 0.8511673212051392, acc: 0.7837837934494019)
[2025-02-13 19:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27][root][INFO] - Training Epoch: 1/2, step 212/7134 completed (loss: 1.1799663305282593, acc: 0.7685950398445129)
[2025-02-13 19:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27][root][INFO] - Training Epoch: 1/2, step 213/7134 completed (loss: 1.2022967338562012, acc: 0.7666666507720947)
[2025-02-13 19:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27][root][INFO] - Training Epoch: 1/2, step 214/7134 completed (loss: 0.6622641682624817, acc: 0.8415841460227966)
[2025-02-13 19:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28][root][INFO] - Training Epoch: 1/2, step 215/7134 completed (loss: 0.620978593826294, acc: 0.8285714387893677)
[2025-02-13 19:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28][root][INFO] - Training Epoch: 1/2, step 216/7134 completed (loss: 0.3911060690879822, acc: 0.8932584524154663)
[2025-02-13 19:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29][root][INFO] - Training Epoch: 1/2, step 217/7134 completed (loss: 0.4953326880931854, acc: 0.8802083134651184)
[2025-02-13 19:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29][root][INFO] - Training Epoch: 1/2, step 218/7134 completed (loss: 0.32385703921318054, acc: 0.9144384860992432)
[2025-02-13 19:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29][root][INFO] - Training Epoch: 1/2, step 219/7134 completed (loss: 0.2648322880268097, acc: 0.9035087823867798)
[2025-02-13 19:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30][root][INFO] - Training Epoch: 1/2, step 220/7134 completed (loss: 0.35333892703056335, acc: 0.904347836971283)
[2025-02-13 19:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30][root][INFO] - Training Epoch: 1/2, step 221/7134 completed (loss: 0.523921012878418, acc: 0.893750011920929)
[2025-02-13 19:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:31][root][INFO] - Training Epoch: 1/2, step 222/7134 completed (loss: 0.6135656833648682, acc: 0.8612716794013977)
[2025-02-13 19:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:31][root][INFO] - Training Epoch: 1/2, step 223/7134 completed (loss: 0.8239468336105347, acc: 0.8108108043670654)
[2025-02-13 19:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32][root][INFO] - Training Epoch: 1/2, step 224/7134 completed (loss: 0.9070250391960144, acc: 0.7932960987091064)
[2025-02-13 19:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32][root][INFO] - Training Epoch: 1/2, step 225/7134 completed (loss: 0.824986457824707, acc: 0.8047337532043457)
[2025-02-13 19:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32][root][INFO] - Training Epoch: 1/2, step 226/7134 completed (loss: 0.691920280456543, acc: 0.8514285683631897)
[2025-02-13 19:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:33][root][INFO] - Training Epoch: 1/2, step 227/7134 completed (loss: 0.8746569752693176, acc: 0.75)
[2025-02-13 19:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:33][root][INFO] - Training Epoch: 1/2, step 228/7134 completed (loss: 0.7786930799484253, acc: 0.8083832263946533)
[2025-02-13 19:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:34][root][INFO] - Training Epoch: 1/2, step 229/7134 completed (loss: 0.9760414361953735, acc: 0.7714285850524902)
[2025-02-13 19:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:34][root][INFO] - Training Epoch: 1/2, step 230/7134 completed (loss: 0.7390641570091248, acc: 0.8121547102928162)
[2025-02-13 19:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35][root][INFO] - Training Epoch: 1/2, step 231/7134 completed (loss: 0.6204975247383118, acc: 0.8421052694320679)
[2025-02-13 19:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35][root][INFO] - Training Epoch: 1/2, step 232/7134 completed (loss: 0.7278450131416321, acc: 0.8181818127632141)
[2025-02-13 19:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35][root][INFO] - Training Epoch: 1/2, step 233/7134 completed (loss: 0.5303047895431519, acc: 0.8905109763145447)
[2025-02-13 19:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36][root][INFO] - Training Epoch: 1/2, step 234/7134 completed (loss: 0.7789466977119446, acc: 0.7957746386528015)
[2025-02-13 19:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36][root][INFO] - Training Epoch: 1/2, step 235/7134 completed (loss: 0.7931415438652039, acc: 0.8170731663703918)
[2025-02-13 19:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37][root][INFO] - Training Epoch: 1/2, step 236/7134 completed (loss: 0.5214357972145081, acc: 0.860927164554596)
[2025-02-13 19:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37][root][INFO] - Training Epoch: 1/2, step 237/7134 completed (loss: 0.6771307587623596, acc: 0.8343949317932129)
[2025-02-13 19:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37][root][INFO] - Training Epoch: 1/2, step 238/7134 completed (loss: 0.852684736251831, acc: 0.8167939186096191)
[2025-02-13 19:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:38][root][INFO] - Training Epoch: 1/2, step 239/7134 completed (loss: 0.7843424677848816, acc: 0.8207547068595886)
[2025-02-13 19:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:38][root][INFO] - Training Epoch: 1/2, step 240/7134 completed (loss: 0.8917466402053833, acc: 0.8349514603614807)
[2025-02-13 19:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39][root][INFO] - Training Epoch: 1/2, step 241/7134 completed (loss: 0.9260782599449158, acc: 0.8059701323509216)
[2025-02-13 19:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39][root][INFO] - Training Epoch: 1/2, step 242/7134 completed (loss: 1.1030482053756714, acc: 0.7583892345428467)
[2025-02-13 19:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40][root][INFO] - Training Epoch: 1/2, step 243/7134 completed (loss: 1.295562982559204, acc: 0.7241379022598267)
[2025-02-13 19:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40][root][INFO] - Training Epoch: 1/2, step 244/7134 completed (loss: 0.5104047060012817, acc: 0.8920863270759583)
[2025-02-13 19:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40][root][INFO] - Training Epoch: 1/2, step 245/7134 completed (loss: 0.7869037389755249, acc: 0.8360655903816223)
[2025-02-13 19:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41][root][INFO] - Training Epoch: 1/2, step 246/7134 completed (loss: 0.6470817923545837, acc: 0.8405796885490417)
[2025-02-13 19:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41][root][INFO] - Training Epoch: 1/2, step 247/7134 completed (loss: 0.8403752446174622, acc: 0.8333333134651184)
[2025-02-13 19:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42][root][INFO] - Training Epoch: 1/2, step 248/7134 completed (loss: 0.6711517572402954, acc: 0.8373983502388)
[2025-02-13 19:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42][root][INFO] - Training Epoch: 1/2, step 249/7134 completed (loss: 0.6925802826881409, acc: 0.8152173757553101)
[2025-02-13 19:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42][root][INFO] - Training Epoch: 1/2, step 250/7134 completed (loss: 0.778296172618866, acc: 0.807947039604187)
[2025-02-13 19:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:43][root][INFO] - Training Epoch: 1/2, step 251/7134 completed (loss: 0.7170163989067078, acc: 0.8417266011238098)
[2025-02-13 19:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:43][root][INFO] - Training Epoch: 1/2, step 252/7134 completed (loss: 0.3978358805179596, acc: 0.8920863270759583)
[2025-02-13 19:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44][root][INFO] - Training Epoch: 1/2, step 253/7134 completed (loss: 0.4943477213382721, acc: 0.8799999952316284)
[2025-02-13 19:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44][root][INFO] - Training Epoch: 1/2, step 254/7134 completed (loss: 0.682952880859375, acc: 0.8421052694320679)
[2025-02-13 19:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44][root][INFO] - Training Epoch: 1/2, step 255/7134 completed (loss: 0.4143470823764801, acc: 0.9294871687889099)
[2025-02-13 19:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:45][root][INFO] - Training Epoch: 1/2, step 256/7134 completed (loss: 1.0803349018096924, acc: 0.7567567825317383)
[2025-02-13 19:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:45][root][INFO] - Training Epoch: 1/2, step 257/7134 completed (loss: 0.823875904083252, acc: 0.8414633870124817)
[2025-02-13 19:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:46][root][INFO] - Training Epoch: 1/2, step 258/7134 completed (loss: 0.5704922080039978, acc: 0.8790322542190552)
[2025-02-13 19:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:46][root][INFO] - Training Epoch: 1/2, step 259/7134 completed (loss: 0.6791384816169739, acc: 0.8409090638160706)
[2025-02-13 19:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47][root][INFO] - Training Epoch: 1/2, step 260/7134 completed (loss: 0.46531450748443604, acc: 0.8947368264198303)
[2025-02-13 19:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47][root][INFO] - Training Epoch: 1/2, step 261/7134 completed (loss: 0.628335177898407, acc: 0.8888888955116272)
[2025-02-13 19:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47][root][INFO] - Training Epoch: 1/2, step 262/7134 completed (loss: 0.2870938777923584, acc: 0.9268292784690857)
[2025-02-13 19:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48][root][INFO] - Training Epoch: 1/2, step 263/7134 completed (loss: 0.32781359553337097, acc: 0.9074074029922485)
[2025-02-13 19:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48][root][INFO] - Training Epoch: 1/2, step 264/7134 completed (loss: 0.4992958605289459, acc: 0.8764705657958984)
[2025-02-13 19:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49][root][INFO] - Training Epoch: 1/2, step 265/7134 completed (loss: 0.4907734990119934, acc: 0.8932584524154663)
[2025-02-13 19:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49][root][INFO] - Training Epoch: 1/2, step 266/7134 completed (loss: 0.40448567271232605, acc: 0.90625)
[2025-02-13 19:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49][root][INFO] - Training Epoch: 1/2, step 267/7134 completed (loss: 0.5935325026512146, acc: 0.8617021441459656)
[2025-02-13 19:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50][root][INFO] - Training Epoch: 1/2, step 268/7134 completed (loss: 0.5698181390762329, acc: 0.8606060743331909)
[2025-02-13 19:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50][root][INFO] - Training Epoch: 1/2, step 269/7134 completed (loss: 0.7691419720649719, acc: 0.8333333134651184)
[2025-02-13 19:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:51][root][INFO] - Training Epoch: 1/2, step 270/7134 completed (loss: 0.536893904209137, acc: 0.893750011920929)
[2025-02-13 19:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:51][root][INFO] - Training Epoch: 1/2, step 271/7134 completed (loss: 0.45367610454559326, acc: 0.9055555462837219)
[2025-02-13 19:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52][root][INFO] - Training Epoch: 1/2, step 272/7134 completed (loss: 0.5974839925765991, acc: 0.8491619825363159)
[2025-02-13 19:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52][root][INFO] - Training Epoch: 1/2, step 273/7134 completed (loss: 0.36364153027534485, acc: 0.914893627166748)
[2025-02-13 19:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52][root][INFO] - Training Epoch: 1/2, step 274/7134 completed (loss: 0.6052497029304504, acc: 0.8830409646034241)
[2025-02-13 19:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:53][root][INFO] - Training Epoch: 1/2, step 275/7134 completed (loss: 0.4072014391422272, acc: 0.8984771370887756)
[2025-02-13 19:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:53][root][INFO] - Training Epoch: 1/2, step 276/7134 completed (loss: 0.4049418866634369, acc: 0.8814433217048645)
[2025-02-13 19:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54][root][INFO] - Training Epoch: 1/2, step 277/7134 completed (loss: 0.6740380525588989, acc: 0.8540540337562561)
[2025-02-13 19:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54][root][INFO] - Training Epoch: 1/2, step 278/7134 completed (loss: 0.6564632058143616, acc: 0.859375)
[2025-02-13 19:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55][root][INFO] - Training Epoch: 1/2, step 279/7134 completed (loss: 0.5904083251953125, acc: 0.8486486673355103)
[2025-02-13 19:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55][root][INFO] - Training Epoch: 1/2, step 280/7134 completed (loss: 0.7396845817565918, acc: 0.8240000009536743)
[2025-02-13 19:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55][root][INFO] - Training Epoch: 1/2, step 281/7134 completed (loss: 0.8171733021736145, acc: 0.8135592937469482)
[2025-02-13 19:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:56][root][INFO] - Training Epoch: 1/2, step 282/7134 completed (loss: 0.9795336127281189, acc: 0.797468364238739)
[2025-02-13 19:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:56][root][INFO] - Training Epoch: 1/2, step 283/7134 completed (loss: 0.8945587873458862, acc: 0.8167939186096191)
[2025-02-13 19:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57][root][INFO] - Training Epoch: 1/2, step 284/7134 completed (loss: 0.5806785821914673, acc: 0.867132842540741)
[2025-02-13 19:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57][root][INFO] - Training Epoch: 1/2, step 285/7134 completed (loss: 0.7632991075515747, acc: 0.8370370268821716)
[2025-02-13 19:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58][root][INFO] - Training Epoch: 1/2, step 286/7134 completed (loss: 0.684969961643219, acc: 0.8333333134651184)
[2025-02-13 19:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58][root][INFO] - Training Epoch: 1/2, step 287/7134 completed (loss: 0.6710459589958191, acc: 0.8255033493041992)
[2025-02-13 19:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58][root][INFO] - Training Epoch: 1/2, step 288/7134 completed (loss: 0.6884148716926575, acc: 0.8188976645469666)
[2025-02-13 19:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:59][root][INFO] - Training Epoch: 1/2, step 289/7134 completed (loss: 0.4528476595878601, acc: 0.8759689927101135)
[2025-02-13 19:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:59][root][INFO] - Training Epoch: 1/2, step 290/7134 completed (loss: 0.7592332363128662, acc: 0.8376623392105103)
[2025-02-13 19:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00][root][INFO] - Training Epoch: 1/2, step 291/7134 completed (loss: 0.6382395625114441, acc: 0.8707482814788818)
[2025-02-13 19:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00][root][INFO] - Training Epoch: 1/2, step 292/7134 completed (loss: 0.9072188138961792, acc: 0.8148148059844971)
[2025-02-13 19:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00][root][INFO] - Training Epoch: 1/2, step 293/7134 completed (loss: 0.7431196570396423, acc: 0.8672566413879395)
[2025-02-13 19:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:01][root][INFO] - Training Epoch: 1/2, step 294/7134 completed (loss: 0.6836832165718079, acc: 0.8389830589294434)
[2025-02-13 19:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:01][root][INFO] - Training Epoch: 1/2, step 295/7134 completed (loss: 0.966776430606842, acc: 0.8205128312110901)
[2025-02-13 19:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02][root][INFO] - Training Epoch: 1/2, step 296/7134 completed (loss: 0.46384721994400024, acc: 0.8787878751754761)
[2025-02-13 19:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02][root][INFO] - Training Epoch: 1/2, step 297/7134 completed (loss: 0.5173471570014954, acc: 0.8760330677032471)
[2025-02-13 19:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03][root][INFO] - Training Epoch: 1/2, step 298/7134 completed (loss: 0.5753236413002014, acc: 0.8846153616905212)
[2025-02-13 19:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03][root][INFO] - Training Epoch: 1/2, step 299/7134 completed (loss: 0.2891678512096405, acc: 0.9197080135345459)
[2025-02-13 19:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03][root][INFO] - Training Epoch: 1/2, step 300/7134 completed (loss: 0.27022236585617065, acc: 0.9202898740768433)
[2025-02-13 19:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04][root][INFO] - Training Epoch: 1/2, step 301/7134 completed (loss: 0.3656638562679291, acc: 0.9090909361839294)
[2025-02-13 19:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04][root][INFO] - Training Epoch: 1/2, step 302/7134 completed (loss: 0.45641636848449707, acc: 0.9009901285171509)
[2025-02-13 19:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05][root][INFO] - Training Epoch: 1/2, step 303/7134 completed (loss: 0.4206976294517517, acc: 0.8738738894462585)
[2025-02-13 19:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05][root][INFO] - Training Epoch: 1/2, step 304/7134 completed (loss: 0.42743951082229614, acc: 0.8910890817642212)
[2025-02-13 19:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05][root][INFO] - Training Epoch: 1/2, step 305/7134 completed (loss: 0.38358134031295776, acc: 0.9453125)
[2025-02-13 19:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06][root][INFO] - Training Epoch: 1/2, step 306/7134 completed (loss: 0.2110164612531662, acc: 0.939130425453186)
[2025-02-13 19:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06][root][INFO] - Training Epoch: 1/2, step 307/7134 completed (loss: 0.6247552037239075, acc: 0.8130841255187988)
[2025-02-13 19:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07][root][INFO] - Training Epoch: 1/2, step 308/7134 completed (loss: 0.3515144884586334, acc: 0.9212598204612732)
[2025-02-13 19:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07][root][INFO] - Training Epoch: 1/2, step 309/7134 completed (loss: 0.3189508318901062, acc: 0.9292035102844238)
[2025-02-13 19:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08][root][INFO] - Training Epoch: 1/2, step 310/7134 completed (loss: 0.6363824605941772, acc: 0.8604651093482971)
[2025-02-13 19:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08][root][INFO] - Training Epoch: 1/2, step 311/7134 completed (loss: 0.3155752420425415, acc: 0.902255654335022)
[2025-02-13 19:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08][root][INFO] - Training Epoch: 1/2, step 312/7134 completed (loss: 0.18663807213306427, acc: 0.9590163826942444)
[2025-02-13 19:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09][root][INFO] - Training Epoch: 1/2, step 313/7134 completed (loss: 0.36488237977027893, acc: 0.9120000004768372)
[2025-02-13 19:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09][root][INFO] - Training Epoch: 1/2, step 314/7134 completed (loss: 0.3892887830734253, acc: 0.9185185432434082)
[2025-02-13 19:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10][root][INFO] - Training Epoch: 1/2, step 315/7134 completed (loss: 0.6142857670783997, acc: 0.8666666746139526)
[2025-02-13 19:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10][root][INFO] - Training Epoch: 1/2, step 316/7134 completed (loss: 0.49380069971084595, acc: 0.8918918967247009)
[2025-02-13 19:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10][root][INFO] - Training Epoch: 1/2, step 317/7134 completed (loss: 0.24471025168895721, acc: 0.949367105960846)
[2025-02-13 19:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11][root][INFO] - Training Epoch: 1/2, step 318/7134 completed (loss: 0.4597260653972626, acc: 0.8870967626571655)
[2025-02-13 19:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11][root][INFO] - Training Epoch: 1/2, step 319/7134 completed (loss: 0.7006624937057495, acc: 0.8385093212127686)
[2025-02-13 19:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12][root][INFO] - Training Epoch: 1/2, step 320/7134 completed (loss: 0.6042569279670715, acc: 0.8571428656578064)
[2025-02-13 19:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12][root][INFO] - Training Epoch: 1/2, step 321/7134 completed (loss: 0.6263619065284729, acc: 0.8505747318267822)
[2025-02-13 19:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12][root][INFO] - Training Epoch: 1/2, step 322/7134 completed (loss: 0.5240880846977234, acc: 0.8758170008659363)
[2025-02-13 19:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:13][root][INFO] - Training Epoch: 1/2, step 323/7134 completed (loss: 0.9106106758117676, acc: 0.7806122303009033)
[2025-02-13 19:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:13][root][INFO] - Training Epoch: 1/2, step 324/7134 completed (loss: 0.7969889640808105, acc: 0.8240740895271301)
[2025-02-13 19:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14][root][INFO] - Training Epoch: 1/2, step 325/7134 completed (loss: 0.5197258591651917, acc: 0.8920454382896423)
[2025-02-13 19:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14][root][INFO] - Training Epoch: 1/2, step 326/7134 completed (loss: 0.7099888324737549, acc: 0.7921348214149475)
[2025-02-13 19:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14][root][INFO] - Training Epoch: 1/2, step 327/7134 completed (loss: 0.538873016834259, acc: 0.8989899158477783)
[2025-02-13 19:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15][root][INFO] - Training Epoch: 1/2, step 328/7134 completed (loss: 0.5088526606559753, acc: 0.875)
[2025-02-13 19:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15][root][INFO] - Training Epoch: 1/2, step 329/7134 completed (loss: 0.5545021295547485, acc: 0.8905472755432129)
[2025-02-13 19:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16][root][INFO] - Training Epoch: 1/2, step 330/7134 completed (loss: 0.5093284845352173, acc: 0.890350878238678)
[2025-02-13 19:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16][root][INFO] - Training Epoch: 1/2, step 331/7134 completed (loss: 0.35474076867103577, acc: 0.9166666865348816)
[2025-02-13 19:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17][root][INFO] - Training Epoch: 1/2, step 332/7134 completed (loss: 0.2192400097846985, acc: 0.9402984976768494)
[2025-02-13 19:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17][root][INFO] - Training Epoch: 1/2, step 333/7134 completed (loss: 0.525902509689331, acc: 0.8602150678634644)
[2025-02-13 19:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17][root][INFO] - Training Epoch: 1/2, step 334/7134 completed (loss: 0.4771571457386017, acc: 0.8736263513565063)
[2025-02-13 19:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18][root][INFO] - Training Epoch: 1/2, step 335/7134 completed (loss: 0.40053239464759827, acc: 0.9256756901741028)
[2025-02-13 19:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18][root][INFO] - Training Epoch: 1/2, step 336/7134 completed (loss: 0.47581958770751953, acc: 0.8963730335235596)
[2025-02-13 19:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:19][root][INFO] - Training Epoch: 1/2, step 337/7134 completed (loss: 0.4530722200870514, acc: 0.8633540272712708)
[2025-02-13 19:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:19][root][INFO] - Training Epoch: 1/2, step 338/7134 completed (loss: 0.6299040913581848, acc: 0.8795811533927917)
[2025-02-13 19:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20][root][INFO] - Training Epoch: 1/2, step 339/7134 completed (loss: 0.4774776101112366, acc: 0.8770053386688232)
[2025-02-13 19:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20][root][INFO] - Training Epoch: 1/2, step 340/7134 completed (loss: 0.3498198091983795, acc: 0.8989899158477783)
[2025-02-13 19:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20][root][INFO] - Training Epoch: 1/2, step 341/7134 completed (loss: 0.3925597667694092, acc: 0.8928571343421936)
[2025-02-13 19:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21][root][INFO] - Training Epoch: 1/2, step 342/7134 completed (loss: 0.41849926114082336, acc: 0.893081784248352)
[2025-02-13 19:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21][root][INFO] - Training Epoch: 1/2, step 343/7134 completed (loss: 0.6451009511947632, acc: 0.8615384697914124)
[2025-02-13 19:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22][root][INFO] - Training Epoch: 1/2, step 344/7134 completed (loss: 0.3889249861240387, acc: 0.8992248177528381)
[2025-02-13 19:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22][root][INFO] - Training Epoch: 1/2, step 345/7134 completed (loss: 0.4033287465572357, acc: 0.914893627166748)
[2025-02-13 19:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22][root][INFO] - Training Epoch: 1/2, step 346/7134 completed (loss: 0.2831243574619293, acc: 0.9342105388641357)
[2025-02-13 19:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23][root][INFO] - Training Epoch: 1/2, step 347/7134 completed (loss: 0.5996924042701721, acc: 0.856249988079071)
[2025-02-13 19:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23][root][INFO] - Training Epoch: 1/2, step 348/7134 completed (loss: 0.4131132960319519, acc: 0.8994082808494568)
[2025-02-13 19:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24][root][INFO] - Training Epoch: 1/2, step 349/7134 completed (loss: 0.5526570677757263, acc: 0.88165682554245)
[2025-02-13 19:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24][root][INFO] - Training Epoch: 1/2, step 350/7134 completed (loss: 0.45554319024086, acc: 0.8894736766815186)
[2025-02-13 19:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24][root][INFO] - Training Epoch: 1/2, step 351/7134 completed (loss: 0.39022549986839294, acc: 0.929347813129425)
[2025-02-13 19:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:25][root][INFO] - Training Epoch: 1/2, step 352/7134 completed (loss: 0.37953364849090576, acc: 0.9415204524993896)
[2025-02-13 19:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:25][root][INFO] - Training Epoch: 1/2, step 353/7134 completed (loss: 0.5661580562591553, acc: 0.8711656332015991)
[2025-02-13 19:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26][root][INFO] - Training Epoch: 1/2, step 354/7134 completed (loss: 0.4955503046512604, acc: 0.8743718862533569)
[2025-02-13 19:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26][root][INFO] - Training Epoch: 1/2, step 355/7134 completed (loss: 0.4879060983657837, acc: 0.9053254723548889)
[2025-02-13 19:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27][root][INFO] - Training Epoch: 1/2, step 356/7134 completed (loss: 0.38286489248275757, acc: 0.9041916131973267)
[2025-02-13 19:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27][root][INFO] - Training Epoch: 1/2, step 357/7134 completed (loss: 0.31325456500053406, acc: 0.9011628031730652)
[2025-02-13 19:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27][root][INFO] - Training Epoch: 1/2, step 358/7134 completed (loss: 0.32059165835380554, acc: 0.9261363744735718)
[2025-02-13 19:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:28][root][INFO] - Training Epoch: 1/2, step 359/7134 completed (loss: 0.5135911107063293, acc: 0.8882352709770203)
[2025-02-13 19:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:28][root][INFO] - Training Epoch: 1/2, step 360/7134 completed (loss: 0.2563535273075104, acc: 0.9281045794487)
[2025-02-13 19:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29][root][INFO] - Training Epoch: 1/2, step 361/7134 completed (loss: 0.28483396768569946, acc: 0.9263803958892822)
[2025-02-13 19:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29][root][INFO] - Training Epoch: 1/2, step 362/7134 completed (loss: 0.39499059319496155, acc: 0.8987341523170471)
[2025-02-13 19:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29][root][INFO] - Training Epoch: 1/2, step 363/7134 completed (loss: 0.5291725993156433, acc: 0.8852459192276001)
[2025-02-13 19:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:30][root][INFO] - Training Epoch: 1/2, step 364/7134 completed (loss: 0.48359087109565735, acc: 0.8640776872634888)
[2025-02-13 19:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:30][root][INFO] - Training Epoch: 1/2, step 365/7134 completed (loss: 0.31953221559524536, acc: 0.9113923907279968)
[2025-02-13 19:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31][root][INFO] - Training Epoch: 1/2, step 366/7134 completed (loss: 0.23404176533222198, acc: 0.9490445852279663)
[2025-02-13 19:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31][root][INFO] - Training Epoch: 1/2, step 367/7134 completed (loss: 0.2942795157432556, acc: 0.9175257682800293)
[2025-02-13 19:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32][root][INFO] - Training Epoch: 1/2, step 368/7134 completed (loss: 0.22848322987556458, acc: 0.9230769276618958)
[2025-02-13 19:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32][root][INFO] - Training Epoch: 1/2, step 369/7134 completed (loss: 0.41420778632164, acc: 0.8895705342292786)
[2025-02-13 19:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32][root][INFO] - Training Epoch: 1/2, step 370/7134 completed (loss: 0.4408978819847107, acc: 0.8860759735107422)
[2025-02-13 19:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:33][root][INFO] - Training Epoch: 1/2, step 371/7134 completed (loss: 0.32993078231811523, acc: 0.9192546606063843)
[2025-02-13 19:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:33][root][INFO] - Training Epoch: 1/2, step 372/7134 completed (loss: 0.42015746235847473, acc: 0.9245283007621765)
[2025-02-13 19:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34][root][INFO] - Training Epoch: 1/2, step 373/7134 completed (loss: 0.30294305086135864, acc: 0.9215686321258545)
[2025-02-13 19:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34][root][INFO] - Training Epoch: 1/2, step 374/7134 completed (loss: 0.31853845715522766, acc: 0.9189189076423645)
[2025-02-13 19:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35][root][INFO] - Training Epoch: 1/2, step 375/7134 completed (loss: 0.47310081124305725, acc: 0.8829787373542786)
[2025-02-13 19:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35][root][INFO] - Training Epoch: 1/2, step 376/7134 completed (loss: 0.6991567015647888, acc: 0.855555534362793)
[2025-02-13 19:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35][root][INFO] - Training Epoch: 1/2, step 377/7134 completed (loss: 0.4915759563446045, acc: 0.8826815485954285)
[2025-02-13 19:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36][root][INFO] - Training Epoch: 1/2, step 378/7134 completed (loss: 0.5382866263389587, acc: 0.8531073331832886)
[2025-02-13 19:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36][root][INFO] - Training Epoch: 1/2, step 379/7134 completed (loss: 0.15166670083999634, acc: 0.970588207244873)
[2025-02-13 19:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37][root][INFO] - Training Epoch: 1/2, step 380/7134 completed (loss: 0.7055771946907043, acc: 0.8350515365600586)
[2025-02-13 19:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37][root][INFO] - Training Epoch: 1/2, step 381/7134 completed (loss: 0.36065223813056946, acc: 0.9315789341926575)
[2025-02-13 19:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37][root][INFO] - Training Epoch: 1/2, step 382/7134 completed (loss: 0.32620587944984436, acc: 0.9128205180168152)
[2025-02-13 19:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38][root][INFO] - Training Epoch: 1/2, step 383/7134 completed (loss: 0.3020874857902527, acc: 0.9379844665527344)
[2025-02-13 19:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38][root][INFO] - Training Epoch: 1/2, step 384/7134 completed (loss: 0.3818246126174927, acc: 0.9447513818740845)
[2025-02-13 19:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38][root][INFO] - Training Epoch: 1/2, step 385/7134 completed (loss: 0.2542096674442291, acc: 0.9346405267715454)
[2025-02-13 19:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39][root][INFO] - Training Epoch: 1/2, step 386/7134 completed (loss: 0.3084651827812195, acc: 0.9369369149208069)
[2025-02-13 19:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39][root][INFO] - Training Epoch: 1/2, step 387/7134 completed (loss: 0.300613135099411, acc: 0.8934911489486694)
[2025-02-13 19:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40][root][INFO] - Training Epoch: 1/2, step 388/7134 completed (loss: 0.2610839903354645, acc: 0.9186602830886841)
[2025-02-13 19:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40][root][INFO] - Training Epoch: 1/2, step 389/7134 completed (loss: 0.3025982081890106, acc: 0.9200000166893005)
[2025-02-13 19:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40][root][INFO] - Training Epoch: 1/2, step 390/7134 completed (loss: 0.32026129961013794, acc: 0.912162184715271)
[2025-02-13 19:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41][root][INFO] - Training Epoch: 1/2, step 391/7134 completed (loss: 0.2639274597167969, acc: 0.9200000166893005)
[2025-02-13 19:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41][root][INFO] - Training Epoch: 1/2, step 392/7134 completed (loss: 0.2663424611091614, acc: 0.930232584476471)
[2025-02-13 19:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42][root][INFO] - Training Epoch: 1/2, step 393/7134 completed (loss: 0.23064912855625153, acc: 0.9197860956192017)
[2025-02-13 19:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42][root][INFO] - Training Epoch: 1/2, step 394/7134 completed (loss: 0.32826662063598633, acc: 0.9236640930175781)
[2025-02-13 19:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42][root][INFO] - Training Epoch: 1/2, step 395/7134 completed (loss: 0.3398951292037964, acc: 0.936170220375061)
[2025-02-13 19:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43][root][INFO] - Training Epoch: 1/2, step 396/7134 completed (loss: 0.2811499834060669, acc: 0.9137930870056152)
[2025-02-13 19:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43][root][INFO] - Training Epoch: 1/2, step 397/7134 completed (loss: 0.48495930433273315, acc: 0.9130434989929199)
[2025-02-13 19:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44][root][INFO] - Training Epoch: 1/2, step 398/7134 completed (loss: 0.3939830958843231, acc: 0.9040403962135315)
[2025-02-13 19:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44][root][INFO] - Training Epoch: 1/2, step 399/7134 completed (loss: 0.3941807448863983, acc: 0.8860759735107422)
[2025-02-13 19:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44][root][INFO] - Training Epoch: 1/2, step 400/7134 completed (loss: 0.6194868087768555, acc: 0.8734177350997925)
[2025-02-13 19:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45][root][INFO] - Training Epoch: 1/2, step 401/7134 completed (loss: 0.4649272561073303, acc: 0.8725489974021912)
[2025-02-13 19:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45][root][INFO] - Training Epoch: 1/2, step 402/7134 completed (loss: 0.39369815587997437, acc: 0.909547746181488)
[2025-02-13 19:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46][root][INFO] - Training Epoch: 1/2, step 403/7134 completed (loss: 0.36998116970062256, acc: 0.903930127620697)
[2025-02-13 19:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46][root][INFO] - Training Epoch: 1/2, step 404/7134 completed (loss: 0.40392303466796875, acc: 0.9032257795333862)
[2025-02-13 19:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46][root][INFO] - Training Epoch: 1/2, step 405/7134 completed (loss: 0.6577260494232178, acc: 0.819767415523529)
[2025-02-13 19:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47][root][INFO] - Training Epoch: 1/2, step 406/7134 completed (loss: 0.7626093626022339, acc: 0.837837815284729)
[2025-02-13 19:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47][root][INFO] - Training Epoch: 1/2, step 407/7134 completed (loss: 0.8440098762512207, acc: 0.808917224407196)
[2025-02-13 19:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48][root][INFO] - Training Epoch: 1/2, step 408/7134 completed (loss: 0.4813176095485687, acc: 0.8659217953681946)
[2025-02-13 19:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48][root][INFO] - Training Epoch: 1/2, step 409/7134 completed (loss: 0.46066588163375854, acc: 0.8842105269432068)
[2025-02-13 19:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48][root][INFO] - Training Epoch: 1/2, step 410/7134 completed (loss: 0.46214234828948975, acc: 0.8969696760177612)
[2025-02-13 19:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49][root][INFO] - Training Epoch: 1/2, step 411/7134 completed (loss: 0.45198264718055725, acc: 0.9144144058227539)
[2025-02-13 19:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49][root][INFO] - Training Epoch: 1/2, step 412/7134 completed (loss: 0.432922899723053, acc: 0.8834080696105957)
[2025-02-13 19:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50][root][INFO] - Training Epoch: 1/2, step 413/7134 completed (loss: 0.48874425888061523, acc: 0.8691099286079407)
[2025-02-13 19:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50][root][INFO] - Training Epoch: 1/2, step 414/7134 completed (loss: 0.44546979665756226, acc: 0.8728323578834534)
[2025-02-13 19:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50][root][INFO] - Training Epoch: 1/2, step 415/7134 completed (loss: 0.32918795943260193, acc: 0.9095237851142883)
[2025-02-13 19:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51][root][INFO] - Training Epoch: 1/2, step 416/7134 completed (loss: 0.22830723226070404, acc: 0.9471153616905212)
[2025-02-13 19:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51][root][INFO] - Training Epoch: 1/2, step 417/7134 completed (loss: 0.5581299066543579, acc: 0.8516483306884766)
[2025-02-13 19:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52][root][INFO] - Training Epoch: 1/2, step 418/7134 completed (loss: 0.33145609498023987, acc: 0.9275362491607666)
[2025-02-13 19:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52][root][INFO] - Training Epoch: 1/2, step 419/7134 completed (loss: 0.39770209789276123, acc: 0.905940592288971)
[2025-02-13 19:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52][root][INFO] - Training Epoch: 1/2, step 420/7134 completed (loss: 0.446768194437027, acc: 0.8857142925262451)
[2025-02-13 19:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53][root][INFO] - Training Epoch: 1/2, step 421/7134 completed (loss: 0.3444127142429352, acc: 0.9056603908538818)
[2025-02-13 19:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53][root][INFO] - Training Epoch: 1/2, step 422/7134 completed (loss: 0.3297082483768463, acc: 0.9171597361564636)
[2025-02-13 19:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54][root][INFO] - Training Epoch: 1/2, step 423/7134 completed (loss: 0.1939970999956131, acc: 0.9606741666793823)
[2025-02-13 19:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54][root][INFO] - Training Epoch: 1/2, step 424/7134 completed (loss: 0.399098664522171, acc: 0.9012345671653748)
[2025-02-13 19:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55][root][INFO] - Training Epoch: 1/2, step 425/7134 completed (loss: 0.16403593122959137, acc: 0.9488636255264282)
[2025-02-13 19:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55][root][INFO] - Training Epoch: 1/2, step 426/7134 completed (loss: 0.23092158138751984, acc: 0.9411764740943909)
[2025-02-13 19:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55][root][INFO] - Training Epoch: 1/2, step 427/7134 completed (loss: 0.25229936838150024, acc: 0.9433962106704712)
[2025-02-13 19:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56][root][INFO] - Training Epoch: 1/2, step 428/7134 completed (loss: 0.3646153509616852, acc: 0.916167676448822)
[2025-02-13 19:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56][root][INFO] - Training Epoch: 1/2, step 429/7134 completed (loss: 0.4688758850097656, acc: 0.8815789222717285)
[2025-02-13 19:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57][root][INFO] - Training Epoch: 1/2, step 430/7134 completed (loss: 0.24713529646396637, acc: 0.929411768913269)
[2025-02-13 19:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57][root][INFO] - Training Epoch: 1/2, step 431/7134 completed (loss: 0.25466910004615784, acc: 0.9526627063751221)
[2025-02-13 19:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57][root][INFO] - Training Epoch: 1/2, step 432/7134 completed (loss: 0.3426354229450226, acc: 0.9166666865348816)
[2025-02-13 19:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:58][root][INFO] - Training Epoch: 1/2, step 433/7134 completed (loss: 0.4199989438056946, acc: 0.9202127456665039)
[2025-02-13 19:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:58][root][INFO] - Training Epoch: 1/2, step 434/7134 completed (loss: 0.2716502547264099, acc: 0.9382715821266174)
[2025-02-13 19:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59][root][INFO] - Training Epoch: 1/2, step 435/7134 completed (loss: 0.22653408348560333, acc: 0.9388889074325562)
[2025-02-13 19:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59][root][INFO] - Training Epoch: 1/2, step 436/7134 completed (loss: 0.3306231200695038, acc: 0.9375)
[2025-02-13 19:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59][root][INFO] - Training Epoch: 1/2, step 437/7134 completed (loss: 0.3287029564380646, acc: 0.9295774698257446)
[2025-02-13 19:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:00][root][INFO] - Training Epoch: 1/2, step 438/7134 completed (loss: 0.11468101292848587, acc: 0.9685039520263672)
[2025-02-13 19:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:00][root][INFO] - Training Epoch: 1/2, step 439/7134 completed (loss: 0.4379507601261139, acc: 0.8908045887947083)
[2025-02-13 19:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01][root][INFO] - Training Epoch: 1/2, step 440/7134 completed (loss: 0.49657681584358215, acc: 0.8846153616905212)
[2025-02-13 19:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01][root][INFO] - Training Epoch: 1/2, step 441/7134 completed (loss: 0.18835079669952393, acc: 0.9589040875434875)
[2025-02-13 19:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01][root][INFO] - Training Epoch: 1/2, step 442/7134 completed (loss: 0.11947913467884064, acc: 0.9689922332763672)
[2025-02-13 19:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02][root][INFO] - Training Epoch: 1/2, step 443/7134 completed (loss: 0.13852019608020782, acc: 0.9659863710403442)
[2025-02-13 19:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02][root][INFO] - Training Epoch: 1/2, step 444/7134 completed (loss: 0.10733714699745178, acc: 0.9767441749572754)
[2025-02-13 19:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03][root][INFO] - Training Epoch: 1/2, step 445/7134 completed (loss: 0.48677119612693787, acc: 0.8805031180381775)
[2025-02-13 19:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03][root][INFO] - Training Epoch: 1/2, step 446/7134 completed (loss: 0.2748541235923767, acc: 0.9171597361564636)
[2025-02-13 19:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03][root][INFO] - Training Epoch: 1/2, step 447/7134 completed (loss: 0.19693943858146667, acc: 0.9447852969169617)
[2025-02-13 19:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04][root][INFO] - Training Epoch: 1/2, step 448/7134 completed (loss: 0.6096428632736206, acc: 0.8888888955116272)
[2025-02-13 19:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04][root][INFO] - Training Epoch: 1/2, step 449/7134 completed (loss: 0.6188408732414246, acc: 0.835106372833252)
[2025-02-13 19:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05][root][INFO] - Training Epoch: 1/2, step 450/7134 completed (loss: 0.4709816575050354, acc: 0.8773584961891174)
[2025-02-13 19:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05][root][INFO] - Training Epoch: 1/2, step 451/7134 completed (loss: 0.9429329037666321, acc: 0.8177965879440308)
[2025-02-13 19:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05][root][INFO] - Training Epoch: 1/2, step 452/7134 completed (loss: 0.8375905156135559, acc: 0.8426966071128845)
[2025-02-13 19:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06][root][INFO] - Training Epoch: 1/2, step 453/7134 completed (loss: 0.5282507538795471, acc: 0.8581560254096985)
[2025-02-13 19:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06][root][INFO] - Training Epoch: 1/2, step 454/7134 completed (loss: 0.7466992139816284, acc: 0.8565400838851929)
[2025-02-13 19:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07][root][INFO] - Training Epoch: 1/2, step 455/7134 completed (loss: 0.7756345272064209, acc: 0.800000011920929)
[2025-02-13 19:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07][root][INFO] - Training Epoch: 1/2, step 456/7134 completed (loss: 0.44477179646492004, acc: 0.8895705342292786)
[2025-02-13 19:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07][root][INFO] - Training Epoch: 1/2, step 457/7134 completed (loss: 0.7040966749191284, acc: 0.8965517282485962)
[2025-02-13 19:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08][root][INFO] - Training Epoch: 1/2, step 458/7134 completed (loss: 0.44116005301475525, acc: 0.9032257795333862)
[2025-02-13 19:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08][root][INFO] - Training Epoch: 1/2, step 459/7134 completed (loss: 0.6896243095397949, acc: 0.8253012299537659)
[2025-02-13 19:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09][root][INFO] - Training Epoch: 1/2, step 460/7134 completed (loss: 0.5721475481987, acc: 0.8733333349227905)
[2025-02-13 19:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09][root][INFO] - Training Epoch: 1/2, step 461/7134 completed (loss: 0.5030266642570496, acc: 0.8882352709770203)
[2025-02-13 19:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09][root][INFO] - Training Epoch: 1/2, step 462/7134 completed (loss: 0.43511733412742615, acc: 0.8796296119689941)
[2025-02-13 19:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10][root][INFO] - Training Epoch: 1/2, step 463/7134 completed (loss: 0.7303907871246338, acc: 0.8507462739944458)
[2025-02-13 19:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10][root][INFO] - Training Epoch: 1/2, step 464/7134 completed (loss: 0.573192834854126, acc: 0.8693181872367859)
[2025-02-13 19:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11][root][INFO] - Training Epoch: 1/2, step 465/7134 completed (loss: 0.46192944049835205, acc: 0.8938547372817993)
[2025-02-13 19:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11][root][INFO] - Training Epoch: 1/2, step 466/7134 completed (loss: 0.5545563697814941, acc: 0.871345043182373)
[2025-02-13 19:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11][root][INFO] - Training Epoch: 1/2, step 467/7134 completed (loss: 0.7120916843414307, acc: 0.8095238208770752)
[2025-02-13 19:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12][root][INFO] - Training Epoch: 1/2, step 468/7134 completed (loss: 0.6805780529975891, acc: 0.8533333539962769)
[2025-02-13 19:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12][root][INFO] - Training Epoch: 1/2, step 469/7134 completed (loss: 0.45442211627960205, acc: 0.8819444179534912)
[2025-02-13 19:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12][root][INFO] - Training Epoch: 1/2, step 470/7134 completed (loss: 0.6887401342391968, acc: 0.837837815284729)
[2025-02-13 19:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13][root][INFO] - Training Epoch: 1/2, step 471/7134 completed (loss: 0.6305826306343079, acc: 0.8406593203544617)
[2025-02-13 19:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13][root][INFO] - Training Epoch: 1/2, step 472/7134 completed (loss: 0.6151256561279297, acc: 0.8733333349227905)
[2025-02-13 19:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14][root][INFO] - Training Epoch: 1/2, step 473/7134 completed (loss: 0.5028643012046814, acc: 0.8409090638160706)
[2025-02-13 19:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14][root][INFO] - Training Epoch: 1/2, step 474/7134 completed (loss: 0.6089457869529724, acc: 0.8484848737716675)
[2025-02-13 19:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14][root][INFO] - Training Epoch: 1/2, step 475/7134 completed (loss: 0.8567658066749573, acc: 0.7972028255462646)
[2025-02-13 19:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:15][root][INFO] - Training Epoch: 1/2, step 476/7134 completed (loss: 0.8621028065681458, acc: 0.831250011920929)
[2025-02-13 19:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:15][root][INFO] - Training Epoch: 1/2, step 477/7134 completed (loss: 0.6636945605278015, acc: 0.8342245817184448)
[2025-02-13 19:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16][root][INFO] - Training Epoch: 1/2, step 478/7134 completed (loss: 0.827393651008606, acc: 0.8173912763595581)
[2025-02-13 19:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16][root][INFO] - Training Epoch: 1/2, step 479/7134 completed (loss: 0.6387744545936584, acc: 0.8476821184158325)
[2025-02-13 19:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16][root][INFO] - Training Epoch: 1/2, step 480/7134 completed (loss: 0.5481510758399963, acc: 0.8687499761581421)
[2025-02-13 19:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17][root][INFO] - Training Epoch: 1/2, step 481/7134 completed (loss: 0.4845999479293823, acc: 0.8944099545478821)
[2025-02-13 19:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17][root][INFO] - Training Epoch: 1/2, step 482/7134 completed (loss: 0.3106658458709717, acc: 0.8993710875511169)
[2025-02-13 19:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18][root][INFO] - Training Epoch: 1/2, step 483/7134 completed (loss: 0.42183631658554077, acc: 0.901098906993866)
[2025-02-13 19:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18][root][INFO] - Training Epoch: 1/2, step 484/7134 completed (loss: 0.5512004494667053, acc: 0.8571428656578064)
[2025-02-13 19:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18][root][INFO] - Training Epoch: 1/2, step 485/7134 completed (loss: 0.5851253271102905, acc: 0.9006211161613464)
[2025-02-13 19:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19][root][INFO] - Training Epoch: 1/2, step 486/7134 completed (loss: 0.7565397024154663, acc: 0.8064516186714172)
[2025-02-13 19:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19][root][INFO] - Training Epoch: 1/2, step 487/7134 completed (loss: 0.38123711943626404, acc: 0.9197530746459961)
[2025-02-13 19:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20][root][INFO] - Training Epoch: 1/2, step 488/7134 completed (loss: 0.8178832530975342, acc: 0.8432835936546326)
[2025-02-13 19:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20][root][INFO] - Training Epoch: 1/2, step 489/7134 completed (loss: 0.81777024269104, acc: 0.8633093237876892)
[2025-02-13 19:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20][root][INFO] - Training Epoch: 1/2, step 490/7134 completed (loss: 0.5238394737243652, acc: 0.8860759735107422)
[2025-02-13 19:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21][root][INFO] - Training Epoch: 1/2, step 491/7134 completed (loss: 0.39886701107025146, acc: 0.9027777910232544)
[2025-02-13 19:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21][root][INFO] - Training Epoch: 1/2, step 492/7134 completed (loss: 0.16871020197868347, acc: 0.9677419066429138)
[2025-02-13 19:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21][root][INFO] - Training Epoch: 1/2, step 493/7134 completed (loss: 0.4606621563434601, acc: 0.8994082808494568)
[2025-02-13 19:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:22][root][INFO] - Training Epoch: 1/2, step 494/7134 completed (loss: 0.2708034813404083, acc: 0.9466666579246521)
[2025-02-13 19:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:22][root][INFO] - Training Epoch: 1/2, step 495/7134 completed (loss: 0.36440348625183105, acc: 0.93388432264328)
[2025-02-13 19:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23][root][INFO] - Training Epoch: 1/2, step 496/7134 completed (loss: 0.8829206824302673, acc: 0.8105263113975525)
[2025-02-13 19:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23][root][INFO] - Training Epoch: 1/2, step 497/7134 completed (loss: 0.47024786472320557, acc: 0.892307698726654)
[2025-02-13 19:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23][root][INFO] - Training Epoch: 1/2, step 498/7134 completed (loss: 0.4737303555011749, acc: 0.899328887462616)
[2025-02-13 19:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24][root][INFO] - Training Epoch: 1/2, step 499/7134 completed (loss: 0.33813971281051636, acc: 0.8999999761581421)
[2025-02-13 19:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24][root][INFO] - Training Epoch: 1/2, step 500/7134 completed (loss: 0.3606661558151245, acc: 0.9069767594337463)
[2025-02-13 19:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25][root][INFO] - Training Epoch: 1/2, step 501/7134 completed (loss: 0.314119428396225, acc: 0.9306358098983765)
[2025-02-13 19:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25][root][INFO] - Training Epoch: 1/2, step 502/7134 completed (loss: 0.2245440036058426, acc: 0.948051929473877)
[2025-02-13 19:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25][root][INFO] - Training Epoch: 1/2, step 503/7134 completed (loss: 0.3298824727535248, acc: 0.9147727489471436)
[2025-02-13 19:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26][root][INFO] - Training Epoch: 1/2, step 504/7134 completed (loss: 0.2561817765235901, acc: 0.9440993666648865)
[2025-02-13 19:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26][root][INFO] - Training Epoch: 1/2, step 505/7134 completed (loss: 0.16727446019649506, acc: 0.9523809552192688)
[2025-02-13 19:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27][root][INFO] - Training Epoch: 1/2, step 506/7134 completed (loss: 0.2878310978412628, acc: 0.9431818127632141)
[2025-02-13 19:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27][root][INFO] - Training Epoch: 1/2, step 507/7134 completed (loss: 0.32910361886024475, acc: 0.9016393423080444)
[2025-02-13 19:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28][root][INFO] - Training Epoch: 1/2, step 508/7134 completed (loss: 0.2971750497817993, acc: 0.9411764740943909)
[2025-02-13 19:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28][root][INFO] - Training Epoch: 1/2, step 509/7134 completed (loss: 0.3725396692752838, acc: 0.9324324131011963)
[2025-02-13 19:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28][root][INFO] - Training Epoch: 1/2, step 510/7134 completed (loss: 0.6865954399108887, acc: 0.8780487775802612)
[2025-02-13 19:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29][root][INFO] - Training Epoch: 1/2, step 511/7134 completed (loss: 0.46250975131988525, acc: 0.8857142925262451)
[2025-02-13 19:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29][root][INFO] - Training Epoch: 1/2, step 512/7134 completed (loss: 0.6813270449638367, acc: 0.8702290058135986)
[2025-02-13 19:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30][root][INFO] - Training Epoch: 1/2, step 513/7134 completed (loss: 0.48570770025253296, acc: 0.9194630980491638)
[2025-02-13 19:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30][root][INFO] - Training Epoch: 1/2, step 514/7134 completed (loss: 0.8352805972099304, acc: 0.8611111044883728)
[2025-02-13 19:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30][root][INFO] - Training Epoch: 1/2, step 515/7134 completed (loss: 0.3464232087135315, acc: 0.9107142686843872)
[2025-02-13 19:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31][root][INFO] - Training Epoch: 1/2, step 516/7134 completed (loss: 0.39692169427871704, acc: 0.9152542352676392)
[2025-02-13 19:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31][root][INFO] - Training Epoch: 1/2, step 517/7134 completed (loss: 0.5166209936141968, acc: 0.9057971239089966)
[2025-02-13 19:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31][root][INFO] - Training Epoch: 1/2, step 518/7134 completed (loss: 0.36226561665534973, acc: 0.9536423683166504)
[2025-02-13 19:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:32][root][INFO] - Training Epoch: 1/2, step 519/7134 completed (loss: 0.36459052562713623, acc: 0.9130434989929199)
[2025-02-13 19:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:32][root][INFO] - Training Epoch: 1/2, step 520/7134 completed (loss: 0.44138050079345703, acc: 0.8815789222717285)
[2025-02-13 19:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33][root][INFO] - Training Epoch: 1/2, step 521/7134 completed (loss: 0.4084458351135254, acc: 0.9122806787490845)
[2025-02-13 19:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33][root][INFO] - Training Epoch: 1/2, step 522/7134 completed (loss: 0.3097747564315796, acc: 0.9101123809814453)
[2025-02-13 19:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33][root][INFO] - Training Epoch: 1/2, step 523/7134 completed (loss: 0.32207030057907104, acc: 0.9244186282157898)
[2025-02-13 19:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:34][root][INFO] - Training Epoch: 1/2, step 524/7134 completed (loss: 0.3177867531776428, acc: 0.9378530979156494)
[2025-02-13 19:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:34][root][INFO] - Training Epoch: 1/2, step 525/7134 completed (loss: 0.4383373260498047, acc: 0.9018405079841614)
[2025-02-13 19:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35][root][INFO] - Training Epoch: 1/2, step 526/7134 completed (loss: 0.28579550981521606, acc: 0.9204545617103577)
[2025-02-13 19:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35][root][INFO] - Training Epoch: 1/2, step 527/7134 completed (loss: 0.3523220419883728, acc: 0.9375)
[2025-02-13 19:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35][root][INFO] - Training Epoch: 1/2, step 528/7134 completed (loss: 0.23213116824626923, acc: 0.9482758641242981)
[2025-02-13 19:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36][root][INFO] - Training Epoch: 1/2, step 529/7134 completed (loss: 0.368621289730072, acc: 0.8980891704559326)
[2025-02-13 19:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36][root][INFO] - Training Epoch: 1/2, step 530/7134 completed (loss: 0.3185199201107025, acc: 0.9307692050933838)
[2025-02-13 19:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37][root][INFO] - Training Epoch: 1/2, step 531/7134 completed (loss: 0.5558915734291077, acc: 0.8835616707801819)
[2025-02-13 19:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37][root][INFO] - Training Epoch: 1/2, step 532/7134 completed (loss: 0.22117699682712555, acc: 0.9734513163566589)
[2025-02-13 19:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37][root][INFO] - Training Epoch: 1/2, step 533/7134 completed (loss: 0.6273732781410217, acc: 0.8503401279449463)
[2025-02-13 19:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:38][root][INFO] - Training Epoch: 1/2, step 534/7134 completed (loss: 0.260448157787323, acc: 0.9281437397003174)
[2025-02-13 19:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:38][root][INFO] - Training Epoch: 1/2, step 535/7134 completed (loss: 0.4434986114501953, acc: 0.8978102207183838)
[2025-02-13 19:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39][root][INFO] - Training Epoch: 1/2, step 536/7134 completed (loss: 0.3175298869609833, acc: 0.9146341681480408)
[2025-02-13 19:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39][root][INFO] - Training Epoch: 1/2, step 537/7134 completed (loss: 0.3875678479671478, acc: 0.884393036365509)
[2025-02-13 19:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40][root][INFO] - Training Epoch: 1/2, step 538/7134 completed (loss: 0.2542734146118164, acc: 0.9457831382751465)
[2025-02-13 19:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40][root][INFO] - Training Epoch: 1/2, step 539/7134 completed (loss: 0.467326283454895, acc: 0.8931297659873962)
[2025-02-13 19:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40][root][INFO] - Training Epoch: 1/2, step 540/7134 completed (loss: 0.3956427574157715, acc: 0.9056603908538818)
[2025-02-13 19:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41][root][INFO] - Training Epoch: 1/2, step 541/7134 completed (loss: 0.4881768524646759, acc: 0.8928571343421936)
[2025-02-13 19:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41][root][INFO] - Training Epoch: 1/2, step 542/7134 completed (loss: 0.795041561126709, acc: 0.8100000023841858)
[2025-02-13 19:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41][root][INFO] - Training Epoch: 1/2, step 543/7134 completed (loss: 0.7111327648162842, acc: 0.8157894611358643)
[2025-02-13 19:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42][root][INFO] - Training Epoch: 1/2, step 544/7134 completed (loss: 0.7430999279022217, acc: 0.8357142806053162)
[2025-02-13 19:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42][root][INFO] - Training Epoch: 1/2, step 545/7134 completed (loss: 0.7061095237731934, acc: 0.8045112490653992)
[2025-02-13 19:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43][root][INFO] - Training Epoch: 1/2, step 546/7134 completed (loss: 1.010571002960205, acc: 0.7870370149612427)
[2025-02-13 19:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43][root][INFO] - Training Epoch: 1/2, step 547/7134 completed (loss: 0.4993021488189697, acc: 0.8741722106933594)
[2025-02-13 19:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43][root][INFO] - Training Epoch: 1/2, step 548/7134 completed (loss: 0.48609432578086853, acc: 0.9210526347160339)
[2025-02-13 19:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44][root][INFO] - Training Epoch: 1/2, step 549/7134 completed (loss: 1.0273816585540771, acc: 0.7642857432365417)
[2025-02-13 19:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44][root][INFO] - Training Epoch: 1/2, step 550/7134 completed (loss: 0.4881616234779358, acc: 0.8582677245140076)
[2025-02-13 19:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45][root][INFO] - Training Epoch: 1/2, step 551/7134 completed (loss: 0.6349056959152222, acc: 0.8203125)
[2025-02-13 19:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45][root][INFO] - Training Epoch: 1/2, step 552/7134 completed (loss: 0.4670916199684143, acc: 0.884353756904602)
[2025-02-13 19:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45][root][INFO] - Training Epoch: 1/2, step 553/7134 completed (loss: 0.5176678895950317, acc: 0.8631578683853149)
[2025-02-13 19:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46][root][INFO] - Training Epoch: 1/2, step 554/7134 completed (loss: 0.7407788038253784, acc: 0.8407643437385559)
[2025-02-13 19:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46][root][INFO] - Training Epoch: 1/2, step 555/7134 completed (loss: 0.6858254671096802, acc: 0.8571428656578064)
[2025-02-13 19:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47][root][INFO] - Training Epoch: 1/2, step 556/7134 completed (loss: 0.26744022965431213, acc: 0.9375)
[2025-02-13 19:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47][root][INFO] - Training Epoch: 1/2, step 557/7134 completed (loss: 0.5913432240486145, acc: 0.8652482032775879)
[2025-02-13 19:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47][root][INFO] - Training Epoch: 1/2, step 558/7134 completed (loss: 0.595709502696991, acc: 0.8782051205635071)
[2025-02-13 19:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48][root][INFO] - Training Epoch: 1/2, step 559/7134 completed (loss: 0.609456479549408, acc: 0.8523489832878113)
[2025-02-13 19:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48][root][INFO] - Training Epoch: 1/2, step 560/7134 completed (loss: 0.2712230086326599, acc: 0.9430894255638123)
[2025-02-13 19:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49][root][INFO] - Training Epoch: 1/2, step 561/7134 completed (loss: 0.6202837228775024, acc: 0.893750011920929)
[2025-02-13 19:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49][root][INFO] - Training Epoch: 1/2, step 562/7134 completed (loss: 0.5096132755279541, acc: 0.8571428656578064)
[2025-02-13 19:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49][root][INFO] - Training Epoch: 1/2, step 563/7134 completed (loss: 0.9109007120132446, acc: 0.811188817024231)
[2025-02-13 19:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50][root][INFO] - Training Epoch: 1/2, step 564/7134 completed (loss: 0.6195951104164124, acc: 0.847953200340271)
[2025-02-13 19:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50][root][INFO] - Training Epoch: 1/2, step 565/7134 completed (loss: 0.5259114503860474, acc: 0.874015748500824)
[2025-02-13 19:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50][root][INFO] - Training Epoch: 1/2, step 566/7134 completed (loss: 0.6720572113990784, acc: 0.7747747898101807)
[2025-02-13 19:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51][root][INFO] - Training Epoch: 1/2, step 567/7134 completed (loss: 0.4740082025527954, acc: 0.8622754216194153)
[2025-02-13 19:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51][root][INFO] - Training Epoch: 1/2, step 568/7134 completed (loss: 0.4284698963165283, acc: 0.8736842274665833)
[2025-02-13 19:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52][root][INFO] - Training Epoch: 1/2, step 569/7134 completed (loss: 0.4444276988506317, acc: 0.9050279259681702)
[2025-02-13 19:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52][root][INFO] - Training Epoch: 1/2, step 570/7134 completed (loss: 0.516741156578064, acc: 0.8656716346740723)
[2025-02-13 19:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52][root][INFO] - Training Epoch: 1/2, step 571/7134 completed (loss: 0.2515791058540344, acc: 0.9285714030265808)
[2025-02-13 19:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:53][root][INFO] - Training Epoch: 1/2, step 572/7134 completed (loss: 0.25513124465942383, acc: 0.9314285516738892)
[2025-02-13 19:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:53][root][INFO] - Training Epoch: 1/2, step 573/7134 completed (loss: 0.42190417647361755, acc: 0.8848484754562378)
[2025-02-13 19:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54][root][INFO] - Training Epoch: 1/2, step 574/7134 completed (loss: 0.3002602756023407, acc: 0.9408283829689026)
[2025-02-13 19:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54][root][INFO] - Training Epoch: 1/2, step 575/7134 completed (loss: 0.39751774072647095, acc: 0.9122806787490845)
[2025-02-13 19:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54][root][INFO] - Training Epoch: 1/2, step 576/7134 completed (loss: 0.2903331220149994, acc: 0.9247311949729919)
[2025-02-13 19:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:55][root][INFO] - Training Epoch: 1/2, step 577/7134 completed (loss: 0.48866111040115356, acc: 0.8969696760177612)
[2025-02-13 19:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:55][root][INFO] - Training Epoch: 1/2, step 578/7134 completed (loss: 0.3959231674671173, acc: 0.9004974961280823)
[2025-02-13 19:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56][root][INFO] - Training Epoch: 1/2, step 579/7134 completed (loss: 0.4907323718070984, acc: 0.8817204236984253)
[2025-02-13 19:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56][root][INFO] - Training Epoch: 1/2, step 580/7134 completed (loss: 0.731951117515564, acc: 0.8333333134651184)
[2025-02-13 19:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56][root][INFO] - Training Epoch: 1/2, step 581/7134 completed (loss: 0.2733415365219116, acc: 0.9407894611358643)
[2025-02-13 19:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:57][root][INFO] - Training Epoch: 1/2, step 582/7134 completed (loss: 0.37228360772132874, acc: 0.91847825050354)
[2025-02-13 19:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:57][root][INFO] - Training Epoch: 1/2, step 583/7134 completed (loss: 0.3886251449584961, acc: 0.8982036113739014)
[2025-02-13 19:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58][root][INFO] - Training Epoch: 1/2, step 584/7134 completed (loss: 0.2510021924972534, acc: 0.9344262480735779)
[2025-02-13 19:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58][root][INFO] - Training Epoch: 1/2, step 585/7134 completed (loss: 0.29607346653938293, acc: 0.9351351261138916)
[2025-02-13 19:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58][root][INFO] - Training Epoch: 1/2, step 586/7134 completed (loss: 0.2094608098268509, acc: 0.9308510422706604)
[2025-02-13 19:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:59][root][INFO] - Training Epoch: 1/2, step 587/7134 completed (loss: 0.32885581254959106, acc: 0.9116021990776062)
[2025-02-13 19:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:59][root][INFO] - Training Epoch: 1/2, step 588/7134 completed (loss: 0.3848939538002014, acc: 0.9054054021835327)
[2025-02-13 19:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00][root][INFO] - Training Epoch: 1/2, step 589/7134 completed (loss: 0.3259648382663727, acc: 0.9141104221343994)
[2025-02-13 19:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00][root][INFO] - Training Epoch: 1/2, step 590/7134 completed (loss: 0.2173253446817398, acc: 0.9470198750495911)
[2025-02-13 19:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00][root][INFO] - Training Epoch: 1/2, step 591/7134 completed (loss: 0.26669368147850037, acc: 0.9491525292396545)
[2025-02-13 19:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01][root][INFO] - Training Epoch: 1/2, step 592/7134 completed (loss: 0.2133440524339676, acc: 0.9602272510528564)
[2025-02-13 19:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01][root][INFO] - Training Epoch: 1/2, step 593/7134 completed (loss: 0.3805578649044037, acc: 0.9239766001701355)
[2025-02-13 19:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02][root][INFO] - Training Epoch: 1/2, step 594/7134 completed (loss: 0.49911680817604065, acc: 0.8982036113739014)
[2025-02-13 19:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02][root][INFO] - Training Epoch: 1/2, step 595/7134 completed (loss: 0.32939404249191284, acc: 0.9301075339317322)
[2025-02-13 19:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02][root][INFO] - Training Epoch: 1/2, step 596/7134 completed (loss: 0.40299728512763977, acc: 0.894444465637207)
[2025-02-13 19:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03][root][INFO] - Training Epoch: 1/2, step 597/7134 completed (loss: 0.5139681696891785, acc: 0.8938547372817993)
[2025-02-13 19:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03][root][INFO] - Training Epoch: 1/2, step 598/7134 completed (loss: 0.686338484287262, acc: 0.864130437374115)
[2025-02-13 19:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04][root][INFO] - Training Epoch: 1/2, step 599/7134 completed (loss: 0.39168262481689453, acc: 0.9193548560142517)
[2025-02-13 19:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04][root][INFO] - Training Epoch: 1/2, step 600/7134 completed (loss: 0.3629750609397888, acc: 0.9180327653884888)
[2025-02-13 19:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04][root][INFO] - Training Epoch: 1/2, step 601/7134 completed (loss: 0.4758223593235016, acc: 0.8806818127632141)
[2025-02-13 19:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05][root][INFO] - Training Epoch: 1/2, step 602/7134 completed (loss: 0.4163229465484619, acc: 0.9027026891708374)
[2025-02-13 19:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05][root][INFO] - Training Epoch: 1/2, step 603/7134 completed (loss: 0.47898048162460327, acc: 0.920634925365448)
[2025-02-13 19:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05][root][INFO] - Training Epoch: 1/2, step 604/7134 completed (loss: 0.4724452495574951, acc: 0.8848484754562378)
[2025-02-13 19:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06][root][INFO] - Training Epoch: 1/2, step 605/7134 completed (loss: 0.3232417106628418, acc: 0.9304812550544739)
[2025-02-13 19:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06][root][INFO] - Training Epoch: 1/2, step 606/7134 completed (loss: 0.37839648127555847, acc: 0.9049999713897705)
[2025-02-13 19:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07][root][INFO] - Training Epoch: 1/2, step 607/7134 completed (loss: 0.3777395784854889, acc: 0.9367815852165222)
[2025-02-13 19:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07][root][INFO] - Training Epoch: 1/2, step 608/7134 completed (loss: 0.3863612413406372, acc: 0.9027026891708374)
[2025-02-13 19:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08][root][INFO] - Training Epoch: 1/2, step 609/7134 completed (loss: 0.43092894554138184, acc: 0.892307698726654)
[2025-02-13 19:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08][root][INFO] - Training Epoch: 1/2, step 610/7134 completed (loss: 0.37946420907974243, acc: 0.9162561297416687)
[2025-02-13 19:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08][root][INFO] - Training Epoch: 1/2, step 611/7134 completed (loss: 0.3334793746471405, acc: 0.9209039807319641)
[2025-02-13 19:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09][root][INFO] - Training Epoch: 1/2, step 612/7134 completed (loss: 0.36842167377471924, acc: 0.9021739363670349)
[2025-02-13 19:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09][root][INFO] - Training Epoch: 1/2, step 613/7134 completed (loss: 0.3523000478744507, acc: 0.9090909361839294)
[2025-02-13 19:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10][root][INFO] - Training Epoch: 1/2, step 614/7134 completed (loss: 0.2607775628566742, acc: 0.9402173757553101)
[2025-02-13 19:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10][root][INFO] - Training Epoch: 1/2, step 615/7134 completed (loss: 0.20099115371704102, acc: 0.942105233669281)
[2025-02-13 19:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10][root][INFO] - Training Epoch: 1/2, step 616/7134 completed (loss: 0.3199051320552826, acc: 0.9308755993843079)
[2025-02-13 19:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11][root][INFO] - Training Epoch: 1/2, step 617/7134 completed (loss: 0.3965905010700226, acc: 0.9090909361839294)
[2025-02-13 19:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11][root][INFO] - Training Epoch: 1/2, step 618/7134 completed (loss: 0.2549063265323639, acc: 0.9251337051391602)
[2025-02-13 19:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12][root][INFO] - Training Epoch: 1/2, step 619/7134 completed (loss: 0.27852991223335266, acc: 0.9426229596138)
[2025-02-13 19:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12][root][INFO] - Training Epoch: 1/2, step 620/7134 completed (loss: 0.35547563433647156, acc: 0.931506872177124)
[2025-02-13 19:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12][root][INFO] - Training Epoch: 1/2, step 621/7134 completed (loss: 0.3633665442466736, acc: 0.8933333158493042)
[2025-02-13 19:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13][root][INFO] - Training Epoch: 1/2, step 622/7134 completed (loss: 0.49760231375694275, acc: 0.875)
[2025-02-13 19:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13][root][INFO] - Training Epoch: 1/2, step 623/7134 completed (loss: 0.638227641582489, acc: 0.8776978254318237)
[2025-02-13 19:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14][root][INFO] - Training Epoch: 1/2, step 624/7134 completed (loss: 0.32966798543930054, acc: 0.9343065619468689)
[2025-02-13 19:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14][root][INFO] - Training Epoch: 1/2, step 625/7134 completed (loss: 0.3981419503688812, acc: 0.8999999761581421)
[2025-02-13 19:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14][root][INFO] - Training Epoch: 1/2, step 626/7134 completed (loss: 0.4654375910758972, acc: 0.8960000276565552)
[2025-02-13 19:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15][root][INFO] - Training Epoch: 1/2, step 627/7134 completed (loss: 0.3897542655467987, acc: 0.918367326259613)
[2025-02-13 19:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15][root][INFO] - Training Epoch: 1/2, step 628/7134 completed (loss: 0.3365691304206848, acc: 0.9387755393981934)
[2025-02-13 19:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16][root][INFO] - Training Epoch: 1/2, step 629/7134 completed (loss: 0.5649892687797546, acc: 0.8723404407501221)
[2025-02-13 19:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16][root][INFO] - Training Epoch: 1/2, step 630/7134 completed (loss: 0.5733993053436279, acc: 0.8535031676292419)
[2025-02-13 19:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16][root][INFO] - Training Epoch: 1/2, step 631/7134 completed (loss: 0.3669203817844391, acc: 0.9285714030265808)
[2025-02-13 19:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:17][root][INFO] - Training Epoch: 1/2, step 632/7134 completed (loss: 0.2640085518360138, acc: 0.9366196990013123)
[2025-02-13 19:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:17][root][INFO] - Training Epoch: 1/2, step 633/7134 completed (loss: 0.2977752089500427, acc: 0.918749988079071)
[2025-02-13 19:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18][root][INFO] - Training Epoch: 1/2, step 634/7134 completed (loss: 0.3760603368282318, acc: 0.9220778942108154)
[2025-02-13 19:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18][root][INFO] - Training Epoch: 1/2, step 635/7134 completed (loss: 0.3513469696044922, acc: 0.9155844449996948)
[2025-02-13 19:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18][root][INFO] - Training Epoch: 1/2, step 636/7134 completed (loss: 0.31264764070510864, acc: 0.940119743347168)
[2025-02-13 19:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:19][root][INFO] - Training Epoch: 1/2, step 637/7134 completed (loss: 0.33783257007598877, acc: 0.9208633303642273)
[2025-02-13 19:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:19][root][INFO] - Training Epoch: 1/2, step 638/7134 completed (loss: 0.33140796422958374, acc: 0.926174521446228)
[2025-02-13 19:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20][root][INFO] - Training Epoch: 1/2, step 639/7134 completed (loss: 0.3037574887275696, acc: 0.8999999761581421)
[2025-02-13 19:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20][root][INFO] - Training Epoch: 1/2, step 640/7134 completed (loss: 0.24251048266887665, acc: 0.95652174949646)
[2025-02-13 19:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20][root][INFO] - Training Epoch: 1/2, step 641/7134 completed (loss: 0.328497976064682, acc: 0.9130434989929199)
[2025-02-13 19:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:21][root][INFO] - Training Epoch: 1/2, step 642/7134 completed (loss: 0.23824669420719147, acc: 0.9411764740943909)
[2025-02-13 19:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:21][root][INFO] - Training Epoch: 1/2, step 643/7134 completed (loss: 0.25018224120140076, acc: 0.932584285736084)
[2025-02-13 19:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22][root][INFO] - Training Epoch: 1/2, step 644/7134 completed (loss: 0.1580594778060913, acc: 0.96875)
[2025-02-13 19:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22][root][INFO] - Training Epoch: 1/2, step 645/7134 completed (loss: 0.10814044624567032, acc: 0.9655172228813171)
[2025-02-13 19:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23][root][INFO] - Training Epoch: 1/2, step 646/7134 completed (loss: 0.22572973370552063, acc: 0.9367088675498962)
[2025-02-13 19:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23][root][INFO] - Training Epoch: 1/2, step 647/7134 completed (loss: 0.14107073843479156, acc: 0.970370352268219)
[2025-02-13 19:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23][root][INFO] - Training Epoch: 1/2, step 648/7134 completed (loss: 0.6223656535148621, acc: 0.8461538553237915)
[2025-02-13 19:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24][root][INFO] - Training Epoch: 1/2, step 649/7134 completed (loss: 0.5589879155158997, acc: 0.8702702522277832)
[2025-02-13 19:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24][root][INFO] - Training Epoch: 1/2, step 650/7134 completed (loss: 0.6088932752609253, acc: 0.8561643958091736)
[2025-02-13 19:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25][root][INFO] - Training Epoch: 1/2, step 651/7134 completed (loss: 0.5219681859016418, acc: 0.8636363744735718)
[2025-02-13 19:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25][root][INFO] - Training Epoch: 1/2, step 652/7134 completed (loss: 0.46889808773994446, acc: 0.8873239159584045)
[2025-02-13 19:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25][root][INFO] - Training Epoch: 1/2, step 653/7134 completed (loss: 0.24510793387889862, acc: 0.9454545378684998)
[2025-02-13 19:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26][root][INFO] - Training Epoch: 1/2, step 654/7134 completed (loss: 0.5785871148109436, acc: 0.8646616339683533)
[2025-02-13 19:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26][root][INFO] - Training Epoch: 1/2, step 655/7134 completed (loss: 0.29398563504219055, acc: 0.9137930870056152)
[2025-02-13 19:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26][root][INFO] - Training Epoch: 1/2, step 656/7134 completed (loss: 0.4677160382270813, acc: 0.8787878751754761)
[2025-02-13 19:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:27][root][INFO] - Training Epoch: 1/2, step 657/7134 completed (loss: 0.45432141423225403, acc: 0.8802816867828369)
[2025-02-13 19:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:27][root][INFO] - Training Epoch: 1/2, step 658/7134 completed (loss: 0.19544358551502228, acc: 0.9548022747039795)
[2025-02-13 19:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28][root][INFO] - Training Epoch: 1/2, step 659/7134 completed (loss: 0.1916297823190689, acc: 0.9640718698501587)
[2025-02-13 19:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28][root][INFO] - Training Epoch: 1/2, step 660/7134 completed (loss: 0.3516520857810974, acc: 0.8999999761581421)
[2025-02-13 19:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28][root][INFO] - Training Epoch: 1/2, step 661/7134 completed (loss: 0.3071959912776947, acc: 0.9279661178588867)
[2025-02-13 19:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29][root][INFO] - Training Epoch: 1/2, step 662/7134 completed (loss: 0.3271746039390564, acc: 0.9388889074325562)
[2025-02-13 19:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29][root][INFO] - Training Epoch: 1/2, step 663/7134 completed (loss: 0.24612760543823242, acc: 0.9368932247161865)
[2025-02-13 19:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30][root][INFO] - Training Epoch: 1/2, step 664/7134 completed (loss: 0.3274783790111542, acc: 0.9300699234008789)
[2025-02-13 19:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30][root][INFO] - Training Epoch: 1/2, step 665/7134 completed (loss: 0.5932170152664185, acc: 0.8907103538513184)
[2025-02-13 19:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30][root][INFO] - Training Epoch: 1/2, step 666/7134 completed (loss: 0.3517271876335144, acc: 0.9314285516738892)
[2025-02-13 19:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31][root][INFO] - Training Epoch: 1/2, step 667/7134 completed (loss: 0.4209045469760895, acc: 0.8982036113739014)
[2025-02-13 19:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31][root][INFO] - Training Epoch: 1/2, step 668/7134 completed (loss: 0.16474513709545135, acc: 0.9617486596107483)
[2025-02-13 19:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32][root][INFO] - Training Epoch: 1/2, step 669/7134 completed (loss: 0.34097009897232056, acc: 0.916201114654541)
[2025-02-13 19:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32][root][INFO] - Training Epoch: 1/2, step 670/7134 completed (loss: 0.29031726717948914, acc: 0.931506872177124)
[2025-02-13 19:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32][root][INFO] - Training Epoch: 1/2, step 671/7134 completed (loss: 0.29424813389778137, acc: 0.9049773812294006)
[2025-02-13 19:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33][root][INFO] - Training Epoch: 1/2, step 672/7134 completed (loss: 0.35064777731895447, acc: 0.9312499761581421)
[2025-02-13 19:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33][root][INFO] - Training Epoch: 1/2, step 673/7134 completed (loss: 0.3422084450721741, acc: 0.9353233575820923)
[2025-02-13 19:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34][root][INFO] - Training Epoch: 1/2, step 674/7134 completed (loss: 0.23978987336158752, acc: 0.9512194991111755)
[2025-02-13 19:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34][root][INFO] - Training Epoch: 1/2, step 675/7134 completed (loss: 0.24479669332504272, acc: 0.939226508140564)
[2025-02-13 19:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34][root][INFO] - Training Epoch: 1/2, step 676/7134 completed (loss: 0.18505164980888367, acc: 0.9619565010070801)
[2025-02-13 19:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:35][root][INFO] - Training Epoch: 1/2, step 677/7134 completed (loss: 0.13580329716205597, acc: 0.9735449552536011)
[2025-02-13 19:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:35][root][INFO] - Training Epoch: 1/2, step 678/7134 completed (loss: 0.42868852615356445, acc: 0.9207317233085632)
[2025-02-13 19:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36][root][INFO] - Training Epoch: 1/2, step 679/7134 completed (loss: 0.34067943692207336, acc: 0.932330846786499)
[2025-02-13 19:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36][root][INFO] - Training Epoch: 1/2, step 680/7134 completed (loss: 0.38933876156806946, acc: 0.909547746181488)
[2025-02-13 19:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36][root][INFO] - Training Epoch: 1/2, step 681/7134 completed (loss: 0.33343741297721863, acc: 0.9052631855010986)
[2025-02-13 19:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37][root][INFO] - Training Epoch: 1/2, step 682/7134 completed (loss: 0.2955041527748108, acc: 0.9322034120559692)
[2025-02-13 19:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37][root][INFO] - Training Epoch: 1/2, step 683/7134 completed (loss: 0.3313806354999542, acc: 0.9189189076423645)
[2025-02-13 19:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:38][root][INFO] - Training Epoch: 1/2, step 684/7134 completed (loss: 0.378589928150177, acc: 0.9111111164093018)
[2025-02-13 19:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:38][root][INFO] - Training Epoch: 1/2, step 685/7134 completed (loss: 0.27192020416259766, acc: 0.909547746181488)
[2025-02-13 19:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:39][root][INFO] - Training Epoch: 1/2, step 686/7134 completed (loss: 0.15418685972690582, acc: 0.9562841653823853)
[2025-02-13 19:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:39][root][INFO] - Training Epoch: 1/2, step 687/7134 completed (loss: 0.23024915158748627, acc: 0.913385808467865)
[2025-02-13 19:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40][root][INFO] - Training Epoch: 1/2, step 688/7134 completed (loss: 0.15398606657981873, acc: 0.949367105960846)
[2025-02-13 19:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40][root][INFO] - Training Epoch: 1/2, step 689/7134 completed (loss: 0.17108973860740662, acc: 0.9555555582046509)
[2025-02-13 19:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40][root][INFO] - Training Epoch: 1/2, step 690/7134 completed (loss: 0.45162221789360046, acc: 0.9096774458885193)
[2025-02-13 19:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:41][root][INFO] - Training Epoch: 1/2, step 691/7134 completed (loss: 0.5041420459747314, acc: 0.8766233921051025)
[2025-02-13 19:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:41][root][INFO] - Training Epoch: 1/2, step 692/7134 completed (loss: 0.7233702540397644, acc: 0.8629441857337952)
[2025-02-13 19:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42][root][INFO] - Training Epoch: 1/2, step 693/7134 completed (loss: 0.5050904154777527, acc: 0.8581560254096985)
[2025-02-13 19:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42][root][INFO] - Training Epoch: 1/2, step 694/7134 completed (loss: 0.6448784470558167, acc: 0.8484848737716675)
[2025-02-13 19:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42][root][INFO] - Training Epoch: 1/2, step 695/7134 completed (loss: 0.6768216490745544, acc: 0.8208954930305481)
[2025-02-13 19:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:43][root][INFO] - Training Epoch: 1/2, step 696/7134 completed (loss: 0.7119496464729309, acc: 0.8427672982215881)
[2025-02-13 19:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:43][root][INFO] - Training Epoch: 1/2, step 697/7134 completed (loss: 0.45016708970069885, acc: 0.8804348111152649)
[2025-02-13 19:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44][root][INFO] - Training Epoch: 1/2, step 698/7134 completed (loss: 0.40669015049934387, acc: 0.9318181872367859)
[2025-02-13 19:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44][root][INFO] - Training Epoch: 1/2, step 699/7134 completed (loss: 0.2088543027639389, acc: 0.9589040875434875)
[2025-02-13 19:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44][root][INFO] - Training Epoch: 1/2, step 700/7134 completed (loss: 0.5718870759010315, acc: 0.8240000009536743)
[2025-02-13 19:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45][root][INFO] - Training Epoch: 1/2, step 701/7134 completed (loss: 0.6554403305053711, acc: 0.8260869383811951)
[2025-02-13 19:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45][root][INFO] - Training Epoch: 1/2, step 702/7134 completed (loss: 0.5408781170845032, acc: 0.8476821184158325)
[2025-02-13 19:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:46][root][INFO] - Training Epoch: 1/2, step 703/7134 completed (loss: 0.5635074377059937, acc: 0.8581081032752991)
[2025-02-13 19:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:46][root][INFO] - Training Epoch: 1/2, step 704/7134 completed (loss: 0.48282793164253235, acc: 0.9025974273681641)
[2025-02-13 19:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47][root][INFO] - Training Epoch: 1/2, step 705/7134 completed (loss: 0.5289908647537231, acc: 0.857798159122467)
[2025-02-13 19:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47][root][INFO] - Training Epoch: 1/2, step 706/7134 completed (loss: 0.33597707748413086, acc: 0.9103773832321167)
[2025-02-13 19:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47][root][INFO] - Training Epoch: 1/2, step 707/7134 completed (loss: 0.37260743975639343, acc: 0.9009901285171509)
[2025-02-13 19:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:48][root][INFO] - Training Epoch: 1/2, step 708/7134 completed (loss: 0.28181642293930054, acc: 0.932584285736084)
[2025-02-13 19:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:48][root][INFO] - Training Epoch: 1/2, step 709/7134 completed (loss: 0.3833644688129425, acc: 0.9085714221000671)
[2025-02-13 19:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49][root][INFO] - Training Epoch: 1/2, step 710/7134 completed (loss: 0.27855509519577026, acc: 0.9114583134651184)
[2025-02-13 19:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49][root][INFO] - Training Epoch: 1/2, step 711/7134 completed (loss: 0.3022271394729614, acc: 0.9313725233078003)
[2025-02-13 19:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49][root][INFO] - Training Epoch: 1/2, step 712/7134 completed (loss: 0.18455927073955536, acc: 0.96875)
[2025-02-13 19:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:50][root][INFO] - Training Epoch: 1/2, step 713/7134 completed (loss: 0.38058072328567505, acc: 0.929648220539093)
[2025-02-13 19:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:50][root][INFO] - Training Epoch: 1/2, step 714/7134 completed (loss: 0.35397395491600037, acc: 0.9200000166893005)
[2025-02-13 19:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51][root][INFO] - Training Epoch: 1/2, step 715/7134 completed (loss: 0.13864651322364807, acc: 0.967391312122345)
[2025-02-13 19:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51][root][INFO] - Training Epoch: 1/2, step 716/7134 completed (loss: 0.2106875479221344, acc: 0.9558823704719543)
[2025-02-13 19:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51][root][INFO] - Training Epoch: 1/2, step 717/7134 completed (loss: 0.27227258682250977, acc: 0.9358288645744324)
[2025-02-13 19:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52][root][INFO] - Training Epoch: 1/2, step 718/7134 completed (loss: 0.20424418151378632, acc: 0.9312169551849365)
[2025-02-13 19:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52][root][INFO] - Training Epoch: 1/2, step 719/7134 completed (loss: 0.18699133396148682, acc: 0.957446813583374)
[2025-02-13 19:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53][root][INFO] - Training Epoch: 1/2, step 720/7134 completed (loss: 0.20950505137443542, acc: 0.9405405521392822)
[2025-02-13 19:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53][root][INFO] - Training Epoch: 1/2, step 721/7134 completed (loss: 0.2035416215658188, acc: 0.942105233669281)
[2025-02-13 19:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53][root][INFO] - Training Epoch: 1/2, step 722/7134 completed (loss: 0.3119641840457916, acc: 0.9141414165496826)
[2025-02-13 19:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:54][root][INFO] - Training Epoch: 1/2, step 723/7134 completed (loss: 0.1669599413871765, acc: 0.945652186870575)
[2025-02-13 19:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:54][root][INFO] - Training Epoch: 1/2, step 724/7134 completed (loss: 0.32920053601264954, acc: 0.9056603908538818)
[2025-02-13 19:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55][root][INFO] - Training Epoch: 1/2, step 725/7134 completed (loss: 0.1462378203868866, acc: 0.976190447807312)
[2025-02-13 19:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55][root][INFO] - Training Epoch: 1/2, step 726/7134 completed (loss: 0.25013983249664307, acc: 0.9553072452545166)
[2025-02-13 19:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55][root][INFO] - Training Epoch: 1/2, step 727/7134 completed (loss: 0.3482309877872467, acc: 0.9306358098983765)
[2025-02-13 19:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56][root][INFO] - Training Epoch: 1/2, step 728/7134 completed (loss: 0.1390780806541443, acc: 0.9599999785423279)
[2025-02-13 19:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56][root][INFO] - Training Epoch: 1/2, step 729/7134 completed (loss: 0.19449667632579803, acc: 0.9620253443717957)
[2025-02-13 19:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:57][root][INFO] - Training Epoch: 1/2, step 730/7134 completed (loss: 0.11420333385467529, acc: 0.978723406791687)
[2025-02-13 19:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:57][root][INFO] - Training Epoch: 1/2, step 731/7134 completed (loss: 0.09651093930006027, acc: 0.9717513918876648)
[2025-02-13 19:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58][root][INFO] - Training Epoch: 1/2, step 732/7134 completed (loss: 0.45197054743766785, acc: 0.8770492076873779)
[2025-02-13 19:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58][root][INFO] - Training Epoch: 1/2, step 733/7134 completed (loss: 0.19650278985500336, acc: 0.9461538195610046)
[2025-02-13 19:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58][root][INFO] - Training Epoch: 1/2, step 734/7134 completed (loss: 0.31488391757011414, acc: 0.9274193644523621)
[2025-02-13 19:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:59][root][INFO] - Training Epoch: 1/2, step 735/7134 completed (loss: 0.34956398606300354, acc: 0.922535240650177)
[2025-02-13 19:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:59][root][INFO] - Training Epoch: 1/2, step 736/7134 completed (loss: 0.707673192024231, acc: 0.8514851331710815)
[2025-02-13 19:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00][root][INFO] - Training Epoch: 1/2, step 737/7134 completed (loss: 0.4447023570537567, acc: 0.8898305296897888)
[2025-02-13 19:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00][root][INFO] - Training Epoch: 1/2, step 738/7134 completed (loss: 0.23411080241203308, acc: 0.9622641801834106)
[2025-02-13 19:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00][root][INFO] - Training Epoch: 1/2, step 739/7134 completed (loss: 0.2840493619441986, acc: 0.939130425453186)
[2025-02-13 19:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01][root][INFO] - Training Epoch: 1/2, step 740/7134 completed (loss: 0.23033128678798676, acc: 0.9186992049217224)
[2025-02-13 19:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01][root][INFO] - Training Epoch: 1/2, step 741/7134 completed (loss: 0.3191992938518524, acc: 0.8962963223457336)
[2025-02-13 19:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02][root][INFO] - Training Epoch: 1/2, step 742/7134 completed (loss: 0.2529509961605072, acc: 0.9368420839309692)
[2025-02-13 19:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02][root][INFO] - Training Epoch: 1/2, step 743/7134 completed (loss: 0.5199211239814758, acc: 0.8911564350128174)
[2025-02-13 19:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02][root][INFO] - Training Epoch: 1/2, step 744/7134 completed (loss: 0.2581205666065216, acc: 0.9399999976158142)
[2025-02-13 19:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03][root][INFO] - Training Epoch: 1/2, step 745/7134 completed (loss: 0.20460520684719086, acc: 0.9411764740943909)
[2025-02-13 19:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03][root][INFO] - Training Epoch: 1/2, step 746/7134 completed (loss: 0.33902305364608765, acc: 0.9083969593048096)
[2025-02-13 19:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:04][root][INFO] - Training Epoch: 1/2, step 747/7134 completed (loss: 0.47319093346595764, acc: 0.8818897604942322)
[2025-02-13 19:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:04][root][INFO] - Training Epoch: 1/2, step 748/7134 completed (loss: 0.34282514452934265, acc: 0.9029850959777832)
[2025-02-13 19:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05][root][INFO] - Training Epoch: 1/2, step 749/7134 completed (loss: 0.26056408882141113, acc: 0.9285714030265808)
[2025-02-13 19:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05][root][INFO] - Training Epoch: 1/2, step 750/7134 completed (loss: 0.3282610774040222, acc: 0.93388432264328)
[2025-02-13 19:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05][root][INFO] - Training Epoch: 1/2, step 751/7134 completed (loss: 0.5017114281654358, acc: 0.8880000114440918)
[2025-02-13 19:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06][root][INFO] - Training Epoch: 1/2, step 752/7134 completed (loss: 0.6061716079711914, acc: 0.8918918967247009)
[2025-02-13 19:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06][root][INFO] - Training Epoch: 1/2, step 753/7134 completed (loss: 0.339893102645874, acc: 0.90625)
[2025-02-13 19:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07][root][INFO] - Training Epoch: 1/2, step 754/7134 completed (loss: 0.29946014285087585, acc: 0.9420289993286133)
[2025-02-13 19:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07][root][INFO] - Training Epoch: 1/2, step 755/7134 completed (loss: 0.6017407178878784, acc: 0.8880000114440918)
[2025-02-13 19:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07][root][INFO] - Training Epoch: 1/2, step 756/7134 completed (loss: 0.27223876118659973, acc: 0.9196428656578064)
[2025-02-13 19:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08][root][INFO] - Training Epoch: 1/2, step 757/7134 completed (loss: 0.16023680567741394, acc: 0.9807692170143127)
[2025-02-13 19:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08][root][INFO] - Training Epoch: 1/2, step 758/7134 completed (loss: 0.18871158361434937, acc: 0.954954981803894)
[2025-02-13 19:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09][root][INFO] - Training Epoch: 1/2, step 759/7134 completed (loss: 0.23327867686748505, acc: 0.9459459185600281)
[2025-02-13 19:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09][root][INFO] - Training Epoch: 1/2, step 760/7134 completed (loss: 0.28612589836120605, acc: 0.9090909361839294)
[2025-02-13 19:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10][root][INFO] - Training Epoch: 1/2, step 761/7134 completed (loss: 0.3097420334815979, acc: 0.916167676448822)
[2025-02-13 19:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10][root][INFO] - Training Epoch: 1/2, step 762/7134 completed (loss: 0.2909093499183655, acc: 0.9275362491607666)
[2025-02-13 19:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10][root][INFO] - Training Epoch: 1/2, step 763/7134 completed (loss: 0.3867792785167694, acc: 0.875)
[2025-02-13 19:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11][root][INFO] - Training Epoch: 1/2, step 764/7134 completed (loss: 0.1743737906217575, acc: 0.9644970297813416)
[2025-02-13 19:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11][root][INFO] - Training Epoch: 1/2, step 765/7134 completed (loss: 0.1730467975139618, acc: 0.955974817276001)
[2025-02-13 19:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12][root][INFO] - Training Epoch: 1/2, step 766/7134 completed (loss: 0.12374432384967804, acc: 0.9750000238418579)
[2025-02-13 19:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12][root][INFO] - Training Epoch: 1/2, step 767/7134 completed (loss: 0.30401915311813354, acc: 0.9107142686843872)
[2025-02-13 19:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12][root][INFO] - Training Epoch: 1/2, step 768/7134 completed (loss: 0.465833455324173, acc: 0.9071038365364075)
[2025-02-13 19:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13][root][INFO] - Training Epoch: 1/2, step 769/7134 completed (loss: 0.21855415403842926, acc: 0.9440993666648865)
[2025-02-13 19:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13][root][INFO] - Training Epoch: 1/2, step 770/7134 completed (loss: 0.3420630693435669, acc: 0.9135802388191223)
[2025-02-13 19:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14][root][INFO] - Training Epoch: 1/2, step 771/7134 completed (loss: 0.20081321895122528, acc: 0.9416058659553528)
[2025-02-13 19:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14][root][INFO] - Training Epoch: 1/2, step 772/7134 completed (loss: 0.27458465099334717, acc: 0.9047619104385376)
[2025-02-13 19:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14][root][INFO] - Training Epoch: 1/2, step 773/7134 completed (loss: 0.38519543409347534, acc: 0.9157894849777222)
[2025-02-13 19:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15][root][INFO] - Training Epoch: 1/2, step 774/7134 completed (loss: 0.250773161649704, acc: 0.9577465057373047)
[2025-02-13 19:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15][root][INFO] - Training Epoch: 1/2, step 775/7134 completed (loss: 0.14715604484081268, acc: 0.9627659320831299)
[2025-02-13 19:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16][root][INFO] - Training Epoch: 1/2, step 776/7134 completed (loss: 0.13470207154750824, acc: 0.9681528806686401)
[2025-02-13 19:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16][root][INFO] - Training Epoch: 1/2, step 777/7134 completed (loss: 0.2107912003993988, acc: 0.9333333373069763)
[2025-02-13 19:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16][root][INFO] - Training Epoch: 1/2, step 778/7134 completed (loss: 0.20721781253814697, acc: 0.955974817276001)
[2025-02-13 19:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:17][root][INFO] - Training Epoch: 1/2, step 779/7134 completed (loss: 0.18797621130943298, acc: 0.9415204524993896)
[2025-02-13 19:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:17][root][INFO] - Training Epoch: 1/2, step 780/7134 completed (loss: 0.29843804240226746, acc: 0.908108115196228)
[2025-02-13 19:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18][root][INFO] - Training Epoch: 1/2, step 781/7134 completed (loss: 0.2800408601760864, acc: 0.9411764740943909)
[2025-02-13 19:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18][root][INFO] - Training Epoch: 1/2, step 782/7134 completed (loss: 0.3150520920753479, acc: 0.9367088675498962)
[2025-02-13 19:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18][root][INFO] - Training Epoch: 1/2, step 783/7134 completed (loss: 0.30905881524086, acc: 0.9299065470695496)
[2025-02-13 19:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:19][root][INFO] - Training Epoch: 1/2, step 784/7134 completed (loss: 0.25881192088127136, acc: 0.9452736377716064)
[2025-02-13 19:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:19][root][INFO] - Training Epoch: 1/2, step 785/7134 completed (loss: 0.42468392848968506, acc: 0.8933333158493042)
[2025-02-13 19:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20][root][INFO] - Training Epoch: 1/2, step 786/7134 completed (loss: 0.3266529142856598, acc: 0.9104477763175964)
[2025-02-13 19:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20][root][INFO] - Training Epoch: 1/2, step 787/7134 completed (loss: 0.14157360792160034, acc: 0.969924807548523)
[2025-02-13 19:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20][root][INFO] - Training Epoch: 1/2, step 788/7134 completed (loss: 0.4026125967502594, acc: 0.8826815485954285)
[2025-02-13 19:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:21][root][INFO] - Training Epoch: 1/2, step 789/7134 completed (loss: 0.4024011790752411, acc: 0.9166666865348816)
[2025-02-13 19:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:21][root][INFO] - Training Epoch: 1/2, step 790/7134 completed (loss: 0.490553617477417, acc: 0.8648648858070374)
[2025-02-13 19:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22][root][INFO] - Training Epoch: 1/2, step 791/7134 completed (loss: 0.5386003851890564, acc: 0.8861788511276245)
[2025-02-13 19:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22][root][INFO] - Training Epoch: 1/2, step 792/7134 completed (loss: 0.30976879596710205, acc: 0.9240506291389465)
[2025-02-13 19:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22][root][INFO] - Training Epoch: 1/2, step 793/7134 completed (loss: 0.2224487066268921, acc: 0.9437500238418579)
[2025-02-13 19:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:23][root][INFO] - Training Epoch: 1/2, step 794/7134 completed (loss: 0.29764696955680847, acc: 0.931034505367279)
[2025-02-13 19:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:23][root][INFO] - Training Epoch: 1/2, step 795/7134 completed (loss: 0.406473845243454, acc: 0.8734177350997925)
[2025-02-13 19:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24][root][INFO] - Training Epoch: 1/2, step 796/7134 completed (loss: 0.506883978843689, acc: 0.9047619104385376)
[2025-02-13 19:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24][root][INFO] - Training Epoch: 1/2, step 797/7134 completed (loss: 0.42153486609458923, acc: 0.9055117964744568)
[2025-02-13 19:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25][root][INFO] - Training Epoch: 1/2, step 798/7134 completed (loss: 0.34972935914993286, acc: 0.8989899158477783)
[2025-02-13 19:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25][root][INFO] - Training Epoch: 1/2, step 799/7134 completed (loss: 0.25335606932640076, acc: 0.9256198406219482)
[2025-02-13 19:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25][root][INFO] - Training Epoch: 1/2, step 800/7134 completed (loss: 0.4484206736087799, acc: 0.8740741014480591)
[2025-02-13 19:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26][root][INFO] - Training Epoch: 1/2, step 801/7134 completed (loss: 0.5084972977638245, acc: 0.8451613187789917)
[2025-02-13 19:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26][root][INFO] - Training Epoch: 1/2, step 802/7134 completed (loss: 0.3221873939037323, acc: 0.9219858050346375)
[2025-02-13 19:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27][root][INFO] - Training Epoch: 1/2, step 803/7134 completed (loss: 0.23382310569286346, acc: 0.9420289993286133)
[2025-02-13 19:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27][root][INFO] - Training Epoch: 1/2, step 804/7134 completed (loss: 0.5587877631187439, acc: 0.8682170510292053)
[2025-02-13 19:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27][root][INFO] - Training Epoch: 1/2, step 805/7134 completed (loss: 0.5499478578567505, acc: 0.867132842540741)
[2025-02-13 19:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28][root][INFO] - Training Epoch: 1/2, step 806/7134 completed (loss: 0.21466274559497833, acc: 0.9506173133850098)
[2025-02-13 19:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28][root][INFO] - Training Epoch: 1/2, step 807/7134 completed (loss: 0.3799854516983032, acc: 0.8799999952316284)
[2025-02-13 19:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29][root][INFO] - Training Epoch: 1/2, step 808/7134 completed (loss: 0.2153647243976593, acc: 0.9340101480484009)
[2025-02-13 19:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29][root][INFO] - Training Epoch: 1/2, step 809/7134 completed (loss: 0.16221065819263458, acc: 0.9666666388511658)
[2025-02-13 19:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29][root][INFO] - Training Epoch: 1/2, step 810/7134 completed (loss: 0.30056437849998474, acc: 0.9470899701118469)
[2025-02-13 19:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30][root][INFO] - Training Epoch: 1/2, step 811/7134 completed (loss: 0.5242819786071777, acc: 0.8551723957061768)
[2025-02-13 19:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30][root][INFO] - Training Epoch: 1/2, step 812/7134 completed (loss: 0.3079768419265747, acc: 0.9210526347160339)
[2025-02-13 19:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30][root][INFO] - Training Epoch: 1/2, step 813/7134 completed (loss: 0.317072331905365, acc: 0.929411768913269)
[2025-02-13 19:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31][root][INFO] - Training Epoch: 1/2, step 814/7134 completed (loss: 0.26587244868278503, acc: 0.9152542352676392)
[2025-02-13 19:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31][root][INFO] - Training Epoch: 1/2, step 815/7134 completed (loss: 0.33851271867752075, acc: 0.9230769276618958)
[2025-02-13 19:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32][root][INFO] - Training Epoch: 1/2, step 816/7134 completed (loss: 0.4084978699684143, acc: 0.9007092118263245)
[2025-02-13 19:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32][root][INFO] - Training Epoch: 1/2, step 817/7134 completed (loss: 0.48836347460746765, acc: 0.8920863270759583)
[2025-02-13 19:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32][root][INFO] - Training Epoch: 1/2, step 818/7134 completed (loss: 0.638123631477356, acc: 0.8562091588973999)
[2025-02-13 19:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33][root][INFO] - Training Epoch: 1/2, step 819/7134 completed (loss: 0.46568548679351807, acc: 0.9041095972061157)
[2025-02-13 19:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33][root][INFO] - Training Epoch: 1/2, step 820/7134 completed (loss: 0.41768893599510193, acc: 0.9220778942108154)
[2025-02-13 19:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34][root][INFO] - Training Epoch: 1/2, step 821/7134 completed (loss: 0.29114311933517456, acc: 0.9421965479850769)
[2025-02-13 19:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34][root][INFO] - Training Epoch: 1/2, step 822/7134 completed (loss: 0.2543257772922516, acc: 0.9177215099334717)
[2025-02-13 19:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34][root][INFO] - Training Epoch: 1/2, step 823/7134 completed (loss: 0.16005228459835052, acc: 0.9593495726585388)
[2025-02-13 19:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35][root][INFO] - Training Epoch: 1/2, step 824/7134 completed (loss: 0.4134584367275238, acc: 0.8863636255264282)
[2025-02-13 19:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35][root][INFO] - Training Epoch: 1/2, step 825/7134 completed (loss: 0.31517937779426575, acc: 0.9261363744735718)
[2025-02-13 19:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36][root][INFO] - Training Epoch: 1/2, step 826/7134 completed (loss: 0.27809035778045654, acc: 0.9181286692619324)
[2025-02-13 19:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36][root][INFO] - Training Epoch: 1/2, step 827/7134 completed (loss: 0.3302685618400574, acc: 0.9122806787490845)
[2025-02-13 19:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36][root][INFO] - Training Epoch: 1/2, step 828/7134 completed (loss: 0.21475283801555634, acc: 0.9545454382896423)
[2025-02-13 19:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37][root][INFO] - Training Epoch: 1/2, step 829/7134 completed (loss: 0.3342883884906769, acc: 0.9038461446762085)
[2025-02-13 19:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37][root][INFO] - Training Epoch: 1/2, step 830/7134 completed (loss: 0.2050982266664505, acc: 0.9496402740478516)
[2025-02-13 19:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37][root][INFO] - Training Epoch: 1/2, step 831/7134 completed (loss: 0.1709717959165573, acc: 0.9578313231468201)
[2025-02-13 19:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38][root][INFO] - Training Epoch: 1/2, step 832/7134 completed (loss: 0.47597798705101013, acc: 0.8979591727256775)
[2025-02-13 19:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38][root][INFO] - Training Epoch: 1/2, step 833/7134 completed (loss: 0.214105486869812, acc: 0.9404761791229248)
[2025-02-13 19:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38][root][INFO] - Training Epoch: 1/2, step 834/7134 completed (loss: 0.2862825095653534, acc: 0.9352940917015076)
[2025-02-13 19:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:39][root][INFO] - Training Epoch: 1/2, step 835/7134 completed (loss: 0.27295029163360596, acc: 0.9473684430122375)
[2025-02-13 19:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:39][root][INFO] - Training Epoch: 1/2, step 836/7134 completed (loss: 0.2699640393257141, acc: 0.9187816977500916)
[2025-02-13 19:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40][root][INFO] - Training Epoch: 1/2, step 837/7134 completed (loss: 0.27592551708221436, acc: 0.9435028433799744)
[2025-02-13 19:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40][root][INFO] - Training Epoch: 1/2, step 838/7134 completed (loss: 0.24554765224456787, acc: 0.9281045794487)
[2025-02-13 19:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40][root][INFO] - Training Epoch: 1/2, step 839/7134 completed (loss: 0.18980571627616882, acc: 0.9306930899620056)
[2025-02-13 19:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41][root][INFO] - Training Epoch: 1/2, step 840/7134 completed (loss: 0.23599544167518616, acc: 0.9520958065986633)
[2025-02-13 19:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41][root][INFO] - Training Epoch: 1/2, step 841/7134 completed (loss: 0.2124488800764084, acc: 0.9473684430122375)
[2025-02-13 19:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42][root][INFO] - Training Epoch: 1/2, step 842/7134 completed (loss: 0.2085951417684555, acc: 0.9470587968826294)
[2025-02-13 19:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42][root][INFO] - Training Epoch: 1/2, step 843/7134 completed (loss: 0.18438757956027985, acc: 0.9415204524993896)
[2025-02-13 19:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42][root][INFO] - Training Epoch: 1/2, step 844/7134 completed (loss: 0.2056383192539215, acc: 0.965753436088562)
[2025-02-13 19:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43][root][INFO] - Training Epoch: 1/2, step 845/7134 completed (loss: 0.19596897065639496, acc: 0.9333333373069763)
[2025-02-13 19:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43][root][INFO] - Training Epoch: 1/2, step 846/7134 completed (loss: 0.5185033082962036, acc: 0.8835616707801819)
[2025-02-13 19:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43][root][INFO] - Training Epoch: 1/2, step 847/7134 completed (loss: 0.4135099947452545, acc: 0.910179615020752)
[2025-02-13 19:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44][root][INFO] - Training Epoch: 1/2, step 848/7134 completed (loss: 0.30377310514450073, acc: 0.9181286692619324)
[2025-02-13 19:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44][root][INFO] - Training Epoch: 1/2, step 849/7134 completed (loss: 0.2504889667034149, acc: 0.9207317233085632)
[2025-02-13 19:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:45][root][INFO] - Training Epoch: 1/2, step 850/7134 completed (loss: 0.2512654960155487, acc: 0.926174521446228)
[2025-02-13 19:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:45][root][INFO] - Training Epoch: 1/2, step 851/7134 completed (loss: 0.31797221302986145, acc: 0.9127907156944275)
[2025-02-13 19:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46][root][INFO] - Training Epoch: 1/2, step 852/7134 completed (loss: 0.19698795676231384, acc: 0.9424083828926086)
[2025-02-13 19:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46][root][INFO] - Training Epoch: 1/2, step 853/7134 completed (loss: 0.3205595314502716, acc: 0.9371727705001831)
[2025-02-13 19:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46][root][INFO] - Training Epoch: 1/2, step 854/7134 completed (loss: 0.2967604100704193, acc: 0.9512194991111755)
[2025-02-13 19:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47][root][INFO] - Training Epoch: 1/2, step 855/7134 completed (loss: 0.2223469465970993, acc: 0.9364162087440491)
[2025-02-13 19:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47][root][INFO] - Training Epoch: 1/2, step 856/7134 completed (loss: 0.1560976654291153, acc: 0.9672130942344666)
[2025-02-13 19:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48][root][INFO] - Training Epoch: 1/2, step 857/7134 completed (loss: 0.4087815284729004, acc: 0.9071428775787354)
[2025-02-13 19:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48][root][INFO] - Training Epoch: 1/2, step 858/7134 completed (loss: 0.4705488085746765, acc: 0.886904776096344)
[2025-02-13 19:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48][root][INFO] - Training Epoch: 1/2, step 859/7134 completed (loss: 0.4739048480987549, acc: 0.8813559412956238)
[2025-02-13 19:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49][root][INFO] - Training Epoch: 1/2, step 860/7134 completed (loss: 0.44350680708885193, acc: 0.8476821184158325)
[2025-02-13 19:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49][root][INFO] - Training Epoch: 1/2, step 861/7134 completed (loss: 0.3577289283275604, acc: 0.9086021780967712)
[2025-02-13 19:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50][root][INFO] - Training Epoch: 1/2, step 862/7134 completed (loss: 0.4237048923969269, acc: 0.9142857193946838)
[2025-02-13 19:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50][root][INFO] - Training Epoch: 1/2, step 863/7134 completed (loss: 0.4923470616340637, acc: 0.8797814249992371)
[2025-02-13 19:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50][root][INFO] - Training Epoch: 1/2, step 864/7134 completed (loss: 0.4975009262561798, acc: 0.9047619104385376)
[2025-02-13 19:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51][root][INFO] - Training Epoch: 1/2, step 865/7134 completed (loss: 0.5630779266357422, acc: 0.8430232405662537)
[2025-02-13 19:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51][root][INFO] - Training Epoch: 1/2, step 866/7134 completed (loss: 0.5616747140884399, acc: 0.8642857074737549)
[2025-02-13 19:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52][root][INFO] - Training Epoch: 1/2, step 867/7134 completed (loss: 0.4673483371734619, acc: 0.887499988079071)
[2025-02-13 19:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52][root][INFO] - Training Epoch: 1/2, step 868/7134 completed (loss: 0.688216507434845, acc: 0.854651153087616)
[2025-02-13 19:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52][root][INFO] - Training Epoch: 1/2, step 869/7134 completed (loss: 0.872600793838501, acc: 0.8313252925872803)
[2025-02-13 19:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53][root][INFO] - Training Epoch: 1/2, step 870/7134 completed (loss: 0.3323519825935364, acc: 0.9064327478408813)
[2025-02-13 19:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53][root][INFO] - Training Epoch: 1/2, step 871/7134 completed (loss: 0.4362192451953888, acc: 0.9041095972061157)
[2025-02-13 19:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54][root][INFO] - Training Epoch: 1/2, step 872/7134 completed (loss: 0.18414469063282013, acc: 0.9444444179534912)
[2025-02-13 19:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54][root][INFO] - Training Epoch: 1/2, step 873/7134 completed (loss: 0.45633774995803833, acc: 0.8969696760177612)
[2025-02-13 19:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54][root][INFO] - Training Epoch: 1/2, step 874/7134 completed (loss: 0.4312026798725128, acc: 0.8680555820465088)
[2025-02-13 19:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:55][root][INFO] - Training Epoch: 1/2, step 875/7134 completed (loss: 0.24979132413864136, acc: 0.935251772403717)
[2025-02-13 19:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:55][root][INFO] - Training Epoch: 1/2, step 876/7134 completed (loss: 0.288984477519989, acc: 0.9276315569877625)
[2025-02-13 19:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56][root][INFO] - Training Epoch: 1/2, step 877/7134 completed (loss: 0.46483832597732544, acc: 0.9202454090118408)
[2025-02-13 19:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56][root][INFO] - Training Epoch: 1/2, step 878/7134 completed (loss: 0.3976288437843323, acc: 0.8999999761581421)
[2025-02-13 19:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56][root][INFO] - Training Epoch: 1/2, step 879/7134 completed (loss: 0.33715394139289856, acc: 0.9299362897872925)
[2025-02-13 19:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57][root][INFO] - Training Epoch: 1/2, step 880/7134 completed (loss: 0.22581271827220917, acc: 0.9386503100395203)
[2025-02-13 19:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57][root][INFO] - Training Epoch: 1/2, step 881/7134 completed (loss: 0.29643505811691284, acc: 0.9024389982223511)
[2025-02-13 19:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58][root][INFO] - Training Epoch: 1/2, step 882/7134 completed (loss: 0.3214721977710724, acc: 0.9112903475761414)
[2025-02-13 19:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58][root][INFO] - Training Epoch: 1/2, step 883/7134 completed (loss: 0.49639150500297546, acc: 0.8552631735801697)
[2025-02-13 19:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58][root][INFO] - Training Epoch: 1/2, step 884/7134 completed (loss: 0.4880983829498291, acc: 0.8802395462989807)
[2025-02-13 19:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:59][root][INFO] - Training Epoch: 1/2, step 885/7134 completed (loss: 0.18519823253154755, acc: 0.9512194991111755)
[2025-02-13 19:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:59][root][INFO] - Training Epoch: 1/2, step 886/7134 completed (loss: 0.36885741353034973, acc: 0.9075630307197571)
[2025-02-13 19:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00][root][INFO] - Training Epoch: 1/2, step 887/7134 completed (loss: 0.3194096088409424, acc: 0.9477611780166626)
[2025-02-13 19:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00][root][INFO] - Training Epoch: 1/2, step 888/7134 completed (loss: 0.3319174349308014, acc: 0.9328858852386475)
[2025-02-13 19:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00][root][INFO] - Training Epoch: 1/2, step 889/7134 completed (loss: 0.3203094005584717, acc: 0.9207921028137207)
[2025-02-13 19:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01][root][INFO] - Training Epoch: 1/2, step 890/7134 completed (loss: 0.3856070041656494, acc: 0.8926174640655518)
[2025-02-13 19:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01][root][INFO] - Training Epoch: 1/2, step 891/7134 completed (loss: 0.33412110805511475, acc: 0.9329608678817749)
[2025-02-13 19:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02][root][INFO] - Training Epoch: 1/2, step 892/7134 completed (loss: 0.24995139241218567, acc: 0.9345238208770752)
[2025-02-13 19:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02][root][INFO] - Training Epoch: 1/2, step 893/7134 completed (loss: 0.18889755010604858, acc: 0.9607843160629272)
[2025-02-13 19:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02][root][INFO] - Training Epoch: 1/2, step 894/7134 completed (loss: 0.11817844957113266, acc: 0.9774011373519897)
[2025-02-13 19:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03][root][INFO] - Training Epoch: 1/2, step 895/7134 completed (loss: 0.14957310259342194, acc: 0.965753436088562)
[2025-02-13 19:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03][root][INFO] - Training Epoch: 1/2, step 896/7134 completed (loss: 0.12405922263860703, acc: 0.960629940032959)
[2025-02-13 19:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04][root][INFO] - Training Epoch: 1/2, step 897/7134 completed (loss: 0.2102624475955963, acc: 0.9553072452545166)
[2025-02-13 19:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04][root][INFO] - Training Epoch: 1/2, step 898/7134 completed (loss: 0.35577645897865295, acc: 0.9204545617103577)
[2025-02-13 19:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04][root][INFO] - Training Epoch: 1/2, step 899/7134 completed (loss: 0.2874371409416199, acc: 0.9197860956192017)
[2025-02-13 19:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05][root][INFO] - Training Epoch: 1/2, step 900/7134 completed (loss: 0.14898809790611267, acc: 0.9756097793579102)
[2025-02-13 19:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05][root][INFO] - Training Epoch: 1/2, step 901/7134 completed (loss: 0.43306881189346313, acc: 0.9107142686843872)
[2025-02-13 19:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06][root][INFO] - Training Epoch: 1/2, step 902/7134 completed (loss: 0.2710252106189728, acc: 0.9155844449996948)
[2025-02-13 19:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06][root][INFO] - Training Epoch: 1/2, step 903/7134 completed (loss: 0.09624096751213074, acc: 0.9768785834312439)
[2025-02-13 19:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07][root][INFO] - Training Epoch: 1/2, step 904/7134 completed (loss: 0.2204420417547226, acc: 0.9717513918876648)
[2025-02-13 19:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07][root][INFO] - Training Epoch: 1/2, step 905/7134 completed (loss: 0.11498944461345673, acc: 0.9712643623352051)
[2025-02-13 19:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07][root][INFO] - Training Epoch: 1/2, step 906/7134 completed (loss: 0.2578006386756897, acc: 0.9226519465446472)
[2025-02-13 19:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08][root][INFO] - Training Epoch: 1/2, step 907/7134 completed (loss: 0.28135520219802856, acc: 0.9354838728904724)
[2025-02-13 19:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08][root][INFO] - Training Epoch: 1/2, step 908/7134 completed (loss: 0.18588201701641083, acc: 0.9722222089767456)
[2025-02-13 19:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08][root][INFO] - Training Epoch: 1/2, step 909/7134 completed (loss: 0.15271495282649994, acc: 0.9539473652839661)
[2025-02-13 19:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:09][root][INFO] - Training Epoch: 1/2, step 910/7134 completed (loss: 0.275199830532074, acc: 0.918749988079071)
[2025-02-13 19:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:09][root][INFO] - Training Epoch: 1/2, step 911/7134 completed (loss: 0.24655687808990479, acc: 0.9548386931419373)
[2025-02-13 19:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10][root][INFO] - Training Epoch: 1/2, step 912/7134 completed (loss: 0.2228337675333023, acc: 0.942105233669281)
[2025-02-13 19:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10][root][INFO] - Training Epoch: 1/2, step 913/7134 completed (loss: 0.2037504017353058, acc: 0.9505494236946106)
[2025-02-13 19:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11][root][INFO] - Training Epoch: 1/2, step 914/7134 completed (loss: 0.20800799131393433, acc: 0.939393937587738)
[2025-02-13 19:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11][root][INFO] - Training Epoch: 1/2, step 915/7134 completed (loss: 0.378691703081131, acc: 0.9071428775787354)
[2025-02-13 19:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11][root][INFO] - Training Epoch: 1/2, step 916/7134 completed (loss: 0.26002931594848633, acc: 0.9147287011146545)
[2025-02-13 19:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12][root][INFO] - Training Epoch: 1/2, step 917/7134 completed (loss: 0.37041205167770386, acc: 0.8961039185523987)
[2025-02-13 19:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12][root][INFO] - Training Epoch: 1/2, step 918/7134 completed (loss: 0.2969154119491577, acc: 0.9337748289108276)
[2025-02-13 19:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12][root][INFO] - Training Epoch: 1/2, step 919/7134 completed (loss: 0.20556151866912842, acc: 0.9545454382896423)
[2025-02-13 19:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:13][root][INFO] - Training Epoch: 1/2, step 920/7134 completed (loss: 0.3145589232444763, acc: 0.9127907156944275)
[2025-02-13 19:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:13][root][INFO] - Training Epoch: 1/2, step 921/7134 completed (loss: 0.3019148111343384, acc: 0.9298245906829834)
[2025-02-13 19:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14][root][INFO] - Training Epoch: 1/2, step 922/7134 completed (loss: 0.16648834943771362, acc: 0.9776119589805603)
[2025-02-13 19:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14][root][INFO] - Training Epoch: 1/2, step 923/7134 completed (loss: 0.1912572979927063, acc: 0.9473684430122375)
[2025-02-13 19:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14][root][INFO] - Training Epoch: 1/2, step 924/7134 completed (loss: 0.18610399961471558, acc: 0.9548022747039795)
[2025-02-13 19:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15][root][INFO] - Training Epoch: 1/2, step 925/7134 completed (loss: 0.5124590396881104, acc: 0.8838709592819214)
[2025-02-13 19:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15][root][INFO] - Training Epoch: 1/2, step 926/7134 completed (loss: 0.2691939175128937, acc: 0.9388889074325562)
[2025-02-13 19:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16][root][INFO] - Training Epoch: 1/2, step 927/7134 completed (loss: 0.34147223830223083, acc: 0.9136690497398376)
[2025-02-13 19:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16][root][INFO] - Training Epoch: 1/2, step 928/7134 completed (loss: 0.09208428114652634, acc: 0.9724770784378052)
[2025-02-13 19:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16][root][INFO] - Training Epoch: 1/2, step 929/7134 completed (loss: 0.25670644640922546, acc: 0.9342105388641357)
[2025-02-13 19:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17][root][INFO] - Training Epoch: 1/2, step 930/7134 completed (loss: 0.3824317753314972, acc: 0.9032257795333862)
[2025-02-13 19:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17][root][INFO] - Training Epoch: 1/2, step 931/7134 completed (loss: 0.3000335097312927, acc: 0.9338235259056091)
[2025-02-13 19:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18][root][INFO] - Training Epoch: 1/2, step 932/7134 completed (loss: 0.2064933329820633, acc: 0.9404761791229248)
[2025-02-13 19:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18][root][INFO] - Training Epoch: 1/2, step 933/7134 completed (loss: 0.2840624451637268, acc: 0.9341317415237427)
[2025-02-13 19:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18][root][INFO] - Training Epoch: 1/2, step 934/7134 completed (loss: 0.21087901294231415, acc: 0.9418604373931885)
[2025-02-13 19:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19][root][INFO] - Training Epoch: 1/2, step 935/7134 completed (loss: 0.13085097074508667, acc: 0.9689922332763672)
[2025-02-13 19:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19][root][INFO] - Training Epoch: 1/2, step 936/7134 completed (loss: 0.1605440229177475, acc: 0.9527559280395508)
[2025-02-13 19:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20][root][INFO] - Training Epoch: 1/2, step 937/7134 completed (loss: 0.21491287648677826, acc: 0.9674796462059021)
[2025-02-13 19:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20][root][INFO] - Training Epoch: 1/2, step 938/7134 completed (loss: 0.28816860914230347, acc: 0.9102563858032227)
[2025-02-13 19:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20][root][INFO] - Training Epoch: 1/2, step 939/7134 completed (loss: 0.35061126947402954, acc: 0.9207317233085632)
[2025-02-13 19:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21][root][INFO] - Training Epoch: 1/2, step 940/7134 completed (loss: 0.25527381896972656, acc: 0.9364162087440491)
[2025-02-13 19:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21][root][INFO] - Training Epoch: 1/2, step 941/7134 completed (loss: 0.2756655514240265, acc: 0.932584285736084)
[2025-02-13 19:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21][root][INFO] - Training Epoch: 1/2, step 942/7134 completed (loss: 0.3389184772968292, acc: 0.9127907156944275)
[2025-02-13 19:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22][root][INFO] - Training Epoch: 1/2, step 943/7134 completed (loss: 0.21823325753211975, acc: 0.9509202241897583)
[2025-02-13 19:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22][root][INFO] - Training Epoch: 1/2, step 944/7134 completed (loss: 0.24805700778961182, acc: 0.9363057613372803)
[2025-02-13 19:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23][root][INFO] - Training Epoch: 1/2, step 945/7134 completed (loss: 0.2736906409263611, acc: 0.940397322177887)
[2025-02-13 19:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23][root][INFO] - Training Epoch: 1/2, step 946/7134 completed (loss: 0.22613154351711273, acc: 0.9417475461959839)
[2025-02-13 19:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23][root][INFO] - Training Epoch: 1/2, step 947/7134 completed (loss: 0.40825751423835754, acc: 0.8970588445663452)
[2025-02-13 19:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24][root][INFO] - Training Epoch: 1/2, step 948/7134 completed (loss: 0.1334199160337448, acc: 0.9638554453849792)
[2025-02-13 19:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24][root][INFO] - Training Epoch: 1/2, step 949/7134 completed (loss: 0.2654717266559601, acc: 0.949999988079071)
[2025-02-13 19:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25][root][INFO] - Training Epoch: 1/2, step 950/7134 completed (loss: 0.2910196781158447, acc: 0.9350649118423462)
[2025-02-13 19:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25][root][INFO] - Training Epoch: 1/2, step 951/7134 completed (loss: 0.23203250765800476, acc: 0.9312977194786072)
[2025-02-13 19:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25][root][INFO] - Training Epoch: 1/2, step 952/7134 completed (loss: 0.42509177327156067, acc: 0.8970588445663452)
[2025-02-13 19:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26][root][INFO] - Training Epoch: 1/2, step 953/7134 completed (loss: 0.3803349733352661, acc: 0.9024389982223511)
[2025-02-13 19:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26][root][INFO] - Training Epoch: 1/2, step 954/7134 completed (loss: 0.36174628138542175, acc: 0.9379310607910156)
[2025-02-13 19:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26][root][INFO] - Training Epoch: 1/2, step 955/7134 completed (loss: 0.3232487738132477, acc: 0.9209039807319641)
[2025-02-13 19:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27][root][INFO] - Training Epoch: 1/2, step 956/7134 completed (loss: 0.3509729206562042, acc: 0.9130434989929199)
[2025-02-13 19:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27][root][INFO] - Training Epoch: 1/2, step 957/7134 completed (loss: 0.343287855386734, acc: 0.9558823704719543)
[2025-02-13 19:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28][root][INFO] - Training Epoch: 1/2, step 958/7134 completed (loss: 0.44907835125923157, acc: 0.9029850959777832)
[2025-02-13 19:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28][root][INFO] - Training Epoch: 1/2, step 959/7134 completed (loss: 0.49144965410232544, acc: 0.8865979313850403)
[2025-02-13 19:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28][root][INFO] - Training Epoch: 1/2, step 960/7134 completed (loss: 0.6999427080154419, acc: 0.8731343150138855)
[2025-02-13 19:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29][root][INFO] - Training Epoch: 1/2, step 961/7134 completed (loss: 0.4843631386756897, acc: 0.9120879173278809)
[2025-02-13 19:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29][root][INFO] - Training Epoch: 1/2, step 962/7134 completed (loss: 0.5111129879951477, acc: 0.9005848169326782)
[2025-02-13 19:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30][root][INFO] - Training Epoch: 1/2, step 963/7134 completed (loss: 0.46474140882492065, acc: 0.8895348906517029)
[2025-02-13 19:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30][root][INFO] - Training Epoch: 1/2, step 964/7134 completed (loss: 0.7241948246955872, acc: 0.8475610017776489)
[2025-02-13 19:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30][root][INFO] - Training Epoch: 1/2, step 965/7134 completed (loss: 0.5108141303062439, acc: 0.903743326663971)
[2025-02-13 19:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31][root][INFO] - Training Epoch: 1/2, step 966/7134 completed (loss: 0.41229739785194397, acc: 0.907975435256958)
[2025-02-13 19:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31][root][INFO] - Training Epoch: 1/2, step 967/7134 completed (loss: 0.31426718831062317, acc: 0.936170220375061)
[2025-02-13 19:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31][root][INFO] - Training Epoch: 1/2, step 968/7134 completed (loss: 0.5395264029502869, acc: 0.8646616339683533)
[2025-02-13 19:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32][root][INFO] - Training Epoch: 1/2, step 969/7134 completed (loss: 0.366470068693161, acc: 0.9200000166893005)
[2025-02-13 19:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32][root][INFO] - Training Epoch: 1/2, step 970/7134 completed (loss: 0.426535040140152, acc: 0.904411792755127)
[2025-02-13 19:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33][root][INFO] - Training Epoch: 1/2, step 971/7134 completed (loss: 0.3554244339466095, acc: 0.9146919250488281)
[2025-02-13 19:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33][root][INFO] - Training Epoch: 1/2, step 972/7134 completed (loss: 0.4588138163089752, acc: 0.8820512890815735)
[2025-02-13 19:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34][root][INFO] - Training Epoch: 1/2, step 973/7134 completed (loss: 0.5220045447349548, acc: 0.8571428656578064)
[2025-02-13 19:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34][root][INFO] - Training Epoch: 1/2, step 974/7134 completed (loss: 0.3546595871448517, acc: 0.9257425665855408)
[2025-02-13 19:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34][root][INFO] - Training Epoch: 1/2, step 975/7134 completed (loss: 0.423688679933548, acc: 0.9095744490623474)
[2025-02-13 19:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35][root][INFO] - Training Epoch: 1/2, step 976/7134 completed (loss: 0.30381524562835693, acc: 0.9072847962379456)
[2025-02-13 19:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35][root][INFO] - Training Epoch: 1/2, step 977/7134 completed (loss: 0.5722055435180664, acc: 0.8595505356788635)
[2025-02-13 19:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35][root][INFO] - Training Epoch: 1/2, step 978/7134 completed (loss: 0.2560456395149231, acc: 0.9329897165298462)
[2025-02-13 19:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:36][root][INFO] - Training Epoch: 1/2, step 979/7134 completed (loss: 0.3640681803226471, acc: 0.9135135412216187)
[2025-02-13 19:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:36][root][INFO] - Training Epoch: 1/2, step 980/7134 completed (loss: 0.4174373149871826, acc: 0.8826290965080261)
[2025-02-13 19:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37][root][INFO] - Training Epoch: 1/2, step 981/7134 completed (loss: 0.3632884621620178, acc: 0.9433962106704712)
[2025-02-13 19:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37][root][INFO] - Training Epoch: 1/2, step 982/7134 completed (loss: 0.610184907913208, acc: 0.9314285516738892)
[2025-02-13 19:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37][root][INFO] - Training Epoch: 1/2, step 983/7134 completed (loss: 0.34770187735557556, acc: 0.9097744226455688)
[2025-02-13 19:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38][root][INFO] - Training Epoch: 1/2, step 984/7134 completed (loss: 0.23108872771263123, acc: 0.957446813583374)
[2025-02-13 19:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38][root][INFO] - Training Epoch: 1/2, step 985/7134 completed (loss: 0.30140408873558044, acc: 0.9388889074325562)
[2025-02-13 19:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38][root][INFO] - Training Epoch: 1/2, step 986/7134 completed (loss: 0.2552764415740967, acc: 0.9385474920272827)
[2025-02-13 19:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39][root][INFO] - Training Epoch: 1/2, step 987/7134 completed (loss: 0.2686808705329895, acc: 0.9395973086357117)
[2025-02-13 19:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39][root][INFO] - Training Epoch: 1/2, step 988/7134 completed (loss: 0.5187931656837463, acc: 0.8757061958312988)
[2025-02-13 19:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40][root][INFO] - Training Epoch: 1/2, step 989/7134 completed (loss: 0.4965935945510864, acc: 0.8895705342292786)
[2025-02-13 19:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40][root][INFO] - Training Epoch: 1/2, step 990/7134 completed (loss: 0.43954169750213623, acc: 0.8791208863258362)
[2025-02-13 19:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40][root][INFO] - Training Epoch: 1/2, step 991/7134 completed (loss: 0.6380183100700378, acc: 0.8679245114326477)
[2025-02-13 19:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41][root][INFO] - Training Epoch: 1/2, step 992/7134 completed (loss: 0.39305490255355835, acc: 0.9075144529342651)
[2025-02-13 19:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41][root][INFO] - Training Epoch: 1/2, step 993/7134 completed (loss: 0.30281466245651245, acc: 0.9136690497398376)
[2025-02-13 19:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41][root][INFO] - Training Epoch: 1/2, step 994/7134 completed (loss: 0.37246373295783997, acc: 0.917475700378418)
[2025-02-13 19:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42][root][INFO] - Training Epoch: 1/2, step 995/7134 completed (loss: 0.34478917717933655, acc: 0.9251700639724731)
[2025-02-13 19:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42][root][INFO] - Training Epoch: 1/2, step 996/7134 completed (loss: 0.3728858232498169, acc: 0.9244186282157898)
[2025-02-13 19:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43][root][INFO] - Training Epoch: 1/2, step 997/7134 completed (loss: 0.1715853363275528, acc: 0.9529411792755127)
[2025-02-13 19:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43][root][INFO] - Training Epoch: 1/2, step 998/7134 completed (loss: 0.3030177056789398, acc: 0.9221556782722473)
[2025-02-13 19:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43][root][INFO] - Training Epoch: 1/2, step 999/7134 completed (loss: 0.2492893785238266, acc: 0.9615384340286255)
[2025-02-13 19:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44][root][INFO] - Training Epoch: 1/2, step 1000/7134 completed (loss: 0.19240622222423553, acc: 0.9485294222831726)
[2025-02-13 19:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44][root][INFO] - Training Epoch: 1/2, step 1001/7134 completed (loss: 0.2920258045196533, acc: 0.918181836605072)
[2025-02-13 19:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44][root][INFO] - Training Epoch: 1/2, step 1002/7134 completed (loss: 0.2354208528995514, acc: 0.9622641801834106)
[2025-02-13 19:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45][root][INFO] - Training Epoch: 1/2, step 1003/7134 completed (loss: 0.1620883345603943, acc: 0.9722222089767456)
[2025-02-13 19:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45][root][INFO] - Training Epoch: 1/2, step 1004/7134 completed (loss: 0.35422632098197937, acc: 0.9203540086746216)
[2025-02-13 19:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46][root][INFO] - Training Epoch: 1/2, step 1005/7134 completed (loss: 0.2899073660373688, acc: 0.9432623982429504)
[2025-02-13 19:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46][root][INFO] - Training Epoch: 1/2, step 1006/7134 completed (loss: 0.31176596879959106, acc: 0.9235668778419495)
[2025-02-13 19:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46][root][INFO] - Training Epoch: 1/2, step 1007/7134 completed (loss: 0.595691978931427, acc: 0.8661971688270569)
[2025-02-13 19:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:47][root][INFO] - Training Epoch: 1/2, step 1008/7134 completed (loss: 0.30700409412384033, acc: 0.895061731338501)
[2025-02-13 19:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:47][root][INFO] - Training Epoch: 1/2, step 1009/7134 completed (loss: 0.23828716576099396, acc: 0.9534883499145508)
[2025-02-13 19:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48][root][INFO] - Training Epoch: 1/2, step 1010/7134 completed (loss: 0.34036779403686523, acc: 0.9059829115867615)
[2025-02-13 19:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48][root][INFO] - Training Epoch: 1/2, step 1011/7134 completed (loss: 0.19486233592033386, acc: 0.955974817276001)
[2025-02-13 19:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48][root][INFO] - Training Epoch: 1/2, step 1012/7134 completed (loss: 0.6048221588134766, acc: 0.8812500238418579)
[2025-02-13 19:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49][root][INFO] - Training Epoch: 1/2, step 1013/7134 completed (loss: 0.42258861660957336, acc: 0.9156626462936401)
[2025-02-13 19:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49][root][INFO] - Training Epoch: 1/2, step 1014/7134 completed (loss: 0.3670729100704193, acc: 0.914893627166748)
[2025-02-13 19:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49][root][INFO] - Training Epoch: 1/2, step 1015/7134 completed (loss: 0.1516655534505844, acc: 0.9867549538612366)
[2025-02-13 19:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50][root][INFO] - Training Epoch: 1/2, step 1016/7134 completed (loss: 0.13039530813694, acc: 0.9663865566253662)
[2025-02-13 19:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50][root][INFO] - Training Epoch: 1/2, step 1017/7134 completed (loss: 0.5776525139808655, acc: 0.9142857193946838)
[2025-02-13 19:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51][root][INFO] - Training Epoch: 1/2, step 1018/7134 completed (loss: 0.49775412678718567, acc: 0.9068322777748108)
[2025-02-13 19:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51][root][INFO] - Training Epoch: 1/2, step 1019/7134 completed (loss: 0.46240234375, acc: 0.9155844449996948)
[2025-02-13 19:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51][root][INFO] - Training Epoch: 1/2, step 1020/7134 completed (loss: 0.20592403411865234, acc: 0.9490445852279663)
[2025-02-13 19:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52][root][INFO] - Training Epoch: 1/2, step 1021/7134 completed (loss: 0.13755269348621368, acc: 0.9738562107086182)
[2025-02-13 19:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52][root][INFO] - Training Epoch: 1/2, step 1022/7134 completed (loss: 0.1389075517654419, acc: 0.9595375657081604)
[2025-02-13 19:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52][root][INFO] - Training Epoch: 1/2, step 1023/7134 completed (loss: 0.2377946525812149, acc: 0.9441340565681458)
[2025-02-13 19:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53][root][INFO] - Training Epoch: 1/2, step 1024/7134 completed (loss: 0.1983572542667389, acc: 0.9626865386962891)
[2025-02-13 19:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53][root][INFO] - Training Epoch: 1/2, step 1025/7134 completed (loss: 0.1571359932422638, acc: 0.9503546357154846)
[2025-02-13 19:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54][root][INFO] - Training Epoch: 1/2, step 1026/7134 completed (loss: 0.25321653485298157, acc: 0.9292035102844238)
[2025-02-13 19:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54][root][INFO] - Training Epoch: 1/2, step 1027/7134 completed (loss: 0.1596108227968216, acc: 0.9647058844566345)
[2025-02-13 19:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54][root][INFO] - Training Epoch: 1/2, step 1028/7134 completed (loss: 0.15870118141174316, acc: 0.955974817276001)
[2025-02-13 19:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55][root][INFO] - Training Epoch: 1/2, step 1029/7134 completed (loss: 0.1933133453130722, acc: 0.9629629850387573)
[2025-02-13 19:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55][root][INFO] - Training Epoch: 1/2, step 1030/7134 completed (loss: 0.08998747169971466, acc: 0.9712643623352051)
[2025-02-13 19:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55][root][INFO] - Training Epoch: 1/2, step 1031/7134 completed (loss: 0.32006338238716125, acc: 0.9130434989929199)
[2025-02-13 19:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56][root][INFO] - Training Epoch: 1/2, step 1032/7134 completed (loss: 0.20569440722465515, acc: 0.9468085169792175)
[2025-02-13 19:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56][root][INFO] - Training Epoch: 1/2, step 1033/7134 completed (loss: 0.2212870866060257, acc: 0.931506872177124)
[2025-02-13 19:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57][root][INFO] - Training Epoch: 1/2, step 1034/7134 completed (loss: 0.5913675427436829, acc: 0.8701298832893372)
[2025-02-13 19:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57][root][INFO] - Training Epoch: 1/2, step 1035/7134 completed (loss: 1.1237854957580566, acc: 0.7972972989082336)
[2025-02-13 19:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57][root][INFO] - Training Epoch: 1/2, step 1036/7134 completed (loss: 1.2382020950317383, acc: 0.7711864113807678)
[2025-02-13 19:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58][root][INFO] - Training Epoch: 1/2, step 1037/7134 completed (loss: 0.22846566140651703, acc: 0.936170220375061)
[2025-02-13 19:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58][root][INFO] - Training Epoch: 1/2, step 1038/7134 completed (loss: 0.35649898648262024, acc: 0.8960000276565552)
[2025-02-13 19:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58][root][INFO] - Training Epoch: 1/2, step 1039/7134 completed (loss: 0.34877392649650574, acc: 0.8990825414657593)
[2025-02-13 19:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59][root][INFO] - Training Epoch: 1/2, step 1040/7134 completed (loss: 0.16321897506713867, acc: 0.9492753744125366)
[2025-02-13 19:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59][root][INFO] - Training Epoch: 1/2, step 1041/7134 completed (loss: 0.1662486493587494, acc: 0.9664429426193237)
[2025-02-13 19:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:00][root][INFO] - Training Epoch: 1/2, step 1042/7134 completed (loss: 0.33101680874824524, acc: 0.9273743033409119)
[2025-02-13 19:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:00][root][INFO] - Training Epoch: 1/2, step 1043/7134 completed (loss: 0.2278059422969818, acc: 0.9504950642585754)
[2025-02-13 19:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01][root][INFO] - Training Epoch: 1/2, step 1044/7134 completed (loss: 0.13524331152439117, acc: 0.9751552939414978)
[2025-02-13 19:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01][root][INFO] - Training Epoch: 1/2, step 1045/7134 completed (loss: 0.7417148947715759, acc: 0.7950310707092285)
[2025-02-13 19:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01][root][INFO] - Training Epoch: 1/2, step 1046/7134 completed (loss: 0.4775391221046448, acc: 0.8947368264198303)
[2025-02-13 19:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02][root][INFO] - Training Epoch: 1/2, step 1047/7134 completed (loss: 0.6725313663482666, acc: 0.8399999737739563)
[2025-02-13 19:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02][root][INFO] - Training Epoch: 1/2, step 1048/7134 completed (loss: 0.48854881525039673, acc: 0.9044585824012756)
[2025-02-13 19:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02][root][INFO] - Training Epoch: 1/2, step 1049/7134 completed (loss: 0.5315558314323425, acc: 0.9144737124443054)
[2025-02-13 19:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03][root][INFO] - Training Epoch: 1/2, step 1050/7134 completed (loss: 0.5881927013397217, acc: 0.859375)
[2025-02-13 19:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03][root][INFO] - Training Epoch: 1/2, step 1051/7134 completed (loss: 0.288146048784256, acc: 0.9085714221000671)
[2025-02-13 19:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04][root][INFO] - Training Epoch: 1/2, step 1052/7134 completed (loss: 0.4656107425689697, acc: 0.8795180916786194)
[2025-02-13 19:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04][root][INFO] - Training Epoch: 1/2, step 1053/7134 completed (loss: 0.28098881244659424, acc: 0.9281437397003174)
[2025-02-13 19:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04][root][INFO] - Training Epoch: 1/2, step 1054/7134 completed (loss: 0.2598326504230499, acc: 0.9375)
[2025-02-13 19:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05][root][INFO] - Training Epoch: 1/2, step 1055/7134 completed (loss: 0.19549888372421265, acc: 0.9465649127960205)
[2025-02-13 19:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05][root][INFO] - Training Epoch: 1/2, step 1056/7134 completed (loss: 0.44076254963874817, acc: 0.895348846912384)
[2025-02-13 19:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06][root][INFO] - Training Epoch: 1/2, step 1057/7134 completed (loss: 0.2263757735490799, acc: 0.9430052042007446)
[2025-02-13 19:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06][root][INFO] - Training Epoch: 1/2, step 1058/7134 completed (loss: 0.22741582989692688, acc: 0.942105233669281)
[2025-02-13 19:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06][root][INFO] - Training Epoch: 1/2, step 1059/7134 completed (loss: 0.33608171343803406, acc: 0.9383886456489563)
[2025-02-13 19:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07][root][INFO] - Training Epoch: 1/2, step 1060/7134 completed (loss: 0.26768597960472107, acc: 0.9358974099159241)
[2025-02-13 19:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07][root][INFO] - Training Epoch: 1/2, step 1061/7134 completed (loss: 0.265076220035553, acc: 0.9432989954948425)
[2025-02-13 19:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08][root][INFO] - Training Epoch: 1/2, step 1062/7134 completed (loss: 0.2604731619358063, acc: 0.9457831382751465)
[2025-02-13 19:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08][root][INFO] - Training Epoch: 1/2, step 1063/7134 completed (loss: 0.19100219011306763, acc: 0.9352940917015076)
[2025-02-13 19:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08][root][INFO] - Training Epoch: 1/2, step 1064/7134 completed (loss: 0.4511376917362213, acc: 0.8918918967247009)
[2025-02-13 19:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09][root][INFO] - Training Epoch: 1/2, step 1065/7134 completed (loss: 0.31777551770210266, acc: 0.9281768202781677)
[2025-02-13 19:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09][root][INFO] - Training Epoch: 1/2, step 1066/7134 completed (loss: 0.48349741101264954, acc: 0.9083969593048096)
[2025-02-13 19:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09][root][INFO] - Training Epoch: 1/2, step 1067/7134 completed (loss: 0.22362391650676727, acc: 0.9346405267715454)
[2025-02-13 19:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:10][root][INFO] - Training Epoch: 1/2, step 1068/7134 completed (loss: 0.3082404136657715, acc: 0.9375)
[2025-02-13 19:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:10][root][INFO] - Training Epoch: 1/2, step 1069/7134 completed (loss: 0.41181135177612305, acc: 0.8888888955116272)
[2025-02-13 19:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11][root][INFO] - Training Epoch: 1/2, step 1070/7134 completed (loss: 0.24431636929512024, acc: 0.9520547986030579)
[2025-02-13 19:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11][root][INFO] - Training Epoch: 1/2, step 1071/7134 completed (loss: 0.20740008354187012, acc: 0.9624999761581421)
[2025-02-13 19:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11][root][INFO] - Training Epoch: 1/2, step 1072/7134 completed (loss: 0.08227984607219696, acc: 0.9756097793579102)
[2025-02-13 19:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12][root][INFO] - Training Epoch: 1/2, step 1073/7134 completed (loss: 0.13555486500263214, acc: 0.9627329111099243)
[2025-02-13 19:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12][root][INFO] - Training Epoch: 1/2, step 1074/7134 completed (loss: 0.3079994022846222, acc: 0.9290780425071716)
[2025-02-13 19:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13][root][INFO] - Training Epoch: 1/2, step 1075/7134 completed (loss: 0.5885145664215088, acc: 0.8686131238937378)
[2025-02-13 19:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13][root][INFO] - Training Epoch: 1/2, step 1076/7134 completed (loss: 0.5158092379570007, acc: 0.8670212626457214)
[2025-02-13 19:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13][root][INFO] - Training Epoch: 1/2, step 1077/7134 completed (loss: 0.474982351064682, acc: 0.9115044474601746)
[2025-02-13 19:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14][root][INFO] - Training Epoch: 1/2, step 1078/7134 completed (loss: 0.6312860250473022, acc: 0.8553459048271179)
[2025-02-13 19:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14][root][INFO] - Training Epoch: 1/2, step 1079/7134 completed (loss: 0.3140838146209717, acc: 0.921658992767334)
[2025-02-13 19:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14][root][INFO] - Training Epoch: 1/2, step 1080/7134 completed (loss: 0.26175811886787415, acc: 0.9534883499145508)
[2025-02-13 19:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:15][root][INFO] - Training Epoch: 1/2, step 1081/7134 completed (loss: 0.3276810646057129, acc: 0.9096385836601257)
[2025-02-13 19:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:15][root][INFO] - Training Epoch: 1/2, step 1082/7134 completed (loss: 0.09870875626802444, acc: 0.9775784611701965)
[2025-02-13 19:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16][root][INFO] - Training Epoch: 1/2, step 1083/7134 completed (loss: 0.31124359369277954, acc: 0.9340101480484009)
[2025-02-13 19:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16][root][INFO] - Training Epoch: 1/2, step 1084/7134 completed (loss: 0.2310619205236435, acc: 0.9202127456665039)
[2025-02-13 19:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16][root][INFO] - Training Epoch: 1/2, step 1085/7134 completed (loss: 0.21977481245994568, acc: 0.9245283007621765)
[2025-02-13 19:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17][root][INFO] - Training Epoch: 1/2, step 1086/7134 completed (loss: 0.5305847525596619, acc: 0.8709677457809448)
[2025-02-13 19:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17][root][INFO] - Training Epoch: 1/2, step 1087/7134 completed (loss: 0.23006902635097504, acc: 0.9569892287254333)
[2025-02-13 19:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17][root][INFO] - Training Epoch: 1/2, step 1088/7134 completed (loss: 0.19842447340488434, acc: 0.9545454382896423)
[2025-02-13 19:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:18][root][INFO] - Training Epoch: 1/2, step 1089/7134 completed (loss: 0.4788174629211426, acc: 0.8773006200790405)
[2025-02-13 19:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:18][root][INFO] - Training Epoch: 1/2, step 1090/7134 completed (loss: 0.4166279733181, acc: 0.886904776096344)
[2025-02-13 19:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19][root][INFO] - Training Epoch: 1/2, step 1091/7134 completed (loss: 0.3702087998390198, acc: 0.9068322777748108)
[2025-02-13 19:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19][root][INFO] - Training Epoch: 1/2, step 1092/7134 completed (loss: 0.1876433789730072, acc: 0.9580838084220886)
[2025-02-13 19:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19][root][INFO] - Training Epoch: 1/2, step 1093/7134 completed (loss: 0.33132997155189514, acc: 0.918367326259613)
[2025-02-13 19:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:20][root][INFO] - Training Epoch: 1/2, step 1094/7134 completed (loss: 0.43719595670700073, acc: 0.8796992301940918)
[2025-02-13 19:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:20][root][INFO] - Training Epoch: 1/2, step 1095/7134 completed (loss: 0.27422335743904114, acc: 0.9312169551849365)
[2025-02-13 19:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21][root][INFO] - Training Epoch: 1/2, step 1096/7134 completed (loss: 0.34302565455436707, acc: 0.9247311949729919)
[2025-02-13 19:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21][root][INFO] - Training Epoch: 1/2, step 1097/7134 completed (loss: 0.4128792881965637, acc: 0.8961748480796814)
[2025-02-13 19:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21][root][INFO] - Training Epoch: 1/2, step 1098/7134 completed (loss: 0.2307387888431549, acc: 0.9444444179534912)
[2025-02-13 19:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:22][root][INFO] - Training Epoch: 1/2, step 1099/7134 completed (loss: 0.3537074029445648, acc: 0.9166666865348816)
[2025-02-13 19:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:22][root][INFO] - Training Epoch: 1/2, step 1100/7134 completed (loss: 0.37254372239112854, acc: 0.9064327478408813)
[2025-02-13 19:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23][root][INFO] - Training Epoch: 1/2, step 1101/7134 completed (loss: 0.34806138277053833, acc: 0.9179487228393555)
[2025-02-13 19:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23][root][INFO] - Training Epoch: 1/2, step 1102/7134 completed (loss: 0.8272759318351746, acc: 0.8611111044883728)
[2025-02-13 19:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23][root][INFO] - Training Epoch: 1/2, step 1103/7134 completed (loss: 0.634127140045166, acc: 0.875)
[2025-02-13 19:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24][root][INFO] - Training Epoch: 1/2, step 1104/7134 completed (loss: 0.3314366340637207, acc: 0.9133333563804626)
[2025-02-13 19:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24][root][INFO] - Training Epoch: 1/2, step 1105/7134 completed (loss: 0.3664872944355011, acc: 0.9320987462997437)
[2025-02-13 19:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24][root][INFO] - Training Epoch: 1/2, step 1106/7134 completed (loss: 0.24867244064807892, acc: 0.9271523356437683)
[2025-02-13 19:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:25][root][INFO] - Training Epoch: 1/2, step 1107/7134 completed (loss: 0.22577521204948425, acc: 0.9388889074325562)
[2025-02-13 19:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:25][root][INFO] - Training Epoch: 1/2, step 1108/7134 completed (loss: 0.24113276600837708, acc: 0.9408283829689026)
[2025-02-13 19:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26][root][INFO] - Training Epoch: 1/2, step 1109/7134 completed (loss: 0.3744068443775177, acc: 0.9017341136932373)
[2025-02-13 19:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26][root][INFO] - Training Epoch: 1/2, step 1110/7134 completed (loss: 0.3198964595794678, acc: 0.9193548560142517)
[2025-02-13 19:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26][root][INFO] - Training Epoch: 1/2, step 1111/7134 completed (loss: 0.2706616520881653, acc: 0.9375)
[2025-02-13 19:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27][root][INFO] - Training Epoch: 1/2, step 1112/7134 completed (loss: 0.2138477861881256, acc: 0.9370629191398621)
[2025-02-13 19:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27][root][INFO] - Training Epoch: 1/2, step 1113/7134 completed (loss: 0.2907102704048157, acc: 0.9245283007621765)
[2025-02-13 19:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28][root][INFO] - Training Epoch: 1/2, step 1114/7134 completed (loss: 0.32190370559692383, acc: 0.9259259104728699)
[2025-02-13 19:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28][root][INFO] - Training Epoch: 1/2, step 1115/7134 completed (loss: 0.2888401746749878, acc: 0.948387086391449)
[2025-02-13 19:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28][root][INFO] - Training Epoch: 1/2, step 1116/7134 completed (loss: 0.24349910020828247, acc: 0.934959352016449)
[2025-02-13 19:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29][root][INFO] - Training Epoch: 1/2, step 1117/7134 completed (loss: 0.14936189353466034, acc: 0.9605262875556946)
[2025-02-13 19:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29][root][INFO] - Training Epoch: 1/2, step 1118/7134 completed (loss: 0.13909180462360382, acc: 0.9541984796524048)
[2025-02-13 19:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30][root][INFO] - Training Epoch: 1/2, step 1119/7134 completed (loss: 0.17517922818660736, acc: 0.951724112033844)
[2025-02-13 19:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30][root][INFO] - Training Epoch: 1/2, step 1120/7134 completed (loss: 0.36753156781196594, acc: 0.9083969593048096)
[2025-02-13 19:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30][root][INFO] - Training Epoch: 1/2, step 1121/7134 completed (loss: 0.18875719606876373, acc: 0.948387086391449)
[2025-02-13 19:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31][root][INFO] - Training Epoch: 1/2, step 1122/7134 completed (loss: 0.09852461516857147, acc: 0.9769230484962463)
[2025-02-13 19:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31][root][INFO] - Training Epoch: 1/2, step 1123/7134 completed (loss: 0.18791519105434418, acc: 0.951724112033844)
[2025-02-13 19:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31][root][INFO] - Training Epoch: 1/2, step 1124/7134 completed (loss: 0.24016927182674408, acc: 0.9189189076423645)
[2025-02-13 19:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32][root][INFO] - Training Epoch: 1/2, step 1125/7134 completed (loss: 0.15991029143333435, acc: 0.9583333134651184)
[2025-02-13 19:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32][root][INFO] - Training Epoch: 1/2, step 1126/7134 completed (loss: 0.16651229560375214, acc: 0.9606741666793823)
[2025-02-13 19:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33][root][INFO] - Training Epoch: 1/2, step 1127/7134 completed (loss: 0.14371474087238312, acc: 0.9779411554336548)
[2025-02-13 19:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33][root][INFO] - Training Epoch: 1/2, step 1128/7134 completed (loss: 0.22407016158103943, acc: 0.9236640930175781)
[2025-02-13 19:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33][root][INFO] - Training Epoch: 1/2, step 1129/7134 completed (loss: 0.28533700108528137, acc: 0.9083333611488342)
[2025-02-13 19:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34][root][INFO] - Training Epoch: 1/2, step 1130/7134 completed (loss: 0.14563339948654175, acc: 0.9640287756919861)
[2025-02-13 19:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34][root][INFO] - Training Epoch: 1/2, step 1131/7134 completed (loss: 0.3374953866004944, acc: 0.9242424368858337)
[2025-02-13 19:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34][root][INFO] - Training Epoch: 1/2, step 1132/7134 completed (loss: 0.6618435382843018, acc: 0.8456375598907471)
[2025-02-13 19:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35][root][INFO] - Training Epoch: 1/2, step 1133/7134 completed (loss: 0.42312684655189514, acc: 0.9075144529342651)
[2025-02-13 19:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35][root][INFO] - Training Epoch: 1/2, step 1134/7134 completed (loss: 0.42163220047950745, acc: 0.9120879173278809)
[2025-02-13 19:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36][root][INFO] - Training Epoch: 1/2, step 1135/7134 completed (loss: 0.3740862309932709, acc: 0.899328887462616)
[2025-02-13 19:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36][root][INFO] - Training Epoch: 1/2, step 1136/7134 completed (loss: 0.48777782917022705, acc: 0.8928571343421936)
[2025-02-13 19:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36][root][INFO] - Training Epoch: 1/2, step 1137/7134 completed (loss: 0.4106045067310333, acc: 0.8962264060974121)
[2025-02-13 19:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37][root][INFO] - Training Epoch: 1/2, step 1138/7134 completed (loss: 0.3883349597454071, acc: 0.9150000214576721)
[2025-02-13 19:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37][root][INFO] - Training Epoch: 1/2, step 1139/7134 completed (loss: 0.43587666749954224, acc: 0.8901098966598511)
[2025-02-13 19:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38][root][INFO] - Training Epoch: 1/2, step 1140/7134 completed (loss: 0.14351233839988708, acc: 0.9578947424888611)
[2025-02-13 19:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38][root][INFO] - Training Epoch: 1/2, step 1141/7134 completed (loss: 0.1486557126045227, acc: 0.9818181991577148)
[2025-02-13 19:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38][root][INFO] - Training Epoch: 1/2, step 1142/7134 completed (loss: 0.20882637798786163, acc: 0.949999988079071)
[2025-02-13 19:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39][root][INFO] - Training Epoch: 1/2, step 1143/7134 completed (loss: 0.40028175711631775, acc: 0.9277108311653137)
[2025-02-13 19:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39][root][INFO] - Training Epoch: 1/2, step 1144/7134 completed (loss: 0.35643309354782104, acc: 0.9226190447807312)
[2025-02-13 19:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39][root][INFO] - Training Epoch: 1/2, step 1145/7134 completed (loss: 0.1710217148065567, acc: 0.9608938694000244)
[2025-02-13 19:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40][root][INFO] - Training Epoch: 1/2, step 1146/7134 completed (loss: 0.08195342868566513, acc: 0.9924242496490479)
[2025-02-13 19:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40][root][INFO] - Training Epoch: 1/2, step 1147/7134 completed (loss: 0.19394424557685852, acc: 0.9621621370315552)
[2025-02-13 19:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40][root][INFO] - Training Epoch: 1/2, step 1148/7134 completed (loss: 0.1343785673379898, acc: 0.971222996711731)
[2025-02-13 19:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41][root][INFO] - Training Epoch: 1/2, step 1149/7134 completed (loss: 0.41417384147644043, acc: 0.9236111044883728)
[2025-02-13 19:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41][root][INFO] - Training Epoch: 1/2, step 1150/7134 completed (loss: 0.19323159754276276, acc: 0.9477124214172363)
[2025-02-13 19:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42][root][INFO] - Training Epoch: 1/2, step 1151/7134 completed (loss: 0.3117009103298187, acc: 0.9327731132507324)
[2025-02-13 19:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42][root][INFO] - Training Epoch: 1/2, step 1152/7134 completed (loss: 0.19541426002979279, acc: 0.9596773982048035)
[2025-02-13 19:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42][root][INFO] - Training Epoch: 1/2, step 1153/7134 completed (loss: 0.25951099395751953, acc: 0.9411764740943909)
[2025-02-13 19:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43][root][INFO] - Training Epoch: 1/2, step 1154/7134 completed (loss: 0.2681899666786194, acc: 0.9609375)
[2025-02-13 19:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43][root][INFO] - Training Epoch: 1/2, step 1155/7134 completed (loss: 0.5188761353492737, acc: 0.8547008633613586)
[2025-02-13 19:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43][root][INFO] - Training Epoch: 1/2, step 1156/7134 completed (loss: 0.18260051310062408, acc: 0.9545454382896423)
[2025-02-13 19:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44][root][INFO] - Training Epoch: 1/2, step 1157/7134 completed (loss: 0.4714239239692688, acc: 0.9172413945198059)
[2025-02-13 19:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44][root][INFO] - Training Epoch: 1/2, step 1158/7134 completed (loss: 0.393544465303421, acc: 0.939130425453186)
[2025-02-13 19:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45][root][INFO] - Training Epoch: 1/2, step 1159/7134 completed (loss: 0.24206037819385529, acc: 0.9281437397003174)
[2025-02-13 19:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45][root][INFO] - Training Epoch: 1/2, step 1160/7134 completed (loss: 0.1429983377456665, acc: 0.9567901492118835)
[2025-02-13 19:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45][root][INFO] - Training Epoch: 1/2, step 1161/7134 completed (loss: 0.1305006891489029, acc: 0.9855072498321533)
[2025-02-13 19:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46][root][INFO] - Training Epoch: 1/2, step 1162/7134 completed (loss: 0.29120343923568726, acc: 0.9240506291389465)
[2025-02-13 19:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46][root][INFO] - Training Epoch: 1/2, step 1163/7134 completed (loss: 0.2364429533481598, acc: 0.931034505367279)
[2025-02-13 19:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46][root][INFO] - Training Epoch: 1/2, step 1164/7134 completed (loss: 0.24264146387577057, acc: 0.9171270728111267)
[2025-02-13 19:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47][root][INFO] - Training Epoch: 1/2, step 1165/7134 completed (loss: 0.1330961436033249, acc: 0.9692307710647583)
[2025-02-13 19:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47][root][INFO] - Training Epoch: 1/2, step 1166/7134 completed (loss: 0.19968348741531372, acc: 0.936170220375061)
[2025-02-13 19:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48][root][INFO] - Training Epoch: 1/2, step 1167/7134 completed (loss: 0.1306484043598175, acc: 0.9440000057220459)
[2025-02-13 19:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48][root][INFO] - Training Epoch: 1/2, step 1168/7134 completed (loss: 0.15969650447368622, acc: 0.9788359999656677)
[2025-02-13 19:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48][root][INFO] - Training Epoch: 1/2, step 1169/7134 completed (loss: 0.26915833353996277, acc: 0.9627659320831299)
[2025-02-13 19:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:49][root][INFO] - Training Epoch: 1/2, step 1170/7134 completed (loss: 0.3116450011730194, acc: 0.9378530979156494)
[2025-02-13 19:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:49][root][INFO] - Training Epoch: 1/2, step 1171/7134 completed (loss: 0.2077884078025818, acc: 0.9399999976158142)
[2025-02-13 19:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50][root][INFO] - Training Epoch: 1/2, step 1172/7134 completed (loss: 0.1526978760957718, acc: 0.9404761791229248)
[2025-02-13 19:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50][root][INFO] - Training Epoch: 1/2, step 1173/7134 completed (loss: 0.17518861591815948, acc: 0.9576719403266907)
[2025-02-13 19:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50][root][INFO] - Training Epoch: 1/2, step 1174/7134 completed (loss: 0.1647452861070633, acc: 0.9570552110671997)
[2025-02-13 19:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51][root][INFO] - Training Epoch: 1/2, step 1175/7134 completed (loss: 0.21842063963413239, acc: 0.9411764740943909)
[2025-02-13 19:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51][root][INFO] - Training Epoch: 1/2, step 1176/7134 completed (loss: 0.1514812558889389, acc: 0.9652777910232544)
[2025-02-13 19:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51][root][INFO] - Training Epoch: 1/2, step 1177/7134 completed (loss: 0.06957422196865082, acc: 0.9846153855323792)
[2025-02-13 19:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:52][root][INFO] - Training Epoch: 1/2, step 1178/7134 completed (loss: 0.12951427698135376, acc: 0.9807692170143127)
[2025-02-13 19:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:52][root][INFO] - Training Epoch: 1/2, step 1179/7134 completed (loss: 0.09424056112766266, acc: 0.9838709831237793)
[2025-02-13 19:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53][root][INFO] - Training Epoch: 1/2, step 1180/7134 completed (loss: 0.24177120625972748, acc: 0.9527027010917664)
[2025-02-13 19:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53][root][INFO] - Training Epoch: 1/2, step 1181/7134 completed (loss: 0.16315925121307373, acc: 0.9615384340286255)
[2025-02-13 19:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53][root][INFO] - Training Epoch: 1/2, step 1182/7134 completed (loss: 0.13472974300384521, acc: 0.9795918464660645)
[2025-02-13 19:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54][root][INFO] - Training Epoch: 1/2, step 1183/7134 completed (loss: 0.0760934129357338, acc: 0.9826589822769165)
[2025-02-13 19:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54][root][INFO] - Training Epoch: 1/2, step 1184/7134 completed (loss: 0.1787572056055069, acc: 0.9653179049491882)
[2025-02-13 19:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54][root][INFO] - Training Epoch: 1/2, step 1185/7134 completed (loss: 0.08788864314556122, acc: 0.9818181991577148)
[2025-02-13 19:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:55][root][INFO] - Training Epoch: 1/2, step 1186/7134 completed (loss: 0.1049603596329689, acc: 0.967391312122345)
[2025-02-13 19:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:55][root][INFO] - Training Epoch: 1/2, step 1187/7134 completed (loss: 0.11537791788578033, acc: 0.9702380895614624)
[2025-02-13 19:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56][root][INFO] - Training Epoch: 1/2, step 1188/7134 completed (loss: 0.09100900590419769, acc: 0.9677419066429138)
[2025-02-13 19:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56][root][INFO] - Training Epoch: 1/2, step 1189/7134 completed (loss: 0.13400815427303314, acc: 0.976331353187561)
[2025-02-13 19:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56][root][INFO] - Training Epoch: 1/2, step 1190/7134 completed (loss: 0.22736483812332153, acc: 0.9666666388511658)
[2025-02-13 19:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57][root][INFO] - Training Epoch: 1/2, step 1191/7134 completed (loss: 0.27317801117897034, acc: 0.9387755393981934)
[2025-02-13 19:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57][root][INFO] - Training Epoch: 1/2, step 1192/7134 completed (loss: 0.22672447562217712, acc: 0.9398496150970459)
[2025-02-13 19:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57][root][INFO] - Training Epoch: 1/2, step 1193/7134 completed (loss: 0.2684759795665741, acc: 0.9448819160461426)
[2025-02-13 19:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58][root][INFO] - Training Epoch: 1/2, step 1194/7134 completed (loss: 0.16616640985012054, acc: 0.9798657894134521)
[2025-02-13 19:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58][root][INFO] - Training Epoch: 1/2, step 1195/7134 completed (loss: 0.2710137963294983, acc: 0.8999999761581421)
[2025-02-13 19:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59][root][INFO] - Training Epoch: 1/2, step 1196/7134 completed (loss: 0.3441654443740845, acc: 0.9290780425071716)
[2025-02-13 19:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59][root][INFO] - Training Epoch: 1/2, step 1197/7134 completed (loss: 0.26847097277641296, acc: 0.9246575236320496)
[2025-02-13 19:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59][root][INFO] - Training Epoch: 1/2, step 1198/7134 completed (loss: 0.27104130387306213, acc: 0.925000011920929)
[2025-02-13 19:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:00][root][INFO] - Training Epoch: 1/2, step 1199/7134 completed (loss: 0.2603982388973236, acc: 0.9350649118423462)
[2025-02-13 19:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:00][root][INFO] - Training Epoch: 1/2, step 1200/7134 completed (loss: 0.11766742169857025, acc: 0.9532710313796997)
[2025-02-13 19:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01][root][INFO] - Training Epoch: 1/2, step 1201/7134 completed (loss: 0.3471482992172241, acc: 0.9182389974594116)
[2025-02-13 19:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01][root][INFO] - Training Epoch: 1/2, step 1202/7134 completed (loss: 0.41367459297180176, acc: 0.89673912525177)
[2025-02-13 19:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01][root][INFO] - Training Epoch: 1/2, step 1203/7134 completed (loss: 0.21346963942050934, acc: 0.95333331823349)
[2025-02-13 19:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02][root][INFO] - Training Epoch: 1/2, step 1204/7134 completed (loss: 0.47396254539489746, acc: 0.904411792755127)
[2025-02-13 19:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02][root][INFO] - Training Epoch: 1/2, step 1205/7134 completed (loss: 0.47874313592910767, acc: 0.8908045887947083)
[2025-02-13 19:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02][root][INFO] - Training Epoch: 1/2, step 1206/7134 completed (loss: 0.41460248827934265, acc: 0.9145728349685669)
[2025-02-13 19:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03][root][INFO] - Training Epoch: 1/2, step 1207/7134 completed (loss: 0.26472288370132446, acc: 0.9247311949729919)
[2025-02-13 19:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03][root][INFO] - Training Epoch: 1/2, step 1208/7134 completed (loss: 0.4219808876514435, acc: 0.9160305261611938)
[2025-02-13 19:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04][root][INFO] - Training Epoch: 1/2, step 1209/7134 completed (loss: 0.36741602420806885, acc: 0.9016393423080444)
[2025-02-13 19:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04][root][INFO] - Training Epoch: 1/2, step 1210/7134 completed (loss: 0.5816527605056763, acc: 0.8533333539962769)
[2025-02-13 19:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04][root][INFO] - Training Epoch: 1/2, step 1211/7134 completed (loss: 1.0004637241363525, acc: 0.7841726541519165)
[2025-02-13 19:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05][root][INFO] - Training Epoch: 1/2, step 1212/7134 completed (loss: 0.6314698457717896, acc: 0.8394160866737366)
[2025-02-13 19:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05][root][INFO] - Training Epoch: 1/2, step 1213/7134 completed (loss: 0.47231197357177734, acc: 0.88165682554245)
[2025-02-13 19:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05][root][INFO] - Training Epoch: 1/2, step 1214/7134 completed (loss: 0.5533509254455566, acc: 0.846666693687439)
[2025-02-13 19:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06][root][INFO] - Training Epoch: 1/2, step 1215/7134 completed (loss: 0.5475478172302246, acc: 0.8764705657958984)
[2025-02-13 19:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06][root][INFO] - Training Epoch: 1/2, step 1216/7134 completed (loss: 0.2785843312740326, acc: 0.9470198750495911)
[2025-02-13 19:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06][root][INFO] - Training Epoch: 1/2, step 1217/7134 completed (loss: 0.3045932352542877, acc: 0.9276315569877625)
[2025-02-13 19:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07][root][INFO] - Training Epoch: 1/2, step 1218/7134 completed (loss: 0.5176156163215637, acc: 0.887499988079071)
[2025-02-13 19:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07][root][INFO] - Training Epoch: 1/2, step 1219/7134 completed (loss: 0.5187327861785889, acc: 0.8666666746139526)
[2025-02-13 19:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08][root][INFO] - Training Epoch: 1/2, step 1220/7134 completed (loss: 0.3786112070083618, acc: 0.9329897165298462)
[2025-02-13 19:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08][root][INFO] - Training Epoch: 1/2, step 1221/7134 completed (loss: 0.34093591570854187, acc: 0.9140271544456482)
[2025-02-13 19:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08][root][INFO] - Training Epoch: 1/2, step 1222/7134 completed (loss: 0.5025856494903564, acc: 0.893203854560852)
[2025-02-13 19:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09][root][INFO] - Training Epoch: 1/2, step 1223/7134 completed (loss: 0.47109776735305786, acc: 0.8920454382896423)
[2025-02-13 19:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09][root][INFO] - Training Epoch: 1/2, step 1224/7134 completed (loss: 0.493713915348053, acc: 0.9056603908538818)
[2025-02-13 19:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09][root][INFO] - Training Epoch: 1/2, step 1225/7134 completed (loss: 0.40229928493499756, acc: 0.9178082346916199)
[2025-02-13 19:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10][root][INFO] - Training Epoch: 1/2, step 1226/7134 completed (loss: 0.6726662516593933, acc: 0.8492063283920288)
[2025-02-13 19:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10][root][INFO] - Training Epoch: 1/2, step 1227/7134 completed (loss: 0.9144574999809265, acc: 0.7835051417350769)
[2025-02-13 19:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11][root][INFO] - Training Epoch: 1/2, step 1228/7134 completed (loss: 0.40175166726112366, acc: 0.895348846912384)
[2025-02-13 19:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11][root][INFO] - Training Epoch: 1/2, step 1229/7134 completed (loss: 0.3733764886856079, acc: 0.9108911156654358)
[2025-02-13 19:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11][root][INFO] - Training Epoch: 1/2, step 1230/7134 completed (loss: 0.3790593147277832, acc: 0.8962264060974121)
[2025-02-13 19:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:12][root][INFO] - Training Epoch: 1/2, step 1231/7134 completed (loss: 0.366656094789505, acc: 0.9008620977401733)
[2025-02-13 19:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:12][root][INFO] - Training Epoch: 1/2, step 1232/7134 completed (loss: 0.42533406615257263, acc: 0.8968609571456909)
[2025-02-13 19:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13][root][INFO] - Training Epoch: 1/2, step 1233/7134 completed (loss: 0.6631256341934204, acc: 0.8552036285400391)
[2025-02-13 19:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13][root][INFO] - Training Epoch: 1/2, step 1234/7134 completed (loss: 0.2855101227760315, acc: 0.9294871687889099)
[2025-02-13 19:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13][root][INFO] - Training Epoch: 1/2, step 1235/7134 completed (loss: 0.3706909120082855, acc: 0.9081632494926453)
[2025-02-13 19:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14][root][INFO] - Training Epoch: 1/2, step 1236/7134 completed (loss: 0.6814921498298645, acc: 0.8529411554336548)
[2025-02-13 19:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14][root][INFO] - Training Epoch: 1/2, step 1237/7134 completed (loss: 0.4334331750869751, acc: 0.9139072895050049)
[2025-02-13 19:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15][root][INFO] - Training Epoch: 1/2, step 1238/7134 completed (loss: 1.008974313735962, acc: 0.7767441868782043)
[2025-02-13 19:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15][root][INFO] - Training Epoch: 1/2, step 1239/7134 completed (loss: 0.5473060607910156, acc: 0.8719512224197388)
[2025-02-13 19:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15][root][INFO] - Training Epoch: 1/2, step 1240/7134 completed (loss: 0.5094529390335083, acc: 0.8430232405662537)
[2025-02-13 19:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16][root][INFO] - Training Epoch: 1/2, step 1241/7134 completed (loss: 0.380481481552124, acc: 0.868852436542511)
[2025-02-13 19:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16][root][INFO] - Training Epoch: 1/2, step 1242/7134 completed (loss: 0.5634238123893738, acc: 0.8638743162155151)
[2025-02-13 19:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16][root][INFO] - Training Epoch: 1/2, step 1243/7134 completed (loss: 0.474946528673172, acc: 0.8991228342056274)
[2025-02-13 19:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:17][root][INFO] - Training Epoch: 1/2, step 1244/7134 completed (loss: 0.3689460754394531, acc: 0.9014084339141846)
[2025-02-13 19:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:17][root][INFO] - Training Epoch: 1/2, step 1245/7134 completed (loss: 0.40317273139953613, acc: 0.9040403962135315)
[2025-02-13 19:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18][root][INFO] - Training Epoch: 1/2, step 1246/7134 completed (loss: 0.5535405874252319, acc: 0.8631578683853149)
[2025-02-13 19:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18][root][INFO] - Training Epoch: 1/2, step 1247/7134 completed (loss: 0.3080207407474518, acc: 0.9277108311653137)
[2025-02-13 19:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18][root][INFO] - Training Epoch: 1/2, step 1248/7134 completed (loss: 0.47475048899650574, acc: 0.8999999761581421)
[2025-02-13 19:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19][root][INFO] - Training Epoch: 1/2, step 1249/7134 completed (loss: 0.34872862696647644, acc: 0.9137930870056152)
[2025-02-13 19:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19][root][INFO] - Training Epoch: 1/2, step 1250/7134 completed (loss: 0.2407550811767578, acc: 0.9466666579246521)
[2025-02-13 19:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19][root][INFO] - Training Epoch: 1/2, step 1251/7134 completed (loss: 0.43354564905166626, acc: 0.8666666746139526)
[2025-02-13 19:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:20][root][INFO] - Training Epoch: 1/2, step 1252/7134 completed (loss: 0.25395381450653076, acc: 0.9763779640197754)
[2025-02-13 19:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:20][root][INFO] - Training Epoch: 1/2, step 1253/7134 completed (loss: 0.6100517511367798, acc: 0.8461538553237915)
[2025-02-13 19:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21][root][INFO] - Training Epoch: 1/2, step 1254/7134 completed (loss: 0.4358345568180084, acc: 0.8799999952316284)
[2025-02-13 19:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21][root][INFO] - Training Epoch: 1/2, step 1255/7134 completed (loss: 0.4429489076137543, acc: 0.8965517282485962)
[2025-02-13 19:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21][root][INFO] - Training Epoch: 1/2, step 1256/7134 completed (loss: 0.3513909876346588, acc: 0.9083969593048096)
[2025-02-13 19:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:22][root][INFO] - Training Epoch: 1/2, step 1257/7134 completed (loss: 0.30546995997428894, acc: 0.9007633328437805)
[2025-02-13 19:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:22][root][INFO] - Training Epoch: 1/2, step 1258/7134 completed (loss: 0.41402339935302734, acc: 0.9047619104385376)
[2025-02-13 19:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23][root][INFO] - Training Epoch: 1/2, step 1259/7134 completed (loss: 0.524308979511261, acc: 0.8557692170143127)
[2025-02-13 19:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23][root][INFO] - Training Epoch: 1/2, step 1260/7134 completed (loss: 0.24607478082180023, acc: 0.9530201554298401)
[2025-02-13 19:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23][root][INFO] - Training Epoch: 1/2, step 1261/7134 completed (loss: 0.27854013442993164, acc: 0.9256756901741028)
[2025-02-13 19:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24][root][INFO] - Training Epoch: 1/2, step 1262/7134 completed (loss: 0.09037958830595016, acc: 0.9922480583190918)
[2025-02-13 19:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24][root][INFO] - Training Epoch: 1/2, step 1263/7134 completed (loss: 0.2156643271446228, acc: 0.9548386931419373)
[2025-02-13 19:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24][root][INFO] - Training Epoch: 1/2, step 1264/7134 completed (loss: 0.2213158756494522, acc: 0.9481481313705444)
[2025-02-13 19:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:25][root][INFO] - Training Epoch: 1/2, step 1265/7134 completed (loss: 0.24719086289405823, acc: 0.9428571462631226)
[2025-02-13 19:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:25][root][INFO] - Training Epoch: 1/2, step 1266/7134 completed (loss: 0.26278865337371826, acc: 0.9465649127960205)
[2025-02-13 19:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26][root][INFO] - Training Epoch: 1/2, step 1267/7134 completed (loss: 0.28320005536079407, acc: 0.9285714030265808)
[2025-02-13 19:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26][root][INFO] - Training Epoch: 1/2, step 1268/7134 completed (loss: 0.34940433502197266, acc: 0.9259259104728699)
[2025-02-13 19:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27][root][INFO] - Training Epoch: 1/2, step 1269/7134 completed (loss: 0.5620083212852478, acc: 0.8636363744735718)
[2025-02-13 19:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27][root][INFO] - Training Epoch: 1/2, step 1270/7134 completed (loss: 0.291469007730484, acc: 0.9246575236320496)
[2025-02-13 19:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27][root][INFO] - Training Epoch: 1/2, step 1271/7134 completed (loss: 0.35344046354293823, acc: 0.925000011920929)
[2025-02-13 19:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28][root][INFO] - Training Epoch: 1/2, step 1272/7134 completed (loss: 0.18526935577392578, acc: 0.955974817276001)
[2025-02-13 19:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28][root][INFO] - Training Epoch: 1/2, step 1273/7134 completed (loss: 0.31280601024627686, acc: 0.9037036895751953)
[2025-02-13 19:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29][root][INFO] - Training Epoch: 1/2, step 1274/7134 completed (loss: 0.09067589044570923, acc: 0.9834710955619812)
[2025-02-13 19:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29][root][INFO] - Training Epoch: 1/2, step 1275/7134 completed (loss: 0.34931254386901855, acc: 0.9130434989929199)
[2025-02-13 19:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29][root][INFO] - Training Epoch: 1/2, step 1276/7134 completed (loss: 0.15436670184135437, acc: 0.9696969985961914)
[2025-02-13 19:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30][root][INFO] - Training Epoch: 1/2, step 1277/7134 completed (loss: 0.12236972898244858, acc: 0.9647887349128723)
[2025-02-13 19:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30][root][INFO] - Training Epoch: 1/2, step 1278/7134 completed (loss: 0.454601913690567, acc: 0.875)
[2025-02-13 19:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30][root][INFO] - Training Epoch: 1/2, step 1279/7134 completed (loss: 0.27605944871902466, acc: 0.929347813129425)
[2025-02-13 19:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31][root][INFO] - Training Epoch: 1/2, step 1280/7134 completed (loss: 0.3307522237300873, acc: 0.9304812550544739)
[2025-02-13 19:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31][root][INFO] - Training Epoch: 1/2, step 1281/7134 completed (loss: 0.33700767159461975, acc: 0.9015151262283325)
[2025-02-13 19:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32][root][INFO] - Training Epoch: 1/2, step 1282/7134 completed (loss: 0.6431187987327576, acc: 0.8797814249992371)
[2025-02-13 19:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32][root][INFO] - Training Epoch: 1/2, step 1283/7134 completed (loss: 0.2574407160282135, acc: 0.9379844665527344)
[2025-02-13 19:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32][root][INFO] - Training Epoch: 1/2, step 1284/7134 completed (loss: 0.31495121121406555, acc: 0.916167676448822)
[2025-02-13 19:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33][root][INFO] - Training Epoch: 1/2, step 1285/7134 completed (loss: 0.22988763451576233, acc: 0.9750000238418579)
[2025-02-13 19:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33][root][INFO] - Training Epoch: 1/2, step 1286/7134 completed (loss: 0.3189316391944885, acc: 0.9329268336296082)
[2025-02-13 19:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34][root][INFO] - Training Epoch: 1/2, step 1287/7134 completed (loss: 0.2631233334541321, acc: 0.9111111164093018)
[2025-02-13 19:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34][root][INFO] - Training Epoch: 1/2, step 1288/7134 completed (loss: 0.28545111417770386, acc: 0.9170984625816345)
[2025-02-13 19:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34][root][INFO] - Training Epoch: 1/2, step 1289/7134 completed (loss: 0.40810877084732056, acc: 0.9379310607910156)
[2025-02-13 19:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35][root][INFO] - Training Epoch: 1/2, step 1290/7134 completed (loss: 0.1615360826253891, acc: 0.9580838084220886)
[2025-02-13 19:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35][root][INFO] - Training Epoch: 1/2, step 1291/7134 completed (loss: 0.27141058444976807, acc: 0.9235293865203857)
[2025-02-13 19:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35][root][INFO] - Training Epoch: 1/2, step 1292/7134 completed (loss: 0.23965680599212646, acc: 0.9395604133605957)
[2025-02-13 19:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36][root][INFO] - Training Epoch: 1/2, step 1293/7134 completed (loss: 0.2325121909379959, acc: 0.9384615421295166)
[2025-02-13 19:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36][root][INFO] - Training Epoch: 1/2, step 1294/7134 completed (loss: 0.24301423132419586, acc: 0.9398906826972961)
[2025-02-13 19:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37][root][INFO] - Training Epoch: 1/2, step 1295/7134 completed (loss: 0.20165131986141205, acc: 0.9626865386962891)
[2025-02-13 19:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37][root][INFO] - Training Epoch: 1/2, step 1296/7134 completed (loss: 0.19135960936546326, acc: 0.9615384340286255)
[2025-02-13 19:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37][root][INFO] - Training Epoch: 1/2, step 1297/7134 completed (loss: 0.27853187918663025, acc: 0.9107142686843872)
[2025-02-13 19:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38][root][INFO] - Training Epoch: 1/2, step 1298/7134 completed (loss: 0.18958064913749695, acc: 0.9669421315193176)
[2025-02-13 19:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38][root][INFO] - Training Epoch: 1/2, step 1299/7134 completed (loss: 0.19624799489974976, acc: 0.9518072009086609)
[2025-02-13 19:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39][root][INFO] - Training Epoch: 1/2, step 1300/7134 completed (loss: 0.19356326758861542, acc: 0.9504950642585754)
[2025-02-13 19:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39][root][INFO] - Training Epoch: 1/2, step 1301/7134 completed (loss: 0.1627458781003952, acc: 0.9473684430122375)
[2025-02-13 19:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39][root][INFO] - Training Epoch: 1/2, step 1302/7134 completed (loss: 0.23666365444660187, acc: 0.9306930899620056)
[2025-02-13 19:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40][root][INFO] - Training Epoch: 1/2, step 1303/7134 completed (loss: 0.25402018427848816, acc: 0.9100000262260437)
[2025-02-13 19:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40][root][INFO] - Training Epoch: 1/2, step 1304/7134 completed (loss: 0.2978517711162567, acc: 0.9435028433799744)
[2025-02-13 19:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41][root][INFO] - Training Epoch: 1/2, step 1305/7134 completed (loss: 0.3131793439388275, acc: 0.9020618796348572)
[2025-02-13 19:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41][root][INFO] - Training Epoch: 1/2, step 1306/7134 completed (loss: 0.25640368461608887, acc: 0.9605262875556946)
[2025-02-13 19:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41][root][INFO] - Training Epoch: 1/2, step 1307/7134 completed (loss: 0.20505847036838531, acc: 0.9496402740478516)
[2025-02-13 19:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42][root][INFO] - Training Epoch: 1/2, step 1308/7134 completed (loss: 0.27249646186828613, acc: 0.9290322661399841)
[2025-02-13 19:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42][root][INFO] - Training Epoch: 1/2, step 1309/7134 completed (loss: 0.25486519932746887, acc: 0.9227052927017212)
[2025-02-13 19:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42][root][INFO] - Training Epoch: 1/2, step 1310/7134 completed (loss: 0.45250988006591797, acc: 0.8674033284187317)
[2025-02-13 19:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43][root][INFO] - Training Epoch: 1/2, step 1311/7134 completed (loss: 0.23202157020568848, acc: 0.9230769276618958)
[2025-02-13 19:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43][root][INFO] - Training Epoch: 1/2, step 1312/7134 completed (loss: 0.5176169872283936, acc: 0.9130434989929199)
[2025-02-13 19:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44][root][INFO] - Training Epoch: 1/2, step 1313/7134 completed (loss: 0.43643757700920105, acc: 0.9197530746459961)
[2025-02-13 19:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44][root][INFO] - Training Epoch: 1/2, step 1314/7134 completed (loss: 0.3257597088813782, acc: 0.9202898740768433)
[2025-02-13 19:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44][root][INFO] - Training Epoch: 1/2, step 1315/7134 completed (loss: 0.4913370907306671, acc: 0.8594594597816467)
[2025-02-13 19:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:45][root][INFO] - Training Epoch: 1/2, step 1316/7134 completed (loss: 0.5036015510559082, acc: 0.8861788511276245)
[2025-02-13 19:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:45][root][INFO] - Training Epoch: 1/2, step 1317/7134 completed (loss: 0.445115864276886, acc: 0.89552241563797)
[2025-02-13 19:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46][root][INFO] - Training Epoch: 1/2, step 1318/7134 completed (loss: 0.2234324812889099, acc: 0.9560439586639404)
[2025-02-13 19:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46][root][INFO] - Training Epoch: 1/2, step 1319/7134 completed (loss: 0.18477635085582733, acc: 0.9818181991577148)
[2025-02-13 19:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46][root][INFO] - Training Epoch: 1/2, step 1320/7134 completed (loss: 0.3853474259376526, acc: 0.9139072895050049)
[2025-02-13 19:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:47][root][INFO] - Training Epoch: 1/2, step 1321/7134 completed (loss: 0.3112742304801941, acc: 0.9119170904159546)
[2025-02-13 19:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:47][root][INFO] - Training Epoch: 1/2, step 1322/7134 completed (loss: 0.42523738741874695, acc: 0.8979591727256775)
[2025-02-13 19:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48][root][INFO] - Training Epoch: 1/2, step 1323/7134 completed (loss: 0.46286848187446594, acc: 0.9154929518699646)
[2025-02-13 19:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48][root][INFO] - Training Epoch: 1/2, step 1324/7134 completed (loss: 0.3328196108341217, acc: 0.9084967374801636)
[2025-02-13 19:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48][root][INFO] - Training Epoch: 1/2, step 1325/7134 completed (loss: 0.3314151465892792, acc: 0.8737863898277283)
[2025-02-13 19:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49][root][INFO] - Training Epoch: 1/2, step 1326/7134 completed (loss: 0.4455793499946594, acc: 0.9086757898330688)
[2025-02-13 19:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49][root][INFO] - Training Epoch: 1/2, step 1327/7134 completed (loss: 0.23393195867538452, acc: 0.9408283829689026)
[2025-02-13 19:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50][root][INFO] - Training Epoch: 1/2, step 1328/7134 completed (loss: 0.3236280679702759, acc: 0.9108911156654358)
[2025-02-13 19:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50][root][INFO] - Training Epoch: 1/2, step 1329/7134 completed (loss: 0.3144740164279938, acc: 0.9399999976158142)
[2025-02-13 19:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50][root][INFO] - Training Epoch: 1/2, step 1330/7134 completed (loss: 0.2475036233663559, acc: 0.9496402740478516)
[2025-02-13 19:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51][root][INFO] - Training Epoch: 1/2, step 1331/7134 completed (loss: 0.15130412578582764, acc: 0.9647887349128723)
[2025-02-13 19:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51][root][INFO] - Training Epoch: 1/2, step 1332/7134 completed (loss: 0.15787965059280396, acc: 0.9473684430122375)
[2025-02-13 19:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51][root][INFO] - Training Epoch: 1/2, step 1333/7134 completed (loss: 0.3350280523300171, acc: 0.9144737124443054)
[2025-02-13 19:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52][root][INFO] - Training Epoch: 1/2, step 1334/7134 completed (loss: 0.5719394683837891, acc: 0.8580247163772583)
[2025-02-13 19:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52][root][INFO] - Training Epoch: 1/2, step 1335/7134 completed (loss: 0.4491385519504547, acc: 0.9162303805351257)
[2025-02-13 19:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52][root][INFO] - Training Epoch: 1/2, step 1336/7134 completed (loss: 0.3189845085144043, acc: 0.9465240836143494)
[2025-02-13 19:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53][root][INFO] - Training Epoch: 1/2, step 1337/7134 completed (loss: 0.24960613250732422, acc: 0.9290322661399841)
[2025-02-13 19:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53][root][INFO] - Training Epoch: 1/2, step 1338/7134 completed (loss: 0.4148581624031067, acc: 0.921875)
[2025-02-13 19:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54][root][INFO] - Training Epoch: 1/2, step 1339/7134 completed (loss: 0.4715954661369324, acc: 0.8775510191917419)
[2025-02-13 19:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54][root][INFO] - Training Epoch: 1/2, step 1340/7134 completed (loss: 0.36947205662727356, acc: 0.9285714030265808)
[2025-02-13 19:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54][root][INFO] - Training Epoch: 1/2, step 1341/7134 completed (loss: 0.15411193668842316, acc: 0.9780219793319702)
[2025-02-13 19:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55][root][INFO] - Training Epoch: 1/2, step 1342/7134 completed (loss: 0.1864853799343109, acc: 0.9470198750495911)
[2025-02-13 19:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55][root][INFO] - Training Epoch: 1/2, step 1343/7134 completed (loss: 0.46228376030921936, acc: 0.8823529481887817)
[2025-02-13 19:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55][root][INFO] - Training Epoch: 1/2, step 1344/7134 completed (loss: 0.19054584205150604, acc: 0.9266055226325989)
[2025-02-13 19:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56][root][INFO] - Training Epoch: 1/2, step 1345/7134 completed (loss: 0.24970173835754395, acc: 0.9139785170555115)
[2025-02-13 19:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56][root][INFO] - Training Epoch: 1/2, step 1346/7134 completed (loss: 0.2806916832923889, acc: 0.9180327653884888)
[2025-02-13 19:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56][root][INFO] - Training Epoch: 1/2, step 1347/7134 completed (loss: 0.21812519431114197, acc: 0.9236111044883728)
[2025-02-13 19:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57][root][INFO] - Training Epoch: 1/2, step 1348/7134 completed (loss: 0.1081458106637001, acc: 0.9779005646705627)
[2025-02-13 19:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57][root][INFO] - Training Epoch: 1/2, step 1349/7134 completed (loss: 0.08546330034732819, acc: 0.9716312289237976)
[2025-02-13 19:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58][root][INFO] - Training Epoch: 1/2, step 1350/7134 completed (loss: 0.12519985437393188, acc: 0.976047933101654)
[2025-02-13 19:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58][root][INFO] - Training Epoch: 1/2, step 1351/7134 completed (loss: 0.051115963608026505, acc: 0.9916666746139526)
[2025-02-13 19:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58][root][INFO] - Training Epoch: 1/2, step 1352/7134 completed (loss: 0.16153863072395325, acc: 0.9695122241973877)
[2025-02-13 19:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59][root][INFO] - Training Epoch: 1/2, step 1353/7134 completed (loss: 0.24891681969165802, acc: 0.9416666626930237)
[2025-02-13 19:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59][root][INFO] - Training Epoch: 1/2, step 1354/7134 completed (loss: 0.07051212340593338, acc: 0.9800000190734863)
[2025-02-13 19:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59][root][INFO] - Training Epoch: 1/2, step 1355/7134 completed (loss: 0.04606451094150543, acc: 0.9935483932495117)
[2025-02-13 19:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00][root][INFO] - Training Epoch: 1/2, step 1356/7134 completed (loss: 0.10159571468830109, acc: 0.9709302186965942)
[2025-02-13 19:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00][root][INFO] - Training Epoch: 1/2, step 1357/7134 completed (loss: 0.08299200981855392, acc: 0.9788359999656677)
[2025-02-13 19:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01][root][INFO] - Training Epoch: 1/2, step 1358/7134 completed (loss: 0.13745875656604767, acc: 0.9772727489471436)
[2025-02-13 19:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01][root][INFO] - Training Epoch: 1/2, step 1359/7134 completed (loss: 0.06366175413131714, acc: 0.9834710955619812)
[2025-02-13 19:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01][root][INFO] - Training Epoch: 1/2, step 1360/7134 completed (loss: 0.05653534084558487, acc: 0.988095223903656)
[2025-02-13 19:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02][root][INFO] - Training Epoch: 1/2, step 1361/7134 completed (loss: 0.08783473074436188, acc: 0.976331353187561)
[2025-02-13 19:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02][root][INFO] - Training Epoch: 1/2, step 1362/7134 completed (loss: 0.20960696041584015, acc: 0.9617486596107483)
[2025-02-13 19:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03][root][INFO] - Training Epoch: 1/2, step 1363/7134 completed (loss: 0.1612335443496704, acc: 0.959770143032074)
[2025-02-13 19:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03][root][INFO] - Training Epoch: 1/2, step 1364/7134 completed (loss: 0.06163613870739937, acc: 0.9941176176071167)
[2025-02-13 19:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03][root][INFO] - Training Epoch: 1/2, step 1365/7134 completed (loss: 0.053256019949913025, acc: 0.9851852059364319)
[2025-02-13 19:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04][root][INFO] - Training Epoch: 1/2, step 1366/7134 completed (loss: 0.02808430604636669, acc: 1.0)
[2025-02-13 19:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04][root][INFO] - Training Epoch: 1/2, step 1367/7134 completed (loss: 0.08502891659736633, acc: 0.9852941036224365)
[2025-02-13 19:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04][root][INFO] - Training Epoch: 1/2, step 1368/7134 completed (loss: 0.1790982186794281, acc: 0.9425287246704102)
[2025-02-13 19:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05][root][INFO] - Training Epoch: 1/2, step 1369/7134 completed (loss: 0.2231459766626358, acc: 0.9489051103591919)
[2025-02-13 19:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05][root][INFO] - Training Epoch: 1/2, step 1370/7134 completed (loss: 0.3555029332637787, acc: 0.9591836929321289)
[2025-02-13 19:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06][root][INFO] - Training Epoch: 1/2, step 1371/7134 completed (loss: 0.19752450287342072, acc: 0.9506173133850098)
[2025-02-13 19:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06][root][INFO] - Training Epoch: 1/2, step 1372/7134 completed (loss: 0.2253953069448471, acc: 0.9242424368858337)
[2025-02-13 19:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06][root][INFO] - Training Epoch: 1/2, step 1373/7134 completed (loss: 0.20209035277366638, acc: 0.9379844665527344)
[2025-02-13 19:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07][root][INFO] - Training Epoch: 1/2, step 1374/7134 completed (loss: 0.2832067310810089, acc: 0.9333333373069763)
[2025-02-13 19:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07][root][INFO] - Training Epoch: 1/2, step 1375/7134 completed (loss: 0.1673678755760193, acc: 0.9509803652763367)
[2025-02-13 19:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08][root][INFO] - Training Epoch: 1/2, step 1376/7134 completed (loss: 0.5548428297042847, acc: 0.8839285969734192)
[2025-02-13 19:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08][root][INFO] - Training Epoch: 1/2, step 1377/7134 completed (loss: 0.47170791029930115, acc: 0.8823529481887817)
[2025-02-13 19:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08][root][INFO] - Training Epoch: 1/2, step 1378/7134 completed (loss: 0.4917342960834503, acc: 0.8846153616905212)
[2025-02-13 19:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09][root][INFO] - Training Epoch: 1/2, step 1379/7134 completed (loss: 0.33849984407424927, acc: 0.9009901285171509)
[2025-02-13 19:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09][root][INFO] - Training Epoch: 1/2, step 1380/7134 completed (loss: 0.4836457073688507, acc: 0.8888888955116272)
[2025-02-13 19:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09][root][INFO] - Training Epoch: 1/2, step 1381/7134 completed (loss: 0.2085196077823639, acc: 0.9492753744125366)
[2025-02-13 19:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10][root][INFO] - Training Epoch: 1/2, step 1382/7134 completed (loss: 0.34028196334838867, acc: 0.9307692050933838)
[2025-02-13 19:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10][root][INFO] - Training Epoch: 1/2, step 1383/7134 completed (loss: 0.5277990102767944, acc: 0.8823529481887817)
[2025-02-13 19:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11][root][INFO] - Training Epoch: 1/2, step 1384/7134 completed (loss: 0.5017809271812439, acc: 0.8652482032775879)
[2025-02-13 19:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11][root][INFO] - Training Epoch: 1/2, step 1385/7134 completed (loss: 0.5672925710678101, acc: 0.8560000061988831)
[2025-02-13 19:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11][root][INFO] - Training Epoch: 1/2, step 1386/7134 completed (loss: 0.3628561198711395, acc: 0.908450722694397)
[2025-02-13 19:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12][root][INFO] - Training Epoch: 1/2, step 1387/7134 completed (loss: 0.25834882259368896, acc: 0.9370629191398621)
[2025-02-13 19:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12][root][INFO] - Training Epoch: 1/2, step 1388/7134 completed (loss: 0.3947764039039612, acc: 0.8799999952316284)
[2025-02-13 19:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12][root][INFO] - Training Epoch: 1/2, step 1389/7134 completed (loss: 0.3426763117313385, acc: 0.9253731369972229)
[2025-02-13 19:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13][root][INFO] - Training Epoch: 1/2, step 1390/7134 completed (loss: 0.41472166776657104, acc: 0.8978102207183838)
[2025-02-13 19:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13][root][INFO] - Training Epoch: 1/2, step 1391/7134 completed (loss: 0.5213207602500916, acc: 0.845588207244873)
[2025-02-13 19:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14][root][INFO] - Training Epoch: 1/2, step 1392/7134 completed (loss: 0.519973874092102, acc: 0.9024389982223511)
[2025-02-13 19:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14][root][INFO] - Training Epoch: 1/2, step 1393/7134 completed (loss: 0.4783380329608917, acc: 0.84375)
[2025-02-13 19:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14][root][INFO] - Training Epoch: 1/2, step 1394/7134 completed (loss: 0.41399553418159485, acc: 0.9245283007621765)
[2025-02-13 19:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15][root][INFO] - Training Epoch: 1/2, step 1395/7134 completed (loss: 0.40583088994026184, acc: 0.9453125)
[2025-02-13 19:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15][root][INFO] - Training Epoch: 1/2, step 1396/7134 completed (loss: 0.25034403800964355, acc: 0.9279999732971191)
[2025-02-13 19:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16][root][INFO] - Training Epoch: 1/2, step 1397/7134 completed (loss: 0.5857754945755005, acc: 0.8175675868988037)
[2025-02-13 19:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16][root][INFO] - Training Epoch: 1/2, step 1398/7134 completed (loss: 0.2458084374666214, acc: 0.9405940771102905)
[2025-02-13 19:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16][root][INFO] - Training Epoch: 1/2, step 1399/7134 completed (loss: 0.4024572968482971, acc: 0.9280575513839722)
[2025-02-13 19:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17][root][INFO] - Training Epoch: 1/2, step 1400/7134 completed (loss: 0.25748929381370544, acc: 0.9347826242446899)
[2025-02-13 19:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17][root][INFO] - Training Epoch: 1/2, step 1401/7134 completed (loss: 0.20382483303546906, acc: 0.9481481313705444)
[2025-02-13 19:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17][root][INFO] - Training Epoch: 1/2, step 1402/7134 completed (loss: 0.26576030254364014, acc: 0.9253731369972229)
[2025-02-13 19:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18][root][INFO] - Training Epoch: 1/2, step 1403/7134 completed (loss: 0.22135837376117706, acc: 0.9349112510681152)
[2025-02-13 19:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18][root][INFO] - Training Epoch: 1/2, step 1404/7134 completed (loss: 0.13742280006408691, acc: 0.9583333134651184)
[2025-02-13 19:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19][root][INFO] - Training Epoch: 1/2, step 1405/7134 completed (loss: 0.2284507006406784, acc: 0.9642857313156128)
[2025-02-13 19:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19][root][INFO] - Training Epoch: 1/2, step 1406/7134 completed (loss: 0.16193924844264984, acc: 0.95652174949646)
[2025-02-13 19:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19][root][INFO] - Training Epoch: 1/2, step 1407/7134 completed (loss: 0.09055311977863312, acc: 0.9767441749572754)
[2025-02-13 19:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20][root][INFO] - Training Epoch: 1/2, step 1408/7134 completed (loss: 0.11099937558174133, acc: 0.9724137783050537)
[2025-02-13 19:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20][root][INFO] - Training Epoch: 1/2, step 1409/7134 completed (loss: 0.0591883510351181, acc: 0.9874213933944702)
[2025-02-13 19:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20][root][INFO] - Training Epoch: 1/2, step 1410/7134 completed (loss: 0.21830520033836365, acc: 0.9613259434700012)
[2025-02-13 19:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:21][root][INFO] - Training Epoch: 1/2, step 1411/7134 completed (loss: 0.10192354768514633, acc: 0.977142870426178)
[2025-02-13 19:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:21][root][INFO] - Training Epoch: 1/2, step 1412/7134 completed (loss: 0.1561654508113861, acc: 0.9684210419654846)
[2025-02-13 19:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22][root][INFO] - Training Epoch: 1/2, step 1413/7134 completed (loss: 0.09660445898771286, acc: 0.976047933101654)
[2025-02-13 19:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22][root][INFO] - Training Epoch: 1/2, step 1414/7134 completed (loss: 0.0731196403503418, acc: 0.9801324605941772)
[2025-02-13 19:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22][root][INFO] - Training Epoch: 1/2, step 1415/7134 completed (loss: 0.18382352590560913, acc: 0.956204354763031)
[2025-02-13 19:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23][root][INFO] - Training Epoch: 1/2, step 1416/7134 completed (loss: 0.05435590073466301, acc: 0.994350254535675)
[2025-02-13 19:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23][root][INFO] - Training Epoch: 1/2, step 1417/7134 completed (loss: 0.06786993891000748, acc: 0.9791666865348816)
[2025-02-13 19:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23][root][INFO] - Training Epoch: 1/2, step 1418/7134 completed (loss: 0.09524016082286835, acc: 0.9689922332763672)
[2025-02-13 19:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:24][root][INFO] - Training Epoch: 1/2, step 1419/7134 completed (loss: 0.11259731650352478, acc: 0.9659090638160706)
[2025-02-13 19:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:24][root][INFO] - Training Epoch: 1/2, step 1420/7134 completed (loss: 0.15733113884925842, acc: 0.9631578922271729)
[2025-02-13 19:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25][root][INFO] - Training Epoch: 1/2, step 1421/7134 completed (loss: 0.06227216124534607, acc: 0.9865771532058716)
[2025-02-13 19:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25][root][INFO] - Training Epoch: 1/2, step 1422/7134 completed (loss: 0.4288000166416168, acc: 0.8975903391838074)
[2025-02-13 19:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25][root][INFO] - Training Epoch: 1/2, step 1423/7134 completed (loss: 0.651077926158905, acc: 0.8549618124961853)
[2025-02-13 19:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26][root][INFO] - Training Epoch: 1/2, step 1424/7134 completed (loss: 0.39868801832199097, acc: 0.8943089246749878)
[2025-02-13 19:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26][root][INFO] - Training Epoch: 1/2, step 1425/7134 completed (loss: 0.2829861640930176, acc: 0.9190751314163208)
[2025-02-13 19:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26][root][INFO] - Training Epoch: 1/2, step 1426/7134 completed (loss: 0.30880168080329895, acc: 0.9175257682800293)
[2025-02-13 19:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27][root][INFO] - Training Epoch: 1/2, step 1427/7134 completed (loss: 0.34900814294815063, acc: 0.903954803943634)
[2025-02-13 19:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27][root][INFO] - Training Epoch: 1/2, step 1428/7134 completed (loss: 1.37895667552948, acc: 0.6774193644523621)
[2025-02-13 19:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28][root][INFO] - Training Epoch: 1/2, step 1429/7134 completed (loss: 0.3445735573768616, acc: 0.9351851940155029)
[2025-02-13 19:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28][root][INFO] - Training Epoch: 1/2, step 1430/7134 completed (loss: 0.2986803948879242, acc: 0.9136690497398376)
[2025-02-13 19:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28][root][INFO] - Training Epoch: 1/2, step 1431/7134 completed (loss: 0.16962337493896484, acc: 0.9516128897666931)
[2025-02-13 19:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29][root][INFO] - Training Epoch: 1/2, step 1432/7134 completed (loss: 0.25634661316871643, acc: 0.8994975090026855)
[2025-02-13 19:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29][root][INFO] - Training Epoch: 1/2, step 1433/7134 completed (loss: 0.27461761236190796, acc: 0.9313725233078003)
[2025-02-13 19:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29][root][INFO] - Training Epoch: 1/2, step 1434/7134 completed (loss: 0.30378004908561707, acc: 0.9203979969024658)
[2025-02-13 19:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30][root][INFO] - Training Epoch: 1/2, step 1435/7134 completed (loss: 0.3377287983894348, acc: 0.9139785170555115)
[2025-02-13 19:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30][root][INFO] - Training Epoch: 1/2, step 1436/7134 completed (loss: 0.29299530386924744, acc: 0.9305555820465088)
[2025-02-13 19:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31][root][INFO] - Training Epoch: 1/2, step 1437/7134 completed (loss: 0.2012597620487213, acc: 0.9731183052062988)
[2025-02-13 19:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31][root][INFO] - Training Epoch: 1/2, step 1438/7134 completed (loss: 0.23339836299419403, acc: 0.9454545378684998)
[2025-02-13 19:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31][root][INFO] - Training Epoch: 1/2, step 1439/7134 completed (loss: 0.14547070860862732, acc: 0.970370352268219)
[2025-02-13 19:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32][root][INFO] - Training Epoch: 1/2, step 1440/7134 completed (loss: 0.1675969958305359, acc: 0.9689440727233887)
[2025-02-13 19:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32][root][INFO] - Training Epoch: 1/2, step 1441/7134 completed (loss: 0.08581945300102234, acc: 0.9655172228813171)
[2025-02-13 19:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33][root][INFO] - Training Epoch: 1/2, step 1442/7134 completed (loss: 0.06961207091808319, acc: 0.9910714030265808)
[2025-02-13 19:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33][root][INFO] - Training Epoch: 1/2, step 1443/7134 completed (loss: 0.2448372095823288, acc: 0.9629629850387573)
[2025-02-13 19:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33][root][INFO] - Training Epoch: 1/2, step 1444/7134 completed (loss: 0.21576973795890808, acc: 0.9444444179534912)
[2025-02-13 19:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34][root][INFO] - Training Epoch: 1/2, step 1445/7134 completed (loss: 0.237001433968544, acc: 0.9299362897872925)
[2025-02-13 19:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34][root][INFO] - Training Epoch: 1/2, step 1446/7134 completed (loss: 0.19382694363594055, acc: 0.9444444179534912)
[2025-02-13 19:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34][root][INFO] - Training Epoch: 1/2, step 1447/7134 completed (loss: 0.1836148202419281, acc: 0.9354838728904724)
[2025-02-13 19:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:35][root][INFO] - Training Epoch: 1/2, step 1448/7134 completed (loss: 0.5444126129150391, acc: 0.89552241563797)
[2025-02-13 19:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:35][root][INFO] - Training Epoch: 1/2, step 1449/7134 completed (loss: 0.4935648739337921, acc: 0.8579235076904297)
[2025-02-13 19:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36][root][INFO] - Training Epoch: 1/2, step 1450/7134 completed (loss: 0.22580763697624207, acc: 0.9411764740943909)
[2025-02-13 19:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36][root][INFO] - Training Epoch: 1/2, step 1451/7134 completed (loss: 0.2122737020254135, acc: 0.9476439952850342)
[2025-02-13 19:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36][root][INFO] - Training Epoch: 1/2, step 1452/7134 completed (loss: 0.23926261067390442, acc: 0.9375)
[2025-02-13 19:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37][root][INFO] - Training Epoch: 1/2, step 1453/7134 completed (loss: 0.6877111196517944, acc: 0.8488371968269348)
[2025-02-13 19:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37][root][INFO] - Training Epoch: 1/2, step 1454/7134 completed (loss: 0.3622403144836426, acc: 0.875)
[2025-02-13 19:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38][root][INFO] - Training Epoch: 1/2, step 1455/7134 completed (loss: 0.5771953463554382, acc: 0.8620689511299133)
[2025-02-13 19:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38][root][INFO] - Training Epoch: 1/2, step 1456/7134 completed (loss: 0.30892419815063477, acc: 0.9256198406219482)
[2025-02-13 19:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38][root][INFO] - Training Epoch: 1/2, step 1457/7134 completed (loss: 0.2557327449321747, acc: 0.9130434989929199)
[2025-02-13 19:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39][root][INFO] - Training Epoch: 1/2, step 1458/7134 completed (loss: 0.16560187935829163, acc: 0.9467455744743347)
[2025-02-13 19:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39][root][INFO] - Training Epoch: 1/2, step 1459/7134 completed (loss: 0.1525305062532425, acc: 0.9481481313705444)
[2025-02-13 19:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39][root][INFO] - Training Epoch: 1/2, step 1460/7134 completed (loss: 0.3727886378765106, acc: 0.8934911489486694)
[2025-02-13 19:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:40][root][INFO] - Training Epoch: 1/2, step 1461/7134 completed (loss: 0.3107568919658661, acc: 0.8940397500991821)
[2025-02-13 19:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:40][root][INFO] - Training Epoch: 1/2, step 1462/7134 completed (loss: 0.3270591199398041, acc: 0.9179104566574097)
[2025-02-13 19:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41][root][INFO] - Training Epoch: 1/2, step 1463/7134 completed (loss: 0.484027236700058, acc: 0.8954248428344727)
[2025-02-13 19:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41][root][INFO] - Training Epoch: 1/2, step 1464/7134 completed (loss: 0.46726423501968384, acc: 0.9096385836601257)
[2025-02-13 19:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41][root][INFO] - Training Epoch: 1/2, step 1465/7134 completed (loss: 0.29211679100990295, acc: 0.9358974099159241)
[2025-02-13 19:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42][root][INFO] - Training Epoch: 1/2, step 1466/7134 completed (loss: 0.10699643194675446, acc: 0.9664804339408875)
[2025-02-13 19:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42][root][INFO] - Training Epoch: 1/2, step 1467/7134 completed (loss: 0.1343374103307724, acc: 0.9515151381492615)
[2025-02-13 19:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42][root][INFO] - Training Epoch: 1/2, step 1468/7134 completed (loss: 0.27622613310813904, acc: 0.932584285736084)
[2025-02-13 19:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43][root][INFO] - Training Epoch: 1/2, step 1469/7134 completed (loss: 0.11181552708148956, acc: 0.9886363744735718)
[2025-02-13 19:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43][root][INFO] - Training Epoch: 1/2, step 1470/7134 completed (loss: 0.2835773527622223, acc: 0.9171974658966064)
[2025-02-13 19:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44][root][INFO] - Training Epoch: 1/2, step 1471/7134 completed (loss: 0.4476010799407959, acc: 0.8999999761581421)
[2025-02-13 19:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44][root][INFO] - Training Epoch: 1/2, step 1472/7134 completed (loss: 0.15209147334098816, acc: 0.9707602262496948)
[2025-02-13 19:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44][root][INFO] - Training Epoch: 1/2, step 1473/7134 completed (loss: 0.20055241882801056, acc: 0.9454545378684998)
[2025-02-13 19:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45][root][INFO] - Training Epoch: 1/2, step 1474/7134 completed (loss: 0.20462775230407715, acc: 0.9379310607910156)
[2025-02-13 19:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45][root][INFO] - Training Epoch: 1/2, step 1475/7134 completed (loss: 0.34297823905944824, acc: 0.9333333373069763)
[2025-02-13 19:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45][root][INFO] - Training Epoch: 1/2, step 1476/7134 completed (loss: 0.31010082364082336, acc: 0.9133333563804626)
[2025-02-13 19:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46][root][INFO] - Training Epoch: 1/2, step 1477/7134 completed (loss: 0.21711066365242004, acc: 0.951724112033844)
[2025-02-13 19:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46][root][INFO] - Training Epoch: 1/2, step 1478/7134 completed (loss: 0.26723742485046387, acc: 0.9539473652839661)
[2025-02-13 19:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47][root][INFO] - Training Epoch: 1/2, step 1479/7134 completed (loss: 0.13454285264015198, acc: 0.9712643623352051)
[2025-02-13 19:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47][root][INFO] - Training Epoch: 1/2, step 1480/7134 completed (loss: 0.1185268834233284, acc: 0.9779005646705627)
[2025-02-13 19:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47][root][INFO] - Training Epoch: 1/2, step 1481/7134 completed (loss: 0.17950835824012756, acc: 0.9666666388511658)
[2025-02-13 19:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48][root][INFO] - Training Epoch: 1/2, step 1482/7134 completed (loss: 0.3164280354976654, acc: 0.9230769276618958)
[2025-02-13 19:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48][root][INFO] - Training Epoch: 1/2, step 1483/7134 completed (loss: 0.11273675411939621, acc: 0.9694656729698181)
[2025-02-13 19:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49][root][INFO] - Training Epoch: 1/2, step 1484/7134 completed (loss: 0.3103700578212738, acc: 0.9337349534034729)
[2025-02-13 19:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49][root][INFO] - Training Epoch: 1/2, step 1485/7134 completed (loss: 0.21129952371120453, acc: 0.9239766001701355)
[2025-02-13 19:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49][root][INFO] - Training Epoch: 1/2, step 1486/7134 completed (loss: 0.14631517231464386, acc: 0.9685534834861755)
[2025-02-13 19:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50][root][INFO] - Training Epoch: 1/2, step 1487/7134 completed (loss: 0.2511017620563507, acc: 0.9351351261138916)
[2025-02-13 19:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50][root][INFO] - Training Epoch: 1/2, step 1488/7134 completed (loss: 0.23379763960838318, acc: 0.934883713722229)
[2025-02-13 19:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50][root][INFO] - Training Epoch: 1/2, step 1489/7134 completed (loss: 0.22407656908035278, acc: 0.9336734414100647)
[2025-02-13 19:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51][root][INFO] - Training Epoch: 1/2, step 1490/7134 completed (loss: 0.24659982323646545, acc: 0.9336283206939697)
[2025-02-13 19:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51][root][INFO] - Training Epoch: 1/2, step 1491/7134 completed (loss: 0.1690751016139984, acc: 0.961904764175415)
[2025-02-13 19:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52][root][INFO] - Training Epoch: 1/2, step 1492/7134 completed (loss: 0.342206209897995, acc: 0.9431279897689819)
[2025-02-13 19:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52][root][INFO] - Training Epoch: 1/2, step 1493/7134 completed (loss: 0.21241378784179688, acc: 0.9532710313796997)
[2025-02-13 19:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52][root][INFO] - Training Epoch: 1/2, step 1494/7134 completed (loss: 0.19873519241809845, acc: 0.9533678889274597)
[2025-02-13 19:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53][root][INFO] - Training Epoch: 1/2, step 1495/7134 completed (loss: 0.13884879648685455, acc: 0.9707317352294922)
[2025-02-13 19:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53][root][INFO] - Training Epoch: 1/2, step 1496/7134 completed (loss: 0.20348885655403137, acc: 0.949999988079071)
[2025-02-13 19:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53][root][INFO] - Training Epoch: 1/2, step 1497/7134 completed (loss: 0.10050172358751297, acc: 0.9722222089767456)
[2025-02-13 19:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54][root][INFO] - Training Epoch: 1/2, step 1498/7134 completed (loss: 0.21044692397117615, acc: 0.9405405521392822)
[2025-02-13 19:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54][root][INFO] - Training Epoch: 1/2, step 1499/7134 completed (loss: 0.1344975084066391, acc: 0.9581395387649536)
[2025-02-13 19:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54][root][INFO] - Training Epoch: 1/2, step 1500/7134 completed (loss: 0.3455720543861389, acc: 0.9178082346916199)
[2025-02-13 19:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55][root][INFO] - Training Epoch: 1/2, step 1501/7134 completed (loss: 0.3444829285144806, acc: 0.8994082808494568)
[2025-02-13 19:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55][root][INFO] - Training Epoch: 1/2, step 1502/7134 completed (loss: 0.23954898118972778, acc: 0.9318181872367859)
[2025-02-13 19:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56][root][INFO] - Training Epoch: 1/2, step 1503/7134 completed (loss: 0.2934749126434326, acc: 0.9314285516738892)
[2025-02-13 19:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56][root][INFO] - Training Epoch: 1/2, step 1504/7134 completed (loss: 0.12046222388744354, acc: 0.9682539701461792)
[2025-02-13 19:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56][root][INFO] - Training Epoch: 1/2, step 1505/7134 completed (loss: 0.22971734404563904, acc: 0.949367105960846)
[2025-02-13 19:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57][root][INFO] - Training Epoch: 1/2, step 1506/7134 completed (loss: 0.3149023652076721, acc: 0.9119496941566467)
[2025-02-13 19:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57][root][INFO] - Training Epoch: 1/2, step 1507/7134 completed (loss: 0.3068106472492218, acc: 0.921875)
[2025-02-13 19:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57][root][INFO] - Training Epoch: 1/2, step 1508/7134 completed (loss: 0.1915666162967682, acc: 0.9532710313796997)
[2025-02-13 19:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58][root][INFO] - Training Epoch: 1/2, step 1509/7134 completed (loss: 0.1421745866537094, acc: 0.9666666388511658)
[2025-02-13 19:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58][root][INFO] - Training Epoch: 1/2, step 1510/7134 completed (loss: 0.38018327951431274, acc: 0.9032257795333862)
[2025-02-13 19:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59][root][INFO] - Training Epoch: 1/2, step 1511/7134 completed (loss: 0.3104633688926697, acc: 0.9273743033409119)
[2025-02-13 19:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59][root][INFO] - Training Epoch: 1/2, step 1512/7134 completed (loss: 0.10160894691944122, acc: 0.9800000190734863)
[2025-02-13 19:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59][root][INFO] - Training Epoch: 1/2, step 1513/7134 completed (loss: 0.13280291855335236, acc: 0.9834254384040833)
[2025-02-13 19:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00][root][INFO] - Training Epoch: 1/2, step 1514/7134 completed (loss: 0.21635660529136658, acc: 0.9430052042007446)
[2025-02-13 19:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00][root][INFO] - Training Epoch: 1/2, step 1515/7134 completed (loss: 0.21457791328430176, acc: 0.954954981803894)
[2025-02-13 19:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00][root][INFO] - Training Epoch: 1/2, step 1516/7134 completed (loss: 0.2338021695613861, acc: 0.9624999761581421)
[2025-02-13 19:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01][root][INFO] - Training Epoch: 1/2, step 1517/7134 completed (loss: 0.4179280400276184, acc: 0.8928571343421936)
[2025-02-13 19:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01][root][INFO] - Training Epoch: 1/2, step 1518/7134 completed (loss: 0.18701311945915222, acc: 0.969072163105011)
[2025-02-13 19:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01][root][INFO] - Training Epoch: 1/2, step 1519/7134 completed (loss: 0.19623765349388123, acc: 0.9708737730979919)
[2025-02-13 19:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02][root][INFO] - Training Epoch: 1/2, step 1520/7134 completed (loss: 0.15130780637264252, acc: 0.970802903175354)
[2025-02-13 19:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02][root][INFO] - Training Epoch: 1/2, step 1521/7134 completed (loss: 0.8179240226745605, acc: 0.8088235259056091)
[2025-02-13 19:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03][root][INFO] - Training Epoch: 1/2, step 1522/7134 completed (loss: 0.5463688373565674, acc: 0.8492063283920288)
[2025-02-13 19:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03][root][INFO] - Training Epoch: 1/2, step 1523/7134 completed (loss: 0.6299624443054199, acc: 0.8289473652839661)
[2025-02-13 19:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03][root][INFO] - Training Epoch: 1/2, step 1524/7134 completed (loss: 0.38119494915008545, acc: 0.931506872177124)
[2025-02-13 19:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04][root][INFO] - Training Epoch: 1/2, step 1525/7134 completed (loss: 0.5958740711212158, acc: 0.8899999856948853)
[2025-02-13 19:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04][root][INFO] - Training Epoch: 1/2, step 1526/7134 completed (loss: 0.5462924242019653, acc: 0.8707482814788818)
[2025-02-13 19:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04][root][INFO] - Training Epoch: 1/2, step 1527/7134 completed (loss: 0.8416523337364197, acc: 0.8584070801734924)
[2025-02-13 19:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:05][root][INFO] - Training Epoch: 1/2, step 1528/7134 completed (loss: 0.44577398896217346, acc: 0.8913043737411499)
[2025-02-13 19:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:05][root][INFO] - Training Epoch: 1/2, step 1529/7134 completed (loss: 0.21664702892303467, acc: 0.9468085169792175)
[2025-02-13 19:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06][root][INFO] - Training Epoch: 1/2, step 1530/7134 completed (loss: 0.4140929579734802, acc: 0.9017857313156128)
[2025-02-13 19:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06][root][INFO] - Training Epoch: 1/2, step 1531/7134 completed (loss: 0.7429521083831787, acc: 0.8181818127632141)
[2025-02-13 19:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06][root][INFO] - Training Epoch: 1/2, step 1532/7134 completed (loss: 0.2891865372657776, acc: 0.9076923131942749)
[2025-02-13 19:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07][root][INFO] - Training Epoch: 1/2, step 1533/7134 completed (loss: 0.40518027544021606, acc: 0.891566276550293)
[2025-02-13 19:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07][root][INFO] - Training Epoch: 1/2, step 1534/7134 completed (loss: 0.384379118680954, acc: 0.9280575513839722)
[2025-02-13 19:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07][root][INFO] - Training Epoch: 1/2, step 1535/7134 completed (loss: 0.33429980278015137, acc: 0.8971428275108337)
[2025-02-13 19:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08][root][INFO] - Training Epoch: 1/2, step 1536/7134 completed (loss: 0.24003151059150696, acc: 0.9259259104728699)
[2025-02-13 19:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08][root][INFO] - Training Epoch: 1/2, step 1537/7134 completed (loss: 0.3462788462638855, acc: 0.9298245906829834)
[2025-02-13 19:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09][root][INFO] - Training Epoch: 1/2, step 1538/7134 completed (loss: 0.4888131320476532, acc: 0.9107142686843872)
[2025-02-13 19:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09][root][INFO] - Training Epoch: 1/2, step 1539/7134 completed (loss: 0.3242494761943817, acc: 0.9090909361839294)
[2025-02-13 19:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09][root][INFO] - Training Epoch: 1/2, step 1540/7134 completed (loss: 0.27568984031677246, acc: 0.9177215099334717)
[2025-02-13 19:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10][root][INFO] - Training Epoch: 1/2, step 1541/7134 completed (loss: 0.3240124583244324, acc: 0.9052631855010986)
[2025-02-13 19:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10][root][INFO] - Training Epoch: 1/2, step 1542/7134 completed (loss: 0.21352078020572662, acc: 0.9382022619247437)
[2025-02-13 19:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11][root][INFO] - Training Epoch: 1/2, step 1543/7134 completed (loss: 0.1766020506620407, acc: 0.9571428298950195)
[2025-02-13 19:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11][root][INFO] - Training Epoch: 1/2, step 1544/7134 completed (loss: 0.18222719430923462, acc: 0.9350000023841858)
[2025-02-13 19:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11][root][INFO] - Training Epoch: 1/2, step 1545/7134 completed (loss: 0.2921093702316284, acc: 0.898809552192688)
[2025-02-13 19:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12][root][INFO] - Training Epoch: 1/2, step 1546/7134 completed (loss: 0.5793969035148621, acc: 0.9058823585510254)
[2025-02-13 19:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12][root][INFO] - Training Epoch: 1/2, step 1547/7134 completed (loss: 0.4443145990371704, acc: 0.9012345671653748)
[2025-02-13 19:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12][root][INFO] - Training Epoch: 1/2, step 1548/7134 completed (loss: 0.20628826320171356, acc: 0.9473684430122375)
[2025-02-13 19:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13][root][INFO] - Training Epoch: 1/2, step 1549/7134 completed (loss: 0.12545807659626007, acc: 0.9714285731315613)
[2025-02-13 19:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13][root][INFO] - Training Epoch: 1/2, step 1550/7134 completed (loss: 0.19897176325321198, acc: 0.9647058844566345)
[2025-02-13 19:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14][root][INFO] - Training Epoch: 1/2, step 1551/7134 completed (loss: 0.1632152944803238, acc: 0.9867549538612366)
[2025-02-13 19:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14][root][INFO] - Training Epoch: 1/2, step 1552/7134 completed (loss: 0.1357102394104004, acc: 0.9642857313156128)
[2025-02-13 19:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14][root][INFO] - Training Epoch: 1/2, step 1553/7134 completed (loss: 0.26006999611854553, acc: 0.9220778942108154)
[2025-02-13 19:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15][root][INFO] - Training Epoch: 1/2, step 1554/7134 completed (loss: 0.2629731595516205, acc: 0.9421965479850769)
[2025-02-13 19:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15][root][INFO] - Training Epoch: 1/2, step 1555/7134 completed (loss: 0.20164982974529266, acc: 0.956250011920929)
[2025-02-13 19:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16][root][INFO] - Training Epoch: 1/2, step 1556/7134 completed (loss: 0.2933918237686157, acc: 0.909604549407959)
[2025-02-13 19:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16][root][INFO] - Training Epoch: 1/2, step 1557/7134 completed (loss: 0.16826948523521423, acc: 0.9555555582046509)
[2025-02-13 19:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16][root][INFO] - Training Epoch: 1/2, step 1558/7134 completed (loss: 0.2774639129638672, acc: 0.931506872177124)
[2025-02-13 19:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17][root][INFO] - Training Epoch: 1/2, step 1559/7134 completed (loss: 0.2479090839624405, acc: 0.9371428489685059)
[2025-02-13 19:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17][root][INFO] - Training Epoch: 1/2, step 1560/7134 completed (loss: 0.1569545716047287, acc: 0.9644669890403748)
[2025-02-13 19:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17][root][INFO] - Training Epoch: 1/2, step 1561/7134 completed (loss: 0.46466541290283203, acc: 0.9051724076271057)
[2025-02-13 19:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:18][root][INFO] - Training Epoch: 1/2, step 1562/7134 completed (loss: 0.29051584005355835, acc: 0.9171597361564636)
[2025-02-13 19:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:18][root][INFO] - Training Epoch: 1/2, step 1563/7134 completed (loss: 0.2898785173892975, acc: 0.9166666865348816)
[2025-02-13 19:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19][root][INFO] - Training Epoch: 1/2, step 1564/7134 completed (loss: 0.4158754348754883, acc: 0.8811880946159363)
[2025-02-13 19:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19][root][INFO] - Training Epoch: 1/2, step 1565/7134 completed (loss: 0.40872159600257874, acc: 0.8867924809455872)
[2025-02-13 19:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19][root][INFO] - Training Epoch: 1/2, step 1566/7134 completed (loss: 0.3215011656284332, acc: 0.9116021990776062)
[2025-02-13 19:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20][root][INFO] - Training Epoch: 1/2, step 1567/7134 completed (loss: 0.5076205134391785, acc: 0.8975609540939331)
[2025-02-13 19:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20][root][INFO] - Training Epoch: 1/2, step 1568/7134 completed (loss: 0.44986027479171753, acc: 0.879807710647583)
[2025-02-13 19:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21][root][INFO] - Training Epoch: 1/2, step 1569/7134 completed (loss: 0.3039463758468628, acc: 0.9090909361839294)
[2025-02-13 19:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21][root][INFO] - Training Epoch: 1/2, step 1570/7134 completed (loss: 0.6220350861549377, acc: 0.8873239159584045)
[2025-02-13 19:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21][root][INFO] - Training Epoch: 1/2, step 1571/7134 completed (loss: 0.24208755791187286, acc: 0.9539170265197754)
[2025-02-13 19:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22][root][INFO] - Training Epoch: 1/2, step 1572/7134 completed (loss: 0.42910072207450867, acc: 0.9295774698257446)
[2025-02-13 19:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22][root][INFO] - Training Epoch: 1/2, step 1573/7134 completed (loss: 0.1508239507675171, acc: 0.9640718698501587)
[2025-02-13 19:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23][root][INFO] - Training Epoch: 1/2, step 1574/7134 completed (loss: 0.26763761043548584, acc: 0.9237667918205261)
[2025-02-13 19:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23][root][INFO] - Training Epoch: 1/2, step 1575/7134 completed (loss: 0.24560897052288055, acc: 0.949999988079071)
[2025-02-13 19:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23][root][INFO] - Training Epoch: 1/2, step 1576/7134 completed (loss: 0.21666982769966125, acc: 0.9530516266822815)
[2025-02-13 19:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:24][root][INFO] - Training Epoch: 1/2, step 1577/7134 completed (loss: 0.2343008816242218, acc: 0.9468598961830139)
[2025-02-13 19:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:24][root][INFO] - Training Epoch: 1/2, step 1578/7134 completed (loss: 0.23955732583999634, acc: 0.9318181872367859)
[2025-02-13 19:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25][root][INFO] - Training Epoch: 1/2, step 1579/7134 completed (loss: 0.17898806929588318, acc: 0.9567307829856873)
[2025-02-13 19:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25][root][INFO] - Training Epoch: 1/2, step 1580/7134 completed (loss: 0.5061156153678894, acc: 0.8599033951759338)
[2025-02-13 19:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25][root][INFO] - Training Epoch: 1/2, step 1581/7134 completed (loss: 0.6038565635681152, acc: 0.851190447807312)
[2025-02-13 19:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26][root][INFO] - Training Epoch: 1/2, step 1582/7134 completed (loss: 0.386405885219574, acc: 0.8799999952316284)
[2025-02-13 19:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26][root][INFO] - Training Epoch: 1/2, step 1583/7134 completed (loss: 0.28364962339401245, acc: 0.9257143139839172)
[2025-02-13 19:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26][root][INFO] - Training Epoch: 1/2, step 1584/7134 completed (loss: 0.30297935009002686, acc: 0.9369369149208069)
[2025-02-13 19:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27][root][INFO] - Training Epoch: 1/2, step 1585/7134 completed (loss: 0.18126721680164337, acc: 0.9514563083648682)
[2025-02-13 19:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27][root][INFO] - Training Epoch: 1/2, step 1586/7134 completed (loss: 0.27056455612182617, acc: 0.9178743958473206)
[2025-02-13 19:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28][root][INFO] - Training Epoch: 1/2, step 1587/7134 completed (loss: 0.2729290723800659, acc: 0.9617834687232971)
[2025-02-13 19:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28][root][INFO] - Training Epoch: 1/2, step 1588/7134 completed (loss: 0.191602423787117, acc: 0.9681528806686401)
[2025-02-13 19:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28][root][INFO] - Training Epoch: 1/2, step 1589/7134 completed (loss: 0.20419514179229736, acc: 0.9451219439506531)
[2025-02-13 19:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29][root][INFO] - Training Epoch: 1/2, step 1590/7134 completed (loss: 0.22721584141254425, acc: 0.9428571462631226)
[2025-02-13 19:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29][root][INFO] - Training Epoch: 1/2, step 1591/7134 completed (loss: 0.17350895702838898, acc: 0.9534883499145508)
[2025-02-13 19:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29][root][INFO] - Training Epoch: 1/2, step 1592/7134 completed (loss: 0.247949481010437, acc: 0.948051929473877)
[2025-02-13 19:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30][root][INFO] - Training Epoch: 1/2, step 1593/7134 completed (loss: 0.1609165072441101, acc: 0.9679144620895386)
[2025-02-13 19:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30][root][INFO] - Training Epoch: 1/2, step 1594/7134 completed (loss: 0.16563807427883148, acc: 0.96875)
[2025-02-13 19:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31][root][INFO] - Training Epoch: 1/2, step 1595/7134 completed (loss: 0.15825322270393372, acc: 0.9579831957817078)
[2025-02-13 19:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31][root][INFO] - Training Epoch: 1/2, step 1596/7134 completed (loss: 0.3355627655982971, acc: 0.9305555820465088)
[2025-02-13 19:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31][root][INFO] - Training Epoch: 1/2, step 1597/7134 completed (loss: 0.1292017102241516, acc: 0.9735099077224731)
[2025-02-13 19:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32][root][INFO] - Training Epoch: 1/2, step 1598/7134 completed (loss: 0.43620479106903076, acc: 0.9166666865348816)
[2025-02-13 19:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32][root][INFO] - Training Epoch: 1/2, step 1599/7134 completed (loss: 0.42192181944847107, acc: 0.882758617401123)
[2025-02-13 19:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33][root][INFO] - Training Epoch: 1/2, step 1600/7134 completed (loss: 0.2978895604610443, acc: 0.9285714030265808)
[2025-02-13 19:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33][root][INFO] - Training Epoch: 1/2, step 1601/7134 completed (loss: 0.14608275890350342, acc: 0.9636363387107849)
[2025-02-13 19:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33][root][INFO] - Training Epoch: 1/2, step 1602/7134 completed (loss: 0.20128031075000763, acc: 0.9615384340286255)
[2025-02-13 19:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34][root][INFO] - Training Epoch: 1/2, step 1603/7134 completed (loss: 0.3352857530117035, acc: 0.9277108311653137)
[2025-02-13 19:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34][root][INFO] - Training Epoch: 1/2, step 1604/7134 completed (loss: 0.15023085474967957, acc: 0.9515151381492615)
[2025-02-13 19:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34][root][INFO] - Training Epoch: 1/2, step 1605/7134 completed (loss: 0.19359588623046875, acc: 0.9647058844566345)
[2025-02-13 19:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:35][root][INFO] - Training Epoch: 1/2, step 1606/7134 completed (loss: 0.13904647529125214, acc: 0.9591836929321289)
[2025-02-13 19:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:35][root][INFO] - Training Epoch: 1/2, step 1607/7134 completed (loss: 0.09715200960636139, acc: 0.9727272987365723)
[2025-02-13 19:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36][root][INFO] - Training Epoch: 1/2, step 1608/7134 completed (loss: 0.1552337408065796, acc: 0.960629940032959)
[2025-02-13 19:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36][root][INFO] - Training Epoch: 1/2, step 1609/7134 completed (loss: 0.24099135398864746, acc: 0.9236111044883728)
[2025-02-13 19:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36][root][INFO] - Training Epoch: 1/2, step 1610/7134 completed (loss: 0.17667722702026367, acc: 0.9618320465087891)
[2025-02-13 19:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37][root][INFO] - Training Epoch: 1/2, step 1611/7134 completed (loss: 0.1801927536725998, acc: 0.9567901492118835)
[2025-02-13 19:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37][root][INFO] - Training Epoch: 1/2, step 1612/7134 completed (loss: 0.2051575630903244, acc: 0.9397590160369873)
[2025-02-13 19:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38][root][INFO] - Training Epoch: 1/2, step 1613/7134 completed (loss: 0.17876149713993073, acc: 0.95652174949646)
[2025-02-13 19:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38][root][INFO] - Training Epoch: 1/2, step 1614/7134 completed (loss: 0.19493505358695984, acc: 0.9473684430122375)
[2025-02-13 19:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38][root][INFO] - Training Epoch: 1/2, step 1615/7134 completed (loss: 0.19727273285388947, acc: 0.9277108311653137)
[2025-02-13 19:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39][root][INFO] - Training Epoch: 1/2, step 1616/7134 completed (loss: 0.14187520742416382, acc: 0.9612902998924255)
[2025-02-13 19:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39][root][INFO] - Training Epoch: 1/2, step 1617/7134 completed (loss: 0.15731196105480194, acc: 0.9568345546722412)
[2025-02-13 19:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39][root][INFO] - Training Epoch: 1/2, step 1618/7134 completed (loss: 0.47287246584892273, acc: 0.9083969593048096)
[2025-02-13 19:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40][root][INFO] - Training Epoch: 1/2, step 1619/7134 completed (loss: 0.2626473605632782, acc: 0.9268292784690857)
[2025-02-13 19:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40][root][INFO] - Training Epoch: 1/2, step 1620/7134 completed (loss: 0.3684682846069336, acc: 0.9025974273681641)
[2025-02-13 19:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40][root][INFO] - Training Epoch: 1/2, step 1621/7134 completed (loss: 0.4297720193862915, acc: 0.915032684803009)
[2025-02-13 19:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:41][root][INFO] - Training Epoch: 1/2, step 1622/7134 completed (loss: 0.26784083247184753, acc: 0.9215686321258545)
[2025-02-13 19:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:41][root][INFO] - Training Epoch: 1/2, step 1623/7134 completed (loss: 0.2282443642616272, acc: 0.9166666865348816)
[2025-02-13 19:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42][root][INFO] - Training Epoch: 1/2, step 1624/7134 completed (loss: 0.3121221363544464, acc: 0.9415584206581116)
[2025-02-13 19:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42][root][INFO] - Training Epoch: 1/2, step 1625/7134 completed (loss: 0.2390027940273285, acc: 0.9375)
[2025-02-13 19:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42][root][INFO] - Training Epoch: 1/2, step 1626/7134 completed (loss: 0.3778947591781616, acc: 0.9278350472450256)
[2025-02-13 19:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:43][root][INFO] - Training Epoch: 1/2, step 1627/7134 completed (loss: 0.22873863577842712, acc: 0.934959352016449)
[2025-02-13 19:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:43][root][INFO] - Training Epoch: 1/2, step 1628/7134 completed (loss: 0.07460656017065048, acc: 0.9814814925193787)
[2025-02-13 19:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44][root][INFO] - Training Epoch: 1/2, step 1629/7134 completed (loss: 0.20894090831279755, acc: 0.9545454382896423)
[2025-02-13 19:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44][root][INFO] - Training Epoch: 1/2, step 1630/7134 completed (loss: 0.2892422676086426, acc: 0.9219858050346375)
[2025-02-13 19:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44][root][INFO] - Training Epoch: 1/2, step 1631/7134 completed (loss: 0.3028806447982788, acc: 0.9280575513839722)
[2025-02-13 19:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45][root][INFO] - Training Epoch: 1/2, step 1632/7134 completed (loss: 0.17788489162921906, acc: 0.9477611780166626)
[2025-02-13 19:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45][root][INFO] - Training Epoch: 1/2, step 1633/7134 completed (loss: 0.2994883060455322, acc: 0.9142857193946838)
[2025-02-13 19:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45][root][INFO] - Training Epoch: 1/2, step 1634/7134 completed (loss: 0.16223397850990295, acc: 0.9496855139732361)
[2025-02-13 19:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:46][root][INFO] - Training Epoch: 1/2, step 1635/7134 completed (loss: 0.16739781200885773, acc: 0.9444444179534912)
[2025-02-13 19:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:46][root][INFO] - Training Epoch: 1/2, step 1636/7134 completed (loss: 0.39237305521965027, acc: 0.9082568883895874)
[2025-02-13 19:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47][root][INFO] - Training Epoch: 1/2, step 1637/7134 completed (loss: 0.1679696887731552, acc: 0.95652174949646)
[2025-02-13 19:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47][root][INFO] - Training Epoch: 1/2, step 1638/7134 completed (loss: 0.5775068998336792, acc: 0.8661417365074158)
[2025-02-13 19:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47][root][INFO] - Training Epoch: 1/2, step 1639/7134 completed (loss: 0.7583154439926147, acc: 0.8550724387168884)
[2025-02-13 19:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48][root][INFO] - Training Epoch: 1/2, step 1640/7134 completed (loss: 0.3327418863773346, acc: 0.9202898740768433)
[2025-02-13 19:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48][root][INFO] - Training Epoch: 1/2, step 1641/7134 completed (loss: 0.28948017954826355, acc: 0.9144737124443054)
[2025-02-13 19:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48][root][INFO] - Training Epoch: 1/2, step 1642/7134 completed (loss: 0.3386542797088623, acc: 0.9119496941566467)
[2025-02-13 19:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:49][root][INFO] - Training Epoch: 1/2, step 1643/7134 completed (loss: 0.5528940558433533, acc: 0.8454545736312866)
[2025-02-13 19:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:49][root][INFO] - Training Epoch: 1/2, step 1644/7134 completed (loss: 0.2703803777694702, acc: 0.9371069073677063)
[2025-02-13 19:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50][root][INFO] - Training Epoch: 1/2, step 1645/7134 completed (loss: 0.3633309304714203, acc: 0.8896551728248596)
[2025-02-13 19:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50][root][INFO] - Training Epoch: 1/2, step 1646/7134 completed (loss: 0.4048066735267639, acc: 0.8838709592819214)
[2025-02-13 19:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50][root][INFO] - Training Epoch: 1/2, step 1647/7134 completed (loss: 0.25896501541137695, acc: 0.9483568072319031)
[2025-02-13 19:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51][root][INFO] - Training Epoch: 1/2, step 1648/7134 completed (loss: 0.27926722168922424, acc: 0.9402984976768494)
[2025-02-13 19:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51][root][INFO] - Training Epoch: 1/2, step 1649/7134 completed (loss: 0.23364126682281494, acc: 0.9424778819084167)
[2025-02-13 19:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51][root][INFO] - Training Epoch: 1/2, step 1650/7134 completed (loss: 0.2496970295906067, acc: 0.9396985173225403)
[2025-02-13 19:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52][root][INFO] - Training Epoch: 1/2, step 1651/7134 completed (loss: 0.31837373971939087, acc: 0.9545454382896423)
[2025-02-13 19:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52][root][INFO] - Training Epoch: 1/2, step 1652/7134 completed (loss: 0.25753679871559143, acc: 0.9316239356994629)
[2025-02-13 19:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53][root][INFO] - Training Epoch: 1/2, step 1653/7134 completed (loss: 0.2809092700481415, acc: 0.8961748480796814)
[2025-02-13 19:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53][root][INFO] - Training Epoch: 1/2, step 1654/7134 completed (loss: 0.24267594516277313, acc: 0.9227467775344849)
[2025-02-13 19:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53][root][INFO] - Training Epoch: 1/2, step 1655/7134 completed (loss: 0.2604636251926422, acc: 0.9234693646430969)
[2025-02-13 19:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54][root][INFO] - Training Epoch: 1/2, step 1656/7134 completed (loss: 0.2985594570636749, acc: 0.9147982001304626)
[2025-02-13 19:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54][root][INFO] - Training Epoch: 1/2, step 1657/7134 completed (loss: 0.207577183842659, acc: 0.9418604373931885)
[2025-02-13 19:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54][root][INFO] - Training Epoch: 1/2, step 1658/7134 completed (loss: 0.1693262755870819, acc: 0.954356849193573)
[2025-02-13 19:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:55][root][INFO] - Training Epoch: 1/2, step 1659/7134 completed (loss: 0.25374636054039, acc: 0.9440000057220459)
[2025-02-13 19:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:55][root][INFO] - Training Epoch: 1/2, step 1660/7134 completed (loss: 0.21410122513771057, acc: 0.9471153616905212)
[2025-02-13 19:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56][root][INFO] - Training Epoch: 1/2, step 1661/7134 completed (loss: 0.2624792158603668, acc: 0.9220778942108154)
[2025-02-13 19:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56][root][INFO] - Training Epoch: 1/2, step 1662/7134 completed (loss: 0.13073205947875977, acc: 0.9620853066444397)
[2025-02-13 19:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56][root][INFO] - Training Epoch: 1/2, step 1663/7134 completed (loss: 0.14240583777427673, acc: 0.9531915187835693)
[2025-02-13 19:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57][root][INFO] - Training Epoch: 1/2, step 1664/7134 completed (loss: 0.17750908434391022, acc: 0.9549999833106995)
[2025-02-13 19:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57][root][INFO] - Training Epoch: 1/2, step 1665/7134 completed (loss: 0.1698547750711441, acc: 0.9447852969169617)
[2025-02-13 19:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57][root][INFO] - Training Epoch: 1/2, step 1666/7134 completed (loss: 0.09767935425043106, acc: 0.9747899174690247)
[2025-02-13 19:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:58][root][INFO] - Training Epoch: 1/2, step 1667/7134 completed (loss: 0.1810884028673172, acc: 0.9494949579238892)
[2025-02-13 19:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:58][root][INFO] - Training Epoch: 1/2, step 1668/7134 completed (loss: 0.11118257790803909, acc: 0.9730941653251648)
[2025-02-13 19:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59][root][INFO] - Training Epoch: 1/2, step 1669/7134 completed (loss: 0.15252776443958282, acc: 0.9656488299369812)
[2025-02-13 19:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59][root][INFO] - Training Epoch: 1/2, step 1670/7134 completed (loss: 0.32047176361083984, acc: 0.927756667137146)
[2025-02-13 19:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59][root][INFO] - Training Epoch: 1/2, step 1671/7134 completed (loss: 0.20173968374729156, acc: 0.954081654548645)
[2025-02-13 19:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00][root][INFO] - Training Epoch: 1/2, step 1672/7134 completed (loss: 0.30720070004463196, acc: 0.9473684430122375)
[2025-02-13 19:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00][root][INFO] - Training Epoch: 1/2, step 1673/7134 completed (loss: 0.28616979718208313, acc: 0.9503105878829956)
[2025-02-13 19:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00][root][INFO] - Training Epoch: 1/2, step 1674/7134 completed (loss: 0.5372114777565002, acc: 0.8947368264198303)
[2025-02-13 19:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01][root][INFO] - Training Epoch: 1/2, step 1675/7134 completed (loss: 0.36952051520347595, acc: 0.8972602486610413)
[2025-02-13 19:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01][root][INFO] - Training Epoch: 1/2, step 1676/7134 completed (loss: 0.2917960584163666, acc: 0.9358974099159241)
[2025-02-13 19:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02][root][INFO] - Training Epoch: 1/2, step 1677/7134 completed (loss: 0.5268399119377136, acc: 0.9078013896942139)
[2025-02-13 19:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02][root][INFO] - Training Epoch: 1/2, step 1678/7134 completed (loss: 0.400794118642807, acc: 0.910179615020752)
[2025-02-13 19:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02][root][INFO] - Training Epoch: 1/2, step 1679/7134 completed (loss: 0.3789495825767517, acc: 0.9207317233085632)
[2025-02-13 19:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:03][root][INFO] - Training Epoch: 1/2, step 1680/7134 completed (loss: 0.289000928401947, acc: 0.9235293865203857)
[2025-02-13 19:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:03][root][INFO] - Training Epoch: 1/2, step 1681/7134 completed (loss: 0.3927145302295685, acc: 0.8993710875511169)
[2025-02-13 19:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04][root][INFO] - Training Epoch: 1/2, step 1682/7134 completed (loss: 0.3421233892440796, acc: 0.9246575236320496)
[2025-02-13 19:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04][root][INFO] - Training Epoch: 1/2, step 1683/7134 completed (loss: 0.25279778242111206, acc: 0.931034505367279)
[2025-02-13 19:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04][root][INFO] - Training Epoch: 1/2, step 1684/7134 completed (loss: 0.14532750844955444, acc: 0.9858155846595764)
[2025-02-13 19:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:05][root][INFO] - Training Epoch: 1/2, step 1685/7134 completed (loss: 0.21859115362167358, acc: 0.9454545378684998)
[2025-02-13 19:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:05][root][INFO] - Training Epoch: 1/2, step 1686/7134 completed (loss: 0.2997300624847412, acc: 0.9624060392379761)
[2025-02-13 19:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06][root][INFO] - Training Epoch: 1/2, step 1687/7134 completed (loss: 0.11548654735088348, acc: 0.9557521939277649)
[2025-02-13 19:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06][root][INFO] - Training Epoch: 1/2, step 1688/7134 completed (loss: 0.20600886642932892, acc: 0.9452054500579834)
[2025-02-13 19:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06][root][INFO] - Training Epoch: 1/2, step 1689/7134 completed (loss: 0.37966975569725037, acc: 0.9225806593894958)
[2025-02-13 19:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07][root][INFO] - Training Epoch: 1/2, step 1690/7134 completed (loss: 0.19170072674751282, acc: 0.9292035102844238)
[2025-02-13 19:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07][root][INFO] - Training Epoch: 1/2, step 1691/7134 completed (loss: 0.1770307421684265, acc: 0.957446813583374)
[2025-02-13 19:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07][root][INFO] - Training Epoch: 1/2, step 1692/7134 completed (loss: 0.20777064561843872, acc: 0.9430894255638123)
[2025-02-13 19:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:08][root][INFO] - Training Epoch: 1/2, step 1693/7134 completed (loss: 0.1959071308374405, acc: 0.9591836929321289)
[2025-02-13 19:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:08][root][INFO] - Training Epoch: 1/2, step 1694/7134 completed (loss: 0.3392347991466522, acc: 0.9103448390960693)
[2025-02-13 19:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09][root][INFO] - Training Epoch: 1/2, step 1695/7134 completed (loss: 0.23926463723182678, acc: 0.9327731132507324)
[2025-02-13 19:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09][root][INFO] - Training Epoch: 1/2, step 1696/7134 completed (loss: 0.288941890001297, acc: 0.9248120188713074)
[2025-02-13 19:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09][root][INFO] - Training Epoch: 1/2, step 1697/7134 completed (loss: 0.27780723571777344, acc: 0.9650349617004395)
[2025-02-13 19:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10][root][INFO] - Training Epoch: 1/2, step 1698/7134 completed (loss: 0.06536860764026642, acc: 0.9930070042610168)
[2025-02-13 19:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10][root][INFO] - Training Epoch: 1/2, step 1699/7134 completed (loss: 0.18613417446613312, acc: 0.9552238583564758)
[2025-02-13 19:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10][root][INFO] - Training Epoch: 1/2, step 1700/7134 completed (loss: 0.27994370460510254, acc: 0.9253731369972229)
[2025-02-13 19:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11][root][INFO] - Training Epoch: 1/2, step 1701/7134 completed (loss: 0.31702908873558044, acc: 0.9200000166893005)
[2025-02-13 19:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11][root][INFO] - Training Epoch: 1/2, step 1702/7134 completed (loss: 0.32019755244255066, acc: 0.9221556782722473)
[2025-02-13 19:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12][root][INFO] - Training Epoch: 1/2, step 1703/7134 completed (loss: 0.45285889506340027, acc: 0.887417197227478)
[2025-02-13 19:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12][root][INFO] - Training Epoch: 1/2, step 1704/7134 completed (loss: 0.5495656132698059, acc: 0.8902438879013062)
[2025-02-13 19:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12][root][INFO] - Training Epoch: 1/2, step 1705/7134 completed (loss: 0.40231338143348694, acc: 0.9200000166893005)
[2025-02-13 19:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13][root][INFO] - Training Epoch: 1/2, step 1706/7134 completed (loss: 0.2738715708255768, acc: 0.9571428298950195)
[2025-02-13 19:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13][root][INFO] - Training Epoch: 1/2, step 1707/7134 completed (loss: 0.3457830250263214, acc: 0.931034505367279)
[2025-02-13 19:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13][root][INFO] - Training Epoch: 1/2, step 1708/7134 completed (loss: 0.23327752947807312, acc: 0.9481481313705444)
[2025-02-13 19:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14][root][INFO] - Training Epoch: 1/2, step 1709/7134 completed (loss: 0.3957005441188812, acc: 0.8866666555404663)
[2025-02-13 19:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14][root][INFO] - Training Epoch: 1/2, step 1710/7134 completed (loss: 0.3735823333263397, acc: 0.8986486196517944)
[2025-02-13 19:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14][root][INFO] - Training Epoch: 1/2, step 1711/7134 completed (loss: 0.33468490839004517, acc: 0.9354838728904724)
[2025-02-13 19:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:15][root][INFO] - Training Epoch: 1/2, step 1712/7134 completed (loss: 0.20232810080051422, acc: 0.9523809552192688)
[2025-02-13 19:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:15][root][INFO] - Training Epoch: 1/2, step 1713/7134 completed (loss: 0.26802167296409607, acc: 0.9047619104385376)
[2025-02-13 19:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16][root][INFO] - Training Epoch: 1/2, step 1714/7134 completed (loss: 0.21104733645915985, acc: 0.9503546357154846)
[2025-02-13 19:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16][root][INFO] - Training Epoch: 1/2, step 1715/7134 completed (loss: 0.26953691244125366, acc: 0.9339622855186462)
[2025-02-13 19:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16][root][INFO] - Training Epoch: 1/2, step 1716/7134 completed (loss: 0.45069965720176697, acc: 0.9210526347160339)
[2025-02-13 19:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17][root][INFO] - Training Epoch: 1/2, step 1717/7134 completed (loss: 0.241104394197464, acc: 0.9457364082336426)
[2025-02-13 19:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17][root][INFO] - Training Epoch: 1/2, step 1718/7134 completed (loss: 0.4033969044685364, acc: 0.8974359035491943)
[2025-02-13 19:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17][root][INFO] - Training Epoch: 1/2, step 1719/7134 completed (loss: 0.32999545335769653, acc: 0.9112426042556763)
[2025-02-13 19:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18][root][INFO] - Training Epoch: 1/2, step 1720/7134 completed (loss: 0.6040011048316956, acc: 0.8384615182876587)
[2025-02-13 19:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18][root][INFO] - Training Epoch: 1/2, step 1721/7134 completed (loss: 0.24456624686717987, acc: 0.921875)
[2025-02-13 19:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19][root][INFO] - Training Epoch: 1/2, step 1722/7134 completed (loss: 0.17619305849075317, acc: 0.9696969985961914)
[2025-02-13 19:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19][root][INFO] - Training Epoch: 1/2, step 1723/7134 completed (loss: 0.5527806282043457, acc: 0.8888888955116272)
[2025-02-13 19:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19][root][INFO] - Training Epoch: 1/2, step 1724/7134 completed (loss: 0.3665001392364502, acc: 0.921875)
[2025-02-13 19:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20][root][INFO] - Training Epoch: 1/2, step 1725/7134 completed (loss: 0.29397881031036377, acc: 0.9402984976768494)
[2025-02-13 19:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20][root][INFO] - Training Epoch: 1/2, step 1726/7134 completed (loss: 0.1570560336112976, acc: 0.9569892287254333)
[2025-02-13 19:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21][root][INFO] - Training Epoch: 1/2, step 1727/7134 completed (loss: 0.18902674317359924, acc: 0.9469026327133179)
[2025-02-13 19:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21][root][INFO] - Training Epoch: 1/2, step 1728/7134 completed (loss: 0.09753499925136566, acc: 0.9846153855323792)
[2025-02-13 19:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21][root][INFO] - Training Epoch: 1/2, step 1729/7134 completed (loss: 0.3106931746006012, acc: 0.9009009003639221)
[2025-02-13 19:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22][root][INFO] - Training Epoch: 1/2, step 1730/7134 completed (loss: 0.18025344610214233, acc: 0.9345794320106506)
[2025-02-13 19:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22][root][INFO] - Training Epoch: 1/2, step 1731/7134 completed (loss: 0.69044429063797, acc: 0.8433734774589539)
[2025-02-13 19:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22][root][INFO] - Training Epoch: 1/2, step 1732/7134 completed (loss: 0.1774391084909439, acc: 0.9537037014961243)
[2025-02-13 19:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23][root][INFO] - Training Epoch: 1/2, step 1733/7134 completed (loss: 0.41229677200317383, acc: 0.8990825414657593)
[2025-02-13 19:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23][root][INFO] - Training Epoch: 1/2, step 1734/7134 completed (loss: 0.385755330324173, acc: 0.9156626462936401)
[2025-02-13 19:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23][root][INFO] - Training Epoch: 1/2, step 1735/7134 completed (loss: 0.14566093683242798, acc: 0.9735099077224731)
[2025-02-13 19:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24][root][INFO] - Training Epoch: 1/2, step 1736/7134 completed (loss: 0.3031850755214691, acc: 0.9556962251663208)
[2025-02-13 19:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24][root][INFO] - Training Epoch: 1/2, step 1737/7134 completed (loss: 0.2802990972995758, acc: 0.9399999976158142)
[2025-02-13 19:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25][root][INFO] - Training Epoch: 1/2, step 1738/7134 completed (loss: 0.11302898824214935, acc: 0.9720670580863953)
[2025-02-13 19:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25][root][INFO] - Training Epoch: 1/2, step 1739/7134 completed (loss: 0.27857017517089844, acc: 0.9248120188713074)
[2025-02-13 19:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25][root][INFO] - Training Epoch: 1/2, step 1740/7134 completed (loss: 0.20077762007713318, acc: 0.9391891956329346)
[2025-02-13 19:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26][root][INFO] - Training Epoch: 1/2, step 1741/7134 completed (loss: 0.22789257764816284, acc: 0.9254658222198486)
[2025-02-13 19:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26][root][INFO] - Training Epoch: 1/2, step 1742/7134 completed (loss: 0.250833123922348, acc: 0.940397322177887)
[2025-02-13 19:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26][root][INFO] - Training Epoch: 1/2, step 1743/7134 completed (loss: 0.21982906758785248, acc: 0.9387755393981934)
[2025-02-13 19:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:27][root][INFO] - Training Epoch: 1/2, step 1744/7134 completed (loss: 0.16332946717739105, acc: 0.9548872113227844)
[2025-02-13 19:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:27][root][INFO] - Training Epoch: 1/2, step 1745/7134 completed (loss: 0.17857472598552704, acc: 0.970588207244873)
[2025-02-13 19:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28][root][INFO] - Training Epoch: 1/2, step 1746/7134 completed (loss: 0.15109995007514954, acc: 0.957446813583374)
[2025-02-13 19:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28][root][INFO] - Training Epoch: 1/2, step 1747/7134 completed (loss: 0.25187763571739197, acc: 0.9390243887901306)
[2025-02-13 19:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28][root][INFO] - Training Epoch: 1/2, step 1748/7134 completed (loss: 0.25350692868232727, acc: 0.9352940917015076)
[2025-02-13 19:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29][root][INFO] - Training Epoch: 1/2, step 1749/7134 completed (loss: 0.3388075828552246, acc: 0.926701545715332)
[2025-02-13 19:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29][root][INFO] - Training Epoch: 1/2, step 1750/7134 completed (loss: 0.38329383730888367, acc: 0.9349112510681152)
[2025-02-13 19:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29][root][INFO] - Training Epoch: 1/2, step 1751/7134 completed (loss: 0.40449029207229614, acc: 0.9226190447807312)
[2025-02-13 19:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:30][root][INFO] - Training Epoch: 1/2, step 1752/7134 completed (loss: 0.4064929485321045, acc: 0.9151515364646912)
[2025-02-13 19:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:30][root][INFO] - Training Epoch: 1/2, step 1753/7134 completed (loss: 0.3298385441303253, acc: 0.9271523356437683)
[2025-02-13 19:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31][root][INFO] - Training Epoch: 1/2, step 1754/7134 completed (loss: 0.3770199716091156, acc: 0.9084967374801636)
[2025-02-13 19:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31][root][INFO] - Training Epoch: 1/2, step 1755/7134 completed (loss: 0.16444213688373566, acc: 0.9542483687400818)
[2025-02-13 19:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31][root][INFO] - Training Epoch: 1/2, step 1756/7134 completed (loss: 0.4059199094772339, acc: 0.8866666555404663)
[2025-02-13 19:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32][root][INFO] - Training Epoch: 1/2, step 1757/7134 completed (loss: 0.2124512642621994, acc: 0.9586777091026306)
[2025-02-13 19:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32][root][INFO] - Training Epoch: 1/2, step 1758/7134 completed (loss: 0.1113358587026596, acc: 0.9647058844566345)
[2025-02-13 19:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32][root][INFO] - Training Epoch: 1/2, step 1759/7134 completed (loss: 0.14369671046733856, acc: 0.9539473652839661)
[2025-02-13 19:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33][root][INFO] - Training Epoch: 1/2, step 1760/7134 completed (loss: 0.12719537317752838, acc: 0.9754098653793335)
[2025-02-13 19:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33][root][INFO] - Training Epoch: 1/2, step 1761/7134 completed (loss: 0.2404390275478363, acc: 0.949999988079071)
[2025-02-13 19:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33][root][INFO] - Training Epoch: 1/2, step 1762/7134 completed (loss: 0.18945856392383575, acc: 0.9417475461959839)
[2025-02-13 19:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:34][root][INFO] - Training Epoch: 1/2, step 1763/7134 completed (loss: 0.24932986497879028, acc: 0.9246575236320496)
[2025-02-13 19:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:34][root][INFO] - Training Epoch: 1/2, step 1764/7134 completed (loss: 0.34055668115615845, acc: 0.9274193644523621)
[2025-02-13 19:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35][root][INFO] - Training Epoch: 1/2, step 1765/7134 completed (loss: 0.30070960521698, acc: 0.9215686321258545)
[2025-02-13 19:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35][root][INFO] - Training Epoch: 1/2, step 1766/7134 completed (loss: 0.3700805902481079, acc: 0.9155844449996948)
[2025-02-13 19:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35][root][INFO] - Training Epoch: 1/2, step 1767/7134 completed (loss: 0.3996141850948334, acc: 0.909604549407959)
[2025-02-13 19:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36][root][INFO] - Training Epoch: 1/2, step 1768/7134 completed (loss: 0.06635553389787674, acc: 0.9821428656578064)
[2025-02-13 19:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36][root][INFO] - Training Epoch: 1/2, step 1769/7134 completed (loss: 0.24885477125644684, acc: 0.9399999976158142)
[2025-02-13 19:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36][root][INFO] - Training Epoch: 1/2, step 1770/7134 completed (loss: 0.18703503906726837, acc: 0.965753436088562)
[2025-02-13 19:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37][root][INFO] - Training Epoch: 1/2, step 1771/7134 completed (loss: 0.27804821729660034, acc: 0.9285714030265808)
[2025-02-13 19:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37][root][INFO] - Training Epoch: 1/2, step 1772/7134 completed (loss: 0.22641676664352417, acc: 0.9571428298950195)
[2025-02-13 19:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38][root][INFO] - Training Epoch: 1/2, step 1773/7134 completed (loss: 0.27434638142585754, acc: 0.936170220375061)
[2025-02-13 19:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38][root][INFO] - Training Epoch: 1/2, step 1774/7134 completed (loss: 0.04442411661148071, acc: 0.9924242496490479)
[2025-02-13 19:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38][root][INFO] - Training Epoch: 1/2, step 1775/7134 completed (loss: 0.09933911263942719, acc: 0.9726027250289917)
[2025-02-13 19:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39][root][INFO] - Training Epoch: 1/2, step 1776/7134 completed (loss: 0.18677477538585663, acc: 0.9448275566101074)
[2025-02-13 19:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39][root][INFO] - Training Epoch: 1/2, step 1777/7134 completed (loss: 0.07089754939079285, acc: 0.9745222926139832)
[2025-02-13 19:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39][root][INFO] - Training Epoch: 1/2, step 1778/7134 completed (loss: 0.33693727850914, acc: 0.9245283007621765)
[2025-02-13 19:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:40][root][INFO] - Training Epoch: 1/2, step 1779/7134 completed (loss: 0.27458974719047546, acc: 0.9380530714988708)
[2025-02-13 19:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:40][root][INFO] - Training Epoch: 1/2, step 1780/7134 completed (loss: 0.17479507625102997, acc: 0.9640718698501587)
[2025-02-13 19:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41][root][INFO] - Training Epoch: 1/2, step 1781/7134 completed (loss: 0.1456967145204544, acc: 0.9731543660163879)
[2025-02-13 19:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41][root][INFO] - Training Epoch: 1/2, step 1782/7134 completed (loss: 0.13389825820922852, acc: 0.9679487347602844)
[2025-02-13 19:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:39][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3824, device='cuda:0') eval_epoch_loss=tensor(0.3238, device='cuda:0') eval_epoch_acc=tensor(0.9234, device='cuda:0')
[2025-02-13 19:34:39][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:34:39][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:34:39][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_1783_loss_0.3238261938095093/model.pt
[2025-02-13 19:34:39][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:34:39][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.3238261938095093
[2025-02-13 19:34:39][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9233985543251038
[2025-02-13 19:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40][root][INFO] - Training Epoch: 1/2, step 1783/7134 completed (loss: 0.19835111498832703, acc: 0.9479768872261047)
[2025-02-13 19:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40][root][INFO] - Training Epoch: 1/2, step 1784/7134 completed (loss: 0.14396819472312927, acc: 0.9743589758872986)
[2025-02-13 19:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40][root][INFO] - Training Epoch: 1/2, step 1785/7134 completed (loss: 0.28862375020980835, acc: 0.9397590160369873)
[2025-02-13 19:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41][root][INFO] - Training Epoch: 1/2, step 1786/7134 completed (loss: 0.13057075440883636, acc: 0.9612902998924255)
[2025-02-13 19:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41][root][INFO] - Training Epoch: 1/2, step 1787/7134 completed (loss: 0.34057852625846863, acc: 0.9402984976768494)
[2025-02-13 19:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41][root][INFO] - Training Epoch: 1/2, step 1788/7134 completed (loss: 0.23146985471248627, acc: 0.931034505367279)
[2025-02-13 19:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42][root][INFO] - Training Epoch: 1/2, step 1789/7134 completed (loss: 0.12264194339513779, acc: 0.9802631735801697)
[2025-02-13 19:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42][root][INFO] - Training Epoch: 1/2, step 1790/7134 completed (loss: 0.5978588461875916, acc: 0.8774193525314331)
[2025-02-13 19:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42][root][INFO] - Training Epoch: 1/2, step 1791/7134 completed (loss: 0.5565981864929199, acc: 0.8650306463241577)
[2025-02-13 19:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43][root][INFO] - Training Epoch: 1/2, step 1792/7134 completed (loss: 1.018307089805603, acc: 0.8044692873954773)
[2025-02-13 19:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43][root][INFO] - Training Epoch: 1/2, step 1793/7134 completed (loss: 0.7218692302703857, acc: 0.8461538553237915)
[2025-02-13 19:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44][root][INFO] - Training Epoch: 1/2, step 1794/7134 completed (loss: 0.48230692744255066, acc: 0.9013158082962036)
[2025-02-13 19:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44][root][INFO] - Training Epoch: 1/2, step 1795/7134 completed (loss: 0.856410026550293, acc: 0.831250011920929)
[2025-02-13 19:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44][root][INFO] - Training Epoch: 1/2, step 1796/7134 completed (loss: 0.6500303149223328, acc: 0.8531073331832886)
[2025-02-13 19:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45][root][INFO] - Training Epoch: 1/2, step 1797/7134 completed (loss: 0.539322555065155, acc: 0.868571400642395)
[2025-02-13 19:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45][root][INFO] - Training Epoch: 1/2, step 1798/7134 completed (loss: 0.49000054597854614, acc: 0.8837209343910217)
[2025-02-13 19:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45][root][INFO] - Training Epoch: 1/2, step 1799/7134 completed (loss: 0.39118558168411255, acc: 0.9191918969154358)
[2025-02-13 19:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46][root][INFO] - Training Epoch: 1/2, step 1800/7134 completed (loss: 0.45847707986831665, acc: 0.8999999761581421)
[2025-02-13 19:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46][root][INFO] - Training Epoch: 1/2, step 1801/7134 completed (loss: 0.4647686183452606, acc: 0.8918918967247009)
[2025-02-13 19:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47][root][INFO] - Training Epoch: 1/2, step 1802/7134 completed (loss: 0.3550381362438202, acc: 0.8882978558540344)
[2025-02-13 19:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47][root][INFO] - Training Epoch: 1/2, step 1803/7134 completed (loss: 0.5033478736877441, acc: 0.8597561120986938)
[2025-02-13 19:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47][root][INFO] - Training Epoch: 1/2, step 1804/7134 completed (loss: 0.1977996826171875, acc: 0.9417989253997803)
[2025-02-13 19:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48][root][INFO] - Training Epoch: 1/2, step 1805/7134 completed (loss: 0.2853122651576996, acc: 0.9298245906829834)
[2025-02-13 19:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48][root][INFO] - Training Epoch: 1/2, step 1806/7134 completed (loss: 0.29788848757743835, acc: 0.9344262480735779)
[2025-02-13 19:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48][root][INFO] - Training Epoch: 1/2, step 1807/7134 completed (loss: 0.3842543959617615, acc: 0.9216867685317993)
[2025-02-13 19:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49][root][INFO] - Training Epoch: 1/2, step 1808/7134 completed (loss: 0.1668299436569214, acc: 0.9435028433799744)
[2025-02-13 19:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49][root][INFO] - Training Epoch: 1/2, step 1809/7134 completed (loss: 0.17013876140117645, acc: 0.9595375657081604)
[2025-02-13 19:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49][root][INFO] - Training Epoch: 1/2, step 1810/7134 completed (loss: 0.15252836048603058, acc: 0.9560439586639404)
[2025-02-13 19:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50][root][INFO] - Training Epoch: 1/2, step 1811/7134 completed (loss: 0.3315620720386505, acc: 0.914893627166748)
[2025-02-13 19:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50][root][INFO] - Training Epoch: 1/2, step 1812/7134 completed (loss: 0.2496166080236435, acc: 0.9491525292396545)
[2025-02-13 19:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51][root][INFO] - Training Epoch: 1/2, step 1813/7134 completed (loss: 0.34051281213760376, acc: 0.9116021990776062)
[2025-02-13 19:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51][root][INFO] - Training Epoch: 1/2, step 1814/7134 completed (loss: 0.23171883821487427, acc: 0.9337016344070435)
[2025-02-13 19:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51][root][INFO] - Training Epoch: 1/2, step 1815/7134 completed (loss: 0.25768646597862244, acc: 0.9299362897872925)
[2025-02-13 19:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52][root][INFO] - Training Epoch: 1/2, step 1816/7134 completed (loss: 0.37645062804222107, acc: 0.8819444179534912)
[2025-02-13 19:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52][root][INFO] - Training Epoch: 1/2, step 1817/7134 completed (loss: 0.2716275751590729, acc: 0.9375)
[2025-02-13 19:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52][root][INFO] - Training Epoch: 1/2, step 1818/7134 completed (loss: 0.22592897713184357, acc: 0.9371727705001831)
[2025-02-13 19:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53][root][INFO] - Training Epoch: 1/2, step 1819/7134 completed (loss: 0.15117080509662628, acc: 0.9651162624359131)
[2025-02-13 19:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53][root][INFO] - Training Epoch: 1/2, step 1820/7134 completed (loss: 0.21372836828231812, acc: 0.9433962106704712)
[2025-02-13 19:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54][root][INFO] - Training Epoch: 1/2, step 1821/7134 completed (loss: 0.20863009989261627, acc: 0.9579439163208008)
[2025-02-13 19:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54][root][INFO] - Training Epoch: 1/2, step 1822/7134 completed (loss: 0.17254839837551117, acc: 0.9736841917037964)
[2025-02-13 19:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54][root][INFO] - Training Epoch: 1/2, step 1823/7134 completed (loss: 0.25914466381073, acc: 0.930232584476471)
[2025-02-13 19:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55][root][INFO] - Training Epoch: 1/2, step 1824/7134 completed (loss: 0.24557626247406006, acc: 0.9617486596107483)
[2025-02-13 19:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55][root][INFO] - Training Epoch: 1/2, step 1825/7134 completed (loss: 0.19577360153198242, acc: 0.9579439163208008)
[2025-02-13 19:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55][root][INFO] - Training Epoch: 1/2, step 1826/7134 completed (loss: 0.140126034617424, acc: 0.9590643048286438)
[2025-02-13 19:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56][root][INFO] - Training Epoch: 1/2, step 1827/7134 completed (loss: 0.21589979529380798, acc: 0.9431279897689819)
[2025-02-13 19:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56][root][INFO] - Training Epoch: 1/2, step 1828/7134 completed (loss: 0.12068816274404526, acc: 0.9777777791023254)
[2025-02-13 19:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56][root][INFO] - Training Epoch: 1/2, step 1829/7134 completed (loss: 0.24541963636875153, acc: 0.9462365508079529)
[2025-02-13 19:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57][root][INFO] - Training Epoch: 1/2, step 1830/7134 completed (loss: 0.2657131850719452, acc: 0.9378238320350647)
[2025-02-13 19:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57][root][INFO] - Training Epoch: 1/2, step 1831/7134 completed (loss: 0.09205741435289383, acc: 0.9772727489471436)
[2025-02-13 19:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57][root][INFO] - Training Epoch: 1/2, step 1832/7134 completed (loss: 0.4503030478954315, acc: 0.8914728760719299)
[2025-02-13 19:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58][root][INFO] - Training Epoch: 1/2, step 1833/7134 completed (loss: 0.7684933543205261, acc: 0.8617886304855347)
[2025-02-13 19:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58][root][INFO] - Training Epoch: 1/2, step 1834/7134 completed (loss: 0.265635222196579, acc: 0.9358974099159241)
[2025-02-13 19:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59][root][INFO] - Training Epoch: 1/2, step 1835/7134 completed (loss: 0.41302937269210815, acc: 0.9350649118423462)
[2025-02-13 19:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59][root][INFO] - Training Epoch: 1/2, step 1836/7134 completed (loss: 0.2784750461578369, acc: 0.9251337051391602)
[2025-02-13 19:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59][root][INFO] - Training Epoch: 1/2, step 1837/7134 completed (loss: 0.2821633517742157, acc: 0.9340659379959106)
[2025-02-13 19:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00][root][INFO] - Training Epoch: 1/2, step 1838/7134 completed (loss: 0.23719067871570587, acc: 0.9449541568756104)
[2025-02-13 19:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00][root][INFO] - Training Epoch: 1/2, step 1839/7134 completed (loss: 0.22713924944400787, acc: 0.9395604133605957)
[2025-02-13 19:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00][root][INFO] - Training Epoch: 1/2, step 1840/7134 completed (loss: 0.3068574368953705, acc: 0.892405092716217)
[2025-02-13 19:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:01][root][INFO] - Training Epoch: 1/2, step 1841/7134 completed (loss: 0.38317933678627014, acc: 0.918181836605072)
[2025-02-13 19:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:01][root][INFO] - Training Epoch: 1/2, step 1842/7134 completed (loss: 0.20849435031414032, acc: 0.9390243887901306)
[2025-02-13 19:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02][root][INFO] - Training Epoch: 1/2, step 1843/7134 completed (loss: 0.31653735041618347, acc: 0.9345238208770752)
[2025-02-13 19:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02][root][INFO] - Training Epoch: 1/2, step 1844/7134 completed (loss: 0.11180984973907471, acc: 0.957446813583374)
[2025-02-13 19:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02][root][INFO] - Training Epoch: 1/2, step 1845/7134 completed (loss: 0.26800352334976196, acc: 0.9130434989929199)
[2025-02-13 19:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03][root][INFO] - Training Epoch: 1/2, step 1846/7134 completed (loss: 0.19447046518325806, acc: 0.942307710647583)
[2025-02-13 19:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03][root][INFO] - Training Epoch: 1/2, step 1847/7134 completed (loss: 0.4176067113876343, acc: 0.8863636255264282)
[2025-02-13 19:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03][root][INFO] - Training Epoch: 1/2, step 1848/7134 completed (loss: 0.4664630591869354, acc: 0.8829787373542786)
[2025-02-13 19:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04][root][INFO] - Training Epoch: 1/2, step 1849/7134 completed (loss: 0.24028711020946503, acc: 0.9436619877815247)
[2025-02-13 19:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04][root][INFO] - Training Epoch: 1/2, step 1850/7134 completed (loss: 0.38534092903137207, acc: 0.9226804375648499)
[2025-02-13 19:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04][root][INFO] - Training Epoch: 1/2, step 1851/7134 completed (loss: 0.44025924801826477, acc: 0.8871794939041138)
[2025-02-13 19:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:05][root][INFO] - Training Epoch: 1/2, step 1852/7134 completed (loss: 0.49055710434913635, acc: 0.887499988079071)
[2025-02-13 19:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:05][root][INFO] - Training Epoch: 1/2, step 1853/7134 completed (loss: 0.31002792716026306, acc: 0.909547746181488)
[2025-02-13 19:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06][root][INFO] - Training Epoch: 1/2, step 1854/7134 completed (loss: 0.274863600730896, acc: 0.9276018142700195)
[2025-02-13 19:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06][root][INFO] - Training Epoch: 1/2, step 1855/7134 completed (loss: 0.23101204633712769, acc: 0.9336493015289307)
[2025-02-13 19:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06][root][INFO] - Training Epoch: 1/2, step 1856/7134 completed (loss: 0.2350989580154419, acc: 0.9387755393981934)
[2025-02-13 19:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07][root][INFO] - Training Epoch: 1/2, step 1857/7134 completed (loss: 0.41387277841567993, acc: 0.8970588445663452)
[2025-02-13 19:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07][root][INFO] - Training Epoch: 1/2, step 1858/7134 completed (loss: 0.21130268275737762, acc: 0.9387755393981934)
[2025-02-13 19:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07][root][INFO] - Training Epoch: 1/2, step 1859/7134 completed (loss: 0.2401508092880249, acc: 0.9282296895980835)
[2025-02-13 19:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08][root][INFO] - Training Epoch: 1/2, step 1860/7134 completed (loss: 0.24236741662025452, acc: 0.9398906826972961)
[2025-02-13 19:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08][root][INFO] - Training Epoch: 1/2, step 1861/7134 completed (loss: 0.21300294995307922, acc: 0.9581151604652405)
[2025-02-13 19:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09][root][INFO] - Training Epoch: 1/2, step 1862/7134 completed (loss: 0.17082557082176208, acc: 0.9576719403266907)
[2025-02-13 19:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09][root][INFO] - Training Epoch: 1/2, step 1863/7134 completed (loss: 0.20232194662094116, acc: 0.9605911374092102)
[2025-02-13 19:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09][root][INFO] - Training Epoch: 1/2, step 1864/7134 completed (loss: 0.7552967071533203, acc: 0.8527919054031372)
[2025-02-13 19:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10][root][INFO] - Training Epoch: 1/2, step 1865/7134 completed (loss: 0.405992329120636, acc: 0.9127516746520996)
[2025-02-13 19:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10][root][INFO] - Training Epoch: 1/2, step 1866/7134 completed (loss: 0.10073932260274887, acc: 0.9764705896377563)
[2025-02-13 19:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10][root][INFO] - Training Epoch: 1/2, step 1867/7134 completed (loss: 0.20704308152198792, acc: 0.9488636255264282)
[2025-02-13 19:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11][root][INFO] - Training Epoch: 1/2, step 1868/7134 completed (loss: 0.5217465162277222, acc: 0.8834356069564819)
[2025-02-13 19:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11][root][INFO] - Training Epoch: 1/2, step 1869/7134 completed (loss: 0.3549225926399231, acc: 0.9246575236320496)
[2025-02-13 19:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11][root][INFO] - Training Epoch: 1/2, step 1870/7134 completed (loss: 0.19977183640003204, acc: 0.9459459185600281)
[2025-02-13 19:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12][root][INFO] - Training Epoch: 1/2, step 1871/7134 completed (loss: 0.438533216714859, acc: 0.9225806593894958)
[2025-02-13 19:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12][root][INFO] - Training Epoch: 1/2, step 1872/7134 completed (loss: 2.540863275527954, acc: 0.5652173757553101)
[2025-02-13 19:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13][root][INFO] - Training Epoch: 1/2, step 1873/7134 completed (loss: 0.8075389862060547, acc: 0.8347107172012329)
[2025-02-13 19:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13][root][INFO] - Training Epoch: 1/2, step 1874/7134 completed (loss: 0.16086794435977936, acc: 0.977011501789093)
[2025-02-13 19:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13][root][INFO] - Training Epoch: 1/2, step 1875/7134 completed (loss: 0.11886007338762283, acc: 0.9732142686843872)
[2025-02-13 19:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14][root][INFO] - Training Epoch: 1/2, step 1876/7134 completed (loss: 0.5729274749755859, acc: 0.8571428656578064)
[2025-02-13 19:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14][root][INFO] - Training Epoch: 1/2, step 1877/7134 completed (loss: 0.2223803699016571, acc: 0.9491525292396545)
[2025-02-13 19:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14][root][INFO] - Training Epoch: 1/2, step 1878/7134 completed (loss: 0.20214976370334625, acc: 0.9281045794487)
[2025-02-13 19:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15][root][INFO] - Training Epoch: 1/2, step 1879/7134 completed (loss: 0.7804937958717346, acc: 0.8271604776382446)
[2025-02-13 19:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15][root][INFO] - Training Epoch: 1/2, step 1880/7134 completed (loss: 0.5527124404907227, acc: 0.8943089246749878)
[2025-02-13 19:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15][root][INFO] - Training Epoch: 1/2, step 1881/7134 completed (loss: 0.5733681321144104, acc: 0.8380281925201416)
[2025-02-13 19:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16][root][INFO] - Training Epoch: 1/2, step 1882/7134 completed (loss: 0.5467978119850159, acc: 0.8804348111152649)
[2025-02-13 19:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16][root][INFO] - Training Epoch: 1/2, step 1883/7134 completed (loss: 0.49279704689979553, acc: 0.8449612259864807)
[2025-02-13 19:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17][root][INFO] - Training Epoch: 1/2, step 1884/7134 completed (loss: 0.4800392985343933, acc: 0.8648648858070374)
[2025-02-13 19:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17][root][INFO] - Training Epoch: 1/2, step 1885/7134 completed (loss: 0.6281322836875916, acc: 0.8591549396514893)
[2025-02-13 19:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17][root][INFO] - Training Epoch: 1/2, step 1886/7134 completed (loss: 0.5300130248069763, acc: 0.8809523582458496)
[2025-02-13 19:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18][root][INFO] - Training Epoch: 1/2, step 1887/7134 completed (loss: 0.37385687232017517, acc: 0.9139072895050049)
[2025-02-13 19:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18][root][INFO] - Training Epoch: 1/2, step 1888/7134 completed (loss: 0.3157050609588623, acc: 0.94017094373703)
[2025-02-13 19:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18][root][INFO] - Training Epoch: 1/2, step 1889/7134 completed (loss: 0.24460144340991974, acc: 0.9473684430122375)
[2025-02-13 19:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19][root][INFO] - Training Epoch: 1/2, step 1890/7134 completed (loss: 0.14922496676445007, acc: 0.9536423683166504)
[2025-02-13 19:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19][root][INFO] - Training Epoch: 1/2, step 1891/7134 completed (loss: 0.47604405879974365, acc: 0.8720930218696594)
[2025-02-13 19:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19][root][INFO] - Training Epoch: 1/2, step 1892/7134 completed (loss: 0.3373123109340668, acc: 0.9041916131973267)
[2025-02-13 19:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:20][root][INFO] - Training Epoch: 1/2, step 1893/7134 completed (loss: 0.30841192603111267, acc: 0.9135802388191223)
[2025-02-13 19:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:20][root][INFO] - Training Epoch: 1/2, step 1894/7134 completed (loss: 0.3757692575454712, acc: 0.912162184715271)
[2025-02-13 19:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21][root][INFO] - Training Epoch: 1/2, step 1895/7134 completed (loss: 1.1888240575790405, acc: 0.7415730357170105)
[2025-02-13 19:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21][root][INFO] - Training Epoch: 1/2, step 1896/7134 completed (loss: 0.39203667640686035, acc: 0.9272727370262146)
[2025-02-13 19:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21][root][INFO] - Training Epoch: 1/2, step 1897/7134 completed (loss: 0.3554418981075287, acc: 0.9366196990013123)
[2025-02-13 19:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22][root][INFO] - Training Epoch: 1/2, step 1898/7134 completed (loss: 0.32606467604637146, acc: 0.9103448390960693)
[2025-02-13 19:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22][root][INFO] - Training Epoch: 1/2, step 1899/7134 completed (loss: 0.4172334372997284, acc: 0.9015151262283325)
[2025-02-13 19:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22][root][INFO] - Training Epoch: 1/2, step 1900/7134 completed (loss: 0.16146495938301086, acc: 0.9485294222831726)
[2025-02-13 19:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23][root][INFO] - Training Epoch: 1/2, step 1901/7134 completed (loss: 0.36258724331855774, acc: 0.935251772403717)
[2025-02-13 19:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23][root][INFO] - Training Epoch: 1/2, step 1902/7134 completed (loss: 0.1744529902935028, acc: 0.9640287756919861)
[2025-02-13 19:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23][root][INFO] - Training Epoch: 1/2, step 1903/7134 completed (loss: 0.11899010837078094, acc: 0.9803921580314636)
[2025-02-13 19:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:24][root][INFO] - Training Epoch: 1/2, step 1904/7134 completed (loss: 0.2992369830608368, acc: 0.9238095283508301)
[2025-02-13 19:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:24][root][INFO] - Training Epoch: 1/2, step 1905/7134 completed (loss: 0.258003294467926, acc: 0.9503546357154846)
[2025-02-13 19:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25][root][INFO] - Training Epoch: 1/2, step 1906/7134 completed (loss: 0.22487227618694305, acc: 0.970802903175354)
[2025-02-13 19:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25][root][INFO] - Training Epoch: 1/2, step 1907/7134 completed (loss: 0.2800794243812561, acc: 0.9115646481513977)
[2025-02-13 19:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25][root][INFO] - Training Epoch: 1/2, step 1908/7134 completed (loss: 0.29785963892936707, acc: 0.9252336621284485)
[2025-02-13 19:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26][root][INFO] - Training Epoch: 1/2, step 1909/7134 completed (loss: 0.3274246156215668, acc: 0.9234693646430969)
[2025-02-13 19:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26][root][INFO] - Training Epoch: 1/2, step 1910/7134 completed (loss: 0.18494470417499542, acc: 0.9674418568611145)
[2025-02-13 19:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26][root][INFO] - Training Epoch: 1/2, step 1911/7134 completed (loss: 0.1756831258535385, acc: 0.9644669890403748)
[2025-02-13 19:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:27][root][INFO] - Training Epoch: 1/2, step 1912/7134 completed (loss: 0.07329227775335312, acc: 0.9818181991577148)
[2025-02-13 19:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:27][root][INFO] - Training Epoch: 1/2, step 1913/7134 completed (loss: 0.29316261410713196, acc: 0.9448819160461426)
[2025-02-13 19:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28][root][INFO] - Training Epoch: 1/2, step 1914/7134 completed (loss: 0.3170587122440338, acc: 0.9398906826972961)
[2025-02-13 19:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28][root][INFO] - Training Epoch: 1/2, step 1915/7134 completed (loss: 0.14540445804595947, acc: 0.9720279574394226)
[2025-02-13 19:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28][root][INFO] - Training Epoch: 1/2, step 1916/7134 completed (loss: 0.19062237441539764, acc: 0.9560439586639404)
[2025-02-13 19:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29][root][INFO] - Training Epoch: 1/2, step 1917/7134 completed (loss: 0.2103804349899292, acc: 0.9575757384300232)
[2025-02-13 19:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29][root][INFO] - Training Epoch: 1/2, step 1918/7134 completed (loss: 0.23341773450374603, acc: 0.9735099077224731)
[2025-02-13 19:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29][root][INFO] - Training Epoch: 1/2, step 1919/7134 completed (loss: 0.17236372828483582, acc: 0.9634146094322205)
[2025-02-13 19:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30][root][INFO] - Training Epoch: 1/2, step 1920/7134 completed (loss: 0.20292718708515167, acc: 0.9387755393981934)
[2025-02-13 19:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30][root][INFO] - Training Epoch: 1/2, step 1921/7134 completed (loss: 0.2056075632572174, acc: 0.9532163739204407)
[2025-02-13 19:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30][root][INFO] - Training Epoch: 1/2, step 1922/7134 completed (loss: 0.08259914070367813, acc: 0.9795082211494446)
[2025-02-13 19:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31][root][INFO] - Training Epoch: 1/2, step 1923/7134 completed (loss: 0.15199649333953857, acc: 0.9440559148788452)
[2025-02-13 19:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31][root][INFO] - Training Epoch: 1/2, step 1924/7134 completed (loss: 0.3124876916408539, acc: 0.921875)
[2025-02-13 19:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32][root][INFO] - Training Epoch: 1/2, step 1925/7134 completed (loss: 0.32284435629844666, acc: 0.9121338725090027)
[2025-02-13 19:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32][root][INFO] - Training Epoch: 1/2, step 1926/7134 completed (loss: 0.09877807646989822, acc: 0.9776785969734192)
[2025-02-13 19:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32][root][INFO] - Training Epoch: 1/2, step 1927/7134 completed (loss: 0.21820491552352905, acc: 0.9669811129570007)
[2025-02-13 19:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33][root][INFO] - Training Epoch: 1/2, step 1928/7134 completed (loss: 0.33000460267066956, acc: 0.9047619104385376)
[2025-02-13 19:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33][root][INFO] - Training Epoch: 1/2, step 1929/7134 completed (loss: 0.15690694749355316, acc: 0.9441340565681458)
[2025-02-13 19:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33][root][INFO] - Training Epoch: 1/2, step 1930/7134 completed (loss: 0.4636530876159668, acc: 0.8792270421981812)
[2025-02-13 19:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34][root][INFO] - Training Epoch: 1/2, step 1931/7134 completed (loss: 0.36399170756340027, acc: 0.9120370149612427)
[2025-02-13 19:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34][root][INFO] - Training Epoch: 1/2, step 1932/7134 completed (loss: 0.35425207018852234, acc: 0.9050279259681702)
[2025-02-13 19:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34][root][INFO] - Training Epoch: 1/2, step 1933/7134 completed (loss: 0.43973737955093384, acc: 0.8844221234321594)
[2025-02-13 19:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35][root][INFO] - Training Epoch: 1/2, step 1934/7134 completed (loss: 0.19189301133155823, acc: 0.9428571462631226)
[2025-02-13 19:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35][root][INFO] - Training Epoch: 1/2, step 1935/7134 completed (loss: 0.44355863332748413, acc: 0.8963414430618286)
[2025-02-13 19:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36][root][INFO] - Training Epoch: 1/2, step 1936/7134 completed (loss: 0.1425478607416153, acc: 0.9738562107086182)
[2025-02-13 19:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36][root][INFO] - Training Epoch: 1/2, step 1937/7134 completed (loss: 0.24128395318984985, acc: 0.9268292784690857)
[2025-02-13 19:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36][root][INFO] - Training Epoch: 1/2, step 1938/7134 completed (loss: 0.5446568727493286, acc: 0.8695651888847351)
[2025-02-13 19:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37][root][INFO] - Training Epoch: 1/2, step 1939/7134 completed (loss: 0.2582278251647949, acc: 0.9396551847457886)
[2025-02-13 19:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37][root][INFO] - Training Epoch: 1/2, step 1940/7134 completed (loss: 0.5443782210350037, acc: 0.8802395462989807)
[2025-02-13 19:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37][root][INFO] - Training Epoch: 1/2, step 1941/7134 completed (loss: 0.4657493531703949, acc: 0.8639053106307983)
[2025-02-13 19:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38][root][INFO] - Training Epoch: 1/2, step 1942/7134 completed (loss: 0.2445225715637207, acc: 0.9351851940155029)
[2025-02-13 19:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38][root][INFO] - Training Epoch: 1/2, step 1943/7134 completed (loss: 0.47391730546951294, acc: 0.8947368264198303)
[2025-02-13 19:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38][root][INFO] - Training Epoch: 1/2, step 1944/7134 completed (loss: 0.27223777770996094, acc: 0.9262295365333557)
[2025-02-13 19:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39][root][INFO] - Training Epoch: 1/2, step 1945/7134 completed (loss: 0.309080570936203, acc: 0.9274193644523621)
[2025-02-13 19:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39][root][INFO] - Training Epoch: 1/2, step 1946/7134 completed (loss: 0.4322127103805542, acc: 0.8999999761581421)
[2025-02-13 19:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39][root][INFO] - Training Epoch: 1/2, step 1947/7134 completed (loss: 0.34002482891082764, acc: 0.913294792175293)
[2025-02-13 19:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:40][root][INFO] - Training Epoch: 1/2, step 1948/7134 completed (loss: 0.2195296287536621, acc: 0.9384615421295166)
[2025-02-13 19:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:40][root][INFO] - Training Epoch: 1/2, step 1949/7134 completed (loss: 0.19891126453876495, acc: 0.9350649118423462)
[2025-02-13 19:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41][root][INFO] - Training Epoch: 1/2, step 1950/7134 completed (loss: 0.2705404758453369, acc: 0.9347826242446899)
[2025-02-13 19:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41][root][INFO] - Training Epoch: 1/2, step 1951/7134 completed (loss: 0.2767309546470642, acc: 0.932330846786499)
[2025-02-13 19:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41][root][INFO] - Training Epoch: 1/2, step 1952/7134 completed (loss: 0.33868056535720825, acc: 0.9096385836601257)
[2025-02-13 19:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42][root][INFO] - Training Epoch: 1/2, step 1953/7134 completed (loss: 0.21656487882137299, acc: 0.9556962251663208)
[2025-02-13 19:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42][root][INFO] - Training Epoch: 1/2, step 1954/7134 completed (loss: 0.13045966625213623, acc: 0.9509803652763367)
[2025-02-13 19:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42][root][INFO] - Training Epoch: 1/2, step 1955/7134 completed (loss: 0.24043633043766022, acc: 0.9477611780166626)
[2025-02-13 19:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43][root][INFO] - Training Epoch: 1/2, step 1956/7134 completed (loss: 0.12957052886486053, acc: 0.989130437374115)
[2025-02-13 19:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43][root][INFO] - Training Epoch: 1/2, step 1957/7134 completed (loss: 0.2712893784046173, acc: 0.932330846786499)
[2025-02-13 19:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43][root][INFO] - Training Epoch: 1/2, step 1958/7134 completed (loss: 0.2869897186756134, acc: 0.9222221970558167)
[2025-02-13 19:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44][root][INFO] - Training Epoch: 1/2, step 1959/7134 completed (loss: 0.3387351632118225, acc: 0.9196428656578064)
[2025-02-13 19:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44][root][INFO] - Training Epoch: 1/2, step 1960/7134 completed (loss: 0.16968479752540588, acc: 0.9666666388511658)
[2025-02-13 19:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44][root][INFO] - Training Epoch: 1/2, step 1961/7134 completed (loss: 0.20840616524219513, acc: 0.949367105960846)
[2025-02-13 19:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:45][root][INFO] - Training Epoch: 1/2, step 1962/7134 completed (loss: 0.1615390181541443, acc: 0.9425287246704102)
[2025-02-13 19:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:45][root][INFO] - Training Epoch: 1/2, step 1963/7134 completed (loss: 0.3948360085487366, acc: 0.9072847962379456)
[2025-02-13 19:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46][root][INFO] - Training Epoch: 1/2, step 1964/7134 completed (loss: 0.36105018854141235, acc: 0.9191918969154358)
[2025-02-13 19:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46][root][INFO] - Training Epoch: 1/2, step 1965/7134 completed (loss: 0.38929852843284607, acc: 0.9044944047927856)
[2025-02-13 19:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46][root][INFO] - Training Epoch: 1/2, step 1966/7134 completed (loss: 0.37798401713371277, acc: 0.9235668778419495)
[2025-02-13 19:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47][root][INFO] - Training Epoch: 1/2, step 1967/7134 completed (loss: 0.2669046223163605, acc: 0.9210526347160339)
[2025-02-13 19:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47][root][INFO] - Training Epoch: 1/2, step 1968/7134 completed (loss: 0.28287073969841003, acc: 0.9202127456665039)
[2025-02-13 19:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48][root][INFO] - Training Epoch: 1/2, step 1969/7134 completed (loss: 0.20999695360660553, acc: 0.9435028433799744)
[2025-02-13 19:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48][root][INFO] - Training Epoch: 1/2, step 1970/7134 completed (loss: 0.21194703876972198, acc: 0.9644970297813416)
[2025-02-13 19:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48][root][INFO] - Training Epoch: 1/2, step 1971/7134 completed (loss: 0.3820778727531433, acc: 0.9162303805351257)
[2025-02-13 19:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49][root][INFO] - Training Epoch: 1/2, step 1972/7134 completed (loss: 0.8486520648002625, acc: 0.7894737124443054)
[2025-02-13 19:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49][root][INFO] - Training Epoch: 1/2, step 1973/7134 completed (loss: 1.0271422863006592, acc: 0.8472222089767456)
[2025-02-13 19:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49][root][INFO] - Training Epoch: 1/2, step 1974/7134 completed (loss: 0.2243676334619522, acc: 0.9587628841400146)
[2025-02-13 19:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50][root][INFO] - Training Epoch: 1/2, step 1975/7134 completed (loss: 0.14338019490242004, acc: 0.949999988079071)
[2025-02-13 19:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50][root][INFO] - Training Epoch: 1/2, step 1976/7134 completed (loss: 0.3800812065601349, acc: 0.9086538553237915)
[2025-02-13 19:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50][root][INFO] - Training Epoch: 1/2, step 1977/7134 completed (loss: 0.2830345332622528, acc: 0.946107804775238)
[2025-02-13 19:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:51][root][INFO] - Training Epoch: 1/2, step 1978/7134 completed (loss: 0.15158356726169586, acc: 0.9825581312179565)
[2025-02-13 19:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:51][root][INFO] - Training Epoch: 1/2, step 1979/7134 completed (loss: 0.1893533319234848, acc: 0.9560439586639404)
[2025-02-13 19:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52][root][INFO] - Training Epoch: 1/2, step 1980/7134 completed (loss: 0.1916304975748062, acc: 0.963350772857666)
[2025-02-13 19:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52][root][INFO] - Training Epoch: 1/2, step 1981/7134 completed (loss: 0.14221924543380737, acc: 0.9725274443626404)
[2025-02-13 19:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52][root][INFO] - Training Epoch: 1/2, step 1982/7134 completed (loss: 0.1788448691368103, acc: 0.9435897469520569)
[2025-02-13 19:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53][root][INFO] - Training Epoch: 1/2, step 1983/7134 completed (loss: 0.16397905349731445, acc: 0.9784946441650391)
[2025-02-13 19:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53][root][INFO] - Training Epoch: 1/2, step 1984/7134 completed (loss: 0.15069547295570374, acc: 0.9709302186965942)
[2025-02-13 19:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53][root][INFO] - Training Epoch: 1/2, step 1985/7134 completed (loss: 0.3062620460987091, acc: 0.9192546606063843)
[2025-02-13 19:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54][root][INFO] - Training Epoch: 1/2, step 1986/7134 completed (loss: 0.10300854593515396, acc: 0.9664804339408875)
[2025-02-13 19:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54][root][INFO] - Training Epoch: 1/2, step 1987/7134 completed (loss: 0.21615801751613617, acc: 0.9550561904907227)
[2025-02-13 19:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54][root][INFO] - Training Epoch: 1/2, step 1988/7134 completed (loss: 0.19948925077915192, acc: 0.9636363387107849)
[2025-02-13 19:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:55][root][INFO] - Training Epoch: 1/2, step 1989/7134 completed (loss: 0.348177045583725, acc: 0.9178082346916199)
[2025-02-13 19:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:55][root][INFO] - Training Epoch: 1/2, step 1990/7134 completed (loss: 0.21493057906627655, acc: 0.950276255607605)
[2025-02-13 19:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56][root][INFO] - Training Epoch: 1/2, step 1991/7134 completed (loss: 0.13513001799583435, acc: 0.9677419066429138)
[2025-02-13 19:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56][root][INFO] - Training Epoch: 1/2, step 1992/7134 completed (loss: 0.4277561902999878, acc: 0.8840579986572266)
[2025-02-13 19:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56][root][INFO] - Training Epoch: 1/2, step 1993/7134 completed (loss: 0.3048306107521057, acc: 0.9217391014099121)
[2025-02-13 19:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57][root][INFO] - Training Epoch: 1/2, step 1994/7134 completed (loss: 0.23541602492332458, acc: 0.9224806427955627)
[2025-02-13 19:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57][root][INFO] - Training Epoch: 1/2, step 1995/7134 completed (loss: 0.09808586537837982, acc: 0.9862068891525269)
[2025-02-13 19:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57][root][INFO] - Training Epoch: 1/2, step 1996/7134 completed (loss: 0.22844289243221283, acc: 0.9489051103591919)
[2025-02-13 19:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58][root][INFO] - Training Epoch: 1/2, step 1997/7134 completed (loss: 0.21026667952537537, acc: 0.949999988079071)
[2025-02-13 19:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58][root][INFO] - Training Epoch: 1/2, step 1998/7134 completed (loss: 0.19455356895923615, acc: 0.9436619877815247)
[2025-02-13 19:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58][root][INFO] - Training Epoch: 1/2, step 1999/7134 completed (loss: 0.23250971734523773, acc: 0.9370629191398621)
[2025-02-13 19:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:59][root][INFO] - Training Epoch: 1/2, step 2000/7134 completed (loss: 0.17503748834133148, acc: 0.9722222089767456)
[2025-02-13 19:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:59][root][INFO] - Training Epoch: 1/2, step 2001/7134 completed (loss: 0.12715627253055573, acc: 0.9793103337287903)
[2025-02-13 19:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00][root][INFO] - Training Epoch: 1/2, step 2002/7134 completed (loss: 0.1932455599308014, acc: 0.9624060392379761)
[2025-02-13 19:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00][root][INFO] - Training Epoch: 1/2, step 2003/7134 completed (loss: 0.16255055367946625, acc: 0.9824561476707458)
[2025-02-13 19:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00][root][INFO] - Training Epoch: 1/2, step 2004/7134 completed (loss: 0.9622665643692017, acc: 0.8524590134620667)
[2025-02-13 19:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01][root][INFO] - Training Epoch: 1/2, step 2005/7134 completed (loss: 0.5688861608505249, acc: 0.8877550959587097)
[2025-02-13 19:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01][root][INFO] - Training Epoch: 1/2, step 2006/7134 completed (loss: 0.11589865386486053, acc: 0.9767441749572754)
[2025-02-13 19:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01][root][INFO] - Training Epoch: 1/2, step 2007/7134 completed (loss: 0.09210804849863052, acc: 0.9747899174690247)
[2025-02-13 19:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02][root][INFO] - Training Epoch: 1/2, step 2008/7134 completed (loss: 0.19272802770137787, acc: 0.9520547986030579)
[2025-02-13 19:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02][root][INFO] - Training Epoch: 1/2, step 2009/7134 completed (loss: 0.14786109328269958, acc: 0.970370352268219)
[2025-02-13 19:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03][root][INFO] - Training Epoch: 1/2, step 2010/7134 completed (loss: 0.17520637810230255, acc: 0.9489051103591919)
[2025-02-13 19:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03][root][INFO] - Training Epoch: 1/2, step 2011/7134 completed (loss: 0.09793577343225479, acc: 0.9841269850730896)
[2025-02-13 19:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03][root][INFO] - Training Epoch: 1/2, step 2012/7134 completed (loss: 0.2077280431985855, acc: 0.9576271176338196)
[2025-02-13 19:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04][root][INFO] - Training Epoch: 1/2, step 2013/7134 completed (loss: 0.15338054299354553, acc: 0.9754098653793335)
[2025-02-13 19:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04][root][INFO] - Training Epoch: 1/2, step 2014/7134 completed (loss: 0.14074158668518066, acc: 0.9567901492118835)
[2025-02-13 19:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04][root][INFO] - Training Epoch: 1/2, step 2015/7134 completed (loss: 0.08072246611118317, acc: 0.987500011920929)
[2025-02-13 19:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05][root][INFO] - Training Epoch: 1/2, step 2016/7134 completed (loss: 0.289309561252594, acc: 0.9319728016853333)
[2025-02-13 19:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05][root][INFO] - Training Epoch: 1/2, step 2017/7134 completed (loss: 0.3814297318458557, acc: 0.9072847962379456)
[2025-02-13 19:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05][root][INFO] - Training Epoch: 1/2, step 2018/7134 completed (loss: 0.07836876809597015, acc: 0.9794520735740662)
[2025-02-13 19:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06][root][INFO] - Training Epoch: 1/2, step 2019/7134 completed (loss: 0.20059964060783386, acc: 0.940397322177887)
[2025-02-13 19:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06][root][INFO] - Training Epoch: 1/2, step 2020/7134 completed (loss: 0.17916086316108704, acc: 0.9504132270812988)
[2025-02-13 19:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07][root][INFO] - Training Epoch: 1/2, step 2021/7134 completed (loss: 0.2728765308856964, acc: 0.9405405521392822)
[2025-02-13 19:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07][root][INFO] - Training Epoch: 1/2, step 2022/7134 completed (loss: 0.253749281167984, acc: 0.9477611780166626)
[2025-02-13 19:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07][root][INFO] - Training Epoch: 1/2, step 2023/7134 completed (loss: 0.18781089782714844, acc: 0.9447852969169617)
[2025-02-13 19:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08][root][INFO] - Training Epoch: 1/2, step 2024/7134 completed (loss: 0.17178577184677124, acc: 0.9593023061752319)
[2025-02-13 19:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08][root][INFO] - Training Epoch: 1/2, step 2025/7134 completed (loss: 0.23225197196006775, acc: 0.9707602262496948)
[2025-02-13 19:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08][root][INFO] - Training Epoch: 1/2, step 2026/7134 completed (loss: 0.2118247002363205, acc: 0.9298245906829834)
[2025-02-13 19:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09][root][INFO] - Training Epoch: 1/2, step 2027/7134 completed (loss: 0.15011271834373474, acc: 0.9473684430122375)
[2025-02-13 19:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09][root][INFO] - Training Epoch: 1/2, step 2028/7134 completed (loss: 0.19710254669189453, acc: 0.9509202241897583)
[2025-02-13 19:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10][root][INFO] - Training Epoch: 1/2, step 2029/7134 completed (loss: 0.2568104565143585, acc: 0.929411768913269)
[2025-02-13 19:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10][root][INFO] - Training Epoch: 1/2, step 2030/7134 completed (loss: 0.25280001759529114, acc: 0.9397590160369873)
[2025-02-13 19:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10][root][INFO] - Training Epoch: 1/2, step 2031/7134 completed (loss: 0.16340740025043488, acc: 0.9668508172035217)
[2025-02-13 19:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11][root][INFO] - Training Epoch: 1/2, step 2032/7134 completed (loss: 0.3554377257823944, acc: 0.9005848169326782)
[2025-02-13 19:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11][root][INFO] - Training Epoch: 1/2, step 2033/7134 completed (loss: 0.13865108788013458, acc: 0.9541984796524048)
[2025-02-13 19:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11][root][INFO] - Training Epoch: 1/2, step 2034/7134 completed (loss: 0.4096568822860718, acc: 0.9060402512550354)
[2025-02-13 19:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12][root][INFO] - Training Epoch: 1/2, step 2035/7134 completed (loss: 0.1267661452293396, acc: 0.9731183052062988)
[2025-02-13 19:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12][root][INFO] - Training Epoch: 1/2, step 2036/7134 completed (loss: 0.24672973155975342, acc: 0.9465649127960205)
[2025-02-13 19:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13][root][INFO] - Training Epoch: 1/2, step 2037/7134 completed (loss: 0.17203396558761597, acc: 0.9570552110671997)
[2025-02-13 19:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13][root][INFO] - Training Epoch: 1/2, step 2038/7134 completed (loss: 0.19032374024391174, acc: 0.9505494236946106)
[2025-02-13 19:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13][root][INFO] - Training Epoch: 1/2, step 2039/7134 completed (loss: 0.12165451049804688, acc: 0.9635036587715149)
[2025-02-13 19:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14][root][INFO] - Training Epoch: 1/2, step 2040/7134 completed (loss: 0.25964245200157166, acc: 0.9741379022598267)
[2025-02-13 19:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14][root][INFO] - Training Epoch: 1/2, step 2041/7134 completed (loss: 0.15139304101467133, acc: 0.9636363387107849)
[2025-02-13 19:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14][root][INFO] - Training Epoch: 1/2, step 2042/7134 completed (loss: 0.28616058826446533, acc: 0.9491525292396545)
[2025-02-13 19:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15][root][INFO] - Training Epoch: 1/2, step 2043/7134 completed (loss: 0.09346627444028854, acc: 0.9798657894134521)
[2025-02-13 19:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15][root][INFO] - Training Epoch: 1/2, step 2044/7134 completed (loss: 0.14814305305480957, acc: 0.9599999785423279)
[2025-02-13 19:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16][root][INFO] - Training Epoch: 1/2, step 2045/7134 completed (loss: 0.27944278717041016, acc: 0.8999999761581421)
[2025-02-13 19:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16][root][INFO] - Training Epoch: 1/2, step 2046/7134 completed (loss: 0.3828715980052948, acc: 0.9159663915634155)
[2025-02-13 19:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16][root][INFO] - Training Epoch: 1/2, step 2047/7134 completed (loss: 0.34182846546173096, acc: 0.9172413945198059)
[2025-02-13 19:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17][root][INFO] - Training Epoch: 1/2, step 2048/7134 completed (loss: 0.41171956062316895, acc: 0.9107142686843872)
[2025-02-13 19:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17][root][INFO] - Training Epoch: 1/2, step 2049/7134 completed (loss: 0.36641156673431396, acc: 0.918181836605072)
[2025-02-13 19:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17][root][INFO] - Training Epoch: 1/2, step 2050/7134 completed (loss: 0.2962181270122528, acc: 0.9041095972061157)
[2025-02-13 19:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18][root][INFO] - Training Epoch: 1/2, step 2051/7134 completed (loss: 0.2875162661075592, acc: 0.9203540086746216)
[2025-02-13 19:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18][root][INFO] - Training Epoch: 1/2, step 2052/7134 completed (loss: 0.42309442162513733, acc: 0.9166666865348816)
[2025-02-13 19:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18][root][INFO] - Training Epoch: 1/2, step 2053/7134 completed (loss: 0.47811827063560486, acc: 0.9007633328437805)
[2025-02-13 19:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19][root][INFO] - Training Epoch: 1/2, step 2054/7134 completed (loss: 0.4439668357372284, acc: 0.9102563858032227)
[2025-02-13 19:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19][root][INFO] - Training Epoch: 1/2, step 2055/7134 completed (loss: 0.35249724984169006, acc: 0.9178082346916199)
[2025-02-13 19:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20][root][INFO] - Training Epoch: 1/2, step 2056/7134 completed (loss: 0.4268341362476349, acc: 0.909604549407959)
[2025-02-13 19:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20][root][INFO] - Training Epoch: 1/2, step 2057/7134 completed (loss: 0.35244059562683105, acc: 0.9044944047927856)
[2025-02-13 19:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20][root][INFO] - Training Epoch: 1/2, step 2058/7134 completed (loss: 0.48768118023872375, acc: 0.884393036365509)
[2025-02-13 19:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21][root][INFO] - Training Epoch: 1/2, step 2059/7134 completed (loss: 0.7581766247749329, acc: 0.8368794322013855)
[2025-02-13 19:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21][root][INFO] - Training Epoch: 1/2, step 2060/7134 completed (loss: 0.2527485191822052, acc: 0.9363057613372803)
[2025-02-13 19:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21][root][INFO] - Training Epoch: 1/2, step 2061/7134 completed (loss: 0.46172064542770386, acc: 0.898809552192688)
[2025-02-13 19:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22][root][INFO] - Training Epoch: 1/2, step 2062/7134 completed (loss: 0.4294307827949524, acc: 0.8984375)
[2025-02-13 19:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22][root][INFO] - Training Epoch: 1/2, step 2063/7134 completed (loss: 0.2389005571603775, acc: 0.9290780425071716)
[2025-02-13 19:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22][root][INFO] - Training Epoch: 1/2, step 2064/7134 completed (loss: 0.20674404501914978, acc: 0.9632353186607361)
[2025-02-13 19:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:23][root][INFO] - Training Epoch: 1/2, step 2065/7134 completed (loss: 0.2526407837867737, acc: 0.9571428298950195)
[2025-02-13 19:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:23][root][INFO] - Training Epoch: 1/2, step 2066/7134 completed (loss: 0.19533106684684753, acc: 0.9512194991111755)
[2025-02-13 19:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24][root][INFO] - Training Epoch: 1/2, step 2067/7134 completed (loss: 0.22679643332958221, acc: 0.925000011920929)
[2025-02-13 19:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24][root][INFO] - Training Epoch: 1/2, step 2068/7134 completed (loss: 0.20953194797039032, acc: 0.9509803652763367)
[2025-02-13 19:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24][root][INFO] - Training Epoch: 1/2, step 2069/7134 completed (loss: 0.1861310601234436, acc: 0.9597315192222595)
[2025-02-13 19:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25][root][INFO] - Training Epoch: 1/2, step 2070/7134 completed (loss: 0.1370108723640442, acc: 0.9396551847457886)
[2025-02-13 19:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25][root][INFO] - Training Epoch: 1/2, step 2071/7134 completed (loss: 0.13843636214733124, acc: 0.9611650705337524)
[2025-02-13 19:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25][root][INFO] - Training Epoch: 1/2, step 2072/7134 completed (loss: 0.18683156371116638, acc: 0.9509803652763367)
[2025-02-13 19:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:26][root][INFO] - Training Epoch: 1/2, step 2073/7134 completed (loss: 0.16949523985385895, acc: 0.9669421315193176)
[2025-02-13 19:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:26][root][INFO] - Training Epoch: 1/2, step 2074/7134 completed (loss: 0.23685409128665924, acc: 0.963350772857666)
[2025-02-13 19:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27][root][INFO] - Training Epoch: 1/2, step 2075/7134 completed (loss: 0.4820162355899811, acc: 0.9182389974594116)
[2025-02-13 19:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27][root][INFO] - Training Epoch: 1/2, step 2076/7134 completed (loss: 0.18675445020198822, acc: 0.9659863710403442)
[2025-02-13 19:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27][root][INFO] - Training Epoch: 1/2, step 2077/7134 completed (loss: 0.2612268626689911, acc: 0.9435028433799744)
[2025-02-13 19:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28][root][INFO] - Training Epoch: 1/2, step 2078/7134 completed (loss: 0.2114550918340683, acc: 0.9543147087097168)
[2025-02-13 19:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28][root][INFO] - Training Epoch: 1/2, step 2079/7134 completed (loss: 0.11825108528137207, acc: 0.9638554453849792)
[2025-02-13 19:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28][root][INFO] - Training Epoch: 1/2, step 2080/7134 completed (loss: 0.20963646471500397, acc: 0.9607843160629272)
[2025-02-13 19:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29][root][INFO] - Training Epoch: 1/2, step 2081/7134 completed (loss: 0.11816371977329254, acc: 0.9745222926139832)
[2025-02-13 19:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29][root][INFO] - Training Epoch: 1/2, step 2082/7134 completed (loss: 0.2544853985309601, acc: 0.9406779408454895)
[2025-02-13 19:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29][root][INFO] - Training Epoch: 1/2, step 2083/7134 completed (loss: 0.15856124460697174, acc: 0.9767441749572754)
[2025-02-13 19:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30][root][INFO] - Training Epoch: 1/2, step 2084/7134 completed (loss: 0.18083813786506653, acc: 0.9666666388511658)
[2025-02-13 19:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30][root][INFO] - Training Epoch: 1/2, step 2085/7134 completed (loss: 0.07510851323604584, acc: 0.9918699264526367)
[2025-02-13 19:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31][root][INFO] - Training Epoch: 1/2, step 2086/7134 completed (loss: 0.11362423747777939, acc: 0.9901960492134094)
[2025-02-13 19:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31][root][INFO] - Training Epoch: 1/2, step 2087/7134 completed (loss: 0.08928941190242767, acc: 0.988095223903656)
[2025-02-13 19:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31][root][INFO] - Training Epoch: 1/2, step 2088/7134 completed (loss: 0.34665510058403015, acc: 0.9526627063751221)
[2025-02-13 19:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32][root][INFO] - Training Epoch: 1/2, step 2089/7134 completed (loss: 0.32434171438217163, acc: 0.9285714030265808)
[2025-02-13 19:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32][root][INFO] - Training Epoch: 1/2, step 2090/7134 completed (loss: 0.09776994585990906, acc: 0.9738562107086182)
[2025-02-13 19:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32][root][INFO] - Training Epoch: 1/2, step 2091/7134 completed (loss: 0.06352178752422333, acc: 0.9862068891525269)
[2025-02-13 19:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33][root][INFO] - Training Epoch: 1/2, step 2092/7134 completed (loss: 0.08168202638626099, acc: 0.9735099077224731)
[2025-02-13 19:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33][root][INFO] - Training Epoch: 1/2, step 2093/7134 completed (loss: 0.17939937114715576, acc: 0.9526315927505493)
[2025-02-13 19:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34][root][INFO] - Training Epoch: 1/2, step 2094/7134 completed (loss: 0.3632236421108246, acc: 0.9328358173370361)
[2025-02-13 19:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34][root][INFO] - Training Epoch: 1/2, step 2095/7134 completed (loss: 0.2538367807865143, acc: 0.93034827709198)
[2025-02-13 19:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34][root][INFO] - Training Epoch: 1/2, step 2096/7134 completed (loss: 0.19033659994602203, acc: 0.9440559148788452)
[2025-02-13 19:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35][root][INFO] - Training Epoch: 1/2, step 2097/7134 completed (loss: 0.12617599964141846, acc: 0.9698795080184937)
[2025-02-13 19:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35][root][INFO] - Training Epoch: 1/2, step 2098/7134 completed (loss: 0.23299917578697205, acc: 0.9473684430122375)
[2025-02-13 19:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35][root][INFO] - Training Epoch: 1/2, step 2099/7134 completed (loss: 0.1753247082233429, acc: 0.953125)
[2025-02-13 19:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36][root][INFO] - Training Epoch: 1/2, step 2100/7134 completed (loss: 0.26429101824760437, acc: 0.9428571462631226)
[2025-02-13 19:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36][root][INFO] - Training Epoch: 1/2, step 2101/7134 completed (loss: 0.28713634610176086, acc: 0.9285714030265808)
[2025-02-13 19:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36][root][INFO] - Training Epoch: 1/2, step 2102/7134 completed (loss: 0.23389600217342377, acc: 0.9351351261138916)
[2025-02-13 19:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:37][root][INFO] - Training Epoch: 1/2, step 2103/7134 completed (loss: 0.26614129543304443, acc: 0.918749988079071)
[2025-02-13 19:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:37][root][INFO] - Training Epoch: 1/2, step 2104/7134 completed (loss: 0.12199117243289948, acc: 0.9714285731315613)
[2025-02-13 19:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38][root][INFO] - Training Epoch: 1/2, step 2105/7134 completed (loss: 0.16346044838428497, acc: 0.9432623982429504)
[2025-02-13 19:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38][root][INFO] - Training Epoch: 1/2, step 2106/7134 completed (loss: 0.2438245266675949, acc: 0.9342105388641357)
[2025-02-13 19:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38][root][INFO] - Training Epoch: 1/2, step 2107/7134 completed (loss: 0.20467029511928558, acc: 0.950276255607605)
[2025-02-13 19:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39][root][INFO] - Training Epoch: 1/2, step 2108/7134 completed (loss: 0.19310936331748962, acc: 0.9503105878829956)
[2025-02-13 19:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39][root][INFO] - Training Epoch: 1/2, step 2109/7134 completed (loss: 0.3868260979652405, acc: 0.9275362491607666)
[2025-02-13 19:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39][root][INFO] - Training Epoch: 1/2, step 2110/7134 completed (loss: 0.3672979474067688, acc: 0.9041916131973267)
[2025-02-13 19:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40][root][INFO] - Training Epoch: 1/2, step 2111/7134 completed (loss: 0.28330832719802856, acc: 0.9170984625816345)
[2025-02-13 19:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40][root][INFO] - Training Epoch: 1/2, step 2112/7134 completed (loss: 0.1829916536808014, acc: 0.9599999785423279)
[2025-02-13 19:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41][root][INFO] - Training Epoch: 1/2, step 2113/7134 completed (loss: 0.20959803462028503, acc: 0.9599999785423279)
[2025-02-13 19:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41][root][INFO] - Training Epoch: 1/2, step 2114/7134 completed (loss: 0.26991236209869385, acc: 0.9289940595626831)
[2025-02-13 19:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41][root][INFO] - Training Epoch: 1/2, step 2115/7134 completed (loss: 0.29766568541526794, acc: 0.9137930870056152)
[2025-02-13 19:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42][root][INFO] - Training Epoch: 1/2, step 2116/7134 completed (loss: 0.2835799753665924, acc: 0.9455782175064087)
[2025-02-13 19:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42][root][INFO] - Training Epoch: 1/2, step 2117/7134 completed (loss: 0.27289801836013794, acc: 0.9378530979156494)
[2025-02-13 19:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42][root][INFO] - Training Epoch: 1/2, step 2118/7134 completed (loss: 0.1862969547510147, acc: 0.9526627063751221)
[2025-02-13 19:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43][root][INFO] - Training Epoch: 1/2, step 2119/7134 completed (loss: 0.24158303439617157, acc: 0.9364162087440491)
[2025-02-13 19:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43][root][INFO] - Training Epoch: 1/2, step 2120/7134 completed (loss: 0.2324542999267578, acc: 0.9506173133850098)
[2025-02-13 19:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43][root][INFO] - Training Epoch: 1/2, step 2121/7134 completed (loss: 0.35274022817611694, acc: 0.9328858852386475)
[2025-02-13 19:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:44][root][INFO] - Training Epoch: 1/2, step 2122/7134 completed (loss: 0.27420860528945923, acc: 0.9322034120559692)
[2025-02-13 19:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:44][root][INFO] - Training Epoch: 1/2, step 2123/7134 completed (loss: 0.33779165148735046, acc: 0.9276315569877625)
[2025-02-13 19:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45][root][INFO] - Training Epoch: 1/2, step 2124/7134 completed (loss: 0.17593994736671448, acc: 0.9459459185600281)
[2025-02-13 19:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45][root][INFO] - Training Epoch: 1/2, step 2125/7134 completed (loss: 0.1223510205745697, acc: 0.970059871673584)
[2025-02-13 19:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45][root][INFO] - Training Epoch: 1/2, step 2126/7134 completed (loss: 0.3181396722793579, acc: 0.9219858050346375)
[2025-02-13 19:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46][root][INFO] - Training Epoch: 1/2, step 2127/7134 completed (loss: 0.19337581098079681, acc: 0.9397590160369873)
[2025-02-13 19:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46][root][INFO] - Training Epoch: 1/2, step 2128/7134 completed (loss: 0.21796536445617676, acc: 0.949999988079071)
[2025-02-13 19:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46][root][INFO] - Training Epoch: 1/2, step 2129/7134 completed (loss: 0.32393649220466614, acc: 0.9230769276618958)
[2025-02-13 19:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47][root][INFO] - Training Epoch: 1/2, step 2130/7134 completed (loss: 0.33445748686790466, acc: 0.9189189076423645)
[2025-02-13 19:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47][root][INFO] - Training Epoch: 1/2, step 2131/7134 completed (loss: 0.12276247888803482, acc: 0.9693251252174377)
[2025-02-13 19:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47][root][INFO] - Training Epoch: 1/2, step 2132/7134 completed (loss: 0.07924313098192215, acc: 0.9878048896789551)
[2025-02-13 19:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48][root][INFO] - Training Epoch: 1/2, step 2133/7134 completed (loss: 0.18270651996135712, acc: 0.9473684430122375)
[2025-02-13 19:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48][root][INFO] - Training Epoch: 1/2, step 2134/7134 completed (loss: 0.15598580241203308, acc: 0.9655172228813171)
[2025-02-13 19:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49][root][INFO] - Training Epoch: 1/2, step 2135/7134 completed (loss: 0.10626666247844696, acc: 0.9668874144554138)
[2025-02-13 19:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49][root][INFO] - Training Epoch: 1/2, step 2136/7134 completed (loss: 0.09968899190425873, acc: 0.9725274443626404)
[2025-02-13 19:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49][root][INFO] - Training Epoch: 1/2, step 2137/7134 completed (loss: 0.23577646911144257, acc: 0.9414893388748169)
[2025-02-13 19:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50][root][INFO] - Training Epoch: 1/2, step 2138/7134 completed (loss: 0.08120596408843994, acc: 0.976047933101654)
[2025-02-13 19:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50][root][INFO] - Training Epoch: 1/2, step 2139/7134 completed (loss: 0.1630890816450119, acc: 0.9709302186965942)
[2025-02-13 19:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50][root][INFO] - Training Epoch: 1/2, step 2140/7134 completed (loss: 0.42722639441490173, acc: 0.903954803943634)
[2025-02-13 19:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51][root][INFO] - Training Epoch: 1/2, step 2141/7134 completed (loss: 0.36160680651664734, acc: 0.8936170339584351)
[2025-02-13 19:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51][root][INFO] - Training Epoch: 1/2, step 2142/7134 completed (loss: 0.3386339843273163, acc: 0.9209039807319641)
[2025-02-13 19:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51][root][INFO] - Training Epoch: 1/2, step 2143/7134 completed (loss: 0.1345677375793457, acc: 0.9655172228813171)
[2025-02-13 19:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52][root][INFO] - Training Epoch: 1/2, step 2144/7134 completed (loss: 0.12240355461835861, acc: 0.9829545617103577)
[2025-02-13 19:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52][root][INFO] - Training Epoch: 1/2, step 2145/7134 completed (loss: 0.20224176347255707, acc: 0.9709302186965942)
[2025-02-13 19:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53][root][INFO] - Training Epoch: 1/2, step 2146/7134 completed (loss: 0.1437263935804367, acc: 0.9527027010917664)
[2025-02-13 19:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53][root][INFO] - Training Epoch: 1/2, step 2147/7134 completed (loss: 0.22645753622055054, acc: 0.9444444179534912)
[2025-02-13 19:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53][root][INFO] - Training Epoch: 1/2, step 2148/7134 completed (loss: 0.14861999452114105, acc: 0.9659090638160706)
[2025-02-13 19:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54][root][INFO] - Training Epoch: 1/2, step 2149/7134 completed (loss: 0.08244065195322037, acc: 0.9824561476707458)
[2025-02-13 19:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54][root][INFO] - Training Epoch: 1/2, step 2150/7134 completed (loss: 0.13504256308078766, acc: 0.9604519605636597)
[2025-02-13 19:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54][root][INFO] - Training Epoch: 1/2, step 2151/7134 completed (loss: 0.15896664559841156, acc: 0.9638554453849792)
[2025-02-13 19:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55][root][INFO] - Training Epoch: 1/2, step 2152/7134 completed (loss: 0.24333690106868744, acc: 0.9354838728904724)
[2025-02-13 19:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55][root][INFO] - Training Epoch: 1/2, step 2153/7134 completed (loss: 0.2459070235490799, acc: 0.9460784196853638)
[2025-02-13 19:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55][root][INFO] - Training Epoch: 1/2, step 2154/7134 completed (loss: 0.2321547567844391, acc: 0.954023003578186)
[2025-02-13 19:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56][root][INFO] - Training Epoch: 1/2, step 2155/7134 completed (loss: 0.1921200305223465, acc: 0.9487179517745972)
[2025-02-13 19:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56][root][INFO] - Training Epoch: 1/2, step 2156/7134 completed (loss: 0.30945831537246704, acc: 0.9411764740943909)
[2025-02-13 19:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56][root][INFO] - Training Epoch: 1/2, step 2157/7134 completed (loss: 0.2570105791091919, acc: 0.9242424368858337)
[2025-02-13 19:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57][root][INFO] - Training Epoch: 1/2, step 2158/7134 completed (loss: 0.27559733390808105, acc: 0.9432989954948425)
[2025-02-13 19:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57][root][INFO] - Training Epoch: 1/2, step 2159/7134 completed (loss: 0.12401698529720306, acc: 0.9702380895614624)
[2025-02-13 19:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57][root][INFO] - Training Epoch: 1/2, step 2160/7134 completed (loss: 0.21019086241722107, acc: 0.9390243887901306)
[2025-02-13 19:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:58][root][INFO] - Training Epoch: 1/2, step 2161/7134 completed (loss: 0.2594200372695923, acc: 0.9388889074325562)
[2025-02-13 19:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:58][root][INFO] - Training Epoch: 1/2, step 2162/7134 completed (loss: 0.15905436873435974, acc: 0.948387086391449)
[2025-02-13 19:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59][root][INFO] - Training Epoch: 1/2, step 2163/7134 completed (loss: 0.23219338059425354, acc: 0.9333333373069763)
[2025-02-13 19:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59][root][INFO] - Training Epoch: 1/2, step 2164/7134 completed (loss: 0.11234936118125916, acc: 0.9772727489471436)
[2025-02-13 19:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59][root][INFO] - Training Epoch: 1/2, step 2165/7134 completed (loss: 0.14341790974140167, acc: 0.9817073345184326)
[2025-02-13 19:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00][root][INFO] - Training Epoch: 1/2, step 2166/7134 completed (loss: 0.13730451464653015, acc: 0.9661017060279846)
[2025-02-13 19:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00][root][INFO] - Training Epoch: 1/2, step 2167/7134 completed (loss: 0.32345107197761536, acc: 0.9320388436317444)
[2025-02-13 19:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00][root][INFO] - Training Epoch: 1/2, step 2168/7134 completed (loss: 0.10144919902086258, acc: 0.9784172773361206)
[2025-02-13 19:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01][root][INFO] - Training Epoch: 1/2, step 2169/7134 completed (loss: 0.1859600841999054, acc: 0.9547738432884216)
[2025-02-13 19:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01][root][INFO] - Training Epoch: 1/2, step 2170/7134 completed (loss: 0.12607234716415405, acc: 0.9803921580314636)
[2025-02-13 19:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01][root][INFO] - Training Epoch: 1/2, step 2171/7134 completed (loss: 0.3283681571483612, acc: 0.9243243336677551)
[2025-02-13 19:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02][root][INFO] - Training Epoch: 1/2, step 2172/7134 completed (loss: 0.1725819855928421, acc: 0.9810426831245422)
[2025-02-13 19:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02][root][INFO] - Training Epoch: 1/2, step 2173/7134 completed (loss: 0.21891379356384277, acc: 0.965753436088562)
[2025-02-13 19:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02][root][INFO] - Training Epoch: 1/2, step 2174/7134 completed (loss: 0.1285085678100586, acc: 0.976047933101654)
[2025-02-13 19:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03][root][INFO] - Training Epoch: 1/2, step 2175/7134 completed (loss: 0.1638406664133072, acc: 0.9696969985961914)
[2025-02-13 19:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03][root][INFO] - Training Epoch: 1/2, step 2176/7134 completed (loss: 0.13522838056087494, acc: 0.967391312122345)
[2025-02-13 19:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04][root][INFO] - Training Epoch: 1/2, step 2177/7134 completed (loss: 0.1683197170495987, acc: 0.9672130942344666)
[2025-02-13 19:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04][root][INFO] - Training Epoch: 1/2, step 2178/7134 completed (loss: 0.1125321164727211, acc: 0.9739583134651184)
[2025-02-13 19:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04][root][INFO] - Training Epoch: 1/2, step 2179/7134 completed (loss: 0.12244406342506409, acc: 0.982758641242981)
[2025-02-13 19:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05][root][INFO] - Training Epoch: 1/2, step 2180/7134 completed (loss: 0.18703177571296692, acc: 0.9560439586639404)
[2025-02-13 19:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05][root][INFO] - Training Epoch: 1/2, step 2181/7134 completed (loss: 0.16625776886940002, acc: 0.95652174949646)
[2025-02-13 19:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05][root][INFO] - Training Epoch: 1/2, step 2182/7134 completed (loss: 0.2601901888847351, acc: 0.9230769276618958)
[2025-02-13 19:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:06][root][INFO] - Training Epoch: 1/2, step 2183/7134 completed (loss: 0.20536090433597565, acc: 0.9447513818740845)
[2025-02-13 19:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:06][root][INFO] - Training Epoch: 1/2, step 2184/7134 completed (loss: 0.1273707002401352, acc: 0.9577465057373047)
[2025-02-13 19:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07][root][INFO] - Training Epoch: 1/2, step 2185/7134 completed (loss: 0.33499881625175476, acc: 0.9245283007621765)
[2025-02-13 19:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07][root][INFO] - Training Epoch: 1/2, step 2186/7134 completed (loss: 0.516697108745575, acc: 0.8799999952316284)
[2025-02-13 19:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07][root][INFO] - Training Epoch: 1/2, step 2187/7134 completed (loss: 0.3854639232158661, acc: 0.8992805480957031)
[2025-02-13 19:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08][root][INFO] - Training Epoch: 1/2, step 2188/7134 completed (loss: 0.1813870519399643, acc: 0.9515151381492615)
[2025-02-13 19:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08][root][INFO] - Training Epoch: 1/2, step 2189/7134 completed (loss: 0.3388281762599945, acc: 0.9025974273681641)
[2025-02-13 19:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08][root][INFO] - Training Epoch: 1/2, step 2190/7134 completed (loss: 0.24716772139072418, acc: 0.9441340565681458)
[2025-02-13 19:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09][root][INFO] - Training Epoch: 1/2, step 2191/7134 completed (loss: 0.4929015338420868, acc: 0.8631578683853149)
[2025-02-13 19:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09][root][INFO] - Training Epoch: 1/2, step 2192/7134 completed (loss: 0.1463378220796585, acc: 0.9629629850387573)
[2025-02-13 19:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09][root][INFO] - Training Epoch: 1/2, step 2193/7134 completed (loss: 0.27027788758277893, acc: 0.9398906826972961)
[2025-02-13 19:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10][root][INFO] - Training Epoch: 1/2, step 2194/7134 completed (loss: 0.2276439666748047, acc: 0.9276315569877625)
[2025-02-13 19:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10][root][INFO] - Training Epoch: 1/2, step 2195/7134 completed (loss: 0.1047687903046608, acc: 0.9753086566925049)
[2025-02-13 19:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11][root][INFO] - Training Epoch: 1/2, step 2196/7134 completed (loss: 0.15448647737503052, acc: 0.9670329689979553)
[2025-02-13 19:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11][root][INFO] - Training Epoch: 1/2, step 2197/7134 completed (loss: 0.180043563246727, acc: 0.9291338324546814)
[2025-02-13 19:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11][root][INFO] - Training Epoch: 1/2, step 2198/7134 completed (loss: 0.13786433637142181, acc: 0.977011501789093)
[2025-02-13 19:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12][root][INFO] - Training Epoch: 1/2, step 2199/7134 completed (loss: 0.07587206363677979, acc: 0.9810126423835754)
[2025-02-13 19:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12][root][INFO] - Training Epoch: 1/2, step 2200/7134 completed (loss: 0.1162896677851677, acc: 0.9842519760131836)
[2025-02-13 19:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12][root][INFO] - Training Epoch: 1/2, step 2201/7134 completed (loss: 0.09277699142694473, acc: 0.9662162065505981)
[2025-02-13 19:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13][root][INFO] - Training Epoch: 1/2, step 2202/7134 completed (loss: 0.15248920023441315, acc: 0.970802903175354)
[2025-02-13 19:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13][root][INFO] - Training Epoch: 1/2, step 2203/7134 completed (loss: 0.08367899060249329, acc: 0.9880239367485046)
[2025-02-13 19:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14][root][INFO] - Training Epoch: 1/2, step 2204/7134 completed (loss: 0.11389409005641937, acc: 0.982758641242981)
[2025-02-13 19:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14][root][INFO] - Training Epoch: 1/2, step 2205/7134 completed (loss: 0.05790719389915466, acc: 0.9858155846595764)
[2025-02-13 19:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14][root][INFO] - Training Epoch: 1/2, step 2206/7134 completed (loss: 0.1473315805196762, acc: 0.9798657894134521)
[2025-02-13 19:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15][root][INFO] - Training Epoch: 1/2, step 2207/7134 completed (loss: 0.06091400235891342, acc: 0.9863945841789246)
[2025-02-13 19:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15][root][INFO] - Training Epoch: 1/2, step 2208/7134 completed (loss: 0.11209823936223984, acc: 0.9741935729980469)
[2025-02-13 19:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15][root][INFO] - Training Epoch: 1/2, step 2209/7134 completed (loss: 0.12025942653417587, acc: 0.9695122241973877)
[2025-02-13 19:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16][root][INFO] - Training Epoch: 1/2, step 2210/7134 completed (loss: 0.14540325105190277, acc: 0.9695122241973877)
[2025-02-13 19:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16][root][INFO] - Training Epoch: 1/2, step 2211/7134 completed (loss: 0.09463198482990265, acc: 0.9759036302566528)
[2025-02-13 19:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16][root][INFO] - Training Epoch: 1/2, step 2212/7134 completed (loss: 0.08925487846136093, acc: 0.9828571677207947)
[2025-02-13 19:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17][root][INFO] - Training Epoch: 1/2, step 2213/7134 completed (loss: 0.17385925352573395, acc: 0.9588235020637512)
[2025-02-13 19:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17][root][INFO] - Training Epoch: 1/2, step 2214/7134 completed (loss: 0.17911431193351746, acc: 0.9729729890823364)
[2025-02-13 19:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17][root][INFO] - Training Epoch: 1/2, step 2215/7134 completed (loss: 0.10983184725046158, acc: 0.9746835231781006)
[2025-02-13 19:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18][root][INFO] - Training Epoch: 1/2, step 2216/7134 completed (loss: 0.12337260693311691, acc: 0.9590643048286438)
[2025-02-13 19:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18][root][INFO] - Training Epoch: 1/2, step 2217/7134 completed (loss: 0.0945611372590065, acc: 0.976047933101654)
[2025-02-13 19:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18][root][INFO] - Training Epoch: 1/2, step 2218/7134 completed (loss: 0.19482719898223877, acc: 0.9506173133850098)
[2025-02-13 19:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:19][root][INFO] - Training Epoch: 1/2, step 2219/7134 completed (loss: 0.15712568163871765, acc: 0.9571428298950195)
[2025-02-13 19:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:19][root][INFO] - Training Epoch: 1/2, step 2220/7134 completed (loss: 0.1957554817199707, acc: 0.9555555582046509)
[2025-02-13 19:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20][root][INFO] - Training Epoch: 1/2, step 2221/7134 completed (loss: 0.21240843832492828, acc: 0.9378882050514221)
[2025-02-13 19:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20][root][INFO] - Training Epoch: 1/2, step 2222/7134 completed (loss: 0.4356415569782257, acc: 0.9178082346916199)
[2025-02-13 19:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20][root][INFO] - Training Epoch: 1/2, step 2223/7134 completed (loss: 0.1752994805574417, acc: 0.9530201554298401)
[2025-02-13 19:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21][root][INFO] - Training Epoch: 1/2, step 2224/7134 completed (loss: 0.38171106576919556, acc: 0.9113923907279968)
[2025-02-13 19:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21][root][INFO] - Training Epoch: 1/2, step 2225/7134 completed (loss: 0.23523686826229095, acc: 0.9319728016853333)
[2025-02-13 19:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21][root][INFO] - Training Epoch: 1/2, step 2226/7134 completed (loss: 0.17647594213485718, acc: 0.9463087320327759)
[2025-02-13 19:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22][root][INFO] - Training Epoch: 1/2, step 2227/7134 completed (loss: 0.0959380567073822, acc: 0.9740259647369385)
[2025-02-13 19:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22][root][INFO] - Training Epoch: 1/2, step 2228/7134 completed (loss: 0.11734926700592041, acc: 0.9844961166381836)
[2025-02-13 19:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22][root][INFO] - Training Epoch: 1/2, step 2229/7134 completed (loss: 0.1862010955810547, acc: 0.9611650705337524)
[2025-02-13 19:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23][root][INFO] - Training Epoch: 1/2, step 2230/7134 completed (loss: 0.04686636105179787, acc: 0.9856114983558655)
[2025-02-13 19:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23][root][INFO] - Training Epoch: 1/2, step 2231/7134 completed (loss: 0.2909073829650879, acc: 0.9159663915634155)
[2025-02-13 19:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23][root][INFO] - Training Epoch: 1/2, step 2232/7134 completed (loss: 0.19939012825489044, acc: 0.9367815852165222)
[2025-02-13 19:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24][root][INFO] - Training Epoch: 1/2, step 2233/7134 completed (loss: 0.20605990290641785, acc: 0.9551281929016113)
[2025-02-13 19:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24][root][INFO] - Training Epoch: 1/2, step 2234/7134 completed (loss: 0.07630198448896408, acc: 0.9805194735527039)
[2025-02-13 19:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25][root][INFO] - Training Epoch: 1/2, step 2235/7134 completed (loss: 0.1654784083366394, acc: 0.9720279574394226)
[2025-02-13 19:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25][root][INFO] - Training Epoch: 1/2, step 2236/7134 completed (loss: 0.17116230726242065, acc: 0.9466666579246521)
[2025-02-13 19:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25][root][INFO] - Training Epoch: 1/2, step 2237/7134 completed (loss: 0.2132340967655182, acc: 0.9466666579246521)
[2025-02-13 19:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26][root][INFO] - Training Epoch: 1/2, step 2238/7134 completed (loss: 0.2942745089530945, acc: 0.9382715821266174)
[2025-02-13 19:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26][root][INFO] - Training Epoch: 1/2, step 2239/7134 completed (loss: 0.5024499893188477, acc: 0.8770053386688232)
[2025-02-13 19:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26][root][INFO] - Training Epoch: 1/2, step 2240/7134 completed (loss: 0.25808656215667725, acc: 0.9375)
[2025-02-13 19:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27][root][INFO] - Training Epoch: 1/2, step 2241/7134 completed (loss: 0.40609875321388245, acc: 0.8854166865348816)
[2025-02-13 19:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27][root][INFO] - Training Epoch: 1/2, step 2242/7134 completed (loss: 0.6953812837600708, acc: 0.8518518805503845)
[2025-02-13 19:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27][root][INFO] - Training Epoch: 1/2, step 2243/7134 completed (loss: 0.7265747785568237, acc: 0.8258064389228821)
[2025-02-13 19:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28][root][INFO] - Training Epoch: 1/2, step 2244/7134 completed (loss: 0.34199878573417664, acc: 0.9109588861465454)
[2025-02-13 19:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28][root][INFO] - Training Epoch: 1/2, step 2245/7134 completed (loss: 0.4826899468898773, acc: 0.8888888955116272)
[2025-02-13 19:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28][root][INFO] - Training Epoch: 1/2, step 2246/7134 completed (loss: 0.7861081957817078, acc: 0.7978141903877258)
[2025-02-13 19:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29][root][INFO] - Training Epoch: 1/2, step 2247/7134 completed (loss: 0.6490131616592407, acc: 0.8150289058685303)
[2025-02-13 19:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29][root][INFO] - Training Epoch: 1/2, step 2248/7134 completed (loss: 0.5632713437080383, acc: 0.8554913401603699)
[2025-02-13 19:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30][root][INFO] - Training Epoch: 1/2, step 2249/7134 completed (loss: 0.4419706463813782, acc: 0.8839778900146484)
[2025-02-13 19:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30][root][INFO] - Training Epoch: 1/2, step 2250/7134 completed (loss: 0.39324453473091125, acc: 0.8936170339584351)
[2025-02-13 19:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30][root][INFO] - Training Epoch: 1/2, step 2251/7134 completed (loss: 0.16553127765655518, acc: 0.9508196711540222)
[2025-02-13 19:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31][root][INFO] - Training Epoch: 1/2, step 2252/7134 completed (loss: 0.15589340031147003, acc: 0.9619565010070801)
[2025-02-13 19:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31][root][INFO] - Training Epoch: 1/2, step 2253/7134 completed (loss: 0.28607377409935, acc: 0.9363057613372803)
[2025-02-13 19:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31][root][INFO] - Training Epoch: 1/2, step 2254/7134 completed (loss: 0.5011842250823975, acc: 0.8548387289047241)
[2025-02-13 19:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32][root][INFO] - Training Epoch: 1/2, step 2255/7134 completed (loss: 0.15953831374645233, acc: 0.9622641801834106)
[2025-02-13 19:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32][root][INFO] - Training Epoch: 1/2, step 2256/7134 completed (loss: 0.6649385094642639, acc: 0.8211382031440735)
[2025-02-13 19:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32][root][INFO] - Training Epoch: 1/2, step 2257/7134 completed (loss: 0.22484567761421204, acc: 0.9444444179534912)
[2025-02-13 19:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33][root][INFO] - Training Epoch: 1/2, step 2258/7134 completed (loss: 0.2719216048717499, acc: 0.930232584476471)
[2025-02-13 19:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33][root][INFO] - Training Epoch: 1/2, step 2259/7134 completed (loss: 0.36184799671173096, acc: 0.8974359035491943)
[2025-02-13 19:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33][root][INFO] - Training Epoch: 1/2, step 2260/7134 completed (loss: 0.5381996631622314, acc: 0.8636363744735718)
[2025-02-13 19:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34][root][INFO] - Training Epoch: 1/2, step 2261/7134 completed (loss: 0.7692100405693054, acc: 0.8395061492919922)
[2025-02-13 19:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34][root][INFO] - Training Epoch: 1/2, step 2262/7134 completed (loss: 0.32697248458862305, acc: 0.9248120188713074)
[2025-02-13 19:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35][root][INFO] - Training Epoch: 1/2, step 2263/7134 completed (loss: 0.38637080788612366, acc: 0.9172932505607605)
[2025-02-13 19:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35][root][INFO] - Training Epoch: 1/2, step 2264/7134 completed (loss: 0.42369726300239563, acc: 0.8810811042785645)
[2025-02-13 19:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35][root][INFO] - Training Epoch: 1/2, step 2265/7134 completed (loss: 0.4188338816165924, acc: 0.9025974273681641)
[2025-02-13 19:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36][root][INFO] - Training Epoch: 1/2, step 2266/7134 completed (loss: 0.29798927903175354, acc: 0.9060773253440857)
[2025-02-13 19:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36][root][INFO] - Training Epoch: 1/2, step 2267/7134 completed (loss: 0.16796259582042694, acc: 0.9674796462059021)
[2025-02-13 19:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37][root][INFO] - Training Epoch: 1/2, step 2268/7134 completed (loss: 0.20562182366847992, acc: 0.9520547986030579)
[2025-02-13 19:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37][root][INFO] - Training Epoch: 1/2, step 2269/7134 completed (loss: 0.21743248403072357, acc: 0.9385474920272827)
[2025-02-13 19:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37][root][INFO] - Training Epoch: 1/2, step 2270/7134 completed (loss: 0.4033196270465851, acc: 0.90625)
[2025-02-13 19:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38][root][INFO] - Training Epoch: 1/2, step 2271/7134 completed (loss: 0.3983495831489563, acc: 0.8918918967247009)
[2025-02-13 19:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38][root][INFO] - Training Epoch: 1/2, step 2272/7134 completed (loss: 0.287432461977005, acc: 0.9301075339317322)
[2025-02-13 19:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38][root][INFO] - Training Epoch: 1/2, step 2273/7134 completed (loss: 0.12185915559530258, acc: 0.9666666388511658)
[2025-02-13 19:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:39][root][INFO] - Training Epoch: 1/2, step 2274/7134 completed (loss: 0.6408620476722717, acc: 0.8820512890815735)
[2025-02-13 19:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:39][root][INFO] - Training Epoch: 1/2, step 2275/7134 completed (loss: 0.14390987157821655, acc: 0.9754902124404907)
[2025-02-13 19:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40][root][INFO] - Training Epoch: 1/2, step 2276/7134 completed (loss: 0.24657578766345978, acc: 0.9523809552192688)
[2025-02-13 19:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40][root][INFO] - Training Epoch: 1/2, step 2277/7134 completed (loss: 0.25215816497802734, acc: 0.9333333373069763)
[2025-02-13 19:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40][root][INFO] - Training Epoch: 1/2, step 2278/7134 completed (loss: 0.6302587985992432, acc: 0.8709677457809448)
[2025-02-13 19:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41][root][INFO] - Training Epoch: 1/2, step 2279/7134 completed (loss: 0.16733671724796295, acc: 0.9543147087097168)
[2025-02-13 19:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41][root][INFO] - Training Epoch: 1/2, step 2280/7134 completed (loss: 0.28799399733543396, acc: 0.9311926364898682)
[2025-02-13 19:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41][root][INFO] - Training Epoch: 1/2, step 2281/7134 completed (loss: 0.44136667251586914, acc: 0.914893627166748)
[2025-02-13 19:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42][root][INFO] - Training Epoch: 1/2, step 2282/7134 completed (loss: 0.14236979186534882, acc: 0.9631578922271729)
[2025-02-13 19:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42][root][INFO] - Training Epoch: 1/2, step 2283/7134 completed (loss: 0.19938844442367554, acc: 0.9567307829856873)
[2025-02-13 19:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43][root][INFO] - Training Epoch: 1/2, step 2284/7134 completed (loss: 0.16933056712150574, acc: 0.9548386931419373)
[2025-02-13 19:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43][root][INFO] - Training Epoch: 1/2, step 2285/7134 completed (loss: 0.15371742844581604, acc: 0.9599999785423279)
[2025-02-13 19:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43][root][INFO] - Training Epoch: 1/2, step 2286/7134 completed (loss: 0.32433024048805237, acc: 0.8965517282485962)
[2025-02-13 19:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44][root][INFO] - Training Epoch: 1/2, step 2287/7134 completed (loss: 0.27262043952941895, acc: 0.9325153231620789)
[2025-02-13 19:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44][root][INFO] - Training Epoch: 1/2, step 2288/7134 completed (loss: 0.10137893259525299, acc: 0.9725274443626404)
[2025-02-13 19:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44][root][INFO] - Training Epoch: 1/2, step 2289/7134 completed (loss: 0.1773950308561325, acc: 0.9465240836143494)
[2025-02-13 19:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45][root][INFO] - Training Epoch: 1/2, step 2290/7134 completed (loss: 0.21558675169944763, acc: 0.9301075339317322)
[2025-02-13 19:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45][root][INFO] - Training Epoch: 1/2, step 2291/7134 completed (loss: 0.21266677975654602, acc: 0.9226519465446472)
[2025-02-13 19:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46][root][INFO] - Training Epoch: 1/2, step 2292/7134 completed (loss: 0.2801567018032074, acc: 0.9242424368858337)
[2025-02-13 19:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46][root][INFO] - Training Epoch: 1/2, step 2293/7134 completed (loss: 0.18789862096309662, acc: 0.9406392574310303)
[2025-02-13 19:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46][root][INFO] - Training Epoch: 1/2, step 2294/7134 completed (loss: 0.20534318685531616, acc: 0.9569377899169922)
[2025-02-13 19:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47][root][INFO] - Training Epoch: 1/2, step 2295/7134 completed (loss: 0.20154082775115967, acc: 0.94921875)
[2025-02-13 19:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47][root][INFO] - Training Epoch: 1/2, step 2296/7134 completed (loss: 0.2209688276052475, acc: 0.9519230723381042)
[2025-02-13 19:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47][root][INFO] - Training Epoch: 1/2, step 2297/7134 completed (loss: 0.13221395015716553, acc: 0.9669811129570007)
[2025-02-13 19:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48][root][INFO] - Training Epoch: 1/2, step 2298/7134 completed (loss: 0.21106207370758057, acc: 0.9476190209388733)
[2025-02-13 19:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48][root][INFO] - Training Epoch: 1/2, step 2299/7134 completed (loss: 0.25186967849731445, acc: 0.9462365508079529)
[2025-02-13 19:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49][root][INFO] - Training Epoch: 1/2, step 2300/7134 completed (loss: 0.20667517185211182, acc: 0.9696969985961914)
[2025-02-13 19:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49][root][INFO] - Training Epoch: 1/2, step 2301/7134 completed (loss: 0.3017156720161438, acc: 0.929347813129425)
[2025-02-13 19:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49][root][INFO] - Training Epoch: 1/2, step 2302/7134 completed (loss: 0.21455493569374084, acc: 0.9384615421295166)
[2025-02-13 19:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50][root][INFO] - Training Epoch: 1/2, step 2303/7134 completed (loss: 0.3417392075061798, acc: 0.9078341126441956)
[2025-02-13 19:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50][root][INFO] - Training Epoch: 1/2, step 2304/7134 completed (loss: 0.3933332860469818, acc: 0.907489001750946)
[2025-02-13 19:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50][root][INFO] - Training Epoch: 1/2, step 2305/7134 completed (loss: 0.2736617922782898, acc: 0.9292452931404114)
[2025-02-13 19:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51][root][INFO] - Training Epoch: 1/2, step 2306/7134 completed (loss: 0.12594692409038544, acc: 0.9720930457115173)
[2025-02-13 19:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51][root][INFO] - Training Epoch: 1/2, step 2307/7134 completed (loss: 0.1590336263179779, acc: 0.9618644118309021)
[2025-02-13 19:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51][root][INFO] - Training Epoch: 1/2, step 2308/7134 completed (loss: 0.26134178042411804, acc: 0.9295154213905334)
[2025-02-13 19:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52][root][INFO] - Training Epoch: 1/2, step 2309/7134 completed (loss: 0.08460472524166107, acc: 0.9788359999656677)
[2025-02-13 19:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52][root][INFO] - Training Epoch: 1/2, step 2310/7134 completed (loss: 0.13984595239162445, acc: 0.961904764175415)
[2025-02-13 19:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53][root][INFO] - Training Epoch: 1/2, step 2311/7134 completed (loss: 0.20234638452529907, acc: 0.931034505367279)
[2025-02-13 19:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53][root][INFO] - Training Epoch: 1/2, step 2312/7134 completed (loss: 0.16909687221050262, acc: 0.9624413251876831)
[2025-02-13 19:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53][root][INFO] - Training Epoch: 1/2, step 2313/7134 completed (loss: 0.2285516858100891, acc: 0.9278846383094788)
[2025-02-13 19:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54][root][INFO] - Training Epoch: 1/2, step 2314/7134 completed (loss: 0.16617752611637115, acc: 0.9541284441947937)
[2025-02-13 19:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54][root][INFO] - Training Epoch: 1/2, step 2315/7134 completed (loss: 0.2302325814962387, acc: 0.9255813956260681)
[2025-02-13 19:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55][root][INFO] - Training Epoch: 1/2, step 2316/7134 completed (loss: 0.10654390603303909, acc: 0.9682539701461792)
[2025-02-13 19:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55][root][INFO] - Training Epoch: 1/2, step 2317/7134 completed (loss: 0.1997056007385254, acc: 0.9537814855575562)
[2025-02-13 19:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55][root][INFO] - Training Epoch: 1/2, step 2318/7134 completed (loss: 0.1123894527554512, acc: 0.9737991094589233)
[2025-02-13 19:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56][root][INFO] - Training Epoch: 1/2, step 2319/7134 completed (loss: 0.13463416695594788, acc: 0.9684684872627258)
[2025-02-13 19:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56][root][INFO] - Training Epoch: 1/2, step 2320/7134 completed (loss: 0.13527846336364746, acc: 0.9589743614196777)
[2025-02-13 19:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56][root][INFO] - Training Epoch: 1/2, step 2321/7134 completed (loss: 0.09264025092124939, acc: 0.9882352948188782)
[2025-02-13 19:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:57][root][INFO] - Training Epoch: 1/2, step 2322/7134 completed (loss: 0.2464616894721985, acc: 0.9325153231620789)
[2025-02-13 19:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:57][root][INFO] - Training Epoch: 1/2, step 2323/7134 completed (loss: 0.2944420576095581, acc: 0.9363057613372803)
[2025-02-13 19:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58][root][INFO] - Training Epoch: 1/2, step 2324/7134 completed (loss: 0.44367915391921997, acc: 0.9578313231468201)
[2025-02-13 19:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58][root][INFO] - Training Epoch: 1/2, step 2325/7134 completed (loss: 0.30497992038726807, acc: 0.9142857193946838)
[2025-02-13 19:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58][root][INFO] - Training Epoch: 1/2, step 2326/7134 completed (loss: 0.10134661942720413, acc: 0.9769230484962463)
[2025-02-13 19:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59][root][INFO] - Training Epoch: 1/2, step 2327/7134 completed (loss: 0.19381169974803925, acc: 0.9530201554298401)
[2025-02-13 19:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59][root][INFO] - Training Epoch: 1/2, step 2328/7134 completed (loss: 0.2743361294269562, acc: 0.9328858852386475)
[2025-02-13 19:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00][root][INFO] - Training Epoch: 1/2, step 2329/7134 completed (loss: 0.2233363687992096, acc: 0.9346405267715454)
[2025-02-13 19:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00][root][INFO] - Training Epoch: 1/2, step 2330/7134 completed (loss: 0.06731405109167099, acc: 0.9757575988769531)
[2025-02-13 19:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00][root][INFO] - Training Epoch: 1/2, step 2331/7134 completed (loss: 0.07699979096651077, acc: 0.9741935729980469)
[2025-02-13 19:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01][root][INFO] - Training Epoch: 1/2, step 2332/7134 completed (loss: 0.1333966851234436, acc: 0.9696969985961914)
[2025-02-13 19:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01][root][INFO] - Training Epoch: 1/2, step 2333/7134 completed (loss: 0.10762527585029602, acc: 0.9756097793579102)
[2025-02-13 19:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01][root][INFO] - Training Epoch: 1/2, step 2334/7134 completed (loss: 0.26947617530822754, acc: 0.9426751732826233)
[2025-02-13 19:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02][root][INFO] - Training Epoch: 1/2, step 2335/7134 completed (loss: 0.1252184957265854, acc: 0.949367105960846)
[2025-02-13 19:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02][root][INFO] - Training Epoch: 1/2, step 2336/7134 completed (loss: 0.10524555295705795, acc: 0.9798657894134521)
[2025-02-13 19:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03][root][INFO] - Training Epoch: 1/2, step 2337/7134 completed (loss: 0.05820287764072418, acc: 0.9861111044883728)
[2025-02-13 19:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03][root][INFO] - Training Epoch: 1/2, step 2338/7134 completed (loss: 0.08387035131454468, acc: 0.9805194735527039)
[2025-02-13 19:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03][root][INFO] - Training Epoch: 1/2, step 2339/7134 completed (loss: 0.18979842960834503, acc: 0.9615384340286255)
[2025-02-13 19:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04][root][INFO] - Training Epoch: 1/2, step 2340/7134 completed (loss: 0.04709968715906143, acc: 0.9870129823684692)
[2025-02-13 19:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04][root][INFO] - Training Epoch: 1/2, step 2341/7134 completed (loss: 0.1453305184841156, acc: 0.9752066135406494)
[2025-02-13 19:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04][root][INFO] - Training Epoch: 1/2, step 2342/7134 completed (loss: 0.08904962986707687, acc: 0.976190447807312)
[2025-02-13 19:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05][root][INFO] - Training Epoch: 1/2, step 2343/7134 completed (loss: 0.09326285123825073, acc: 0.987500011920929)
[2025-02-13 19:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05][root][INFO] - Training Epoch: 1/2, step 2344/7134 completed (loss: 0.10459796339273453, acc: 0.9691358208656311)
[2025-02-13 19:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05][root][INFO] - Training Epoch: 1/2, step 2345/7134 completed (loss: 0.14828674495220184, acc: 0.9425287246704102)
[2025-02-13 19:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06][root][INFO] - Training Epoch: 1/2, step 2346/7134 completed (loss: 0.08384969085454941, acc: 0.9723756909370422)
[2025-02-13 19:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06][root][INFO] - Training Epoch: 1/2, step 2347/7134 completed (loss: 0.183216854929924, acc: 0.9507042169570923)
[2025-02-13 19:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07][root][INFO] - Training Epoch: 1/2, step 2348/7134 completed (loss: 0.563766598701477, acc: 0.8556700944900513)
[2025-02-13 19:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07][root][INFO] - Training Epoch: 1/2, step 2349/7134 completed (loss: 0.49468064308166504, acc: 0.875)
[2025-02-13 19:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07][root][INFO] - Training Epoch: 1/2, step 2350/7134 completed (loss: 0.4737685024738312, acc: 0.892405092716217)
[2025-02-13 19:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08][root][INFO] - Training Epoch: 1/2, step 2351/7134 completed (loss: 0.40398430824279785, acc: 0.8961748480796814)
[2025-02-13 19:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08][root][INFO] - Training Epoch: 1/2, step 2352/7134 completed (loss: 0.402636855840683, acc: 0.896774172782898)
[2025-02-13 19:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08][root][INFO] - Training Epoch: 1/2, step 2353/7134 completed (loss: 0.33183327317237854, acc: 0.9161290526390076)
[2025-02-13 19:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09][root][INFO] - Training Epoch: 1/2, step 2354/7134 completed (loss: 0.1588522344827652, acc: 0.9562841653823853)
[2025-02-13 19:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09][root][INFO] - Training Epoch: 1/2, step 2355/7134 completed (loss: 0.29607686400413513, acc: 0.9281768202781677)
[2025-02-13 19:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10][root][INFO] - Training Epoch: 1/2, step 2356/7134 completed (loss: 0.3504182696342468, acc: 0.9057971239089966)
[2025-02-13 19:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10][root][INFO] - Training Epoch: 1/2, step 2357/7134 completed (loss: 0.3388945460319519, acc: 0.9047619104385376)
[2025-02-13 19:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10][root][INFO] - Training Epoch: 1/2, step 2358/7134 completed (loss: 0.35092058777809143, acc: 0.918367326259613)
[2025-02-13 19:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11][root][INFO] - Training Epoch: 1/2, step 2359/7134 completed (loss: 0.4617626667022705, acc: 0.8500000238418579)
[2025-02-13 19:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11][root][INFO] - Training Epoch: 1/2, step 2360/7134 completed (loss: 0.2884434461593628, acc: 0.9177215099334717)
[2025-02-13 19:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11][root][INFO] - Training Epoch: 1/2, step 2361/7134 completed (loss: 0.2813636362552643, acc: 0.9285714030265808)
[2025-02-13 19:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12][root][INFO] - Training Epoch: 1/2, step 2362/7134 completed (loss: 0.3923516571521759, acc: 0.9279279112815857)
[2025-02-13 19:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12][root][INFO] - Training Epoch: 1/2, step 2363/7134 completed (loss: 0.22242917120456696, acc: 0.9476439952850342)
[2025-02-13 19:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13][root][INFO] - Training Epoch: 1/2, step 2364/7134 completed (loss: 0.1243412047624588, acc: 0.9738562107086182)
[2025-02-13 19:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13][root][INFO] - Training Epoch: 1/2, step 2365/7134 completed (loss: 0.13883543014526367, acc: 0.9534883499145508)
[2025-02-13 19:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13][root][INFO] - Training Epoch: 1/2, step 2366/7134 completed (loss: 0.11041583865880966, acc: 0.9846153855323792)
[2025-02-13 19:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14][root][INFO] - Training Epoch: 1/2, step 2367/7134 completed (loss: 0.2244313508272171, acc: 0.9595959782600403)
[2025-02-13 19:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14][root][INFO] - Training Epoch: 1/2, step 2368/7134 completed (loss: 0.3410273790359497, acc: 0.9346733689308167)
[2025-02-13 19:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14][root][INFO] - Training Epoch: 1/2, step 2369/7134 completed (loss: 0.1987939029932022, acc: 0.9622641801834106)
[2025-02-13 19:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15][root][INFO] - Training Epoch: 1/2, step 2370/7134 completed (loss: 0.17230771481990814, acc: 0.9666666388511658)
[2025-02-13 19:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15][root][INFO] - Training Epoch: 1/2, step 2371/7134 completed (loss: 0.26591843366622925, acc: 0.9440993666648865)
[2025-02-13 19:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15][root][INFO] - Training Epoch: 1/2, step 2372/7134 completed (loss: 0.10235734283924103, acc: 0.9750000238418579)
[2025-02-13 19:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16][root][INFO] - Training Epoch: 1/2, step 2373/7134 completed (loss: 0.13253483176231384, acc: 0.9792746305465698)
[2025-02-13 19:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16][root][INFO] - Training Epoch: 1/2, step 2374/7134 completed (loss: 0.18233928084373474, acc: 0.9558011293411255)
[2025-02-13 19:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17][root][INFO] - Training Epoch: 1/2, step 2375/7134 completed (loss: 0.21274693310260773, acc: 0.9333333373069763)
[2025-02-13 19:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17][root][INFO] - Training Epoch: 1/2, step 2376/7134 completed (loss: 0.4289277493953705, acc: 0.9019607901573181)
[2025-02-13 19:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17][root][INFO] - Training Epoch: 1/2, step 2377/7134 completed (loss: 0.2644951641559601, acc: 0.9252336621284485)
[2025-02-13 19:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18][root][INFO] - Training Epoch: 1/2, step 2378/7134 completed (loss: 0.4211394786834717, acc: 0.9009901285171509)
[2025-02-13 19:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18][root][INFO] - Training Epoch: 1/2, step 2379/7134 completed (loss: 0.5422975420951843, acc: 0.8520408272743225)
[2025-02-13 19:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18][root][INFO] - Training Epoch: 1/2, step 2380/7134 completed (loss: 0.32385751605033875, acc: 0.907608687877655)
[2025-02-13 19:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19][root][INFO] - Training Epoch: 1/2, step 2381/7134 completed (loss: 0.8027171492576599, acc: 0.8260869383811951)
[2025-02-13 19:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19][root][INFO] - Training Epoch: 1/2, step 2382/7134 completed (loss: 0.2818740904331207, acc: 0.9197530746459961)
[2025-02-13 19:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19][root][INFO] - Training Epoch: 1/2, step 2383/7134 completed (loss: 0.28345850110054016, acc: 0.9125683307647705)
[2025-02-13 19:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20][root][INFO] - Training Epoch: 1/2, step 2384/7134 completed (loss: 0.3505062758922577, acc: 0.9195402264595032)
[2025-02-13 19:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20][root][INFO] - Training Epoch: 1/2, step 2385/7134 completed (loss: 0.2109592705965042, acc: 0.9615384340286255)
[2025-02-13 19:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21][root][INFO] - Training Epoch: 1/2, step 2386/7134 completed (loss: 0.31812965869903564, acc: 0.9047619104385376)
[2025-02-13 19:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21][root][INFO] - Training Epoch: 1/2, step 2387/7134 completed (loss: 0.2517494857311249, acc: 0.9285714030265808)
[2025-02-13 19:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21][root][INFO] - Training Epoch: 1/2, step 2388/7134 completed (loss: 0.29744240641593933, acc: 0.9086538553237915)
[2025-02-13 19:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22][root][INFO] - Training Epoch: 1/2, step 2389/7134 completed (loss: 0.3714354932308197, acc: 0.9146919250488281)
[2025-02-13 19:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22][root][INFO] - Training Epoch: 1/2, step 2390/7134 completed (loss: 0.4671007990837097, acc: 0.8909952640533447)
[2025-02-13 19:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22][root][INFO] - Training Epoch: 1/2, step 2391/7134 completed (loss: 0.31961318850517273, acc: 0.908108115196228)
[2025-02-13 19:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23][root][INFO] - Training Epoch: 1/2, step 2392/7134 completed (loss: 0.31170138716697693, acc: 0.9234972596168518)
[2025-02-13 19:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23][root][INFO] - Training Epoch: 1/2, step 2393/7134 completed (loss: 0.2674427926540375, acc: 0.9326424598693848)
[2025-02-13 19:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24][root][INFO] - Training Epoch: 1/2, step 2394/7134 completed (loss: 0.24768385291099548, acc: 0.9234693646430969)
[2025-02-13 19:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24][root][INFO] - Training Epoch: 1/2, step 2395/7134 completed (loss: 0.6593132019042969, acc: 0.8285714387893677)
[2025-02-13 19:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24][root][INFO] - Training Epoch: 1/2, step 2396/7134 completed (loss: 0.6922910213470459, acc: 0.8333333134651184)
[2025-02-13 19:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25][root][INFO] - Training Epoch: 1/2, step 2397/7134 completed (loss: 0.46976643800735474, acc: 0.8963414430618286)
[2025-02-13 19:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25][root][INFO] - Training Epoch: 1/2, step 2398/7134 completed (loss: 0.19365809857845306, acc: 0.9459459185600281)
[2025-02-13 19:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25][root][INFO] - Training Epoch: 1/2, step 2399/7134 completed (loss: 0.5923568606376648, acc: 0.8549222946166992)
[2025-02-13 19:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:26][root][INFO] - Training Epoch: 1/2, step 2400/7134 completed (loss: 0.4731251299381256, acc: 0.8693467378616333)
[2025-02-13 19:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:26][root][INFO] - Training Epoch: 1/2, step 2401/7134 completed (loss: 0.9933773279190063, acc: 0.7596153616905212)
[2025-02-13 19:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27][root][INFO] - Training Epoch: 1/2, step 2402/7134 completed (loss: 0.47387078404426575, acc: 0.8985507488250732)
[2025-02-13 19:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27][root][INFO] - Training Epoch: 1/2, step 2403/7134 completed (loss: 0.2693454921245575, acc: 0.9221556782722473)
[2025-02-13 19:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27][root][INFO] - Training Epoch: 1/2, step 2404/7134 completed (loss: 0.24538180232048035, acc: 0.9342105388641357)
[2025-02-13 19:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28][root][INFO] - Training Epoch: 1/2, step 2405/7134 completed (loss: 0.4112817049026489, acc: 0.8980891704559326)
[2025-02-13 19:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28][root][INFO] - Training Epoch: 1/2, step 2406/7134 completed (loss: 0.10216186940670013, acc: 0.976331353187561)
[2025-02-13 19:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29][root][INFO] - Training Epoch: 1/2, step 2407/7134 completed (loss: 0.24165359139442444, acc: 0.9607843160629272)
[2025-02-13 19:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29][root][INFO] - Training Epoch: 1/2, step 2408/7134 completed (loss: 0.31993913650512695, acc: 0.9226519465446472)
[2025-02-13 19:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29][root][INFO] - Training Epoch: 1/2, step 2409/7134 completed (loss: 0.21744661033153534, acc: 0.949999988079071)
[2025-02-13 19:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30][root][INFO] - Training Epoch: 1/2, step 2410/7134 completed (loss: 0.16717778146266937, acc: 0.9577465057373047)
[2025-02-13 19:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30][root][INFO] - Training Epoch: 1/2, step 2411/7134 completed (loss: 0.11682385951280594, acc: 0.9613259434700012)
[2025-02-13 19:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30][root][INFO] - Training Epoch: 1/2, step 2412/7134 completed (loss: 0.11866572499275208, acc: 0.9715909361839294)
[2025-02-13 19:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31][root][INFO] - Training Epoch: 1/2, step 2413/7134 completed (loss: 0.1393965482711792, acc: 0.9545454382896423)
[2025-02-13 19:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31][root][INFO] - Training Epoch: 1/2, step 2414/7134 completed (loss: 0.1602986603975296, acc: 0.9617486596107483)
[2025-02-13 19:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31][root][INFO] - Training Epoch: 1/2, step 2415/7134 completed (loss: 0.5022907257080078, acc: 0.8841463327407837)
[2025-02-13 19:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32][root][INFO] - Training Epoch: 1/2, step 2416/7134 completed (loss: 0.2119864970445633, acc: 0.955974817276001)
[2025-02-13 19:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32][root][INFO] - Training Epoch: 1/2, step 2417/7134 completed (loss: 0.20746254920959473, acc: 0.9658536314964294)
[2025-02-13 19:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32][root][INFO] - Training Epoch: 1/2, step 2418/7134 completed (loss: 0.2723546624183655, acc: 0.9370629191398621)
[2025-02-13 19:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:33][root][INFO] - Training Epoch: 1/2, step 2419/7134 completed (loss: 0.1481454223394394, acc: 0.9815950989723206)
[2025-02-13 19:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:33][root][INFO] - Training Epoch: 1/2, step 2420/7134 completed (loss: 0.20114023983478546, acc: 0.9375)
[2025-02-13 19:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34][root][INFO] - Training Epoch: 1/2, step 2421/7134 completed (loss: 0.13456667959690094, acc: 0.9621621370315552)
[2025-02-13 19:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34][root][INFO] - Training Epoch: 1/2, step 2422/7134 completed (loss: 0.1598047912120819, acc: 0.954285740852356)
[2025-02-13 19:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34][root][INFO] - Training Epoch: 1/2, step 2423/7134 completed (loss: 0.2061033695936203, acc: 0.9523809552192688)
[2025-02-13 19:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35][root][INFO] - Training Epoch: 1/2, step 2424/7134 completed (loss: 0.21277441084384918, acc: 0.9602272510528564)
[2025-02-13 19:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35][root][INFO] - Training Epoch: 1/2, step 2425/7134 completed (loss: 0.2609913945198059, acc: 0.9382022619247437)
[2025-02-13 19:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35][root][INFO] - Training Epoch: 1/2, step 2426/7134 completed (loss: 0.13185139000415802, acc: 0.9655172228813171)
[2025-02-13 19:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36][root][INFO] - Training Epoch: 1/2, step 2427/7134 completed (loss: 0.09176055341959, acc: 0.96875)
[2025-02-13 19:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36][root][INFO] - Training Epoch: 1/2, step 2428/7134 completed (loss: 0.13194838166236877, acc: 0.9526315927505493)
[2025-02-13 19:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37][root][INFO] - Training Epoch: 1/2, step 2429/7134 completed (loss: 0.23695078492164612, acc: 0.9367088675498962)
[2025-02-13 19:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37][root][INFO] - Training Epoch: 1/2, step 2430/7134 completed (loss: 0.22182396054267883, acc: 0.9399999976158142)
[2025-02-13 19:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37][root][INFO] - Training Epoch: 1/2, step 2431/7134 completed (loss: 0.2345259040594101, acc: 0.9510489702224731)
[2025-02-13 19:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38][root][INFO] - Training Epoch: 1/2, step 2432/7134 completed (loss: 0.43680429458618164, acc: 0.9008264541625977)
[2025-02-13 19:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38][root][INFO] - Training Epoch: 1/2, step 2433/7134 completed (loss: 0.2605191171169281, acc: 0.9424460530281067)
[2025-02-13 19:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38][root][INFO] - Training Epoch: 1/2, step 2434/7134 completed (loss: 0.3829755187034607, acc: 0.9127516746520996)
[2025-02-13 19:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39][root][INFO] - Training Epoch: 1/2, step 2435/7134 completed (loss: 0.4070979654788971, acc: 0.9072847962379456)
[2025-02-13 19:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39][root][INFO] - Training Epoch: 1/2, step 2436/7134 completed (loss: 0.18249428272247314, acc: 0.9492385983467102)
[2025-02-13 19:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39][root][INFO] - Training Epoch: 1/2, step 2437/7134 completed (loss: 0.2685166895389557, acc: 0.9295774698257446)
[2025-02-13 19:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40][root][INFO] - Training Epoch: 1/2, step 2438/7134 completed (loss: 0.2733600437641144, acc: 0.9259259104728699)
[2025-02-13 19:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40][root][INFO] - Training Epoch: 1/2, step 2439/7134 completed (loss: 0.2228868007659912, acc: 0.9358288645744324)
[2025-02-13 19:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41][root][INFO] - Training Epoch: 1/2, step 2440/7134 completed (loss: 0.19523954391479492, acc: 0.9545454382896423)
[2025-02-13 19:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41][root][INFO] - Training Epoch: 1/2, step 2441/7134 completed (loss: 0.6369271278381348, acc: 0.8640000224113464)
[2025-02-13 19:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41][root][INFO] - Training Epoch: 1/2, step 2442/7134 completed (loss: 0.41715681552886963, acc: 0.9289940595626831)
[2025-02-13 19:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42][root][INFO] - Training Epoch: 1/2, step 2443/7134 completed (loss: 0.3425583243370056, acc: 0.9263803958892822)
[2025-02-13 19:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42][root][INFO] - Training Epoch: 1/2, step 2444/7134 completed (loss: 0.28353041410446167, acc: 0.9224806427955627)
[2025-02-13 19:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42][root][INFO] - Training Epoch: 1/2, step 2445/7134 completed (loss: 0.24900656938552856, acc: 0.9554139971733093)
[2025-02-13 19:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43][root][INFO] - Training Epoch: 1/2, step 2446/7134 completed (loss: 0.442130982875824, acc: 0.8831169009208679)
[2025-02-13 19:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43][root][INFO] - Training Epoch: 1/2, step 2447/7134 completed (loss: 0.18723517656326294, acc: 0.966292142868042)
[2025-02-13 19:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43][root][INFO] - Training Epoch: 1/2, step 2448/7134 completed (loss: 0.22252514958381653, acc: 0.9328858852386475)
[2025-02-13 19:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44][root][INFO] - Training Epoch: 1/2, step 2449/7134 completed (loss: 0.41129156947135925, acc: 0.9127907156944275)
[2025-02-13 19:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44][root][INFO] - Training Epoch: 1/2, step 2450/7134 completed (loss: 0.19276906549930573, acc: 0.9411764740943909)
[2025-02-13 19:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45][root][INFO] - Training Epoch: 1/2, step 2451/7134 completed (loss: 0.17865924537181854, acc: 0.9572192430496216)
[2025-02-13 19:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45][root][INFO] - Training Epoch: 1/2, step 2452/7134 completed (loss: 0.21874934434890747, acc: 0.9301075339317322)
[2025-02-13 19:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45][root][INFO] - Training Epoch: 1/2, step 2453/7134 completed (loss: 0.23483256995677948, acc: 0.9378238320350647)
[2025-02-13 19:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46][root][INFO] - Training Epoch: 1/2, step 2454/7134 completed (loss: 0.20241916179656982, acc: 0.936170220375061)
[2025-02-13 19:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46][root][INFO] - Training Epoch: 1/2, step 2455/7134 completed (loss: 0.103855662047863, acc: 0.9631901979446411)
[2025-02-13 19:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46][root][INFO] - Training Epoch: 1/2, step 2456/7134 completed (loss: 0.2329992949962616, acc: 0.9430052042007446)
[2025-02-13 19:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:47][root][INFO] - Training Epoch: 1/2, step 2457/7134 completed (loss: 0.39975404739379883, acc: 0.9019607901573181)
[2025-02-13 19:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:47][root][INFO] - Training Epoch: 1/2, step 2458/7134 completed (loss: 0.4659532904624939, acc: 0.90625)
[2025-02-13 19:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48][root][INFO] - Training Epoch: 1/2, step 2459/7134 completed (loss: 0.09729964286088943, acc: 0.9766082167625427)
[2025-02-13 19:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48][root][INFO] - Training Epoch: 1/2, step 2460/7134 completed (loss: 0.1908012479543686, acc: 0.9505494236946106)
[2025-02-13 19:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48][root][INFO] - Training Epoch: 1/2, step 2461/7134 completed (loss: 0.0826021209359169, acc: 0.9748427867889404)
[2025-02-13 19:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49][root][INFO] - Training Epoch: 1/2, step 2462/7134 completed (loss: 0.3287913203239441, acc: 0.9515151381492615)
[2025-02-13 19:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49][root][INFO] - Training Epoch: 1/2, step 2463/7134 completed (loss: 0.0909237489104271, acc: 0.9750000238418579)
[2025-02-13 19:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49][root][INFO] - Training Epoch: 1/2, step 2464/7134 completed (loss: 0.2855232059955597, acc: 0.949999988079071)
[2025-02-13 19:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50][root][INFO] - Training Epoch: 1/2, step 2465/7134 completed (loss: 0.13585031032562256, acc: 0.9664804339408875)
[2025-02-13 19:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50][root][INFO] - Training Epoch: 1/2, step 2466/7134 completed (loss: 0.12856319546699524, acc: 0.9640718698501587)
[2025-02-13 19:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51][root][INFO] - Training Epoch: 1/2, step 2467/7134 completed (loss: 0.35376831889152527, acc: 0.9269663095474243)
[2025-02-13 19:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51][root][INFO] - Training Epoch: 1/2, step 2468/7134 completed (loss: 0.20491112768650055, acc: 0.9534883499145508)
[2025-02-13 19:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51][root][INFO] - Training Epoch: 1/2, step 2469/7134 completed (loss: 0.08524753153324127, acc: 0.9883720874786377)
[2025-02-13 19:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52][root][INFO] - Training Epoch: 1/2, step 2470/7134 completed (loss: 0.24484653770923615, acc: 0.957446813583374)
[2025-02-13 19:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52][root][INFO] - Training Epoch: 1/2, step 2471/7134 completed (loss: 0.3248991072177887, acc: 0.936170220375061)
[2025-02-13 19:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52][root][INFO] - Training Epoch: 1/2, step 2472/7134 completed (loss: 0.08643921464681625, acc: 0.9729729890823364)
[2025-02-13 19:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53][root][INFO] - Training Epoch: 1/2, step 2473/7134 completed (loss: 0.13808423280715942, acc: 0.9719626307487488)
[2025-02-13 19:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53][root][INFO] - Training Epoch: 1/2, step 2474/7134 completed (loss: 0.06378290057182312, acc: 0.9878787994384766)
[2025-02-13 19:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53][root][INFO] - Training Epoch: 1/2, step 2475/7134 completed (loss: 0.07471570372581482, acc: 0.9918032884597778)
[2025-02-13 19:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54][root][INFO] - Training Epoch: 1/2, step 2476/7134 completed (loss: 0.08049177378416061, acc: 0.9824561476707458)
[2025-02-13 19:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54][root][INFO] - Training Epoch: 1/2, step 2477/7134 completed (loss: 0.07737623900175095, acc: 0.9753086566925049)
[2025-02-13 19:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55][root][INFO] - Training Epoch: 1/2, step 2478/7134 completed (loss: 0.09664665907621384, acc: 0.9731543660163879)
[2025-02-13 19:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55][root][INFO] - Training Epoch: 1/2, step 2479/7134 completed (loss: 0.47007179260253906, acc: 0.8775510191917419)
[2025-02-13 19:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55][root][INFO] - Training Epoch: 1/2, step 2480/7134 completed (loss: 0.10265589505434036, acc: 0.9711538553237915)
[2025-02-13 19:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56][root][INFO] - Training Epoch: 1/2, step 2481/7134 completed (loss: 0.2174624353647232, acc: 0.9519230723381042)
[2025-02-13 19:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56][root][INFO] - Training Epoch: 1/2, step 2482/7134 completed (loss: 0.2398284375667572, acc: 0.9220778942108154)
[2025-02-13 19:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56][root][INFO] - Training Epoch: 1/2, step 2483/7134 completed (loss: 0.0873950943350792, acc: 0.9820359349250793)
[2025-02-13 19:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:57][root][INFO] - Training Epoch: 1/2, step 2484/7134 completed (loss: 0.1925671100616455, acc: 0.9638554453849792)
[2025-02-13 19:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:57][root][INFO] - Training Epoch: 1/2, step 2485/7134 completed (loss: 0.08945802599191666, acc: 0.9851852059364319)
[2025-02-13 19:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58][root][INFO] - Training Epoch: 1/2, step 2486/7134 completed (loss: 0.12344808131456375, acc: 0.9794520735740662)
[2025-02-13 19:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58][root][INFO] - Training Epoch: 1/2, step 2487/7134 completed (loss: 0.40020546317100525, acc: 0.9097222089767456)
[2025-02-13 19:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58][root][INFO] - Training Epoch: 1/2, step 2488/7134 completed (loss: 0.19846634566783905, acc: 0.9575757384300232)
[2025-02-13 19:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59][root][INFO] - Training Epoch: 1/2, step 2489/7134 completed (loss: 0.12831808626651764, acc: 0.9644970297813416)
[2025-02-13 19:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59][root][INFO] - Training Epoch: 1/2, step 2490/7134 completed (loss: 0.1109435185790062, acc: 0.9805194735527039)
[2025-02-13 19:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59][root][INFO] - Training Epoch: 1/2, step 2491/7134 completed (loss: 0.4749061167240143, acc: 0.9029850959777832)
[2025-02-13 19:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:00][root][INFO] - Training Epoch: 1/2, step 2492/7134 completed (loss: 0.20885612070560455, acc: 0.9685534834861755)
[2025-02-13 19:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:00][root][INFO] - Training Epoch: 1/2, step 2493/7134 completed (loss: 0.1819809228181839, acc: 0.9523809552192688)
[2025-02-13 19:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01][root][INFO] - Training Epoch: 1/2, step 2494/7134 completed (loss: 0.21193630993366241, acc: 0.9320987462997437)
[2025-02-13 19:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01][root][INFO] - Training Epoch: 1/2, step 2495/7134 completed (loss: 0.2522491216659546, acc: 0.9411764740943909)
[2025-02-13 19:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01][root][INFO] - Training Epoch: 1/2, step 2496/7134 completed (loss: 0.22099798917770386, acc: 0.949367105960846)
[2025-02-13 19:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02][root][INFO] - Training Epoch: 1/2, step 2497/7134 completed (loss: 0.17994244396686554, acc: 0.954023003578186)
[2025-02-13 19:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02][root][INFO] - Training Epoch: 1/2, step 2498/7134 completed (loss: 0.12285122275352478, acc: 0.9714285731315613)
[2025-02-13 19:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02][root][INFO] - Training Epoch: 1/2, step 2499/7134 completed (loss: 0.1328873485326767, acc: 0.9698795080184937)
[2025-02-13 19:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03][root][INFO] - Training Epoch: 1/2, step 2500/7134 completed (loss: 0.1463170200586319, acc: 0.9864864945411682)
[2025-02-13 19:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03][root][INFO] - Training Epoch: 1/2, step 2501/7134 completed (loss: 0.26253437995910645, acc: 0.9438202381134033)
[2025-02-13 19:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03][root][INFO] - Training Epoch: 1/2, step 2502/7134 completed (loss: 0.15449592471122742, acc: 0.9743589758872986)
[2025-02-13 19:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04][root][INFO] - Training Epoch: 1/2, step 2503/7134 completed (loss: 0.1231122612953186, acc: 0.95652174949646)
[2025-02-13 19:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04][root][INFO] - Training Epoch: 1/2, step 2504/7134 completed (loss: 0.23834897577762604, acc: 0.9303797483444214)
[2025-02-13 19:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04][root][INFO] - Training Epoch: 1/2, step 2505/7134 completed (loss: 0.1409318447113037, acc: 0.9651162624359131)
[2025-02-13 19:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05][root][INFO] - Training Epoch: 1/2, step 2506/7134 completed (loss: 0.14898957312107086, acc: 0.9508196711540222)
[2025-02-13 19:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05][root][INFO] - Training Epoch: 1/2, step 2507/7134 completed (loss: 0.06215197592973709, acc: 0.9833333492279053)
[2025-02-13 19:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06][root][INFO] - Training Epoch: 1/2, step 2508/7134 completed (loss: 0.09697499126195908, acc: 0.9717513918876648)
[2025-02-13 19:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06][root][INFO] - Training Epoch: 1/2, step 2509/7134 completed (loss: 0.09102603048086166, acc: 0.9764705896377563)
[2025-02-13 19:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06][root][INFO] - Training Epoch: 1/2, step 2510/7134 completed (loss: 0.07889939844608307, acc: 0.9785714149475098)
[2025-02-13 19:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07][root][INFO] - Training Epoch: 1/2, step 2511/7134 completed (loss: 0.2202473133802414, acc: 0.9562841653823853)
[2025-02-13 19:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07][root][INFO] - Training Epoch: 1/2, step 2512/7134 completed (loss: 0.21315303444862366, acc: 0.9235293865203857)
[2025-02-13 19:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07][root][INFO] - Training Epoch: 1/2, step 2513/7134 completed (loss: 0.14349614083766937, acc: 0.9623655676841736)
[2025-02-13 19:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08][root][INFO] - Training Epoch: 1/2, step 2514/7134 completed (loss: 0.14852668344974518, acc: 0.9554139971733093)
[2025-02-13 19:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08][root][INFO] - Training Epoch: 1/2, step 2515/7134 completed (loss: 0.10972481220960617, acc: 0.9870129823684692)
[2025-02-13 19:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08][root][INFO] - Training Epoch: 1/2, step 2516/7134 completed (loss: 0.17216959595680237, acc: 0.9720670580863953)
[2025-02-13 19:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09][root][INFO] - Training Epoch: 1/2, step 2517/7134 completed (loss: 0.1831488013267517, acc: 0.9666666388511658)
[2025-02-13 19:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09][root][INFO] - Training Epoch: 1/2, step 2518/7134 completed (loss: 0.1811748445034027, acc: 0.9477124214172363)
[2025-02-13 19:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10][root][INFO] - Training Epoch: 1/2, step 2519/7134 completed (loss: 0.05834806337952614, acc: 0.9870967864990234)
[2025-02-13 19:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10][root][INFO] - Training Epoch: 1/2, step 2520/7134 completed (loss: 0.14376656711101532, acc: 0.9539473652839661)
[2025-02-13 19:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10][root][INFO] - Training Epoch: 1/2, step 2521/7134 completed (loss: 0.3419989347457886, acc: 0.9285714030265808)
[2025-02-13 19:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11][root][INFO] - Training Epoch: 1/2, step 2522/7134 completed (loss: 0.1232493445277214, acc: 0.9586206674575806)
[2025-02-13 19:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11][root][INFO] - Training Epoch: 1/2, step 2523/7134 completed (loss: 0.1866072714328766, acc: 0.9570552110671997)
[2025-02-13 19:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11][root][INFO] - Training Epoch: 1/2, step 2524/7134 completed (loss: 0.10936824232339859, acc: 0.957317054271698)
[2025-02-13 19:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12][root][INFO] - Training Epoch: 1/2, step 2525/7134 completed (loss: 0.1517692357301712, acc: 0.9452054500579834)
[2025-02-13 19:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12][root][INFO] - Training Epoch: 1/2, step 2526/7134 completed (loss: 0.08550114929676056, acc: 0.9898989796638489)
[2025-02-13 19:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12][root][INFO] - Training Epoch: 1/2, step 2527/7134 completed (loss: 0.08825388550758362, acc: 0.9726775884628296)
[2025-02-13 19:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:13][root][INFO] - Training Epoch: 1/2, step 2528/7134 completed (loss: 0.07738564908504486, acc: 0.9879518151283264)
[2025-02-13 19:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:13][root][INFO] - Training Epoch: 1/2, step 2529/7134 completed (loss: 0.08126863837242126, acc: 0.9821428656578064)
[2025-02-13 19:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14][root][INFO] - Training Epoch: 1/2, step 2530/7134 completed (loss: 0.10070843994617462, acc: 0.9775280952453613)
[2025-02-13 19:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14][root][INFO] - Training Epoch: 1/2, step 2531/7134 completed (loss: 0.04874134063720703, acc: 0.9879518151283264)
[2025-02-13 19:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14][root][INFO] - Training Epoch: 1/2, step 2532/7134 completed (loss: 0.10812395811080933, acc: 0.9733333587646484)
[2025-02-13 19:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15][root][INFO] - Training Epoch: 1/2, step 2533/7134 completed (loss: 0.15023204684257507, acc: 0.9615384340286255)
[2025-02-13 19:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15][root][INFO] - Training Epoch: 1/2, step 2534/7134 completed (loss: 0.1446586400270462, acc: 0.9776536226272583)
[2025-02-13 19:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15][root][INFO] - Training Epoch: 1/2, step 2535/7134 completed (loss: 0.07753638923168182, acc: 0.9830508232116699)
[2025-02-13 19:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:16][root][INFO] - Training Epoch: 1/2, step 2536/7134 completed (loss: 0.23910382390022278, acc: 0.9274611473083496)
[2025-02-13 19:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:16][root][INFO] - Training Epoch: 1/2, step 2537/7134 completed (loss: 0.24045996367931366, acc: 0.931506872177124)
[2025-02-13 19:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17][root][INFO] - Training Epoch: 1/2, step 2538/7134 completed (loss: 0.23916327953338623, acc: 0.954023003578186)
[2025-02-13 19:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17][root][INFO] - Training Epoch: 1/2, step 2539/7134 completed (loss: 0.18279747664928436, acc: 0.969924807548523)
[2025-02-13 19:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17][root][INFO] - Training Epoch: 1/2, step 2540/7134 completed (loss: 0.20161758363246918, acc: 0.9536423683166504)
[2025-02-13 19:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18][root][INFO] - Training Epoch: 1/2, step 2541/7134 completed (loss: 0.07228175550699234, acc: 1.0)
[2025-02-13 19:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18][root][INFO] - Training Epoch: 1/2, step 2542/7134 completed (loss: 0.19440676271915436, acc: 0.9437500238418579)
[2025-02-13 19:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18][root][INFO] - Training Epoch: 1/2, step 2543/7134 completed (loss: 0.2196521908044815, acc: 0.9622641801834106)
[2025-02-13 19:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19][root][INFO] - Training Epoch: 1/2, step 2544/7134 completed (loss: 0.17503397166728973, acc: 0.9642857313156128)
[2025-02-13 19:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19][root][INFO] - Training Epoch: 1/2, step 2545/7134 completed (loss: 0.21375501155853271, acc: 0.9179104566574097)
[2025-02-13 19:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20][root][INFO] - Training Epoch: 1/2, step 2546/7134 completed (loss: 0.40071871876716614, acc: 0.8775510191917419)
[2025-02-13 19:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20][root][INFO] - Training Epoch: 1/2, step 2547/7134 completed (loss: 0.3084373474121094, acc: 0.9280575513839722)
[2025-02-13 19:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20][root][INFO] - Training Epoch: 1/2, step 2548/7134 completed (loss: 0.09364496171474457, acc: 0.9734513163566589)
[2025-02-13 19:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21][root][INFO] - Training Epoch: 1/2, step 2549/7134 completed (loss: 0.1756128966808319, acc: 0.9268292784690857)
[2025-02-13 19:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21][root][INFO] - Training Epoch: 1/2, step 2550/7134 completed (loss: 0.12176688760519028, acc: 0.965753436088562)
[2025-02-13 19:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21][root][INFO] - Training Epoch: 1/2, step 2551/7134 completed (loss: 0.24285128712654114, acc: 0.9492753744125366)
[2025-02-13 19:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22][root][INFO] - Training Epoch: 1/2, step 2552/7134 completed (loss: 0.16868220269680023, acc: 0.949367105960846)
[2025-02-13 19:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22][root][INFO] - Training Epoch: 1/2, step 2553/7134 completed (loss: 0.25247153639793396, acc: 0.9605262875556946)
[2025-02-13 19:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23][root][INFO] - Training Epoch: 1/2, step 2554/7134 completed (loss: 0.2659369111061096, acc: 0.9370078444480896)
[2025-02-13 19:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23][root][INFO] - Training Epoch: 1/2, step 2555/7134 completed (loss: 0.193182110786438, acc: 0.9459459185600281)
[2025-02-13 19:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23][root][INFO] - Training Epoch: 1/2, step 2556/7134 completed (loss: 0.18393898010253906, acc: 0.9800000190734863)
[2025-02-13 19:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24][root][INFO] - Training Epoch: 1/2, step 2557/7134 completed (loss: 0.16865725815296173, acc: 0.9727891087532043)
[2025-02-13 19:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24][root][INFO] - Training Epoch: 1/2, step 2558/7134 completed (loss: 0.06164804846048355, acc: 0.985401451587677)
[2025-02-13 19:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24][root][INFO] - Training Epoch: 1/2, step 2559/7134 completed (loss: 0.09212352335453033, acc: 0.9750000238418579)
[2025-02-13 19:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25][root][INFO] - Training Epoch: 1/2, step 2560/7134 completed (loss: 0.15839816629886627, acc: 0.9662162065505981)
[2025-02-13 19:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25][root][INFO] - Training Epoch: 1/2, step 2561/7134 completed (loss: 0.18091417849063873, acc: 0.9479166865348816)
[2025-02-13 19:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25][root][INFO] - Training Epoch: 1/2, step 2562/7134 completed (loss: 0.11699200421571732, acc: 0.9599999785423279)
[2025-02-13 19:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:26][root][INFO] - Training Epoch: 1/2, step 2563/7134 completed (loss: 0.15877334773540497, acc: 0.9526627063751221)
[2025-02-13 19:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:26][root][INFO] - Training Epoch: 1/2, step 2564/7134 completed (loss: 0.38738176226615906, acc: 0.9064748287200928)
[2025-02-13 19:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27][root][INFO] - Training Epoch: 1/2, step 2565/7134 completed (loss: 0.11469938606023788, acc: 0.9640287756919861)
[2025-02-13 19:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27][root][INFO] - Training Epoch: 1/2, step 2566/7134 completed (loss: 0.15003232657909393, acc: 0.9518072009086609)
[2025-02-13 19:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27][root][INFO] - Training Epoch: 1/2, step 2567/7134 completed (loss: 0.12058793008327484, acc: 0.9624060392379761)
[2025-02-13 19:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28][root][INFO] - Training Epoch: 1/2, step 2568/7134 completed (loss: 0.17982730269432068, acc: 0.9551281929016113)
[2025-02-13 19:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28][root][INFO] - Training Epoch: 1/2, step 2569/7134 completed (loss: 0.08179187774658203, acc: 0.9929577708244324)
[2025-02-13 19:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28][root][INFO] - Training Epoch: 1/2, step 2570/7134 completed (loss: 0.15242238342761993, acc: 0.9602649211883545)
[2025-02-13 19:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29][root][INFO] - Training Epoch: 1/2, step 2571/7134 completed (loss: 0.12306076288223267, acc: 0.9754098653793335)
[2025-02-13 19:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29][root][INFO] - Training Epoch: 1/2, step 2572/7134 completed (loss: 0.11893686652183533, acc: 0.9659863710403442)
[2025-02-13 19:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29][root][INFO] - Training Epoch: 1/2, step 2573/7134 completed (loss: 0.10885334014892578, acc: 0.9784172773361206)
[2025-02-13 19:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30][root][INFO] - Training Epoch: 1/2, step 2574/7134 completed (loss: 0.15234801173210144, acc: 0.9595959782600403)
[2025-02-13 19:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30][root][INFO] - Training Epoch: 1/2, step 2575/7134 completed (loss: 0.08863335102796555, acc: 0.9836065769195557)
[2025-02-13 19:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31][root][INFO] - Training Epoch: 1/2, step 2576/7134 completed (loss: 0.08190613985061646, acc: 0.9837398529052734)
[2025-02-13 19:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31][root][INFO] - Training Epoch: 1/2, step 2577/7134 completed (loss: 0.277043879032135, acc: 0.9527559280395508)
[2025-02-13 19:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31][root][INFO] - Training Epoch: 1/2, step 2578/7134 completed (loss: 0.3508075475692749, acc: 0.9215686321258545)
[2025-02-13 19:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32][root][INFO] - Training Epoch: 1/2, step 2579/7134 completed (loss: 0.2879163920879364, acc: 0.9230769276618958)
[2025-02-13 19:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32][root][INFO] - Training Epoch: 1/2, step 2580/7134 completed (loss: 0.23936325311660767, acc: 0.9364162087440491)
[2025-02-13 19:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32][root][INFO] - Training Epoch: 1/2, step 2581/7134 completed (loss: 0.306776225566864, acc: 0.9209039807319641)
[2025-02-13 19:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33][root][INFO] - Training Epoch: 1/2, step 2582/7134 completed (loss: 0.3059687912464142, acc: 0.9152542352676392)
[2025-02-13 19:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33][root][INFO] - Training Epoch: 1/2, step 2583/7134 completed (loss: 0.39184027910232544, acc: 0.9080459475517273)
[2025-02-13 19:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33][root][INFO] - Training Epoch: 1/2, step 2584/7134 completed (loss: 0.5732783079147339, acc: 0.8936170339584351)
[2025-02-13 19:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34][root][INFO] - Training Epoch: 1/2, step 2585/7134 completed (loss: 0.29771560430526733, acc: 0.9470198750495911)
[2025-02-13 19:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34][root][INFO] - Training Epoch: 1/2, step 2586/7134 completed (loss: 0.275528222322464, acc: 0.9230769276618958)
[2025-02-13 19:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35][root][INFO] - Training Epoch: 1/2, step 2587/7134 completed (loss: 0.15677639842033386, acc: 0.96875)
[2025-02-13 19:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35][root][INFO] - Training Epoch: 1/2, step 2588/7134 completed (loss: 0.12425979226827621, acc: 0.9591836929321289)
[2025-02-13 19:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35][root][INFO] - Training Epoch: 1/2, step 2589/7134 completed (loss: 0.4227257966995239, acc: 0.8809523582458496)
[2025-02-13 19:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36][root][INFO] - Training Epoch: 1/2, step 2590/7134 completed (loss: 0.26904022693634033, acc: 0.9259259104728699)
[2025-02-13 19:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36][root][INFO] - Training Epoch: 1/2, step 2591/7134 completed (loss: 0.4594612717628479, acc: 0.871345043182373)
[2025-02-13 19:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36][root][INFO] - Training Epoch: 1/2, step 2592/7134 completed (loss: 0.3171297013759613, acc: 0.9385474920272827)
[2025-02-13 19:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37][root][INFO] - Training Epoch: 1/2, step 2593/7134 completed (loss: 0.17852036654949188, acc: 0.9817073345184326)
[2025-02-13 19:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37][root][INFO] - Training Epoch: 1/2, step 2594/7134 completed (loss: 0.3627089262008667, acc: 0.9084967374801636)
[2025-02-13 19:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38][root][INFO] - Training Epoch: 1/2, step 2595/7134 completed (loss: 0.17495481669902802, acc: 0.957317054271698)
[2025-02-13 19:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38][root][INFO] - Training Epoch: 1/2, step 2596/7134 completed (loss: 0.23924289643764496, acc: 0.9235293865203857)
[2025-02-13 19:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38][root][INFO] - Training Epoch: 1/2, step 2597/7134 completed (loss: 0.2769813537597656, acc: 0.9209039807319641)
[2025-02-13 19:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39][root][INFO] - Training Epoch: 1/2, step 2598/7134 completed (loss: 0.3498481214046478, acc: 0.9006622433662415)
[2025-02-13 19:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39][root][INFO] - Training Epoch: 1/2, step 2599/7134 completed (loss: 0.053326401859521866, acc: 0.9874213933944702)
[2025-02-13 19:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39][root][INFO] - Training Epoch: 1/2, step 2600/7134 completed (loss: 0.34264227747917175, acc: 0.949438214302063)
[2025-02-13 19:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40][root][INFO] - Training Epoch: 1/2, step 2601/7134 completed (loss: 0.31027674674987793, acc: 0.9290322661399841)
[2025-02-13 19:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40][root][INFO] - Training Epoch: 1/2, step 2602/7134 completed (loss: 0.16662000119686127, acc: 0.9642857313156128)
[2025-02-13 19:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40][root][INFO] - Training Epoch: 1/2, step 2603/7134 completed (loss: 0.2338581532239914, acc: 0.9281045794487)
[2025-02-13 19:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:41][root][INFO] - Training Epoch: 1/2, step 2604/7134 completed (loss: 0.19301770627498627, acc: 0.9365079402923584)
[2025-02-13 19:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:41][root][INFO] - Training Epoch: 1/2, step 2605/7134 completed (loss: 0.27211740612983704, acc: 0.9469026327133179)
[2025-02-13 19:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42][root][INFO] - Training Epoch: 1/2, step 2606/7134 completed (loss: 0.2595076560974121, acc: 0.95333331823349)
[2025-02-13 19:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42][root][INFO] - Training Epoch: 1/2, step 2607/7134 completed (loss: 0.1771782487630844, acc: 0.9382022619247437)
[2025-02-13 19:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42][root][INFO] - Training Epoch: 1/2, step 2608/7134 completed (loss: 0.48145225644111633, acc: 0.8994709253311157)
[2025-02-13 19:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43][root][INFO] - Training Epoch: 1/2, step 2609/7134 completed (loss: 0.39859244227409363, acc: 0.9215686321258545)
[2025-02-13 19:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43][root][INFO] - Training Epoch: 1/2, step 2610/7134 completed (loss: 0.22397756576538086, acc: 0.971222996711731)
[2025-02-13 19:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43][root][INFO] - Training Epoch: 1/2, step 2611/7134 completed (loss: 0.4896602928638458, acc: 0.910614550113678)
[2025-02-13 19:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44][root][INFO] - Training Epoch: 1/2, step 2612/7134 completed (loss: 0.3764205873012543, acc: 0.8949999809265137)
[2025-02-13 19:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44][root][INFO] - Training Epoch: 1/2, step 2613/7134 completed (loss: 0.2941223680973053, acc: 0.9047619104385376)
[2025-02-13 19:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44][root][INFO] - Training Epoch: 1/2, step 2614/7134 completed (loss: 0.44758254289627075, acc: 0.8670212626457214)
[2025-02-13 19:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45][root][INFO] - Training Epoch: 1/2, step 2615/7134 completed (loss: 0.2735540568828583, acc: 0.932584285736084)
[2025-02-13 19:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45][root][INFO] - Training Epoch: 1/2, step 2616/7134 completed (loss: 0.4332154393196106, acc: 0.9327731132507324)
[2025-02-13 19:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45][root][INFO] - Training Epoch: 1/2, step 2617/7134 completed (loss: 0.49879586696624756, acc: 0.8730964660644531)
[2025-02-13 19:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46][root][INFO] - Training Epoch: 1/2, step 2618/7134 completed (loss: 0.20298175513744354, acc: 0.9230769276618958)
[2025-02-13 19:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46][root][INFO] - Training Epoch: 1/2, step 2619/7134 completed (loss: 0.233024463057518, acc: 0.9420289993286133)
[2025-02-13 19:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47][root][INFO] - Training Epoch: 1/2, step 2620/7134 completed (loss: 0.2856196463108063, acc: 0.9047619104385376)
[2025-02-13 19:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47][root][INFO] - Training Epoch: 1/2, step 2621/7134 completed (loss: 0.3700210154056549, acc: 0.9090909361839294)
[2025-02-13 19:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47][root][INFO] - Training Epoch: 1/2, step 2622/7134 completed (loss: 0.26400473713874817, acc: 0.9178082346916199)
[2025-02-13 19:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48][root][INFO] - Training Epoch: 1/2, step 2623/7134 completed (loss: 0.38390257954597473, acc: 0.9128440618515015)
[2025-02-13 19:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48][root][INFO] - Training Epoch: 1/2, step 2624/7134 completed (loss: 0.42609626054763794, acc: 0.8952381014823914)
[2025-02-13 19:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48][root][INFO] - Training Epoch: 1/2, step 2625/7134 completed (loss: 0.29352885484695435, acc: 0.9226190447807312)
[2025-02-13 19:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:49][root][INFO] - Training Epoch: 1/2, step 2626/7134 completed (loss: 0.2361074686050415, acc: 0.9425287246704102)
[2025-02-13 19:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:49][root][INFO] - Training Epoch: 1/2, step 2627/7134 completed (loss: 0.2345832884311676, acc: 0.9509202241897583)
[2025-02-13 19:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50][root][INFO] - Training Epoch: 1/2, step 2628/7134 completed (loss: 0.27739235758781433, acc: 0.9388889074325562)
[2025-02-13 19:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50][root][INFO] - Training Epoch: 1/2, step 2629/7134 completed (loss: 0.1569649577140808, acc: 0.9580838084220886)
[2025-02-13 19:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50][root][INFO] - Training Epoch: 1/2, step 2630/7134 completed (loss: 0.23460905253887177, acc: 0.9275362491607666)
[2025-02-13 19:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51][root][INFO] - Training Epoch: 1/2, step 2631/7134 completed (loss: 0.21773579716682434, acc: 0.9441624283790588)
[2025-02-13 19:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51][root][INFO] - Training Epoch: 1/2, step 2632/7134 completed (loss: 0.13394179940223694, acc: 0.9652174115180969)
[2025-02-13 19:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51][root][INFO] - Training Epoch: 1/2, step 2633/7134 completed (loss: 0.3322930932044983, acc: 0.906593382358551)
[2025-02-13 19:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:52][root][INFO] - Training Epoch: 1/2, step 2634/7134 completed (loss: 0.0996498391032219, acc: 0.976331353187561)
[2025-02-13 19:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:52][root][INFO] - Training Epoch: 1/2, step 2635/7134 completed (loss: 0.4142872989177704, acc: 0.9222221970558167)
[2025-02-13 19:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53][root][INFO] - Training Epoch: 1/2, step 2636/7134 completed (loss: 0.389389306306839, acc: 0.910614550113678)
[2025-02-13 19:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53][root][INFO] - Training Epoch: 1/2, step 2637/7134 completed (loss: 0.6181129217147827, acc: 0.8773006200790405)
[2025-02-13 19:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53][root][INFO] - Training Epoch: 1/2, step 2638/7134 completed (loss: 0.4140138328075409, acc: 0.9054726362228394)
[2025-02-13 19:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54][root][INFO] - Training Epoch: 1/2, step 2639/7134 completed (loss: 0.6321574449539185, acc: 0.8571428656578064)
[2025-02-13 19:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54][root][INFO] - Training Epoch: 1/2, step 2640/7134 completed (loss: 0.5099263191223145, acc: 0.9154228568077087)
[2025-02-13 19:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54][root][INFO] - Training Epoch: 1/2, step 2641/7134 completed (loss: 0.3749549686908722, acc: 0.9142857193946838)
[2025-02-13 19:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55][root][INFO] - Training Epoch: 1/2, step 2642/7134 completed (loss: 0.812771737575531, acc: 0.8392857313156128)
[2025-02-13 19:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55][root][INFO] - Training Epoch: 1/2, step 2643/7134 completed (loss: 0.5554894804954529, acc: 0.8837209343910217)
[2025-02-13 19:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56][root][INFO] - Training Epoch: 1/2, step 2644/7134 completed (loss: 0.4349498450756073, acc: 0.9047619104385376)
[2025-02-13 19:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56][root][INFO] - Training Epoch: 1/2, step 2645/7134 completed (loss: 0.35871273279190063, acc: 0.9257143139839172)
[2025-02-13 19:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56][root][INFO] - Training Epoch: 1/2, step 2646/7134 completed (loss: 0.4679754376411438, acc: 0.9006211161613464)
[2025-02-13 19:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57][root][INFO] - Training Epoch: 1/2, step 2647/7134 completed (loss: 0.318984717130661, acc: 0.91847825050354)
[2025-02-13 19:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57][root][INFO] - Training Epoch: 1/2, step 2648/7134 completed (loss: 0.450583279132843, acc: 0.9009901285171509)
[2025-02-13 19:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58][root][INFO] - Training Epoch: 1/2, step 2649/7134 completed (loss: 0.585378110408783, acc: 0.8594594597816467)
[2025-02-13 19:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58][root][INFO] - Training Epoch: 1/2, step 2650/7134 completed (loss: 0.6020780205726624, acc: 0.841176450252533)
[2025-02-13 19:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58][root][INFO] - Training Epoch: 1/2, step 2651/7134 completed (loss: 0.22194956243038177, acc: 0.9609755873680115)
[2025-02-13 19:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59][root][INFO] - Training Epoch: 1/2, step 2652/7134 completed (loss: 0.23616163432598114, acc: 0.949438214302063)
[2025-02-13 19:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59][root][INFO] - Training Epoch: 1/2, step 2653/7134 completed (loss: 0.28517407178878784, acc: 0.9346405267715454)
[2025-02-13 19:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59][root][INFO] - Training Epoch: 1/2, step 2654/7134 completed (loss: 0.18425777554512024, acc: 0.9642857313156128)
[2025-02-13 19:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00][root][INFO] - Training Epoch: 1/2, step 2655/7134 completed (loss: 0.1964903175830841, acc: 0.9503105878829956)
[2025-02-13 19:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00][root][INFO] - Training Epoch: 1/2, step 2656/7134 completed (loss: 0.38608813285827637, acc: 0.9186046719551086)
[2025-02-13 19:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00][root][INFO] - Training Epoch: 1/2, step 2657/7134 completed (loss: 0.2958305776119232, acc: 0.9177215099334717)
[2025-02-13 19:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01][root][INFO] - Training Epoch: 1/2, step 2658/7134 completed (loss: 0.44317206740379333, acc: 0.9141414165496826)
[2025-02-13 19:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01][root][INFO] - Training Epoch: 1/2, step 2659/7134 completed (loss: 0.38205355405807495, acc: 0.9029850959777832)
[2025-02-13 19:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02][root][INFO] - Training Epoch: 1/2, step 2660/7134 completed (loss: 0.4784896969795227, acc: 0.8974359035491943)
[2025-02-13 19:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02][root][INFO] - Training Epoch: 1/2, step 2661/7134 completed (loss: 0.15761488676071167, acc: 0.9644970297813416)
[2025-02-13 19:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02][root][INFO] - Training Epoch: 1/2, step 2662/7134 completed (loss: 0.2049889713525772, acc: 0.9629629850387573)
[2025-02-13 19:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03][root][INFO] - Training Epoch: 1/2, step 2663/7134 completed (loss: 0.259597510099411, acc: 0.9326424598693848)
[2025-02-13 19:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03][root][INFO] - Training Epoch: 1/2, step 2664/7134 completed (loss: 0.2615722417831421, acc: 0.9418604373931885)
[2025-02-13 19:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03][root][INFO] - Training Epoch: 1/2, step 2665/7134 completed (loss: 0.14816808700561523, acc: 0.9638554453849792)
[2025-02-13 19:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:04][root][INFO] - Training Epoch: 1/2, step 2666/7134 completed (loss: 0.09745126217603683, acc: 0.9883720874786377)
[2025-02-13 19:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:04][root][INFO] - Training Epoch: 1/2, step 2667/7134 completed (loss: 0.07580243796110153, acc: 0.9810126423835754)
[2025-02-13 19:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05][root][INFO] - Training Epoch: 1/2, step 2668/7134 completed (loss: 0.05959606543183327, acc: 0.9942857027053833)
[2025-02-13 19:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05][root][INFO] - Training Epoch: 1/2, step 2669/7134 completed (loss: 0.06025715172290802, acc: 0.9900497794151306)
[2025-02-13 19:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05][root][INFO] - Training Epoch: 1/2, step 2670/7134 completed (loss: 0.12105204910039902, acc: 0.9714285731315613)
[2025-02-13 19:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06][root][INFO] - Training Epoch: 1/2, step 2671/7134 completed (loss: 0.1381043791770935, acc: 0.954285740852356)
[2025-02-13 19:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06][root][INFO] - Training Epoch: 1/2, step 2672/7134 completed (loss: 0.05817628279328346, acc: 0.9879518151283264)
[2025-02-13 19:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06][root][INFO] - Training Epoch: 1/2, step 2673/7134 completed (loss: 0.09318797290325165, acc: 0.9723756909370422)
[2025-02-13 19:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07][root][INFO] - Training Epoch: 1/2, step 2674/7134 completed (loss: 0.14439649879932404, acc: 0.9629629850387573)
[2025-02-13 19:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07][root][INFO] - Training Epoch: 1/2, step 2675/7134 completed (loss: 0.04193045198917389, acc: 0.9878787994384766)
[2025-02-13 19:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08][root][INFO] - Training Epoch: 1/2, step 2676/7134 completed (loss: 0.08550617098808289, acc: 0.9772727489471436)
[2025-02-13 19:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08][root][INFO] - Training Epoch: 1/2, step 2677/7134 completed (loss: 0.06606126576662064, acc: 0.9693251252174377)
[2025-02-13 19:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08][root][INFO] - Training Epoch: 1/2, step 2678/7134 completed (loss: 0.06056012585759163, acc: 0.9830508232116699)
[2025-02-13 19:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09][root][INFO] - Training Epoch: 1/2, step 2679/7134 completed (loss: 0.058907970786094666, acc: 0.9797979593276978)
[2025-02-13 19:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09][root][INFO] - Training Epoch: 1/2, step 2680/7134 completed (loss: 0.12376932799816132, acc: 0.9670329689979553)
[2025-02-13 19:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09][root][INFO] - Training Epoch: 1/2, step 2681/7134 completed (loss: 0.029717031866312027, acc: 0.9936708807945251)
[2025-02-13 19:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:10][root][INFO] - Training Epoch: 1/2, step 2682/7134 completed (loss: 0.13946232199668884, acc: 0.9530201554298401)
[2025-02-13 19:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:10][root][INFO] - Training Epoch: 1/2, step 2683/7134 completed (loss: 0.0570811964571476, acc: 0.9801324605941772)
[2025-02-13 19:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11][root][INFO] - Training Epoch: 1/2, step 2684/7134 completed (loss: 0.292769193649292, acc: 0.9453125)
[2025-02-13 19:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11][root][INFO] - Training Epoch: 1/2, step 2685/7134 completed (loss: 0.2102707028388977, acc: 0.9704142212867737)
[2025-02-13 19:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11][root][INFO] - Training Epoch: 1/2, step 2686/7134 completed (loss: 0.2400425672531128, acc: 0.9399999976158142)
[2025-02-13 19:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12][root][INFO] - Training Epoch: 1/2, step 2687/7134 completed (loss: 0.22739338874816895, acc: 0.9411764740943909)
[2025-02-13 19:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12][root][INFO] - Training Epoch: 1/2, step 2688/7134 completed (loss: 0.26704350113868713, acc: 0.9097744226455688)
[2025-02-13 19:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12][root][INFO] - Training Epoch: 1/2, step 2689/7134 completed (loss: 0.23537740111351013, acc: 0.934959352016449)
[2025-02-13 19:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:13][root][INFO] - Training Epoch: 1/2, step 2690/7134 completed (loss: 0.21480923891067505, acc: 0.935251772403717)
[2025-02-13 19:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:13][root][INFO] - Training Epoch: 1/2, step 2691/7134 completed (loss: 0.3862154185771942, acc: 0.9026548862457275)
[2025-02-13 19:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14][root][INFO] - Training Epoch: 1/2, step 2692/7134 completed (loss: 0.44791173934936523, acc: 0.9008264541625977)
[2025-02-13 19:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14][root][INFO] - Training Epoch: 1/2, step 2693/7134 completed (loss: 0.52106112241745, acc: 0.8938053250312805)
[2025-02-13 19:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14][root][INFO] - Training Epoch: 1/2, step 2694/7134 completed (loss: 0.3597911298274994, acc: 0.9337748289108276)
[2025-02-13 19:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15][root][INFO] - Training Epoch: 1/2, step 2695/7134 completed (loss: 0.2406739443540573, acc: 0.9555555582046509)
[2025-02-13 19:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15][root][INFO] - Training Epoch: 1/2, step 2696/7134 completed (loss: 0.2666621208190918, acc: 0.9357143044471741)
[2025-02-13 19:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15][root][INFO] - Training Epoch: 1/2, step 2697/7134 completed (loss: 0.32092946767807007, acc: 0.9155844449996948)
[2025-02-13 19:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16][root][INFO] - Training Epoch: 1/2, step 2698/7134 completed (loss: 0.28951138257980347, acc: 0.918367326259613)
[2025-02-13 19:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16][root][INFO] - Training Epoch: 1/2, step 2699/7134 completed (loss: 0.16465900838375092, acc: 0.982758641242981)
[2025-02-13 19:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16][root][INFO] - Training Epoch: 1/2, step 2700/7134 completed (loss: 0.3413497507572174, acc: 0.8999999761581421)
[2025-02-13 19:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17][root][INFO] - Training Epoch: 1/2, step 2701/7134 completed (loss: 0.2118075042963028, acc: 0.9398496150970459)
[2025-02-13 19:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17][root][INFO] - Training Epoch: 1/2, step 2702/7134 completed (loss: 0.6412146687507629, acc: 0.8703703880310059)
[2025-02-13 19:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17][root][INFO] - Training Epoch: 1/2, step 2703/7134 completed (loss: 0.06973877549171448, acc: 0.9866666793823242)
[2025-02-13 19:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:18][root][INFO] - Training Epoch: 1/2, step 2704/7134 completed (loss: 0.16365492343902588, acc: 0.9464285969734192)
[2025-02-13 19:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:18][root][INFO] - Training Epoch: 1/2, step 2705/7134 completed (loss: 0.3020104765892029, acc: 0.908450722694397)
[2025-02-13 19:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19][root][INFO] - Training Epoch: 1/2, step 2706/7134 completed (loss: 0.2045440375804901, acc: 0.9383561611175537)
[2025-02-13 19:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19][root][INFO] - Training Epoch: 1/2, step 2707/7134 completed (loss: 0.29731523990631104, acc: 0.9166666865348816)
[2025-02-13 19:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19][root][INFO] - Training Epoch: 1/2, step 2708/7134 completed (loss: 0.21351398527622223, acc: 0.9371428489685059)
[2025-02-13 19:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20][root][INFO] - Training Epoch: 1/2, step 2709/7134 completed (loss: 0.1305369883775711, acc: 0.9647887349128723)
[2025-02-13 19:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20][root][INFO] - Training Epoch: 1/2, step 2710/7134 completed (loss: 0.29542720317840576, acc: 0.8999999761581421)
[2025-02-13 19:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20][root][INFO] - Training Epoch: 1/2, step 2711/7134 completed (loss: 0.06881725788116455, acc: 0.9793814420700073)
[2025-02-13 19:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21][root][INFO] - Training Epoch: 1/2, step 2712/7134 completed (loss: 0.31457987427711487, acc: 0.914893627166748)
[2025-02-13 19:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21][root][INFO] - Training Epoch: 1/2, step 2713/7134 completed (loss: 0.25905779004096985, acc: 0.9375)
[2025-02-13 19:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22][root][INFO] - Training Epoch: 1/2, step 2714/7134 completed (loss: 0.09190002828836441, acc: 0.9852941036224365)
[2025-02-13 19:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22][root][INFO] - Training Epoch: 1/2, step 2715/7134 completed (loss: 0.09969419240951538, acc: 0.9724137783050537)
[2025-02-13 19:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22][root][INFO] - Training Epoch: 1/2, step 2716/7134 completed (loss: 0.13860833644866943, acc: 0.9651162624359131)
[2025-02-13 19:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23][root][INFO] - Training Epoch: 1/2, step 2717/7134 completed (loss: 0.2524610161781311, acc: 0.9316239356994629)
[2025-02-13 19:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23][root][INFO] - Training Epoch: 1/2, step 2718/7134 completed (loss: 0.339600145816803, acc: 0.9236111044883728)
[2025-02-13 19:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23][root][INFO] - Training Epoch: 1/2, step 2719/7134 completed (loss: 0.20737004280090332, acc: 0.9496402740478516)
[2025-02-13 19:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24][root][INFO] - Training Epoch: 1/2, step 2720/7134 completed (loss: 0.30475980043411255, acc: 0.9197080135345459)
[2025-02-13 19:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24][root][INFO] - Training Epoch: 1/2, step 2721/7134 completed (loss: 0.20580846071243286, acc: 0.9735099077224731)
[2025-02-13 19:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24][root][INFO] - Training Epoch: 1/2, step 2722/7134 completed (loss: 0.18354512751102448, acc: 0.957446813583374)
[2025-02-13 19:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25][root][INFO] - Training Epoch: 1/2, step 2723/7134 completed (loss: 0.43562522530555725, acc: 0.910179615020752)
[2025-02-13 19:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25][root][INFO] - Training Epoch: 1/2, step 2724/7134 completed (loss: 0.10839607566595078, acc: 0.9887005686759949)
[2025-02-13 19:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25][root][INFO] - Training Epoch: 1/2, step 2725/7134 completed (loss: 0.09676044434309006, acc: 0.9701492786407471)
[2025-02-13 19:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26][root][INFO] - Training Epoch: 1/2, step 2726/7134 completed (loss: 0.2914779782295227, acc: 0.934959352016449)
[2025-02-13 19:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26][root][INFO] - Training Epoch: 1/2, step 2727/7134 completed (loss: 0.10033717751502991, acc: 0.9736841917037964)
[2025-02-13 19:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26][root][INFO] - Training Epoch: 1/2, step 2728/7134 completed (loss: 1.0057802200317383, acc: 0.7555555701255798)
[2025-02-13 19:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27][root][INFO] - Training Epoch: 1/2, step 2729/7134 completed (loss: 0.6585283875465393, acc: 0.8642857074737549)
[2025-02-13 19:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27][root][INFO] - Training Epoch: 1/2, step 2730/7134 completed (loss: 0.6423873901367188, acc: 0.8495575189590454)
[2025-02-13 19:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27][root][INFO] - Training Epoch: 1/2, step 2731/7134 completed (loss: 0.44237518310546875, acc: 0.893203854560852)
[2025-02-13 19:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28][root][INFO] - Training Epoch: 1/2, step 2732/7134 completed (loss: 0.49277639389038086, acc: 0.8774510025978088)
[2025-02-13 19:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28][root][INFO] - Training Epoch: 1/2, step 2733/7134 completed (loss: 0.3636881411075592, acc: 0.9277777671813965)
[2025-02-13 19:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29][root][INFO] - Training Epoch: 1/2, step 2734/7134 completed (loss: 0.4310690462589264, acc: 0.9008264541625977)
[2025-02-13 19:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29][root][INFO] - Training Epoch: 1/2, step 2735/7134 completed (loss: 0.3158183991909027, acc: 0.9512194991111755)
[2025-02-13 19:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29][root][INFO] - Training Epoch: 1/2, step 2736/7134 completed (loss: 0.5312674045562744, acc: 0.8993710875511169)
[2025-02-13 19:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30][root][INFO] - Training Epoch: 1/2, step 2737/7134 completed (loss: 0.4607003331184387, acc: 0.9285714030265808)
[2025-02-13 19:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30][root][INFO] - Training Epoch: 1/2, step 2738/7134 completed (loss: 0.343166708946228, acc: 0.8922155499458313)
[2025-02-13 19:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30][root][INFO] - Training Epoch: 1/2, step 2739/7134 completed (loss: 0.4446367621421814, acc: 0.8623188138008118)
[2025-02-13 19:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31][root][INFO] - Training Epoch: 1/2, step 2740/7134 completed (loss: 0.30806291103363037, acc: 0.9251700639724731)
[2025-02-13 19:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31][root][INFO] - Training Epoch: 1/2, step 2741/7134 completed (loss: 0.21084648370742798, acc: 0.9482758641242981)
[2025-02-13 19:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31][root][INFO] - Training Epoch: 1/2, step 2742/7134 completed (loss: 0.26422154903411865, acc: 0.9411764740943909)
[2025-02-13 19:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32][root][INFO] - Training Epoch: 1/2, step 2743/7134 completed (loss: 0.1411830335855484, acc: 0.9468085169792175)
[2025-02-13 19:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32][root][INFO] - Training Epoch: 1/2, step 2744/7134 completed (loss: 0.13424068689346313, acc: 0.9516128897666931)
[2025-02-13 19:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32][root][INFO] - Training Epoch: 1/2, step 2745/7134 completed (loss: 0.6413266658782959, acc: 0.851190447807312)
[2025-02-13 19:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:33][root][INFO] - Training Epoch: 1/2, step 2746/7134 completed (loss: 0.43964359164237976, acc: 0.8636363744735718)
[2025-02-13 19:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:33][root][INFO] - Training Epoch: 1/2, step 2747/7134 completed (loss: 0.5357575416564941, acc: 0.9074074029922485)
[2025-02-13 19:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34][root][INFO] - Training Epoch: 1/2, step 2748/7134 completed (loss: 0.22817827761173248, acc: 0.9610389471054077)
[2025-02-13 19:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34][root][INFO] - Training Epoch: 1/2, step 2749/7134 completed (loss: 0.27994558215141296, acc: 0.9278350472450256)
[2025-02-13 19:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34][root][INFO] - Training Epoch: 1/2, step 2750/7134 completed (loss: 0.23188714683055878, acc: 0.9277108311653137)
[2025-02-13 19:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35][root][INFO] - Training Epoch: 1/2, step 2751/7134 completed (loss: 0.4443642199039459, acc: 0.8828828930854797)
[2025-02-13 19:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35][root][INFO] - Training Epoch: 1/2, step 2752/7134 completed (loss: 0.29908087849617004, acc: 0.9343065619468689)
[2025-02-13 19:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35][root][INFO] - Training Epoch: 1/2, step 2753/7134 completed (loss: 0.3349374532699585, acc: 0.924369752407074)
[2025-02-13 19:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36][root][INFO] - Training Epoch: 1/2, step 2754/7134 completed (loss: 0.3588279187679291, acc: 0.925000011920929)
[2025-02-13 19:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36][root][INFO] - Training Epoch: 1/2, step 2755/7134 completed (loss: 0.2614383399486542, acc: 0.9453125)
[2025-02-13 19:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36][root][INFO] - Training Epoch: 1/2, step 2756/7134 completed (loss: 0.32039958238601685, acc: 0.9054054021835327)
[2025-02-13 19:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37][root][INFO] - Training Epoch: 1/2, step 2757/7134 completed (loss: 0.3479406535625458, acc: 0.8913043737411499)
[2025-02-13 19:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37][root][INFO] - Training Epoch: 1/2, step 2758/7134 completed (loss: 0.338346004486084, acc: 0.9016393423080444)
[2025-02-13 19:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37][root][INFO] - Training Epoch: 1/2, step 2759/7134 completed (loss: 0.16183409094810486, acc: 0.9571428298950195)
[2025-02-13 19:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38][root][INFO] - Training Epoch: 1/2, step 2760/7134 completed (loss: 0.18993277847766876, acc: 0.9555555582046509)
[2025-02-13 19:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38][root][INFO] - Training Epoch: 1/2, step 2761/7134 completed (loss: 0.42977169156074524, acc: 0.878947377204895)
[2025-02-13 19:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38][root][INFO] - Training Epoch: 1/2, step 2762/7134 completed (loss: 0.35243651270866394, acc: 0.9261083602905273)
[2025-02-13 19:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39][root][INFO] - Training Epoch: 1/2, step 2763/7134 completed (loss: 0.22529514133930206, acc: 0.9344262480735779)
[2025-02-13 19:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39][root][INFO] - Training Epoch: 1/2, step 2764/7134 completed (loss: 0.28575655817985535, acc: 0.9166666865348816)
[2025-02-13 19:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40][root][INFO] - Training Epoch: 1/2, step 2765/7134 completed (loss: 0.2586045563220978, acc: 0.929411768913269)
[2025-02-13 19:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40][root][INFO] - Training Epoch: 1/2, step 2766/7134 completed (loss: 0.08070404082536697, acc: 0.9745222926139832)
[2025-02-13 19:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40][root][INFO] - Training Epoch: 1/2, step 2767/7134 completed (loss: 0.18833065032958984, acc: 0.949999988079071)
[2025-02-13 19:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41][root][INFO] - Training Epoch: 1/2, step 2768/7134 completed (loss: 0.2409132719039917, acc: 0.9337349534034729)
[2025-02-13 19:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41][root][INFO] - Training Epoch: 1/2, step 2769/7134 completed (loss: 0.1843906044960022, acc: 0.9368420839309692)
[2025-02-13 19:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42][root][INFO] - Training Epoch: 1/2, step 2770/7134 completed (loss: 0.12704375386238098, acc: 0.9814814925193787)
[2025-02-13 19:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42][root][INFO] - Training Epoch: 1/2, step 2771/7134 completed (loss: 0.41396912932395935, acc: 0.9099525809288025)
[2025-02-13 19:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42][root][INFO] - Training Epoch: 1/2, step 2772/7134 completed (loss: 0.2631213665008545, acc: 0.925000011920929)
[2025-02-13 19:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43][root][INFO] - Training Epoch: 1/2, step 2773/7134 completed (loss: 0.19980478286743164, acc: 0.9547738432884216)
[2025-02-13 19:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43][root][INFO] - Training Epoch: 1/2, step 2774/7134 completed (loss: 0.3331416845321655, acc: 0.9268292784690857)
[2025-02-13 19:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43][root][INFO] - Training Epoch: 1/2, step 2775/7134 completed (loss: 0.23603251576423645, acc: 0.9234972596168518)
[2025-02-13 19:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:44][root][INFO] - Training Epoch: 1/2, step 2776/7134 completed (loss: 0.1622207909822464, acc: 0.9653679728507996)
[2025-02-13 19:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:44][root][INFO] - Training Epoch: 1/2, step 2777/7134 completed (loss: 0.07637207210063934, acc: 0.9828571677207947)
[2025-02-13 19:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45][root][INFO] - Training Epoch: 1/2, step 2778/7134 completed (loss: 0.28590869903564453, acc: 0.9191918969154358)
[2025-02-13 19:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45][root][INFO] - Training Epoch: 1/2, step 2779/7134 completed (loss: 0.22914792597293854, acc: 0.9406392574310303)
[2025-02-13 19:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45][root][INFO] - Training Epoch: 1/2, step 2780/7134 completed (loss: 0.3096996545791626, acc: 0.9203540086746216)
[2025-02-13 19:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46][root][INFO] - Training Epoch: 1/2, step 2781/7134 completed (loss: 0.1924876570701599, acc: 0.95652174949646)
[2025-02-13 19:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46][root][INFO] - Training Epoch: 1/2, step 2782/7134 completed (loss: 0.11447356641292572, acc: 0.9732142686843872)
[2025-02-13 19:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47][root][INFO] - Training Epoch: 1/2, step 2783/7134 completed (loss: 0.11141882091760635, acc: 0.9931972622871399)
[2025-02-13 19:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47][root][INFO] - Training Epoch: 1/2, step 2784/7134 completed (loss: 0.1566825658082962, acc: 0.9613526463508606)
[2025-02-13 19:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47][root][INFO] - Training Epoch: 1/2, step 2785/7134 completed (loss: 0.17788080871105194, acc: 0.9611650705337524)
[2025-02-13 19:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48][root][INFO] - Training Epoch: 1/2, step 2786/7134 completed (loss: 0.32127997279167175, acc: 0.9111111164093018)
[2025-02-13 19:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48][root][INFO] - Training Epoch: 1/2, step 2787/7134 completed (loss: 0.16929014027118683, acc: 0.9537572264671326)
[2025-02-13 19:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48][root][INFO] - Training Epoch: 1/2, step 2788/7134 completed (loss: 0.40033531188964844, acc: 0.8980891704559326)
[2025-02-13 19:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49][root][INFO] - Training Epoch: 1/2, step 2789/7134 completed (loss: 1.5526846647262573, acc: 0.6761904954910278)
[2025-02-13 19:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49][root][INFO] - Training Epoch: 1/2, step 2790/7134 completed (loss: 1.0582304000854492, acc: 0.7333333492279053)
[2025-02-13 19:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50][root][INFO] - Training Epoch: 1/2, step 2791/7134 completed (loss: 0.5804245471954346, acc: 0.860927164554596)
[2025-02-13 19:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50][root][INFO] - Training Epoch: 1/2, step 2792/7134 completed (loss: 0.20068368315696716, acc: 0.9534883499145508)
[2025-02-13 19:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50][root][INFO] - Training Epoch: 1/2, step 2793/7134 completed (loss: 0.15478499233722687, acc: 0.970059871673584)
[2025-02-13 19:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51][root][INFO] - Training Epoch: 1/2, step 2794/7134 completed (loss: 0.1591852307319641, acc: 0.939393937587738)
[2025-02-13 19:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51][root][INFO] - Training Epoch: 1/2, step 2795/7134 completed (loss: 0.5598839521408081, acc: 0.8703703880310059)
[2025-02-13 19:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51][root][INFO] - Training Epoch: 1/2, step 2796/7134 completed (loss: 0.33792129158973694, acc: 0.9024389982223511)
[2025-02-13 19:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52][root][INFO] - Training Epoch: 1/2, step 2797/7134 completed (loss: 0.6690587401390076, acc: 0.8048780560493469)
[2025-02-13 19:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52][root][INFO] - Training Epoch: 1/2, step 2798/7134 completed (loss: 0.3673652708530426, acc: 0.8833333253860474)
[2025-02-13 19:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52][root][INFO] - Training Epoch: 1/2, step 2799/7134 completed (loss: 0.4465089738368988, acc: 0.875)
[2025-02-13 19:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53][root][INFO] - Training Epoch: 1/2, step 2800/7134 completed (loss: 0.263324499130249, acc: 0.9270073175430298)
[2025-02-13 19:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53][root][INFO] - Training Epoch: 1/2, step 2801/7134 completed (loss: 0.31261926889419556, acc: 0.9344262480735779)
[2025-02-13 19:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53][root][INFO] - Training Epoch: 1/2, step 2802/7134 completed (loss: 0.28442639112472534, acc: 0.9145728349685669)
[2025-02-13 19:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54][root][INFO] - Training Epoch: 1/2, step 2803/7134 completed (loss: 0.14134302735328674, acc: 0.9691358208656311)
[2025-02-13 19:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54][root][INFO] - Training Epoch: 1/2, step 2804/7134 completed (loss: 0.18194404244422913, acc: 0.9653179049491882)
[2025-02-13 19:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54][root][INFO] - Training Epoch: 1/2, step 2805/7134 completed (loss: 0.09735684096813202, acc: 0.9709302186965942)
[2025-02-13 19:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:55][root][INFO] - Training Epoch: 1/2, step 2806/7134 completed (loss: 0.1605713814496994, acc: 0.970588207244873)
[2025-02-13 19:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:55][root][INFO] - Training Epoch: 1/2, step 2807/7134 completed (loss: 0.22499345242977142, acc: 0.9514563083648682)
[2025-02-13 19:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56][root][INFO] - Training Epoch: 1/2, step 2808/7134 completed (loss: 0.23172222077846527, acc: 0.9246231317520142)
[2025-02-13 19:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56][root][INFO] - Training Epoch: 1/2, step 2809/7134 completed (loss: 0.2362772524356842, acc: 0.9370078444480896)
[2025-02-13 19:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56][root][INFO] - Training Epoch: 1/2, step 2810/7134 completed (loss: 0.11421150714159012, acc: 0.9726775884628296)
[2025-02-13 19:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:57][root][INFO] - Training Epoch: 1/2, step 2811/7134 completed (loss: 0.09166324138641357, acc: 0.9779005646705627)
[2025-02-13 19:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:57][root][INFO] - Training Epoch: 1/2, step 2812/7134 completed (loss: 0.0752277821302414, acc: 0.9933775067329407)
[2025-02-13 19:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58][root][INFO] - Training Epoch: 1/2, step 2813/7134 completed (loss: 0.2014569491147995, acc: 0.9482758641242981)
[2025-02-13 19:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58][root][INFO] - Training Epoch: 1/2, step 2814/7134 completed (loss: 0.23296023905277252, acc: 0.9418604373931885)
[2025-02-13 19:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58][root][INFO] - Training Epoch: 1/2, step 2815/7134 completed (loss: 0.1813533753156662, acc: 0.9560439586639404)
[2025-02-13 19:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59][root][INFO] - Training Epoch: 1/2, step 2816/7134 completed (loss: 0.1898186355829239, acc: 0.9607843160629272)
[2025-02-13 19:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59][root][INFO] - Training Epoch: 1/2, step 2817/7134 completed (loss: 0.200853168964386, acc: 0.9492385983467102)
[2025-02-13 19:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59][root][INFO] - Training Epoch: 1/2, step 2818/7134 completed (loss: 0.15996180474758148, acc: 0.9627659320831299)
[2025-02-13 19:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00][root][INFO] - Training Epoch: 1/2, step 2819/7134 completed (loss: 0.13933168351650238, acc: 0.965753436088562)
[2025-02-13 19:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00][root][INFO] - Training Epoch: 1/2, step 2820/7134 completed (loss: 0.4297517240047455, acc: 0.9018405079841614)
[2025-02-13 19:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00][root][INFO] - Training Epoch: 1/2, step 2821/7134 completed (loss: 0.536740243434906, acc: 0.9067357778549194)
[2025-02-13 19:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:01][root][INFO] - Training Epoch: 1/2, step 2822/7134 completed (loss: 0.591106653213501, acc: 0.8918918967247009)
[2025-02-13 19:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:01][root][INFO] - Training Epoch: 1/2, step 2823/7134 completed (loss: 0.37855321168899536, acc: 0.9316770434379578)
[2025-02-13 19:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02][root][INFO] - Training Epoch: 1/2, step 2824/7134 completed (loss: 0.762563943862915, acc: 0.8553459048271179)
[2025-02-13 19:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02][root][INFO] - Training Epoch: 1/2, step 2825/7134 completed (loss: 0.2303977906703949, acc: 0.9555555582046509)
[2025-02-13 19:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02][root][INFO] - Training Epoch: 1/2, step 2826/7134 completed (loss: 0.34065914154052734, acc: 0.8908045887947083)
[2025-02-13 19:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03][root][INFO] - Training Epoch: 1/2, step 2827/7134 completed (loss: 0.21785710752010345, acc: 0.9617224931716919)
[2025-02-13 19:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03][root][INFO] - Training Epoch: 1/2, step 2828/7134 completed (loss: 0.22968123853206635, acc: 0.9556962251663208)
[2025-02-13 19:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03][root][INFO] - Training Epoch: 1/2, step 2829/7134 completed (loss: 0.22958292067050934, acc: 0.9642857313156128)
[2025-02-13 19:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:04][root][INFO] - Training Epoch: 1/2, step 2830/7134 completed (loss: 0.1198538988828659, acc: 0.9552238583564758)
[2025-02-13 19:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:04][root][INFO] - Training Epoch: 1/2, step 2831/7134 completed (loss: 0.18184266984462738, acc: 0.9459459185600281)
[2025-02-13 19:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05][root][INFO] - Training Epoch: 1/2, step 2832/7134 completed (loss: 0.31534257531166077, acc: 0.9193548560142517)
[2025-02-13 19:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05][root][INFO] - Training Epoch: 1/2, step 2833/7134 completed (loss: 0.29083698987960815, acc: 0.920634925365448)
[2025-02-13 19:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05][root][INFO] - Training Epoch: 1/2, step 2834/7134 completed (loss: 0.28979137539863586, acc: 0.9222221970558167)
[2025-02-13 19:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06][root][INFO] - Training Epoch: 1/2, step 2835/7134 completed (loss: 0.15086577832698822, acc: 0.9671052694320679)
[2025-02-13 19:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06][root][INFO] - Training Epoch: 1/2, step 2836/7134 completed (loss: 0.36010414361953735, acc: 0.9200000166893005)
[2025-02-13 19:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06][root][INFO] - Training Epoch: 1/2, step 2837/7134 completed (loss: 0.1404910385608673, acc: 0.9590643048286438)
[2025-02-13 19:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07][root][INFO] - Training Epoch: 1/2, step 2838/7134 completed (loss: 0.31251731514930725, acc: 0.931034505367279)
[2025-02-13 19:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07][root][INFO] - Training Epoch: 1/2, step 2839/7134 completed (loss: 0.28185972571372986, acc: 0.9626865386962891)
[2025-02-13 19:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08][root][INFO] - Training Epoch: 1/2, step 2840/7134 completed (loss: 0.17993195354938507, acc: 0.9647058844566345)
[2025-02-13 19:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08][root][INFO] - Training Epoch: 1/2, step 2841/7134 completed (loss: 0.1517539620399475, acc: 0.9811320900917053)
[2025-02-13 19:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08][root][INFO] - Training Epoch: 1/2, step 2842/7134 completed (loss: 0.3109045624732971, acc: 0.9322916865348816)
[2025-02-13 19:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09][root][INFO] - Training Epoch: 1/2, step 2843/7134 completed (loss: 0.19198939204216003, acc: 0.9496855139732361)
[2025-02-13 19:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09][root][INFO] - Training Epoch: 1/2, step 2844/7134 completed (loss: 0.42345547676086426, acc: 0.8804348111152649)
[2025-02-13 19:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09][root][INFO] - Training Epoch: 1/2, step 2845/7134 completed (loss: 0.3343378007411957, acc: 0.9054054021835327)
[2025-02-13 19:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:10][root][INFO] - Training Epoch: 1/2, step 2846/7134 completed (loss: 0.19788306951522827, acc: 0.9542483687400818)
[2025-02-13 19:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:10][root][INFO] - Training Epoch: 1/2, step 2847/7134 completed (loss: 0.16287986934185028, acc: 0.9745222926139832)
[2025-02-13 19:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11][root][INFO] - Training Epoch: 1/2, step 2848/7134 completed (loss: 0.23283369839191437, acc: 0.9541284441947937)
[2025-02-13 19:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11][root][INFO] - Training Epoch: 1/2, step 2849/7134 completed (loss: 0.33165979385375977, acc: 0.9200000166893005)
[2025-02-13 19:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11][root][INFO] - Training Epoch: 1/2, step 2850/7134 completed (loss: 0.5661624073982239, acc: 0.8717948794364929)
[2025-02-13 19:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12][root][INFO] - Training Epoch: 1/2, step 2851/7134 completed (loss: 0.26350876688957214, acc: 0.9185185432434082)
[2025-02-13 19:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12][root][INFO] - Training Epoch: 1/2, step 2852/7134 completed (loss: 0.23402108252048492, acc: 0.9512194991111755)
[2025-02-13 19:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12][root][INFO] - Training Epoch: 1/2, step 2853/7134 completed (loss: 0.1651156097650528, acc: 0.9591836929321289)
[2025-02-13 19:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13][root][INFO] - Training Epoch: 1/2, step 2854/7134 completed (loss: 0.2589341998100281, acc: 0.918367326259613)
[2025-02-13 19:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13][root][INFO] - Training Epoch: 1/2, step 2855/7134 completed (loss: 0.20343033969402313, acc: 0.9459459185600281)
[2025-02-13 19:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14][root][INFO] - Training Epoch: 1/2, step 2856/7134 completed (loss: 0.13149888813495636, acc: 0.9627329111099243)
[2025-02-13 19:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14][root][INFO] - Training Epoch: 1/2, step 2857/7134 completed (loss: 0.20366127789020538, acc: 0.9580838084220886)
[2025-02-13 19:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14][root][INFO] - Training Epoch: 1/2, step 2858/7134 completed (loss: 0.20027102530002594, acc: 0.9696969985961914)
[2025-02-13 19:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15][root][INFO] - Training Epoch: 1/2, step 2859/7134 completed (loss: 0.4262230396270752, acc: 0.9166666865348816)
[2025-02-13 19:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15][root][INFO] - Training Epoch: 1/2, step 2860/7134 completed (loss: 0.23303747177124023, acc: 0.9375)
[2025-02-13 19:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15][root][INFO] - Training Epoch: 1/2, step 2861/7134 completed (loss: 0.14873480796813965, acc: 0.9642857313156128)
[2025-02-13 19:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16][root][INFO] - Training Epoch: 1/2, step 2862/7134 completed (loss: 0.15419381856918335, acc: 0.9774436354637146)
[2025-02-13 19:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16][root][INFO] - Training Epoch: 1/2, step 2863/7134 completed (loss: 0.1538143903017044, acc: 0.9668508172035217)
[2025-02-13 19:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16][root][INFO] - Training Epoch: 1/2, step 2864/7134 completed (loss: 0.18994680047035217, acc: 0.9789473414421082)
[2025-02-13 19:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17][root][INFO] - Training Epoch: 1/2, step 2865/7134 completed (loss: 0.15362532436847687, acc: 0.95652174949646)
[2025-02-13 19:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17][root][INFO] - Training Epoch: 1/2, step 2866/7134 completed (loss: 0.1481088548898697, acc: 0.9548386931419373)
[2025-02-13 19:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18][root][INFO] - Training Epoch: 1/2, step 2867/7134 completed (loss: 0.15449613332748413, acc: 0.9259259104728699)
[2025-02-13 19:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18][root][INFO] - Training Epoch: 1/2, step 2868/7134 completed (loss: 0.1781770884990692, acc: 0.9647058844566345)
[2025-02-13 19:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18][root][INFO] - Training Epoch: 1/2, step 2869/7134 completed (loss: 0.2563221752643585, acc: 0.9448275566101074)
[2025-02-13 19:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19][root][INFO] - Training Epoch: 1/2, step 2870/7134 completed (loss: 0.5334166884422302, acc: 0.9090909361839294)
[2025-02-13 19:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19][root][INFO] - Training Epoch: 1/2, step 2871/7134 completed (loss: 0.22741462290287018, acc: 0.9586206674575806)
[2025-02-13 19:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19][root][INFO] - Training Epoch: 1/2, step 2872/7134 completed (loss: 0.12176788598299026, acc: 0.9646017551422119)
[2025-02-13 19:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20][root][INFO] - Training Epoch: 1/2, step 2873/7134 completed (loss: 0.13058346509933472, acc: 0.961240291595459)
[2025-02-13 19:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20][root][INFO] - Training Epoch: 1/2, step 2874/7134 completed (loss: 0.10060807317495346, acc: 0.9722222089767456)
[2025-02-13 19:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21][root][INFO] - Training Epoch: 1/2, step 2875/7134 completed (loss: 0.2839714586734772, acc: 0.9285714030265808)
[2025-02-13 19:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21][root][INFO] - Training Epoch: 1/2, step 2876/7134 completed (loss: 0.40967071056365967, acc: 0.9245283007621765)
[2025-02-13 19:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21][root][INFO] - Training Epoch: 1/2, step 2877/7134 completed (loss: 0.16021698713302612, acc: 0.9572649598121643)
[2025-02-13 19:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22][root][INFO] - Training Epoch: 1/2, step 2878/7134 completed (loss: 0.13624809682369232, acc: 0.9736841917037964)
[2025-02-13 19:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22][root][INFO] - Training Epoch: 1/2, step 2879/7134 completed (loss: 0.0741826519370079, acc: 0.9795918464660645)
[2025-02-13 19:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23][root][INFO] - Training Epoch: 1/2, step 2880/7134 completed (loss: 0.060063768178224564, acc: 0.9794520735740662)
[2025-02-13 19:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23][root][INFO] - Training Epoch: 1/2, step 2881/7134 completed (loss: 0.15340298414230347, acc: 0.9523809552192688)
[2025-02-13 19:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23][root][INFO] - Training Epoch: 1/2, step 2882/7134 completed (loss: 0.35066890716552734, acc: 0.884393036365509)
[2025-02-13 19:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24][root][INFO] - Training Epoch: 1/2, step 2883/7134 completed (loss: 0.354112446308136, acc: 0.9041916131973267)
[2025-02-13 19:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24][root][INFO] - Training Epoch: 1/2, step 2884/7134 completed (loss: 0.36726245284080505, acc: 0.8787878751754761)
[2025-02-13 19:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24][root][INFO] - Training Epoch: 1/2, step 2885/7134 completed (loss: 0.5094114542007446, acc: 0.8545454740524292)
[2025-02-13 19:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25][root][INFO] - Training Epoch: 1/2, step 2886/7134 completed (loss: 0.4202430844306946, acc: 0.8999999761581421)
[2025-02-13 19:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25][root][INFO] - Training Epoch: 1/2, step 2887/7134 completed (loss: 0.3596038818359375, acc: 0.9126637578010559)
[2025-02-13 19:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25][root][INFO] - Training Epoch: 1/2, step 2888/7134 completed (loss: 0.3026024401187897, acc: 0.9263157844543457)
[2025-02-13 19:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26][root][INFO] - Training Epoch: 1/2, step 2889/7134 completed (loss: 0.24595265090465546, acc: 0.9217877388000488)
[2025-02-13 19:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26][root][INFO] - Training Epoch: 1/2, step 2890/7134 completed (loss: 0.22397808730602264, acc: 0.9384615421295166)
[2025-02-13 19:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27][root][INFO] - Training Epoch: 1/2, step 2891/7134 completed (loss: 0.1747121512889862, acc: 0.9333333373069763)
[2025-02-13 19:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27][root][INFO] - Training Epoch: 1/2, step 2892/7134 completed (loss: 0.36116865277290344, acc: 0.9289940595626831)
[2025-02-13 19:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27][root][INFO] - Training Epoch: 1/2, step 2893/7134 completed (loss: 0.29875871539115906, acc: 0.9016393423080444)
[2025-02-13 19:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28][root][INFO] - Training Epoch: 1/2, step 2894/7134 completed (loss: 0.3615244925022125, acc: 0.8888888955116272)
[2025-02-13 19:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28][root][INFO] - Training Epoch: 1/2, step 2895/7134 completed (loss: 0.10894489288330078, acc: 0.9661017060279846)
[2025-02-13 19:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28][root][INFO] - Training Epoch: 1/2, step 2896/7134 completed (loss: 0.1922270953655243, acc: 0.9640718698501587)
[2025-02-13 19:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:29][root][INFO] - Training Epoch: 1/2, step 2897/7134 completed (loss: 0.3969905972480774, acc: 0.9172932505607605)
[2025-02-13 19:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:29][root][INFO] - Training Epoch: 1/2, step 2898/7134 completed (loss: 0.20291948318481445, acc: 0.9510869383811951)
[2025-02-13 19:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30][root][INFO] - Training Epoch: 1/2, step 2899/7134 completed (loss: 0.2572503089904785, acc: 0.9281437397003174)
[2025-02-13 19:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30][root][INFO] - Training Epoch: 1/2, step 2900/7134 completed (loss: 0.19502174854278564, acc: 0.957446813583374)
[2025-02-13 19:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30][root][INFO] - Training Epoch: 1/2, step 2901/7134 completed (loss: 0.22479844093322754, acc: 0.9324324131011963)
[2025-02-13 19:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31][root][INFO] - Training Epoch: 1/2, step 2902/7134 completed (loss: 0.18845510482788086, acc: 0.9572649598121643)
[2025-02-13 19:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31][root][INFO] - Training Epoch: 1/2, step 2903/7134 completed (loss: 0.15605272352695465, acc: 0.949438214302063)
[2025-02-13 19:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32][root][INFO] - Training Epoch: 1/2, step 2904/7134 completed (loss: 0.1158701702952385, acc: 0.9783783555030823)
[2025-02-13 19:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32][root][INFO] - Training Epoch: 1/2, step 2905/7134 completed (loss: 0.19475123286247253, acc: 0.9572192430496216)
[2025-02-13 19:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32][root][INFO] - Training Epoch: 1/2, step 2906/7134 completed (loss: 0.14876680076122284, acc: 0.9738219976425171)
[2025-02-13 19:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33][root][INFO] - Training Epoch: 1/2, step 2907/7134 completed (loss: 0.23709797859191895, acc: 0.9281437397003174)
[2025-02-13 19:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33][root][INFO] - Training Epoch: 1/2, step 2908/7134 completed (loss: 0.16657765209674835, acc: 0.9712643623352051)
[2025-02-13 19:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33][root][INFO] - Training Epoch: 1/2, step 2909/7134 completed (loss: 0.09596865624189377, acc: 0.9890710115432739)
[2025-02-13 19:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34][root][INFO] - Training Epoch: 1/2, step 2910/7134 completed (loss: 0.17732997238636017, acc: 0.9625668525695801)
[2025-02-13 19:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34][root][INFO] - Training Epoch: 1/2, step 2911/7134 completed (loss: 0.1446380466222763, acc: 0.9580419659614563)
[2025-02-13 19:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35][root][INFO] - Training Epoch: 1/2, step 2912/7134 completed (loss: 0.11243714392185211, acc: 0.965753436088562)
[2025-02-13 19:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35][root][INFO] - Training Epoch: 1/2, step 2913/7134 completed (loss: 0.23509864509105682, acc: 0.9568345546722412)
[2025-02-13 19:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35][root][INFO] - Training Epoch: 1/2, step 2914/7134 completed (loss: 0.17281432449817657, acc: 0.9575757384300232)
[2025-02-13 19:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:36][root][INFO] - Training Epoch: 1/2, step 2915/7134 completed (loss: 0.5020059943199158, acc: 0.8965517282485962)
[2025-02-13 19:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:36][root][INFO] - Training Epoch: 1/2, step 2916/7134 completed (loss: 0.2196589857339859, acc: 0.9298245906829834)
[2025-02-13 19:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37][root][INFO] - Training Epoch: 1/2, step 2917/7134 completed (loss: 0.1387077271938324, acc: 0.9736841917037964)
[2025-02-13 19:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37][root][INFO] - Training Epoch: 1/2, step 2918/7134 completed (loss: 0.21691934764385223, acc: 0.9415204524993896)
[2025-02-13 19:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37][root][INFO] - Training Epoch: 1/2, step 2919/7134 completed (loss: 0.15621626377105713, acc: 0.9578313231468201)
[2025-02-13 19:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38][root][INFO] - Training Epoch: 1/2, step 2920/7134 completed (loss: 0.22269809246063232, acc: 0.955974817276001)
[2025-02-13 19:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38][root][INFO] - Training Epoch: 1/2, step 2921/7134 completed (loss: 0.18111221492290497, acc: 0.9357143044471741)
[2025-02-13 19:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38][root][INFO] - Training Epoch: 1/2, step 2922/7134 completed (loss: 0.3599544167518616, acc: 0.9202454090118408)
[2025-02-13 19:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39][root][INFO] - Training Epoch: 1/2, step 2923/7134 completed (loss: 0.7731761932373047, acc: 0.8802816867828369)
[2025-02-13 19:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39][root][INFO] - Training Epoch: 1/2, step 2924/7134 completed (loss: 0.2557187080383301, acc: 0.9390243887901306)
[2025-02-13 19:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39][root][INFO] - Training Epoch: 1/2, step 2925/7134 completed (loss: 0.15234225988388062, acc: 0.9647058844566345)
[2025-02-13 19:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:40][root][INFO] - Training Epoch: 1/2, step 2926/7134 completed (loss: 0.42342710494995117, acc: 0.9195979833602905)
[2025-02-13 19:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:40][root][INFO] - Training Epoch: 1/2, step 2927/7134 completed (loss: 0.15826596319675446, acc: 0.9537572264671326)
[2025-02-13 19:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41][root][INFO] - Training Epoch: 1/2, step 2928/7134 completed (loss: 0.2491055577993393, acc: 0.9513888955116272)
[2025-02-13 19:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41][root][INFO] - Training Epoch: 1/2, step 2929/7134 completed (loss: 0.11213003098964691, acc: 0.976190447807312)
[2025-02-13 19:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41][root][INFO] - Training Epoch: 1/2, step 2930/7134 completed (loss: 0.2584703266620636, acc: 0.9455782175064087)
[2025-02-13 19:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42][root][INFO] - Training Epoch: 1/2, step 2931/7134 completed (loss: 0.4498836398124695, acc: 0.8910256624221802)
[2025-02-13 19:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42][root][INFO] - Training Epoch: 1/2, step 2932/7134 completed (loss: 0.18561653792858124, acc: 0.9625668525695801)
[2025-02-13 19:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42][root][INFO] - Training Epoch: 1/2, step 2933/7134 completed (loss: 0.2100064605474472, acc: 0.9416058659553528)
[2025-02-13 19:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43][root][INFO] - Training Epoch: 1/2, step 2934/7134 completed (loss: 0.08170115947723389, acc: 0.9928571581840515)
[2025-02-13 19:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43][root][INFO] - Training Epoch: 1/2, step 2935/7134 completed (loss: 0.08440790325403214, acc: 0.9788359999656677)
[2025-02-13 19:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43][root][INFO] - Training Epoch: 1/2, step 2936/7134 completed (loss: 0.2370678037405014, acc: 0.9469026327133179)
[2025-02-13 19:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44][root][INFO] - Training Epoch: 1/2, step 2937/7134 completed (loss: 0.23684841394424438, acc: 0.9411764740943909)
[2025-02-13 19:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44][root][INFO] - Training Epoch: 1/2, step 2938/7134 completed (loss: 0.2535364031791687, acc: 0.9398906826972961)
[2025-02-13 19:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45][root][INFO] - Training Epoch: 1/2, step 2939/7134 completed (loss: 0.12370074540376663, acc: 0.9801324605941772)
[2025-02-13 19:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45][root][INFO] - Training Epoch: 1/2, step 2940/7134 completed (loss: 0.16873407363891602, acc: 0.9508196711540222)
[2025-02-13 19:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45][root][INFO] - Training Epoch: 1/2, step 2941/7134 completed (loss: 0.20521393418312073, acc: 0.932584285736084)
[2025-02-13 19:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46][root][INFO] - Training Epoch: 1/2, step 2942/7134 completed (loss: 0.15704993903636932, acc: 0.9696969985961914)
[2025-02-13 19:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46][root][INFO] - Training Epoch: 1/2, step 2943/7134 completed (loss: 0.24198928475379944, acc: 0.9189189076423645)
[2025-02-13 19:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46][root][INFO] - Training Epoch: 1/2, step 2944/7134 completed (loss: 0.16352777183055878, acc: 0.9457831382751465)
[2025-02-13 19:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47][root][INFO] - Training Epoch: 1/2, step 2945/7134 completed (loss: 0.21551862359046936, acc: 0.9328858852386475)
[2025-02-13 19:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47][root][INFO] - Training Epoch: 1/2, step 2946/7134 completed (loss: 0.27961450815200806, acc: 0.9444444179534912)
[2025-02-13 19:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47][root][INFO] - Training Epoch: 1/2, step 2947/7134 completed (loss: 0.2824409306049347, acc: 0.9375)
[2025-02-13 19:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48][root][INFO] - Training Epoch: 1/2, step 2948/7134 completed (loss: 0.3459377884864807, acc: 0.9027777910232544)
[2025-02-13 19:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48][root][INFO] - Training Epoch: 1/2, step 2949/7134 completed (loss: 0.10517656058073044, acc: 0.9803921580314636)
[2025-02-13 19:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49][root][INFO] - Training Epoch: 1/2, step 2950/7134 completed (loss: 0.1894155591726303, acc: 0.9497206807136536)
[2025-02-13 19:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49][root][INFO] - Training Epoch: 1/2, step 2951/7134 completed (loss: 0.1608336716890335, acc: 0.9627329111099243)
[2025-02-13 19:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49][root][INFO] - Training Epoch: 1/2, step 2952/7134 completed (loss: 0.09920526295900345, acc: 0.9681528806686401)
[2025-02-13 19:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50][root][INFO] - Training Epoch: 1/2, step 2953/7134 completed (loss: 0.21392807364463806, acc: 0.9387755393981934)
[2025-02-13 19:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50][root][INFO] - Training Epoch: 1/2, step 2954/7134 completed (loss: 0.21459347009658813, acc: 0.9484536051750183)
[2025-02-13 19:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50][root][INFO] - Training Epoch: 1/2, step 2955/7134 completed (loss: 0.17890581488609314, acc: 0.9454545378684998)
[2025-02-13 19:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:51][root][INFO] - Training Epoch: 1/2, step 2956/7134 completed (loss: 0.2200249284505844, acc: 0.9447852969169617)
[2025-02-13 19:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:51][root][INFO] - Training Epoch: 1/2, step 2957/7134 completed (loss: 0.13535849750041962, acc: 0.959770143032074)
[2025-02-13 19:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52][root][INFO] - Training Epoch: 1/2, step 2958/7134 completed (loss: 0.1399458646774292, acc: 0.9652777910232544)
[2025-02-13 19:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52][root][INFO] - Training Epoch: 1/2, step 2959/7134 completed (loss: 0.12007249891757965, acc: 0.9746835231781006)
[2025-02-13 19:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52][root][INFO] - Training Epoch: 1/2, step 2960/7134 completed (loss: 0.16907459497451782, acc: 0.9450549483299255)
[2025-02-13 19:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53][root][INFO] - Training Epoch: 1/2, step 2961/7134 completed (loss: 0.14845815300941467, acc: 0.9520958065986633)
[2025-02-13 19:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53][root][INFO] - Training Epoch: 1/2, step 2962/7134 completed (loss: 0.12004609405994415, acc: 0.976190447807312)
[2025-02-13 19:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53][root][INFO] - Training Epoch: 1/2, step 2963/7134 completed (loss: 0.1584315001964569, acc: 0.9329608678817749)
[2025-02-13 19:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54][root][INFO] - Training Epoch: 1/2, step 2964/7134 completed (loss: 0.08466893434524536, acc: 0.9800000190734863)
[2025-02-13 19:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54][root][INFO] - Training Epoch: 1/2, step 2965/7134 completed (loss: 0.23309645056724548, acc: 0.9318181872367859)
[2025-02-13 19:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55][root][INFO] - Training Epoch: 1/2, step 2966/7134 completed (loss: 0.04446036368608475, acc: 0.9914529919624329)
[2025-02-13 19:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55][root][INFO] - Training Epoch: 1/2, step 2967/7134 completed (loss: 0.3827846050262451, acc: 0.9071038365364075)
[2025-02-13 19:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55][root][INFO] - Training Epoch: 1/2, step 2968/7134 completed (loss: 0.6142817735671997, acc: 0.862500011920929)
[2025-02-13 19:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56][root][INFO] - Training Epoch: 1/2, step 2969/7134 completed (loss: 1.3382750749588013, acc: 0.7864077687263489)
[2025-02-13 19:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56][root][INFO] - Training Epoch: 1/2, step 2970/7134 completed (loss: 1.480136513710022, acc: 0.7238805890083313)
[2025-02-13 19:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56][root][INFO] - Training Epoch: 1/2, step 2971/7134 completed (loss: 0.3109733760356903, acc: 0.9447513818740845)
[2025-02-13 19:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57][root][INFO] - Training Epoch: 1/2, step 2972/7134 completed (loss: 0.37156954407691956, acc: 0.9133333563804626)
[2025-02-13 19:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57][root][INFO] - Training Epoch: 1/2, step 2973/7134 completed (loss: 0.19729362428188324, acc: 0.9454545378684998)
[2025-02-13 19:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58][root][INFO] - Training Epoch: 1/2, step 2974/7134 completed (loss: 0.3153662085533142, acc: 0.9122806787490845)
[2025-02-13 19:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58][root][INFO] - Training Epoch: 1/2, step 2975/7134 completed (loss: 0.22609995305538177, acc: 0.9418604373931885)
[2025-02-13 19:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58][root][INFO] - Training Epoch: 1/2, step 2976/7134 completed (loss: 0.20003844797611237, acc: 0.9528301954269409)
[2025-02-13 19:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59][root][INFO] - Training Epoch: 1/2, step 2977/7134 completed (loss: 0.23960758745670319, acc: 0.951724112033844)
[2025-02-13 19:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59][root][INFO] - Training Epoch: 1/2, step 2978/7134 completed (loss: 0.26727351546287537, acc: 0.9457831382751465)
[2025-02-13 19:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00][root][INFO] - Training Epoch: 1/2, step 2979/7134 completed (loss: 0.2398006021976471, acc: 0.9448275566101074)
[2025-02-13 19:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00][root][INFO] - Training Epoch: 1/2, step 2980/7134 completed (loss: 0.31942135095596313, acc: 0.9142857193946838)
[2025-02-13 19:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00][root][INFO] - Training Epoch: 1/2, step 2981/7134 completed (loss: 0.3310401439666748, acc: 0.9195402264595032)
[2025-02-13 19:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01][root][INFO] - Training Epoch: 1/2, step 2982/7134 completed (loss: 0.4253619611263275, acc: 0.906593382358551)
[2025-02-13 19:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01][root][INFO] - Training Epoch: 1/2, step 2983/7134 completed (loss: 0.2885776460170746, acc: 0.9202454090118408)
[2025-02-13 19:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01][root][INFO] - Training Epoch: 1/2, step 2984/7134 completed (loss: 0.34459835290908813, acc: 0.9395604133605957)
[2025-02-13 19:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:02][root][INFO] - Training Epoch: 1/2, step 2985/7134 completed (loss: 0.14896029233932495, acc: 0.9658536314964294)
[2025-02-13 19:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:02][root][INFO] - Training Epoch: 1/2, step 2986/7134 completed (loss: 0.19162854552268982, acc: 0.9536082744598389)
[2025-02-13 19:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03][root][INFO] - Training Epoch: 1/2, step 2987/7134 completed (loss: 0.2361535131931305, acc: 0.95652174949646)
[2025-02-13 19:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03][root][INFO] - Training Epoch: 1/2, step 2988/7134 completed (loss: 0.22841013967990875, acc: 0.9513513445854187)
[2025-02-13 19:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03][root][INFO] - Training Epoch: 1/2, step 2989/7134 completed (loss: 0.22921599447727203, acc: 0.9281437397003174)
[2025-02-13 19:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:04][root][INFO] - Training Epoch: 1/2, step 2990/7134 completed (loss: 0.4784749150276184, acc: 0.8918918967247009)
[2025-02-13 19:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:04][root][INFO] - Training Epoch: 1/2, step 2991/7134 completed (loss: 0.8073005676269531, acc: 0.8229166865348816)
[2025-02-13 19:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05][root][INFO] - Training Epoch: 1/2, step 2992/7134 completed (loss: 0.4000648856163025, acc: 0.9371069073677063)
[2025-02-13 19:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05][root][INFO] - Training Epoch: 1/2, step 2993/7134 completed (loss: 0.18425938487052917, acc: 0.9447236061096191)
[2025-02-13 19:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05][root][INFO] - Training Epoch: 1/2, step 2994/7134 completed (loss: 0.1630207896232605, acc: 0.9365079402923584)
[2025-02-13 19:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06][root][INFO] - Training Epoch: 1/2, step 2995/7134 completed (loss: 0.19054213166236877, acc: 0.9417475461959839)
[2025-02-13 19:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06][root][INFO] - Training Epoch: 1/2, step 2996/7134 completed (loss: 0.16363677382469177, acc: 0.9536082744598389)
[2025-02-13 19:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06][root][INFO] - Training Epoch: 1/2, step 2997/7134 completed (loss: 0.1805257648229599, acc: 0.9528301954269409)
[2025-02-13 19:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07][root][INFO] - Training Epoch: 1/2, step 2998/7134 completed (loss: 0.14462001621723175, acc: 0.976190447807312)
[2025-02-13 19:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07][root][INFO] - Training Epoch: 1/2, step 2999/7134 completed (loss: 0.14155498147010803, acc: 0.96875)
[2025-02-13 19:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07][root][INFO] - Training Epoch: 1/2, step 3000/7134 completed (loss: 0.331569641828537, acc: 0.9027777910232544)
[2025-02-13 19:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08][root][INFO] - Training Epoch: 1/2, step 3001/7134 completed (loss: 0.20010212063789368, acc: 0.9593908786773682)
[2025-02-13 19:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08][root][INFO] - Training Epoch: 1/2, step 3002/7134 completed (loss: 0.2675634026527405, acc: 0.9539170265197754)
[2025-02-13 19:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09][root][INFO] - Training Epoch: 1/2, step 3003/7134 completed (loss: 0.06356091052293777, acc: 0.9847715497016907)
[2025-02-13 19:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09][root][INFO] - Training Epoch: 1/2, step 3004/7134 completed (loss: 0.0667528361082077, acc: 0.9851484894752502)
[2025-02-13 19:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09][root][INFO] - Training Epoch: 1/2, step 3005/7134 completed (loss: 0.04787752404808998, acc: 0.984455943107605)
[2025-02-13 19:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10][root][INFO] - Training Epoch: 1/2, step 3006/7134 completed (loss: 0.0458509624004364, acc: 0.9828571677207947)
[2025-02-13 19:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10][root][INFO] - Training Epoch: 1/2, step 3007/7134 completed (loss: 0.22319085896015167, acc: 0.9476743936538696)
[2025-02-13 19:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11][root][INFO] - Training Epoch: 1/2, step 3008/7134 completed (loss: 0.2513478994369507, acc: 0.9387755393981934)
[2025-02-13 19:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11][root][INFO] - Training Epoch: 1/2, step 3009/7134 completed (loss: 0.2843201756477356, acc: 0.9345238208770752)
[2025-02-13 19:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11][root][INFO] - Training Epoch: 1/2, step 3010/7134 completed (loss: 0.21483437716960907, acc: 0.9657142758369446)
[2025-02-13 19:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12][root][INFO] - Training Epoch: 1/2, step 3011/7134 completed (loss: 0.6674959063529968, acc: 0.8484848737716675)
[2025-02-13 19:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12][root][INFO] - Training Epoch: 1/2, step 3012/7134 completed (loss: 0.465423047542572, acc: 0.8848921060562134)
[2025-02-13 19:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12][root][INFO] - Training Epoch: 1/2, step 3013/7134 completed (loss: 0.04589279741048813, acc: 1.0)
[2025-02-13 19:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13][root][INFO] - Training Epoch: 1/2, step 3014/7134 completed (loss: 0.12715403735637665, acc: 0.9746835231781006)
[2025-02-13 19:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13][root][INFO] - Training Epoch: 1/2, step 3015/7134 completed (loss: 0.12343813478946686, acc: 0.9647887349128723)
[2025-02-13 19:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13][root][INFO] - Training Epoch: 1/2, step 3016/7134 completed (loss: 0.12379112094640732, acc: 0.9726027250289917)
[2025-02-13 19:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14][root][INFO] - Training Epoch: 1/2, step 3017/7134 completed (loss: 0.08709544688463211, acc: 0.9752066135406494)
[2025-02-13 19:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14][root][INFO] - Training Epoch: 1/2, step 3018/7134 completed (loss: 0.1293802261352539, acc: 0.9694656729698181)
[2025-02-13 19:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15][root][INFO] - Training Epoch: 1/2, step 3019/7134 completed (loss: 0.15088224411010742, acc: 0.9779411554336548)
[2025-02-13 19:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15][root][INFO] - Training Epoch: 1/2, step 3020/7134 completed (loss: 0.18756544589996338, acc: 0.9496402740478516)
[2025-02-13 19:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15][root][INFO] - Training Epoch: 1/2, step 3021/7134 completed (loss: 0.15804703533649445, acc: 0.9545454382896423)
[2025-02-13 19:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16][root][INFO] - Training Epoch: 1/2, step 3022/7134 completed (loss: 0.21043790876865387, acc: 0.9624060392379761)
[2025-02-13 19:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16][root][INFO] - Training Epoch: 1/2, step 3023/7134 completed (loss: 0.3126692771911621, acc: 0.9153845906257629)
[2025-02-13 19:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17][root][INFO] - Training Epoch: 1/2, step 3024/7134 completed (loss: 0.11066946387290955, acc: 0.9750000238418579)
[2025-02-13 19:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17][root][INFO] - Training Epoch: 1/2, step 3025/7134 completed (loss: 0.3703567385673523, acc: 0.9307692050933838)
[2025-02-13 19:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17][root][INFO] - Training Epoch: 1/2, step 3026/7134 completed (loss: 0.14875754714012146, acc: 0.9754098653793335)
[2025-02-13 19:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18][root][INFO] - Training Epoch: 1/2, step 3027/7134 completed (loss: 0.26415714621543884, acc: 0.9479166865348816)
[2025-02-13 19:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18][root][INFO] - Training Epoch: 1/2, step 3028/7134 completed (loss: 0.10983230918645859, acc: 0.969072163105011)
[2025-02-13 19:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18][root][INFO] - Training Epoch: 1/2, step 3029/7134 completed (loss: 0.2339378297328949, acc: 0.9520547986030579)
[2025-02-13 19:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19][root][INFO] - Training Epoch: 1/2, step 3030/7134 completed (loss: 0.12966914474964142, acc: 0.9729729890823364)
[2025-02-13 19:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19][root][INFO] - Training Epoch: 1/2, step 3031/7134 completed (loss: 0.09021160751581192, acc: 0.9797979593276978)
[2025-02-13 19:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19][root][INFO] - Training Epoch: 1/2, step 3032/7134 completed (loss: 0.23218469321727753, acc: 0.9274193644523621)
[2025-02-13 19:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:20][root][INFO] - Training Epoch: 1/2, step 3033/7134 completed (loss: 0.1528266966342926, acc: 0.96875)
[2025-02-13 19:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:20][root][INFO] - Training Epoch: 1/2, step 3034/7134 completed (loss: 0.1697441190481186, acc: 0.9459459185600281)
[2025-02-13 19:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21][root][INFO] - Training Epoch: 1/2, step 3035/7134 completed (loss: 0.1520978957414627, acc: 0.9455782175064087)
[2025-02-13 19:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21][root][INFO] - Training Epoch: 1/2, step 3036/7134 completed (loss: 0.2465507537126541, acc: 0.9440000057220459)
[2025-02-13 19:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21][root][INFO] - Training Epoch: 1/2, step 3037/7134 completed (loss: 0.16551290452480316, acc: 0.9395973086357117)
[2025-02-13 19:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22][root][INFO] - Training Epoch: 1/2, step 3038/7134 completed (loss: 0.16099204123020172, acc: 0.9701492786407471)
[2025-02-13 19:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22][root][INFO] - Training Epoch: 1/2, step 3039/7134 completed (loss: 0.18041378259658813, acc: 0.9729729890823364)
[2025-02-13 19:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22][root][INFO] - Training Epoch: 1/2, step 3040/7134 completed (loss: 0.20637862384319305, acc: 0.9224806427955627)
[2025-02-13 19:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23][root][INFO] - Training Epoch: 1/2, step 3041/7134 completed (loss: 0.3331248164176941, acc: 0.9285714030265808)
[2025-02-13 19:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23][root][INFO] - Training Epoch: 1/2, step 3042/7134 completed (loss: 0.13781973719596863, acc: 0.9551281929016113)
[2025-02-13 19:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24][root][INFO] - Training Epoch: 1/2, step 3043/7134 completed (loss: 0.09080108255147934, acc: 0.9642857313156128)
[2025-02-13 19:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24][root][INFO] - Training Epoch: 1/2, step 3044/7134 completed (loss: 0.17976486682891846, acc: 0.9624060392379761)
[2025-02-13 19:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24][root][INFO] - Training Epoch: 1/2, step 3045/7134 completed (loss: 0.08384396135807037, acc: 0.9844961166381836)
[2025-02-13 19:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25][root][INFO] - Training Epoch: 1/2, step 3046/7134 completed (loss: 0.13861478865146637, acc: 0.9585798978805542)
[2025-02-13 19:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25][root][INFO] - Training Epoch: 1/2, step 3047/7134 completed (loss: 0.18121935427188873, acc: 0.9548022747039795)
[2025-02-13 19:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25][root][INFO] - Training Epoch: 1/2, step 3048/7134 completed (loss: 0.2428968995809555, acc: 0.9210526347160339)
[2025-02-13 19:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:26][root][INFO] - Training Epoch: 1/2, step 3049/7134 completed (loss: 0.23341035842895508, acc: 0.9375)
[2025-02-13 19:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:26][root][INFO] - Training Epoch: 1/2, step 3050/7134 completed (loss: 0.2623198926448822, acc: 0.9340101480484009)
[2025-02-13 19:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27][root][INFO] - Training Epoch: 1/2, step 3051/7134 completed (loss: 0.2897418439388275, acc: 0.9107142686843872)
[2025-02-13 19:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27][root][INFO] - Training Epoch: 1/2, step 3052/7134 completed (loss: 0.18097969889640808, acc: 0.9621621370315552)
[2025-02-13 19:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27][root][INFO] - Training Epoch: 1/2, step 3053/7134 completed (loss: 0.11320096254348755, acc: 0.9800994992256165)
[2025-02-13 19:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:28][root][INFO] - Training Epoch: 1/2, step 3054/7134 completed (loss: 0.0789744183421135, acc: 0.978723406791687)
[2025-02-13 19:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:28][root][INFO] - Training Epoch: 1/2, step 3055/7134 completed (loss: 0.1694759577512741, acc: 0.9594594836235046)
[2025-02-13 19:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29][root][INFO] - Training Epoch: 1/2, step 3056/7134 completed (loss: 0.11323829740285873, acc: 0.9675675630569458)
[2025-02-13 19:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29][root][INFO] - Training Epoch: 1/2, step 3057/7134 completed (loss: 0.13162407279014587, acc: 0.9693251252174377)
[2025-02-13 19:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29][root][INFO] - Training Epoch: 1/2, step 3058/7134 completed (loss: 0.15722568333148956, acc: 0.9684210419654846)
[2025-02-13 19:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:30][root][INFO] - Training Epoch: 1/2, step 3059/7134 completed (loss: 0.049682993441820145, acc: 0.9885714054107666)
[2025-02-13 19:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:30][root][INFO] - Training Epoch: 1/2, step 3060/7134 completed (loss: 0.15894421935081482, acc: 0.963350772857666)
[2025-02-13 19:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31][root][INFO] - Training Epoch: 1/2, step 3061/7134 completed (loss: 0.09638109803199768, acc: 0.9750000238418579)
[2025-02-13 19:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31][root][INFO] - Training Epoch: 1/2, step 3062/7134 completed (loss: 0.09526049345731735, acc: 0.9811320900917053)
[2025-02-13 19:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31][root][INFO] - Training Epoch: 1/2, step 3063/7134 completed (loss: 0.17411670088768005, acc: 0.9431818127632141)
[2025-02-13 19:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:32][root][INFO] - Training Epoch: 1/2, step 3064/7134 completed (loss: 0.07009908556938171, acc: 0.9940828680992126)
[2025-02-13 19:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:32][root][INFO] - Training Epoch: 1/2, step 3065/7134 completed (loss: 0.04411676898598671, acc: 0.9887640476226807)
[2025-02-13 19:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33][root][INFO] - Training Epoch: 1/2, step 3066/7134 completed (loss: 0.07212550193071365, acc: 0.9933775067329407)
[2025-02-13 19:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33][root][INFO] - Training Epoch: 1/2, step 3067/7134 completed (loss: 0.11694884300231934, acc: 0.9632353186607361)
[2025-02-13 19:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33][root][INFO] - Training Epoch: 1/2, step 3068/7134 completed (loss: 0.19747470319271088, acc: 0.9497206807136536)
[2025-02-13 19:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:34][root][INFO] - Training Epoch: 1/2, step 3069/7134 completed (loss: 0.02186655066907406, acc: 1.0)
[2025-02-13 19:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:34][root][INFO] - Training Epoch: 1/2, step 3070/7134 completed (loss: 0.030252132564783096, acc: 1.0)
[2025-02-13 19:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35][root][INFO] - Training Epoch: 1/2, step 3071/7134 completed (loss: 0.13697797060012817, acc: 0.9777777791023254)
[2025-02-13 19:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35][root][INFO] - Training Epoch: 1/2, step 3072/7134 completed (loss: 0.05223170295357704, acc: 0.98591548204422)
[2025-02-13 19:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35][root][INFO] - Training Epoch: 1/2, step 3073/7134 completed (loss: 0.11830728501081467, acc: 0.9774011373519897)
[2025-02-13 19:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36][root][INFO] - Training Epoch: 1/2, step 3074/7134 completed (loss: 0.034744396805763245, acc: 0.9933775067329407)
[2025-02-13 19:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36][root][INFO] - Training Epoch: 1/2, step 3075/7134 completed (loss: 0.11774343252182007, acc: 0.9691358208656311)
[2025-02-13 19:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36][root][INFO] - Training Epoch: 1/2, step 3076/7134 completed (loss: 0.056762080639600754, acc: 0.9892473220825195)
[2025-02-13 19:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37][root][INFO] - Training Epoch: 1/2, step 3077/7134 completed (loss: 0.28005972504615784, acc: 0.9371727705001831)
[2025-02-13 19:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37][root][INFO] - Training Epoch: 1/2, step 3078/7134 completed (loss: 0.14980117976665497, acc: 0.9635416865348816)
[2025-02-13 19:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38][root][INFO] - Training Epoch: 1/2, step 3079/7134 completed (loss: 0.23080192506313324, acc: 0.9576719403266907)
[2025-02-13 19:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38][root][INFO] - Training Epoch: 1/2, step 3080/7134 completed (loss: 0.30366280674934387, acc: 0.9351851940155029)
[2025-02-13 19:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38][root][INFO] - Training Epoch: 1/2, step 3081/7134 completed (loss: 0.2119721919298172, acc: 0.9504950642585754)
[2025-02-13 19:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39][root][INFO] - Training Epoch: 1/2, step 3082/7134 completed (loss: 0.3632192611694336, acc: 0.9054726362228394)
[2025-02-13 19:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39][root][INFO] - Training Epoch: 1/2, step 3083/7134 completed (loss: 0.18096908926963806, acc: 0.9459459185600281)
[2025-02-13 19:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39][root][INFO] - Training Epoch: 1/2, step 3084/7134 completed (loss: 0.3033299744129181, acc: 0.9162303805351257)
[2025-02-13 19:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40][root][INFO] - Training Epoch: 1/2, step 3085/7134 completed (loss: 0.2780536413192749, acc: 0.9290322661399841)
[2025-02-13 19:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40][root][INFO] - Training Epoch: 1/2, step 3086/7134 completed (loss: 0.2591312825679779, acc: 0.9452054500579834)
[2025-02-13 19:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40][root][INFO] - Training Epoch: 1/2, step 3087/7134 completed (loss: 0.16644056141376495, acc: 0.9408283829689026)
[2025-02-13 19:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:41][root][INFO] - Training Epoch: 1/2, step 3088/7134 completed (loss: 0.2238394170999527, acc: 0.9158878326416016)
[2025-02-13 19:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:41][root][INFO] - Training Epoch: 1/2, step 3089/7134 completed (loss: 0.17348329722881317, acc: 0.9581395387649536)
[2025-02-13 19:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42][root][INFO] - Training Epoch: 1/2, step 3090/7134 completed (loss: 0.5017053484916687, acc: 0.8926553726196289)
[2025-02-13 19:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42][root][INFO] - Training Epoch: 1/2, step 3091/7134 completed (loss: 0.5871865153312683, acc: 0.8497853875160217)
[2025-02-13 19:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42][root][INFO] - Training Epoch: 1/2, step 3092/7134 completed (loss: 0.3807143270969391, acc: 0.9083969593048096)
[2025-02-13 19:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:43][root][INFO] - Training Epoch: 1/2, step 3093/7134 completed (loss: 0.23619475960731506, acc: 0.9436619877815247)
[2025-02-13 19:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:43][root][INFO] - Training Epoch: 1/2, step 3094/7134 completed (loss: 0.09248194843530655, acc: 0.9826589822769165)
[2025-02-13 19:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44][root][INFO] - Training Epoch: 1/2, step 3095/7134 completed (loss: 0.2288118153810501, acc: 0.9555555582046509)
[2025-02-13 19:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44][root][INFO] - Training Epoch: 1/2, step 3096/7134 completed (loss: 0.2251257300376892, acc: 0.9424083828926086)
[2025-02-13 19:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44][root][INFO] - Training Epoch: 1/2, step 3097/7134 completed (loss: 0.28280773758888245, acc: 0.9207317233085632)
[2025-02-13 19:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45][root][INFO] - Training Epoch: 1/2, step 3098/7134 completed (loss: 0.18879945576190948, acc: 0.9607843160629272)
[2025-02-13 19:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45][root][INFO] - Training Epoch: 1/2, step 3099/7134 completed (loss: 0.20748229324817657, acc: 0.9416058659553528)
[2025-02-13 19:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45][root][INFO] - Training Epoch: 1/2, step 3100/7134 completed (loss: 0.1480158269405365, acc: 0.9551569223403931)
[2025-02-13 19:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46][root][INFO] - Training Epoch: 1/2, step 3101/7134 completed (loss: 0.19582611322402954, acc: 0.9390863180160522)
[2025-02-13 19:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46][root][INFO] - Training Epoch: 1/2, step 3102/7134 completed (loss: 0.1126408576965332, acc: 0.987730085849762)
[2025-02-13 19:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46][root][INFO] - Training Epoch: 1/2, step 3103/7134 completed (loss: 0.14855961501598358, acc: 0.9741379022598267)
[2025-02-13 19:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:47][root][INFO] - Training Epoch: 1/2, step 3104/7134 completed (loss: 0.08481895178556442, acc: 0.9767441749572754)
[2025-02-13 19:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:47][root][INFO] - Training Epoch: 1/2, step 3105/7134 completed (loss: 0.25673234462738037, acc: 0.9434782862663269)
[2025-02-13 19:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48][root][INFO] - Training Epoch: 1/2, step 3106/7134 completed (loss: 0.13504301011562347, acc: 0.9597315192222595)
[2025-02-13 19:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48][root][INFO] - Training Epoch: 1/2, step 3107/7134 completed (loss: 0.08213269710540771, acc: 0.9825581312179565)
[2025-02-13 19:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48][root][INFO] - Training Epoch: 1/2, step 3108/7134 completed (loss: 0.1425846666097641, acc: 0.9795918464660645)
[2025-02-13 19:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49][root][INFO] - Training Epoch: 1/2, step 3109/7134 completed (loss: 0.2779468894004822, acc: 0.9230769276618958)
[2025-02-13 19:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49][root][INFO] - Training Epoch: 1/2, step 3110/7134 completed (loss: 0.15934552252292633, acc: 0.9514563083648682)
[2025-02-13 19:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49][root][INFO] - Training Epoch: 1/2, step 3111/7134 completed (loss: 0.510165810585022, acc: 0.8623188138008118)
[2025-02-13 19:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50][root][INFO] - Training Epoch: 1/2, step 3112/7134 completed (loss: 0.3678196668624878, acc: 0.9124087691307068)
[2025-02-13 19:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50][root][INFO] - Training Epoch: 1/2, step 3113/7134 completed (loss: 0.20239941775798798, acc: 0.9512194991111755)
[2025-02-13 19:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50][root][INFO] - Training Epoch: 1/2, step 3114/7134 completed (loss: 0.15974491834640503, acc: 0.9624060392379761)
[2025-02-13 19:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:51][root][INFO] - Training Epoch: 1/2, step 3115/7134 completed (loss: 0.17733296751976013, acc: 0.9736841917037964)
[2025-02-13 19:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:51][root][INFO] - Training Epoch: 1/2, step 3116/7134 completed (loss: 0.2622120976448059, acc: 0.9491525292396545)
[2025-02-13 19:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52][root][INFO] - Training Epoch: 1/2, step 3117/7134 completed (loss: 0.2633455991744995, acc: 0.9438202381134033)
[2025-02-13 19:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52][root][INFO] - Training Epoch: 1/2, step 3118/7134 completed (loss: 0.24460239708423615, acc: 0.9337016344070435)
[2025-02-13 19:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52][root][INFO] - Training Epoch: 1/2, step 3119/7134 completed (loss: 0.20691244304180145, acc: 0.9576719403266907)
[2025-02-13 19:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53][root][INFO] - Training Epoch: 1/2, step 3120/7134 completed (loss: 0.3233526051044464, acc: 0.9506173133850098)
[2025-02-13 19:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53][root][INFO] - Training Epoch: 1/2, step 3121/7134 completed (loss: 0.1644454151391983, acc: 0.9659090638160706)
[2025-02-13 19:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53][root][INFO] - Training Epoch: 1/2, step 3122/7134 completed (loss: 0.3443126380443573, acc: 0.9263157844543457)
[2025-02-13 19:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54][root][INFO] - Training Epoch: 1/2, step 3123/7134 completed (loss: 0.29231002926826477, acc: 0.9135135412216187)
[2025-02-13 19:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54][root][INFO] - Training Epoch: 1/2, step 3124/7134 completed (loss: 0.24130405485630035, acc: 0.9388889074325562)
[2025-02-13 19:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55][root][INFO] - Training Epoch: 1/2, step 3125/7134 completed (loss: 0.11531390994787216, acc: 0.9795918464660645)
[2025-02-13 19:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55][root][INFO] - Training Epoch: 1/2, step 3126/7134 completed (loss: 0.2833247184753418, acc: 0.9329608678817749)
[2025-02-13 19:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55][root][INFO] - Training Epoch: 1/2, step 3127/7134 completed (loss: 0.19525845348834991, acc: 0.9436619877815247)
[2025-02-13 19:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56][root][INFO] - Training Epoch: 1/2, step 3128/7134 completed (loss: 0.17899377644062042, acc: 0.9572192430496216)
[2025-02-13 19:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56][root][INFO] - Training Epoch: 1/2, step 3129/7134 completed (loss: 0.14936423301696777, acc: 0.9715909361839294)
[2025-02-13 19:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56][root][INFO] - Training Epoch: 1/2, step 3130/7134 completed (loss: 0.10536894202232361, acc: 0.9847715497016907)
[2025-02-13 19:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:57][root][INFO] - Training Epoch: 1/2, step 3131/7134 completed (loss: 0.2431706339120865, acc: 0.9435028433799744)
[2025-02-13 19:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:57][root][INFO] - Training Epoch: 1/2, step 3132/7134 completed (loss: 0.13136419653892517, acc: 0.9469026327133179)
[2025-02-13 19:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58][root][INFO] - Training Epoch: 1/2, step 3133/7134 completed (loss: 0.31470128893852234, acc: 0.9488636255264282)
[2025-02-13 19:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58][root][INFO] - Training Epoch: 1/2, step 3134/7134 completed (loss: 0.2140807807445526, acc: 0.9453125)
[2025-02-13 19:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58][root][INFO] - Training Epoch: 1/2, step 3135/7134 completed (loss: 0.32377636432647705, acc: 0.9175257682800293)
[2025-02-13 19:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:59][root][INFO] - Training Epoch: 1/2, step 3136/7134 completed (loss: 0.22914592921733856, acc: 0.9312977194786072)
[2025-02-13 19:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:59][root][INFO] - Training Epoch: 1/2, step 3137/7134 completed (loss: 0.33670279383659363, acc: 0.9078013896942139)
[2025-02-13 19:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00][root][INFO] - Training Epoch: 1/2, step 3138/7134 completed (loss: 0.3749336302280426, acc: 0.9333333373069763)
[2025-02-13 19:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00][root][INFO] - Training Epoch: 1/2, step 3139/7134 completed (loss: 0.3445543348789215, acc: 0.9208633303642273)
[2025-02-13 19:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00][root][INFO] - Training Epoch: 1/2, step 3140/7134 completed (loss: 0.16663812100887299, acc: 0.9589040875434875)
[2025-02-13 19:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01][root][INFO] - Training Epoch: 1/2, step 3141/7134 completed (loss: 0.27517062425613403, acc: 0.9391891956329346)
[2025-02-13 19:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01][root][INFO] - Training Epoch: 1/2, step 3142/7134 completed (loss: 0.09194055199623108, acc: 0.9880239367485046)
[2025-02-13 19:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01][root][INFO] - Training Epoch: 1/2, step 3143/7134 completed (loss: 0.22585074603557587, acc: 0.948387086391449)
[2025-02-13 19:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02][root][INFO] - Training Epoch: 1/2, step 3144/7134 completed (loss: 0.30947667360305786, acc: 0.9294871687889099)
[2025-02-13 19:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02][root][INFO] - Training Epoch: 1/2, step 3145/7134 completed (loss: 0.26424187421798706, acc: 0.9369369149208069)
[2025-02-13 19:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02][root][INFO] - Training Epoch: 1/2, step 3146/7134 completed (loss: 0.3174569010734558, acc: 0.9242424368858337)
[2025-02-13 19:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03][root][INFO] - Training Epoch: 1/2, step 3147/7134 completed (loss: 0.1286802887916565, acc: 0.9589040875434875)
[2025-02-13 19:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03][root][INFO] - Training Epoch: 1/2, step 3148/7134 completed (loss: 0.12347674369812012, acc: 0.9608938694000244)
[2025-02-13 19:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04][root][INFO] - Training Epoch: 1/2, step 3149/7134 completed (loss: 0.09016580879688263, acc: 0.9741935729980469)
[2025-02-13 19:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04][root][INFO] - Training Epoch: 1/2, step 3150/7134 completed (loss: 1.0165932178497314, acc: 0.8071428537368774)
[2025-02-13 19:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04][root][INFO] - Training Epoch: 1/2, step 3151/7134 completed (loss: 0.4642640948295593, acc: 0.9007633328437805)
[2025-02-13 19:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05][root][INFO] - Training Epoch: 1/2, step 3152/7134 completed (loss: 0.2476755827665329, acc: 0.9436619877815247)
[2025-02-13 19:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05][root][INFO] - Training Epoch: 1/2, step 3153/7134 completed (loss: 0.1942678987979889, acc: 0.949438214302063)
[2025-02-13 19:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05][root][INFO] - Training Epoch: 1/2, step 3154/7134 completed (loss: 0.2971673011779785, acc: 0.932330846786499)
[2025-02-13 19:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06][root][INFO] - Training Epoch: 1/2, step 3155/7134 completed (loss: 0.7438459992408752, acc: 0.8721804618835449)
[2025-02-13 19:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06][root][INFO] - Training Epoch: 1/2, step 3156/7134 completed (loss: 0.42890664935112, acc: 0.9011628031730652)
[2025-02-13 19:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06][root][INFO] - Training Epoch: 1/2, step 3157/7134 completed (loss: 0.26162752509117126, acc: 0.947826087474823)
[2025-02-13 19:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07][root][INFO] - Training Epoch: 1/2, step 3158/7134 completed (loss: 0.16377823054790497, acc: 0.978723406791687)
[2025-02-13 19:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07][root][INFO] - Training Epoch: 1/2, step 3159/7134 completed (loss: 0.25180742144584656, acc: 0.969072163105011)
[2025-02-13 19:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08][root][INFO] - Training Epoch: 1/2, step 3160/7134 completed (loss: 0.12764866650104523, acc: 0.9743589758872986)
[2025-02-13 19:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08][root][INFO] - Training Epoch: 1/2, step 3161/7134 completed (loss: 0.21534495055675507, acc: 0.9490445852279663)
[2025-02-13 19:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08][root][INFO] - Training Epoch: 1/2, step 3162/7134 completed (loss: 0.13545645773410797, acc: 0.9589040875434875)
[2025-02-13 19:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09][root][INFO] - Training Epoch: 1/2, step 3163/7134 completed (loss: 0.084332175552845, acc: 0.9672130942344666)
[2025-02-13 19:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09][root][INFO] - Training Epoch: 1/2, step 3164/7134 completed (loss: 0.2495819330215454, acc: 0.9275362491607666)
[2025-02-13 19:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09][root][INFO] - Training Epoch: 1/2, step 3165/7134 completed (loss: 0.23402775824069977, acc: 0.9440559148788452)
[2025-02-13 19:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10][root][INFO] - Training Epoch: 1/2, step 3166/7134 completed (loss: 0.40146759152412415, acc: 0.8987341523170471)
[2025-02-13 19:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10][root][INFO] - Training Epoch: 1/2, step 3167/7134 completed (loss: 0.47868549823760986, acc: 0.895061731338501)
[2025-02-13 19:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10][root][INFO] - Training Epoch: 1/2, step 3168/7134 completed (loss: 0.24395930767059326, acc: 0.9477124214172363)
[2025-02-13 19:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:11][root][INFO] - Training Epoch: 1/2, step 3169/7134 completed (loss: 0.2947148084640503, acc: 0.9180327653884888)
[2025-02-13 19:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:11][root][INFO] - Training Epoch: 1/2, step 3170/7134 completed (loss: 0.19779019057750702, acc: 0.9411764740943909)
[2025-02-13 19:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12][root][INFO] - Training Epoch: 1/2, step 3171/7134 completed (loss: 0.2030729204416275, acc: 0.9450549483299255)
[2025-02-13 19:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12][root][INFO] - Training Epoch: 1/2, step 3172/7134 completed (loss: 0.134139284491539, acc: 0.9841269850730896)
[2025-02-13 19:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12][root][INFO] - Training Epoch: 1/2, step 3173/7134 completed (loss: 0.22766664624214172, acc: 0.9595959782600403)
[2025-02-13 19:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13][root][INFO] - Training Epoch: 1/2, step 3174/7134 completed (loss: 0.28903862833976746, acc: 0.9337349534034729)
[2025-02-13 19:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13][root][INFO] - Training Epoch: 1/2, step 3175/7134 completed (loss: 0.22108730673789978, acc: 0.9476743936538696)
[2025-02-13 19:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13][root][INFO] - Training Epoch: 1/2, step 3176/7134 completed (loss: 0.48181724548339844, acc: 0.8837209343910217)
[2025-02-13 19:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14][root][INFO] - Training Epoch: 1/2, step 3177/7134 completed (loss: 0.396882563829422, acc: 0.9012345671653748)
[2025-02-13 19:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14][root][INFO] - Training Epoch: 1/2, step 3178/7134 completed (loss: 0.2656819820404053, acc: 0.9399999976158142)
[2025-02-13 19:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14][root][INFO] - Training Epoch: 1/2, step 3179/7134 completed (loss: 0.3695395886898041, acc: 0.9107142686843872)
[2025-02-13 19:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:15][root][INFO] - Training Epoch: 1/2, step 3180/7134 completed (loss: 0.21676424145698547, acc: 0.9444444179534912)
[2025-02-13 19:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:15][root][INFO] - Training Epoch: 1/2, step 3181/7134 completed (loss: 0.3088633716106415, acc: 0.9281768202781677)
[2025-02-13 19:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16][root][INFO] - Training Epoch: 1/2, step 3182/7134 completed (loss: 0.15314960479736328, acc: 0.9695122241973877)
[2025-02-13 19:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16][root][INFO] - Training Epoch: 1/2, step 3183/7134 completed (loss: 0.2737310528755188, acc: 0.9343065619468689)
[2025-02-13 19:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16][root][INFO] - Training Epoch: 1/2, step 3184/7134 completed (loss: 0.21775265038013458, acc: 0.96875)
[2025-02-13 19:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17][root][INFO] - Training Epoch: 1/2, step 3185/7134 completed (loss: 0.07838719338178635, acc: 0.9874213933944702)
[2025-02-13 19:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17][root][INFO] - Training Epoch: 1/2, step 3186/7134 completed (loss: 0.1247057318687439, acc: 0.9615384340286255)
[2025-02-13 19:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17][root][INFO] - Training Epoch: 1/2, step 3187/7134 completed (loss: 0.22347614169120789, acc: 0.9285714030265808)
[2025-02-13 19:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18][root][INFO] - Training Epoch: 1/2, step 3188/7134 completed (loss: 0.14637115597724915, acc: 0.9497206807136536)
[2025-02-13 19:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18][root][INFO] - Training Epoch: 1/2, step 3189/7134 completed (loss: 0.153427854180336, acc: 0.9567901492118835)
[2025-02-13 19:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19][root][INFO] - Training Epoch: 1/2, step 3190/7134 completed (loss: 0.24168165028095245, acc: 0.9437500238418579)
[2025-02-13 19:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19][root][INFO] - Training Epoch: 1/2, step 3191/7134 completed (loss: 0.1386461853981018, acc: 0.9813664555549622)
[2025-02-13 19:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19][root][INFO] - Training Epoch: 1/2, step 3192/7134 completed (loss: 0.5943393707275391, acc: 0.8832116723060608)
[2025-02-13 19:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20][root][INFO] - Training Epoch: 1/2, step 3193/7134 completed (loss: 0.2880772352218628, acc: 0.9135135412216187)
[2025-02-13 19:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20][root][INFO] - Training Epoch: 1/2, step 3194/7134 completed (loss: 0.1541673094034195, acc: 0.9757575988769531)
[2025-02-13 19:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21][root][INFO] - Training Epoch: 1/2, step 3195/7134 completed (loss: 0.3050493896007538, acc: 0.9254658222198486)
[2025-02-13 19:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21][root][INFO] - Training Epoch: 1/2, step 3196/7134 completed (loss: 0.19812092185020447, acc: 0.95652174949646)
[2025-02-13 19:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21][root][INFO] - Training Epoch: 1/2, step 3197/7134 completed (loss: 0.20446021854877472, acc: 0.940119743347168)
[2025-02-13 19:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22][root][INFO] - Training Epoch: 1/2, step 3198/7134 completed (loss: 0.18620136380195618, acc: 0.949999988079071)
[2025-02-13 19:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22][root][INFO] - Training Epoch: 1/2, step 3199/7134 completed (loss: 0.09346985071897507, acc: 0.9709302186965942)
[2025-02-13 19:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22][root][INFO] - Training Epoch: 1/2, step 3200/7134 completed (loss: 0.2106771618127823, acc: 0.9645389914512634)
[2025-02-13 19:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23][root][INFO] - Training Epoch: 1/2, step 3201/7134 completed (loss: 0.27401331067085266, acc: 0.9448819160461426)
[2025-02-13 19:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23][root][INFO] - Training Epoch: 1/2, step 3202/7134 completed (loss: 0.17623429000377655, acc: 0.9438202381134033)
[2025-02-13 19:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23][root][INFO] - Training Epoch: 1/2, step 3203/7134 completed (loss: 0.16347941756248474, acc: 0.9437500238418579)
[2025-02-13 19:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24][root][INFO] - Training Epoch: 1/2, step 3204/7134 completed (loss: 0.17941181361675262, acc: 0.9452054500579834)
[2025-02-13 19:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24][root][INFO] - Training Epoch: 1/2, step 3205/7134 completed (loss: 0.08168277144432068, acc: 0.9645389914512634)
[2025-02-13 19:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24][root][INFO] - Training Epoch: 1/2, step 3206/7134 completed (loss: 0.13906177878379822, acc: 0.9575757384300232)
[2025-02-13 19:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25][root][INFO] - Training Epoch: 1/2, step 3207/7134 completed (loss: 0.194206103682518, acc: 0.9386503100395203)
[2025-02-13 19:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25][root][INFO] - Training Epoch: 1/2, step 3208/7134 completed (loss: 0.1775175780057907, acc: 0.9568345546722412)
[2025-02-13 19:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26][root][INFO] - Training Epoch: 1/2, step 3209/7134 completed (loss: 0.14216777682304382, acc: 0.9437500238418579)
[2025-02-13 19:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26][root][INFO] - Training Epoch: 1/2, step 3210/7134 completed (loss: 0.13730907440185547, acc: 0.9548386931419373)
[2025-02-13 19:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26][root][INFO] - Training Epoch: 1/2, step 3211/7134 completed (loss: 0.18098202347755432, acc: 0.9579831957817078)
[2025-02-13 19:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27][root][INFO] - Training Epoch: 1/2, step 3212/7134 completed (loss: 0.21592138707637787, acc: 0.9532163739204407)
[2025-02-13 19:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27][root][INFO] - Training Epoch: 1/2, step 3213/7134 completed (loss: 0.20016790926456451, acc: 0.9404761791229248)
[2025-02-13 19:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27][root][INFO] - Training Epoch: 1/2, step 3214/7134 completed (loss: 0.16602198779582977, acc: 0.9649122953414917)
[2025-02-13 19:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:28][root][INFO] - Training Epoch: 1/2, step 3215/7134 completed (loss: 0.13782624900341034, acc: 0.9704142212867737)
[2025-02-13 19:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:28][root][INFO] - Training Epoch: 1/2, step 3216/7134 completed (loss: 0.1437569260597229, acc: 0.9629629850387573)
[2025-02-13 19:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29][root][INFO] - Training Epoch: 1/2, step 3217/7134 completed (loss: 0.15010878443717957, acc: 0.966292142868042)
[2025-02-13 19:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29][root][INFO] - Training Epoch: 1/2, step 3218/7134 completed (loss: 0.1454363614320755, acc: 0.9612902998924255)
[2025-02-13 19:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29][root][INFO] - Training Epoch: 1/2, step 3219/7134 completed (loss: 0.22432565689086914, acc: 0.9107142686843872)
[2025-02-13 19:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30][root][INFO] - Training Epoch: 1/2, step 3220/7134 completed (loss: 0.17535065114498138, acc: 0.9322034120559692)
[2025-02-13 19:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30][root][INFO] - Training Epoch: 1/2, step 3221/7134 completed (loss: 0.12624211609363556, acc: 0.9677419066429138)
[2025-02-13 19:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30][root][INFO] - Training Epoch: 1/2, step 3222/7134 completed (loss: 0.38082924485206604, acc: 0.89552241563797)
[2025-02-13 19:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31][root][INFO] - Training Epoch: 1/2, step 3223/7134 completed (loss: 0.38408222794532776, acc: 0.9386503100395203)
[2025-02-13 19:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31][root][INFO] - Training Epoch: 1/2, step 3224/7134 completed (loss: 0.3117040991783142, acc: 0.913294792175293)
[2025-02-13 19:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31][root][INFO] - Training Epoch: 1/2, step 3225/7134 completed (loss: 0.19266293942928314, acc: 0.9399999976158142)
[2025-02-13 19:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:32][root][INFO] - Training Epoch: 1/2, step 3226/7134 completed (loss: 0.28726431727409363, acc: 0.9139072895050049)
[2025-02-13 19:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:32][root][INFO] - Training Epoch: 1/2, step 3227/7134 completed (loss: 0.29470255970954895, acc: 0.9382715821266174)
[2025-02-13 19:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33][root][INFO] - Training Epoch: 1/2, step 3228/7134 completed (loss: 0.1995062530040741, acc: 0.9343434572219849)
[2025-02-13 19:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33][root][INFO] - Training Epoch: 1/2, step 3229/7134 completed (loss: 0.24556444585323334, acc: 0.9545454382896423)
[2025-02-13 19:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33][root][INFO] - Training Epoch: 1/2, step 3230/7134 completed (loss: 0.16762419044971466, acc: 0.956250011920929)
[2025-02-13 19:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34][root][INFO] - Training Epoch: 1/2, step 3231/7134 completed (loss: 0.2690935730934143, acc: 0.9450549483299255)
[2025-02-13 19:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34][root][INFO] - Training Epoch: 1/2, step 3232/7134 completed (loss: 0.062149956822395325, acc: 0.9861111044883728)
[2025-02-13 19:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34][root][INFO] - Training Epoch: 1/2, step 3233/7134 completed (loss: 0.315405935049057, acc: 0.9291338324546814)
[2025-02-13 19:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35][root][INFO] - Training Epoch: 1/2, step 3234/7134 completed (loss: 0.07759828120470047, acc: 0.9811320900917053)
[2025-02-13 19:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35][root][INFO] - Training Epoch: 1/2, step 3235/7134 completed (loss: 0.12083700299263, acc: 0.957446813583374)
[2025-02-13 19:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36][root][INFO] - Training Epoch: 1/2, step 3236/7134 completed (loss: 0.3135015368461609, acc: 0.909604549407959)
[2025-02-13 19:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36][root][INFO] - Training Epoch: 1/2, step 3237/7134 completed (loss: 0.16738684475421906, acc: 0.948051929473877)
[2025-02-13 19:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36][root][INFO] - Training Epoch: 1/2, step 3238/7134 completed (loss: 0.24196292459964752, acc: 0.9388889074325562)
[2025-02-13 19:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37][root][INFO] - Training Epoch: 1/2, step 3239/7134 completed (loss: 0.08199021220207214, acc: 0.9756097793579102)
[2025-02-13 19:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37][root][INFO] - Training Epoch: 1/2, step 3240/7134 completed (loss: 0.09153804928064346, acc: 0.9776536226272583)
[2025-02-13 19:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37][root][INFO] - Training Epoch: 1/2, step 3241/7134 completed (loss: 0.15823842585086823, acc: 0.9529411792755127)
[2025-02-13 19:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:38][root][INFO] - Training Epoch: 1/2, step 3242/7134 completed (loss: 0.08030250668525696, acc: 0.9740259647369385)
[2025-02-13 19:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:38][root][INFO] - Training Epoch: 1/2, step 3243/7134 completed (loss: 0.09042396396398544, acc: 0.9679144620895386)
[2025-02-13 19:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39][root][INFO] - Training Epoch: 1/2, step 3244/7134 completed (loss: 0.23643696308135986, acc: 0.9537572264671326)
[2025-02-13 19:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39][root][INFO] - Training Epoch: 1/2, step 3245/7134 completed (loss: 0.17307965457439423, acc: 0.9589743614196777)
[2025-02-13 19:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39][root][INFO] - Training Epoch: 1/2, step 3246/7134 completed (loss: 0.15907993912696838, acc: 0.9743589758872986)
[2025-02-13 19:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40][root][INFO] - Training Epoch: 1/2, step 3247/7134 completed (loss: 0.13438241183757782, acc: 0.9679487347602844)
[2025-02-13 19:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40][root][INFO] - Training Epoch: 1/2, step 3248/7134 completed (loss: 0.08721303939819336, acc: 0.985401451587677)
[2025-02-13 19:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41][root][INFO] - Training Epoch: 1/2, step 3249/7134 completed (loss: 0.11787798255681992, acc: 0.9644970297813416)
[2025-02-13 19:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41][root][INFO] - Training Epoch: 1/2, step 3250/7134 completed (loss: 0.22216568887233734, acc: 0.940119743347168)
[2025-02-13 19:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41][root][INFO] - Training Epoch: 1/2, step 3251/7134 completed (loss: 0.17696256935596466, acc: 0.9487179517745972)
[2025-02-13 19:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42][root][INFO] - Training Epoch: 1/2, step 3252/7134 completed (loss: 0.0499831959605217, acc: 1.0)
[2025-02-13 19:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42][root][INFO] - Training Epoch: 1/2, step 3253/7134 completed (loss: 0.12082415819168091, acc: 0.9605262875556946)
[2025-02-13 19:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42][root][INFO] - Training Epoch: 1/2, step 3254/7134 completed (loss: 0.13206692039966583, acc: 0.9740259647369385)
[2025-02-13 19:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43][root][INFO] - Training Epoch: 1/2, step 3255/7134 completed (loss: 0.18097759783267975, acc: 0.9659090638160706)
[2025-02-13 19:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43][root][INFO] - Training Epoch: 1/2, step 3256/7134 completed (loss: 0.10706118494272232, acc: 0.9813664555549622)
[2025-02-13 19:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43][root][INFO] - Training Epoch: 1/2, step 3257/7134 completed (loss: 0.31947943568229675, acc: 0.9575757384300232)
[2025-02-13 19:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44][root][INFO] - Training Epoch: 1/2, step 3258/7134 completed (loss: 0.49552005529403687, acc: 0.8943089246749878)
[2025-02-13 19:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44][root][INFO] - Training Epoch: 1/2, step 3259/7134 completed (loss: 0.29167068004608154, acc: 0.9192546606063843)
[2025-02-13 19:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45][root][INFO] - Training Epoch: 1/2, step 3260/7134 completed (loss: 0.32524412870407104, acc: 0.931034505367279)
[2025-02-13 19:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45][root][INFO] - Training Epoch: 1/2, step 3261/7134 completed (loss: 0.33305004239082336, acc: 0.8978102207183838)
[2025-02-13 19:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45][root][INFO] - Training Epoch: 1/2, step 3262/7134 completed (loss: 0.2466227263212204, acc: 0.9567901492118835)
[2025-02-13 19:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46][root][INFO] - Training Epoch: 1/2, step 3263/7134 completed (loss: 0.266523540019989, acc: 0.9053254723548889)
[2025-02-13 19:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46][root][INFO] - Training Epoch: 1/2, step 3264/7134 completed (loss: 0.2712072432041168, acc: 0.9329268336296082)
[2025-02-13 19:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47][root][INFO] - Training Epoch: 1/2, step 3265/7134 completed (loss: 0.2331438958644867, acc: 0.9490445852279663)
[2025-02-13 19:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47][root][INFO] - Training Epoch: 1/2, step 3266/7134 completed (loss: 0.15032930672168732, acc: 0.951724112033844)
[2025-02-13 19:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47][root][INFO] - Training Epoch: 1/2, step 3267/7134 completed (loss: 0.2584449052810669, acc: 0.9515151381492615)
[2025-02-13 19:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48][root][INFO] - Training Epoch: 1/2, step 3268/7134 completed (loss: 0.24760156869888306, acc: 0.9452054500579834)
[2025-02-13 19:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48][root][INFO] - Training Epoch: 1/2, step 3269/7134 completed (loss: 0.1784513294696808, acc: 0.9521276354789734)
[2025-02-13 19:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48][root][INFO] - Training Epoch: 1/2, step 3270/7134 completed (loss: 0.11896708607673645, acc: 0.9621621370315552)
[2025-02-13 19:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49][root][INFO] - Training Epoch: 1/2, step 3271/7134 completed (loss: 0.1364322155714035, acc: 0.98591548204422)
[2025-02-13 19:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49][root][INFO] - Training Epoch: 1/2, step 3272/7134 completed (loss: 0.21137076616287231, acc: 0.9666666388511658)
[2025-02-13 19:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49][root][INFO] - Training Epoch: 1/2, step 3273/7134 completed (loss: 0.1260242909193039, acc: 0.954285740852356)
[2025-02-13 19:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50][root][INFO] - Training Epoch: 1/2, step 3274/7134 completed (loss: 0.12492860853672028, acc: 0.9813664555549622)
[2025-02-13 19:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50][root][INFO] - Training Epoch: 1/2, step 3275/7134 completed (loss: 0.8554555773735046, acc: 0.8443113565444946)
[2025-02-13 19:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51][root][INFO] - Training Epoch: 1/2, step 3276/7134 completed (loss: 0.24406889081001282, acc: 0.9607843160629272)
[2025-02-13 19:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51][root][INFO] - Training Epoch: 1/2, step 3277/7134 completed (loss: 0.30193522572517395, acc: 0.9136690497398376)
[2025-02-13 19:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51][root][INFO] - Training Epoch: 1/2, step 3278/7134 completed (loss: 0.28653088212013245, acc: 0.9193548560142517)
[2025-02-13 19:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52][root][INFO] - Training Epoch: 1/2, step 3279/7134 completed (loss: 0.26507794857025146, acc: 0.9176470637321472)
[2025-02-13 19:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52][root][INFO] - Training Epoch: 1/2, step 3280/7134 completed (loss: 0.5953218340873718, acc: 0.8557692170143127)
[2025-02-13 19:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52][root][INFO] - Training Epoch: 1/2, step 3281/7134 completed (loss: 0.31608471274375916, acc: 0.9523809552192688)
[2025-02-13 19:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53][root][INFO] - Training Epoch: 1/2, step 3282/7134 completed (loss: 0.1561470776796341, acc: 0.9465649127960205)
[2025-02-13 19:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53][root][INFO] - Training Epoch: 1/2, step 3283/7134 completed (loss: 0.1479978710412979, acc: 0.9607843160629272)
[2025-02-13 19:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54][root][INFO] - Training Epoch: 1/2, step 3284/7134 completed (loss: 0.20923037827014923, acc: 0.9586206674575806)
[2025-02-13 19:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54][root][INFO] - Training Epoch: 1/2, step 3285/7134 completed (loss: 0.250886470079422, acc: 0.9411764740943909)
[2025-02-13 19:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54][root][INFO] - Training Epoch: 1/2, step 3286/7134 completed (loss: 0.2140548825263977, acc: 0.9538461565971375)
[2025-02-13 19:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55][root][INFO] - Training Epoch: 1/2, step 3287/7134 completed (loss: 0.22675926983356476, acc: 0.9333333373069763)
[2025-02-13 19:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55][root][INFO] - Training Epoch: 1/2, step 3288/7134 completed (loss: 0.2367255687713623, acc: 0.935251772403717)
[2025-02-13 19:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55][root][INFO] - Training Epoch: 1/2, step 3289/7134 completed (loss: 0.3070347011089325, acc: 0.9264705777168274)
[2025-02-13 19:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56][root][INFO] - Training Epoch: 1/2, step 3290/7134 completed (loss: 0.27399322390556335, acc: 0.9495798349380493)
[2025-02-13 19:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56][root][INFO] - Training Epoch: 1/2, step 3291/7134 completed (loss: 0.3148558735847473, acc: 0.9405940771102905)
[2025-02-13 19:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56][root][INFO] - Training Epoch: 1/2, step 3292/7134 completed (loss: 0.3310348391532898, acc: 0.9411764740943909)
[2025-02-13 19:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:57][root][INFO] - Training Epoch: 1/2, step 3293/7134 completed (loss: 0.20950210094451904, acc: 0.9469696879386902)
[2025-02-13 19:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:57][root][INFO] - Training Epoch: 1/2, step 3294/7134 completed (loss: 0.3191502094268799, acc: 0.9166666865348816)
[2025-02-13 19:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58][root][INFO] - Training Epoch: 1/2, step 3295/7134 completed (loss: 0.04204783961176872, acc: 1.0)
[2025-02-13 19:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58][root][INFO] - Training Epoch: 1/2, step 3296/7134 completed (loss: 0.2815437316894531, acc: 0.9307692050933838)
[2025-02-13 19:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58][root][INFO] - Training Epoch: 1/2, step 3297/7134 completed (loss: 0.344452828168869, acc: 0.8867924809455872)
[2025-02-13 19:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59][root][INFO] - Training Epoch: 1/2, step 3298/7134 completed (loss: 0.37996208667755127, acc: 0.9248120188713074)
[2025-02-13 19:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59][root][INFO] - Training Epoch: 1/2, step 3299/7134 completed (loss: 0.1832205206155777, acc: 0.9448819160461426)
[2025-02-13 19:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59][root][INFO] - Training Epoch: 1/2, step 3300/7134 completed (loss: 0.2560677230358124, acc: 0.932330846786499)
[2025-02-13 19:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00][root][INFO] - Training Epoch: 1/2, step 3301/7134 completed (loss: 0.18045967817306519, acc: 0.9417475461959839)
[2025-02-13 19:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00][root][INFO] - Training Epoch: 1/2, step 3302/7134 completed (loss: 0.2575169801712036, acc: 0.9510489702224731)
[2025-02-13 19:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00][root][INFO] - Training Epoch: 1/2, step 3303/7134 completed (loss: 0.32053813338279724, acc: 0.9603960514068604)
[2025-02-13 19:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01][root][INFO] - Training Epoch: 1/2, step 3304/7134 completed (loss: 0.11913960427045822, acc: 0.9724770784378052)
[2025-02-13 19:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01][root][INFO] - Training Epoch: 1/2, step 3305/7134 completed (loss: 0.204752579331398, acc: 0.955974817276001)
[2025-02-13 19:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01][root][INFO] - Training Epoch: 1/2, step 3306/7134 completed (loss: 0.13670143485069275, acc: 0.959770143032074)
[2025-02-13 19:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:02][root][INFO] - Training Epoch: 1/2, step 3307/7134 completed (loss: 0.10339643061161041, acc: 0.9777777791023254)
[2025-02-13 19:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:02][root][INFO] - Training Epoch: 1/2, step 3308/7134 completed (loss: 0.24454684555530548, acc: 0.9529411792755127)
[2025-02-13 19:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03][root][INFO] - Training Epoch: 1/2, step 3309/7134 completed (loss: 0.32445600628852844, acc: 0.9038461446762085)
[2025-02-13 19:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03][root][INFO] - Training Epoch: 1/2, step 3310/7134 completed (loss: 0.19567808508872986, acc: 0.9363636374473572)
[2025-02-13 19:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03][root][INFO] - Training Epoch: 1/2, step 3311/7134 completed (loss: 0.24562543630599976, acc: 0.9306930899620056)
[2025-02-13 19:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04][root][INFO] - Training Epoch: 1/2, step 3312/7134 completed (loss: 0.25793495774269104, acc: 0.932330846786499)
[2025-02-13 19:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04][root][INFO] - Training Epoch: 1/2, step 3313/7134 completed (loss: 0.3670235574245453, acc: 0.8877550959587097)
[2025-02-13 19:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05][root][INFO] - Training Epoch: 1/2, step 3314/7134 completed (loss: 0.2534165382385254, acc: 0.9357798099517822)
[2025-02-13 19:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05][root][INFO] - Training Epoch: 1/2, step 3315/7134 completed (loss: 0.24430477619171143, acc: 0.9408283829689026)
[2025-02-13 19:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05][root][INFO] - Training Epoch: 1/2, step 3316/7134 completed (loss: 0.27180320024490356, acc: 0.9104477763175964)
[2025-02-13 19:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06][root][INFO] - Training Epoch: 1/2, step 3317/7134 completed (loss: 0.4274963438510895, acc: 0.8842975497245789)
[2025-02-13 19:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06][root][INFO] - Training Epoch: 1/2, step 3318/7134 completed (loss: 0.34372034668922424, acc: 0.8983050584793091)
[2025-02-13 19:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06][root][INFO] - Training Epoch: 1/2, step 3319/7134 completed (loss: 0.27498963475227356, acc: 0.9347826242446899)
[2025-02-13 19:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07][root][INFO] - Training Epoch: 1/2, step 3320/7134 completed (loss: 0.29538100957870483, acc: 0.9378530979156494)
[2025-02-13 19:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07][root][INFO] - Training Epoch: 1/2, step 3321/7134 completed (loss: 0.3438335061073303, acc: 0.9142857193946838)
[2025-02-13 19:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08][root][INFO] - Training Epoch: 1/2, step 3322/7134 completed (loss: 0.3010472357273102, acc: 0.9078947305679321)
[2025-02-13 19:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08][root][INFO] - Training Epoch: 1/2, step 3323/7134 completed (loss: 0.24338550865650177, acc: 0.9239766001701355)
[2025-02-13 19:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08][root][INFO] - Training Epoch: 1/2, step 3324/7134 completed (loss: 0.347607284784317, acc: 0.9067357778549194)
[2025-02-13 19:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09][root][INFO] - Training Epoch: 1/2, step 3325/7134 completed (loss: 0.19738280773162842, acc: 0.9520958065986633)
[2025-02-13 19:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09][root][INFO] - Training Epoch: 1/2, step 3326/7134 completed (loss: 0.29355788230895996, acc: 0.9151515364646912)
[2025-02-13 19:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09][root][INFO] - Training Epoch: 1/2, step 3327/7134 completed (loss: 0.2606278359889984, acc: 0.9171270728111267)
[2025-02-13 19:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:10][root][INFO] - Training Epoch: 1/2, step 3328/7134 completed (loss: 0.1351752132177353, acc: 0.9680851101875305)
[2025-02-13 19:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:10][root][INFO] - Training Epoch: 1/2, step 3329/7134 completed (loss: 0.24024654924869537, acc: 0.9322916865348816)
[2025-02-13 19:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11][root][INFO] - Training Epoch: 1/2, step 3330/7134 completed (loss: 0.15105880796909332, acc: 0.9589743614196777)
[2025-02-13 19:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11][root][INFO] - Training Epoch: 1/2, step 3331/7134 completed (loss: 0.49935248494148254, acc: 0.8520709872245789)
[2025-02-13 19:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11][root][INFO] - Training Epoch: 1/2, step 3332/7134 completed (loss: 0.31449273228645325, acc: 0.9399999976158142)
[2025-02-13 19:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12][root][INFO] - Training Epoch: 1/2, step 3333/7134 completed (loss: 0.46438854932785034, acc: 0.904347836971283)
[2025-02-13 19:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12][root][INFO] - Training Epoch: 1/2, step 3334/7134 completed (loss: 0.2784274220466614, acc: 0.929347813129425)
[2025-02-13 19:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12][root][INFO] - Training Epoch: 1/2, step 3335/7134 completed (loss: 0.10160192102193832, acc: 0.95333331823349)
[2025-02-13 19:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13][root][INFO] - Training Epoch: 1/2, step 3336/7134 completed (loss: 0.0838041752576828, acc: 0.9820359349250793)
[2025-02-13 19:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13][root][INFO] - Training Epoch: 1/2, step 3337/7134 completed (loss: 0.08567333966493607, acc: 0.9831932783126831)
[2025-02-13 19:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14][root][INFO] - Training Epoch: 1/2, step 3338/7134 completed (loss: 0.102410688996315, acc: 0.9637305736541748)
[2025-02-13 19:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14][root][INFO] - Training Epoch: 1/2, step 3339/7134 completed (loss: 0.1988723874092102, acc: 0.9323671460151672)
[2025-02-13 19:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14][root][INFO] - Training Epoch: 1/2, step 3340/7134 completed (loss: 0.28469839692115784, acc: 0.9523809552192688)
[2025-02-13 19:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15][root][INFO] - Training Epoch: 1/2, step 3341/7134 completed (loss: 0.24787642061710358, acc: 0.9281437397003174)
[2025-02-13 19:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15][root][INFO] - Training Epoch: 1/2, step 3342/7134 completed (loss: 0.16579197347164154, acc: 0.9520547986030579)
[2025-02-13 19:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15][root][INFO] - Training Epoch: 1/2, step 3343/7134 completed (loss: 0.1654597669839859, acc: 0.9568965435028076)
[2025-02-13 19:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:16][root][INFO] - Training Epoch: 1/2, step 3344/7134 completed (loss: 0.1699800342321396, acc: 0.9779411554336548)
[2025-02-13 19:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:16][root][INFO] - Training Epoch: 1/2, step 3345/7134 completed (loss: 0.2069089412689209, acc: 0.9510489702224731)
[2025-02-13 19:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17][root][INFO] - Training Epoch: 1/2, step 3346/7134 completed (loss: 0.1721908450126648, acc: 0.9390243887901306)
[2025-02-13 19:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17][root][INFO] - Training Epoch: 1/2, step 3347/7134 completed (loss: 0.3785581886768341, acc: 0.9281045794487)
[2025-02-13 19:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17][root][INFO] - Training Epoch: 1/2, step 3348/7134 completed (loss: 0.14680425822734833, acc: 0.9620253443717957)
[2025-02-13 19:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18][root][INFO] - Training Epoch: 1/2, step 3349/7134 completed (loss: 0.16968555748462677, acc: 0.961240291595459)
[2025-02-13 19:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18][root][INFO] - Training Epoch: 1/2, step 3350/7134 completed (loss: 0.16289013624191284, acc: 0.9752066135406494)
[2025-02-13 19:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18][root][INFO] - Training Epoch: 1/2, step 3351/7134 completed (loss: 0.4053632915019989, acc: 0.9042553305625916)
[2025-02-13 19:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19][root][INFO] - Training Epoch: 1/2, step 3352/7134 completed (loss: 0.17056222259998322, acc: 0.9629629850387573)
[2025-02-13 19:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19][root][INFO] - Training Epoch: 1/2, step 3353/7134 completed (loss: 0.17977160215377808, acc: 0.9430894255638123)
[2025-02-13 19:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19][root][INFO] - Training Epoch: 1/2, step 3354/7134 completed (loss: 0.11023122817277908, acc: 0.9683544039726257)
[2025-02-13 19:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20][root][INFO] - Training Epoch: 1/2, step 3355/7134 completed (loss: 0.05656914412975311, acc: 0.9939024448394775)
[2025-02-13 19:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20][root][INFO] - Training Epoch: 1/2, step 3356/7134 completed (loss: 0.027271125465631485, acc: 1.0)
[2025-02-13 19:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21][root][INFO] - Training Epoch: 1/2, step 3357/7134 completed (loss: 0.1404256969690323, acc: 0.9640718698501587)
[2025-02-13 19:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21][root][INFO] - Training Epoch: 1/2, step 3358/7134 completed (loss: 0.42970898747444153, acc: 0.8823529481887817)
[2025-02-13 19:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21][root][INFO] - Training Epoch: 1/2, step 3359/7134 completed (loss: 0.2501711845397949, acc: 0.9178082346916199)
[2025-02-13 19:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22][root][INFO] - Training Epoch: 1/2, step 3360/7134 completed (loss: 0.23923790454864502, acc: 0.95652174949646)
[2025-02-13 19:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22][root][INFO] - Training Epoch: 1/2, step 3361/7134 completed (loss: 0.3417380154132843, acc: 0.9017857313156128)
[2025-02-13 19:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22][root][INFO] - Training Epoch: 1/2, step 3362/7134 completed (loss: 0.2143157720565796, acc: 0.918181836605072)
[2025-02-13 19:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:23][root][INFO] - Training Epoch: 1/2, step 3363/7134 completed (loss: 0.22310571372509003, acc: 0.9345794320106506)
[2025-02-13 19:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:23][root][INFO] - Training Epoch: 1/2, step 3364/7134 completed (loss: 0.44201526045799255, acc: 0.9019607901573181)
[2025-02-13 19:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24][root][INFO] - Training Epoch: 1/2, step 3365/7134 completed (loss: 0.27889716625213623, acc: 0.9026548862457275)
[2025-02-13 19:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24][root][INFO] - Training Epoch: 1/2, step 3366/7134 completed (loss: 0.1487938016653061, acc: 0.9629629850387573)
[2025-02-13 19:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24][root][INFO] - Training Epoch: 1/2, step 3367/7134 completed (loss: 0.2800217270851135, acc: 0.8983957171440125)
[2025-02-13 19:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25][root][INFO] - Training Epoch: 1/2, step 3368/7134 completed (loss: 0.30686837434768677, acc: 0.9378882050514221)
[2025-02-13 19:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25][root][INFO] - Training Epoch: 1/2, step 3369/7134 completed (loss: 0.22137956321239471, acc: 0.9459459185600281)
[2025-02-13 19:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25][root][INFO] - Training Epoch: 1/2, step 3370/7134 completed (loss: 0.17673133313655853, acc: 0.9509202241897583)
[2025-02-13 19:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26][root][INFO] - Training Epoch: 1/2, step 3371/7134 completed (loss: 0.1618747115135193, acc: 0.9652777910232544)
[2025-02-13 19:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26][root][INFO] - Training Epoch: 1/2, step 3372/7134 completed (loss: 0.21984972059726715, acc: 0.9319728016853333)
[2025-02-13 19:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26][root][INFO] - Training Epoch: 1/2, step 3373/7134 completed (loss: 0.15241581201553345, acc: 0.9803921580314636)
[2025-02-13 19:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27][root][INFO] - Training Epoch: 1/2, step 3374/7134 completed (loss: 0.11833636462688446, acc: 0.9828571677207947)
[2025-02-13 19:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27][root][INFO] - Training Epoch: 1/2, step 3375/7134 completed (loss: 0.14560970664024353, acc: 0.9640718698501587)
[2025-02-13 19:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28][root][INFO] - Training Epoch: 1/2, step 3376/7134 completed (loss: 0.05860155448317528, acc: 0.9894179701805115)
[2025-02-13 19:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28][root][INFO] - Training Epoch: 1/2, step 3377/7134 completed (loss: 0.08805660158395767, acc: 0.9698492288589478)
[2025-02-13 19:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28][root][INFO] - Training Epoch: 1/2, step 3378/7134 completed (loss: 0.07291658222675323, acc: 0.9802631735801697)
[2025-02-13 19:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29][root][INFO] - Training Epoch: 1/2, step 3379/7134 completed (loss: 0.18049053847789764, acc: 0.9459459185600281)
[2025-02-13 19:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29][root][INFO] - Training Epoch: 1/2, step 3380/7134 completed (loss: 0.16194181144237518, acc: 0.9492753744125366)
[2025-02-13 19:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29][root][INFO] - Training Epoch: 1/2, step 3381/7134 completed (loss: 0.12443078309297562, acc: 0.9756097793579102)
[2025-02-13 19:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30][root][INFO] - Training Epoch: 1/2, step 3382/7134 completed (loss: 0.11176634579896927, acc: 0.953125)
[2025-02-13 19:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30][root][INFO] - Training Epoch: 1/2, step 3383/7134 completed (loss: 0.13146205246448517, acc: 0.9860140085220337)
[2025-02-13 19:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30][root][INFO] - Training Epoch: 1/2, step 3384/7134 completed (loss: 0.26374804973602295, acc: 0.9197080135345459)
[2025-02-13 19:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:31][root][INFO] - Training Epoch: 1/2, step 3385/7134 completed (loss: 0.1006140410900116, acc: 0.9652777910232544)
[2025-02-13 19:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:31][root][INFO] - Training Epoch: 1/2, step 3386/7134 completed (loss: 0.12052768468856812, acc: 0.9644970297813416)
[2025-02-13 19:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32][root][INFO] - Training Epoch: 1/2, step 3387/7134 completed (loss: 0.24556386470794678, acc: 0.9360465407371521)
[2025-02-13 19:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32][root][INFO] - Training Epoch: 1/2, step 3388/7134 completed (loss: 0.16628439724445343, acc: 0.9487179517745972)
[2025-02-13 19:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32][root][INFO] - Training Epoch: 1/2, step 3389/7134 completed (loss: 0.16050882637500763, acc: 0.9617486596107483)
[2025-02-13 19:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33][root][INFO] - Training Epoch: 1/2, step 3390/7134 completed (loss: 0.16198156774044037, acc: 0.9545454382896423)
[2025-02-13 19:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33][root][INFO] - Training Epoch: 1/2, step 3391/7134 completed (loss: 0.09958913922309875, acc: 0.9817073345184326)
[2025-02-13 19:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33][root][INFO] - Training Epoch: 1/2, step 3392/7134 completed (loss: 0.13308443129062653, acc: 0.9770992398262024)
[2025-02-13 19:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34][root][INFO] - Training Epoch: 1/2, step 3393/7134 completed (loss: 0.19853302836418152, acc: 0.9510489702224731)
[2025-02-13 19:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34][root][INFO] - Training Epoch: 1/2, step 3394/7134 completed (loss: 0.16612058877944946, acc: 0.949999988079071)
[2025-02-13 19:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34][root][INFO] - Training Epoch: 1/2, step 3395/7134 completed (loss: 0.34723660349845886, acc: 0.9078947305679321)
[2025-02-13 19:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35][root][INFO] - Training Epoch: 1/2, step 3396/7134 completed (loss: 0.1576935201883316, acc: 0.9663461446762085)
[2025-02-13 19:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35][root][INFO] - Training Epoch: 1/2, step 3397/7134 completed (loss: 0.08938591182231903, acc: 0.9850746393203735)
[2025-02-13 19:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35][root][INFO] - Training Epoch: 1/2, step 3398/7134 completed (loss: 0.17998725175857544, acc: 0.95652174949646)
[2025-02-13 19:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:36][root][INFO] - Training Epoch: 1/2, step 3399/7134 completed (loss: 0.5021689534187317, acc: 0.9097222089767456)
[2025-02-13 19:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:36][root][INFO] - Training Epoch: 1/2, step 3400/7134 completed (loss: 0.26530003547668457, acc: 0.9212598204612732)
[2025-02-13 19:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37][root][INFO] - Training Epoch: 1/2, step 3401/7134 completed (loss: 0.1908697783946991, acc: 0.9523809552192688)
[2025-02-13 19:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37][root][INFO] - Training Epoch: 1/2, step 3402/7134 completed (loss: 0.5023402571678162, acc: 0.903743326663971)
[2025-02-13 19:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37][root][INFO] - Training Epoch: 1/2, step 3403/7134 completed (loss: 0.41041722893714905, acc: 0.8888888955116272)
[2025-02-13 19:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38][root][INFO] - Training Epoch: 1/2, step 3404/7134 completed (loss: 0.1005377396941185, acc: 0.9811320900917053)
[2025-02-13 19:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38][root][INFO] - Training Epoch: 1/2, step 3405/7134 completed (loss: 0.3298439383506775, acc: 0.9338235259056091)
[2025-02-13 19:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38][root][INFO] - Training Epoch: 1/2, step 3406/7134 completed (loss: 0.3942025303840637, acc: 0.9049999713897705)
[2025-02-13 19:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:39][root][INFO] - Training Epoch: 1/2, step 3407/7134 completed (loss: 0.19328953325748444, acc: 0.9386503100395203)
[2025-02-13 19:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:39][root][INFO] - Training Epoch: 1/2, step 3408/7134 completed (loss: 0.1451631486415863, acc: 0.9716981053352356)
[2025-02-13 19:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40][root][INFO] - Training Epoch: 1/2, step 3409/7134 completed (loss: 0.20874349772930145, acc: 0.959770143032074)
[2025-02-13 19:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40][root][INFO] - Training Epoch: 1/2, step 3410/7134 completed (loss: 0.24873670935630798, acc: 0.9512194991111755)
[2025-02-13 19:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40][root][INFO] - Training Epoch: 1/2, step 3411/7134 completed (loss: 0.2274566888809204, acc: 0.9505494236946106)
[2025-02-13 19:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41][root][INFO] - Training Epoch: 1/2, step 3412/7134 completed (loss: 0.3390018343925476, acc: 0.925000011920929)
[2025-02-13 19:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41][root][INFO] - Training Epoch: 1/2, step 3413/7134 completed (loss: 0.23336943984031677, acc: 0.9572649598121643)
[2025-02-13 19:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41][root][INFO] - Training Epoch: 1/2, step 3414/7134 completed (loss: 0.14417214691638947, acc: 0.9513888955116272)
[2025-02-13 19:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42][root][INFO] - Training Epoch: 1/2, step 3415/7134 completed (loss: 0.19243265688419342, acc: 0.9714285731315613)
[2025-02-13 19:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42][root][INFO] - Training Epoch: 1/2, step 3416/7134 completed (loss: 0.24316099286079407, acc: 0.9508196711540222)
[2025-02-13 19:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42][root][INFO] - Training Epoch: 1/2, step 3417/7134 completed (loss: 0.12857526540756226, acc: 0.970059871673584)
[2025-02-13 19:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43][root][INFO] - Training Epoch: 1/2, step 3418/7134 completed (loss: 0.15032653510570526, acc: 0.9570552110671997)
[2025-02-13 19:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43][root][INFO] - Training Epoch: 1/2, step 3419/7134 completed (loss: 0.18198856711387634, acc: 0.9698795080184937)
[2025-02-13 19:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44][root][INFO] - Training Epoch: 1/2, step 3420/7134 completed (loss: 0.0641198679804802, acc: 0.9735099077224731)
[2025-02-13 19:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44][root][INFO] - Training Epoch: 1/2, step 3421/7134 completed (loss: 0.2057093381881714, acc: 0.9551281929016113)
[2025-02-13 19:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44][root][INFO] - Training Epoch: 1/2, step 3422/7134 completed (loss: 0.15505026280879974, acc: 0.9632353186607361)
[2025-02-13 19:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45][root][INFO] - Training Epoch: 1/2, step 3423/7134 completed (loss: 0.039573460817337036, acc: 0.9934210777282715)
[2025-02-13 19:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45][root][INFO] - Training Epoch: 1/2, step 3424/7134 completed (loss: 0.20176538825035095, acc: 0.9399999976158142)
[2025-02-13 19:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45][root][INFO] - Training Epoch: 1/2, step 3425/7134 completed (loss: 0.3872661292552948, acc: 0.9263157844543457)
[2025-02-13 19:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46][root][INFO] - Training Epoch: 1/2, step 3426/7134 completed (loss: 0.23856621980667114, acc: 0.976190447807312)
[2025-02-13 19:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46][root][INFO] - Training Epoch: 1/2, step 3427/7134 completed (loss: 0.16549408435821533, acc: 0.9740259647369385)
[2025-02-13 19:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46][root][INFO] - Training Epoch: 1/2, step 3428/7134 completed (loss: 0.15654835104942322, acc: 0.9647058844566345)
[2025-02-13 19:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47][root][INFO] - Training Epoch: 1/2, step 3429/7134 completed (loss: 0.21305730938911438, acc: 0.9577465057373047)
[2025-02-13 19:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47][root][INFO] - Training Epoch: 1/2, step 3430/7134 completed (loss: 0.25592923164367676, acc: 0.931506872177124)
[2025-02-13 19:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48][root][INFO] - Training Epoch: 1/2, step 3431/7134 completed (loss: 0.12102013826370239, acc: 0.97826087474823)
[2025-02-13 19:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48][root][INFO] - Training Epoch: 1/2, step 3432/7134 completed (loss: 0.15419456362724304, acc: 0.9693251252174377)
[2025-02-13 19:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48][root][INFO] - Training Epoch: 1/2, step 3433/7134 completed (loss: 0.2409205138683319, acc: 0.9558823704719543)
[2025-02-13 19:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49][root][INFO] - Training Epoch: 1/2, step 3434/7134 completed (loss: 0.17978836596012115, acc: 0.9357143044471741)
[2025-02-13 19:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49][root][INFO] - Training Epoch: 1/2, step 3435/7134 completed (loss: 0.15963728725910187, acc: 0.9724137783050537)
[2025-02-13 19:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49][root][INFO] - Training Epoch: 1/2, step 3436/7134 completed (loss: 0.1751241236925125, acc: 0.9489051103591919)
[2025-02-13 19:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:50][root][INFO] - Training Epoch: 1/2, step 3437/7134 completed (loss: 0.26151585578918457, acc: 0.9256198406219482)
[2025-02-13 19:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:50][root][INFO] - Training Epoch: 1/2, step 3438/7134 completed (loss: 0.2517342269420624, acc: 0.9473684430122375)
[2025-02-13 19:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51][root][INFO] - Training Epoch: 1/2, step 3439/7134 completed (loss: 0.36638131737709045, acc: 0.920634925365448)
[2025-02-13 19:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51][root][INFO] - Training Epoch: 1/2, step 3440/7134 completed (loss: 0.4621625542640686, acc: 0.8706896305084229)
[2025-02-13 19:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51][root][INFO] - Training Epoch: 1/2, step 3441/7134 completed (loss: 0.36836791038513184, acc: 0.9186046719551086)
[2025-02-13 19:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52][root][INFO] - Training Epoch: 1/2, step 3442/7134 completed (loss: 0.14336514472961426, acc: 0.9645389914512634)
[2025-02-13 19:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52][root][INFO] - Training Epoch: 1/2, step 3443/7134 completed (loss: 0.2691674828529358, acc: 0.966292142868042)
[2025-02-13 19:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52][root][INFO] - Training Epoch: 1/2, step 3444/7134 completed (loss: 0.192424938082695, acc: 0.9426751732826233)
[2025-02-13 19:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53][root][INFO] - Training Epoch: 1/2, step 3445/7134 completed (loss: 0.45056167244911194, acc: 0.9097744226455688)
[2025-02-13 19:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53][root][INFO] - Training Epoch: 1/2, step 3446/7134 completed (loss: 0.404670774936676, acc: 0.8999999761581421)
[2025-02-13 19:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53][root][INFO] - Training Epoch: 1/2, step 3447/7134 completed (loss: 0.43682125210762024, acc: 0.9012345671653748)
[2025-02-13 19:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:54][root][INFO] - Training Epoch: 1/2, step 3448/7134 completed (loss: 0.3011390268802643, acc: 0.9306358098983765)
[2025-02-13 19:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:54][root][INFO] - Training Epoch: 1/2, step 3449/7134 completed (loss: 0.19083376228809357, acc: 0.9504950642585754)
[2025-02-13 19:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55][root][INFO] - Training Epoch: 1/2, step 3450/7134 completed (loss: 0.25434666872024536, acc: 0.9117646813392639)
[2025-02-13 19:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55][root][INFO] - Training Epoch: 1/2, step 3451/7134 completed (loss: 0.3950847387313843, acc: 0.8794326186180115)
[2025-02-13 19:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55][root][INFO] - Training Epoch: 1/2, step 3452/7134 completed (loss: 0.5362244248390198, acc: 0.8617886304855347)
[2025-02-13 19:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56][root][INFO] - Training Epoch: 1/2, step 3453/7134 completed (loss: 0.2684570848941803, acc: 0.9236111044883728)
[2025-02-13 19:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56][root][INFO] - Training Epoch: 1/2, step 3454/7134 completed (loss: 0.1964682638645172, acc: 0.9578313231468201)
[2025-02-13 19:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56][root][INFO] - Training Epoch: 1/2, step 3455/7134 completed (loss: 0.08958855271339417, acc: 0.9751552939414978)
[2025-02-13 19:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57][root][INFO] - Training Epoch: 1/2, step 3456/7134 completed (loss: 0.08907531201839447, acc: 0.9849246144294739)
[2025-02-13 19:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57][root][INFO] - Training Epoch: 1/2, step 3457/7134 completed (loss: 0.0673542469739914, acc: 0.9929577708244324)
[2025-02-13 19:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58][root][INFO] - Training Epoch: 1/2, step 3458/7134 completed (loss: 0.27577587962150574, acc: 0.9505494236946106)
[2025-02-13 19:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58][root][INFO] - Training Epoch: 1/2, step 3459/7134 completed (loss: 0.23771576583385468, acc: 0.9433962106704712)
[2025-02-13 19:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58][root][INFO] - Training Epoch: 1/2, step 3460/7134 completed (loss: 1.0360114574432373, acc: 0.7878788113594055)
[2025-02-13 19:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59][root][INFO] - Training Epoch: 1/2, step 3461/7134 completed (loss: 0.8735329508781433, acc: 0.8230769038200378)
[2025-02-13 19:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59][root][INFO] - Training Epoch: 1/2, step 3462/7134 completed (loss: 0.4007444679737091, acc: 0.9181286692619324)
[2025-02-13 19:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59][root][INFO] - Training Epoch: 1/2, step 3463/7134 completed (loss: 0.09348117560148239, acc: 0.9814814925193787)
[2025-02-13 19:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00][root][INFO] - Training Epoch: 1/2, step 3464/7134 completed (loss: 0.18219801783561707, acc: 0.9606741666793823)
[2025-02-13 19:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00][root][INFO] - Training Epoch: 1/2, step 3465/7134 completed (loss: 0.4009760022163391, acc: 0.9259259104728699)
[2025-02-13 19:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00][root][INFO] - Training Epoch: 1/2, step 3466/7134 completed (loss: 0.32519519329071045, acc: 0.9197530746459961)
[2025-02-13 19:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:01][root][INFO] - Training Epoch: 1/2, step 3467/7134 completed (loss: 0.14605170488357544, acc: 0.9576271176338196)
[2025-02-13 19:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:01][root][INFO] - Training Epoch: 1/2, step 3468/7134 completed (loss: 0.04957282543182373, acc: 1.0)
[2025-02-13 19:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02][root][INFO] - Training Epoch: 1/2, step 3469/7134 completed (loss: 0.17998185753822327, acc: 0.9548872113227844)
[2025-02-13 19:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02][root][INFO] - Training Epoch: 1/2, step 3470/7134 completed (loss: 0.05391966551542282, acc: 0.9814814925193787)
[2025-02-13 19:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02][root][INFO] - Training Epoch: 1/2, step 3471/7134 completed (loss: 0.21561694145202637, acc: 0.9333333373069763)
[2025-02-13 19:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03][root][INFO] - Training Epoch: 1/2, step 3472/7134 completed (loss: 0.06493382155895233, acc: 0.987261176109314)
[2025-02-13 19:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03][root][INFO] - Training Epoch: 1/2, step 3473/7134 completed (loss: 0.12167613208293915, acc: 0.9736841917037964)
[2025-02-13 19:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04][root][INFO] - Training Epoch: 1/2, step 3474/7134 completed (loss: 0.18778873980045319, acc: 0.9637681245803833)
[2025-02-13 19:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04][root][INFO] - Training Epoch: 1/2, step 3475/7134 completed (loss: 0.21400828659534454, acc: 0.9545454382896423)
[2025-02-13 19:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04][root][INFO] - Training Epoch: 1/2, step 3476/7134 completed (loss: 0.3645918369293213, acc: 0.931506872177124)
[2025-02-13 19:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05][root][INFO] - Training Epoch: 1/2, step 3477/7134 completed (loss: 0.18893064558506012, acc: 0.9594594836235046)
[2025-02-13 19:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05][root][INFO] - Training Epoch: 1/2, step 3478/7134 completed (loss: 0.05052730813622475, acc: 0.9940119981765747)
[2025-02-13 19:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05][root][INFO] - Training Epoch: 1/2, step 3479/7134 completed (loss: 0.03293988108634949, acc: 0.9871794581413269)
[2025-02-13 19:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06][root][INFO] - Training Epoch: 1/2, step 3480/7134 completed (loss: 0.21568788588047028, acc: 0.9725274443626404)
[2025-02-13 19:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06][root][INFO] - Training Epoch: 1/2, step 3481/7134 completed (loss: 0.13477082550525665, acc: 0.9774436354637146)
[2025-02-13 19:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06][root][INFO] - Training Epoch: 1/2, step 3482/7134 completed (loss: 0.019742093980312347, acc: 1.0)
[2025-02-13 19:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07][root][INFO] - Training Epoch: 1/2, step 3483/7134 completed (loss: 0.09216723591089249, acc: 0.9806451797485352)
[2025-02-13 19:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07][root][INFO] - Training Epoch: 1/2, step 3484/7134 completed (loss: 0.34924811124801636, acc: 0.9642857313156128)
[2025-02-13 19:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08][root][INFO] - Training Epoch: 1/2, step 3485/7134 completed (loss: 0.1689426153898239, acc: 0.9454545378684998)
[2025-02-13 19:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08][root][INFO] - Training Epoch: 1/2, step 3486/7134 completed (loss: 0.30482035875320435, acc: 0.9230769276618958)
[2025-02-13 19:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08][root][INFO] - Training Epoch: 1/2, step 3487/7134 completed (loss: 0.2414354532957077, acc: 0.9457831382751465)
[2025-02-13 19:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09][root][INFO] - Training Epoch: 1/2, step 3488/7134 completed (loss: 0.14944231510162354, acc: 0.9714285731315613)
[2025-02-13 19:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09][root][INFO] - Training Epoch: 1/2, step 3489/7134 completed (loss: 0.4009306728839874, acc: 0.8962963223457336)
[2025-02-13 19:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09][root][INFO] - Training Epoch: 1/2, step 3490/7134 completed (loss: 0.21771492063999176, acc: 0.9650349617004395)
[2025-02-13 19:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10][root][INFO] - Training Epoch: 1/2, step 3491/7134 completed (loss: 0.41329333186149597, acc: 0.9044585824012756)
[2025-02-13 19:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10][root][INFO] - Training Epoch: 1/2, step 3492/7134 completed (loss: 0.19887448847293854, acc: 0.9357798099517822)
[2025-02-13 19:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10][root][INFO] - Training Epoch: 1/2, step 3493/7134 completed (loss: 0.18098214268684387, acc: 0.9506173133850098)
[2025-02-13 19:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11][root][INFO] - Training Epoch: 1/2, step 3494/7134 completed (loss: 0.16848832368850708, acc: 0.9594594836235046)
[2025-02-13 19:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11][root][INFO] - Training Epoch: 1/2, step 3495/7134 completed (loss: 0.09831763803958893, acc: 0.9793814420700073)
[2025-02-13 19:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11][root][INFO] - Training Epoch: 1/2, step 3496/7134 completed (loss: 0.18583691120147705, acc: 0.9509803652763367)
[2025-02-13 19:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12][root][INFO] - Training Epoch: 1/2, step 3497/7134 completed (loss: 0.18212346732616425, acc: 0.949367105960846)
[2025-02-13 19:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12][root][INFO] - Training Epoch: 1/2, step 3498/7134 completed (loss: 0.33914080262184143, acc: 0.9303797483444214)
[2025-02-13 19:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13][root][INFO] - Training Epoch: 1/2, step 3499/7134 completed (loss: 0.2206578403711319, acc: 0.9542483687400818)
[2025-02-13 19:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13][root][INFO] - Training Epoch: 1/2, step 3500/7134 completed (loss: 0.2979462146759033, acc: 0.9356725215911865)
[2025-02-13 19:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13][root][INFO] - Training Epoch: 1/2, step 3501/7134 completed (loss: 0.2726130783557892, acc: 0.9242424368858337)
[2025-02-13 19:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14][root][INFO] - Training Epoch: 1/2, step 3502/7134 completed (loss: 0.4525579512119293, acc: 0.8716216087341309)
[2025-02-13 19:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14][root][INFO] - Training Epoch: 1/2, step 3503/7134 completed (loss: 0.3676444888114929, acc: 0.9064748287200928)
[2025-02-13 19:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14][root][INFO] - Training Epoch: 1/2, step 3504/7134 completed (loss: 0.06861452013254166, acc: 0.9733333587646484)
[2025-02-13 19:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15][root][INFO] - Training Epoch: 1/2, step 3505/7134 completed (loss: 0.19682617485523224, acc: 0.9444444179534912)
[2025-02-13 19:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15][root][INFO] - Training Epoch: 1/2, step 3506/7134 completed (loss: 0.17117246985435486, acc: 0.961240291595459)
[2025-02-13 19:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15][root][INFO] - Training Epoch: 1/2, step 3507/7134 completed (loss: 0.08795322477817535, acc: 0.9791666865348816)
[2025-02-13 19:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:16][root][INFO] - Training Epoch: 1/2, step 3508/7134 completed (loss: 0.12068546563386917, acc: 0.9602649211883545)
[2025-02-13 19:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:16][root][INFO] - Training Epoch: 1/2, step 3509/7134 completed (loss: 0.2650493085384369, acc: 0.9693251252174377)
[2025-02-13 19:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17][root][INFO] - Training Epoch: 1/2, step 3510/7134 completed (loss: 0.10725507885217667, acc: 0.969924807548523)
[2025-02-13 19:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17][root][INFO] - Training Epoch: 1/2, step 3511/7134 completed (loss: 0.0811898484826088, acc: 0.9852941036224365)
[2025-02-13 19:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17][root][INFO] - Training Epoch: 1/2, step 3512/7134 completed (loss: 0.249622642993927, acc: 0.9487179517745972)
[2025-02-13 19:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18][root][INFO] - Training Epoch: 1/2, step 3513/7134 completed (loss: 0.08286544680595398, acc: 0.97826087474823)
[2025-02-13 19:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18][root][INFO] - Training Epoch: 1/2, step 3514/7134 completed (loss: 0.08894842118024826, acc: 0.9760765433311462)
[2025-02-13 19:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18][root][INFO] - Training Epoch: 1/2, step 3515/7134 completed (loss: 0.04172910004854202, acc: 0.994350254535675)
[2025-02-13 19:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19][root][INFO] - Training Epoch: 1/2, step 3516/7134 completed (loss: 0.054754797369241714, acc: 1.0)
[2025-02-13 19:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19][root][INFO] - Training Epoch: 1/2, step 3517/7134 completed (loss: 0.07079120725393295, acc: 0.9848484992980957)
[2025-02-13 19:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20][root][INFO] - Training Epoch: 1/2, step 3518/7134 completed (loss: 0.19910597801208496, acc: 0.9407407641410828)
[2025-02-13 19:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20][root][INFO] - Training Epoch: 1/2, step 3519/7134 completed (loss: 0.09762696176767349, acc: 0.9733333587646484)
[2025-02-13 19:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20][root][INFO] - Training Epoch: 1/2, step 3520/7134 completed (loss: 0.18609356880187988, acc: 0.9595959782600403)
[2025-02-13 19:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21][root][INFO] - Training Epoch: 1/2, step 3521/7134 completed (loss: 0.05567897856235504, acc: 0.9820359349250793)
[2025-02-13 19:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21][root][INFO] - Training Epoch: 1/2, step 3522/7134 completed (loss: 0.058799128979444504, acc: 0.9777777791023254)
[2025-02-13 19:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22][root][INFO] - Training Epoch: 1/2, step 3523/7134 completed (loss: 0.053394727408885956, acc: 0.9867549538612366)
[2025-02-13 19:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22][root][INFO] - Training Epoch: 1/2, step 3524/7134 completed (loss: 0.09567303955554962, acc: 0.9835164546966553)
[2025-02-13 19:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22][root][INFO] - Training Epoch: 1/2, step 3525/7134 completed (loss: 0.08835697174072266, acc: 0.9662162065505981)
[2025-02-13 19:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23][root][INFO] - Training Epoch: 1/2, step 3526/7134 completed (loss: 0.12175928056240082, acc: 0.9595959782600403)
[2025-02-13 19:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23][root][INFO] - Training Epoch: 1/2, step 3527/7134 completed (loss: 0.16834889352321625, acc: 0.957446813583374)
[2025-02-13 19:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24][root][INFO] - Training Epoch: 1/2, step 3528/7134 completed (loss: 0.021595625206828117, acc: 1.0)
[2025-02-13 19:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24][root][INFO] - Training Epoch: 1/2, step 3529/7134 completed (loss: 0.11904033273458481, acc: 0.9545454382896423)
[2025-02-13 19:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24][root][INFO] - Training Epoch: 1/2, step 3530/7134 completed (loss: 0.2915956676006317, acc: 0.9159291982650757)
[2025-02-13 19:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25][root][INFO] - Training Epoch: 1/2, step 3531/7134 completed (loss: 0.1890793740749359, acc: 0.9375)
[2025-02-13 19:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25][root][INFO] - Training Epoch: 1/2, step 3532/7134 completed (loss: 0.09711561352014542, acc: 0.9729729890823364)
[2025-02-13 19:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25][root][INFO] - Training Epoch: 1/2, step 3533/7134 completed (loss: 0.13284021615982056, acc: 0.9644444584846497)
[2025-02-13 19:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26][root][INFO] - Training Epoch: 1/2, step 3534/7134 completed (loss: 0.1363580971956253, acc: 0.9766082167625427)
[2025-02-13 19:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26][root][INFO] - Training Epoch: 1/2, step 3535/7134 completed (loss: 0.14086845517158508, acc: 0.9640287756919861)
[2025-02-13 19:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26][root][INFO] - Training Epoch: 1/2, step 3536/7134 completed (loss: 0.232812762260437, acc: 0.9259259104728699)
[2025-02-13 19:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27][root][INFO] - Training Epoch: 1/2, step 3537/7134 completed (loss: 0.20201586186885834, acc: 0.953125)
[2025-02-13 19:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27][root][INFO] - Training Epoch: 1/2, step 3538/7134 completed (loss: 0.11886759102344513, acc: 0.9579831957817078)
[2025-02-13 19:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28][root][INFO] - Training Epoch: 1/2, step 3539/7134 completed (loss: 0.20892448723316193, acc: 0.931034505367279)
[2025-02-13 19:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28][root][INFO] - Training Epoch: 1/2, step 3540/7134 completed (loss: 0.18522845208644867, acc: 0.9538461565971375)
[2025-02-13 19:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28][root][INFO] - Training Epoch: 1/2, step 3541/7134 completed (loss: 0.18195182085037231, acc: 0.95333331823349)
[2025-02-13 19:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29][root][INFO] - Training Epoch: 1/2, step 3542/7134 completed (loss: 0.10440786927938461, acc: 0.9794520735740662)
[2025-02-13 19:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29][root][INFO] - Training Epoch: 1/2, step 3543/7134 completed (loss: 0.21382978558540344, acc: 0.9154929518699646)
[2025-02-13 19:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29][root][INFO] - Training Epoch: 1/2, step 3544/7134 completed (loss: 0.2773427963256836, acc: 0.9205297827720642)
[2025-02-13 19:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30][root][INFO] - Training Epoch: 1/2, step 3545/7134 completed (loss: 0.13390567898750305, acc: 0.9652777910232544)
[2025-02-13 19:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30][root][INFO] - Training Epoch: 1/2, step 3546/7134 completed (loss: 0.26825204491615295, acc: 0.935251772403717)
[2025-02-13 19:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30][root][INFO] - Training Epoch: 1/2, step 3547/7134 completed (loss: 0.37445127964019775, acc: 0.903954803943634)
[2025-02-13 19:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:31][root][INFO] - Training Epoch: 1/2, step 3548/7134 completed (loss: 0.20539677143096924, acc: 0.9541284441947937)
[2025-02-13 19:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:31][root][INFO] - Training Epoch: 1/2, step 3549/7134 completed (loss: 0.2505633533000946, acc: 0.9473684430122375)
[2025-02-13 19:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32][root][INFO] - Training Epoch: 1/2, step 3550/7134 completed (loss: 0.20895260572433472, acc: 0.9343065619468689)
[2025-02-13 19:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32][root][INFO] - Training Epoch: 1/2, step 3551/7134 completed (loss: 0.45134302973747253, acc: 0.8775510191917419)
[2025-02-13 19:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32][root][INFO] - Training Epoch: 1/2, step 3552/7134 completed (loss: 0.282133013010025, acc: 0.9212598204612732)
[2025-02-13 19:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33][root][INFO] - Training Epoch: 1/2, step 3553/7134 completed (loss: 0.25696995854377747, acc: 0.9411764740943909)
[2025-02-13 19:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33][root][INFO] - Training Epoch: 1/2, step 3554/7134 completed (loss: 0.39093104004859924, acc: 0.9202454090118408)
[2025-02-13 19:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33][root][INFO] - Training Epoch: 1/2, step 3555/7134 completed (loss: 0.4103107452392578, acc: 0.9096774458885193)
[2025-02-13 19:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:34][root][INFO] - Training Epoch: 1/2, step 3556/7134 completed (loss: 0.25828787684440613, acc: 0.9318181872367859)
[2025-02-13 19:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:34][root][INFO] - Training Epoch: 1/2, step 3557/7134 completed (loss: 0.31790491938591003, acc: 0.936170220375061)
[2025-02-13 19:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35][root][INFO] - Training Epoch: 1/2, step 3558/7134 completed (loss: 0.35295557975769043, acc: 0.8880000114440918)
[2025-02-13 19:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35][root][INFO] - Training Epoch: 1/2, step 3559/7134 completed (loss: 0.38133734464645386, acc: 0.8928571343421936)
[2025-02-13 19:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35][root][INFO] - Training Epoch: 1/2, step 3560/7134 completed (loss: 0.22377066314220428, acc: 0.9398906826972961)
[2025-02-13 19:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36][root][INFO] - Training Epoch: 1/2, step 3561/7134 completed (loss: 0.255231648683548, acc: 0.9491525292396545)
[2025-02-13 19:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36][root][INFO] - Training Epoch: 1/2, step 3562/7134 completed (loss: 0.17283666133880615, acc: 0.9485294222831726)
[2025-02-13 19:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36][root][INFO] - Training Epoch: 1/2, step 3563/7134 completed (loss: 0.22809332609176636, acc: 0.9432623982429504)
[2025-02-13 19:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:37][root][INFO] - Training Epoch: 1/2, step 3564/7134 completed (loss: 0.3182612359523773, acc: 0.9126983880996704)
[2025-02-13 19:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:37][root][INFO] - Training Epoch: 1/2, step 3565/7134 completed (loss: 0.3522421419620514, acc: 0.904411792755127)
[2025-02-13 19:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3001, device='cuda:0') eval_epoch_loss=tensor(0.2624, device='cuda:0') eval_epoch_acc=tensor(0.9366, device='cuda:0')
[2025-02-13 19:49:26][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:49:26][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:49:27][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_3566_loss_0.26242145895957947/model.pt
[2025-02-13 19:49:27][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:49:27][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.26242145895957947
[2025-02-13 19:49:27][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9365698099136353
[2025-02-13 19:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:27][root][INFO] - Training Epoch: 1/2, step 3566/7134 completed (loss: 0.4157836139202118, acc: 0.8823529481887817)
[2025-02-13 19:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:27][root][INFO] - Training Epoch: 1/2, step 3567/7134 completed (loss: 0.2216774970293045, acc: 0.9719101190567017)
[2025-02-13 19:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28][root][INFO] - Training Epoch: 1/2, step 3568/7134 completed (loss: 0.27243974804878235, acc: 0.9219858050346375)
[2025-02-13 19:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28][root][INFO] - Training Epoch: 1/2, step 3569/7134 completed (loss: 0.27217093110084534, acc: 0.9333333373069763)
[2025-02-13 19:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29][root][INFO] - Training Epoch: 1/2, step 3570/7134 completed (loss: 0.43539342284202576, acc: 0.8897058963775635)
[2025-02-13 19:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29][root][INFO] - Training Epoch: 1/2, step 3571/7134 completed (loss: 0.43513205647468567, acc: 0.915730357170105)
[2025-02-13 19:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29][root][INFO] - Training Epoch: 1/2, step 3572/7134 completed (loss: 0.1904405653476715, acc: 0.9523809552192688)
[2025-02-13 19:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30][root][INFO] - Training Epoch: 1/2, step 3573/7134 completed (loss: 0.25835123658180237, acc: 0.9276315569877625)
[2025-02-13 19:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30][root][INFO] - Training Epoch: 1/2, step 3574/7134 completed (loss: 0.2605101764202118, acc: 0.942148745059967)
[2025-02-13 19:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30][root][INFO] - Training Epoch: 1/2, step 3575/7134 completed (loss: 0.6439172625541687, acc: 0.8684210777282715)
[2025-02-13 19:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:31][root][INFO] - Training Epoch: 1/2, step 3576/7134 completed (loss: 0.24851815402507782, acc: 0.9104477763175964)
[2025-02-13 19:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:31][root][INFO] - Training Epoch: 1/2, step 3577/7134 completed (loss: 0.2637125253677368, acc: 0.938144326210022)
[2025-02-13 19:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32][root][INFO] - Training Epoch: 1/2, step 3578/7134 completed (loss: 0.12669846415519714, acc: 0.9518072009086609)
[2025-02-13 19:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32][root][INFO] - Training Epoch: 1/2, step 3579/7134 completed (loss: 0.29178762435913086, acc: 0.9244186282157898)
[2025-02-13 19:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32][root][INFO] - Training Epoch: 1/2, step 3580/7134 completed (loss: 0.5197198987007141, acc: 0.8670520186424255)
[2025-02-13 19:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33][root][INFO] - Training Epoch: 1/2, step 3581/7134 completed (loss: 0.4525062143802643, acc: 0.8647058606147766)
[2025-02-13 19:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33][root][INFO] - Training Epoch: 1/2, step 3582/7134 completed (loss: 0.4567616581916809, acc: 0.9059829115867615)
[2025-02-13 19:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33][root][INFO] - Training Epoch: 1/2, step 3583/7134 completed (loss: 0.4249507784843445, acc: 0.8922155499458313)
[2025-02-13 19:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:34][root][INFO] - Training Epoch: 1/2, step 3584/7134 completed (loss: 0.5160701870918274, acc: 0.8510638475418091)
[2025-02-13 19:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:34][root][INFO] - Training Epoch: 1/2, step 3585/7134 completed (loss: 0.31333449482917786, acc: 0.9325153231620789)
[2025-02-13 19:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35][root][INFO] - Training Epoch: 1/2, step 3586/7134 completed (loss: 0.29622194170951843, acc: 0.9426751732826233)
[2025-02-13 19:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35][root][INFO] - Training Epoch: 1/2, step 3587/7134 completed (loss: 0.29813626408576965, acc: 0.9230769276618958)
[2025-02-13 19:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35][root][INFO] - Training Epoch: 1/2, step 3588/7134 completed (loss: 0.24979497492313385, acc: 0.9479768872261047)
[2025-02-13 19:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36][root][INFO] - Training Epoch: 1/2, step 3589/7134 completed (loss: 0.28895482420921326, acc: 0.9617486596107483)
[2025-02-13 19:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36][root][INFO] - Training Epoch: 1/2, step 3590/7134 completed (loss: 0.11722257733345032, acc: 0.9666666388511658)
[2025-02-13 19:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36][root][INFO] - Training Epoch: 1/2, step 3591/7134 completed (loss: 0.2380778044462204, acc: 0.9473684430122375)
[2025-02-13 19:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37][root][INFO] - Training Epoch: 1/2, step 3592/7134 completed (loss: 0.11310654133558273, acc: 0.949999988079071)
[2025-02-13 19:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37][root][INFO] - Training Epoch: 1/2, step 3593/7134 completed (loss: 0.3511312007904053, acc: 0.9507042169570923)
[2025-02-13 19:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38][root][INFO] - Training Epoch: 1/2, step 3594/7134 completed (loss: 0.15565598011016846, acc: 0.9668874144554138)
[2025-02-13 19:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38][root][INFO] - Training Epoch: 1/2, step 3595/7134 completed (loss: 0.23553933203220367, acc: 0.9346405267715454)
[2025-02-13 19:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38][root][INFO] - Training Epoch: 1/2, step 3596/7134 completed (loss: 0.2729257345199585, acc: 0.9235293865203857)
[2025-02-13 19:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39][root][INFO] - Training Epoch: 1/2, step 3597/7134 completed (loss: 0.22199919819831848, acc: 0.9671052694320679)
[2025-02-13 19:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39][root][INFO] - Training Epoch: 1/2, step 3598/7134 completed (loss: 0.3153175115585327, acc: 0.9052132964134216)
[2025-02-13 19:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39][root][INFO] - Training Epoch: 1/2, step 3599/7134 completed (loss: 0.3298511505126953, acc: 0.908108115196228)
[2025-02-13 19:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40][root][INFO] - Training Epoch: 1/2, step 3600/7134 completed (loss: 0.32739606499671936, acc: 0.9222797751426697)
[2025-02-13 19:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40][root][INFO] - Training Epoch: 1/2, step 3601/7134 completed (loss: 0.761141300201416, acc: 0.8258426785469055)
[2025-02-13 19:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41][root][INFO] - Training Epoch: 1/2, step 3602/7134 completed (loss: 0.4639393389225006, acc: 0.8930232524871826)
[2025-02-13 19:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41][root][INFO] - Training Epoch: 1/2, step 3603/7134 completed (loss: 0.3234899640083313, acc: 0.893081784248352)
[2025-02-13 19:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41][root][INFO] - Training Epoch: 1/2, step 3604/7134 completed (loss: 0.4015430510044098, acc: 0.8897637724876404)
[2025-02-13 19:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:42][root][INFO] - Training Epoch: 1/2, step 3605/7134 completed (loss: 0.2856444716453552, acc: 0.9282511472702026)
[2025-02-13 19:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:42][root][INFO] - Training Epoch: 1/2, step 3606/7134 completed (loss: 0.5066614151000977, acc: 0.9178082346916199)
[2025-02-13 19:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43][root][INFO] - Training Epoch: 1/2, step 3607/7134 completed (loss: 0.24536104500293732, acc: 0.931506872177124)
[2025-02-13 19:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43][root][INFO] - Training Epoch: 1/2, step 3608/7134 completed (loss: 0.38258957862854004, acc: 0.917475700378418)
[2025-02-13 19:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43][root][INFO] - Training Epoch: 1/2, step 3609/7134 completed (loss: 0.2186596542596817, acc: 0.942105233669281)
[2025-02-13 19:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44][root][INFO] - Training Epoch: 1/2, step 3610/7134 completed (loss: 0.28722578287124634, acc: 0.9230769276618958)
[2025-02-13 19:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44][root][INFO] - Training Epoch: 1/2, step 3611/7134 completed (loss: 0.33475297689437866, acc: 0.9085714221000671)
[2025-02-13 19:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44][root][INFO] - Training Epoch: 1/2, step 3612/7134 completed (loss: 0.4320392608642578, acc: 0.8965517282485962)
[2025-02-13 19:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45][root][INFO] - Training Epoch: 1/2, step 3613/7134 completed (loss: 0.2628304958343506, acc: 0.9508196711540222)
[2025-02-13 19:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45][root][INFO] - Training Epoch: 1/2, step 3614/7134 completed (loss: 0.2119118720293045, acc: 0.9414893388748169)
[2025-02-13 19:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46][root][INFO] - Training Epoch: 1/2, step 3615/7134 completed (loss: 0.17458666861057281, acc: 0.9594594836235046)
[2025-02-13 19:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46][root][INFO] - Training Epoch: 1/2, step 3616/7134 completed (loss: 0.3298493027687073, acc: 0.9006211161613464)
[2025-02-13 19:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46][root][INFO] - Training Epoch: 1/2, step 3617/7134 completed (loss: 0.2037467062473297, acc: 0.9513513445854187)
[2025-02-13 19:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:47][root][INFO] - Training Epoch: 1/2, step 3618/7134 completed (loss: 0.3237858712673187, acc: 0.9386503100395203)
[2025-02-13 19:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:47][root][INFO] - Training Epoch: 1/2, step 3619/7134 completed (loss: 0.44631797075271606, acc: 0.8733624219894409)
[2025-02-13 19:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48][root][INFO] - Training Epoch: 1/2, step 3620/7134 completed (loss: 0.25502490997314453, acc: 0.910179615020752)
[2025-02-13 19:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48][root][INFO] - Training Epoch: 1/2, step 3621/7134 completed (loss: 0.20656666159629822, acc: 0.9363057613372803)
[2025-02-13 19:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48][root][INFO] - Training Epoch: 1/2, step 3622/7134 completed (loss: 0.21089740097522736, acc: 0.9350649118423462)
[2025-02-13 19:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:49][root][INFO] - Training Epoch: 1/2, step 3623/7134 completed (loss: 0.20076029002666473, acc: 0.9285714030265808)
[2025-02-13 19:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:49][root][INFO] - Training Epoch: 1/2, step 3624/7134 completed (loss: 0.2644796073436737, acc: 0.9424460530281067)
[2025-02-13 19:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50][root][INFO] - Training Epoch: 1/2, step 3625/7134 completed (loss: 0.464063435792923, acc: 0.8739495873451233)
[2025-02-13 19:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50][root][INFO] - Training Epoch: 1/2, step 3626/7134 completed (loss: 0.5631890892982483, acc: 0.8651685118675232)
[2025-02-13 19:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50][root][INFO] - Training Epoch: 1/2, step 3627/7134 completed (loss: 0.5368920564651489, acc: 0.8773584961891174)
[2025-02-13 19:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51][root][INFO] - Training Epoch: 1/2, step 3628/7134 completed (loss: 0.36844387650489807, acc: 0.9142857193946838)
[2025-02-13 19:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51][root][INFO] - Training Epoch: 1/2, step 3629/7134 completed (loss: 0.4883270859718323, acc: 0.8870967626571655)
[2025-02-13 19:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51][root][INFO] - Training Epoch: 1/2, step 3630/7134 completed (loss: 0.2889515161514282, acc: 0.9462365508079529)
[2025-02-13 19:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52][root][INFO] - Training Epoch: 1/2, step 3631/7134 completed (loss: 0.2915935218334198, acc: 0.8999999761581421)
[2025-02-13 19:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52][root][INFO] - Training Epoch: 1/2, step 3632/7134 completed (loss: 0.23384135961532593, acc: 0.9672130942344666)
[2025-02-13 19:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53][root][INFO] - Training Epoch: 1/2, step 3633/7134 completed (loss: 0.29952701926231384, acc: 0.9426751732826233)
[2025-02-13 19:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53][root][INFO] - Training Epoch: 1/2, step 3634/7134 completed (loss: 0.2692320644855499, acc: 0.9439252614974976)
[2025-02-13 19:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53][root][INFO] - Training Epoch: 1/2, step 3635/7134 completed (loss: 0.21055684983730316, acc: 0.9396551847457886)
[2025-02-13 19:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54][root][INFO] - Training Epoch: 1/2, step 3636/7134 completed (loss: 0.1493813842535019, acc: 0.9528301954269409)
[2025-02-13 19:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54][root][INFO] - Training Epoch: 1/2, step 3637/7134 completed (loss: 0.2717360556125641, acc: 0.9363636374473572)
[2025-02-13 19:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54][root][INFO] - Training Epoch: 1/2, step 3638/7134 completed (loss: 0.18891841173171997, acc: 0.9230769276618958)
[2025-02-13 19:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55][root][INFO] - Training Epoch: 1/2, step 3639/7134 completed (loss: 0.3093409836292267, acc: 0.957446813583374)
[2025-02-13 19:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55][root][INFO] - Training Epoch: 1/2, step 3640/7134 completed (loss: 0.2640930116176605, acc: 0.942148745059967)
[2025-02-13 19:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55][root][INFO] - Training Epoch: 1/2, step 3641/7134 completed (loss: 0.11205729097127914, acc: 0.975806474685669)
[2025-02-13 19:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:56][root][INFO] - Training Epoch: 1/2, step 3642/7134 completed (loss: 0.06794106960296631, acc: 0.9798657894134521)
[2025-02-13 19:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:56][root][INFO] - Training Epoch: 1/2, step 3643/7134 completed (loss: 0.158453568816185, acc: 0.9453125)
[2025-02-13 19:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57][root][INFO] - Training Epoch: 1/2, step 3644/7134 completed (loss: 0.27456891536712646, acc: 0.9324324131011963)
[2025-02-13 19:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57][root][INFO] - Training Epoch: 1/2, step 3645/7134 completed (loss: 0.45984411239624023, acc: 0.8984375)
[2025-02-13 19:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57][root][INFO] - Training Epoch: 1/2, step 3646/7134 completed (loss: 0.4993935227394104, acc: 0.9015151262283325)
[2025-02-13 19:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58][root][INFO] - Training Epoch: 1/2, step 3647/7134 completed (loss: 0.17869889736175537, acc: 0.9550561904907227)
[2025-02-13 19:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58][root][INFO] - Training Epoch: 1/2, step 3648/7134 completed (loss: 0.30359870195388794, acc: 0.9053254723548889)
[2025-02-13 19:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59][root][INFO] - Training Epoch: 1/2, step 3649/7134 completed (loss: 0.35150378942489624, acc: 0.9194630980491638)
[2025-02-13 19:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59][root][INFO] - Training Epoch: 1/2, step 3650/7134 completed (loss: 0.20447036623954773, acc: 0.9349112510681152)
[2025-02-13 19:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59][root][INFO] - Training Epoch: 1/2, step 3651/7134 completed (loss: 0.3794742822647095, acc: 0.891566276550293)
[2025-02-13 19:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00][root][INFO] - Training Epoch: 1/2, step 3652/7134 completed (loss: 0.266706645488739, acc: 0.9387755393981934)
[2025-02-13 19:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00][root][INFO] - Training Epoch: 1/2, step 3653/7134 completed (loss: 0.4834686517715454, acc: 0.8711656332015991)
[2025-02-13 19:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01][root][INFO] - Training Epoch: 1/2, step 3654/7134 completed (loss: 0.0929996445775032, acc: 0.9599999785423279)
[2025-02-13 19:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01][root][INFO] - Training Epoch: 1/2, step 3655/7134 completed (loss: 0.49696916341781616, acc: 0.8982036113739014)
[2025-02-13 19:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01][root][INFO] - Training Epoch: 1/2, step 3656/7134 completed (loss: 0.43622809648513794, acc: 0.9202898740768433)
[2025-02-13 19:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02][root][INFO] - Training Epoch: 1/2, step 3657/7134 completed (loss: 0.17051918804645538, acc: 0.9440559148788452)
[2025-02-13 19:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02][root][INFO] - Training Epoch: 1/2, step 3658/7134 completed (loss: 0.25555044412612915, acc: 0.9375)
[2025-02-13 19:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02][root][INFO] - Training Epoch: 1/2, step 3659/7134 completed (loss: 0.29555144906044006, acc: 0.9251700639724731)
[2025-02-13 19:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03][root][INFO] - Training Epoch: 1/2, step 3660/7134 completed (loss: 0.5706990957260132, acc: 0.8024691343307495)
[2025-02-13 19:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03][root][INFO] - Training Epoch: 1/2, step 3661/7134 completed (loss: 0.2188548892736435, acc: 0.9428571462631226)
[2025-02-13 19:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04][root][INFO] - Training Epoch: 1/2, step 3662/7134 completed (loss: 0.4369637072086334, acc: 0.888198733329773)
[2025-02-13 19:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04][root][INFO] - Training Epoch: 1/2, step 3663/7134 completed (loss: 0.35741615295410156, acc: 0.8984375)
[2025-02-13 19:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04][root][INFO] - Training Epoch: 1/2, step 3664/7134 completed (loss: 0.45994964241981506, acc: 0.8766233921051025)
[2025-02-13 19:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05][root][INFO] - Training Epoch: 1/2, step 3665/7134 completed (loss: 0.19239194691181183, acc: 0.9527027010917664)
[2025-02-13 19:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05][root][INFO] - Training Epoch: 1/2, step 3666/7134 completed (loss: 0.39827197790145874, acc: 0.8972602486610413)
[2025-02-13 19:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06][root][INFO] - Training Epoch: 1/2, step 3667/7134 completed (loss: 0.847658634185791, acc: 0.8294573426246643)
[2025-02-13 19:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06][root][INFO] - Training Epoch: 1/2, step 3668/7134 completed (loss: 0.3774353265762329, acc: 0.9103448390960693)
[2025-02-13 19:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06][root][INFO] - Training Epoch: 1/2, step 3669/7134 completed (loss: 0.13019882142543793, acc: 0.9752066135406494)
[2025-02-13 19:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:07][root][INFO] - Training Epoch: 1/2, step 3670/7134 completed (loss: 0.08883427083492279, acc: 0.9879518151283264)
[2025-02-13 19:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:07][root][INFO] - Training Epoch: 1/2, step 3671/7134 completed (loss: 0.12686489522457123, acc: 0.9683544039726257)
[2025-02-13 19:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08][root][INFO] - Training Epoch: 1/2, step 3672/7134 completed (loss: 0.16250312328338623, acc: 0.9613259434700012)
[2025-02-13 19:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08][root][INFO] - Training Epoch: 1/2, step 3673/7134 completed (loss: 0.22051545977592468, acc: 0.931034505367279)
[2025-02-13 19:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08][root][INFO] - Training Epoch: 1/2, step 3674/7134 completed (loss: 0.08496265113353729, acc: 0.9864864945411682)
[2025-02-13 19:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09][root][INFO] - Training Epoch: 1/2, step 3675/7134 completed (loss: 0.2062276303768158, acc: 0.950276255607605)
[2025-02-13 19:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09][root][INFO] - Training Epoch: 1/2, step 3676/7134 completed (loss: 0.21713337302207947, acc: 0.942307710647583)
[2025-02-13 19:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09][root][INFO] - Training Epoch: 1/2, step 3677/7134 completed (loss: 0.2660823166370392, acc: 0.9261363744735718)
[2025-02-13 19:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10][root][INFO] - Training Epoch: 1/2, step 3678/7134 completed (loss: 0.430003821849823, acc: 0.9303797483444214)
[2025-02-13 19:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10][root][INFO] - Training Epoch: 1/2, step 3679/7134 completed (loss: 0.17879697680473328, acc: 0.9538461565971375)
[2025-02-13 19:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10][root][INFO] - Training Epoch: 1/2, step 3680/7134 completed (loss: 0.2753315269947052, acc: 0.9399999976158142)
[2025-02-13 19:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11][root][INFO] - Training Epoch: 1/2, step 3681/7134 completed (loss: 0.20878663659095764, acc: 0.9437500238418579)
[2025-02-13 19:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11][root][INFO] - Training Epoch: 1/2, step 3682/7134 completed (loss: 0.09414522349834442, acc: 0.9642857313156128)
[2025-02-13 19:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12][root][INFO] - Training Epoch: 1/2, step 3683/7134 completed (loss: 0.10331302881240845, acc: 0.9852941036224365)
[2025-02-13 19:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12][root][INFO] - Training Epoch: 1/2, step 3684/7134 completed (loss: 0.07015475630760193, acc: 0.9934210777282715)
[2025-02-13 19:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12][root][INFO] - Training Epoch: 1/2, step 3685/7134 completed (loss: 0.16811126470565796, acc: 0.9430894255638123)
[2025-02-13 19:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:13][root][INFO] - Training Epoch: 1/2, step 3686/7134 completed (loss: 0.13931822776794434, acc: 0.9577465057373047)
[2025-02-13 19:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:13][root][INFO] - Training Epoch: 1/2, step 3687/7134 completed (loss: 0.14523455500602722, acc: 0.95652174949646)
[2025-02-13 19:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14][root][INFO] - Training Epoch: 1/2, step 3688/7134 completed (loss: 0.08652730286121368, acc: 0.9833333492279053)
[2025-02-13 19:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14][root][INFO] - Training Epoch: 1/2, step 3689/7134 completed (loss: 0.17383216321468353, acc: 0.9503546357154846)
[2025-02-13 19:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14][root][INFO] - Training Epoch: 1/2, step 3690/7134 completed (loss: 0.07525627315044403, acc: 0.9873417615890503)
[2025-02-13 19:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15][root][INFO] - Training Epoch: 1/2, step 3691/7134 completed (loss: 0.18312382698059082, acc: 0.9504950642585754)
[2025-02-13 19:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15][root][INFO] - Training Epoch: 1/2, step 3692/7134 completed (loss: 0.1537209004163742, acc: 0.9693251252174377)
[2025-02-13 19:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15][root][INFO] - Training Epoch: 1/2, step 3693/7134 completed (loss: 0.09005177021026611, acc: 0.9924242496490479)
[2025-02-13 19:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:16][root][INFO] - Training Epoch: 1/2, step 3694/7134 completed (loss: 0.2269945591688156, acc: 0.9437500238418579)
[2025-02-13 19:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:16][root][INFO] - Training Epoch: 1/2, step 3695/7134 completed (loss: 0.1407739818096161, acc: 0.9617834687232971)
[2025-02-13 19:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17][root][INFO] - Training Epoch: 1/2, step 3696/7134 completed (loss: 0.12671013176441193, acc: 0.977011501789093)
[2025-02-13 19:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17][root][INFO] - Training Epoch: 1/2, step 3697/7134 completed (loss: 0.11962714046239853, acc: 0.9756097793579102)
[2025-02-13 19:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17][root][INFO] - Training Epoch: 1/2, step 3698/7134 completed (loss: 0.08937563747167587, acc: 0.9724137783050537)
[2025-02-13 19:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:18][root][INFO] - Training Epoch: 1/2, step 3699/7134 completed (loss: 0.08980732411146164, acc: 0.9842519760131836)
[2025-02-13 19:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:18][root][INFO] - Training Epoch: 1/2, step 3700/7134 completed (loss: 0.056269776076078415, acc: 0.9937106966972351)
[2025-02-13 19:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:18][root][INFO] - Training Epoch: 1/2, step 3701/7134 completed (loss: 0.1854456514120102, acc: 0.9635036587715149)
[2025-02-13 19:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:19][root][INFO] - Training Epoch: 1/2, step 3702/7134 completed (loss: 0.1474934071302414, acc: 0.9818181991577148)
[2025-02-13 19:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:19][root][INFO] - Training Epoch: 1/2, step 3703/7134 completed (loss: 0.11709573864936829, acc: 0.9772727489471436)
[2025-02-13 19:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20][root][INFO] - Training Epoch: 1/2, step 3704/7134 completed (loss: 0.12661056220531464, acc: 0.9801980257034302)
[2025-02-13 19:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20][root][INFO] - Training Epoch: 1/2, step 3705/7134 completed (loss: 0.32862389087677, acc: 0.9263803958892822)
[2025-02-13 19:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20][root][INFO] - Training Epoch: 1/2, step 3706/7134 completed (loss: 0.3836601674556732, acc: 0.9108911156654358)
[2025-02-13 19:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21][root][INFO] - Training Epoch: 1/2, step 3707/7134 completed (loss: 0.10924580693244934, acc: 0.9866666793823242)
[2025-02-13 19:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21][root][INFO] - Training Epoch: 1/2, step 3708/7134 completed (loss: 0.23977111279964447, acc: 0.9384615421295166)
[2025-02-13 19:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21][root][INFO] - Training Epoch: 1/2, step 3709/7134 completed (loss: 0.14596319198608398, acc: 0.9583333134651184)
[2025-02-13 19:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22][root][INFO] - Training Epoch: 1/2, step 3710/7134 completed (loss: 0.18728119134902954, acc: 0.9537037014961243)
[2025-02-13 19:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22][root][INFO] - Training Epoch: 1/2, step 3711/7134 completed (loss: 0.19958876073360443, acc: 0.9505494236946106)
[2025-02-13 19:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23][root][INFO] - Training Epoch: 1/2, step 3712/7134 completed (loss: 0.3353438675403595, acc: 0.907608687877655)
[2025-02-13 19:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23][root][INFO] - Training Epoch: 1/2, step 3713/7134 completed (loss: 0.13579118251800537, acc: 0.9496402740478516)
[2025-02-13 19:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23][root][INFO] - Training Epoch: 1/2, step 3714/7134 completed (loss: 0.22844399511814117, acc: 0.9378238320350647)
[2025-02-13 19:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24][root][INFO] - Training Epoch: 1/2, step 3715/7134 completed (loss: 0.14955885708332062, acc: 0.957446813583374)
[2025-02-13 19:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24][root][INFO] - Training Epoch: 1/2, step 3716/7134 completed (loss: 0.3446536064147949, acc: 0.9238578677177429)
[2025-02-13 19:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24][root][INFO] - Training Epoch: 1/2, step 3717/7134 completed (loss: 0.14941191673278809, acc: 0.9634146094322205)
[2025-02-13 19:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25][root][INFO] - Training Epoch: 1/2, step 3718/7134 completed (loss: 0.21016494929790497, acc: 0.953125)
[2025-02-13 19:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25][root][INFO] - Training Epoch: 1/2, step 3719/7134 completed (loss: 0.22549034655094147, acc: 0.9415204524993896)
[2025-02-13 19:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26][root][INFO] - Training Epoch: 1/2, step 3720/7134 completed (loss: 0.06356953084468842, acc: 0.9810126423835754)
[2025-02-13 19:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26][root][INFO] - Training Epoch: 1/2, step 3721/7134 completed (loss: 0.15611301362514496, acc: 0.9479768872261047)
[2025-02-13 19:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26][root][INFO] - Training Epoch: 1/2, step 3722/7134 completed (loss: 0.16244754195213318, acc: 0.9430052042007446)
[2025-02-13 19:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27][root][INFO] - Training Epoch: 1/2, step 3723/7134 completed (loss: 0.18688000738620758, acc: 0.9677419066429138)
[2025-02-13 19:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27][root][INFO] - Training Epoch: 1/2, step 3724/7134 completed (loss: 0.12469418346881866, acc: 0.9759036302566528)
[2025-02-13 19:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28][root][INFO] - Training Epoch: 1/2, step 3725/7134 completed (loss: 0.3894089162349701, acc: 0.9390243887901306)
[2025-02-13 19:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28][root][INFO] - Training Epoch: 1/2, step 3726/7134 completed (loss: 0.1327686905860901, acc: 0.9497487545013428)
[2025-02-13 19:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28][root][INFO] - Training Epoch: 1/2, step 3727/7134 completed (loss: 0.15766094624996185, acc: 0.9653179049491882)
[2025-02-13 19:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29][root][INFO] - Training Epoch: 1/2, step 3728/7134 completed (loss: 0.08381742238998413, acc: 0.9802955389022827)
[2025-02-13 19:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29][root][INFO] - Training Epoch: 1/2, step 3729/7134 completed (loss: 0.147000253200531, acc: 0.9622641801834106)
[2025-02-13 19:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29][root][INFO] - Training Epoch: 1/2, step 3730/7134 completed (loss: 0.1963372677564621, acc: 0.9454545378684998)
[2025-02-13 19:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30][root][INFO] - Training Epoch: 1/2, step 3731/7134 completed (loss: 0.08286836743354797, acc: 0.9712643623352051)
[2025-02-13 19:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30][root][INFO] - Training Epoch: 1/2, step 3732/7134 completed (loss: 0.11961537599563599, acc: 0.964102566242218)
[2025-02-13 19:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31][root][INFO] - Training Epoch: 1/2, step 3733/7134 completed (loss: 0.203172504901886, acc: 0.9503105878829956)
[2025-02-13 19:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31][root][INFO] - Training Epoch: 1/2, step 3734/7134 completed (loss: 0.3953714966773987, acc: 0.8974359035491943)
[2025-02-13 19:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31][root][INFO] - Training Epoch: 1/2, step 3735/7134 completed (loss: 0.1943337470293045, acc: 0.9545454382896423)
[2025-02-13 19:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32][root][INFO] - Training Epoch: 1/2, step 3736/7134 completed (loss: 0.2532126009464264, acc: 0.9411764740943909)
[2025-02-13 19:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32][root][INFO] - Training Epoch: 1/2, step 3737/7134 completed (loss: 0.21721847355365753, acc: 0.9333333373069763)
[2025-02-13 19:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33][root][INFO] - Training Epoch: 1/2, step 3738/7134 completed (loss: 0.09669402241706848, acc: 0.9722222089767456)
[2025-02-13 19:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33][root][INFO] - Training Epoch: 1/2, step 3739/7134 completed (loss: 0.1781357228755951, acc: 0.957446813583374)
[2025-02-13 19:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33][root][INFO] - Training Epoch: 1/2, step 3740/7134 completed (loss: 0.17607919871807098, acc: 0.9490445852279663)
[2025-02-13 19:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:34][root][INFO] - Training Epoch: 1/2, step 3741/7134 completed (loss: 0.13824625313282013, acc: 0.977011501789093)
[2025-02-13 19:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:34][root][INFO] - Training Epoch: 1/2, step 3742/7134 completed (loss: 0.23225371539592743, acc: 0.9591836929321289)
[2025-02-13 19:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35][root][INFO] - Training Epoch: 1/2, step 3743/7134 completed (loss: 0.21382610499858856, acc: 0.9352940917015076)
[2025-02-13 19:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35][root][INFO] - Training Epoch: 1/2, step 3744/7134 completed (loss: 0.24047155678272247, acc: 0.9418604373931885)
[2025-02-13 19:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36][root][INFO] - Training Epoch: 1/2, step 3745/7134 completed (loss: 0.2250135838985443, acc: 0.9428571462631226)
[2025-02-13 19:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36][root][INFO] - Training Epoch: 1/2, step 3746/7134 completed (loss: 0.16032278537750244, acc: 0.9604519605636597)
[2025-02-13 19:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36][root][INFO] - Training Epoch: 1/2, step 3747/7134 completed (loss: 0.17329645156860352, acc: 0.9388889074325562)
[2025-02-13 19:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37][root][INFO] - Training Epoch: 1/2, step 3748/7134 completed (loss: 0.18978512287139893, acc: 0.9333333373069763)
[2025-02-13 19:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37][root][INFO] - Training Epoch: 1/2, step 3749/7134 completed (loss: 0.3540635108947754, acc: 0.9107142686843872)
[2025-02-13 19:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37][root][INFO] - Training Epoch: 1/2, step 3750/7134 completed (loss: 0.07158232480287552, acc: 0.9754601120948792)
[2025-02-13 19:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38][root][INFO] - Training Epoch: 1/2, step 3751/7134 completed (loss: 0.19723588228225708, acc: 0.9518072009086609)
[2025-02-13 19:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38][root][INFO] - Training Epoch: 1/2, step 3752/7134 completed (loss: 0.2633703052997589, acc: 0.9371069073677063)
[2025-02-13 19:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39][root][INFO] - Training Epoch: 1/2, step 3753/7134 completed (loss: 0.3436805009841919, acc: 0.9018405079841614)
[2025-02-13 19:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39][root][INFO] - Training Epoch: 1/2, step 3754/7134 completed (loss: 0.06976291537284851, acc: 0.9741935729980469)
[2025-02-13 19:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39][root][INFO] - Training Epoch: 1/2, step 3755/7134 completed (loss: 0.18744125962257385, acc: 0.9504132270812988)
[2025-02-13 19:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40][root][INFO] - Training Epoch: 1/2, step 3756/7134 completed (loss: 0.161685049533844, acc: 0.9537572264671326)
[2025-02-13 19:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40][root][INFO] - Training Epoch: 1/2, step 3757/7134 completed (loss: 0.28658097982406616, acc: 0.9285714030265808)
[2025-02-13 19:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41][root][INFO] - Training Epoch: 1/2, step 3758/7134 completed (loss: 0.2621488869190216, acc: 0.9370078444480896)
[2025-02-13 19:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41][root][INFO] - Training Epoch: 1/2, step 3759/7134 completed (loss: 0.191448375582695, acc: 0.9440559148788452)
[2025-02-13 19:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41][root][INFO] - Training Epoch: 1/2, step 3760/7134 completed (loss: 0.22922809422016144, acc: 0.9390243887901306)
[2025-02-13 19:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42][root][INFO] - Training Epoch: 1/2, step 3761/7134 completed (loss: 0.10782888531684875, acc: 0.9696969985961914)
[2025-02-13 19:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42][root][INFO] - Training Epoch: 1/2, step 3762/7134 completed (loss: 0.1707163006067276, acc: 0.9342105388641357)
[2025-02-13 19:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43][root][INFO] - Training Epoch: 1/2, step 3763/7134 completed (loss: 0.07890628278255463, acc: 0.9857142567634583)
[2025-02-13 19:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43][root][INFO] - Training Epoch: 1/2, step 3764/7134 completed (loss: 0.19970308244228363, acc: 0.946107804775238)
[2025-02-13 19:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43][root][INFO] - Training Epoch: 1/2, step 3765/7134 completed (loss: 0.12897120416164398, acc: 0.9659090638160706)
[2025-02-13 19:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:44][root][INFO] - Training Epoch: 1/2, step 3766/7134 completed (loss: 0.217833012342453, acc: 0.9395973086357117)
[2025-02-13 19:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:44][root][INFO] - Training Epoch: 1/2, step 3767/7134 completed (loss: 0.17645230889320374, acc: 0.9608938694000244)
[2025-02-13 19:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45][root][INFO] - Training Epoch: 1/2, step 3768/7134 completed (loss: 0.2234978824853897, acc: 0.942307710647583)
[2025-02-13 19:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45][root][INFO] - Training Epoch: 1/2, step 3769/7134 completed (loss: 0.1983477920293808, acc: 0.929411768913269)
[2025-02-13 19:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45][root][INFO] - Training Epoch: 1/2, step 3770/7134 completed (loss: 0.14393529295921326, acc: 0.9597315192222595)
[2025-02-13 19:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46][root][INFO] - Training Epoch: 1/2, step 3771/7134 completed (loss: 0.06966352462768555, acc: 0.9868420958518982)
[2025-02-13 19:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46][root][INFO] - Training Epoch: 1/2, step 3772/7134 completed (loss: 0.17310865223407745, acc: 0.949999988079071)
[2025-02-13 19:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47][root][INFO] - Training Epoch: 1/2, step 3773/7134 completed (loss: 0.1532045155763626, acc: 0.9627659320831299)
[2025-02-13 19:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47][root][INFO] - Training Epoch: 1/2, step 3774/7134 completed (loss: 0.1894758939743042, acc: 0.9450549483299255)
[2025-02-13 19:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47][root][INFO] - Training Epoch: 1/2, step 3775/7134 completed (loss: 0.16193687915802002, acc: 0.9440993666648865)
[2025-02-13 19:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48][root][INFO] - Training Epoch: 1/2, step 3776/7134 completed (loss: 0.2700745165348053, acc: 0.9358974099159241)
[2025-02-13 19:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48][root][INFO] - Training Epoch: 1/2, step 3777/7134 completed (loss: 0.1312028169631958, acc: 0.970588207244873)
[2025-02-13 19:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49][root][INFO] - Training Epoch: 1/2, step 3778/7134 completed (loss: 0.19184543192386627, acc: 0.9593023061752319)
[2025-02-13 19:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49][root][INFO] - Training Epoch: 1/2, step 3779/7134 completed (loss: 0.19516687095165253, acc: 0.9437500238418579)
[2025-02-13 19:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49][root][INFO] - Training Epoch: 1/2, step 3780/7134 completed (loss: 0.11352987587451935, acc: 0.9757575988769531)
[2025-02-13 19:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:50][root][INFO] - Training Epoch: 1/2, step 3781/7134 completed (loss: 0.30454501509666443, acc: 0.9254658222198486)
[2025-02-13 19:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:50][root][INFO] - Training Epoch: 1/2, step 3782/7134 completed (loss: 0.1801806092262268, acc: 0.955974817276001)
[2025-02-13 19:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51][root][INFO] - Training Epoch: 1/2, step 3783/7134 completed (loss: 0.23479817807674408, acc: 0.9526627063751221)
[2025-02-13 19:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51][root][INFO] - Training Epoch: 1/2, step 3784/7134 completed (loss: 0.20147496461868286, acc: 0.9597315192222595)
[2025-02-13 19:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51][root][INFO] - Training Epoch: 1/2, step 3785/7134 completed (loss: 0.14607509970664978, acc: 0.978723406791687)
[2025-02-13 19:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52][root][INFO] - Training Epoch: 1/2, step 3786/7134 completed (loss: 0.12240658700466156, acc: 0.9702380895614624)
[2025-02-13 19:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52][root][INFO] - Training Epoch: 1/2, step 3787/7134 completed (loss: 0.14789526164531708, acc: 0.9538461565971375)
[2025-02-13 19:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53][root][INFO] - Training Epoch: 1/2, step 3788/7134 completed (loss: 0.31131672859191895, acc: 0.9325153231620789)
[2025-02-13 19:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53][root][INFO] - Training Epoch: 1/2, step 3789/7134 completed (loss: 0.2711270749568939, acc: 0.929411768913269)
[2025-02-13 19:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53][root][INFO] - Training Epoch: 1/2, step 3790/7134 completed (loss: 0.395153671503067, acc: 0.9117646813392639)
[2025-02-13 19:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54][root][INFO] - Training Epoch: 1/2, step 3791/7134 completed (loss: 0.1562625616788864, acc: 0.9595375657081604)
[2025-02-13 19:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54][root][INFO] - Training Epoch: 1/2, step 3792/7134 completed (loss: 0.18711787462234497, acc: 0.932330846786499)
[2025-02-13 19:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55][root][INFO] - Training Epoch: 1/2, step 3793/7134 completed (loss: 0.17181265354156494, acc: 0.959770143032074)
[2025-02-13 19:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55][root][INFO] - Training Epoch: 1/2, step 3794/7134 completed (loss: 0.35548847913742065, acc: 0.9277777671813965)
[2025-02-13 19:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55][root][INFO] - Training Epoch: 1/2, step 3795/7134 completed (loss: 0.23216789960861206, acc: 0.949999988079071)
[2025-02-13 19:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56][root][INFO] - Training Epoch: 1/2, step 3796/7134 completed (loss: 0.25098979473114014, acc: 0.9181286692619324)
[2025-02-13 19:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56][root][INFO] - Training Epoch: 1/2, step 3797/7134 completed (loss: 0.12189875543117523, acc: 0.9575757384300232)
[2025-02-13 19:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56][root][INFO] - Training Epoch: 1/2, step 3798/7134 completed (loss: 0.4236348867416382, acc: 0.9182389974594116)
[2025-02-13 19:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57][root][INFO] - Training Epoch: 1/2, step 3799/7134 completed (loss: 0.4032515585422516, acc: 0.9209039807319641)
[2025-02-13 19:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57][root][INFO] - Training Epoch: 1/2, step 3800/7134 completed (loss: 0.39545053243637085, acc: 0.9172413945198059)
[2025-02-13 19:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58][root][INFO] - Training Epoch: 1/2, step 3801/7134 completed (loss: 0.1757918894290924, acc: 0.950276255607605)
[2025-02-13 19:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58][root][INFO] - Training Epoch: 1/2, step 3802/7134 completed (loss: 0.21438287198543549, acc: 0.9427083134651184)
[2025-02-13 19:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58][root][INFO] - Training Epoch: 1/2, step 3803/7134 completed (loss: 0.286024808883667, acc: 0.9235668778419495)
[2025-02-13 19:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59][root][INFO] - Training Epoch: 1/2, step 3804/7134 completed (loss: 0.2551328241825104, acc: 0.9647887349128723)
[2025-02-13 19:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59][root][INFO] - Training Epoch: 1/2, step 3805/7134 completed (loss: 0.22868028283119202, acc: 0.954285740852356)
[2025-02-13 19:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00][root][INFO] - Training Epoch: 1/2, step 3806/7134 completed (loss: 0.21943353116512299, acc: 0.9444444179534912)
[2025-02-13 19:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00][root][INFO] - Training Epoch: 1/2, step 3807/7134 completed (loss: 0.14151374995708466, acc: 0.9589040875434875)
[2025-02-13 19:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01][root][INFO] - Training Epoch: 1/2, step 3808/7134 completed (loss: 0.27522704005241394, acc: 0.9347826242446899)
[2025-02-13 19:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01][root][INFO] - Training Epoch: 1/2, step 3809/7134 completed (loss: 0.2027355283498764, acc: 0.9651162624359131)
[2025-02-13 19:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01][root][INFO] - Training Epoch: 1/2, step 3810/7134 completed (loss: 0.18504054844379425, acc: 0.9415584206581116)
[2025-02-13 19:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02][root][INFO] - Training Epoch: 1/2, step 3811/7134 completed (loss: 0.2971399128437042, acc: 0.940119743347168)
[2025-02-13 19:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02][root][INFO] - Training Epoch: 1/2, step 3812/7134 completed (loss: 0.4838520884513855, acc: 0.8911564350128174)
[2025-02-13 19:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02][root][INFO] - Training Epoch: 1/2, step 3813/7134 completed (loss: 0.42301511764526367, acc: 0.9189189076423645)
[2025-02-13 19:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03][root][INFO] - Training Epoch: 1/2, step 3814/7134 completed (loss: 0.12930840253829956, acc: 0.977142870426178)
[2025-02-13 19:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03][root][INFO] - Training Epoch: 1/2, step 3815/7134 completed (loss: 0.20097312331199646, acc: 0.9539473652839661)
[2025-02-13 19:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03][root][INFO] - Training Epoch: 1/2, step 3816/7134 completed (loss: 0.1530488133430481, acc: 0.9626865386962891)
[2025-02-13 19:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:04][root][INFO] - Training Epoch: 1/2, step 3817/7134 completed (loss: 0.28937339782714844, acc: 0.9101123809814453)
[2025-02-13 19:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:04][root][INFO] - Training Epoch: 1/2, step 3818/7134 completed (loss: 0.360821932554245, acc: 0.9210526347160339)
[2025-02-13 19:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05][root][INFO] - Training Epoch: 1/2, step 3819/7134 completed (loss: 0.32606300711631775, acc: 0.9142857193946838)
[2025-02-13 19:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05][root][INFO] - Training Epoch: 1/2, step 3820/7134 completed (loss: 0.29126080870628357, acc: 0.9166666865348816)
[2025-02-13 19:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05][root][INFO] - Training Epoch: 1/2, step 3821/7134 completed (loss: 0.15340949594974518, acc: 0.965753436088562)
[2025-02-13 19:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06][root][INFO] - Training Epoch: 1/2, step 3822/7134 completed (loss: 0.18588420748710632, acc: 0.9622641801834106)
[2025-02-13 19:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06][root][INFO] - Training Epoch: 1/2, step 3823/7134 completed (loss: 0.07495784759521484, acc: 0.9932885766029358)
[2025-02-13 19:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06][root][INFO] - Training Epoch: 1/2, step 3824/7134 completed (loss: 0.2035994827747345, acc: 0.953125)
[2025-02-13 19:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:07][root][INFO] - Training Epoch: 1/2, step 3825/7134 completed (loss: 0.2113228738307953, acc: 0.9351851940155029)
[2025-02-13 19:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:07][root][INFO] - Training Epoch: 1/2, step 3826/7134 completed (loss: 0.17171934247016907, acc: 0.9507042169570923)
[2025-02-13 19:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08][root][INFO] - Training Epoch: 1/2, step 3827/7134 completed (loss: 0.0934855192899704, acc: 0.9808917045593262)
[2025-02-13 19:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08][root][INFO] - Training Epoch: 1/2, step 3828/7134 completed (loss: 0.09196050465106964, acc: 0.987500011920929)
[2025-02-13 19:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08][root][INFO] - Training Epoch: 1/2, step 3829/7134 completed (loss: 0.21537382900714874, acc: 0.9296875)
[2025-02-13 19:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:09][root][INFO] - Training Epoch: 1/2, step 3830/7134 completed (loss: 0.2404545247554779, acc: 0.9629629850387573)
[2025-02-13 19:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:09][root][INFO] - Training Epoch: 1/2, step 3831/7134 completed (loss: 0.28606081008911133, acc: 0.9342105388641357)
[2025-02-13 19:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10][root][INFO] - Training Epoch: 1/2, step 3832/7134 completed (loss: 0.13122998178005219, acc: 0.9821428656578064)
[2025-02-13 19:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10][root][INFO] - Training Epoch: 1/2, step 3833/7134 completed (loss: 0.08357851207256317, acc: 0.9726775884628296)
[2025-02-13 19:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10][root][INFO] - Training Epoch: 1/2, step 3834/7134 completed (loss: 0.039994776248931885, acc: 0.9878048896789551)
[2025-02-13 19:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11][root][INFO] - Training Epoch: 1/2, step 3835/7134 completed (loss: 0.08497753739356995, acc: 0.9739583134651184)
[2025-02-13 19:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11][root][INFO] - Training Epoch: 1/2, step 3836/7134 completed (loss: 0.22118045389652252, acc: 0.9629629850387573)
[2025-02-13 19:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11][root][INFO] - Training Epoch: 1/2, step 3837/7134 completed (loss: 0.15412281453609467, acc: 0.9808917045593262)
[2025-02-13 19:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12][root][INFO] - Training Epoch: 1/2, step 3838/7134 completed (loss: 0.11740662902593613, acc: 0.9606741666793823)
[2025-02-13 19:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12][root][INFO] - Training Epoch: 1/2, step 3839/7134 completed (loss: 0.09328114241361618, acc: 0.9689440727233887)
[2025-02-13 19:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13][root][INFO] - Training Epoch: 1/2, step 3840/7134 completed (loss: 0.11713897436857224, acc: 0.9696969985961914)
[2025-02-13 19:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13][root][INFO] - Training Epoch: 1/2, step 3841/7134 completed (loss: 0.1962466835975647, acc: 0.9497487545013428)
[2025-02-13 19:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13][root][INFO] - Training Epoch: 1/2, step 3842/7134 completed (loss: 0.025908714160323143, acc: 1.0)
[2025-02-13 19:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14][root][INFO] - Training Epoch: 1/2, step 3843/7134 completed (loss: 0.16204231977462769, acc: 0.9684684872627258)
[2025-02-13 19:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14][root][INFO] - Training Epoch: 1/2, step 3844/7134 completed (loss: 0.279158353805542, acc: 0.9214659929275513)
[2025-02-13 19:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14][root][INFO] - Training Epoch: 1/2, step 3845/7134 completed (loss: 0.21955889463424683, acc: 0.9714285731315613)
[2025-02-13 19:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15][root][INFO] - Training Epoch: 1/2, step 3846/7134 completed (loss: 0.08460260182619095, acc: 0.985401451587677)
[2025-02-13 19:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15][root][INFO] - Training Epoch: 1/2, step 3847/7134 completed (loss: 0.11701562255620956, acc: 0.9716312289237976)
[2025-02-13 19:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16][root][INFO] - Training Epoch: 1/2, step 3848/7134 completed (loss: 0.13191138207912445, acc: 0.9602272510528564)
[2025-02-13 19:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16][root][INFO] - Training Epoch: 1/2, step 3849/7134 completed (loss: 0.24707534909248352, acc: 0.9481481313705444)
[2025-02-13 19:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16][root][INFO] - Training Epoch: 1/2, step 3850/7134 completed (loss: 0.24137747287750244, acc: 0.9398906826972961)
[2025-02-13 19:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17][root][INFO] - Training Epoch: 1/2, step 3851/7134 completed (loss: 0.21245938539505005, acc: 0.9404761791229248)
[2025-02-13 19:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17][root][INFO] - Training Epoch: 1/2, step 3852/7134 completed (loss: 0.09688861668109894, acc: 0.9640287756919861)
[2025-02-13 19:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17][root][INFO] - Training Epoch: 1/2, step 3853/7134 completed (loss: 0.2672056257724762, acc: 0.9385474920272827)
[2025-02-13 19:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18][root][INFO] - Training Epoch: 1/2, step 3854/7134 completed (loss: 0.18837061524391174, acc: 0.9554139971733093)
[2025-02-13 19:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18][root][INFO] - Training Epoch: 1/2, step 3855/7134 completed (loss: 0.22315293550491333, acc: 0.946107804775238)
[2025-02-13 19:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18][root][INFO] - Training Epoch: 1/2, step 3856/7134 completed (loss: 0.17966614663600922, acc: 0.9490445852279663)
[2025-02-13 19:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19][root][INFO] - Training Epoch: 1/2, step 3857/7134 completed (loss: 0.20678414404392242, acc: 0.9594594836235046)
[2025-02-13 19:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19][root][INFO] - Training Epoch: 1/2, step 3858/7134 completed (loss: 0.3789708614349365, acc: 0.9125000238418579)
[2025-02-13 19:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20][root][INFO] - Training Epoch: 1/2, step 3859/7134 completed (loss: 0.25729623436927795, acc: 0.9491525292396545)
[2025-02-13 19:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20][root][INFO] - Training Epoch: 1/2, step 3860/7134 completed (loss: 0.09867934882640839, acc: 0.9685863852500916)
[2025-02-13 19:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20][root][INFO] - Training Epoch: 1/2, step 3861/7134 completed (loss: 0.2257375717163086, acc: 0.9281045794487)
[2025-02-13 19:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21][root][INFO] - Training Epoch: 1/2, step 3862/7134 completed (loss: 0.1607225090265274, acc: 0.9692307710647583)
[2025-02-13 19:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21][root][INFO] - Training Epoch: 1/2, step 3863/7134 completed (loss: 0.08176583796739578, acc: 0.9767441749572754)
[2025-02-13 19:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22][root][INFO] - Training Epoch: 1/2, step 3864/7134 completed (loss: 0.1434972882270813, acc: 0.9545454382896423)
[2025-02-13 19:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22][root][INFO] - Training Epoch: 1/2, step 3865/7134 completed (loss: 0.24413971602916718, acc: 0.9375)
[2025-02-13 19:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22][root][INFO] - Training Epoch: 1/2, step 3866/7134 completed (loss: 0.2737888991832733, acc: 0.9300000071525574)
[2025-02-13 19:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23][root][INFO] - Training Epoch: 1/2, step 3867/7134 completed (loss: 0.1604158580303192, acc: 0.9545454382896423)
[2025-02-13 19:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23][root][INFO] - Training Epoch: 1/2, step 3868/7134 completed (loss: 0.08397354930639267, acc: 0.9852941036224365)
[2025-02-13 19:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23][root][INFO] - Training Epoch: 1/2, step 3869/7134 completed (loss: 0.17195263504981995, acc: 0.9370629191398621)
[2025-02-13 19:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:24][root][INFO] - Training Epoch: 1/2, step 3870/7134 completed (loss: 0.300677090883255, acc: 0.9477124214172363)
[2025-02-13 19:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:24][root][INFO] - Training Epoch: 1/2, step 3871/7134 completed (loss: 0.18509608507156372, acc: 0.959770143032074)
[2025-02-13 19:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25][root][INFO] - Training Epoch: 1/2, step 3872/7134 completed (loss: 0.168413445353508, acc: 0.9696969985961914)
[2025-02-13 19:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25][root][INFO] - Training Epoch: 1/2, step 3873/7134 completed (loss: 0.13926807045936584, acc: 0.9515151381492615)
[2025-02-13 19:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25][root][INFO] - Training Epoch: 1/2, step 3874/7134 completed (loss: 0.14734859764575958, acc: 0.9590643048286438)
[2025-02-13 19:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26][root][INFO] - Training Epoch: 1/2, step 3875/7134 completed (loss: 0.24317118525505066, acc: 0.9240506291389465)
[2025-02-13 19:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26][root][INFO] - Training Epoch: 1/2, step 3876/7134 completed (loss: 0.08998633176088333, acc: 0.984000027179718)
[2025-02-13 19:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27][root][INFO] - Training Epoch: 1/2, step 3877/7134 completed (loss: 0.19321873784065247, acc: 0.9593023061752319)
[2025-02-13 19:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27][root][INFO] - Training Epoch: 1/2, step 3878/7134 completed (loss: 0.5622398257255554, acc: 0.8837209343910217)
[2025-02-13 19:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27][root][INFO] - Training Epoch: 1/2, step 3879/7134 completed (loss: 0.2009725421667099, acc: 0.9402984976768494)
[2025-02-13 19:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28][root][INFO] - Training Epoch: 1/2, step 3880/7134 completed (loss: 0.20702838897705078, acc: 0.9487179517745972)
[2025-02-13 19:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28][root][INFO] - Training Epoch: 1/2, step 3881/7134 completed (loss: 0.39741241931915283, acc: 0.9197860956192017)
[2025-02-13 19:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29][root][INFO] - Training Epoch: 1/2, step 3882/7134 completed (loss: 0.34963878989219666, acc: 0.9175257682800293)
[2025-02-13 19:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29][root][INFO] - Training Epoch: 1/2, step 3883/7134 completed (loss: 0.2566129267215729, acc: 0.9542483687400818)
[2025-02-13 19:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29][root][INFO] - Training Epoch: 1/2, step 3884/7134 completed (loss: 0.17050670087337494, acc: 0.954285740852356)
[2025-02-13 19:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30][root][INFO] - Training Epoch: 1/2, step 3885/7134 completed (loss: 0.07520243525505066, acc: 0.987261176109314)
[2025-02-13 19:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30][root][INFO] - Training Epoch: 1/2, step 3886/7134 completed (loss: 0.2235092669725418, acc: 0.9523809552192688)
[2025-02-13 19:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31][root][INFO] - Training Epoch: 1/2, step 3887/7134 completed (loss: 0.20173873007297516, acc: 0.9622641801834106)
[2025-02-13 19:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31][root][INFO] - Training Epoch: 1/2, step 3888/7134 completed (loss: 0.1831689476966858, acc: 0.9418604373931885)
[2025-02-13 19:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31][root][INFO] - Training Epoch: 1/2, step 3889/7134 completed (loss: 0.08275254815816879, acc: 0.9774011373519897)
[2025-02-13 19:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32][root][INFO] - Training Epoch: 1/2, step 3890/7134 completed (loss: 0.10662752389907837, acc: 0.9793814420700073)
[2025-02-13 19:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32][root][INFO] - Training Epoch: 1/2, step 3891/7134 completed (loss: 0.0950569286942482, acc: 0.9826589822769165)
[2025-02-13 19:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32][root][INFO] - Training Epoch: 1/2, step 3892/7134 completed (loss: 0.2289622277021408, acc: 0.9466666579246521)
[2025-02-13 19:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33][root][INFO] - Training Epoch: 1/2, step 3893/7134 completed (loss: 0.27943161129951477, acc: 0.9484536051750183)
[2025-02-13 19:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33][root][INFO] - Training Epoch: 1/2, step 3894/7134 completed (loss: 0.12209191173315048, acc: 0.9746835231781006)
[2025-02-13 19:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33][root][INFO] - Training Epoch: 1/2, step 3895/7134 completed (loss: 0.2391243577003479, acc: 0.9447513818740845)
[2025-02-13 19:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34][root][INFO] - Training Epoch: 1/2, step 3896/7134 completed (loss: 0.14350254833698273, acc: 0.9668246507644653)
[2025-02-13 19:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34][root][INFO] - Training Epoch: 1/2, step 3897/7134 completed (loss: 0.3499121367931366, acc: 0.9534883499145508)
[2025-02-13 19:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35][root][INFO] - Training Epoch: 1/2, step 3898/7134 completed (loss: 0.20793041586875916, acc: 0.9655172228813171)
[2025-02-13 19:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35][root][INFO] - Training Epoch: 1/2, step 3899/7134 completed (loss: 0.17524589598178864, acc: 0.9537572264671326)
[2025-02-13 19:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35][root][INFO] - Training Epoch: 1/2, step 3900/7134 completed (loss: 0.15919041633605957, acc: 0.9587628841400146)
[2025-02-13 19:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36][root][INFO] - Training Epoch: 1/2, step 3901/7134 completed (loss: 0.37299853563308716, acc: 0.9337349534034729)
[2025-02-13 19:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36][root][INFO] - Training Epoch: 1/2, step 3902/7134 completed (loss: 0.3495676517486572, acc: 0.8958333134651184)
[2025-02-13 19:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37][root][INFO] - Training Epoch: 1/2, step 3903/7134 completed (loss: 0.526369571685791, acc: 0.8601036071777344)
[2025-02-13 19:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37][root][INFO] - Training Epoch: 1/2, step 3904/7134 completed (loss: 0.3242582082748413, acc: 0.9239766001701355)
[2025-02-13 19:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37][root][INFO] - Training Epoch: 1/2, step 3905/7134 completed (loss: 0.11205577105283737, acc: 0.9777777791023254)
[2025-02-13 19:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38][root][INFO] - Training Epoch: 1/2, step 3906/7134 completed (loss: 0.17004285752773285, acc: 0.9438202381134033)
[2025-02-13 19:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38][root][INFO] - Training Epoch: 1/2, step 3907/7134 completed (loss: 0.2833270728588104, acc: 0.9226804375648499)
[2025-02-13 19:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38][root][INFO] - Training Epoch: 1/2, step 3908/7134 completed (loss: 0.3362930715084076, acc: 0.9364162087440491)
[2025-02-13 19:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39][root][INFO] - Training Epoch: 1/2, step 3909/7134 completed (loss: 0.1161658838391304, acc: 0.9685534834861755)
[2025-02-13 19:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39][root][INFO] - Training Epoch: 1/2, step 3910/7134 completed (loss: 0.11213898658752441, acc: 0.9375)
[2025-02-13 19:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40][root][INFO] - Training Epoch: 1/2, step 3911/7134 completed (loss: 0.13708826899528503, acc: 0.9496855139732361)
[2025-02-13 19:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40][root][INFO] - Training Epoch: 1/2, step 3912/7134 completed (loss: 0.0451301671564579, acc: 0.9882352948188782)
[2025-02-13 19:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40][root][INFO] - Training Epoch: 1/2, step 3913/7134 completed (loss: 0.13189160823822021, acc: 0.9646464586257935)
[2025-02-13 19:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41][root][INFO] - Training Epoch: 1/2, step 3914/7134 completed (loss: 0.13180656731128693, acc: 0.955974817276001)
[2025-02-13 19:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41][root][INFO] - Training Epoch: 1/2, step 3915/7134 completed (loss: 0.060968972742557526, acc: 0.9942528605461121)
[2025-02-13 19:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42][root][INFO] - Training Epoch: 1/2, step 3916/7134 completed (loss: 0.12103316187858582, acc: 0.9693877696990967)
[2025-02-13 19:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42][root][INFO] - Training Epoch: 1/2, step 3917/7134 completed (loss: 0.08331339061260223, acc: 0.9767441749572754)
[2025-02-13 19:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42][root][INFO] - Training Epoch: 1/2, step 3918/7134 completed (loss: 0.12843917310237885, acc: 0.9714285731315613)
[2025-02-13 19:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43][root][INFO] - Training Epoch: 1/2, step 3919/7134 completed (loss: 0.3718271553516388, acc: 0.9230769276618958)
[2025-02-13 19:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43][root][INFO] - Training Epoch: 1/2, step 3920/7134 completed (loss: 0.3648649752140045, acc: 0.916167676448822)
[2025-02-13 19:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44][root][INFO] - Training Epoch: 1/2, step 3921/7134 completed (loss: 0.34098121523857117, acc: 0.908450722694397)
[2025-02-13 19:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44][root][INFO] - Training Epoch: 1/2, step 3922/7134 completed (loss: 0.4869677424430847, acc: 0.8961039185523987)
[2025-02-13 19:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44][root][INFO] - Training Epoch: 1/2, step 3923/7134 completed (loss: 0.4675055146217346, acc: 0.8928571343421936)
[2025-02-13 19:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:45][root][INFO] - Training Epoch: 1/2, step 3924/7134 completed (loss: 0.22569239139556885, acc: 0.9536082744598389)
[2025-02-13 19:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:45][root][INFO] - Training Epoch: 1/2, step 3925/7134 completed (loss: 0.1940147876739502, acc: 0.950276255607605)
[2025-02-13 19:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46][root][INFO] - Training Epoch: 1/2, step 3926/7134 completed (loss: 0.21862399578094482, acc: 0.949367105960846)
[2025-02-13 19:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46][root][INFO] - Training Epoch: 1/2, step 3927/7134 completed (loss: 0.1941521316766739, acc: 0.9515151381492615)
[2025-02-13 19:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46][root][INFO] - Training Epoch: 1/2, step 3928/7134 completed (loss: 0.4196762144565582, acc: 0.9341317415237427)
[2025-02-13 19:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47][root][INFO] - Training Epoch: 1/2, step 3929/7134 completed (loss: 0.16282016038894653, acc: 0.9675324559211731)
[2025-02-13 19:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47][root][INFO] - Training Epoch: 1/2, step 3930/7134 completed (loss: 0.3351879119873047, acc: 0.9491525292396545)
[2025-02-13 19:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47][root][INFO] - Training Epoch: 1/2, step 3931/7134 completed (loss: 0.14645826816558838, acc: 0.9556962251663208)
[2025-02-13 19:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:48][root][INFO] - Training Epoch: 1/2, step 3932/7134 completed (loss: 0.15961910784244537, acc: 0.9545454382896423)
[2025-02-13 19:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:48][root][INFO] - Training Epoch: 1/2, step 3933/7134 completed (loss: 0.047330092638731, acc: 0.9857142567634583)
[2025-02-13 19:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49][root][INFO] - Training Epoch: 1/2, step 3934/7134 completed (loss: 0.13027188181877136, acc: 0.9629629850387573)
[2025-02-13 19:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49][root][INFO] - Training Epoch: 1/2, step 3935/7134 completed (loss: 0.13865125179290771, acc: 0.9589040875434875)
[2025-02-13 19:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49][root][INFO] - Training Epoch: 1/2, step 3936/7134 completed (loss: 0.14645856618881226, acc: 0.9677419066429138)
[2025-02-13 19:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50][root][INFO] - Training Epoch: 1/2, step 3937/7134 completed (loss: 0.3185456693172455, acc: 0.9322034120559692)
[2025-02-13 19:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50][root][INFO] - Training Epoch: 1/2, step 3938/7134 completed (loss: 0.271138995885849, acc: 0.9369369149208069)
[2025-02-13 19:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50][root][INFO] - Training Epoch: 1/2, step 3939/7134 completed (loss: 0.2388153374195099, acc: 0.9407407641410828)
[2025-02-13 19:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51][root][INFO] - Training Epoch: 1/2, step 3940/7134 completed (loss: 0.2075941115617752, acc: 0.945652186870575)
[2025-02-13 19:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51][root][INFO] - Training Epoch: 1/2, step 3941/7134 completed (loss: 0.20853270590305328, acc: 0.9390243887901306)
[2025-02-13 19:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51][root][INFO] - Training Epoch: 1/2, step 3942/7134 completed (loss: 0.17548659443855286, acc: 0.9599999785423279)
[2025-02-13 19:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:52][root][INFO] - Training Epoch: 1/2, step 3943/7134 completed (loss: 0.21307288110256195, acc: 0.9295774698257446)
[2025-02-13 19:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:52][root][INFO] - Training Epoch: 1/2, step 3944/7134 completed (loss: 0.16813285648822784, acc: 0.9701492786407471)
[2025-02-13 19:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53][root][INFO] - Training Epoch: 1/2, step 3945/7134 completed (loss: 0.05873531475663185, acc: 0.9934210777282715)
[2025-02-13 19:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53][root][INFO] - Training Epoch: 1/2, step 3946/7134 completed (loss: 0.11945968121290207, acc: 0.9642857313156128)
[2025-02-13 19:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53][root][INFO] - Training Epoch: 1/2, step 3947/7134 completed (loss: 0.049651265144348145, acc: 0.9890109896659851)
[2025-02-13 19:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54][root][INFO] - Training Epoch: 1/2, step 3948/7134 completed (loss: 0.03589554503560066, acc: 0.9900000095367432)
[2025-02-13 19:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54][root][INFO] - Training Epoch: 1/2, step 3949/7134 completed (loss: 0.13906307518482208, acc: 0.9609375)
[2025-02-13 19:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54][root][INFO] - Training Epoch: 1/2, step 3950/7134 completed (loss: 0.1787905991077423, acc: 0.9736841917037964)
[2025-02-13 19:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55][root][INFO] - Training Epoch: 1/2, step 3951/7134 completed (loss: 0.058567967265844345, acc: 0.9793814420700073)
[2025-02-13 19:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55][root][INFO] - Training Epoch: 1/2, step 3952/7134 completed (loss: 0.18180371820926666, acc: 0.9693877696990967)
[2025-02-13 19:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55][root][INFO] - Training Epoch: 1/2, step 3953/7134 completed (loss: 0.08000989258289337, acc: 0.9855072498321533)
[2025-02-13 19:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56][root][INFO] - Training Epoch: 1/2, step 3954/7134 completed (loss: 0.04694893956184387, acc: 1.0)
[2025-02-13 19:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56][root][INFO] - Training Epoch: 1/2, step 3955/7134 completed (loss: 0.13008932769298553, acc: 0.9685039520263672)
[2025-02-13 19:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56][root][INFO] - Training Epoch: 1/2, step 3956/7134 completed (loss: 0.19007574021816254, acc: 0.9607843160629272)
[2025-02-13 19:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57][root][INFO] - Training Epoch: 1/2, step 3957/7134 completed (loss: 0.1243157684803009, acc: 0.9572649598121643)
[2025-02-13 19:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57][root][INFO] - Training Epoch: 1/2, step 3958/7134 completed (loss: 0.15517084300518036, acc: 0.9404761791229248)
[2025-02-13 19:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57][root][INFO] - Training Epoch: 1/2, step 3959/7134 completed (loss: 0.15863390266895294, acc: 0.9741379022598267)
[2025-02-13 19:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58][root][INFO] - Training Epoch: 1/2, step 3960/7134 completed (loss: 0.22865350544452667, acc: 0.9553072452545166)
[2025-02-13 19:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58][root][INFO] - Training Epoch: 1/2, step 3961/7134 completed (loss: 0.13618648052215576, acc: 0.9753086566925049)
[2025-02-13 19:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59][root][INFO] - Training Epoch: 1/2, step 3962/7134 completed (loss: 0.13446033000946045, acc: 0.9731543660163879)
[2025-02-13 19:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59][root][INFO] - Training Epoch: 1/2, step 3963/7134 completed (loss: 0.3819664716720581, acc: 0.929347813129425)
[2025-02-13 19:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59][root][INFO] - Training Epoch: 1/2, step 3964/7134 completed (loss: 0.3323685824871063, acc: 0.9307692050933838)
[2025-02-13 19:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00][root][INFO] - Training Epoch: 1/2, step 3965/7134 completed (loss: 0.16666941344738007, acc: 0.9636363387107849)
[2025-02-13 19:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00][root][INFO] - Training Epoch: 1/2, step 3966/7134 completed (loss: 0.13495668768882751, acc: 0.984375)
[2025-02-13 19:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00][root][INFO] - Training Epoch: 1/2, step 3967/7134 completed (loss: 0.4289744198322296, acc: 0.9347826242446899)
[2025-02-13 19:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01][root][INFO] - Training Epoch: 1/2, step 3968/7134 completed (loss: 0.10670330375432968, acc: 0.9776536226272583)
[2025-02-13 19:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01][root][INFO] - Training Epoch: 1/2, step 3969/7134 completed (loss: 0.42099106311798096, acc: 0.9150943160057068)
[2025-02-13 19:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01][root][INFO] - Training Epoch: 1/2, step 3970/7134 completed (loss: 0.2819731533527374, acc: 0.9421965479850769)
[2025-02-13 19:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02][root][INFO] - Training Epoch: 1/2, step 3971/7134 completed (loss: 0.5451908111572266, acc: 0.8820512890815735)
[2025-02-13 19:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02][root][INFO] - Training Epoch: 1/2, step 3972/7134 completed (loss: 0.8462377190589905, acc: 0.817307710647583)
[2025-02-13 19:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03][root][INFO] - Training Epoch: 1/2, step 3973/7134 completed (loss: 0.32478320598602295, acc: 0.9295774698257446)
[2025-02-13 19:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03][root][INFO] - Training Epoch: 1/2, step 3974/7134 completed (loss: 0.398318886756897, acc: 0.8820512890815735)
[2025-02-13 19:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03][root][INFO] - Training Epoch: 1/2, step 3975/7134 completed (loss: 0.28534114360809326, acc: 0.9200000166893005)
[2025-02-13 19:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04][root][INFO] - Training Epoch: 1/2, step 3976/7134 completed (loss: 0.45809656381607056, acc: 0.891566276550293)
[2025-02-13 19:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04][root][INFO] - Training Epoch: 1/2, step 3977/7134 completed (loss: 0.17496603727340698, acc: 0.95652174949646)
[2025-02-13 19:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04][root][INFO] - Training Epoch: 1/2, step 3978/7134 completed (loss: 0.12648479640483856, acc: 0.9751243591308594)
[2025-02-13 19:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05][root][INFO] - Training Epoch: 1/2, step 3979/7134 completed (loss: 0.2192043662071228, acc: 0.9635036587715149)
[2025-02-13 19:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05][root][INFO] - Training Epoch: 1/2, step 3980/7134 completed (loss: 0.11786230653524399, acc: 0.9612902998924255)
[2025-02-13 19:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06][root][INFO] - Training Epoch: 1/2, step 3981/7134 completed (loss: 0.23074112832546234, acc: 0.9578313231468201)
[2025-02-13 19:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06][root][INFO] - Training Epoch: 1/2, step 3982/7134 completed (loss: 0.40947040915489197, acc: 0.9186046719551086)
[2025-02-13 19:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06][root][INFO] - Training Epoch: 1/2, step 3983/7134 completed (loss: 1.0068509578704834, acc: 0.7751938104629517)
[2025-02-13 19:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07][root][INFO] - Training Epoch: 1/2, step 3984/7134 completed (loss: 0.4612140357494354, acc: 0.9042553305625916)
[2025-02-13 19:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07][root][INFO] - Training Epoch: 1/2, step 3985/7134 completed (loss: 0.12393784523010254, acc: 0.9692307710647583)
[2025-02-13 19:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07][root][INFO] - Training Epoch: 1/2, step 3986/7134 completed (loss: 0.6221169829368591, acc: 0.8693181872367859)
[2025-02-13 19:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08][root][INFO] - Training Epoch: 1/2, step 3987/7134 completed (loss: 0.3059017062187195, acc: 0.89682537317276)
[2025-02-13 19:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08][root][INFO] - Training Epoch: 1/2, step 3988/7134 completed (loss: 0.3302868902683258, acc: 0.9138755798339844)
[2025-02-13 19:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08][root][INFO] - Training Epoch: 1/2, step 3989/7134 completed (loss: 0.3610905110836029, acc: 0.9090909361839294)
[2025-02-13 19:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09][root][INFO] - Training Epoch: 1/2, step 3990/7134 completed (loss: 0.38848719000816345, acc: 0.9139072895050049)
[2025-02-13 19:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09][root][INFO] - Training Epoch: 1/2, step 3991/7134 completed (loss: 0.20040203630924225, acc: 0.9466666579246521)
[2025-02-13 19:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09][root][INFO] - Training Epoch: 1/2, step 3992/7134 completed (loss: 0.2884628176689148, acc: 0.9230769276618958)
[2025-02-13 19:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10][root][INFO] - Training Epoch: 1/2, step 3993/7134 completed (loss: 0.4236276149749756, acc: 0.8796296119689941)
[2025-02-13 19:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10][root][INFO] - Training Epoch: 1/2, step 3994/7134 completed (loss: 0.285704642534256, acc: 0.9117646813392639)
[2025-02-13 19:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11][root][INFO] - Training Epoch: 1/2, step 3995/7134 completed (loss: 0.5086140632629395, acc: 0.8682170510292053)
[2025-02-13 19:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11][root][INFO] - Training Epoch: 1/2, step 3996/7134 completed (loss: 0.24344518780708313, acc: 0.9173553586006165)
[2025-02-13 19:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11][root][INFO] - Training Epoch: 1/2, step 3997/7134 completed (loss: 0.43534600734710693, acc: 0.8793103694915771)
[2025-02-13 19:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12][root][INFO] - Training Epoch: 1/2, step 3998/7134 completed (loss: 0.319602906703949, acc: 0.9230769276618958)
[2025-02-13 19:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12][root][INFO] - Training Epoch: 1/2, step 3999/7134 completed (loss: 0.23028196394443512, acc: 0.9354838728904724)
[2025-02-13 19:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12][root][INFO] - Training Epoch: 1/2, step 4000/7134 completed (loss: 0.2534814774990082, acc: 0.9268292784690857)
[2025-02-13 19:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13][root][INFO] - Training Epoch: 1/2, step 4001/7134 completed (loss: 0.5092746615409851, acc: 0.8709677457809448)
[2025-02-13 19:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13][root][INFO] - Training Epoch: 1/2, step 4002/7134 completed (loss: 0.47768434882164, acc: 0.8785046935081482)
[2025-02-13 19:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13][root][INFO] - Training Epoch: 1/2, step 4003/7134 completed (loss: 0.47910329699516296, acc: 0.8947368264198303)
[2025-02-13 19:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14][root][INFO] - Training Epoch: 1/2, step 4004/7134 completed (loss: 0.2901298403739929, acc: 0.9300000071525574)
[2025-02-13 19:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14][root][INFO] - Training Epoch: 1/2, step 4005/7134 completed (loss: 0.13960367441177368, acc: 0.9542483687400818)
[2025-02-13 19:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14][root][INFO] - Training Epoch: 1/2, step 4006/7134 completed (loss: 0.10643433034420013, acc: 0.9639639854431152)
[2025-02-13 19:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15][root][INFO] - Training Epoch: 1/2, step 4007/7134 completed (loss: 0.2228803187608719, acc: 0.9642857313156128)
[2025-02-13 19:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15][root][INFO] - Training Epoch: 1/2, step 4008/7134 completed (loss: 0.10965116322040558, acc: 0.9730941653251648)
[2025-02-13 19:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16][root][INFO] - Training Epoch: 1/2, step 4009/7134 completed (loss: 0.07405072450637817, acc: 0.9888268113136292)
[2025-02-13 19:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16][root][INFO] - Training Epoch: 1/2, step 4010/7134 completed (loss: 0.09223951399326324, acc: 0.9760765433311462)
[2025-02-13 19:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16][root][INFO] - Training Epoch: 1/2, step 4011/7134 completed (loss: 0.1386609524488449, acc: 0.9485714435577393)
[2025-02-13 19:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17][root][INFO] - Training Epoch: 1/2, step 4012/7134 completed (loss: 0.1729394942522049, acc: 0.9457831382751465)
[2025-02-13 19:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17][root][INFO] - Training Epoch: 1/2, step 4013/7134 completed (loss: 0.0593191497027874, acc: 0.9790209531784058)
[2025-02-13 19:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17][root][INFO] - Training Epoch: 1/2, step 4014/7134 completed (loss: 0.08853381127119064, acc: 0.9800000190734863)
[2025-02-13 19:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18][root][INFO] - Training Epoch: 1/2, step 4015/7134 completed (loss: 0.03936193883419037, acc: 0.9862068891525269)
[2025-02-13 19:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18][root][INFO] - Training Epoch: 1/2, step 4016/7134 completed (loss: 0.24932599067687988, acc: 0.9326424598693848)
[2025-02-13 19:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18][root][INFO] - Training Epoch: 1/2, step 4017/7134 completed (loss: 0.09602595865726471, acc: 0.9639175534248352)
[2025-02-13 19:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19][root][INFO] - Training Epoch: 1/2, step 4018/7134 completed (loss: 0.16840772330760956, acc: 0.9516907930374146)
[2025-02-13 19:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19][root][INFO] - Training Epoch: 1/2, step 4019/7134 completed (loss: 0.11627350002527237, acc: 0.9722222089767456)
[2025-02-13 19:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20][root][INFO] - Training Epoch: 1/2, step 4020/7134 completed (loss: 0.18038113415241241, acc: 0.9431818127632141)
[2025-02-13 19:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20][root][INFO] - Training Epoch: 1/2, step 4021/7134 completed (loss: 0.11091657727956772, acc: 0.970588207244873)
[2025-02-13 19:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20][root][INFO] - Training Epoch: 1/2, step 4022/7134 completed (loss: 0.13403022289276123, acc: 0.9567307829856873)
[2025-02-13 19:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21][root][INFO] - Training Epoch: 1/2, step 4023/7134 completed (loss: 0.18403445184230804, acc: 0.954285740852356)
[2025-02-13 19:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21][root][INFO] - Training Epoch: 1/2, step 4024/7134 completed (loss: 0.12099763751029968, acc: 0.959770143032074)
[2025-02-13 19:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21][root][INFO] - Training Epoch: 1/2, step 4025/7134 completed (loss: 0.3193017542362213, acc: 0.9005235433578491)
[2025-02-13 19:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22][root][INFO] - Training Epoch: 1/2, step 4026/7134 completed (loss: 0.3585261106491089, acc: 0.9068322777748108)
[2025-02-13 19:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22][root][INFO] - Training Epoch: 1/2, step 4027/7134 completed (loss: 0.24459154903888702, acc: 0.932584285736084)
[2025-02-13 19:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22][root][INFO] - Training Epoch: 1/2, step 4028/7134 completed (loss: 0.45161372423171997, acc: 0.8818897604942322)
[2025-02-13 19:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23][root][INFO] - Training Epoch: 1/2, step 4029/7134 completed (loss: 0.09949763864278793, acc: 0.9783783555030823)
[2025-02-13 19:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23][root][INFO] - Training Epoch: 1/2, step 4030/7134 completed (loss: 0.10632751882076263, acc: 0.9615384340286255)
[2025-02-13 19:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23][root][INFO] - Training Epoch: 1/2, step 4031/7134 completed (loss: 0.05460115894675255, acc: 0.9937888383865356)
[2025-02-13 19:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24][root][INFO] - Training Epoch: 1/2, step 4032/7134 completed (loss: 0.23639315366744995, acc: 0.9593023061752319)
[2025-02-13 19:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24][root][INFO] - Training Epoch: 1/2, step 4033/7134 completed (loss: 0.0724615529179573, acc: 0.9786096215248108)
[2025-02-13 19:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25][root][INFO] - Training Epoch: 1/2, step 4034/7134 completed (loss: 0.11170589923858643, acc: 0.9795918464660645)
[2025-02-13 19:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25][root][INFO] - Training Epoch: 1/2, step 4035/7134 completed (loss: 0.33695581555366516, acc: 0.9365853667259216)
[2025-02-13 19:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25][root][INFO] - Training Epoch: 1/2, step 4036/7134 completed (loss: 0.1517619639635086, acc: 0.9538461565971375)
[2025-02-13 19:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26][root][INFO] - Training Epoch: 1/2, step 4037/7134 completed (loss: 0.13785314559936523, acc: 0.9583333134651184)
[2025-02-13 19:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26][root][INFO] - Training Epoch: 1/2, step 4038/7134 completed (loss: 0.20110082626342773, acc: 0.9431818127632141)
[2025-02-13 19:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26][root][INFO] - Training Epoch: 1/2, step 4039/7134 completed (loss: 0.25370466709136963, acc: 0.9349112510681152)
[2025-02-13 19:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27][root][INFO] - Training Epoch: 1/2, step 4040/7134 completed (loss: 0.13563521206378937, acc: 0.9617486596107483)
[2025-02-13 19:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27][root][INFO] - Training Epoch: 1/2, step 4041/7134 completed (loss: 0.20077794790267944, acc: 0.9653179049491882)
[2025-02-13 19:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27][root][INFO] - Training Epoch: 1/2, step 4042/7134 completed (loss: 0.1863415390253067, acc: 0.9594594836235046)
[2025-02-13 19:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:28][root][INFO] - Training Epoch: 1/2, step 4043/7134 completed (loss: 0.2853844463825226, acc: 0.9367088675498962)
[2025-02-13 19:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:28][root][INFO] - Training Epoch: 1/2, step 4044/7134 completed (loss: 0.20281019806861877, acc: 0.9670329689979553)
[2025-02-13 19:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29][root][INFO] - Training Epoch: 1/2, step 4045/7134 completed (loss: 0.21546314656734467, acc: 0.9428571462631226)
[2025-02-13 19:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29][root][INFO] - Training Epoch: 1/2, step 4046/7134 completed (loss: 0.2148895263671875, acc: 0.9415204524993896)
[2025-02-13 19:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29][root][INFO] - Training Epoch: 1/2, step 4047/7134 completed (loss: 0.2022319883108139, acc: 0.9562841653823853)
[2025-02-13 19:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30][root][INFO] - Training Epoch: 1/2, step 4048/7134 completed (loss: 0.08108973503112793, acc: 0.9746192693710327)
[2025-02-13 19:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30][root][INFO] - Training Epoch: 1/2, step 4049/7134 completed (loss: 0.2712169587612152, acc: 0.9353233575820923)
[2025-02-13 19:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30][root][INFO] - Training Epoch: 1/2, step 4050/7134 completed (loss: 0.26273787021636963, acc: 0.9375)
[2025-02-13 19:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31][root][INFO] - Training Epoch: 1/2, step 4051/7134 completed (loss: 0.29940420389175415, acc: 0.9385474920272827)
[2025-02-13 19:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31][root][INFO] - Training Epoch: 1/2, step 4052/7134 completed (loss: 0.15486104786396027, acc: 0.9661017060279846)
[2025-02-13 19:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31][root][INFO] - Training Epoch: 1/2, step 4053/7134 completed (loss: 0.19344516098499298, acc: 0.9523809552192688)
[2025-02-13 19:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32][root][INFO] - Training Epoch: 1/2, step 4054/7134 completed (loss: 0.4337767958641052, acc: 0.9130434989929199)
[2025-02-13 19:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32][root][INFO] - Training Epoch: 1/2, step 4055/7134 completed (loss: 0.16264155507087708, acc: 0.9734042286872864)
[2025-02-13 19:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33][root][INFO] - Training Epoch: 1/2, step 4056/7134 completed (loss: 0.15710337460041046, acc: 0.9789473414421082)
[2025-02-13 19:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33][root][INFO] - Training Epoch: 1/2, step 4057/7134 completed (loss: 0.2290680855512619, acc: 0.9470587968826294)
[2025-02-13 19:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33][root][INFO] - Training Epoch: 1/2, step 4058/7134 completed (loss: 0.2908632159233093, acc: 0.9318181872367859)
[2025-02-13 19:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34][root][INFO] - Training Epoch: 1/2, step 4059/7134 completed (loss: 0.15650054812431335, acc: 0.9701492786407471)
[2025-02-13 19:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34][root][INFO] - Training Epoch: 1/2, step 4060/7134 completed (loss: 0.07983291149139404, acc: 0.9729729890823364)
[2025-02-13 19:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34][root][INFO] - Training Epoch: 1/2, step 4061/7134 completed (loss: 0.04210394620895386, acc: 0.9825581312179565)
[2025-02-13 19:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35][root][INFO] - Training Epoch: 1/2, step 4062/7134 completed (loss: 0.05585465952754021, acc: 0.9802631735801697)
[2025-02-13 19:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35][root][INFO] - Training Epoch: 1/2, step 4063/7134 completed (loss: 0.0623060017824173, acc: 1.0)
[2025-02-13 19:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35][root][INFO] - Training Epoch: 1/2, step 4064/7134 completed (loss: 0.13881655037403107, acc: 0.9869281053543091)
[2025-02-13 19:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36][root][INFO] - Training Epoch: 1/2, step 4065/7134 completed (loss: 0.07302476465702057, acc: 0.9806451797485352)
[2025-02-13 19:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36][root][INFO] - Training Epoch: 1/2, step 4066/7134 completed (loss: 0.033657629042863846, acc: 1.0)
[2025-02-13 19:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37][root][INFO] - Training Epoch: 1/2, step 4067/7134 completed (loss: 0.18888744711875916, acc: 0.9452054500579834)
[2025-02-13 19:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37][root][INFO] - Training Epoch: 1/2, step 4068/7134 completed (loss: 0.352957546710968, acc: 0.9173553586006165)
[2025-02-13 19:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37][root][INFO] - Training Epoch: 1/2, step 4069/7134 completed (loss: 0.10573594272136688, acc: 0.9718309640884399)
[2025-02-13 19:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38][root][INFO] - Training Epoch: 1/2, step 4070/7134 completed (loss: 0.1908724308013916, acc: 0.9536423683166504)
[2025-02-13 19:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38][root][INFO] - Training Epoch: 1/2, step 4071/7134 completed (loss: 0.10452103614807129, acc: 0.965753436088562)
[2025-02-13 19:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38][root][INFO] - Training Epoch: 1/2, step 4072/7134 completed (loss: 0.0978325754404068, acc: 0.9756097793579102)
[2025-02-13 19:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39][root][INFO] - Training Epoch: 1/2, step 4073/7134 completed (loss: 0.042954474687576294, acc: 0.9939758777618408)
[2025-02-13 19:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39][root][INFO] - Training Epoch: 1/2, step 4074/7134 completed (loss: 0.17168882489204407, acc: 0.9735099077224731)
[2025-02-13 19:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39][root][INFO] - Training Epoch: 1/2, step 4075/7134 completed (loss: 0.024124516174197197, acc: 1.0)
[2025-02-13 19:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40][root][INFO] - Training Epoch: 1/2, step 4076/7134 completed (loss: 0.04968727007508278, acc: 1.0)
[2025-02-13 19:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40][root][INFO] - Training Epoch: 1/2, step 4077/7134 completed (loss: 0.15382665395736694, acc: 0.9727891087532043)
[2025-02-13 19:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41][root][INFO] - Training Epoch: 1/2, step 4078/7134 completed (loss: 0.10355835407972336, acc: 0.9685534834861755)
[2025-02-13 19:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41][root][INFO] - Training Epoch: 1/2, step 4079/7134 completed (loss: 0.07880277186632156, acc: 0.9923664331436157)
[2025-02-13 19:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41][root][INFO] - Training Epoch: 1/2, step 4080/7134 completed (loss: 0.06374258548021317, acc: 0.9793103337287903)
[2025-02-13 19:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42][root][INFO] - Training Epoch: 1/2, step 4081/7134 completed (loss: 0.026165666058659554, acc: 0.9930555820465088)
[2025-02-13 19:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42][root][INFO] - Training Epoch: 1/2, step 4082/7134 completed (loss: 0.3823259472846985, acc: 0.9210526347160339)
[2025-02-13 19:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42][root][INFO] - Training Epoch: 1/2, step 4083/7134 completed (loss: 0.4601515829563141, acc: 0.9005848169326782)
[2025-02-13 19:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43][root][INFO] - Training Epoch: 1/2, step 4084/7134 completed (loss: 0.4993821084499359, acc: 0.8724831938743591)
[2025-02-13 19:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43][root][INFO] - Training Epoch: 1/2, step 4085/7134 completed (loss: 0.145169198513031, acc: 0.9473684430122375)
[2025-02-13 19:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43][root][INFO] - Training Epoch: 1/2, step 4086/7134 completed (loss: 0.33307838439941406, acc: 0.9102563858032227)
[2025-02-13 19:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44][root][INFO] - Training Epoch: 1/2, step 4087/7134 completed (loss: 0.12183243781328201, acc: 0.9892473220825195)
[2025-02-13 19:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44][root][INFO] - Training Epoch: 1/2, step 4088/7134 completed (loss: 0.323087215423584, acc: 0.9187816977500916)
[2025-02-13 19:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44][root][INFO] - Training Epoch: 1/2, step 4089/7134 completed (loss: 0.442585289478302, acc: 0.9015151262283325)
[2025-02-13 19:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45][root][INFO] - Training Epoch: 1/2, step 4090/7134 completed (loss: 0.4798728823661804, acc: 0.880382776260376)
[2025-02-13 19:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45][root][INFO] - Training Epoch: 1/2, step 4091/7134 completed (loss: 0.6042240262031555, acc: 0.8707864880561829)
[2025-02-13 19:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46][root][INFO] - Training Epoch: 1/2, step 4092/7134 completed (loss: 0.24417871236801147, acc: 0.9319371581077576)
[2025-02-13 19:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46][root][INFO] - Training Epoch: 1/2, step 4093/7134 completed (loss: 0.21493089199066162, acc: 0.9599999785423279)
[2025-02-13 19:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46][root][INFO] - Training Epoch: 1/2, step 4094/7134 completed (loss: 0.24963250756263733, acc: 0.9390243887901306)
[2025-02-13 19:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47][root][INFO] - Training Epoch: 1/2, step 4095/7134 completed (loss: 0.16220809519290924, acc: 0.9554139971733093)
[2025-02-13 19:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47][root][INFO] - Training Epoch: 1/2, step 4096/7134 completed (loss: 0.3029612898826599, acc: 0.8831169009208679)
[2025-02-13 19:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47][root][INFO] - Training Epoch: 1/2, step 4097/7134 completed (loss: 0.2981204092502594, acc: 0.9133333563804626)
[2025-02-13 19:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:48][root][INFO] - Training Epoch: 1/2, step 4098/7134 completed (loss: 0.094661645591259, acc: 0.977011501789093)
[2025-02-13 19:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:48][root][INFO] - Training Epoch: 1/2, step 4099/7134 completed (loss: 0.12437975406646729, acc: 0.9772727489471436)
[2025-02-13 19:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49][root][INFO] - Training Epoch: 1/2, step 4100/7134 completed (loss: 0.03286464139819145, acc: 1.0)
[2025-02-13 19:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49][root][INFO] - Training Epoch: 1/2, step 4101/7134 completed (loss: 0.27251288294792175, acc: 0.9200000166893005)
[2025-02-13 19:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49][root][INFO] - Training Epoch: 1/2, step 4102/7134 completed (loss: 0.22657078504562378, acc: 0.9674796462059021)
[2025-02-13 19:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50][root][INFO] - Training Epoch: 1/2, step 4103/7134 completed (loss: 0.09431704878807068, acc: 0.9738562107086182)
[2025-02-13 19:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50][root][INFO] - Training Epoch: 1/2, step 4104/7134 completed (loss: 0.2081393003463745, acc: 0.9347826242446899)
[2025-02-13 19:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50][root][INFO] - Training Epoch: 1/2, step 4105/7134 completed (loss: 0.09090516716241837, acc: 0.9800000190734863)
[2025-02-13 19:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51][root][INFO] - Training Epoch: 1/2, step 4106/7134 completed (loss: 0.13481496274471283, acc: 0.9659863710403442)
[2025-02-13 19:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51][root][INFO] - Training Epoch: 1/2, step 4107/7134 completed (loss: 0.1870308816432953, acc: 0.9632353186607361)
[2025-02-13 19:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51][root][INFO] - Training Epoch: 1/2, step 4108/7134 completed (loss: 0.1013115867972374, acc: 0.9727891087532043)
[2025-02-13 19:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52][root][INFO] - Training Epoch: 1/2, step 4109/7134 completed (loss: 0.10244221985340118, acc: 0.9863945841789246)
[2025-02-13 19:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52][root][INFO] - Training Epoch: 1/2, step 4110/7134 completed (loss: 0.04652976244688034, acc: 0.9922480583190918)
[2025-02-13 19:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52][root][INFO] - Training Epoch: 1/2, step 4111/7134 completed (loss: 0.12844914197921753, acc: 0.951724112033844)
[2025-02-13 19:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53][root][INFO] - Training Epoch: 1/2, step 4112/7134 completed (loss: 0.11082958430051804, acc: 0.9689922332763672)
[2025-02-13 19:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53][root][INFO] - Training Epoch: 1/2, step 4113/7134 completed (loss: 0.10315114259719849, acc: 0.9720279574394226)
[2025-02-13 19:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54][root][INFO] - Training Epoch: 1/2, step 4114/7134 completed (loss: 0.4609972834587097, acc: 0.8608695864677429)
[2025-02-13 19:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54][root][INFO] - Training Epoch: 1/2, step 4115/7134 completed (loss: 0.38243231177330017, acc: 0.9152542352676392)
[2025-02-13 19:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54][root][INFO] - Training Epoch: 1/2, step 4116/7134 completed (loss: 0.11086973547935486, acc: 0.9618320465087891)
[2025-02-13 19:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55][root][INFO] - Training Epoch: 1/2, step 4117/7134 completed (loss: 0.09117496013641357, acc: 0.9722222089767456)
[2025-02-13 19:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55][root][INFO] - Training Epoch: 1/2, step 4118/7134 completed (loss: 0.0811658576130867, acc: 0.9791666865348816)
[2025-02-13 19:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55][root][INFO] - Training Epoch: 1/2, step 4119/7134 completed (loss: 0.19567663967609406, acc: 0.9527027010917664)
[2025-02-13 19:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56][root][INFO] - Training Epoch: 1/2, step 4120/7134 completed (loss: 0.14125704765319824, acc: 0.9849624037742615)
[2025-02-13 19:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56][root][INFO] - Training Epoch: 1/2, step 4121/7134 completed (loss: 0.06653215736150742, acc: 0.9817073345184326)
[2025-02-13 19:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57][root][INFO] - Training Epoch: 1/2, step 4122/7134 completed (loss: 0.054097291082143784, acc: 0.9831932783126831)
[2025-02-13 19:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57][root][INFO] - Training Epoch: 1/2, step 4123/7134 completed (loss: 0.05316546559333801, acc: 0.9870129823684692)
[2025-02-13 19:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57][root][INFO] - Training Epoch: 1/2, step 4124/7134 completed (loss: 0.06336747854948044, acc: 0.987500011920929)
[2025-02-13 19:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58][root][INFO] - Training Epoch: 1/2, step 4125/7134 completed (loss: 0.08375272154808044, acc: 0.9685534834861755)
[2025-02-13 19:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58][root][INFO] - Training Epoch: 1/2, step 4126/7134 completed (loss: 0.14000096917152405, acc: 0.9539473652839661)
[2025-02-13 19:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58][root][INFO] - Training Epoch: 1/2, step 4127/7134 completed (loss: 0.13054244220256805, acc: 0.9731543660163879)
[2025-02-13 19:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59][root][INFO] - Training Epoch: 1/2, step 4128/7134 completed (loss: 0.049926429986953735, acc: 0.9924812316894531)
[2025-02-13 19:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59][root][INFO] - Training Epoch: 1/2, step 4129/7134 completed (loss: 0.10441378504037857, acc: 0.9597315192222595)
[2025-02-13 19:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59][root][INFO] - Training Epoch: 1/2, step 4130/7134 completed (loss: 0.1212066262960434, acc: 0.9482758641242981)
[2025-02-13 19:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00][root][INFO] - Training Epoch: 1/2, step 4131/7134 completed (loss: 0.12896892428398132, acc: 0.9629629850387573)
[2025-02-13 19:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00][root][INFO] - Training Epoch: 1/2, step 4132/7134 completed (loss: 0.048533663153648376, acc: 1.0)
[2025-02-13 19:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01][root][INFO] - Training Epoch: 1/2, step 4133/7134 completed (loss: 0.2822728157043457, acc: 0.9440559148788452)
[2025-02-13 19:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01][root][INFO] - Training Epoch: 1/2, step 4134/7134 completed (loss: 0.3951198160648346, acc: 0.8999999761581421)
[2025-02-13 19:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01][root][INFO] - Training Epoch: 1/2, step 4135/7134 completed (loss: 0.059325315058231354, acc: 0.9857142567634583)
[2025-02-13 19:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02][root][INFO] - Training Epoch: 1/2, step 4136/7134 completed (loss: 0.08198489993810654, acc: 0.969924807548523)
[2025-02-13 19:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02][root][INFO] - Training Epoch: 1/2, step 4137/7134 completed (loss: 0.08205201476812363, acc: 0.9926470518112183)
[2025-02-13 19:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02][root][INFO] - Training Epoch: 1/2, step 4138/7134 completed (loss: 0.22097797691822052, acc: 0.9411764740943909)
[2025-02-13 19:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03][root][INFO] - Training Epoch: 1/2, step 4139/7134 completed (loss: 0.11750946193933487, acc: 0.9714285731315613)
[2025-02-13 19:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03][root][INFO] - Training Epoch: 1/2, step 4140/7134 completed (loss: 0.36966580152511597, acc: 0.9152542352676392)
[2025-02-13 19:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04][root][INFO] - Training Epoch: 1/2, step 4141/7134 completed (loss: 0.25666147470474243, acc: 0.9166666865348816)
[2025-02-13 19:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04][root][INFO] - Training Epoch: 1/2, step 4142/7134 completed (loss: 0.33772528171539307, acc: 0.8999999761581421)
[2025-02-13 19:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04][root][INFO] - Training Epoch: 1/2, step 4143/7134 completed (loss: 0.16924425959587097, acc: 0.9650654792785645)
[2025-02-13 19:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05][root][INFO] - Training Epoch: 1/2, step 4144/7134 completed (loss: 0.2801250219345093, acc: 0.9128205180168152)
[2025-02-13 19:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05][root][INFO] - Training Epoch: 1/2, step 4145/7134 completed (loss: 0.3864874839782715, acc: 0.8813559412956238)
[2025-02-13 19:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05][root][INFO] - Training Epoch: 1/2, step 4146/7134 completed (loss: 0.4965593218803406, acc: 0.8482142686843872)
[2025-02-13 19:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06][root][INFO] - Training Epoch: 1/2, step 4147/7134 completed (loss: 0.7127331495285034, acc: 0.8474576473236084)
[2025-02-13 19:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06][root][INFO] - Training Epoch: 1/2, step 4148/7134 completed (loss: 0.8726903796195984, acc: 0.8120805621147156)
[2025-02-13 19:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06][root][INFO] - Training Epoch: 1/2, step 4149/7134 completed (loss: 0.6060090661048889, acc: 0.8604651093482971)
[2025-02-13 19:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07][root][INFO] - Training Epoch: 1/2, step 4150/7134 completed (loss: 0.8532052040100098, acc: 0.8461538553237915)
[2025-02-13 19:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07][root][INFO] - Training Epoch: 1/2, step 4151/7134 completed (loss: 0.4057948589324951, acc: 0.9256198406219482)
[2025-02-13 19:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08][root][INFO] - Training Epoch: 1/2, step 4152/7134 completed (loss: 0.20355510711669922, acc: 0.9251700639724731)
[2025-02-13 19:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08][root][INFO] - Training Epoch: 1/2, step 4153/7134 completed (loss: 0.3586052358150482, acc: 0.9428571462631226)
[2025-02-13 19:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08][root][INFO] - Training Epoch: 1/2, step 4154/7134 completed (loss: 0.25966987013816833, acc: 0.9138755798339844)
[2025-02-13 19:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09][root][INFO] - Training Epoch: 1/2, step 4155/7134 completed (loss: 0.26299014687538147, acc: 0.9194630980491638)
[2025-02-13 19:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09][root][INFO] - Training Epoch: 1/2, step 4156/7134 completed (loss: 0.2809370756149292, acc: 0.9415204524993896)
[2025-02-13 19:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09][root][INFO] - Training Epoch: 1/2, step 4157/7134 completed (loss: 0.20681509375572205, acc: 0.9411764740943909)
[2025-02-13 19:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10][root][INFO] - Training Epoch: 1/2, step 4158/7134 completed (loss: 0.5616021752357483, acc: 0.901098906993866)
[2025-02-13 19:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10][root][INFO] - Training Epoch: 1/2, step 4159/7134 completed (loss: 0.25258514285087585, acc: 0.9548872113227844)
[2025-02-13 19:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11][root][INFO] - Training Epoch: 1/2, step 4160/7134 completed (loss: 0.3074837923049927, acc: 0.9327731132507324)
[2025-02-13 19:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11][root][INFO] - Training Epoch: 1/2, step 4161/7134 completed (loss: 0.1446196436882019, acc: 0.982758641242981)
[2025-02-13 19:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11][root][INFO] - Training Epoch: 1/2, step 4162/7134 completed (loss: 0.23965252935886383, acc: 0.9341317415237427)
[2025-02-13 19:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12][root][INFO] - Training Epoch: 1/2, step 4163/7134 completed (loss: 0.24460013210773468, acc: 0.9597315192222595)
[2025-02-13 19:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12][root][INFO] - Training Epoch: 1/2, step 4164/7134 completed (loss: 0.18312808871269226, acc: 0.9503105878829956)
[2025-02-13 19:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12][root][INFO] - Training Epoch: 1/2, step 4165/7134 completed (loss: 0.22412802278995514, acc: 0.9408602118492126)
[2025-02-13 19:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13][root][INFO] - Training Epoch: 1/2, step 4166/7134 completed (loss: 0.2971861660480499, acc: 0.9166666865348816)
[2025-02-13 19:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13][root][INFO] - Training Epoch: 1/2, step 4167/7134 completed (loss: 0.26632627844810486, acc: 0.9268292784690857)
[2025-02-13 19:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14][root][INFO] - Training Epoch: 1/2, step 4168/7134 completed (loss: 0.25078463554382324, acc: 0.9398496150970459)
[2025-02-13 19:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14][root][INFO] - Training Epoch: 1/2, step 4169/7134 completed (loss: 0.17557670176029205, acc: 0.9539170265197754)
[2025-02-13 19:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14][root][INFO] - Training Epoch: 1/2, step 4170/7134 completed (loss: 0.26412779092788696, acc: 0.9428571462631226)
[2025-02-13 19:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15][root][INFO] - Training Epoch: 1/2, step 4171/7134 completed (loss: 0.11752751469612122, acc: 0.9704433679580688)
[2025-02-13 19:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15][root][INFO] - Training Epoch: 1/2, step 4172/7134 completed (loss: 0.19442790746688843, acc: 0.9444444179534912)
[2025-02-13 19:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15][root][INFO] - Training Epoch: 1/2, step 4173/7134 completed (loss: 0.18961213529109955, acc: 0.9351851940155029)
[2025-02-13 19:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16][root][INFO] - Training Epoch: 1/2, step 4174/7134 completed (loss: 0.024898072704672813, acc: 0.9920634627342224)
[2025-02-13 19:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16][root][INFO] - Training Epoch: 1/2, step 4175/7134 completed (loss: 0.1894742101430893, acc: 0.9366196990013123)
[2025-02-13 19:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17][root][INFO] - Training Epoch: 1/2, step 4176/7134 completed (loss: 0.056102924048900604, acc: 0.9935483932495117)
[2025-02-13 19:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17][root][INFO] - Training Epoch: 1/2, step 4177/7134 completed (loss: 0.13005532324314117, acc: 0.9649122953414917)
[2025-02-13 19:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17][root][INFO] - Training Epoch: 1/2, step 4178/7134 completed (loss: 0.1577661633491516, acc: 0.9605262875556946)
[2025-02-13 19:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:18][root][INFO] - Training Epoch: 1/2, step 4179/7134 completed (loss: 0.04915718361735344, acc: 1.0)
[2025-02-13 19:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:18][root][INFO] - Training Epoch: 1/2, step 4180/7134 completed (loss: 0.05845290422439575, acc: 0.987500011920929)
[2025-02-13 19:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19][root][INFO] - Training Epoch: 1/2, step 4181/7134 completed (loss: 0.07571464776992798, acc: 0.9710144996643066)
[2025-02-13 19:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19][root][INFO] - Training Epoch: 1/2, step 4182/7134 completed (loss: 0.06494951993227005, acc: 0.9870129823684692)
[2025-02-13 19:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19][root][INFO] - Training Epoch: 1/2, step 4183/7134 completed (loss: 0.1315067857503891, acc: 0.9693251252174377)
[2025-02-13 19:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20][root][INFO] - Training Epoch: 1/2, step 4184/7134 completed (loss: 0.10367846488952637, acc: 0.970588207244873)
[2025-02-13 19:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20][root][INFO] - Training Epoch: 1/2, step 4185/7134 completed (loss: 0.1151229739189148, acc: 0.9809523820877075)
[2025-02-13 19:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21][root][INFO] - Training Epoch: 1/2, step 4186/7134 completed (loss: 0.14565639197826385, acc: 0.9845361113548279)
[2025-02-13 19:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21][root][INFO] - Training Epoch: 1/2, step 4187/7134 completed (loss: 0.036762550473213196, acc: 0.9919354915618896)
[2025-02-13 19:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21][root][INFO] - Training Epoch: 1/2, step 4188/7134 completed (loss: 0.055678848177194595, acc: 0.9842105507850647)
[2025-02-13 19:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22][root][INFO] - Training Epoch: 1/2, step 4189/7134 completed (loss: 0.07447637617588043, acc: 0.9820359349250793)
[2025-02-13 19:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22][root][INFO] - Training Epoch: 1/2, step 4190/7134 completed (loss: 0.13729329407215118, acc: 0.9649122953414917)
[2025-02-13 19:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22][root][INFO] - Training Epoch: 1/2, step 4191/7134 completed (loss: 0.13055019080638885, acc: 0.9711538553237915)
[2025-02-13 19:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23][root][INFO] - Training Epoch: 1/2, step 4192/7134 completed (loss: 0.050928495824337006, acc: 0.9863013625144958)
[2025-02-13 19:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23][root][INFO] - Training Epoch: 1/2, step 4193/7134 completed (loss: 0.11619234085083008, acc: 0.9720670580863953)
[2025-02-13 19:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23][root][INFO] - Training Epoch: 1/2, step 4194/7134 completed (loss: 0.036301445215940475, acc: 0.9871794581413269)
[2025-02-13 19:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:24][root][INFO] - Training Epoch: 1/2, step 4195/7134 completed (loss: 0.16164302825927734, acc: 0.9599999785423279)
[2025-02-13 19:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:24][root][INFO] - Training Epoch: 1/2, step 4196/7134 completed (loss: 0.11944472789764404, acc: 0.9784172773361206)
[2025-02-13 19:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25][root][INFO] - Training Epoch: 1/2, step 4197/7134 completed (loss: 0.11708712577819824, acc: 0.9728260636329651)
[2025-02-13 19:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25][root][INFO] - Training Epoch: 1/2, step 4198/7134 completed (loss: 0.07862425595521927, acc: 0.9942857027053833)
[2025-02-13 19:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25][root][INFO] - Training Epoch: 1/2, step 4199/7134 completed (loss: 0.25953733921051025, acc: 0.9507042169570923)
[2025-02-13 19:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26][root][INFO] - Training Epoch: 1/2, step 4200/7134 completed (loss: 0.17996372282505035, acc: 0.9586206674575806)
[2025-02-13 19:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26][root][INFO] - Training Epoch: 1/2, step 4201/7134 completed (loss: 0.06239798292517662, acc: 0.9858155846595764)
[2025-02-13 19:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26][root][INFO] - Training Epoch: 1/2, step 4202/7134 completed (loss: 0.14408889412879944, acc: 0.969924807548523)
[2025-02-13 19:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:27][root][INFO] - Training Epoch: 1/2, step 4203/7134 completed (loss: 0.12146785855293274, acc: 0.9746835231781006)
[2025-02-13 19:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:27][root][INFO] - Training Epoch: 1/2, step 4204/7134 completed (loss: 0.19409114122390747, acc: 0.9599999785423279)
[2025-02-13 19:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28][root][INFO] - Training Epoch: 1/2, step 4205/7134 completed (loss: 0.19132840633392334, acc: 0.9647058844566345)
[2025-02-13 19:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28][root][INFO] - Training Epoch: 1/2, step 4206/7134 completed (loss: 0.20363040268421173, acc: 0.9623655676841736)
[2025-02-13 19:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28][root][INFO] - Training Epoch: 1/2, step 4207/7134 completed (loss: 0.15219035744667053, acc: 0.9811320900917053)
[2025-02-13 19:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29][root][INFO] - Training Epoch: 1/2, step 4208/7134 completed (loss: 0.16092140972614288, acc: 0.9712643623352051)
[2025-02-13 19:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29][root][INFO] - Training Epoch: 1/2, step 4209/7134 completed (loss: 0.24218618869781494, acc: 0.9668508172035217)
[2025-02-13 19:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29][root][INFO] - Training Epoch: 1/2, step 4210/7134 completed (loss: 0.13075418770313263, acc: 0.9561403393745422)
[2025-02-13 19:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:30][root][INFO] - Training Epoch: 1/2, step 4211/7134 completed (loss: 0.07100566476583481, acc: 0.9672130942344666)
[2025-02-13 19:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:30][root][INFO] - Training Epoch: 1/2, step 4212/7134 completed (loss: 0.12876731157302856, acc: 0.9677419066429138)
[2025-02-13 19:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31][root][INFO] - Training Epoch: 1/2, step 4213/7134 completed (loss: 0.036301389336586, acc: 0.9942528605461121)
[2025-02-13 19:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31][root][INFO] - Training Epoch: 1/2, step 4214/7134 completed (loss: 0.19196845591068268, acc: 0.9649122953414917)
[2025-02-13 19:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31][root][INFO] - Training Epoch: 1/2, step 4215/7134 completed (loss: 0.10579090565443039, acc: 0.9763779640197754)
[2025-02-13 19:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32][root][INFO] - Training Epoch: 1/2, step 4216/7134 completed (loss: 0.10200653225183487, acc: 0.9923076629638672)
[2025-02-13 19:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32][root][INFO] - Training Epoch: 1/2, step 4217/7134 completed (loss: 0.05474357679486275, acc: 0.9833333492279053)
[2025-02-13 19:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32][root][INFO] - Training Epoch: 1/2, step 4218/7134 completed (loss: 0.2246534675359726, acc: 0.9701492786407471)
[2025-02-13 19:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33][root][INFO] - Training Epoch: 1/2, step 4219/7134 completed (loss: 0.07819808274507523, acc: 0.9710144996643066)
[2025-02-13 19:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33][root][INFO] - Training Epoch: 1/2, step 4220/7134 completed (loss: 0.2664898633956909, acc: 0.9545454382896423)
[2025-02-13 19:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33][root][INFO] - Training Epoch: 1/2, step 4221/7134 completed (loss: 0.16641288995742798, acc: 0.9662162065505981)
[2025-02-13 19:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34][root][INFO] - Training Epoch: 1/2, step 4222/7134 completed (loss: 0.5333948135375977, acc: 0.9210526347160339)
[2025-02-13 19:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34][root][INFO] - Training Epoch: 1/2, step 4223/7134 completed (loss: 0.3134326934814453, acc: 0.909547746181488)
[2025-02-13 19:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35][root][INFO] - Training Epoch: 1/2, step 4224/7134 completed (loss: 0.3449772000312805, acc: 0.9435483813285828)
[2025-02-13 19:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35][root][INFO] - Training Epoch: 1/2, step 4225/7134 completed (loss: 0.28411927819252014, acc: 0.9294871687889099)
[2025-02-13 19:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35][root][INFO] - Training Epoch: 1/2, step 4226/7134 completed (loss: 0.35956284403800964, acc: 0.9086294174194336)
[2025-02-13 19:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36][root][INFO] - Training Epoch: 1/2, step 4227/7134 completed (loss: 0.49065399169921875, acc: 0.8679245114326477)
[2025-02-13 19:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36][root][INFO] - Training Epoch: 1/2, step 4228/7134 completed (loss: 0.597571611404419, acc: 0.8589743375778198)
[2025-02-13 19:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36][root][INFO] - Training Epoch: 1/2, step 4229/7134 completed (loss: 0.4379374086856842, acc: 0.8799999952316284)
[2025-02-13 19:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:37][root][INFO] - Training Epoch: 1/2, step 4230/7134 completed (loss: 0.4297953248023987, acc: 0.930232584476471)
[2025-02-13 19:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:37][root][INFO] - Training Epoch: 1/2, step 4231/7134 completed (loss: 0.621705949306488, acc: 0.8657718300819397)
[2025-02-13 19:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38][root][INFO] - Training Epoch: 1/2, step 4232/7134 completed (loss: 0.3313846290111542, acc: 0.9111111164093018)
[2025-02-13 19:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38][root][INFO] - Training Epoch: 1/2, step 4233/7134 completed (loss: 0.2595912516117096, acc: 0.9503546357154846)
[2025-02-13 19:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38][root][INFO] - Training Epoch: 1/2, step 4234/7134 completed (loss: 0.1366075873374939, acc: 0.9645389914512634)
[2025-02-13 19:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39][root][INFO] - Training Epoch: 1/2, step 4235/7134 completed (loss: 0.14759042859077454, acc: 0.95652174949646)
[2025-02-13 19:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39][root][INFO] - Training Epoch: 1/2, step 4236/7134 completed (loss: 0.09069085866212845, acc: 0.9784946441650391)
[2025-02-13 19:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39][root][INFO] - Training Epoch: 1/2, step 4237/7134 completed (loss: 0.15058697760105133, acc: 0.9626168012619019)
[2025-02-13 19:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40][root][INFO] - Training Epoch: 1/2, step 4238/7134 completed (loss: 0.18856221437454224, acc: 0.9846153855323792)
[2025-02-13 19:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40][root][INFO] - Training Epoch: 1/2, step 4239/7134 completed (loss: 0.1505303829908371, acc: 0.9679999947547913)
[2025-02-13 19:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41][root][INFO] - Training Epoch: 1/2, step 4240/7134 completed (loss: 0.26310721039772034, acc: 0.9366196990013123)
[2025-02-13 19:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41][root][INFO] - Training Epoch: 1/2, step 4241/7134 completed (loss: 0.06098105385899544, acc: 0.9900000095367432)
[2025-02-13 19:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41][root][INFO] - Training Epoch: 1/2, step 4242/7134 completed (loss: 0.17062683403491974, acc: 0.9693877696990967)
[2025-02-13 19:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42][root][INFO] - Training Epoch: 1/2, step 4243/7134 completed (loss: 0.1831236332654953, acc: 0.9504132270812988)
[2025-02-13 19:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42][root][INFO] - Training Epoch: 1/2, step 4244/7134 completed (loss: 0.07093024998903275, acc: 0.9905660152435303)
[2025-02-13 19:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42][root][INFO] - Training Epoch: 1/2, step 4245/7134 completed (loss: 0.13632874190807343, acc: 0.9578947424888611)
[2025-02-13 19:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:43][root][INFO] - Training Epoch: 1/2, step 4246/7134 completed (loss: 0.12439074367284775, acc: 0.9589040875434875)
[2025-02-13 19:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:43][root][INFO] - Training Epoch: 1/2, step 4247/7134 completed (loss: 0.09931778162717819, acc: 0.984000027179718)
[2025-02-13 19:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44][root][INFO] - Training Epoch: 1/2, step 4248/7134 completed (loss: 0.24981997907161713, acc: 0.9520547986030579)
[2025-02-13 19:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44][root][INFO] - Training Epoch: 1/2, step 4249/7134 completed (loss: 0.280049592256546, acc: 0.9558823704719543)
[2025-02-13 19:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44][root][INFO] - Training Epoch: 1/2, step 4250/7134 completed (loss: 0.33888304233551025, acc: 0.9454545378684998)
[2025-02-13 19:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45][root][INFO] - Training Epoch: 1/2, step 4251/7134 completed (loss: 0.15924029052257538, acc: 0.9548872113227844)
[2025-02-13 19:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45][root][INFO] - Training Epoch: 1/2, step 4252/7134 completed (loss: 0.1859525442123413, acc: 0.9516128897666931)
[2025-02-13 19:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46][root][INFO] - Training Epoch: 1/2, step 4253/7134 completed (loss: 0.15425415337085724, acc: 0.95652174949646)
[2025-02-13 19:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46][root][INFO] - Training Epoch: 1/2, step 4254/7134 completed (loss: 0.09491610527038574, acc: 0.9772727489471436)
[2025-02-13 19:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46][root][INFO] - Training Epoch: 1/2, step 4255/7134 completed (loss: 0.22437232732772827, acc: 0.9461538195610046)
[2025-02-13 19:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47][root][INFO] - Training Epoch: 1/2, step 4256/7134 completed (loss: 0.11289329081773758, acc: 0.9658119678497314)
[2025-02-13 19:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47][root][INFO] - Training Epoch: 1/2, step 4257/7134 completed (loss: 0.09066648781299591, acc: 0.9621211886405945)
[2025-02-13 19:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47][root][INFO] - Training Epoch: 1/2, step 4258/7134 completed (loss: 0.06149899959564209, acc: 0.9913793206214905)
[2025-02-13 19:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:48][root][INFO] - Training Epoch: 1/2, step 4259/7134 completed (loss: 0.28821977972984314, acc: 0.9437500238418579)
[2025-02-13 19:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:48][root][INFO] - Training Epoch: 1/2, step 4260/7134 completed (loss: 0.12581698596477509, acc: 0.9724770784378052)
[2025-02-13 19:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49][root][INFO] - Training Epoch: 1/2, step 4261/7134 completed (loss: 0.13015708327293396, acc: 0.947826087474823)
[2025-02-13 19:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49][root][INFO] - Training Epoch: 1/2, step 4262/7134 completed (loss: 0.16415604948997498, acc: 0.9510489702224731)
[2025-02-13 19:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49][root][INFO] - Training Epoch: 1/2, step 4263/7134 completed (loss: 0.17274431884288788, acc: 0.9629629850387573)
[2025-02-13 19:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50][root][INFO] - Training Epoch: 1/2, step 4264/7134 completed (loss: 0.31012988090515137, acc: 0.915730357170105)
[2025-02-13 19:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50][root][INFO] - Training Epoch: 1/2, step 4265/7134 completed (loss: 0.21245789527893066, acc: 0.9360465407371521)
[2025-02-13 19:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51][root][INFO] - Training Epoch: 1/2, step 4266/7134 completed (loss: 0.10058095306158066, acc: 0.9738219976425171)
[2025-02-13 19:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51][root][INFO] - Training Epoch: 1/2, step 4267/7134 completed (loss: 0.1338542103767395, acc: 0.9545454382896423)
[2025-02-13 19:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51][root][INFO] - Training Epoch: 1/2, step 4268/7134 completed (loss: 0.28441762924194336, acc: 0.9268292784690857)
[2025-02-13 19:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52][root][INFO] - Training Epoch: 1/2, step 4269/7134 completed (loss: 0.3171388804912567, acc: 0.9364162087440491)
[2025-02-13 19:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52][root][INFO] - Training Epoch: 1/2, step 4270/7134 completed (loss: 0.1989894062280655, acc: 0.9427083134651184)
[2025-02-13 19:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53][root][INFO] - Training Epoch: 1/2, step 4271/7134 completed (loss: 0.1121753454208374, acc: 0.9621621370315552)
[2025-02-13 19:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53][root][INFO] - Training Epoch: 1/2, step 4272/7134 completed (loss: 0.1957835853099823, acc: 0.9502487778663635)
[2025-02-13 19:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53][root][INFO] - Training Epoch: 1/2, step 4273/7134 completed (loss: 0.12762674689292908, acc: 0.9682539701461792)
[2025-02-13 19:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54][root][INFO] - Training Epoch: 1/2, step 4274/7134 completed (loss: 0.3006672263145447, acc: 0.936274528503418)
[2025-02-13 19:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54][root][INFO] - Training Epoch: 1/2, step 4275/7134 completed (loss: 0.1298617720603943, acc: 0.9608938694000244)
[2025-02-13 19:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54][root][INFO] - Training Epoch: 1/2, step 4276/7134 completed (loss: 0.1493014097213745, acc: 0.9729729890823364)
[2025-02-13 19:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55][root][INFO] - Training Epoch: 1/2, step 4277/7134 completed (loss: 0.06624811887741089, acc: 0.9788732528686523)
[2025-02-13 19:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55][root][INFO] - Training Epoch: 1/2, step 4278/7134 completed (loss: 0.1236565038561821, acc: 0.9795918464660645)
[2025-02-13 19:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56][root][INFO] - Training Epoch: 1/2, step 4279/7134 completed (loss: 0.10021655261516571, acc: 0.9751552939414978)
[2025-02-13 19:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56][root][INFO] - Training Epoch: 1/2, step 4280/7134 completed (loss: 0.049185846000909805, acc: 0.9858155846595764)
[2025-02-13 19:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56][root][INFO] - Training Epoch: 1/2, step 4281/7134 completed (loss: 0.1623786836862564, acc: 0.9569892287254333)
[2025-02-13 19:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57][root][INFO] - Training Epoch: 1/2, step 4282/7134 completed (loss: 0.14435021579265594, acc: 0.9396551847457886)
[2025-02-13 19:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57][root][INFO] - Training Epoch: 1/2, step 4283/7134 completed (loss: 0.11643263697624207, acc: 0.9744898080825806)
[2025-02-13 19:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58][root][INFO] - Training Epoch: 1/2, step 4284/7134 completed (loss: 0.19247952103614807, acc: 0.9457364082336426)
[2025-02-13 19:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58][root][INFO] - Training Epoch: 1/2, step 4285/7134 completed (loss: 0.06559215486049652, acc: 0.9844961166381836)
[2025-02-13 19:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58][root][INFO] - Training Epoch: 1/2, step 4286/7134 completed (loss: 0.08505865186452866, acc: 0.9663865566253662)
[2025-02-13 19:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59][root][INFO] - Training Epoch: 1/2, step 4287/7134 completed (loss: 0.1855960488319397, acc: 0.967391312122345)
[2025-02-13 19:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59][root][INFO] - Training Epoch: 1/2, step 4288/7134 completed (loss: 0.13439135253429413, acc: 0.9588235020637512)
[2025-02-13 19:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00][root][INFO] - Training Epoch: 1/2, step 4289/7134 completed (loss: 0.224356546998024, acc: 0.9386503100395203)
[2025-02-13 19:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00][root][INFO] - Training Epoch: 1/2, step 4290/7134 completed (loss: 0.124534972012043, acc: 0.9826589822769165)
[2025-02-13 19:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00][root][INFO] - Training Epoch: 1/2, step 4291/7134 completed (loss: 0.07277676463127136, acc: 0.9768785834312439)
[2025-02-13 19:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:01][root][INFO] - Training Epoch: 1/2, step 4292/7134 completed (loss: 0.165161594748497, acc: 0.9677419066429138)
[2025-02-13 19:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:01][root][INFO] - Training Epoch: 1/2, step 4293/7134 completed (loss: 0.13638237118721008, acc: 0.9666666388511658)
[2025-02-13 19:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02][root][INFO] - Training Epoch: 1/2, step 4294/7134 completed (loss: 0.2568529546260834, acc: 0.9371428489685059)
[2025-02-13 19:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02][root][INFO] - Training Epoch: 1/2, step 4295/7134 completed (loss: 0.2251589149236679, acc: 0.949999988079071)
[2025-02-13 19:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02][root][INFO] - Training Epoch: 1/2, step 4296/7134 completed (loss: 0.0725623145699501, acc: 0.9801980257034302)
[2025-02-13 19:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03][root][INFO] - Training Epoch: 1/2, step 4297/7134 completed (loss: 0.04343139007687569, acc: 0.9809523820877075)
[2025-02-13 19:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03][root][INFO] - Training Epoch: 1/2, step 4298/7134 completed (loss: 0.1274213194847107, acc: 0.9659090638160706)
[2025-02-13 19:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04][root][INFO] - Training Epoch: 1/2, step 4299/7134 completed (loss: 0.19565139710903168, acc: 0.9301075339317322)
[2025-02-13 19:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04][root][INFO] - Training Epoch: 1/2, step 4300/7134 completed (loss: 0.3200951814651489, acc: 0.9159663915634155)
[2025-02-13 19:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04][root][INFO] - Training Epoch: 1/2, step 4301/7134 completed (loss: 0.08734317123889923, acc: 0.9718309640884399)
[2025-02-13 19:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:05][root][INFO] - Training Epoch: 1/2, step 4302/7134 completed (loss: 0.28274527192115784, acc: 0.9426751732826233)
[2025-02-13 19:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:05][root][INFO] - Training Epoch: 1/2, step 4303/7134 completed (loss: 0.19956998527050018, acc: 0.9504950642585754)
[2025-02-13 19:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06][root][INFO] - Training Epoch: 1/2, step 4304/7134 completed (loss: 0.14187024533748627, acc: 0.9585798978805542)
[2025-02-13 19:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06][root][INFO] - Training Epoch: 1/2, step 4305/7134 completed (loss: 0.12836068868637085, acc: 0.9700000286102295)
[2025-02-13 19:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06][root][INFO] - Training Epoch: 1/2, step 4306/7134 completed (loss: 0.20191216468811035, acc: 0.9581151604652405)
[2025-02-13 19:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:07][root][INFO] - Training Epoch: 1/2, step 4307/7134 completed (loss: 0.15149487555027008, acc: 0.9607843160629272)
[2025-02-13 19:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:07][root][INFO] - Training Epoch: 1/2, step 4308/7134 completed (loss: 0.17958781123161316, acc: 0.9518716335296631)
[2025-02-13 19:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08][root][INFO] - Training Epoch: 1/2, step 4309/7134 completed (loss: 0.2264302670955658, acc: 0.9431818127632141)
[2025-02-13 19:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08][root][INFO] - Training Epoch: 1/2, step 4310/7134 completed (loss: 0.17675304412841797, acc: 0.9593908786773682)
[2025-02-13 19:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08][root][INFO] - Training Epoch: 1/2, step 4311/7134 completed (loss: 0.10423887521028519, acc: 0.9716312289237976)
[2025-02-13 19:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09][root][INFO] - Training Epoch: 1/2, step 4312/7134 completed (loss: 0.13903601467609406, acc: 0.9631901979446411)
[2025-02-13 19:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09][root][INFO] - Training Epoch: 1/2, step 4313/7134 completed (loss: 0.09537577629089355, acc: 0.9636363387107849)
[2025-02-13 19:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09][root][INFO] - Training Epoch: 1/2, step 4314/7134 completed (loss: 0.13749338686466217, acc: 0.9803921580314636)
[2025-02-13 19:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10][root][INFO] - Training Epoch: 1/2, step 4315/7134 completed (loss: 0.13032935559749603, acc: 0.9642857313156128)
[2025-02-13 19:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10][root][INFO] - Training Epoch: 1/2, step 4316/7134 completed (loss: 0.07913918793201447, acc: 0.9820359349250793)
[2025-02-13 19:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11][root][INFO] - Training Epoch: 1/2, step 4317/7134 completed (loss: 0.07190114259719849, acc: 0.9825581312179565)
[2025-02-13 19:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11][root][INFO] - Training Epoch: 1/2, step 4318/7134 completed (loss: 0.1826486587524414, acc: 0.954081654548645)
[2025-02-13 19:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11][root][INFO] - Training Epoch: 1/2, step 4319/7134 completed (loss: 0.09343759715557098, acc: 0.9655172228813171)
[2025-02-13 19:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12][root][INFO] - Training Epoch: 1/2, step 4320/7134 completed (loss: 0.18602408468723297, acc: 0.942307710647583)
[2025-02-13 19:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12][root][INFO] - Training Epoch: 1/2, step 4321/7134 completed (loss: 0.49537575244903564, acc: 0.9207921028137207)
[2025-02-13 19:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13][root][INFO] - Training Epoch: 1/2, step 4322/7134 completed (loss: 0.3808940351009369, acc: 0.9119496941566467)
[2025-02-13 19:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13][root][INFO] - Training Epoch: 1/2, step 4323/7134 completed (loss: 0.14585094153881073, acc: 0.9473684430122375)
[2025-02-13 19:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13][root][INFO] - Training Epoch: 1/2, step 4324/7134 completed (loss: 0.118087999522686, acc: 0.9739130139350891)
[2025-02-13 19:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14][root][INFO] - Training Epoch: 1/2, step 4325/7134 completed (loss: 0.2047613561153412, acc: 0.9383561611175537)
[2025-02-13 19:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14][root][INFO] - Training Epoch: 1/2, step 4326/7134 completed (loss: 0.2648946940898895, acc: 0.9545454382896423)
[2025-02-13 19:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14][root][INFO] - Training Epoch: 1/2, step 4327/7134 completed (loss: 0.0908621996641159, acc: 0.9635036587715149)
[2025-02-13 19:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15][root][INFO] - Training Epoch: 1/2, step 4328/7134 completed (loss: 0.11168409883975983, acc: 0.9765625)
[2025-02-13 19:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15][root][INFO] - Training Epoch: 1/2, step 4329/7134 completed (loss: 0.18136422336101532, acc: 0.9590643048286438)
[2025-02-13 19:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16][root][INFO] - Training Epoch: 1/2, step 4330/7134 completed (loss: 0.12554670870304108, acc: 0.9639639854431152)
[2025-02-13 19:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16][root][INFO] - Training Epoch: 1/2, step 4331/7134 completed (loss: 0.1341872215270996, acc: 0.9629629850387573)
[2025-02-13 19:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16][root][INFO] - Training Epoch: 1/2, step 4332/7134 completed (loss: 0.22505708038806915, acc: 0.9365079402923584)
[2025-02-13 19:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17][root][INFO] - Training Epoch: 1/2, step 4333/7134 completed (loss: 0.101314976811409, acc: 0.981249988079071)
[2025-02-13 19:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17][root][INFO] - Training Epoch: 1/2, step 4334/7134 completed (loss: 0.1932714283466339, acc: 0.9631901979446411)
[2025-02-13 19:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18][root][INFO] - Training Epoch: 1/2, step 4335/7134 completed (loss: 0.16013038158416748, acc: 0.9834710955619812)
[2025-02-13 19:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18][root][INFO] - Training Epoch: 1/2, step 4336/7134 completed (loss: 0.054433587938547134, acc: 0.9784172773361206)
[2025-02-13 19:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18][root][INFO] - Training Epoch: 1/2, step 4337/7134 completed (loss: 0.06073096767067909, acc: 0.9860140085220337)
[2025-02-13 19:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19][root][INFO] - Training Epoch: 1/2, step 4338/7134 completed (loss: 0.09393902122974396, acc: 0.9818181991577148)
[2025-02-13 19:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19][root][INFO] - Training Epoch: 1/2, step 4339/7134 completed (loss: 0.19174297153949738, acc: 0.9588235020637512)
[2025-02-13 19:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19][root][INFO] - Training Epoch: 1/2, step 4340/7134 completed (loss: 0.16749320924282074, acc: 0.951724112033844)
[2025-02-13 19:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:20][root][INFO] - Training Epoch: 1/2, step 4341/7134 completed (loss: 0.11398109048604965, acc: 0.9727891087532043)
[2025-02-13 19:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:20][root][INFO] - Training Epoch: 1/2, step 4342/7134 completed (loss: 0.24401342868804932, acc: 0.9271523356437683)
[2025-02-13 19:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:21][root][INFO] - Training Epoch: 1/2, step 4343/7134 completed (loss: 0.25889456272125244, acc: 0.9481481313705444)
[2025-02-13 19:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:21][root][INFO] - Training Epoch: 1/2, step 4344/7134 completed (loss: 0.186895489692688, acc: 0.95652174949646)
[2025-02-13 19:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:21][root][INFO] - Training Epoch: 1/2, step 4345/7134 completed (loss: 0.10740570724010468, acc: 0.9819819927215576)
[2025-02-13 19:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22][root][INFO] - Training Epoch: 1/2, step 4346/7134 completed (loss: 0.1469082236289978, acc: 0.9729729890823364)
[2025-02-13 19:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22][root][INFO] - Training Epoch: 1/2, step 4347/7134 completed (loss: 0.250995397567749, acc: 0.9349112510681152)
[2025-02-13 19:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22][root][INFO] - Training Epoch: 1/2, step 4348/7134 completed (loss: 0.14771194756031036, acc: 0.9591836929321289)
[2025-02-13 19:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:23][root][INFO] - Training Epoch: 1/2, step 4349/7134 completed (loss: 0.18988825380802155, acc: 0.9583333134651184)
[2025-02-13 19:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:23][root][INFO] - Training Epoch: 1/2, step 4350/7134 completed (loss: 0.14476922154426575, acc: 0.9550561904907227)
[2025-02-13 19:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24][root][INFO] - Training Epoch: 1/2, step 4351/7134 completed (loss: 0.15865719318389893, acc: 0.9551281929016113)
[2025-02-13 19:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24][root][INFO] - Training Epoch: 1/2, step 4352/7134 completed (loss: 0.2822190821170807, acc: 0.9316770434379578)
[2025-02-13 19:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24][root][INFO] - Training Epoch: 1/2, step 4353/7134 completed (loss: 0.17209531366825104, acc: 0.9341317415237427)
[2025-02-13 19:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:25][root][INFO] - Training Epoch: 1/2, step 4354/7134 completed (loss: 0.1619088053703308, acc: 0.9693251252174377)
[2025-02-13 19:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:25][root][INFO] - Training Epoch: 1/2, step 4355/7134 completed (loss: 0.34146347641944885, acc: 0.9254658222198486)
[2025-02-13 19:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26][root][INFO] - Training Epoch: 1/2, step 4356/7134 completed (loss: 0.12049727141857147, acc: 0.9825581312179565)
[2025-02-13 19:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26][root][INFO] - Training Epoch: 1/2, step 4357/7134 completed (loss: 0.3095416724681854, acc: 0.9328858852386475)
[2025-02-13 19:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26][root][INFO] - Training Epoch: 1/2, step 4358/7134 completed (loss: 0.26338568329811096, acc: 0.9268292784690857)
[2025-02-13 19:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:27][root][INFO] - Training Epoch: 1/2, step 4359/7134 completed (loss: 0.1343073695898056, acc: 0.976190447807312)
[2025-02-13 19:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:27][root][INFO] - Training Epoch: 1/2, step 4360/7134 completed (loss: 0.10092483460903168, acc: 0.9745222926139832)
[2025-02-13 19:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28][root][INFO] - Training Epoch: 1/2, step 4361/7134 completed (loss: 0.15219537913799286, acc: 0.9602649211883545)
[2025-02-13 19:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28][root][INFO] - Training Epoch: 1/2, step 4362/7134 completed (loss: 0.2326628416776657, acc: 0.9402173757553101)
[2025-02-13 19:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28][root][INFO] - Training Epoch: 1/2, step 4363/7134 completed (loss: 0.15807947516441345, acc: 0.9696969985961914)
[2025-02-13 19:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29][root][INFO] - Training Epoch: 1/2, step 4364/7134 completed (loss: 0.1513814628124237, acc: 0.9534883499145508)
[2025-02-13 19:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29][root][INFO] - Training Epoch: 1/2, step 4365/7134 completed (loss: 0.11296310275793076, acc: 0.971222996711731)
[2025-02-13 19:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29][root][INFO] - Training Epoch: 1/2, step 4366/7134 completed (loss: 0.21152032911777496, acc: 0.9476743936538696)
[2025-02-13 19:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30][root][INFO] - Training Epoch: 1/2, step 4367/7134 completed (loss: 0.11266360431909561, acc: 0.9798657894134521)
[2025-02-13 19:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30][root][INFO] - Training Epoch: 1/2, step 4368/7134 completed (loss: 0.09154344350099564, acc: 0.9940119981765747)
[2025-02-13 19:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30][root][INFO] - Training Epoch: 1/2, step 4369/7134 completed (loss: 0.11326373368501663, acc: 0.9756097793579102)
[2025-02-13 19:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:31][root][INFO] - Training Epoch: 1/2, step 4370/7134 completed (loss: 0.2457578331232071, acc: 0.9709302186965942)
[2025-02-13 19:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:31][root][INFO] - Training Epoch: 1/2, step 4371/7134 completed (loss: 0.12265966832637787, acc: 0.9817073345184326)
[2025-02-13 19:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32][root][INFO] - Training Epoch: 1/2, step 4372/7134 completed (loss: 0.08156605809926987, acc: 0.9754098653793335)
[2025-02-13 19:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32][root][INFO] - Training Epoch: 1/2, step 4373/7134 completed (loss: 0.15586189925670624, acc: 0.9668508172035217)
[2025-02-13 19:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32][root][INFO] - Training Epoch: 1/2, step 4374/7134 completed (loss: 0.2127869576215744, acc: 0.9512194991111755)
[2025-02-13 19:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33][root][INFO] - Training Epoch: 1/2, step 4375/7134 completed (loss: 0.22547315061092377, acc: 0.9404761791229248)
[2025-02-13 19:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33][root][INFO] - Training Epoch: 1/2, step 4376/7134 completed (loss: 0.17439162731170654, acc: 0.9375)
[2025-02-13 19:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33][root][INFO] - Training Epoch: 1/2, step 4377/7134 completed (loss: 0.2626081705093384, acc: 0.9647058844566345)
[2025-02-13 19:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34][root][INFO] - Training Epoch: 1/2, step 4378/7134 completed (loss: 0.3148120939731598, acc: 0.9473684430122375)
[2025-02-13 19:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34][root][INFO] - Training Epoch: 1/2, step 4379/7134 completed (loss: 0.3388163149356842, acc: 0.9117646813392639)
[2025-02-13 19:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35][root][INFO] - Training Epoch: 1/2, step 4380/7134 completed (loss: 0.2491065412759781, acc: 0.9300699234008789)
[2025-02-13 19:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35][root][INFO] - Training Epoch: 1/2, step 4381/7134 completed (loss: 0.11467590928077698, acc: 0.9820359349250793)
[2025-02-13 19:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35][root][INFO] - Training Epoch: 1/2, step 4382/7134 completed (loss: 0.3364894688129425, acc: 0.9161290526390076)
[2025-02-13 19:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36][root][INFO] - Training Epoch: 1/2, step 4383/7134 completed (loss: 0.45787736773490906, acc: 0.893401026725769)
[2025-02-13 19:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36][root][INFO] - Training Epoch: 1/2, step 4384/7134 completed (loss: 0.10801887512207031, acc: 0.9829545617103577)
[2025-02-13 19:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37][root][INFO] - Training Epoch: 1/2, step 4385/7134 completed (loss: 0.3294515907764435, acc: 0.9119170904159546)
[2025-02-13 19:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37][root][INFO] - Training Epoch: 1/2, step 4386/7134 completed (loss: 0.19875991344451904, acc: 0.9514563083648682)
[2025-02-13 19:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37][root][INFO] - Training Epoch: 1/2, step 4387/7134 completed (loss: 0.45310673117637634, acc: 0.9058823585510254)
[2025-02-13 19:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38][root][INFO] - Training Epoch: 1/2, step 4388/7134 completed (loss: 0.3897199034690857, acc: 0.9090909361839294)
[2025-02-13 19:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38][root][INFO] - Training Epoch: 1/2, step 4389/7134 completed (loss: 0.3953455090522766, acc: 0.8768116235733032)
[2025-02-13 19:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38][root][INFO] - Training Epoch: 1/2, step 4390/7134 completed (loss: 0.3885650634765625, acc: 0.9018405079841614)
[2025-02-13 19:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:39][root][INFO] - Training Epoch: 1/2, step 4391/7134 completed (loss: 0.47626930475234985, acc: 0.9178082346916199)
[2025-02-13 19:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:39][root][INFO] - Training Epoch: 1/2, step 4392/7134 completed (loss: 0.16329793632030487, acc: 0.9629629850387573)
[2025-02-13 19:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40][root][INFO] - Training Epoch: 1/2, step 4393/7134 completed (loss: 0.1618037223815918, acc: 0.9599999785423279)
[2025-02-13 19:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40][root][INFO] - Training Epoch: 1/2, step 4394/7134 completed (loss: 0.16084717214107513, acc: 0.9597315192222595)
[2025-02-13 19:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40][root][INFO] - Training Epoch: 1/2, step 4395/7134 completed (loss: 0.308736115694046, acc: 0.9378882050514221)
[2025-02-13 19:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41][root][INFO] - Training Epoch: 1/2, step 4396/7134 completed (loss: 0.5288761258125305, acc: 0.8994709253311157)
[2025-02-13 19:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41][root][INFO] - Training Epoch: 1/2, step 4397/7134 completed (loss: 0.9592301845550537, acc: 0.8198198080062866)
[2025-02-13 19:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42][root][INFO] - Training Epoch: 1/2, step 4398/7134 completed (loss: 0.30923277139663696, acc: 0.9555555582046509)
[2025-02-13 19:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42][root][INFO] - Training Epoch: 1/2, step 4399/7134 completed (loss: 0.1922483742237091, acc: 0.9652777910232544)
[2025-02-13 19:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42][root][INFO] - Training Epoch: 1/2, step 4400/7134 completed (loss: 0.1840127408504486, acc: 0.9631578922271729)
[2025-02-13 19:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43][root][INFO] - Training Epoch: 1/2, step 4401/7134 completed (loss: 0.2647796869277954, acc: 0.9419354796409607)
[2025-02-13 19:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43][root][INFO] - Training Epoch: 1/2, step 4402/7134 completed (loss: 0.5465080142021179, acc: 0.8765432238578796)
[2025-02-13 19:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43][root][INFO] - Training Epoch: 1/2, step 4403/7134 completed (loss: 0.6534736156463623, acc: 0.8545454740524292)
[2025-02-13 19:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:44][root][INFO] - Training Epoch: 1/2, step 4404/7134 completed (loss: 0.5031823515892029, acc: 0.8608247637748718)
[2025-02-13 19:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:44][root][INFO] - Training Epoch: 1/2, step 4405/7134 completed (loss: 0.2625603973865509, acc: 0.9356435537338257)
[2025-02-13 19:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45][root][INFO] - Training Epoch: 1/2, step 4406/7134 completed (loss: 0.15175791084766388, acc: 0.9658536314964294)
[2025-02-13 19:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45][root][INFO] - Training Epoch: 1/2, step 4407/7134 completed (loss: 0.23954372107982635, acc: 0.9489796161651611)
[2025-02-13 19:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45][root][INFO] - Training Epoch: 1/2, step 4408/7134 completed (loss: 0.3638969659805298, acc: 0.8942307829856873)
[2025-02-13 19:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46][root][INFO] - Training Epoch: 1/2, step 4409/7134 completed (loss: 0.21860812604427338, acc: 0.9583333134651184)
[2025-02-13 19:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46][root][INFO] - Training Epoch: 1/2, step 4410/7134 completed (loss: 0.4474559426307678, acc: 0.8951048851013184)
[2025-02-13 19:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47][root][INFO] - Training Epoch: 1/2, step 4411/7134 completed (loss: 0.4488648474216461, acc: 0.897849440574646)
[2025-02-13 19:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47][root][INFO] - Training Epoch: 1/2, step 4412/7134 completed (loss: 0.7642192840576172, acc: 0.839195966720581)
[2025-02-13 19:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47][root][INFO] - Training Epoch: 1/2, step 4413/7134 completed (loss: 0.49215471744537354, acc: 0.9119170904159546)
[2025-02-13 19:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48][root][INFO] - Training Epoch: 1/2, step 4414/7134 completed (loss: 0.2399045079946518, acc: 0.9408602118492126)
[2025-02-13 19:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48][root][INFO] - Training Epoch: 1/2, step 4415/7134 completed (loss: 0.2435649037361145, acc: 0.932584285736084)
[2025-02-13 19:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48][root][INFO] - Training Epoch: 1/2, step 4416/7134 completed (loss: 0.2123723179101944, acc: 0.9430052042007446)
[2025-02-13 19:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:49][root][INFO] - Training Epoch: 1/2, step 4417/7134 completed (loss: 0.41479548811912537, acc: 0.9141414165496826)
[2025-02-13 19:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:49][root][INFO] - Training Epoch: 1/2, step 4418/7134 completed (loss: 0.30833354592323303, acc: 0.9021739363670349)
[2025-02-13 19:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50][root][INFO] - Training Epoch: 1/2, step 4419/7134 completed (loss: 0.3001425266265869, acc: 0.9414634108543396)
[2025-02-13 19:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50][root][INFO] - Training Epoch: 1/2, step 4420/7134 completed (loss: 0.337413489818573, acc: 0.9170984625816345)
[2025-02-13 19:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50][root][INFO] - Training Epoch: 1/2, step 4421/7134 completed (loss: 0.24957884848117828, acc: 0.9449541568756104)
[2025-02-13 19:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51][root][INFO] - Training Epoch: 1/2, step 4422/7134 completed (loss: 0.35348600149154663, acc: 0.910179615020752)
[2025-02-13 19:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51][root][INFO] - Training Epoch: 1/2, step 4423/7134 completed (loss: 0.2169402837753296, acc: 0.9419642686843872)
[2025-02-13 19:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52][root][INFO] - Training Epoch: 1/2, step 4424/7134 completed (loss: 0.18417511880397797, acc: 0.9545454382896423)
[2025-02-13 19:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52][root][INFO] - Training Epoch: 1/2, step 4425/7134 completed (loss: 0.1635398417711258, acc: 0.9603960514068604)
[2025-02-13 19:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52][root][INFO] - Training Epoch: 1/2, step 4426/7134 completed (loss: 0.29379045963287354, acc: 0.9279999732971191)
[2025-02-13 19:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53][root][INFO] - Training Epoch: 1/2, step 4427/7134 completed (loss: 0.6759971976280212, acc: 0.8362069129943848)
[2025-02-13 19:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53][root][INFO] - Training Epoch: 1/2, step 4428/7134 completed (loss: 0.16200096905231476, acc: 0.9537572264671326)
[2025-02-13 19:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54][root][INFO] - Training Epoch: 1/2, step 4429/7134 completed (loss: 0.27459752559661865, acc: 0.9142857193946838)
[2025-02-13 19:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54][root][INFO] - Training Epoch: 1/2, step 4430/7134 completed (loss: 0.2953316867351532, acc: 0.9269663095474243)
[2025-02-13 19:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54][root][INFO] - Training Epoch: 1/2, step 4431/7134 completed (loss: 0.29363417625427246, acc: 0.9244186282157898)
[2025-02-13 19:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55][root][INFO] - Training Epoch: 1/2, step 4432/7134 completed (loss: 0.1967749446630478, acc: 0.9617834687232971)
[2025-02-13 19:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55][root][INFO] - Training Epoch: 1/2, step 4433/7134 completed (loss: 0.187594473361969, acc: 0.961904764175415)
[2025-02-13 19:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56][root][INFO] - Training Epoch: 1/2, step 4434/7134 completed (loss: 0.33875903487205505, acc: 0.9257143139839172)
[2025-02-13 19:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56][root][INFO] - Training Epoch: 1/2, step 4435/7134 completed (loss: 0.2697325646877289, acc: 0.9236111044883728)
[2025-02-13 19:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56][root][INFO] - Training Epoch: 1/2, step 4436/7134 completed (loss: 0.310996949672699, acc: 0.9230769276618958)
[2025-02-13 19:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57][root][INFO] - Training Epoch: 1/2, step 4437/7134 completed (loss: 0.06340239942073822, acc: 0.9781420826911926)
[2025-02-13 19:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57][root][INFO] - Training Epoch: 1/2, step 4438/7134 completed (loss: 0.2956882119178772, acc: 0.9402984976768494)
[2025-02-13 19:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58][root][INFO] - Training Epoch: 1/2, step 4439/7134 completed (loss: 0.05145485699176788, acc: 0.9929078221321106)
[2025-02-13 19:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58][root][INFO] - Training Epoch: 1/2, step 4440/7134 completed (loss: 0.17228347063064575, acc: 0.9578313231468201)
[2025-02-13 19:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58][root][INFO] - Training Epoch: 1/2, step 4441/7134 completed (loss: 0.1323452740907669, acc: 0.9748427867889404)
[2025-02-13 19:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59][root][INFO] - Training Epoch: 1/2, step 4442/7134 completed (loss: 0.3684327304363251, acc: 0.9382022619247437)
[2025-02-13 19:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59][root][INFO] - Training Epoch: 1/2, step 4443/7134 completed (loss: 0.25048670172691345, acc: 0.9352940917015076)
[2025-02-13 19:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59][root][INFO] - Training Epoch: 1/2, step 4444/7134 completed (loss: 0.18980206549167633, acc: 0.9489051103591919)
[2025-02-13 19:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:00][root][INFO] - Training Epoch: 1/2, step 4445/7134 completed (loss: 0.35000720620155334, acc: 0.9112426042556763)
[2025-02-13 19:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:00][root][INFO] - Training Epoch: 1/2, step 4446/7134 completed (loss: 0.14204710721969604, acc: 0.9534883499145508)
[2025-02-13 19:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01][root][INFO] - Training Epoch: 1/2, step 4447/7134 completed (loss: 0.09765208512544632, acc: 0.982758641242981)
[2025-02-13 19:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01][root][INFO] - Training Epoch: 1/2, step 4448/7134 completed (loss: 0.32524681091308594, acc: 0.9147287011146545)
[2025-02-13 19:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01][root][INFO] - Training Epoch: 1/2, step 4449/7134 completed (loss: 0.31581056118011475, acc: 0.9576271176338196)
[2025-02-13 19:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02][root][INFO] - Training Epoch: 1/2, step 4450/7134 completed (loss: 0.2487291842699051, acc: 0.9255319237709045)
[2025-02-13 19:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02][root][INFO] - Training Epoch: 1/2, step 4451/7134 completed (loss: 0.4578414261341095, acc: 0.9108280539512634)
[2025-02-13 19:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03][root][INFO] - Training Epoch: 1/2, step 4452/7134 completed (loss: 0.21801413595676422, acc: 0.9407407641410828)
[2025-02-13 19:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03][root][INFO] - Training Epoch: 1/2, step 4453/7134 completed (loss: 0.18746758997440338, acc: 0.9542483687400818)
[2025-02-13 19:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03][root][INFO] - Training Epoch: 1/2, step 4454/7134 completed (loss: 0.2551147937774658, acc: 0.9503105878829956)
[2025-02-13 19:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04][root][INFO] - Training Epoch: 1/2, step 4455/7134 completed (loss: 0.3705854117870331, acc: 0.9444444179534912)
[2025-02-13 19:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04][root][INFO] - Training Epoch: 1/2, step 4456/7134 completed (loss: 0.07755453884601593, acc: 0.9878787994384766)
[2025-02-13 19:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04][root][INFO] - Training Epoch: 1/2, step 4457/7134 completed (loss: 0.07608065754175186, acc: 0.9870129823684692)
[2025-02-13 19:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05][root][INFO] - Training Epoch: 1/2, step 4458/7134 completed (loss: 0.06135889142751694, acc: 0.9860140085220337)
[2025-02-13 19:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05][root][INFO] - Training Epoch: 1/2, step 4459/7134 completed (loss: 0.08292508870363235, acc: 0.9883720874786377)
[2025-02-13 19:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06][root][INFO] - Training Epoch: 1/2, step 4460/7134 completed (loss: 0.11340208351612091, acc: 0.9802955389022827)
[2025-02-13 19:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06][root][INFO] - Training Epoch: 1/2, step 4461/7134 completed (loss: 0.10393927246332169, acc: 0.9739583134651184)
[2025-02-13 19:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06][root][INFO] - Training Epoch: 1/2, step 4462/7134 completed (loss: 0.12522628903388977, acc: 0.9657142758369446)
[2025-02-13 19:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:07][root][INFO] - Training Epoch: 1/2, step 4463/7134 completed (loss: 0.06422775238752365, acc: 0.9841269850730896)
[2025-02-13 19:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:07][root][INFO] - Training Epoch: 1/2, step 4464/7134 completed (loss: 0.1414695680141449, acc: 0.9589040875434875)
[2025-02-13 19:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08][root][INFO] - Training Epoch: 1/2, step 4465/7134 completed (loss: 0.10498451441526413, acc: 0.9898989796638489)
[2025-02-13 19:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08][root][INFO] - Training Epoch: 1/2, step 4466/7134 completed (loss: 0.09740359336137772, acc: 0.9696969985961914)
[2025-02-13 19:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08][root][INFO] - Training Epoch: 1/2, step 4467/7134 completed (loss: 0.07930923998355865, acc: 0.9679144620895386)
[2025-02-13 19:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09][root][INFO] - Training Epoch: 1/2, step 4468/7134 completed (loss: 0.03854325786232948, acc: 0.9893048405647278)
[2025-02-13 19:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09][root][INFO] - Training Epoch: 1/2, step 4469/7134 completed (loss: 0.06890588998794556, acc: 0.9653465151786804)
[2025-02-13 19:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10][root][INFO] - Training Epoch: 1/2, step 4470/7134 completed (loss: 0.10093583166599274, acc: 0.9711538553237915)
[2025-02-13 19:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10][root][INFO] - Training Epoch: 1/2, step 4471/7134 completed (loss: 0.05916571989655495, acc: 0.9848484992980957)
[2025-02-13 19:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10][root][INFO] - Training Epoch: 1/2, step 4472/7134 completed (loss: 0.07563701272010803, acc: 0.9722222089767456)
[2025-02-13 19:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11][root][INFO] - Training Epoch: 1/2, step 4473/7134 completed (loss: 0.2305898517370224, acc: 0.9349112510681152)
[2025-02-13 19:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11][root][INFO] - Training Epoch: 1/2, step 4474/7134 completed (loss: 0.21983273327350616, acc: 0.9689440727233887)
[2025-02-13 19:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12][root][INFO] - Training Epoch: 1/2, step 4475/7134 completed (loss: 0.43395525217056274, acc: 0.8829787373542786)
[2025-02-13 19:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12][root][INFO] - Training Epoch: 1/2, step 4476/7134 completed (loss: 0.3227284252643585, acc: 0.9175823926925659)
[2025-02-13 19:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12][root][INFO] - Training Epoch: 1/2, step 4477/7134 completed (loss: 0.07921306788921356, acc: 0.9795918464660645)
[2025-02-13 19:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:13][root][INFO] - Training Epoch: 1/2, step 4478/7134 completed (loss: 0.14348946511745453, acc: 0.9581151604652405)
[2025-02-13 19:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:13][root][INFO] - Training Epoch: 1/2, step 4479/7134 completed (loss: 0.11853718012571335, acc: 0.9611111283302307)
[2025-02-13 19:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14][root][INFO] - Training Epoch: 1/2, step 4480/7134 completed (loss: 0.2653193771839142, acc: 0.9455445408821106)
[2025-02-13 19:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14][root][INFO] - Training Epoch: 1/2, step 4481/7134 completed (loss: 0.14619606733322144, acc: 0.9700000286102295)
[2025-02-13 19:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14][root][INFO] - Training Epoch: 1/2, step 4482/7134 completed (loss: 0.06663284450769424, acc: 0.9851484894752502)
[2025-02-13 19:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15][root][INFO] - Training Epoch: 1/2, step 4483/7134 completed (loss: 0.21587276458740234, acc: 0.9503105878829956)
[2025-02-13 19:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15][root][INFO] - Training Epoch: 1/2, step 4484/7134 completed (loss: 0.0966787114739418, acc: 0.9829545617103577)
[2025-02-13 19:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15][root][INFO] - Training Epoch: 1/2, step 4485/7134 completed (loss: 0.06835466623306274, acc: 0.9869281053543091)
[2025-02-13 19:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16][root][INFO] - Training Epoch: 1/2, step 4486/7134 completed (loss: 0.07099141925573349, acc: 0.9875776171684265)
[2025-02-13 19:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16][root][INFO] - Training Epoch: 1/2, step 4487/7134 completed (loss: 0.16100159287452698, acc: 0.9631901979446411)
[2025-02-13 19:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17][root][INFO] - Training Epoch: 1/2, step 4488/7134 completed (loss: 0.11929981410503387, acc: 0.9767441749572754)
[2025-02-13 19:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17][root][INFO] - Training Epoch: 1/2, step 4489/7134 completed (loss: 0.08946084976196289, acc: 0.9724137783050537)
[2025-02-13 19:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17][root][INFO] - Training Epoch: 1/2, step 4490/7134 completed (loss: 0.09467538446187973, acc: 0.9679999947547913)
[2025-02-13 19:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18][root][INFO] - Training Epoch: 1/2, step 4491/7134 completed (loss: 0.1009574830532074, acc: 0.9770992398262024)
[2025-02-13 19:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18][root][INFO] - Training Epoch: 1/2, step 4492/7134 completed (loss: 0.1289043426513672, acc: 0.9849624037742615)
[2025-02-13 19:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18][root][INFO] - Training Epoch: 1/2, step 4493/7134 completed (loss: 0.19436658918857574, acc: 0.9360465407371521)
[2025-02-13 19:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19][root][INFO] - Training Epoch: 1/2, step 4494/7134 completed (loss: 0.09253817051649094, acc: 0.9775280952453613)
[2025-02-13 19:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19][root][INFO] - Training Epoch: 1/2, step 4495/7134 completed (loss: 0.0706680566072464, acc: 0.9742268323898315)
[2025-02-13 19:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20][root][INFO] - Training Epoch: 1/2, step 4496/7134 completed (loss: 0.13503551483154297, acc: 0.9707602262496948)
[2025-02-13 19:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20][root][INFO] - Training Epoch: 1/2, step 4497/7134 completed (loss: 0.044597283005714417, acc: 0.9894179701805115)
[2025-02-13 19:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20][root][INFO] - Training Epoch: 1/2, step 4498/7134 completed (loss: 0.044850487262010574, acc: 0.9942196607589722)
[2025-02-13 19:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21][root][INFO] - Training Epoch: 1/2, step 4499/7134 completed (loss: 0.09619899839162827, acc: 0.978723406791687)
[2025-02-13 19:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21][root][INFO] - Training Epoch: 1/2, step 4500/7134 completed (loss: 0.08593399077653885, acc: 0.9893048405647278)
[2025-02-13 19:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21][root][INFO] - Training Epoch: 1/2, step 4501/7134 completed (loss: 0.06074251979589462, acc: 0.9779005646705627)
[2025-02-13 19:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:22][root][INFO] - Training Epoch: 1/2, step 4502/7134 completed (loss: 0.04219554737210274, acc: 0.9941176176071167)
[2025-02-13 19:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:22][root][INFO] - Training Epoch: 1/2, step 4503/7134 completed (loss: 0.05712360516190529, acc: 0.9811320900917053)
[2025-02-13 19:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23][root][INFO] - Training Epoch: 1/2, step 4504/7134 completed (loss: 0.2041805535554886, acc: 0.95652174949646)
[2025-02-13 19:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23][root][INFO] - Training Epoch: 1/2, step 4505/7134 completed (loss: 0.19174715876579285, acc: 0.9351351261138916)
[2025-02-13 19:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23][root][INFO] - Training Epoch: 1/2, step 4506/7134 completed (loss: 0.20588457584381104, acc: 0.9570552110671997)
[2025-02-13 19:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24][root][INFO] - Training Epoch: 1/2, step 4507/7134 completed (loss: 0.0820327028632164, acc: 0.9740259647369385)
[2025-02-13 19:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24][root][INFO] - Training Epoch: 1/2, step 4508/7134 completed (loss: 0.1779731661081314, acc: 0.9440993666648865)
[2025-02-13 19:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25][root][INFO] - Training Epoch: 1/2, step 4509/7134 completed (loss: 0.11484938859939575, acc: 0.9575757384300232)
[2025-02-13 19:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25][root][INFO] - Training Epoch: 1/2, step 4510/7134 completed (loss: 0.2147817611694336, acc: 0.9285714030265808)
[2025-02-13 19:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25][root][INFO] - Training Epoch: 1/2, step 4511/7134 completed (loss: 0.21889986097812653, acc: 0.9261363744735718)
[2025-02-13 19:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26][root][INFO] - Training Epoch: 1/2, step 4512/7134 completed (loss: 0.32252639532089233, acc: 0.9111111164093018)
[2025-02-13 19:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26][root][INFO] - Training Epoch: 1/2, step 4513/7134 completed (loss: 0.17103956639766693, acc: 0.9649122953414917)
[2025-02-13 19:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26][root][INFO] - Training Epoch: 1/2, step 4514/7134 completed (loss: 0.3628039062023163, acc: 0.9364162087440491)
[2025-02-13 19:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:27][root][INFO] - Training Epoch: 1/2, step 4515/7134 completed (loss: 0.2533666789531708, acc: 0.9473684430122375)
[2025-02-13 19:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:27][root][INFO] - Training Epoch: 1/2, step 4516/7134 completed (loss: 0.16208136081695557, acc: 0.9411764740943909)
[2025-02-13 19:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28][root][INFO] - Training Epoch: 1/2, step 4517/7134 completed (loss: 0.15342706441879272, acc: 0.9649999737739563)
[2025-02-13 19:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28][root][INFO] - Training Epoch: 1/2, step 4518/7134 completed (loss: 0.17848992347717285, acc: 0.9484536051750183)
[2025-02-13 19:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28][root][INFO] - Training Epoch: 1/2, step 4519/7134 completed (loss: 0.10577962547540665, acc: 0.9701492786407471)
[2025-02-13 19:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:29][root][INFO] - Training Epoch: 1/2, step 4520/7134 completed (loss: 0.1395978480577469, acc: 0.9572192430496216)
[2025-02-13 19:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:29][root][INFO] - Training Epoch: 1/2, step 4521/7134 completed (loss: 0.13933967053890228, acc: 0.9750000238418579)
[2025-02-13 19:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30][root][INFO] - Training Epoch: 1/2, step 4522/7134 completed (loss: 0.19026802480220795, acc: 0.9573459625244141)
[2025-02-13 19:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30][root][INFO] - Training Epoch: 1/2, step 4523/7134 completed (loss: 0.05772152170538902, acc: 0.98591548204422)
[2025-02-13 19:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30][root][INFO] - Training Epoch: 1/2, step 4524/7134 completed (loss: 0.2051527351140976, acc: 0.9319371581077576)
[2025-02-13 19:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31][root][INFO] - Training Epoch: 1/2, step 4525/7134 completed (loss: 0.19201534986495972, acc: 0.9605911374092102)
[2025-02-13 19:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31][root][INFO] - Training Epoch: 1/2, step 4526/7134 completed (loss: 0.23975150287151337, acc: 0.9450549483299255)
[2025-02-13 19:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31][root][INFO] - Training Epoch: 1/2, step 4527/7134 completed (loss: 0.2167738527059555, acc: 0.9372197389602661)
[2025-02-13 19:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32][root][INFO] - Training Epoch: 1/2, step 4528/7134 completed (loss: 0.21197400987148285, acc: 0.931034505367279)
[2025-02-13 19:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32][root][INFO] - Training Epoch: 1/2, step 4529/7134 completed (loss: 0.22168336808681488, acc: 0.9319371581077576)
[2025-02-13 19:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32][root][INFO] - Training Epoch: 1/2, step 4530/7134 completed (loss: 0.062087077647447586, acc: 0.9817073345184326)
[2025-02-13 19:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33][root][INFO] - Training Epoch: 1/2, step 4531/7134 completed (loss: 0.12569662928581238, acc: 0.953125)
[2025-02-13 19:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33][root][INFO] - Training Epoch: 1/2, step 4532/7134 completed (loss: 0.18932455778121948, acc: 0.9535865187644958)
[2025-02-13 19:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34][root][INFO] - Training Epoch: 1/2, step 4533/7134 completed (loss: 0.10948190093040466, acc: 0.9722222089767456)
[2025-02-13 19:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34][root][INFO] - Training Epoch: 1/2, step 4534/7134 completed (loss: 0.11201860755681992, acc: 0.9746192693710327)
[2025-02-13 19:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34][root][INFO] - Training Epoch: 1/2, step 4535/7134 completed (loss: 0.07411890476942062, acc: 0.9807692170143127)
[2025-02-13 19:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:35][root][INFO] - Training Epoch: 1/2, step 4536/7134 completed (loss: 0.07470925897359848, acc: 0.9879518151283264)
[2025-02-13 19:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:35][root][INFO] - Training Epoch: 1/2, step 4537/7134 completed (loss: 0.042953964322805405, acc: 0.9954751133918762)
[2025-02-13 19:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36][root][INFO] - Training Epoch: 1/2, step 4538/7134 completed (loss: 0.10589751601219177, acc: 0.9710144996643066)
[2025-02-13 19:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36][root][INFO] - Training Epoch: 1/2, step 4539/7134 completed (loss: 0.1223122775554657, acc: 0.9718309640884399)
[2025-02-13 19:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36][root][INFO] - Training Epoch: 1/2, step 4540/7134 completed (loss: 0.226710706949234, acc: 0.9226190447807312)
[2025-02-13 19:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37][root][INFO] - Training Epoch: 1/2, step 4541/7134 completed (loss: 0.2941126227378845, acc: 0.8934426307678223)
[2025-02-13 19:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37][root][INFO] - Training Epoch: 1/2, step 4542/7134 completed (loss: 0.29955190420150757, acc: 0.9122806787490845)
[2025-02-13 19:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37][root][INFO] - Training Epoch: 1/2, step 4543/7134 completed (loss: 0.3978256583213806, acc: 0.8896104097366333)
[2025-02-13 19:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:38][root][INFO] - Training Epoch: 1/2, step 4544/7134 completed (loss: 0.1472984254360199, acc: 0.9662162065505981)
[2025-02-13 19:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:38][root][INFO] - Training Epoch: 1/2, step 4545/7134 completed (loss: 0.14027853310108185, acc: 0.9629629850387573)
[2025-02-13 19:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39][root][INFO] - Training Epoch: 1/2, step 4546/7134 completed (loss: 0.14413712918758392, acc: 0.9685039520263672)
[2025-02-13 19:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39][root][INFO] - Training Epoch: 1/2, step 4547/7134 completed (loss: 0.11976375430822372, acc: 0.9661017060279846)
[2025-02-13 19:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39][root][INFO] - Training Epoch: 1/2, step 4548/7134 completed (loss: 0.2589428722858429, acc: 0.9370078444480896)
[2025-02-13 19:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:40][root][INFO] - Training Epoch: 1/2, step 4549/7134 completed (loss: 0.24241787195205688, acc: 0.9210526347160339)
[2025-02-13 19:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:40][root][INFO] - Training Epoch: 1/2, step 4550/7134 completed (loss: 0.19565266370773315, acc: 0.9548872113227844)
[2025-02-13 19:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41][root][INFO] - Training Epoch: 1/2, step 4551/7134 completed (loss: 0.30273643136024475, acc: 0.9391891956329346)
[2025-02-13 19:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41][root][INFO] - Training Epoch: 1/2, step 4552/7134 completed (loss: 0.21258427202701569, acc: 0.9561403393745422)
[2025-02-13 19:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41][root][INFO] - Training Epoch: 1/2, step 4553/7134 completed (loss: 0.13786420226097107, acc: 0.9696969985961914)
[2025-02-13 19:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42][root][INFO] - Training Epoch: 1/2, step 4554/7134 completed (loss: 0.1917608380317688, acc: 0.970802903175354)
[2025-02-13 19:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42][root][INFO] - Training Epoch: 1/2, step 4555/7134 completed (loss: 0.31394970417022705, acc: 0.9270833134651184)
[2025-02-13 19:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42][root][INFO] - Training Epoch: 1/2, step 4556/7134 completed (loss: 0.35922786593437195, acc: 0.8838709592819214)
[2025-02-13 19:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43][root][INFO] - Training Epoch: 1/2, step 4557/7134 completed (loss: 0.09635046869516373, acc: 0.9701492786407471)
[2025-02-13 19:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43][root][INFO] - Training Epoch: 1/2, step 4558/7134 completed (loss: 0.22353176772594452, acc: 0.9379844665527344)
[2025-02-13 19:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43][root][INFO] - Training Epoch: 1/2, step 4559/7134 completed (loss: 0.3259549140930176, acc: 0.9090909361839294)
[2025-02-13 19:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44][root][INFO] - Training Epoch: 1/2, step 4560/7134 completed (loss: 0.14934521913528442, acc: 0.9677419066429138)
[2025-02-13 19:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44][root][INFO] - Training Epoch: 1/2, step 4561/7134 completed (loss: 0.20604407787322998, acc: 0.9508196711540222)
[2025-02-13 19:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44][root][INFO] - Training Epoch: 1/2, step 4562/7134 completed (loss: 0.23288452625274658, acc: 0.9512194991111755)
[2025-02-13 19:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45][root][INFO] - Training Epoch: 1/2, step 4563/7134 completed (loss: 0.12762808799743652, acc: 0.9645389914512634)
[2025-02-13 19:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45][root][INFO] - Training Epoch: 1/2, step 4564/7134 completed (loss: 0.322329044342041, acc: 0.9285714030265808)
[2025-02-13 19:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46][root][INFO] - Training Epoch: 1/2, step 4565/7134 completed (loss: 0.24104304611682892, acc: 0.9380530714988708)
[2025-02-13 19:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46][root][INFO] - Training Epoch: 1/2, step 4566/7134 completed (loss: 0.21827641129493713, acc: 0.932330846786499)
[2025-02-13 19:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46][root][INFO] - Training Epoch: 1/2, step 4567/7134 completed (loss: 0.05816720053553581, acc: 0.9909909963607788)
[2025-02-13 19:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47][root][INFO] - Training Epoch: 1/2, step 4568/7134 completed (loss: 0.3541954755783081, acc: 0.9684210419654846)
[2025-02-13 19:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47][root][INFO] - Training Epoch: 1/2, step 4569/7134 completed (loss: 0.1260686069726944, acc: 0.9646017551422119)
[2025-02-13 19:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47][root][INFO] - Training Epoch: 1/2, step 4570/7134 completed (loss: 0.1356026828289032, acc: 0.9610389471054077)
[2025-02-13 19:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:48][root][INFO] - Training Epoch: 1/2, step 4571/7134 completed (loss: 0.190032497048378, acc: 0.9482758641242981)
[2025-02-13 19:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:48][root][INFO] - Training Epoch: 1/2, step 4572/7134 completed (loss: 0.10552670061588287, acc: 0.9802631735801697)
[2025-02-13 19:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49][root][INFO] - Training Epoch: 1/2, step 4573/7134 completed (loss: 0.14104042947292328, acc: 0.9615384340286255)
[2025-02-13 19:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49][root][INFO] - Training Epoch: 1/2, step 4574/7134 completed (loss: 0.08543241024017334, acc: 0.9848484992980957)
[2025-02-13 19:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49][root][INFO] - Training Epoch: 1/2, step 4575/7134 completed (loss: 0.1192697063088417, acc: 0.9779005646705627)
[2025-02-13 19:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:50][root][INFO] - Training Epoch: 1/2, step 4576/7134 completed (loss: 0.07674626260995865, acc: 0.9885714054107666)
[2025-02-13 19:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:50][root][INFO] - Training Epoch: 1/2, step 4577/7134 completed (loss: 0.22287794947624207, acc: 0.9360465407371521)
[2025-02-13 19:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51][root][INFO] - Training Epoch: 1/2, step 4578/7134 completed (loss: 0.07347950339317322, acc: 0.9818181991577148)
[2025-02-13 19:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51][root][INFO] - Training Epoch: 1/2, step 4579/7134 completed (loss: 0.19386060535907745, acc: 0.9683544039726257)
[2025-02-13 19:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51][root][INFO] - Training Epoch: 1/2, step 4580/7134 completed (loss: 0.12775373458862305, acc: 0.9620253443717957)
[2025-02-13 19:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52][root][INFO] - Training Epoch: 1/2, step 4581/7134 completed (loss: 0.03857055678963661, acc: 0.9860140085220337)
[2025-02-13 19:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52][root][INFO] - Training Epoch: 1/2, step 4582/7134 completed (loss: 0.12930448353290558, acc: 0.9583333134651184)
[2025-02-13 19:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52][root][INFO] - Training Epoch: 1/2, step 4583/7134 completed (loss: 0.21788115799427032, acc: 0.9599999785423279)
[2025-02-13 19:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:53][root][INFO] - Training Epoch: 1/2, step 4584/7134 completed (loss: 0.1328166276216507, acc: 0.9655172228813171)
[2025-02-13 19:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:53][root][INFO] - Training Epoch: 1/2, step 4585/7134 completed (loss: 0.037566330283880234, acc: 0.9935897588729858)
[2025-02-13 19:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54][root][INFO] - Training Epoch: 1/2, step 4586/7134 completed (loss: 0.11509159207344055, acc: 0.976331353187561)
[2025-02-13 19:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54][root][INFO] - Training Epoch: 1/2, step 4587/7134 completed (loss: 0.29891693592071533, acc: 0.9418604373931885)
[2025-02-13 19:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54][root][INFO] - Training Epoch: 1/2, step 4588/7134 completed (loss: 0.16357925534248352, acc: 0.949999988079071)
[2025-02-13 19:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55][root][INFO] - Training Epoch: 1/2, step 4589/7134 completed (loss: 0.15100733935832977, acc: 0.9712643623352051)
[2025-02-13 19:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55][root][INFO] - Training Epoch: 1/2, step 4590/7134 completed (loss: 0.2354062795639038, acc: 0.9285714030265808)
[2025-02-13 19:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56][root][INFO] - Training Epoch: 1/2, step 4591/7134 completed (loss: 0.21714839339256287, acc: 0.9602272510528564)
[2025-02-13 19:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56][root][INFO] - Training Epoch: 1/2, step 4592/7134 completed (loss: 0.1845896989107132, acc: 0.9634146094322205)
[2025-02-13 19:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56][root][INFO] - Training Epoch: 1/2, step 4593/7134 completed (loss: 0.23844817280769348, acc: 0.9210526347160339)
[2025-02-13 19:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57][root][INFO] - Training Epoch: 1/2, step 4594/7134 completed (loss: 0.3065798580646515, acc: 0.8994709253311157)
[2025-02-13 19:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57][root][INFO] - Training Epoch: 1/2, step 4595/7134 completed (loss: 0.17128033936023712, acc: 0.949367105960846)
[2025-02-13 19:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57][root][INFO] - Training Epoch: 1/2, step 4596/7134 completed (loss: 0.1824178248643875, acc: 0.9375)
[2025-02-13 19:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:58][root][INFO] - Training Epoch: 1/2, step 4597/7134 completed (loss: 0.18341763317584991, acc: 0.9487179517745972)
[2025-02-13 19:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:58][root][INFO] - Training Epoch: 1/2, step 4598/7134 completed (loss: 0.27438443899154663, acc: 0.9351351261138916)
[2025-02-13 19:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59][root][INFO] - Training Epoch: 1/2, step 4599/7134 completed (loss: 0.0507216677069664, acc: 0.9855072498321533)
[2025-02-13 19:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59][root][INFO] - Training Epoch: 1/2, step 4600/7134 completed (loss: 0.13433438539505005, acc: 0.9685534834861755)
[2025-02-13 19:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59][root][INFO] - Training Epoch: 1/2, step 4601/7134 completed (loss: 0.11112365871667862, acc: 0.9661017060279846)
[2025-02-13 19:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00][root][INFO] - Training Epoch: 1/2, step 4602/7134 completed (loss: 0.16674554347991943, acc: 0.9555555582046509)
[2025-02-13 19:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00][root][INFO] - Training Epoch: 1/2, step 4603/7134 completed (loss: 0.12514172494411469, acc: 0.9651162624359131)
[2025-02-13 19:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01][root][INFO] - Training Epoch: 1/2, step 4604/7134 completed (loss: 0.128190815448761, acc: 0.9650349617004395)
[2025-02-13 19:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01][root][INFO] - Training Epoch: 1/2, step 4605/7134 completed (loss: 0.18874536454677582, acc: 0.9363636374473572)
[2025-02-13 19:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01][root][INFO] - Training Epoch: 1/2, step 4606/7134 completed (loss: 0.13470284640789032, acc: 0.9622641801834106)
[2025-02-13 19:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02][root][INFO] - Training Epoch: 1/2, step 4607/7134 completed (loss: 0.15234938263893127, acc: 0.957446813583374)
[2025-02-13 19:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02][root][INFO] - Training Epoch: 1/2, step 4608/7134 completed (loss: 0.11743083596229553, acc: 0.9824561476707458)
[2025-02-13 19:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03][root][INFO] - Training Epoch: 1/2, step 4609/7134 completed (loss: 0.11064133048057556, acc: 0.9694656729698181)
[2025-02-13 19:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03][root][INFO] - Training Epoch: 1/2, step 4610/7134 completed (loss: 0.06503460556268692, acc: 0.9832402467727661)
[2025-02-13 19:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03][root][INFO] - Training Epoch: 1/2, step 4611/7134 completed (loss: 0.07622319459915161, acc: 0.9931034445762634)
[2025-02-13 19:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:04][root][INFO] - Training Epoch: 1/2, step 4612/7134 completed (loss: 0.15355278551578522, acc: 0.9629629850387573)
[2025-02-13 19:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:04][root][INFO] - Training Epoch: 1/2, step 4613/7134 completed (loss: 0.1370965838432312, acc: 0.9814814925193787)
[2025-02-13 19:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05][root][INFO] - Training Epoch: 1/2, step 4614/7134 completed (loss: 0.13651680946350098, acc: 0.9461538195610046)
[2025-02-13 19:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05][root][INFO] - Training Epoch: 1/2, step 4615/7134 completed (loss: 0.20832687616348267, acc: 0.9341317415237427)
[2025-02-13 19:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05][root][INFO] - Training Epoch: 1/2, step 4616/7134 completed (loss: 0.23805268108844757, acc: 0.9186046719551086)
[2025-02-13 19:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:06][root][INFO] - Training Epoch: 1/2, step 4617/7134 completed (loss: 0.470746248960495, acc: 0.9041916131973267)
[2025-02-13 19:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:06][root][INFO] - Training Epoch: 1/2, step 4618/7134 completed (loss: 0.2845272123813629, acc: 0.9069767594337463)
[2025-02-13 19:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07][root][INFO] - Training Epoch: 1/2, step 4619/7134 completed (loss: 0.2207147181034088, acc: 0.9624999761581421)
[2025-02-13 19:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07][root][INFO] - Training Epoch: 1/2, step 4620/7134 completed (loss: 0.19289082288742065, acc: 0.9823529124259949)
[2025-02-13 19:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07][root][INFO] - Training Epoch: 1/2, step 4621/7134 completed (loss: 0.08336217701435089, acc: 0.9878048896789551)
[2025-02-13 19:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08][root][INFO] - Training Epoch: 1/2, step 4622/7134 completed (loss: 0.09439413994550705, acc: 0.9722222089767456)
[2025-02-13 19:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08][root][INFO] - Training Epoch: 1/2, step 4623/7134 completed (loss: 0.17001712322235107, acc: 0.970588207244873)
[2025-02-13 19:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09][root][INFO] - Training Epoch: 1/2, step 4624/7134 completed (loss: 0.28179389238357544, acc: 0.9235293865203857)
[2025-02-13 19:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09][root][INFO] - Training Epoch: 1/2, step 4625/7134 completed (loss: 0.41952359676361084, acc: 0.8588957190513611)
[2025-02-13 19:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09][root][INFO] - Training Epoch: 1/2, step 4626/7134 completed (loss: 0.30992409586906433, acc: 0.9130434989929199)
[2025-02-13 19:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10][root][INFO] - Training Epoch: 1/2, step 4627/7134 completed (loss: 0.2044731229543686, acc: 0.948387086391449)
[2025-02-13 19:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10][root][INFO] - Training Epoch: 1/2, step 4628/7134 completed (loss: 0.16457761824131012, acc: 0.9634146094322205)
[2025-02-13 19:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11][root][INFO] - Training Epoch: 1/2, step 4629/7134 completed (loss: 0.20388665795326233, acc: 0.9220778942108154)
[2025-02-13 19:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11][root][INFO] - Training Epoch: 1/2, step 4630/7134 completed (loss: 0.1539074182510376, acc: 0.9722222089767456)
[2025-02-13 19:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11][root][INFO] - Training Epoch: 1/2, step 4631/7134 completed (loss: 0.14848195016384125, acc: 0.9674418568611145)
[2025-02-13 19:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12][root][INFO] - Training Epoch: 1/2, step 4632/7134 completed (loss: 0.1890697479248047, acc: 0.9698795080184937)
[2025-02-13 19:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12][root][INFO] - Training Epoch: 1/2, step 4633/7134 completed (loss: 0.28876084089279175, acc: 0.931506872177124)
[2025-02-13 19:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12][root][INFO] - Training Epoch: 1/2, step 4634/7134 completed (loss: 0.13473708927631378, acc: 0.9629629850387573)
[2025-02-13 19:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13][root][INFO] - Training Epoch: 1/2, step 4635/7134 completed (loss: 0.2244093418121338, acc: 0.9817073345184326)
[2025-02-13 19:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13][root][INFO] - Training Epoch: 1/2, step 4636/7134 completed (loss: 0.11939525604248047, acc: 0.9680851101875305)
[2025-02-13 19:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14][root][INFO] - Training Epoch: 1/2, step 4637/7134 completed (loss: 0.2042054533958435, acc: 0.9682539701461792)
[2025-02-13 19:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14][root][INFO] - Training Epoch: 1/2, step 4638/7134 completed (loss: 0.23812498152256012, acc: 0.9595375657081604)
[2025-02-13 19:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14][root][INFO] - Training Epoch: 1/2, step 4639/7134 completed (loss: 0.1571480631828308, acc: 0.9675675630569458)
[2025-02-13 19:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:15][root][INFO] - Training Epoch: 1/2, step 4640/7134 completed (loss: 0.18690912425518036, acc: 0.9523809552192688)
[2025-02-13 19:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:15][root][INFO] - Training Epoch: 1/2, step 4641/7134 completed (loss: 0.1565527617931366, acc: 0.9791666865348816)
[2025-02-13 19:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16][root][INFO] - Training Epoch: 1/2, step 4642/7134 completed (loss: 0.0839894562959671, acc: 0.9717513918876648)
[2025-02-13 19:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16][root][INFO] - Training Epoch: 1/2, step 4643/7134 completed (loss: 0.1007114052772522, acc: 0.9658536314964294)
[2025-02-13 19:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16][root][INFO] - Training Epoch: 1/2, step 4644/7134 completed (loss: 0.09172981977462769, acc: 0.9704433679580688)
[2025-02-13 19:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17][root][INFO] - Training Epoch: 1/2, step 4645/7134 completed (loss: 0.04110347479581833, acc: 0.9939024448394775)
[2025-02-13 19:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17][root][INFO] - Training Epoch: 1/2, step 4646/7134 completed (loss: 0.1541949361562729, acc: 0.9571428298950195)
[2025-02-13 19:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17][root][INFO] - Training Epoch: 1/2, step 4647/7134 completed (loss: 0.25587111711502075, acc: 0.9453551769256592)
[2025-02-13 19:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18][root][INFO] - Training Epoch: 1/2, step 4648/7134 completed (loss: 0.25644880533218384, acc: 0.9554139971733093)
[2025-02-13 19:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18][root][INFO] - Training Epoch: 1/2, step 4649/7134 completed (loss: 0.2101420909166336, acc: 0.9512194991111755)
[2025-02-13 19:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19][root][INFO] - Training Epoch: 1/2, step 4650/7134 completed (loss: 0.20489639043807983, acc: 0.9391891956329346)
[2025-02-13 19:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19][root][INFO] - Training Epoch: 1/2, step 4651/7134 completed (loss: 0.2795332372188568, acc: 0.9497206807136536)
[2025-02-13 19:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19][root][INFO] - Training Epoch: 1/2, step 4652/7134 completed (loss: 0.13338539004325867, acc: 0.9567901492118835)
[2025-02-13 19:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20][root][INFO] - Training Epoch: 1/2, step 4653/7134 completed (loss: 0.12515124678611755, acc: 0.9710144996643066)
[2025-02-13 19:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20][root][INFO] - Training Epoch: 1/2, step 4654/7134 completed (loss: 0.2790561318397522, acc: 0.9243243336677551)
[2025-02-13 19:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20][root][INFO] - Training Epoch: 1/2, step 4655/7134 completed (loss: 0.3234635591506958, acc: 0.9485714435577393)
[2025-02-13 19:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21][root][INFO] - Training Epoch: 1/2, step 4656/7134 completed (loss: 0.16583214700222015, acc: 0.9576271176338196)
[2025-02-13 19:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21][root][INFO] - Training Epoch: 1/2, step 4657/7134 completed (loss: 0.20899562537670135, acc: 0.931034505367279)
[2025-02-13 19:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22][root][INFO] - Training Epoch: 1/2, step 4658/7134 completed (loss: 0.18055123090744019, acc: 0.9375)
[2025-02-13 19:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22][root][INFO] - Training Epoch: 1/2, step 4659/7134 completed (loss: 0.2310156673192978, acc: 0.9312169551849365)
[2025-02-13 19:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22][root][INFO] - Training Epoch: 1/2, step 4660/7134 completed (loss: 0.22563469409942627, acc: 0.942307710647583)
[2025-02-13 19:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23][root][INFO] - Training Epoch: 1/2, step 4661/7134 completed (loss: 0.1863134801387787, acc: 0.9325153231620789)
[2025-02-13 19:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23][root][INFO] - Training Epoch: 1/2, step 4662/7134 completed (loss: 0.288095086812973, acc: 0.9290780425071716)
[2025-02-13 19:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23][root][INFO] - Training Epoch: 1/2, step 4663/7134 completed (loss: 0.30256757140159607, acc: 0.9180327653884888)
[2025-02-13 19:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:24][root][INFO] - Training Epoch: 1/2, step 4664/7134 completed (loss: 0.20585569739341736, acc: 0.9675675630569458)
[2025-02-13 19:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:24][root][INFO] - Training Epoch: 1/2, step 4665/7134 completed (loss: 0.13276813924312592, acc: 0.9505494236946106)
[2025-02-13 19:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25][root][INFO] - Training Epoch: 1/2, step 4666/7134 completed (loss: 0.35918769240379333, acc: 0.9274611473083496)
[2025-02-13 19:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25][root][INFO] - Training Epoch: 1/2, step 4667/7134 completed (loss: 0.40985602140426636, acc: 0.912162184715271)
[2025-02-13 19:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25][root][INFO] - Training Epoch: 1/2, step 4668/7134 completed (loss: 0.15018805861473083, acc: 0.9664429426193237)
[2025-02-13 19:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:26][root][INFO] - Training Epoch: 1/2, step 4669/7134 completed (loss: 0.16713136434555054, acc: 0.9661017060279846)
[2025-02-13 19:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:26][root][INFO] - Training Epoch: 1/2, step 4670/7134 completed (loss: 0.1627798229455948, acc: 0.9701492786407471)
[2025-02-13 19:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27][root][INFO] - Training Epoch: 1/2, step 4671/7134 completed (loss: 0.13359026610851288, acc: 0.9617486596107483)
[2025-02-13 19:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27][root][INFO] - Training Epoch: 1/2, step 4672/7134 completed (loss: 0.09899110347032547, acc: 0.9724137783050537)
[2025-02-13 19:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27][root][INFO] - Training Epoch: 1/2, step 4673/7134 completed (loss: 0.17601357400417328, acc: 0.9622641801834106)
[2025-02-13 19:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:28][root][INFO] - Training Epoch: 1/2, step 4674/7134 completed (loss: 0.1221187487244606, acc: 0.9610389471054077)
[2025-02-13 19:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:28][root][INFO] - Training Epoch: 1/2, step 4675/7134 completed (loss: 0.2057400494813919, acc: 0.9538461565971375)
[2025-02-13 19:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29][root][INFO] - Training Epoch: 1/2, step 4676/7134 completed (loss: 0.09877283126115799, acc: 0.9805194735527039)
[2025-02-13 19:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29][root][INFO] - Training Epoch: 1/2, step 4677/7134 completed (loss: 0.1325630396604538, acc: 0.9663865566253662)
[2025-02-13 19:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29][root][INFO] - Training Epoch: 1/2, step 4678/7134 completed (loss: 0.21948444843292236, acc: 0.9580838084220886)
[2025-02-13 19:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30][root][INFO] - Training Epoch: 1/2, step 4679/7134 completed (loss: 0.10119088739156723, acc: 0.9884393215179443)
[2025-02-13 19:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30][root][INFO] - Training Epoch: 1/2, step 4680/7134 completed (loss: 0.24604758620262146, acc: 0.9363636374473572)
[2025-02-13 19:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30][root][INFO] - Training Epoch: 1/2, step 4681/7134 completed (loss: 0.2684178650379181, acc: 0.9415584206581116)
[2025-02-13 19:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31][root][INFO] - Training Epoch: 1/2, step 4682/7134 completed (loss: 0.15089067816734314, acc: 0.9668508172035217)
[2025-02-13 19:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31][root][INFO] - Training Epoch: 1/2, step 4683/7134 completed (loss: 0.1929926872253418, acc: 0.95333331823349)
[2025-02-13 19:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32][root][INFO] - Training Epoch: 1/2, step 4684/7134 completed (loss: 0.2489921748638153, acc: 0.9536082744598389)
[2025-02-13 19:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32][root][INFO] - Training Epoch: 1/2, step 4685/7134 completed (loss: 0.10560666769742966, acc: 0.9802955389022827)
[2025-02-13 19:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32][root][INFO] - Training Epoch: 1/2, step 4686/7134 completed (loss: 0.14139877259731293, acc: 0.9629629850387573)
[2025-02-13 19:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33][root][INFO] - Training Epoch: 1/2, step 4687/7134 completed (loss: 0.5490275025367737, acc: 0.8478260636329651)
[2025-02-13 19:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33][root][INFO] - Training Epoch: 1/2, step 4688/7134 completed (loss: 0.7792466282844543, acc: 0.875)
[2025-02-13 19:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33][root][INFO] - Training Epoch: 1/2, step 4689/7134 completed (loss: 0.39675450325012207, acc: 0.8965517282485962)
[2025-02-13 19:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34][root][INFO] - Training Epoch: 1/2, step 4690/7134 completed (loss: 0.44508859515190125, acc: 0.9035087823867798)
[2025-02-13 19:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34][root][INFO] - Training Epoch: 1/2, step 4691/7134 completed (loss: 0.19760024547576904, acc: 0.9538461565971375)
[2025-02-13 19:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35][root][INFO] - Training Epoch: 1/2, step 4692/7134 completed (loss: 0.7870548963546753, acc: 0.817460298538208)
[2025-02-13 19:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35][root][INFO] - Training Epoch: 1/2, step 4693/7134 completed (loss: 0.6596878170967102, acc: 0.8571428656578064)
[2025-02-13 19:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35][root][INFO] - Training Epoch: 1/2, step 4694/7134 completed (loss: 0.579188883304596, acc: 0.8571428656578064)
[2025-02-13 19:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36][root][INFO] - Training Epoch: 1/2, step 4695/7134 completed (loss: 0.4212992787361145, acc: 0.9133333563804626)
[2025-02-13 19:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36][root][INFO] - Training Epoch: 1/2, step 4696/7134 completed (loss: 0.10858558863401413, acc: 0.9757575988769531)
[2025-02-13 19:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37][root][INFO] - Training Epoch: 1/2, step 4697/7134 completed (loss: 0.1609894037246704, acc: 0.9682539701461792)
[2025-02-13 19:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37][root][INFO] - Training Epoch: 1/2, step 4698/7134 completed (loss: 0.5192720293998718, acc: 0.8857142925262451)
[2025-02-13 19:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37][root][INFO] - Training Epoch: 1/2, step 4699/7134 completed (loss: 0.4607129395008087, acc: 0.8920863270759583)
[2025-02-13 19:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38][root][INFO] - Training Epoch: 1/2, step 4700/7134 completed (loss: 0.24242906272411346, acc: 0.9467455744743347)
[2025-02-13 19:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38][root][INFO] - Training Epoch: 1/2, step 4701/7134 completed (loss: 0.6787872910499573, acc: 0.8500000238418579)
[2025-02-13 19:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38][root][INFO] - Training Epoch: 1/2, step 4702/7134 completed (loss: 0.15973398089408875, acc: 0.9548386931419373)
[2025-02-13 19:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39][root][INFO] - Training Epoch: 1/2, step 4703/7134 completed (loss: 0.12303964048624039, acc: 0.970588207244873)
[2025-02-13 19:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39][root][INFO] - Training Epoch: 1/2, step 4704/7134 completed (loss: 0.1217455342411995, acc: 0.964102566242218)
[2025-02-13 19:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39][root][INFO] - Training Epoch: 1/2, step 4705/7134 completed (loss: 0.2435709834098816, acc: 0.9408866763114929)
[2025-02-13 19:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:40][root][INFO] - Training Epoch: 1/2, step 4706/7134 completed (loss: 0.3128468692302704, acc: 0.9230769276618958)
[2025-02-13 19:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:40][root][INFO] - Training Epoch: 1/2, step 4707/7134 completed (loss: 0.703307032585144, acc: 0.8805031180381775)
[2025-02-13 19:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41][root][INFO] - Training Epoch: 1/2, step 4708/7134 completed (loss: 0.1774713099002838, acc: 0.9597989916801453)
[2025-02-13 19:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41][root][INFO] - Training Epoch: 1/2, step 4709/7134 completed (loss: 0.13763290643692017, acc: 0.9675675630569458)
[2025-02-13 19:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41][root][INFO] - Training Epoch: 1/2, step 4710/7134 completed (loss: 0.18094931542873383, acc: 0.9515151381492615)
[2025-02-13 19:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42][root][INFO] - Training Epoch: 1/2, step 4711/7134 completed (loss: 0.22106032073497772, acc: 0.9459459185600281)
[2025-02-13 19:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42][root][INFO] - Training Epoch: 1/2, step 4712/7134 completed (loss: 0.5116567611694336, acc: 0.9019607901573181)
[2025-02-13 19:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42][root][INFO] - Training Epoch: 1/2, step 4713/7134 completed (loss: 0.18489019572734833, acc: 0.9402984976768494)
[2025-02-13 19:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43][root][INFO] - Training Epoch: 1/2, step 4714/7134 completed (loss: 0.3457225263118744, acc: 0.9541984796524048)
[2025-02-13 19:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43][root][INFO] - Training Epoch: 1/2, step 4715/7134 completed (loss: 0.41296470165252686, acc: 0.9416666626930237)
[2025-02-13 19:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43][root][INFO] - Training Epoch: 1/2, step 4716/7134 completed (loss: 0.15625396370887756, acc: 0.9612902998924255)
[2025-02-13 19:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:44][root][INFO] - Training Epoch: 1/2, step 4717/7134 completed (loss: 0.11115410178899765, acc: 0.9733333587646484)
[2025-02-13 19:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:44][root][INFO] - Training Epoch: 1/2, step 4718/7134 completed (loss: 0.09077870100736618, acc: 0.9784172773361206)
[2025-02-13 19:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45][root][INFO] - Training Epoch: 1/2, step 4719/7134 completed (loss: 0.11554406583309174, acc: 0.9704142212867737)
[2025-02-13 19:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45][root][INFO] - Training Epoch: 1/2, step 4720/7134 completed (loss: 0.04813940450549126, acc: 0.987500011920929)
[2025-02-13 19:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45][root][INFO] - Training Epoch: 1/2, step 4721/7134 completed (loss: 0.036948129534721375, acc: 0.9938271641731262)
[2025-02-13 19:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46][root][INFO] - Training Epoch: 1/2, step 4722/7134 completed (loss: 0.2511463165283203, acc: 0.955974817276001)
[2025-02-13 19:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46][root][INFO] - Training Epoch: 1/2, step 4723/7134 completed (loss: 0.1348009705543518, acc: 0.9745222926139832)
[2025-02-13 19:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46][root][INFO] - Training Epoch: 1/2, step 4724/7134 completed (loss: 0.1574372500181198, acc: 0.9390243887901306)
[2025-02-13 19:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47][root][INFO] - Training Epoch: 1/2, step 4725/7134 completed (loss: 0.25321295857429504, acc: 0.9127907156944275)
[2025-02-13 19:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47][root][INFO] - Training Epoch: 1/2, step 4726/7134 completed (loss: 0.35705816745758057, acc: 0.8965517282485962)
[2025-02-13 19:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48][root][INFO] - Training Epoch: 1/2, step 4727/7134 completed (loss: 0.1986340880393982, acc: 0.9536423683166504)
[2025-02-13 19:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48][root][INFO] - Training Epoch: 1/2, step 4728/7134 completed (loss: 0.19162781536579132, acc: 0.9271523356437683)
[2025-02-13 19:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48][root][INFO] - Training Epoch: 1/2, step 4729/7134 completed (loss: 0.12858819961547852, acc: 0.9710982441902161)
[2025-02-13 19:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49][root][INFO] - Training Epoch: 1/2, step 4730/7134 completed (loss: 0.20422565937042236, acc: 0.9415204524993896)
[2025-02-13 19:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49][root][INFO] - Training Epoch: 1/2, step 4731/7134 completed (loss: 0.31565093994140625, acc: 0.9448275566101074)
[2025-02-13 19:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49][root][INFO] - Training Epoch: 1/2, step 4732/7134 completed (loss: 0.23327897489070892, acc: 0.9411764740943909)
[2025-02-13 19:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:50][root][INFO] - Training Epoch: 1/2, step 4733/7134 completed (loss: 0.21647605299949646, acc: 0.9492753744125366)
[2025-02-13 19:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:50][root][INFO] - Training Epoch: 1/2, step 4734/7134 completed (loss: 0.3834821283817291, acc: 0.9119496941566467)
[2025-02-13 19:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51][root][INFO] - Training Epoch: 1/2, step 4735/7134 completed (loss: 0.21264494955539703, acc: 0.955974817276001)
[2025-02-13 19:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51][root][INFO] - Training Epoch: 1/2, step 4736/7134 completed (loss: 0.34339407086372375, acc: 0.925000011920929)
[2025-02-13 19:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51][root][INFO] - Training Epoch: 1/2, step 4737/7134 completed (loss: 0.23769035935401917, acc: 0.931034505367279)
[2025-02-13 19:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:52][root][INFO] - Training Epoch: 1/2, step 4738/7134 completed (loss: 0.1931716799736023, acc: 0.956250011920929)
[2025-02-13 19:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:52][root][INFO] - Training Epoch: 1/2, step 4739/7134 completed (loss: 0.36992985010147095, acc: 0.9044944047927856)
[2025-02-13 19:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53][root][INFO] - Training Epoch: 1/2, step 4740/7134 completed (loss: 0.3598502576351166, acc: 0.9333333373069763)
[2025-02-13 19:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53][root][INFO] - Training Epoch: 1/2, step 4741/7134 completed (loss: 0.21769006550312042, acc: 0.9357143044471741)
[2025-02-13 19:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53][root][INFO] - Training Epoch: 1/2, step 4742/7134 completed (loss: 0.5674933195114136, acc: 0.8909090757369995)
[2025-02-13 19:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54][root][INFO] - Training Epoch: 1/2, step 4743/7134 completed (loss: 0.35305356979370117, acc: 0.9144737124443054)
[2025-02-13 19:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54][root][INFO] - Training Epoch: 1/2, step 4744/7134 completed (loss: 0.1257087141275406, acc: 0.9724770784378052)
[2025-02-13 19:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54][root][INFO] - Training Epoch: 1/2, step 4745/7134 completed (loss: 0.31674158573150635, acc: 0.9444444179534912)
[2025-02-13 19:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55][root][INFO] - Training Epoch: 1/2, step 4746/7134 completed (loss: 0.15831170976161957, acc: 0.9844961166381836)
[2025-02-13 19:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55][root][INFO] - Training Epoch: 1/2, step 4747/7134 completed (loss: 0.16717787086963654, acc: 0.9651162624359131)
[2025-02-13 19:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55][root][INFO] - Training Epoch: 1/2, step 4748/7134 completed (loss: 0.3465459942817688, acc: 0.9171597361564636)
[2025-02-13 19:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56][root][INFO] - Training Epoch: 1/2, step 4749/7134 completed (loss: 0.09904993325471878, acc: 0.9651162624359131)
[2025-02-13 19:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56][root][INFO] - Training Epoch: 1/2, step 4750/7134 completed (loss: 0.1390712857246399, acc: 0.9674796462059021)
[2025-02-13 19:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57][root][INFO] - Training Epoch: 1/2, step 4751/7134 completed (loss: 0.1117331013083458, acc: 0.9634146094322205)
[2025-02-13 19:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57][root][INFO] - Training Epoch: 1/2, step 4752/7134 completed (loss: 0.25321561098098755, acc: 0.9378882050514221)
[2025-02-13 19:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57][root][INFO] - Training Epoch: 1/2, step 4753/7134 completed (loss: 0.32631734013557434, acc: 0.9349112510681152)
[2025-02-13 19:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58][root][INFO] - Training Epoch: 1/2, step 4754/7134 completed (loss: 0.049021393060684204, acc: 0.9935064911842346)
[2025-02-13 19:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58][root][INFO] - Training Epoch: 1/2, step 4755/7134 completed (loss: 0.23937149345874786, acc: 0.9615384340286255)
[2025-02-13 19:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58][root][INFO] - Training Epoch: 1/2, step 4756/7134 completed (loss: 0.2989940941333771, acc: 0.9354838728904724)
[2025-02-13 19:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59][root][INFO] - Training Epoch: 1/2, step 4757/7134 completed (loss: 0.11006499081850052, acc: 0.9599999785423279)
[2025-02-13 19:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59][root][INFO] - Training Epoch: 1/2, step 4758/7134 completed (loss: 0.09348241239786148, acc: 0.988950252532959)
[2025-02-13 19:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00][root][INFO] - Training Epoch: 1/2, step 4759/7134 completed (loss: 0.06133081018924713, acc: 0.978723406791687)
[2025-02-13 19:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00][root][INFO] - Training Epoch: 1/2, step 4760/7134 completed (loss: 0.12333137542009354, acc: 0.9663865566253662)
[2025-02-13 19:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00][root][INFO] - Training Epoch: 1/2, step 4761/7134 completed (loss: 0.14335401356220245, acc: 0.9695122241973877)
[2025-02-13 19:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01][root][INFO] - Training Epoch: 1/2, step 4762/7134 completed (loss: 0.3041573762893677, acc: 0.9395604133605957)
[2025-02-13 19:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01][root][INFO] - Training Epoch: 1/2, step 4763/7134 completed (loss: 0.15916264057159424, acc: 0.9735099077224731)
[2025-02-13 19:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01][root][INFO] - Training Epoch: 1/2, step 4764/7134 completed (loss: 0.18753394484519958, acc: 0.9432989954948425)
[2025-02-13 19:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02][root][INFO] - Training Epoch: 1/2, step 4765/7134 completed (loss: 0.3383714556694031, acc: 0.9333333373069763)
[2025-02-13 19:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02][root][INFO] - Training Epoch: 1/2, step 4766/7134 completed (loss: 0.15208479762077332, acc: 0.9611111283302307)
[2025-02-13 19:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02][root][INFO] - Training Epoch: 1/2, step 4767/7134 completed (loss: 0.14808547496795654, acc: 0.9795918464660645)
[2025-02-13 19:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:03][root][INFO] - Training Epoch: 1/2, step 4768/7134 completed (loss: 0.13585422933101654, acc: 0.9647058844566345)
[2025-02-13 19:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:03][root][INFO] - Training Epoch: 1/2, step 4769/7134 completed (loss: 0.23586393892765045, acc: 0.9539473652839661)
[2025-02-13 19:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04][root][INFO] - Training Epoch: 1/2, step 4770/7134 completed (loss: 0.07365427911281586, acc: 0.984000027179718)
[2025-02-13 19:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04][root][INFO] - Training Epoch: 1/2, step 4771/7134 completed (loss: 0.29034700989723206, acc: 0.9386503100395203)
[2025-02-13 19:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04][root][INFO] - Training Epoch: 1/2, step 4772/7134 completed (loss: 0.24783971905708313, acc: 0.9354838728904724)
[2025-02-13 19:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05][root][INFO] - Training Epoch: 1/2, step 4773/7134 completed (loss: 0.12689389288425446, acc: 0.9615384340286255)
[2025-02-13 19:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05][root][INFO] - Training Epoch: 1/2, step 4774/7134 completed (loss: 0.14360201358795166, acc: 0.9704142212867737)
[2025-02-13 19:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05][root][INFO] - Training Epoch: 1/2, step 4775/7134 completed (loss: 0.2635716199874878, acc: 0.9622641801834106)
[2025-02-13 19:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06][root][INFO] - Training Epoch: 1/2, step 4776/7134 completed (loss: 0.33053985238075256, acc: 0.9154929518699646)
[2025-02-13 19:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06][root][INFO] - Training Epoch: 1/2, step 4777/7134 completed (loss: 0.23744551837444305, acc: 0.9383561611175537)
[2025-02-13 19:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06][root][INFO] - Training Epoch: 1/2, step 4778/7134 completed (loss: 0.17117540538311005, acc: 0.9464285969734192)
[2025-02-13 19:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07][root][INFO] - Training Epoch: 1/2, step 4779/7134 completed (loss: 0.1289440542459488, acc: 0.970370352268219)
[2025-02-13 19:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07][root][INFO] - Training Epoch: 1/2, step 4780/7134 completed (loss: 0.05930715799331665, acc: 0.9807692170143127)
[2025-02-13 19:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08][root][INFO] - Training Epoch: 1/2, step 4781/7134 completed (loss: 0.07477570325136185, acc: 0.9838709831237793)
[2025-02-13 19:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08][root][INFO] - Training Epoch: 1/2, step 4782/7134 completed (loss: 0.12191402912139893, acc: 0.981249988079071)
[2025-02-13 19:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08][root][INFO] - Training Epoch: 1/2, step 4783/7134 completed (loss: 0.35591602325439453, acc: 0.9202898740768433)
[2025-02-13 19:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09][root][INFO] - Training Epoch: 1/2, step 4784/7134 completed (loss: 0.20806805789470673, acc: 0.9555555582046509)
[2025-02-13 19:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09][root][INFO] - Training Epoch: 1/2, step 4785/7134 completed (loss: 0.16111724078655243, acc: 0.9776119589805603)
[2025-02-13 19:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09][root][INFO] - Training Epoch: 1/2, step 4786/7134 completed (loss: 0.08204342424869537, acc: 0.9691358208656311)
[2025-02-13 19:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10][root][INFO] - Training Epoch: 1/2, step 4787/7134 completed (loss: 0.10963723063468933, acc: 0.9811320900917053)
[2025-02-13 19:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10][root][INFO] - Training Epoch: 1/2, step 4788/7134 completed (loss: 0.06740457564592361, acc: 0.9846153855323792)
[2025-02-13 19:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11][root][INFO] - Training Epoch: 1/2, step 4789/7134 completed (loss: 0.16288459300994873, acc: 0.9679487347602844)
[2025-02-13 19:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11][root][INFO] - Training Epoch: 1/2, step 4790/7134 completed (loss: 0.16768233478069305, acc: 0.9550561904907227)
[2025-02-13 19:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11][root][INFO] - Training Epoch: 1/2, step 4791/7134 completed (loss: 0.061186447739601135, acc: 0.9850746393203735)
[2025-02-13 19:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12][root][INFO] - Training Epoch: 1/2, step 4792/7134 completed (loss: 0.052955832332372665, acc: 0.9932432174682617)
[2025-02-13 19:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12][root][INFO] - Training Epoch: 1/2, step 4793/7134 completed (loss: 0.37097805738449097, acc: 0.8947368264198303)
[2025-02-13 19:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13][root][INFO] - Training Epoch: 1/2, step 4794/7134 completed (loss: 0.3767816722393036, acc: 0.9078013896942139)
[2025-02-13 19:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13][root][INFO] - Training Epoch: 1/2, step 4795/7134 completed (loss: 0.15220579504966736, acc: 0.9509202241897583)
[2025-02-13 19:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13][root][INFO] - Training Epoch: 1/2, step 4796/7134 completed (loss: 0.255966454744339, acc: 0.9473684430122375)
[2025-02-13 19:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14][root][INFO] - Training Epoch: 1/2, step 4797/7134 completed (loss: 0.12075113505125046, acc: 0.9672130942344666)
[2025-02-13 19:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14][root][INFO] - Training Epoch: 1/2, step 4798/7134 completed (loss: 0.14825305342674255, acc: 0.976047933101654)
[2025-02-13 19:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14][root][INFO] - Training Epoch: 1/2, step 4799/7134 completed (loss: 0.24717235565185547, acc: 0.9431818127632141)
[2025-02-13 19:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15][root][INFO] - Training Epoch: 1/2, step 4800/7134 completed (loss: 0.1560712307691574, acc: 0.9473684430122375)
[2025-02-13 19:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15][root][INFO] - Training Epoch: 1/2, step 4801/7134 completed (loss: 0.17664973437786102, acc: 0.9693877696990967)
[2025-02-13 19:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16][root][INFO] - Training Epoch: 1/2, step 4802/7134 completed (loss: 0.29204127192497253, acc: 0.938144326210022)
[2025-02-13 19:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16][root][INFO] - Training Epoch: 1/2, step 4803/7134 completed (loss: 0.179923877120018, acc: 0.9636363387107849)
[2025-02-13 19:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16][root][INFO] - Training Epoch: 1/2, step 4804/7134 completed (loss: 0.2560791075229645, acc: 0.9337349534034729)
[2025-02-13 19:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17][root][INFO] - Training Epoch: 1/2, step 4805/7134 completed (loss: 0.1305282562971115, acc: 0.9801324605941772)
[2025-02-13 19:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17][root][INFO] - Training Epoch: 1/2, step 4806/7134 completed (loss: 0.18305692076683044, acc: 0.9790209531784058)
[2025-02-13 19:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17][root][INFO] - Training Epoch: 1/2, step 4807/7134 completed (loss: 0.1400073766708374, acc: 0.9746835231781006)
[2025-02-13 19:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18][root][INFO] - Training Epoch: 1/2, step 4808/7134 completed (loss: 0.1981385499238968, acc: 0.9545454382896423)
[2025-02-13 19:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18][root][INFO] - Training Epoch: 1/2, step 4809/7134 completed (loss: 0.16490773856639862, acc: 0.9586206674575806)
[2025-02-13 19:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18][root][INFO] - Training Epoch: 1/2, step 4810/7134 completed (loss: 0.1780378669500351, acc: 0.9529411792755127)
[2025-02-13 19:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:19][root][INFO] - Training Epoch: 1/2, step 4811/7134 completed (loss: 0.18096141517162323, acc: 0.9672130942344666)
[2025-02-13 19:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:19][root][INFO] - Training Epoch: 1/2, step 4812/7134 completed (loss: 0.24577993154525757, acc: 0.9604519605636597)
[2025-02-13 19:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20][root][INFO] - Training Epoch: 1/2, step 4813/7134 completed (loss: 0.05645782873034477, acc: 0.9895833134651184)
[2025-02-13 19:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20][root][INFO] - Training Epoch: 1/2, step 4814/7134 completed (loss: 0.15868805348873138, acc: 0.95652174949646)
[2025-02-13 19:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20][root][INFO] - Training Epoch: 1/2, step 4815/7134 completed (loss: 0.018300868570804596, acc: 1.0)
[2025-02-13 19:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21][root][INFO] - Training Epoch: 1/2, step 4816/7134 completed (loss: 0.03253256902098656, acc: 0.9899497628211975)
[2025-02-13 19:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21][root][INFO] - Training Epoch: 1/2, step 4817/7134 completed (loss: 0.08589129894971848, acc: 0.9797979593276978)
[2025-02-13 19:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21][root][INFO] - Training Epoch: 1/2, step 4818/7134 completed (loss: 0.11263912171125412, acc: 0.9659863710403442)
[2025-02-13 19:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22][root][INFO] - Training Epoch: 1/2, step 4819/7134 completed (loss: 0.04716550186276436, acc: 0.9916666746139526)
[2025-02-13 19:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22][root][INFO] - Training Epoch: 1/2, step 4820/7134 completed (loss: 0.08321934193372726, acc: 0.9893048405647278)
[2025-02-13 19:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23][root][INFO] - Training Epoch: 1/2, step 4821/7134 completed (loss: 0.08942253142595291, acc: 0.9710982441902161)
[2025-02-13 19:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23][root][INFO] - Training Epoch: 1/2, step 4822/7134 completed (loss: 0.07387600094079971, acc: 0.970588207244873)
[2025-02-13 19:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23][root][INFO] - Training Epoch: 1/2, step 4823/7134 completed (loss: 0.2340213805437088, acc: 0.955974817276001)
[2025-02-13 19:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24][root][INFO] - Training Epoch: 1/2, step 4824/7134 completed (loss: 0.27761998772621155, acc: 0.9491525292396545)
[2025-02-13 19:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24][root][INFO] - Training Epoch: 1/2, step 4825/7134 completed (loss: 0.07185196131467819, acc: 0.970588207244873)
[2025-02-13 19:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25][root][INFO] - Training Epoch: 1/2, step 4826/7134 completed (loss: 0.13544583320617676, acc: 0.9504132270812988)
[2025-02-13 19:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25][root][INFO] - Training Epoch: 1/2, step 4827/7134 completed (loss: 0.32588592171669006, acc: 0.948387086391449)
[2025-02-13 19:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25][root][INFO] - Training Epoch: 1/2, step 4828/7134 completed (loss: 0.061975400894880295, acc: 0.9948979616165161)
[2025-02-13 19:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:26][root][INFO] - Training Epoch: 1/2, step 4829/7134 completed (loss: 0.020288942381739616, acc: 0.9950494766235352)
[2025-02-13 19:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:26][root][INFO] - Training Epoch: 1/2, step 4830/7134 completed (loss: 0.04368631914258003, acc: 0.9939393997192383)
[2025-02-13 19:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27][root][INFO] - Training Epoch: 1/2, step 4831/7134 completed (loss: 0.03023291751742363, acc: 0.9942857027053833)
[2025-02-13 19:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27][root][INFO] - Training Epoch: 1/2, step 4832/7134 completed (loss: 0.028286872431635857, acc: 0.9934640526771545)
[2025-02-13 19:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27][root][INFO] - Training Epoch: 1/2, step 4833/7134 completed (loss: 0.05766649916768074, acc: 0.9746192693710327)
[2025-02-13 19:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28][root][INFO] - Training Epoch: 1/2, step 4834/7134 completed (loss: 0.053364358842372894, acc: 0.9906976819038391)
[2025-02-13 19:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28][root][INFO] - Training Epoch: 1/2, step 4835/7134 completed (loss: 0.06962589919567108, acc: 0.9743589758872986)
[2025-02-13 19:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28][root][INFO] - Training Epoch: 1/2, step 4836/7134 completed (loss: 0.057725295424461365, acc: 0.9938271641731262)
[2025-02-13 19:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:29][root][INFO] - Training Epoch: 1/2, step 4837/7134 completed (loss: 0.3504788875579834, acc: 0.9357143044471741)
[2025-02-13 19:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:29][root][INFO] - Training Epoch: 1/2, step 4838/7134 completed (loss: 0.3095933794975281, acc: 0.935251772403717)
[2025-02-13 19:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30][root][INFO] - Training Epoch: 1/2, step 4839/7134 completed (loss: 0.24646659195423126, acc: 0.961240291595459)
[2025-02-13 19:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30][root][INFO] - Training Epoch: 1/2, step 4840/7134 completed (loss: 0.16614043712615967, acc: 0.9407894611358643)
[2025-02-13 19:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30][root][INFO] - Training Epoch: 1/2, step 4841/7134 completed (loss: 0.1584623157978058, acc: 0.9444444179534912)
[2025-02-13 19:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31][root][INFO] - Training Epoch: 1/2, step 4842/7134 completed (loss: 0.2218962162733078, acc: 0.9555555582046509)
[2025-02-13 19:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31][root][INFO] - Training Epoch: 1/2, step 4843/7134 completed (loss: 0.23212085664272308, acc: 0.9245283007621765)
[2025-02-13 19:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32][root][INFO] - Training Epoch: 1/2, step 4844/7134 completed (loss: 0.14792068302631378, acc: 0.9307692050933838)
[2025-02-13 19:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32][root][INFO] - Training Epoch: 1/2, step 4845/7134 completed (loss: 0.2028369903564453, acc: 0.949999988079071)
[2025-02-13 19:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32][root][INFO] - Training Epoch: 1/2, step 4846/7134 completed (loss: 0.30802756547927856, acc: 0.918749988079071)
[2025-02-13 19:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33][root][INFO] - Training Epoch: 1/2, step 4847/7134 completed (loss: 0.1235608235001564, acc: 0.9704142212867737)
[2025-02-13 19:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33][root][INFO] - Training Epoch: 1/2, step 4848/7134 completed (loss: 0.3037731945514679, acc: 0.9444444179534912)
[2025-02-13 19:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33][root][INFO] - Training Epoch: 1/2, step 4849/7134 completed (loss: 0.15035705268383026, acc: 0.965753436088562)
[2025-02-13 19:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34][root][INFO] - Training Epoch: 1/2, step 4850/7134 completed (loss: 0.20421640574932098, acc: 0.9273743033409119)
[2025-02-13 19:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34][root][INFO] - Training Epoch: 1/2, step 4851/7134 completed (loss: 0.1788337230682373, acc: 0.9459459185600281)
[2025-02-13 19:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35][root][INFO] - Training Epoch: 1/2, step 4852/7134 completed (loss: 0.28126174211502075, acc: 0.9186046719551086)
[2025-02-13 19:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35][root][INFO] - Training Epoch: 1/2, step 4853/7134 completed (loss: 0.449971467256546, acc: 0.8802395462989807)
[2025-02-13 19:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35][root][INFO] - Training Epoch: 1/2, step 4854/7134 completed (loss: 0.1374429315328598, acc: 0.9664429426193237)
[2025-02-13 19:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36][root][INFO] - Training Epoch: 1/2, step 4855/7134 completed (loss: 0.1780286580324173, acc: 0.9655172228813171)
[2025-02-13 19:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36][root][INFO] - Training Epoch: 1/2, step 4856/7134 completed (loss: 0.10001204162836075, acc: 0.976190447807312)
[2025-02-13 19:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37][root][INFO] - Training Epoch: 1/2, step 4857/7134 completed (loss: 0.18197044730186462, acc: 0.9444444179534912)
[2025-02-13 19:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37][root][INFO] - Training Epoch: 1/2, step 4858/7134 completed (loss: 0.3460830748081207, acc: 0.9320987462997437)
[2025-02-13 19:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37][root][INFO] - Training Epoch: 1/2, step 4859/7134 completed (loss: 0.12135238945484161, acc: 0.9523809552192688)
[2025-02-13 19:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38][root][INFO] - Training Epoch: 1/2, step 4860/7134 completed (loss: 0.13916784524917603, acc: 0.9649122953414917)
[2025-02-13 19:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38][root][INFO] - Training Epoch: 1/2, step 4861/7134 completed (loss: 0.1269102692604065, acc: 0.9585798978805542)
[2025-02-13 19:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39][root][INFO] - Training Epoch: 1/2, step 4862/7134 completed (loss: 0.16801795363426208, acc: 0.9671052694320679)
[2025-02-13 19:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39][root][INFO] - Training Epoch: 1/2, step 4863/7134 completed (loss: 0.13737788796424866, acc: 0.9689922332763672)
[2025-02-13 19:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39][root][INFO] - Training Epoch: 1/2, step 4864/7134 completed (loss: 0.17764022946357727, acc: 0.9554139971733093)
[2025-02-13 19:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40][root][INFO] - Training Epoch: 1/2, step 4865/7134 completed (loss: 0.1640024483203888, acc: 0.9569892287254333)
[2025-02-13 19:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40][root][INFO] - Training Epoch: 1/2, step 4866/7134 completed (loss: 0.2348598688840866, acc: 0.9186046719551086)
[2025-02-13 19:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40][root][INFO] - Training Epoch: 1/2, step 4867/7134 completed (loss: 0.18918322026729584, acc: 0.9608938694000244)
[2025-02-13 19:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41][root][INFO] - Training Epoch: 1/2, step 4868/7134 completed (loss: 0.18191857635974884, acc: 0.9327731132507324)
[2025-02-13 19:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41][root][INFO] - Training Epoch: 1/2, step 4869/7134 completed (loss: 0.14830860495567322, acc: 0.9591836929321289)
[2025-02-13 19:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42][root][INFO] - Training Epoch: 1/2, step 4870/7134 completed (loss: 0.08864282816648483, acc: 0.9729729890823364)
[2025-02-13 19:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42][root][INFO] - Training Epoch: 1/2, step 4871/7134 completed (loss: 0.14973080158233643, acc: 0.9539473652839661)
[2025-02-13 19:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42][root][INFO] - Training Epoch: 1/2, step 4872/7134 completed (loss: 0.024634139612317085, acc: 1.0)
[2025-02-13 19:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43][root][INFO] - Training Epoch: 1/2, step 4873/7134 completed (loss: 0.1786700189113617, acc: 0.9672130942344666)
[2025-02-13 19:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43][root][INFO] - Training Epoch: 1/2, step 4874/7134 completed (loss: 0.1191229596734047, acc: 0.9707317352294922)
[2025-02-13 19:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44][root][INFO] - Training Epoch: 1/2, step 4875/7134 completed (loss: 0.11482751369476318, acc: 0.9751552939414978)
[2025-02-13 19:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44][root][INFO] - Training Epoch: 1/2, step 4876/7134 completed (loss: 0.21199455857276917, acc: 0.9444444179534912)
[2025-02-13 19:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44][root][INFO] - Training Epoch: 1/2, step 4877/7134 completed (loss: 0.16292715072631836, acc: 0.9692307710647583)
[2025-02-13 19:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:45][root][INFO] - Training Epoch: 1/2, step 4878/7134 completed (loss: 0.14915494620800018, acc: 0.948051929473877)
[2025-02-13 19:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:45][root][INFO] - Training Epoch: 1/2, step 4879/7134 completed (loss: 0.12442567944526672, acc: 0.9551281929016113)
[2025-02-13 19:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46][root][INFO] - Training Epoch: 1/2, step 4880/7134 completed (loss: 0.0908093973994255, acc: 0.970588207244873)
[2025-02-13 19:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46][root][INFO] - Training Epoch: 1/2, step 4881/7134 completed (loss: 0.20745708048343658, acc: 0.9597315192222595)
[2025-02-13 19:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46][root][INFO] - Training Epoch: 1/2, step 4882/7134 completed (loss: 0.14687329530715942, acc: 0.9677419066429138)
[2025-02-13 19:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47][root][INFO] - Training Epoch: 1/2, step 4883/7134 completed (loss: 0.10439661890268326, acc: 0.9599999785423279)
[2025-02-13 19:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47][root][INFO] - Training Epoch: 1/2, step 4884/7134 completed (loss: 0.09877313673496246, acc: 0.9673202633857727)
[2025-02-13 19:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48][root][INFO] - Training Epoch: 1/2, step 4885/7134 completed (loss: 0.09392765909433365, acc: 0.9784946441650391)
[2025-02-13 19:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48][root][INFO] - Training Epoch: 1/2, step 4886/7134 completed (loss: 0.16284731030464172, acc: 0.9711538553237915)
[2025-02-13 19:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48][root][INFO] - Training Epoch: 1/2, step 4887/7134 completed (loss: 0.1025552824139595, acc: 0.9738219976425171)
[2025-02-13 19:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49][root][INFO] - Training Epoch: 1/2, step 4888/7134 completed (loss: 0.09821998327970505, acc: 0.9780219793319702)
[2025-02-13 19:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49][root][INFO] - Training Epoch: 1/2, step 4889/7134 completed (loss: 0.06571114808320999, acc: 0.9732620120048523)
[2025-02-13 19:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50][root][INFO] - Training Epoch: 1/2, step 4890/7134 completed (loss: 0.2565942108631134, acc: 0.9433962106704712)
[2025-02-13 19:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50][root][INFO] - Training Epoch: 1/2, step 4891/7134 completed (loss: 0.15390372276306152, acc: 0.954081654548645)
[2025-02-13 19:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50][root][INFO] - Training Epoch: 1/2, step 4892/7134 completed (loss: 0.11930832266807556, acc: 0.9805194735527039)
[2025-02-13 19:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51][root][INFO] - Training Epoch: 1/2, step 4893/7134 completed (loss: 0.6307779550552368, acc: 0.8534482717514038)
[2025-02-13 19:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51][root][INFO] - Training Epoch: 1/2, step 4894/7134 completed (loss: 0.08500014245510101, acc: 0.9882352948188782)
[2025-02-13 19:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52][root][INFO] - Training Epoch: 1/2, step 4895/7134 completed (loss: 0.08998271077871323, acc: 0.9744898080825806)
[2025-02-13 19:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52][root][INFO] - Training Epoch: 1/2, step 4896/7134 completed (loss: 0.07021967321634293, acc: 0.9888268113136292)
[2025-02-13 19:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52][root][INFO] - Training Epoch: 1/2, step 4897/7134 completed (loss: 0.10750710219144821, acc: 0.9802955389022827)
[2025-02-13 19:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53][root][INFO] - Training Epoch: 1/2, step 4898/7134 completed (loss: 0.11455881595611572, acc: 0.9731183052062988)
[2025-02-13 19:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53][root][INFO] - Training Epoch: 1/2, step 4899/7134 completed (loss: 0.09884825348854065, acc: 0.9756097793579102)
[2025-02-13 19:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53][root][INFO] - Training Epoch: 1/2, step 4900/7134 completed (loss: 0.15718168020248413, acc: 0.9893048405647278)
[2025-02-13 19:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54][root][INFO] - Training Epoch: 1/2, step 4901/7134 completed (loss: 0.10167281329631805, acc: 0.9756097793579102)
[2025-02-13 19:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54][root][INFO] - Training Epoch: 1/2, step 4902/7134 completed (loss: 0.06946192681789398, acc: 0.9743589758872986)
[2025-02-13 19:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54][root][INFO] - Training Epoch: 1/2, step 4903/7134 completed (loss: 0.06468641757965088, acc: 0.985401451587677)
[2025-02-13 19:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55][root][INFO] - Training Epoch: 1/2, step 4904/7134 completed (loss: 0.05892670527100563, acc: 0.9823529124259949)
[2025-02-13 19:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55][root][INFO] - Training Epoch: 1/2, step 4905/7134 completed (loss: 0.03402550145983696, acc: 0.9934640526771545)
[2025-02-13 19:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56][root][INFO] - Training Epoch: 1/2, step 4906/7134 completed (loss: 0.04761166125535965, acc: 0.9849624037742615)
[2025-02-13 19:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56][root][INFO] - Training Epoch: 1/2, step 4907/7134 completed (loss: 0.0756358727812767, acc: 0.9795918464660645)
[2025-02-13 19:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56][root][INFO] - Training Epoch: 1/2, step 4908/7134 completed (loss: 0.04193859547376633, acc: 0.9870129823684692)
[2025-02-13 19:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57][root][INFO] - Training Epoch: 1/2, step 4909/7134 completed (loss: 0.03571052476763725, acc: 0.9932885766029358)
[2025-02-13 19:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57][root][INFO] - Training Epoch: 1/2, step 4910/7134 completed (loss: 0.14049704372882843, acc: 0.9612902998924255)
[2025-02-13 19:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57][root][INFO] - Training Epoch: 1/2, step 4911/7134 completed (loss: 0.06738176941871643, acc: 0.9839572310447693)
[2025-02-13 19:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:58][root][INFO] - Training Epoch: 1/2, step 4912/7134 completed (loss: 0.05426141619682312, acc: 0.9900497794151306)
[2025-02-13 19:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:58][root][INFO] - Training Epoch: 1/2, step 4913/7134 completed (loss: 0.21017687022686005, acc: 0.9510489702224731)
[2025-02-13 19:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59][root][INFO] - Training Epoch: 1/2, step 4914/7134 completed (loss: 0.10369540750980377, acc: 0.9741935729980469)
[2025-02-13 19:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59][root][INFO] - Training Epoch: 1/2, step 4915/7134 completed (loss: 0.017799004912376404, acc: 0.9938271641731262)
[2025-02-13 19:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59][root][INFO] - Training Epoch: 1/2, step 4916/7134 completed (loss: 0.21959446370601654, acc: 0.945652186870575)
[2025-02-13 19:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00][root][INFO] - Training Epoch: 1/2, step 4917/7134 completed (loss: 0.20931097865104675, acc: 0.9554139971733093)
[2025-02-13 19:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00][root][INFO] - Training Epoch: 1/2, step 4918/7134 completed (loss: 0.028854258358478546, acc: 0.9938271641731262)
[2025-02-13 19:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00][root][INFO] - Training Epoch: 1/2, step 4919/7134 completed (loss: 0.15751095116138458, acc: 0.9717513918876648)
[2025-02-13 19:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:01][root][INFO] - Training Epoch: 1/2, step 4920/7134 completed (loss: 0.20184963941574097, acc: 0.9316770434379578)
[2025-02-13 19:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:01][root][INFO] - Training Epoch: 1/2, step 4921/7134 completed (loss: 0.1754634976387024, acc: 0.9635036587715149)
[2025-02-13 19:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02][root][INFO] - Training Epoch: 1/2, step 4922/7134 completed (loss: 0.10901716351509094, acc: 0.9751552939414978)
[2025-02-13 19:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02][root][INFO] - Training Epoch: 1/2, step 4923/7134 completed (loss: 0.0635799989104271, acc: 0.97826087474823)
[2025-02-13 19:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02][root][INFO] - Training Epoch: 1/2, step 4924/7134 completed (loss: 0.1097131073474884, acc: 0.9708737730979919)
[2025-02-13 19:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03][root][INFO] - Training Epoch: 1/2, step 4925/7134 completed (loss: 0.16550731658935547, acc: 0.9463087320327759)
[2025-02-13 19:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03][root][INFO] - Training Epoch: 1/2, step 4926/7134 completed (loss: 0.03219818323850632, acc: 0.9932885766029358)
[2025-02-13 19:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03][root][INFO] - Training Epoch: 1/2, step 4927/7134 completed (loss: 0.15919385850429535, acc: 0.9806451797485352)
[2025-02-13 19:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:04][root][INFO] - Training Epoch: 1/2, step 4928/7134 completed (loss: 0.06265993416309357, acc: 0.9842105507850647)
[2025-02-13 19:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:04][root][INFO] - Training Epoch: 1/2, step 4929/7134 completed (loss: 0.045143093913793564, acc: 0.9934210777282715)
[2025-02-13 19:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05][root][INFO] - Training Epoch: 1/2, step 4930/7134 completed (loss: 0.09310922026634216, acc: 0.9729729890823364)
[2025-02-13 19:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05][root][INFO] - Training Epoch: 1/2, step 4931/7134 completed (loss: 0.1229526549577713, acc: 0.9570552110671997)
[2025-02-13 19:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05][root][INFO] - Training Epoch: 1/2, step 4932/7134 completed (loss: 0.05751057341694832, acc: 0.9882352948188782)
[2025-02-13 19:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06][root][INFO] - Training Epoch: 1/2, step 4933/7134 completed (loss: 0.14807823300361633, acc: 0.963350772857666)
[2025-02-13 19:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06][root][INFO] - Training Epoch: 1/2, step 4934/7134 completed (loss: 0.09414060413837433, acc: 0.9764705896377563)
[2025-02-13 19:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06][root][INFO] - Training Epoch: 1/2, step 4935/7134 completed (loss: 0.0783323273062706, acc: 0.970588207244873)
[2025-02-13 19:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:07][root][INFO] - Training Epoch: 1/2, step 4936/7134 completed (loss: 0.03644367679953575, acc: 0.9931972622871399)
[2025-02-13 19:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:07][root][INFO] - Training Epoch: 1/2, step 4937/7134 completed (loss: 0.15852542221546173, acc: 0.9580419659614563)
[2025-02-13 19:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08][root][INFO] - Training Epoch: 1/2, step 4938/7134 completed (loss: 0.026584722101688385, acc: 0.9938271641731262)
[2025-02-13 19:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08][root][INFO] - Training Epoch: 1/2, step 4939/7134 completed (loss: 0.13032293319702148, acc: 0.9714285731315613)
[2025-02-13 19:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08][root][INFO] - Training Epoch: 1/2, step 4940/7134 completed (loss: 0.18031233549118042, acc: 0.9689922332763672)
[2025-02-13 19:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09][root][INFO] - Training Epoch: 1/2, step 4941/7134 completed (loss: 0.1484922617673874, acc: 0.976047933101654)
[2025-02-13 19:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09][root][INFO] - Training Epoch: 1/2, step 4942/7134 completed (loss: 0.1571587473154068, acc: 0.949367105960846)
[2025-02-13 19:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09][root][INFO] - Training Epoch: 1/2, step 4943/7134 completed (loss: 0.15499648451805115, acc: 0.956250011920929)
[2025-02-13 19:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10][root][INFO] - Training Epoch: 1/2, step 4944/7134 completed (loss: 0.2920845150947571, acc: 0.9411764740943909)
[2025-02-13 19:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10][root][INFO] - Training Epoch: 1/2, step 4945/7134 completed (loss: 0.281399130821228, acc: 0.939393937587738)
[2025-02-13 19:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11][root][INFO] - Training Epoch: 1/2, step 4946/7134 completed (loss: 0.24365359544754028, acc: 0.9375)
[2025-02-13 19:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11][root][INFO] - Training Epoch: 1/2, step 4947/7134 completed (loss: 0.2030511498451233, acc: 0.9652174115180969)
[2025-02-13 19:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11][root][INFO] - Training Epoch: 1/2, step 4948/7134 completed (loss: 0.2831878066062927, acc: 0.9523809552192688)
[2025-02-13 19:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12][root][INFO] - Training Epoch: 1/2, step 4949/7134 completed (loss: 0.049964599311351776, acc: 0.9876543283462524)
[2025-02-13 19:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12][root][INFO] - Training Epoch: 1/2, step 4950/7134 completed (loss: 0.054084789007902145, acc: 0.9795918464660645)
[2025-02-13 19:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12][root][INFO] - Training Epoch: 1/2, step 4951/7134 completed (loss: 0.1565481424331665, acc: 0.966292142868042)
[2025-02-13 19:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13][root][INFO] - Training Epoch: 1/2, step 4952/7134 completed (loss: 0.08883029967546463, acc: 0.9662162065505981)
[2025-02-13 19:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13][root][INFO] - Training Epoch: 1/2, step 4953/7134 completed (loss: 0.15439672768115997, acc: 0.9369369149208069)
[2025-02-13 19:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14][root][INFO] - Training Epoch: 1/2, step 4954/7134 completed (loss: 0.11105083674192429, acc: 0.9795918464660645)
[2025-02-13 19:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14][root][INFO] - Training Epoch: 1/2, step 4955/7134 completed (loss: 0.15617604553699493, acc: 0.9583333134651184)
[2025-02-13 19:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14][root][INFO] - Training Epoch: 1/2, step 4956/7134 completed (loss: 0.25408729910850525, acc: 0.9529411792755127)
[2025-02-13 19:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15][root][INFO] - Training Epoch: 1/2, step 4957/7134 completed (loss: 0.16983018815517426, acc: 0.957446813583374)
[2025-02-13 19:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15][root][INFO] - Training Epoch: 1/2, step 4958/7134 completed (loss: 0.13900943100452423, acc: 0.9496855139732361)
[2025-02-13 19:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15][root][INFO] - Training Epoch: 1/2, step 4959/7134 completed (loss: 0.18553364276885986, acc: 0.9698795080184937)
[2025-02-13 19:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16][root][INFO] - Training Epoch: 1/2, step 4960/7134 completed (loss: 0.17063525319099426, acc: 0.9558823704719543)
[2025-02-13 19:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16][root][INFO] - Training Epoch: 1/2, step 4961/7134 completed (loss: 0.14836199581623077, acc: 0.96875)
[2025-02-13 19:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16][root][INFO] - Training Epoch: 1/2, step 4962/7134 completed (loss: 0.08083128929138184, acc: 0.9817073345184326)
[2025-02-13 19:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17][root][INFO] - Training Epoch: 1/2, step 4963/7134 completed (loss: 0.06732052564620972, acc: 0.9806451797485352)
[2025-02-13 19:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17][root][INFO] - Training Epoch: 1/2, step 4964/7134 completed (loss: 0.21902258694171906, acc: 0.935251772403717)
[2025-02-13 19:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18][root][INFO] - Training Epoch: 1/2, step 4965/7134 completed (loss: 0.03908804431557655, acc: 1.0)
[2025-02-13 19:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18][root][INFO] - Training Epoch: 1/2, step 4966/7134 completed (loss: 0.22418871521949768, acc: 0.9450549483299255)
[2025-02-13 19:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18][root][INFO] - Training Epoch: 1/2, step 4967/7134 completed (loss: 0.11836491525173187, acc: 0.9844961166381836)
[2025-02-13 19:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19][root][INFO] - Training Epoch: 1/2, step 4968/7134 completed (loss: 0.3877868950366974, acc: 0.8936170339584351)
[2025-02-13 19:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19][root][INFO] - Training Epoch: 1/2, step 4969/7134 completed (loss: 0.15805278718471527, acc: 0.9586206674575806)
[2025-02-13 19:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19][root][INFO] - Training Epoch: 1/2, step 4970/7134 completed (loss: 0.09085836261510849, acc: 0.9793103337287903)
[2025-02-13 19:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20][root][INFO] - Training Epoch: 1/2, step 4971/7134 completed (loss: 0.06904102861881256, acc: 0.9735099077224731)
[2025-02-13 19:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20][root][INFO] - Training Epoch: 1/2, step 4972/7134 completed (loss: 0.15956731140613556, acc: 0.949999988079071)
[2025-02-13 19:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20][root][INFO] - Training Epoch: 1/2, step 4973/7134 completed (loss: 0.09655551612377167, acc: 0.9772727489471436)
[2025-02-13 19:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:21][root][INFO] - Training Epoch: 1/2, step 4974/7134 completed (loss: 0.19120344519615173, acc: 0.9469696879386902)
[2025-02-13 19:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:21][root][INFO] - Training Epoch: 1/2, step 4975/7134 completed (loss: 0.34603801369667053, acc: 0.9241379499435425)
[2025-02-13 19:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22][root][INFO] - Training Epoch: 1/2, step 4976/7134 completed (loss: 0.26633644104003906, acc: 0.9473684430122375)
[2025-02-13 19:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22][root][INFO] - Training Epoch: 1/2, step 4977/7134 completed (loss: 0.27554288506507874, acc: 0.9514563083648682)
[2025-02-13 19:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22][root][INFO] - Training Epoch: 1/2, step 4978/7134 completed (loss: 0.07655280083417892, acc: 0.9885057210922241)
[2025-02-13 19:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23][root][INFO] - Training Epoch: 1/2, step 4979/7134 completed (loss: 0.3699915409088135, acc: 0.9300699234008789)
[2025-02-13 19:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23][root][INFO] - Training Epoch: 1/2, step 4980/7134 completed (loss: 0.284473717212677, acc: 0.949999988079071)
[2025-02-13 19:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23][root][INFO] - Training Epoch: 1/2, step 4981/7134 completed (loss: 0.19590376317501068, acc: 0.9571428298950195)
[2025-02-13 19:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24][root][INFO] - Training Epoch: 1/2, step 4982/7134 completed (loss: 0.18504253029823303, acc: 0.9639639854431152)
[2025-02-13 19:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24][root][INFO] - Training Epoch: 1/2, step 4983/7134 completed (loss: 0.14810830354690552, acc: 0.9784172773361206)
[2025-02-13 19:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24][root][INFO] - Training Epoch: 1/2, step 4984/7134 completed (loss: 0.25321564078330994, acc: 0.9811320900917053)
[2025-02-13 19:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25][root][INFO] - Training Epoch: 1/2, step 4985/7134 completed (loss: 0.17766359448432922, acc: 0.9729729890823364)
[2025-02-13 19:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25][root][INFO] - Training Epoch: 1/2, step 4986/7134 completed (loss: 0.13851165771484375, acc: 0.9720279574394226)
[2025-02-13 19:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26][root][INFO] - Training Epoch: 1/2, step 4987/7134 completed (loss: 0.06453467905521393, acc: 0.9716981053352356)
[2025-02-13 19:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26][root][INFO] - Training Epoch: 1/2, step 4988/7134 completed (loss: 0.08287081122398376, acc: 0.982758641242981)
[2025-02-13 19:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26][root][INFO] - Training Epoch: 1/2, step 4989/7134 completed (loss: 0.07335344702005386, acc: 0.9764705896377563)
[2025-02-13 19:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27][root][INFO] - Training Epoch: 1/2, step 4990/7134 completed (loss: 0.059810202568769455, acc: 0.9829059839248657)
[2025-02-13 19:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27][root][INFO] - Training Epoch: 1/2, step 4991/7134 completed (loss: 0.06150442734360695, acc: 0.969924807548523)
[2025-02-13 19:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27][root][INFO] - Training Epoch: 1/2, step 4992/7134 completed (loss: 0.16826887428760529, acc: 0.9741379022598267)
[2025-02-13 19:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28][root][INFO] - Training Epoch: 1/2, step 4993/7134 completed (loss: 0.05956922098994255, acc: 0.9800000190734863)
[2025-02-13 19:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28][root][INFO] - Training Epoch: 1/2, step 4994/7134 completed (loss: 0.07392367720603943, acc: 0.9867549538612366)
[2025-02-13 19:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28][root][INFO] - Training Epoch: 1/2, step 4995/7134 completed (loss: 0.17884086072444916, acc: 0.9629629850387573)
[2025-02-13 19:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29][root][INFO] - Training Epoch: 1/2, step 4996/7134 completed (loss: 0.09081749618053436, acc: 0.9750000238418579)
[2025-02-13 19:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29][root][INFO] - Training Epoch: 1/2, step 4997/7134 completed (loss: 0.05972765386104584, acc: 0.9923664331436157)
[2025-02-13 19:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29][root][INFO] - Training Epoch: 1/2, step 4998/7134 completed (loss: 0.14313016831874847, acc: 0.9733333587646484)
[2025-02-13 19:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30][root][INFO] - Training Epoch: 1/2, step 4999/7134 completed (loss: 0.14933954179286957, acc: 0.960629940032959)
[2025-02-13 19:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30][root][INFO] - Training Epoch: 1/2, step 5000/7134 completed (loss: 0.14365419745445251, acc: 0.969072163105011)
[2025-02-13 19:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31][root][INFO] - Training Epoch: 1/2, step 5001/7134 completed (loss: 0.2294548898935318, acc: 0.9659863710403442)
[2025-02-13 19:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31][root][INFO] - Training Epoch: 1/2, step 5002/7134 completed (loss: 0.09714356809854507, acc: 0.9734513163566589)
[2025-02-13 19:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31][root][INFO] - Training Epoch: 1/2, step 5003/7134 completed (loss: 0.06792740523815155, acc: 0.9851852059364319)
[2025-02-13 19:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32][root][INFO] - Training Epoch: 1/2, step 5004/7134 completed (loss: 0.07205339521169662, acc: 0.9813084006309509)
[2025-02-13 19:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32][root][INFO] - Training Epoch: 1/2, step 5005/7134 completed (loss: 0.1577761471271515, acc: 0.9780219793319702)
[2025-02-13 19:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33][root][INFO] - Training Epoch: 1/2, step 5006/7134 completed (loss: 0.1230364590883255, acc: 0.9802631735801697)
[2025-02-13 19:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33][root][INFO] - Training Epoch: 1/2, step 5007/7134 completed (loss: 0.06912581622600555, acc: 0.9876543283462524)
[2025-02-13 19:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33][root][INFO] - Training Epoch: 1/2, step 5008/7134 completed (loss: 0.18253889679908752, acc: 0.9833333492279053)
[2025-02-13 19:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34][root][INFO] - Training Epoch: 1/2, step 5009/7134 completed (loss: 0.2593044936656952, acc: 0.9484536051750183)
[2025-02-13 19:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34][root][INFO] - Training Epoch: 1/2, step 5010/7134 completed (loss: 0.1635650247335434, acc: 0.9658119678497314)
[2025-02-13 19:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35][root][INFO] - Training Epoch: 1/2, step 5011/7134 completed (loss: 0.19674979150295258, acc: 0.931034505367279)
[2025-02-13 19:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35][root][INFO] - Training Epoch: 1/2, step 5012/7134 completed (loss: 0.05736524611711502, acc: 1.0)
[2025-02-13 19:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35][root][INFO] - Training Epoch: 1/2, step 5013/7134 completed (loss: 0.05781852453947067, acc: 0.9914529919624329)
[2025-02-13 19:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36][root][INFO] - Training Epoch: 1/2, step 5014/7134 completed (loss: 0.13203126192092896, acc: 0.9711538553237915)
[2025-02-13 19:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36][root][INFO] - Training Epoch: 1/2, step 5015/7134 completed (loss: 0.2863108515739441, acc: 0.9386503100395203)
[2025-02-13 19:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36][root][INFO] - Training Epoch: 1/2, step 5016/7134 completed (loss: 0.41931816935539246, acc: 0.9083969593048096)
[2025-02-13 19:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:37][root][INFO] - Training Epoch: 1/2, step 5017/7134 completed (loss: 0.11421225965023041, acc: 0.9848484992980957)
[2025-02-13 19:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:37][root][INFO] - Training Epoch: 1/2, step 5018/7134 completed (loss: 0.23230215907096863, acc: 0.9398496150970459)
[2025-02-13 19:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38][root][INFO] - Training Epoch: 1/2, step 5019/7134 completed (loss: 0.40857580304145813, acc: 0.909604549407959)
[2025-02-13 19:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38][root][INFO] - Training Epoch: 1/2, step 5020/7134 completed (loss: 0.41017767786979675, acc: 0.9047619104385376)
[2025-02-13 19:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38][root][INFO] - Training Epoch: 1/2, step 5021/7134 completed (loss: 0.11786273866891861, acc: 0.9904761910438538)
[2025-02-13 19:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39][root][INFO] - Training Epoch: 1/2, step 5022/7134 completed (loss: 0.0839160904288292, acc: 0.9927536249160767)
[2025-02-13 19:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39][root][INFO] - Training Epoch: 1/2, step 5023/7134 completed (loss: 0.10220909118652344, acc: 0.9741935729980469)
[2025-02-13 19:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39][root][INFO] - Training Epoch: 1/2, step 5024/7134 completed (loss: 0.08571074903011322, acc: 0.9638554453849792)
[2025-02-13 19:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40][root][INFO] - Training Epoch: 1/2, step 5025/7134 completed (loss: 0.14622579514980316, acc: 0.9692307710647583)
[2025-02-13 19:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40][root][INFO] - Training Epoch: 1/2, step 5026/7134 completed (loss: 0.13558663427829742, acc: 0.9541984796524048)
[2025-02-13 19:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40][root][INFO] - Training Epoch: 1/2, step 5027/7134 completed (loss: 0.08459221571683884, acc: 0.9844961166381836)
[2025-02-13 19:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41][root][INFO] - Training Epoch: 1/2, step 5028/7134 completed (loss: 0.10858307778835297, acc: 0.9788732528686523)
[2025-02-13 19:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41][root][INFO] - Training Epoch: 1/2, step 5029/7134 completed (loss: 0.0660259798169136, acc: 0.9846153855323792)
[2025-02-13 19:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42][root][INFO] - Training Epoch: 1/2, step 5030/7134 completed (loss: 0.1212184801697731, acc: 0.9640718698501587)
[2025-02-13 19:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42][root][INFO] - Training Epoch: 1/2, step 5031/7134 completed (loss: 0.3068428337574005, acc: 0.9347826242446899)
[2025-02-13 19:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42][root][INFO] - Training Epoch: 1/2, step 5032/7134 completed (loss: 0.08175364136695862, acc: 0.98591548204422)
[2025-02-13 19:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43][root][INFO] - Training Epoch: 1/2, step 5033/7134 completed (loss: 0.05966111272573471, acc: 0.98591548204422)
[2025-02-13 19:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43][root][INFO] - Training Epoch: 1/2, step 5034/7134 completed (loss: 0.06772526353597641, acc: 0.9774436354637146)
[2025-02-13 19:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43][root][INFO] - Training Epoch: 1/2, step 5035/7134 completed (loss: 0.011186350136995316, acc: 1.0)
[2025-02-13 19:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44][root][INFO] - Training Epoch: 1/2, step 5036/7134 completed (loss: 0.22601741552352905, acc: 0.9519230723381042)
[2025-02-13 19:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44][root][INFO] - Training Epoch: 1/2, step 5037/7134 completed (loss: 0.17198945581912994, acc: 0.9588235020637512)
[2025-02-13 19:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45][root][INFO] - Training Epoch: 1/2, step 5038/7134 completed (loss: 0.42069607973098755, acc: 0.886956512928009)
[2025-02-13 19:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45][root][INFO] - Training Epoch: 1/2, step 5039/7134 completed (loss: 0.12006081640720367, acc: 0.9503546357154846)
[2025-02-13 19:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45][root][INFO] - Training Epoch: 1/2, step 5040/7134 completed (loss: 0.112767793238163, acc: 0.9666666388511658)
[2025-02-13 19:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46][root][INFO] - Training Epoch: 1/2, step 5041/7134 completed (loss: 0.02426530234515667, acc: 1.0)
[2025-02-13 19:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46][root][INFO] - Training Epoch: 1/2, step 5042/7134 completed (loss: 0.13304711878299713, acc: 0.9708737730979919)
[2025-02-13 19:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47][root][INFO] - Training Epoch: 1/2, step 5043/7134 completed (loss: 0.10956812649965286, acc: 0.9646017551422119)
[2025-02-13 19:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47][root][INFO] - Training Epoch: 1/2, step 5044/7134 completed (loss: 0.12407351285219193, acc: 0.9681528806686401)
[2025-02-13 19:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47][root][INFO] - Training Epoch: 1/2, step 5045/7134 completed (loss: 0.17547136545181274, acc: 0.9621621370315552)
[2025-02-13 19:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48][root][INFO] - Training Epoch: 1/2, step 5046/7134 completed (loss: 0.2236550748348236, acc: 0.9435483813285828)
[2025-02-13 19:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48][root][INFO] - Training Epoch: 1/2, step 5047/7134 completed (loss: 0.19235026836395264, acc: 0.9635036587715149)
[2025-02-13 19:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49][root][INFO] - Training Epoch: 1/2, step 5048/7134 completed (loss: 0.15744544565677643, acc: 0.9642857313156128)
[2025-02-13 19:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49][root][INFO] - Training Epoch: 1/2, step 5049/7134 completed (loss: 0.20087577402591705, acc: 0.9490445852279663)
[2025-02-13 19:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49][root][INFO] - Training Epoch: 1/2, step 5050/7134 completed (loss: 0.22781483829021454, acc: 0.949999988079071)
[2025-02-13 19:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50][root][INFO] - Training Epoch: 1/2, step 5051/7134 completed (loss: 0.22257378697395325, acc: 0.9523809552192688)
[2025-02-13 19:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50][root][INFO] - Training Epoch: 1/2, step 5052/7134 completed (loss: 0.1991640329360962, acc: 0.9551281929016113)
[2025-02-13 19:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50][root][INFO] - Training Epoch: 1/2, step 5053/7134 completed (loss: 0.3355085551738739, acc: 0.9083333611488342)
[2025-02-13 19:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51][root][INFO] - Training Epoch: 1/2, step 5054/7134 completed (loss: 0.27646276354789734, acc: 0.9333333373069763)
[2025-02-13 19:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51][root][INFO] - Training Epoch: 1/2, step 5055/7134 completed (loss: 0.1937256157398224, acc: 0.9469026327133179)
[2025-02-13 19:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52][root][INFO] - Training Epoch: 1/2, step 5056/7134 completed (loss: 0.1985546350479126, acc: 0.9530201554298401)
[2025-02-13 19:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52][root][INFO] - Training Epoch: 1/2, step 5057/7134 completed (loss: 0.29834887385368347, acc: 0.9426751732826233)
[2025-02-13 19:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52][root][INFO] - Training Epoch: 1/2, step 5058/7134 completed (loss: 0.16609206795692444, acc: 0.9512194991111755)
[2025-02-13 19:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53][root][INFO] - Training Epoch: 1/2, step 5059/7134 completed (loss: 0.1350162923336029, acc: 0.9580838084220886)
[2025-02-13 19:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53][root][INFO] - Training Epoch: 1/2, step 5060/7134 completed (loss: 0.17649739980697632, acc: 0.9733333587646484)
[2025-02-13 19:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53][root][INFO] - Training Epoch: 1/2, step 5061/7134 completed (loss: 0.2846478819847107, acc: 0.9428571462631226)
[2025-02-13 19:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54][root][INFO] - Training Epoch: 1/2, step 5062/7134 completed (loss: 0.11600451916456223, acc: 0.9629629850387573)
[2025-02-13 19:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54][root][INFO] - Training Epoch: 1/2, step 5063/7134 completed (loss: 0.18551285564899445, acc: 0.9510489702224731)
[2025-02-13 19:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55][root][INFO] - Training Epoch: 1/2, step 5064/7134 completed (loss: 0.12548740208148956, acc: 0.9642857313156128)
[2025-02-13 19:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55][root][INFO] - Training Epoch: 1/2, step 5065/7134 completed (loss: 0.18932265043258667, acc: 0.9622641801834106)
[2025-02-13 19:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55][root][INFO] - Training Epoch: 1/2, step 5066/7134 completed (loss: 0.16257327795028687, acc: 0.9448275566101074)
[2025-02-13 19:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56][root][INFO] - Training Epoch: 1/2, step 5067/7134 completed (loss: 0.03985201194882393, acc: 1.0)
[2025-02-13 19:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56][root][INFO] - Training Epoch: 1/2, step 5068/7134 completed (loss: 0.19457916915416718, acc: 0.9507042169570923)
[2025-02-13 19:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56][root][INFO] - Training Epoch: 1/2, step 5069/7134 completed (loss: 0.0967140644788742, acc: 0.9729729890823364)
[2025-02-13 19:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57][root][INFO] - Training Epoch: 1/2, step 5070/7134 completed (loss: 0.026703527197241783, acc: 1.0)
[2025-02-13 19:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57][root][INFO] - Training Epoch: 1/2, step 5071/7134 completed (loss: 0.09962230175733566, acc: 0.9764705896377563)
[2025-02-13 19:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57][root][INFO] - Training Epoch: 1/2, step 5072/7134 completed (loss: 0.2365829348564148, acc: 0.9450549483299255)
[2025-02-13 19:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58][root][INFO] - Training Epoch: 1/2, step 5073/7134 completed (loss: 0.14953075349330902, acc: 0.9541984796524048)
[2025-02-13 19:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58][root][INFO] - Training Epoch: 1/2, step 5074/7134 completed (loss: 0.21327389776706696, acc: 0.9457364082336426)
[2025-02-13 19:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59][root][INFO] - Training Epoch: 1/2, step 5075/7134 completed (loss: 0.18541941046714783, acc: 0.95652174949646)
[2025-02-13 19:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59][root][INFO] - Training Epoch: 1/2, step 5076/7134 completed (loss: 0.14506877958774567, acc: 0.9462365508079529)
[2025-02-13 19:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59][root][INFO] - Training Epoch: 1/2, step 5077/7134 completed (loss: 0.0920478031039238, acc: 0.970588207244873)
[2025-02-13 19:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:00][root][INFO] - Training Epoch: 1/2, step 5078/7134 completed (loss: 0.3323652148246765, acc: 0.9333333373069763)
[2025-02-13 19:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:00][root][INFO] - Training Epoch: 1/2, step 5079/7134 completed (loss: 0.14303290843963623, acc: 0.9666666388511658)
[2025-02-13 19:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01][root][INFO] - Training Epoch: 1/2, step 5080/7134 completed (loss: 0.05629601329565048, acc: 0.9803921580314636)
[2025-02-13 19:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01][root][INFO] - Training Epoch: 1/2, step 5081/7134 completed (loss: 0.08693845570087433, acc: 0.9653179049491882)
[2025-02-13 19:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01][root][INFO] - Training Epoch: 1/2, step 5082/7134 completed (loss: 0.11148037016391754, acc: 0.9793103337287903)
[2025-02-13 19:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02][root][INFO] - Training Epoch: 1/2, step 5083/7134 completed (loss: 0.18360236287117004, acc: 0.9644970297813416)
[2025-02-13 19:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02][root][INFO] - Training Epoch: 1/2, step 5084/7134 completed (loss: 0.135285422205925, acc: 0.9653465151786804)
[2025-02-13 19:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02][root][INFO] - Training Epoch: 1/2, step 5085/7134 completed (loss: 0.1358935683965683, acc: 0.969072163105011)
[2025-02-13 19:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:03][root][INFO] - Training Epoch: 1/2, step 5086/7134 completed (loss: 0.09973353892564774, acc: 0.9668508172035217)
[2025-02-13 19:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:03][root][INFO] - Training Epoch: 1/2, step 5087/7134 completed (loss: 0.063070148229599, acc: 0.9837837815284729)
[2025-02-13 19:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04][root][INFO] - Training Epoch: 1/2, step 5088/7134 completed (loss: 0.023820316419005394, acc: 1.0)
[2025-02-13 19:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04][root][INFO] - Training Epoch: 1/2, step 5089/7134 completed (loss: 0.08180397748947144, acc: 0.9897959232330322)
[2025-02-13 19:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04][root][INFO] - Training Epoch: 1/2, step 5090/7134 completed (loss: 0.03866216167807579, acc: 0.9902912378311157)
[2025-02-13 19:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05][root][INFO] - Training Epoch: 1/2, step 5091/7134 completed (loss: 0.0985216274857521, acc: 0.9615384340286255)
[2025-02-13 19:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05][root][INFO] - Training Epoch: 1/2, step 5092/7134 completed (loss: 0.1247665286064148, acc: 0.9728260636329651)
[2025-02-13 19:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05][root][INFO] - Training Epoch: 1/2, step 5093/7134 completed (loss: 0.109598807990551, acc: 0.9659090638160706)
[2025-02-13 19:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:06][root][INFO] - Training Epoch: 1/2, step 5094/7134 completed (loss: 0.08140664547681808, acc: 0.9923076629638672)
[2025-02-13 19:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:06][root][INFO] - Training Epoch: 1/2, step 5095/7134 completed (loss: 0.2778457701206207, acc: 0.9299362897872925)
[2025-02-13 19:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07][root][INFO] - Training Epoch: 1/2, step 5096/7134 completed (loss: 0.1916474550962448, acc: 0.9414893388748169)
[2025-02-13 19:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07][root][INFO] - Training Epoch: 1/2, step 5097/7134 completed (loss: 0.14704324305057526, acc: 0.9679144620895386)
[2025-02-13 19:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07][root][INFO] - Training Epoch: 1/2, step 5098/7134 completed (loss: 0.17559412121772766, acc: 0.9489796161651611)
[2025-02-13 19:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08][root][INFO] - Training Epoch: 1/2, step 5099/7134 completed (loss: 0.155069962143898, acc: 0.9629629850387573)
[2025-02-13 19:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08][root][INFO] - Training Epoch: 1/2, step 5100/7134 completed (loss: 0.10816076397895813, acc: 0.9607843160629272)
[2025-02-13 19:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08][root][INFO] - Training Epoch: 1/2, step 5101/7134 completed (loss: 0.07161920517683029, acc: 0.9738219976425171)
[2025-02-13 19:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09][root][INFO] - Training Epoch: 1/2, step 5102/7134 completed (loss: 0.16104881465435028, acc: 0.9512194991111755)
[2025-02-13 19:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09][root][INFO] - Training Epoch: 1/2, step 5103/7134 completed (loss: 0.11077211797237396, acc: 0.976190447807312)
[2025-02-13 19:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10][root][INFO] - Training Epoch: 1/2, step 5104/7134 completed (loss: 0.08687278628349304, acc: 0.9736841917037964)
[2025-02-13 19:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10][root][INFO] - Training Epoch: 1/2, step 5105/7134 completed (loss: 0.17677819728851318, acc: 0.9668874144554138)
[2025-02-13 19:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10][root][INFO] - Training Epoch: 1/2, step 5106/7134 completed (loss: 0.19535984098911285, acc: 0.9520547986030579)
[2025-02-13 19:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:11][root][INFO] - Training Epoch: 1/2, step 5107/7134 completed (loss: 0.34089571237564087, acc: 0.9047619104385376)
[2025-02-13 19:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:11][root][INFO] - Training Epoch: 1/2, step 5108/7134 completed (loss: 0.43110713362693787, acc: 0.8723404407501221)
[2025-02-13 19:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12][root][INFO] - Training Epoch: 1/2, step 5109/7134 completed (loss: 0.4451909363269806, acc: 0.8894472122192383)
[2025-02-13 19:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12][root][INFO] - Training Epoch: 1/2, step 5110/7134 completed (loss: 0.2507777810096741, acc: 0.9395604133605957)
[2025-02-13 19:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12][root][INFO] - Training Epoch: 1/2, step 5111/7134 completed (loss: 0.2215823531150818, acc: 0.931034505367279)
[2025-02-13 19:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13][root][INFO] - Training Epoch: 1/2, step 5112/7134 completed (loss: 0.2760053873062134, acc: 0.9459459185600281)
[2025-02-13 19:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13][root][INFO] - Training Epoch: 1/2, step 5113/7134 completed (loss: 0.3526427745819092, acc: 0.9139072895050049)
[2025-02-13 19:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13][root][INFO] - Training Epoch: 1/2, step 5114/7134 completed (loss: 0.11583001166582108, acc: 0.9668874144554138)
[2025-02-13 19:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:14][root][INFO] - Training Epoch: 1/2, step 5115/7134 completed (loss: 0.13254211843013763, acc: 0.9576719403266907)
[2025-02-13 19:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:14][root][INFO] - Training Epoch: 1/2, step 5116/7134 completed (loss: 0.12332332134246826, acc: 0.9735449552536011)
[2025-02-13 19:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15][root][INFO] - Training Epoch: 1/2, step 5117/7134 completed (loss: 0.10342299938201904, acc: 0.9723756909370422)
[2025-02-13 19:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15][root][INFO] - Training Epoch: 1/2, step 5118/7134 completed (loss: 0.1398245245218277, acc: 0.9759036302566528)
[2025-02-13 19:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15][root][INFO] - Training Epoch: 1/2, step 5119/7134 completed (loss: 0.09430162608623505, acc: 0.9885057210922241)
[2025-02-13 19:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16][root][INFO] - Training Epoch: 1/2, step 5120/7134 completed (loss: 0.1872926950454712, acc: 0.9365079402923584)
[2025-02-13 19:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16][root][INFO] - Training Epoch: 1/2, step 5121/7134 completed (loss: 0.06588643044233322, acc: 0.9818181991577148)
[2025-02-13 19:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17][root][INFO] - Training Epoch: 1/2, step 5122/7134 completed (loss: 0.25175583362579346, acc: 0.9127907156944275)
[2025-02-13 19:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17][root][INFO] - Training Epoch: 1/2, step 5123/7134 completed (loss: 0.09935632348060608, acc: 0.9801980257034302)
[2025-02-13 19:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17][root][INFO] - Training Epoch: 1/2, step 5124/7134 completed (loss: 0.1659686267375946, acc: 0.9556650519371033)
[2025-02-13 19:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18][root][INFO] - Training Epoch: 1/2, step 5125/7134 completed (loss: 0.05249428749084473, acc: 0.9803921580314636)
[2025-02-13 19:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18][root][INFO] - Training Epoch: 1/2, step 5126/7134 completed (loss: 0.14854364097118378, acc: 0.9793103337287903)
[2025-02-13 19:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18][root][INFO] - Training Epoch: 1/2, step 5127/7134 completed (loss: 0.3649542033672333, acc: 0.931034505367279)
[2025-02-13 19:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:19][root][INFO] - Training Epoch: 1/2, step 5128/7134 completed (loss: 0.08611894398927689, acc: 0.9784482717514038)
[2025-02-13 19:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:19][root][INFO] - Training Epoch: 1/2, step 5129/7134 completed (loss: 0.23405465483665466, acc: 0.9534883499145508)
[2025-02-13 19:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20][root][INFO] - Training Epoch: 1/2, step 5130/7134 completed (loss: 0.0762118324637413, acc: 0.9805194735527039)
[2025-02-13 19:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20][root][INFO] - Training Epoch: 1/2, step 5131/7134 completed (loss: 0.25065264105796814, acc: 0.9551281929016113)
[2025-02-13 19:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20][root][INFO] - Training Epoch: 1/2, step 5132/7134 completed (loss: 0.20627552270889282, acc: 0.9453125)
[2025-02-13 19:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21][root][INFO] - Training Epoch: 1/2, step 5133/7134 completed (loss: 0.19116073846817017, acc: 0.9451219439506531)
[2025-02-13 19:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21][root][INFO] - Training Epoch: 1/2, step 5134/7134 completed (loss: 0.14090466499328613, acc: 0.9492753744125366)
[2025-02-13 19:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21][root][INFO] - Training Epoch: 1/2, step 5135/7134 completed (loss: 0.23367172479629517, acc: 0.9768785834312439)
[2025-02-13 19:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22][root][INFO] - Training Epoch: 1/2, step 5136/7134 completed (loss: 0.11047077924013138, acc: 0.9677419066429138)
[2025-02-13 19:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22][root][INFO] - Training Epoch: 1/2, step 5137/7134 completed (loss: 0.316546767950058, acc: 0.9492753744125366)
[2025-02-13 19:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22][root][INFO] - Training Epoch: 1/2, step 5138/7134 completed (loss: 0.371698260307312, acc: 0.9468085169792175)
[2025-02-13 19:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23][root][INFO] - Training Epoch: 1/2, step 5139/7134 completed (loss: 0.18096359074115753, acc: 0.9529411792755127)
[2025-02-13 19:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23][root][INFO] - Training Epoch: 1/2, step 5140/7134 completed (loss: 0.25954899191856384, acc: 0.9375)
[2025-02-13 19:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24][root][INFO] - Training Epoch: 1/2, step 5141/7134 completed (loss: 0.23887787759304047, acc: 0.9508196711540222)
[2025-02-13 19:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24][root][INFO] - Training Epoch: 1/2, step 5142/7134 completed (loss: 0.10919032245874405, acc: 0.9800994992256165)
[2025-02-13 19:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24][root][INFO] - Training Epoch: 1/2, step 5143/7134 completed (loss: 0.16799508035182953, acc: 0.9552238583564758)
[2025-02-13 19:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25][root][INFO] - Training Epoch: 1/2, step 5144/7134 completed (loss: 0.08942798525094986, acc: 0.9735449552536011)
[2025-02-13 19:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25][root][INFO] - Training Epoch: 1/2, step 5145/7134 completed (loss: 0.08770812302827835, acc: 0.9791666865348816)
[2025-02-13 19:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25][root][INFO] - Training Epoch: 1/2, step 5146/7134 completed (loss: 0.14401815831661224, acc: 0.9726775884628296)
[2025-02-13 19:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26][root][INFO] - Training Epoch: 1/2, step 5147/7134 completed (loss: 0.08992976695299149, acc: 0.9679487347602844)
[2025-02-13 19:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26][root][INFO] - Training Epoch: 1/2, step 5148/7134 completed (loss: 0.23710551857948303, acc: 0.9437500238418579)
[2025-02-13 19:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26][root][INFO] - Training Epoch: 1/2, step 5149/7134 completed (loss: 0.10222739726305008, acc: 0.9728506803512573)
[2025-02-13 19:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:27][root][INFO] - Training Epoch: 1/2, step 5150/7134 completed (loss: 0.19164063036441803, acc: 0.9640718698501587)
[2025-02-13 19:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:27][root][INFO] - Training Epoch: 1/2, step 5151/7134 completed (loss: 0.20665381848812103, acc: 0.954285740852356)
[2025-02-13 19:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28][root][INFO] - Training Epoch: 1/2, step 5152/7134 completed (loss: 0.09478852897882462, acc: 0.9764705896377563)
[2025-02-13 19:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28][root][INFO] - Training Epoch: 1/2, step 5153/7134 completed (loss: 0.15133124589920044, acc: 0.9542483687400818)
[2025-02-13 19:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28][root][INFO] - Training Epoch: 1/2, step 5154/7134 completed (loss: 0.09024964272975922, acc: 0.9750000238418579)
[2025-02-13 19:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29][root][INFO] - Training Epoch: 1/2, step 5155/7134 completed (loss: 0.1863512396812439, acc: 0.9635036587715149)
[2025-02-13 19:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29][root][INFO] - Training Epoch: 1/2, step 5156/7134 completed (loss: 0.12157929688692093, acc: 0.9722222089767456)
[2025-02-13 19:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29][root][INFO] - Training Epoch: 1/2, step 5157/7134 completed (loss: 0.12351493537425995, acc: 0.9772727489471436)
[2025-02-13 19:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:30][root][INFO] - Training Epoch: 1/2, step 5158/7134 completed (loss: 0.2514243423938751, acc: 0.9358974099159241)
[2025-02-13 19:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:30][root][INFO] - Training Epoch: 1/2, step 5159/7134 completed (loss: 0.17235618829727173, acc: 0.9595959782600403)
[2025-02-13 19:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31][root][INFO] - Training Epoch: 1/2, step 5160/7134 completed (loss: 0.22790637612342834, acc: 0.9388889074325562)
[2025-02-13 19:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31][root][INFO] - Training Epoch: 1/2, step 5161/7134 completed (loss: 0.28606918454170227, acc: 0.9193548560142517)
[2025-02-13 19:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31][root][INFO] - Training Epoch: 1/2, step 5162/7134 completed (loss: 0.256680965423584, acc: 0.9200000166893005)
[2025-02-13 19:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32][root][INFO] - Training Epoch: 1/2, step 5163/7134 completed (loss: 0.1280234009027481, acc: 0.9677419066429138)
[2025-02-13 19:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32][root][INFO] - Training Epoch: 1/2, step 5164/7134 completed (loss: 0.09073061496019363, acc: 0.9806451797485352)
[2025-02-13 19:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32][root][INFO] - Training Epoch: 1/2, step 5165/7134 completed (loss: 0.18399813771247864, acc: 0.9575757384300232)
[2025-02-13 19:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33][root][INFO] - Training Epoch: 1/2, step 5166/7134 completed (loss: 0.19464848935604095, acc: 0.9320987462997437)
[2025-02-13 19:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33][root][INFO] - Training Epoch: 1/2, step 5167/7134 completed (loss: 0.24008680880069733, acc: 0.9447513818740845)
[2025-02-13 19:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33][root][INFO] - Training Epoch: 1/2, step 5168/7134 completed (loss: 0.35115107893943787, acc: 0.9244186282157898)
[2025-02-13 19:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34][root][INFO] - Training Epoch: 1/2, step 5169/7134 completed (loss: 0.2068374902009964, acc: 0.9512194991111755)
[2025-02-13 19:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34][root][INFO] - Training Epoch: 1/2, step 5170/7134 completed (loss: 0.3273073732852936, acc: 0.9274611473083496)
[2025-02-13 19:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35][root][INFO] - Training Epoch: 1/2, step 5171/7134 completed (loss: 0.15278086066246033, acc: 0.9756097793579102)
[2025-02-13 19:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35][root][INFO] - Training Epoch: 1/2, step 5172/7134 completed (loss: 0.44831857085227966, acc: 0.8963414430618286)
[2025-02-13 19:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35][root][INFO] - Training Epoch: 1/2, step 5173/7134 completed (loss: 0.18563398718833923, acc: 0.9570552110671997)
[2025-02-13 19:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36][root][INFO] - Training Epoch: 1/2, step 5174/7134 completed (loss: 0.10309139639139175, acc: 0.9710982441902161)
[2025-02-13 19:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36][root][INFO] - Training Epoch: 1/2, step 5175/7134 completed (loss: 0.14200732111930847, acc: 0.9640718698501587)
[2025-02-13 19:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36][root][INFO] - Training Epoch: 1/2, step 5176/7134 completed (loss: 0.13225026428699493, acc: 0.9605262875556946)
[2025-02-13 19:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37][root][INFO] - Training Epoch: 1/2, step 5177/7134 completed (loss: 0.24910610914230347, acc: 0.9599999785423279)
[2025-02-13 19:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37][root][INFO] - Training Epoch: 1/2, step 5178/7134 completed (loss: 0.4452834129333496, acc: 0.9333333373069763)
[2025-02-13 19:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38][root][INFO] - Training Epoch: 1/2, step 5179/7134 completed (loss: 0.2287018895149231, acc: 0.9464285969734192)
[2025-02-13 19:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38][root][INFO] - Training Epoch: 1/2, step 5180/7134 completed (loss: 0.2904911935329437, acc: 0.936170220375061)
[2025-02-13 19:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38][root][INFO] - Training Epoch: 1/2, step 5181/7134 completed (loss: 0.4928050637245178, acc: 0.8876404762268066)
[2025-02-13 19:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39][root][INFO] - Training Epoch: 1/2, step 5182/7134 completed (loss: 0.6839260458946228, acc: 0.8897637724876404)
[2025-02-13 19:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39][root][INFO] - Training Epoch: 1/2, step 5183/7134 completed (loss: 0.26121270656585693, acc: 0.9485294222831726)
[2025-02-13 19:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39][root][INFO] - Training Epoch: 1/2, step 5184/7134 completed (loss: 0.1500035673379898, acc: 0.9711538553237915)
[2025-02-13 19:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:40][root][INFO] - Training Epoch: 1/2, step 5185/7134 completed (loss: 0.2154318243265152, acc: 0.9504950642585754)
[2025-02-13 19:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:40][root][INFO] - Training Epoch: 1/2, step 5186/7134 completed (loss: 0.1613103449344635, acc: 0.960629940032959)
[2025-02-13 19:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41][root][INFO] - Training Epoch: 1/2, step 5187/7134 completed (loss: 0.09217587858438492, acc: 0.984000027179718)
[2025-02-13 19:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41][root][INFO] - Training Epoch: 1/2, step 5188/7134 completed (loss: 0.24172843992710114, acc: 0.9420289993286133)
[2025-02-13 19:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41][root][INFO] - Training Epoch: 1/2, step 5189/7134 completed (loss: 0.2718783915042877, acc: 0.9702970385551453)
[2025-02-13 19:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42][root][INFO] - Training Epoch: 1/2, step 5190/7134 completed (loss: 0.14527517557144165, acc: 0.9636363387107849)
[2025-02-13 19:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42][root][INFO] - Training Epoch: 1/2, step 5191/7134 completed (loss: 0.24468129873275757, acc: 0.9320987462997437)
[2025-02-13 19:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42][root][INFO] - Training Epoch: 1/2, step 5192/7134 completed (loss: 0.1794721484184265, acc: 0.9696969985961914)
[2025-02-13 19:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43][root][INFO] - Training Epoch: 1/2, step 5193/7134 completed (loss: 0.1541270613670349, acc: 0.9583333134651184)
[2025-02-13 19:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43][root][INFO] - Training Epoch: 1/2, step 5194/7134 completed (loss: 0.46454405784606934, acc: 0.9108280539512634)
[2025-02-13 19:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43][root][INFO] - Training Epoch: 1/2, step 5195/7134 completed (loss: 0.2945726811885834, acc: 0.9351851940155029)
[2025-02-13 19:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44][root][INFO] - Training Epoch: 1/2, step 5196/7134 completed (loss: 0.3412362337112427, acc: 0.9300000071525574)
[2025-02-13 19:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44][root][INFO] - Training Epoch: 1/2, step 5197/7134 completed (loss: 0.23272605240345, acc: 0.920634925365448)
[2025-02-13 19:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44][root][INFO] - Training Epoch: 1/2, step 5198/7134 completed (loss: 0.18199265003204346, acc: 0.9672130942344666)
[2025-02-13 19:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:45][root][INFO] - Training Epoch: 1/2, step 5199/7134 completed (loss: 0.3599420487880707, acc: 0.9186046719551086)
[2025-02-13 19:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:45][root][INFO] - Training Epoch: 1/2, step 5200/7134 completed (loss: 0.15608185529708862, acc: 0.9603174328804016)
[2025-02-13 19:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46][root][INFO] - Training Epoch: 1/2, step 5201/7134 completed (loss: 0.3295234739780426, acc: 0.9323671460151672)
[2025-02-13 19:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46][root][INFO] - Training Epoch: 1/2, step 5202/7134 completed (loss: 0.15479221940040588, acc: 0.9739583134651184)
[2025-02-13 19:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46][root][INFO] - Training Epoch: 1/2, step 5203/7134 completed (loss: 0.10015557706356049, acc: 0.9725274443626404)
[2025-02-13 19:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47][root][INFO] - Training Epoch: 1/2, step 5204/7134 completed (loss: 0.05226786434650421, acc: 1.0)
[2025-02-13 19:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47][root][INFO] - Training Epoch: 1/2, step 5205/7134 completed (loss: 0.12965716421604156, acc: 0.9638554453849792)
[2025-02-13 19:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47][root][INFO] - Training Epoch: 1/2, step 5206/7134 completed (loss: 0.17206168174743652, acc: 0.9590643048286438)
[2025-02-13 19:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48][root][INFO] - Training Epoch: 1/2, step 5207/7134 completed (loss: 0.5643322467803955, acc: 0.89682537317276)
[2025-02-13 19:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48][root][INFO] - Training Epoch: 1/2, step 5208/7134 completed (loss: 0.13074727356433868, acc: 0.9655172228813171)
[2025-02-13 19:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49][root][INFO] - Training Epoch: 1/2, step 5209/7134 completed (loss: 0.26705801486968994, acc: 0.9248120188713074)
[2025-02-13 19:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49][root][INFO] - Training Epoch: 1/2, step 5210/7134 completed (loss: 0.1136443018913269, acc: 0.9644970297813416)
[2025-02-13 19:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49][root][INFO] - Training Epoch: 1/2, step 5211/7134 completed (loss: 0.17917466163635254, acc: 0.9603174328804016)
[2025-02-13 19:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50][root][INFO] - Training Epoch: 1/2, step 5212/7134 completed (loss: 0.21922430396080017, acc: 0.9220778942108154)
[2025-02-13 19:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50][root][INFO] - Training Epoch: 1/2, step 5213/7134 completed (loss: 0.18977537751197815, acc: 0.9629629850387573)
[2025-02-13 19:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50][root][INFO] - Training Epoch: 1/2, step 5214/7134 completed (loss: 0.36341559886932373, acc: 0.9159663915634155)
[2025-02-13 19:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:51][root][INFO] - Training Epoch: 1/2, step 5215/7134 completed (loss: 0.3414076864719391, acc: 0.9306930899620056)
[2025-02-13 19:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:51][root][INFO] - Training Epoch: 1/2, step 5216/7134 completed (loss: 0.23834262788295746, acc: 0.9305555820465088)
[2025-02-13 19:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52][root][INFO] - Training Epoch: 1/2, step 5217/7134 completed (loss: 0.417949914932251, acc: 0.9239130616188049)
[2025-02-13 19:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52][root][INFO] - Training Epoch: 1/2, step 5218/7134 completed (loss: 0.30559778213500977, acc: 0.9207921028137207)
[2025-02-13 19:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52][root][INFO] - Training Epoch: 1/2, step 5219/7134 completed (loss: 0.24023644626140594, acc: 0.9444444179534912)
[2025-02-13 19:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53][root][INFO] - Training Epoch: 1/2, step 5220/7134 completed (loss: 0.15500590205192566, acc: 0.9383561611175537)
[2025-02-13 19:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53][root][INFO] - Training Epoch: 1/2, step 5221/7134 completed (loss: 0.149346262216568, acc: 0.9691358208656311)
[2025-02-13 19:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53][root][INFO] - Training Epoch: 1/2, step 5222/7134 completed (loss: 0.09558278322219849, acc: 0.976190447807312)
[2025-02-13 19:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54][root][INFO] - Training Epoch: 1/2, step 5223/7134 completed (loss: 0.1717001348733902, acc: 0.9557521939277649)
[2025-02-13 19:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54][root][INFO] - Training Epoch: 1/2, step 5224/7134 completed (loss: 0.16271597146987915, acc: 0.9629629850387573)
[2025-02-13 19:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55][root][INFO] - Training Epoch: 1/2, step 5225/7134 completed (loss: 0.15537965297698975, acc: 0.9671052694320679)
[2025-02-13 19:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55][root][INFO] - Training Epoch: 1/2, step 5226/7134 completed (loss: 0.24145081639289856, acc: 0.942307710647583)
[2025-02-13 19:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55][root][INFO] - Training Epoch: 1/2, step 5227/7134 completed (loss: 0.1832149773836136, acc: 0.9487179517745972)
[2025-02-13 19:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56][root][INFO] - Training Epoch: 1/2, step 5228/7134 completed (loss: 0.1088380292057991, acc: 0.9599999785423279)
[2025-02-13 19:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56][root][INFO] - Training Epoch: 1/2, step 5229/7134 completed (loss: 0.05359853431582451, acc: 0.9806451797485352)
[2025-02-13 19:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56][root][INFO] - Training Epoch: 1/2, step 5230/7134 completed (loss: 0.07962597906589508, acc: 0.9723756909370422)
[2025-02-13 19:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57][root][INFO] - Training Epoch: 1/2, step 5231/7134 completed (loss: 0.18043962121009827, acc: 0.9639175534248352)
[2025-02-13 19:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57][root][INFO] - Training Epoch: 1/2, step 5232/7134 completed (loss: 0.12655967473983765, acc: 0.9661017060279846)
[2025-02-13 19:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58][root][INFO] - Training Epoch: 1/2, step 5233/7134 completed (loss: 0.14987313747406006, acc: 0.9541284441947937)
[2025-02-13 19:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58][root][INFO] - Training Epoch: 1/2, step 5234/7134 completed (loss: 0.10667350143194199, acc: 0.9736841917037964)
[2025-02-13 19:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58][root][INFO] - Training Epoch: 1/2, step 5235/7134 completed (loss: 0.03252488374710083, acc: 1.0)
[2025-02-13 19:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59][root][INFO] - Training Epoch: 1/2, step 5236/7134 completed (loss: 0.02337292954325676, acc: 0.9927007555961609)
[2025-02-13 19:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59][root][INFO] - Training Epoch: 1/2, step 5237/7134 completed (loss: 0.06833066046237946, acc: 0.98591548204422)
[2025-02-13 19:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59][root][INFO] - Training Epoch: 1/2, step 5238/7134 completed (loss: 0.06521356850862503, acc: 0.9925373196601868)
[2025-02-13 20:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:00][root][INFO] - Training Epoch: 1/2, step 5239/7134 completed (loss: 0.19895726442337036, acc: 0.9411764740943909)
[2025-02-13 20:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:00][root][INFO] - Training Epoch: 1/2, step 5240/7134 completed (loss: 0.07343266904354095, acc: 0.9941860437393188)
[2025-02-13 20:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01][root][INFO] - Training Epoch: 1/2, step 5241/7134 completed (loss: 0.21937470138072968, acc: 0.9548386931419373)
[2025-02-13 20:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01][root][INFO] - Training Epoch: 1/2, step 5242/7134 completed (loss: 0.09069322794675827, acc: 0.9736841917037964)
[2025-02-13 20:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01][root][INFO] - Training Epoch: 1/2, step 5243/7134 completed (loss: 0.03763395547866821, acc: 0.9928571581840515)
[2025-02-13 20:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02][root][INFO] - Training Epoch: 1/2, step 5244/7134 completed (loss: 0.16254280507564545, acc: 0.9593023061752319)
[2025-02-13 20:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02][root][INFO] - Training Epoch: 1/2, step 5245/7134 completed (loss: 0.08279795199632645, acc: 0.9920634627342224)
[2025-02-13 20:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02][root][INFO] - Training Epoch: 1/2, step 5246/7134 completed (loss: 0.13800010085105896, acc: 0.949367105960846)
[2025-02-13 20:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03][root][INFO] - Training Epoch: 1/2, step 5247/7134 completed (loss: 0.1450243890285492, acc: 0.9629629850387573)
[2025-02-13 20:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03][root][INFO] - Training Epoch: 1/2, step 5248/7134 completed (loss: 0.11067257076501846, acc: 0.9923664331436157)
[2025-02-13 20:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03][root][INFO] - Training Epoch: 1/2, step 5249/7134 completed (loss: 0.10224958509206772, acc: 0.9745222926139832)
[2025-02-13 20:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:04][root][INFO] - Training Epoch: 1/2, step 5250/7134 completed (loss: 0.10358886420726776, acc: 0.9728260636329651)
[2025-02-13 20:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:04][root][INFO] - Training Epoch: 1/2, step 5251/7134 completed (loss: 0.11529656499624252, acc: 0.9635416865348816)
[2025-02-13 20:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05][root][INFO] - Training Epoch: 1/2, step 5252/7134 completed (loss: 0.21882645785808563, acc: 0.9469026327133179)
[2025-02-13 20:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05][root][INFO] - Training Epoch: 1/2, step 5253/7134 completed (loss: 0.06288638710975647, acc: 0.9861111044883728)
[2025-02-13 20:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05][root][INFO] - Training Epoch: 1/2, step 5254/7134 completed (loss: 0.3493320345878601, acc: 0.9090909361839294)
[2025-02-13 20:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06][root][INFO] - Training Epoch: 1/2, step 5255/7134 completed (loss: 0.1062302216887474, acc: 0.9655172228813171)
[2025-02-13 20:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06][root][INFO] - Training Epoch: 1/2, step 5256/7134 completed (loss: 0.24258778989315033, acc: 0.9466666579246521)
[2025-02-13 20:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06][root][INFO] - Training Epoch: 1/2, step 5257/7134 completed (loss: 0.36220476031303406, acc: 0.9281437397003174)
[2025-02-13 20:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07][root][INFO] - Training Epoch: 1/2, step 5258/7134 completed (loss: 0.25612685084342957, acc: 0.9417989253997803)
[2025-02-13 20:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07][root][INFO] - Training Epoch: 1/2, step 5259/7134 completed (loss: 0.2864392399787903, acc: 0.935251772403717)
[2025-02-13 20:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07][root][INFO] - Training Epoch: 1/2, step 5260/7134 completed (loss: 0.15460394322872162, acc: 0.9576719403266907)
[2025-02-13 20:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:08][root][INFO] - Training Epoch: 1/2, step 5261/7134 completed (loss: 0.2447260022163391, acc: 0.9580419659614563)
[2025-02-13 20:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:08][root][INFO] - Training Epoch: 1/2, step 5262/7134 completed (loss: 0.1039809063076973, acc: 0.9638554453849792)
[2025-02-13 20:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09][root][INFO] - Training Epoch: 1/2, step 5263/7134 completed (loss: 0.15006254613399506, acc: 0.9647887349128723)
[2025-02-13 20:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09][root][INFO] - Training Epoch: 1/2, step 5264/7134 completed (loss: 0.11920209974050522, acc: 0.9693877696990967)
[2025-02-13 20:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09][root][INFO] - Training Epoch: 1/2, step 5265/7134 completed (loss: 0.19432595372200012, acc: 0.9562841653823853)
[2025-02-13 20:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10][root][INFO] - Training Epoch: 1/2, step 5266/7134 completed (loss: 0.04305518418550491, acc: 0.9876543283462524)
[2025-02-13 20:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10][root][INFO] - Training Epoch: 1/2, step 5267/7134 completed (loss: 0.3937220871448517, acc: 0.9135802388191223)
[2025-02-13 20:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10][root][INFO] - Training Epoch: 1/2, step 5268/7134 completed (loss: 0.24607731401920319, acc: 0.9457364082336426)
[2025-02-13 20:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11][root][INFO] - Training Epoch: 1/2, step 5269/7134 completed (loss: 0.24964414536952972, acc: 0.9571428298950195)
[2025-02-13 20:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11][root][INFO] - Training Epoch: 1/2, step 5270/7134 completed (loss: 0.13214002549648285, acc: 0.9655172228813171)
[2025-02-13 20:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11][root][INFO] - Training Epoch: 1/2, step 5271/7134 completed (loss: 0.13948030769824982, acc: 0.9640287756919861)
[2025-02-13 20:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12][root][INFO] - Training Epoch: 1/2, step 5272/7134 completed (loss: 0.11735114455223083, acc: 0.9826589822769165)
[2025-02-13 20:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12][root][INFO] - Training Epoch: 1/2, step 5273/7134 completed (loss: 0.18267017602920532, acc: 0.9583333134651184)
[2025-02-13 20:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13][root][INFO] - Training Epoch: 1/2, step 5274/7134 completed (loss: 0.20760905742645264, acc: 0.9551281929016113)
[2025-02-13 20:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13][root][INFO] - Training Epoch: 1/2, step 5275/7134 completed (loss: 0.11579284071922302, acc: 0.9756097793579102)
[2025-02-13 20:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13][root][INFO] - Training Epoch: 1/2, step 5276/7134 completed (loss: 0.18757911026477814, acc: 0.9671052694320679)
[2025-02-13 20:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14][root][INFO] - Training Epoch: 1/2, step 5277/7134 completed (loss: 0.10951049625873566, acc: 0.9745222926139832)
[2025-02-13 20:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14][root][INFO] - Training Epoch: 1/2, step 5278/7134 completed (loss: 0.27710145711898804, acc: 0.9333333373069763)
[2025-02-13 20:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14][root][INFO] - Training Epoch: 1/2, step 5279/7134 completed (loss: 0.03961953520774841, acc: 0.9918699264526367)
[2025-02-13 20:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15][root][INFO] - Training Epoch: 1/2, step 5280/7134 completed (loss: 0.16151869297027588, acc: 0.9714285731315613)
[2025-02-13 20:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15][root][INFO] - Training Epoch: 1/2, step 5281/7134 completed (loss: 0.13149182498455048, acc: 0.9637681245803833)
[2025-02-13 20:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15][root][INFO] - Training Epoch: 1/2, step 5282/7134 completed (loss: 0.2223929762840271, acc: 0.9593023061752319)
[2025-02-13 20:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16][root][INFO] - Training Epoch: 1/2, step 5283/7134 completed (loss: 0.1528107225894928, acc: 0.9508196711540222)
[2025-02-13 20:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16][root][INFO] - Training Epoch: 1/2, step 5284/7134 completed (loss: 0.23403827846050262, acc: 0.96875)
[2025-02-13 20:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17][root][INFO] - Training Epoch: 1/2, step 5285/7134 completed (loss: 0.17282241582870483, acc: 0.9677419066429138)
[2025-02-13 20:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17][root][INFO] - Training Epoch: 1/2, step 5286/7134 completed (loss: 0.22292795777320862, acc: 0.9798657894134521)
[2025-02-13 20:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17][root][INFO] - Training Epoch: 1/2, step 5287/7134 completed (loss: 0.1993815153837204, acc: 0.9545454382896423)
[2025-02-13 20:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18][root][INFO] - Training Epoch: 1/2, step 5288/7134 completed (loss: 0.11782141029834747, acc: 0.96875)
[2025-02-13 20:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18][root][INFO] - Training Epoch: 1/2, step 5289/7134 completed (loss: 0.089118592441082, acc: 0.9715909361839294)
[2025-02-13 20:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19][root][INFO] - Training Epoch: 1/2, step 5290/7134 completed (loss: 0.22927923500537872, acc: 0.9576719403266907)
[2025-02-13 20:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19][root][INFO] - Training Epoch: 1/2, step 5291/7134 completed (loss: 0.06633210927248001, acc: 0.9837837815284729)
[2025-02-13 20:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19][root][INFO] - Training Epoch: 1/2, step 5292/7134 completed (loss: 0.09561122953891754, acc: 0.9830508232116699)
[2025-02-13 20:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20][root][INFO] - Training Epoch: 1/2, step 5293/7134 completed (loss: 0.10837607085704803, acc: 0.9681528806686401)
[2025-02-13 20:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20][root][INFO] - Training Epoch: 1/2, step 5294/7134 completed (loss: 0.1618628203868866, acc: 0.9526627063751221)
[2025-02-13 20:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20][root][INFO] - Training Epoch: 1/2, step 5295/7134 completed (loss: 0.1149350181221962, acc: 0.9767441749572754)
[2025-02-13 20:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21][root][INFO] - Training Epoch: 1/2, step 5296/7134 completed (loss: 0.1554153859615326, acc: 0.9661017060279846)
[2025-02-13 20:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21][root][INFO] - Training Epoch: 1/2, step 5297/7134 completed (loss: 0.11982555687427521, acc: 0.9638554453849792)
[2025-02-13 20:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22][root][INFO] - Training Epoch: 1/2, step 5298/7134 completed (loss: 0.13176967203617096, acc: 0.9503546357154846)
[2025-02-13 20:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22][root][INFO] - Training Epoch: 1/2, step 5299/7134 completed (loss: 0.044831134378910065, acc: 1.0)
[2025-02-13 20:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22][root][INFO] - Training Epoch: 1/2, step 5300/7134 completed (loss: 0.032860707491636276, acc: 0.9939758777618408)
[2025-02-13 20:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23][root][INFO] - Training Epoch: 1/2, step 5301/7134 completed (loss: 0.07861509919166565, acc: 0.9817073345184326)
[2025-02-13 20:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23][root][INFO] - Training Epoch: 1/2, step 5302/7134 completed (loss: 0.22921310365200043, acc: 0.9497206807136536)
[2025-02-13 20:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23][root][INFO] - Training Epoch: 1/2, step 5303/7134 completed (loss: 0.4107203185558319, acc: 0.9166666865348816)
[2025-02-13 20:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24][root][INFO] - Training Epoch: 1/2, step 5304/7134 completed (loss: 0.47022745013237, acc: 0.8835616707801819)
[2025-02-13 20:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24][root][INFO] - Training Epoch: 1/2, step 5305/7134 completed (loss: 0.2811133563518524, acc: 0.9375)
[2025-02-13 20:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24][root][INFO] - Training Epoch: 1/2, step 5306/7134 completed (loss: 0.10864368826150894, acc: 0.9833333492279053)
[2025-02-13 20:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:25][root][INFO] - Training Epoch: 1/2, step 5307/7134 completed (loss: 0.4266263246536255, acc: 0.9239766001701355)
[2025-02-13 20:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:25][root][INFO] - Training Epoch: 1/2, step 5308/7134 completed (loss: 0.1825554519891739, acc: 0.9629629850387573)
[2025-02-13 20:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26][root][INFO] - Training Epoch: 1/2, step 5309/7134 completed (loss: 0.5427207946777344, acc: 0.9020978808403015)
[2025-02-13 20:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26][root][INFO] - Training Epoch: 1/2, step 5310/7134 completed (loss: 0.37829160690307617, acc: 0.9034090638160706)
[2025-02-13 20:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26][root][INFO] - Training Epoch: 1/2, step 5311/7134 completed (loss: 0.31266510486602783, acc: 0.9192546606063843)
[2025-02-13 20:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27][root][INFO] - Training Epoch: 1/2, step 5312/7134 completed (loss: 0.21813835203647614, acc: 0.9444444179534912)
[2025-02-13 20:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27][root][INFO] - Training Epoch: 1/2, step 5313/7134 completed (loss: 0.16557271778583527, acc: 0.9590163826942444)
[2025-02-13 20:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27][root][INFO] - Training Epoch: 1/2, step 5314/7134 completed (loss: 0.2673893868923187, acc: 0.9285714030265808)
[2025-02-13 20:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28][root][INFO] - Training Epoch: 1/2, step 5315/7134 completed (loss: 0.1857001930475235, acc: 0.9640287756919861)
[2025-02-13 20:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28][root][INFO] - Training Epoch: 1/2, step 5316/7134 completed (loss: 0.13662835955619812, acc: 0.95333331823349)
[2025-02-13 20:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28][root][INFO] - Training Epoch: 1/2, step 5317/7134 completed (loss: 0.15783365070819855, acc: 0.9523809552192688)
[2025-02-13 20:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29][root][INFO] - Training Epoch: 1/2, step 5318/7134 completed (loss: 0.0687464252114296, acc: 0.9808917045593262)
[2025-02-13 20:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29][root][INFO] - Training Epoch: 1/2, step 5319/7134 completed (loss: 0.2654714584350586, acc: 0.9248120188713074)
[2025-02-13 20:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30][root][INFO] - Training Epoch: 1/2, step 5320/7134 completed (loss: 0.25617074966430664, acc: 0.9371069073677063)
[2025-02-13 20:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30][root][INFO] - Training Epoch: 1/2, step 5321/7134 completed (loss: 0.19160231947898865, acc: 0.9610389471054077)
[2025-02-13 20:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30][root][INFO] - Training Epoch: 1/2, step 5322/7134 completed (loss: 0.1145268902182579, acc: 0.9599999785423279)
[2025-02-13 20:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31][root][INFO] - Training Epoch: 1/2, step 5323/7134 completed (loss: 0.19784855842590332, acc: 0.9194630980491638)
[2025-02-13 20:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31][root][INFO] - Training Epoch: 1/2, step 5324/7134 completed (loss: 0.1551819145679474, acc: 0.9671052694320679)
[2025-02-13 20:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31][root][INFO] - Training Epoch: 1/2, step 5325/7134 completed (loss: 0.12878482043743134, acc: 0.9710144996643066)
[2025-02-13 20:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32][root][INFO] - Training Epoch: 1/2, step 5326/7134 completed (loss: 0.25211721658706665, acc: 0.951724112033844)
[2025-02-13 20:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32][root][INFO] - Training Epoch: 1/2, step 5327/7134 completed (loss: 0.1574295610189438, acc: 0.9419354796409607)
[2025-02-13 20:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33][root][INFO] - Training Epoch: 1/2, step 5328/7134 completed (loss: 0.14032262563705444, acc: 0.969924807548523)
[2025-02-13 20:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33][root][INFO] - Training Epoch: 1/2, step 5329/7134 completed (loss: 0.16280950605869293, acc: 0.9599999785423279)
[2025-02-13 20:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33][root][INFO] - Training Epoch: 1/2, step 5330/7134 completed (loss: 0.2868383228778839, acc: 0.9240506291389465)
[2025-02-13 20:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34][root][INFO] - Training Epoch: 1/2, step 5331/7134 completed (loss: 0.16222667694091797, acc: 0.9777777791023254)
[2025-02-13 20:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34][root][INFO] - Training Epoch: 1/2, step 5332/7134 completed (loss: 0.1554465889930725, acc: 0.9520547986030579)
[2025-02-13 20:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34][root][INFO] - Training Epoch: 1/2, step 5333/7134 completed (loss: 0.09819459170103073, acc: 0.9671052694320679)
[2025-02-13 20:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35][root][INFO] - Training Epoch: 1/2, step 5334/7134 completed (loss: 0.21214161813259125, acc: 0.949999988079071)
[2025-02-13 20:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35][root][INFO] - Training Epoch: 1/2, step 5335/7134 completed (loss: 0.10823682695627213, acc: 0.9798657894134521)
[2025-02-13 20:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35][root][INFO] - Training Epoch: 1/2, step 5336/7134 completed (loss: 0.16042187809944153, acc: 0.9577465057373047)
[2025-02-13 20:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:36][root][INFO] - Training Epoch: 1/2, step 5337/7134 completed (loss: 0.23672083020210266, acc: 0.9485981464385986)
[2025-02-13 20:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:36][root][INFO] - Training Epoch: 1/2, step 5338/7134 completed (loss: 0.11774855107069016, acc: 0.9620853066444397)
[2025-02-13 20:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37][root][INFO] - Training Epoch: 1/2, step 5339/7134 completed (loss: 0.15466120839118958, acc: 0.9560439586639404)
[2025-02-13 20:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37][root][INFO] - Training Epoch: 1/2, step 5340/7134 completed (loss: 0.3465062975883484, acc: 0.899328887462616)
[2025-02-13 20:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37][root][INFO] - Training Epoch: 1/2, step 5341/7134 completed (loss: 0.14289888739585876, acc: 0.9649999737739563)
[2025-02-13 20:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38][root][INFO] - Training Epoch: 1/2, step 5342/7134 completed (loss: 0.3973751664161682, acc: 0.8736842274665833)
[2025-02-13 20:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38][root][INFO] - Training Epoch: 1/2, step 5343/7134 completed (loss: 0.20371077954769135, acc: 0.9513513445854187)
[2025-02-13 20:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38][root][INFO] - Training Epoch: 1/2, step 5344/7134 completed (loss: 0.262997031211853, acc: 0.9298245906829834)
[2025-02-13 20:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39][root][INFO] - Training Epoch: 1/2, step 5345/7134 completed (loss: 0.2094283401966095, acc: 0.9545454382896423)
[2025-02-13 20:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39][root][INFO] - Training Epoch: 1/2, step 5346/7134 completed (loss: 0.18825426697731018, acc: 0.9587156176567078)
[2025-02-13 20:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39][root][INFO] - Training Epoch: 1/2, step 5347/7134 completed (loss: 0.18987177312374115, acc: 0.9516128897666931)
[2025-02-13 20:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:40][root][INFO] - Training Epoch: 1/2, step 5348/7134 completed (loss: 0.25381630659103394, acc: 0.9216867685317993)
[2025-02-13 20:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2793, device='cuda:0') eval_epoch_loss=tensor(0.2463, device='cuda:0') eval_epoch_acc=tensor(0.9418, device='cuda:0')
[2025-02-13 20:04:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:04:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:04:33][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_5349_loss_0.2462950050830841/model.pt
[2025-02-13 20:04:33][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:04:33][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.2462950050830841
[2025-02-13 20:04:33][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9417964220046997
[2025-02-13 20:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:33][root][INFO] - Training Epoch: 1/2, step 5349/7134 completed (loss: 0.4334340989589691, acc: 0.8971428275108337)
[2025-02-13 20:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34][root][INFO] - Training Epoch: 1/2, step 5350/7134 completed (loss: 0.1777186393737793, acc: 0.9411764740943909)
[2025-02-13 20:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34][root][INFO] - Training Epoch: 1/2, step 5351/7134 completed (loss: 0.2618526220321655, acc: 0.929729700088501)
[2025-02-13 20:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34][root][INFO] - Training Epoch: 1/2, step 5352/7134 completed (loss: 0.2123231738805771, acc: 0.9580838084220886)
[2025-02-13 20:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:35][root][INFO] - Training Epoch: 1/2, step 5353/7134 completed (loss: 0.1638754904270172, acc: 0.9545454382896423)
[2025-02-13 20:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:35][root][INFO] - Training Epoch: 1/2, step 5354/7134 completed (loss: 0.2642911374568939, acc: 0.9305555820465088)
[2025-02-13 20:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36][root][INFO] - Training Epoch: 1/2, step 5355/7134 completed (loss: 0.2841666042804718, acc: 0.9270833134651184)
[2025-02-13 20:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36][root][INFO] - Training Epoch: 1/2, step 5356/7134 completed (loss: 0.24883690476417542, acc: 0.9308510422706604)
[2025-02-13 20:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36][root][INFO] - Training Epoch: 1/2, step 5357/7134 completed (loss: 0.17961421608924866, acc: 0.9503546357154846)
[2025-02-13 20:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37][root][INFO] - Training Epoch: 1/2, step 5358/7134 completed (loss: 0.434775710105896, acc: 0.9175823926925659)
[2025-02-13 20:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37][root][INFO] - Training Epoch: 1/2, step 5359/7134 completed (loss: 0.14544373750686646, acc: 0.9753694534301758)
[2025-02-13 20:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38][root][INFO] - Training Epoch: 1/2, step 5360/7134 completed (loss: 0.09728045016527176, acc: 0.9712643623352051)
[2025-02-13 20:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38][root][INFO] - Training Epoch: 1/2, step 5361/7134 completed (loss: 0.31130000948905945, acc: 0.9289340376853943)
[2025-02-13 20:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38][root][INFO] - Training Epoch: 1/2, step 5362/7134 completed (loss: 0.33387473225593567, acc: 0.9189189076423645)
[2025-02-13 20:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39][root][INFO] - Training Epoch: 1/2, step 5363/7134 completed (loss: 0.24564838409423828, acc: 0.9329268336296082)
[2025-02-13 20:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39][root][INFO] - Training Epoch: 1/2, step 5364/7134 completed (loss: 0.16352583467960358, acc: 0.9595375657081604)
[2025-02-13 20:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39][root][INFO] - Training Epoch: 1/2, step 5365/7134 completed (loss: 0.11937527358531952, acc: 0.9621211886405945)
[2025-02-13 20:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:40][root][INFO] - Training Epoch: 1/2, step 5366/7134 completed (loss: 0.27829691767692566, acc: 0.9626865386962891)
[2025-02-13 20:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:40][root][INFO] - Training Epoch: 1/2, step 5367/7134 completed (loss: 0.2078057825565338, acc: 0.967391312122345)
[2025-02-13 20:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41][root][INFO] - Training Epoch: 1/2, step 5368/7134 completed (loss: 0.2275143265724182, acc: 0.9457364082336426)
[2025-02-13 20:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41][root][INFO] - Training Epoch: 1/2, step 5369/7134 completed (loss: 0.16117703914642334, acc: 0.9745762944221497)
[2025-02-13 20:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41][root][INFO] - Training Epoch: 1/2, step 5370/7134 completed (loss: 0.2834119200706482, acc: 0.9545454382896423)
[2025-02-13 20:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42][root][INFO] - Training Epoch: 1/2, step 5371/7134 completed (loss: 0.34870949387550354, acc: 0.9266666769981384)
[2025-02-13 20:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42][root][INFO] - Training Epoch: 1/2, step 5372/7134 completed (loss: 0.077053502202034, acc: 0.9937106966972351)
[2025-02-13 20:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42][root][INFO] - Training Epoch: 1/2, step 5373/7134 completed (loss: 0.10946066677570343, acc: 0.976190447807312)
[2025-02-13 20:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43][root][INFO] - Training Epoch: 1/2, step 5374/7134 completed (loss: 0.08552171289920807, acc: 0.976190447807312)
[2025-02-13 20:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43][root][INFO] - Training Epoch: 1/2, step 5375/7134 completed (loss: 0.09523443877696991, acc: 0.9922480583190918)
[2025-02-13 20:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44][root][INFO] - Training Epoch: 1/2, step 5376/7134 completed (loss: 0.1100185364484787, acc: 0.9662162065505981)
[2025-02-13 20:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44][root][INFO] - Training Epoch: 1/2, step 5377/7134 completed (loss: 0.11477525532245636, acc: 0.9753086566925049)
[2025-02-13 20:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44][root][INFO] - Training Epoch: 1/2, step 5378/7134 completed (loss: 0.05298641696572304, acc: 0.9894737005233765)
[2025-02-13 20:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45][root][INFO] - Training Epoch: 1/2, step 5379/7134 completed (loss: 0.07902941107749939, acc: 0.9895833134651184)
[2025-02-13 20:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45][root][INFO] - Training Epoch: 1/2, step 5380/7134 completed (loss: 0.15331080555915833, acc: 0.970370352268219)
[2025-02-13 20:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46][root][INFO] - Training Epoch: 1/2, step 5381/7134 completed (loss: 0.07939408719539642, acc: 0.9717513918876648)
[2025-02-13 20:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46][root][INFO] - Training Epoch: 1/2, step 5382/7134 completed (loss: 0.0674930214881897, acc: 0.9927536249160767)
[2025-02-13 20:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46][root][INFO] - Training Epoch: 1/2, step 5383/7134 completed (loss: 0.05687804892659187, acc: 0.9838709831237793)
[2025-02-13 20:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47][root][INFO] - Training Epoch: 1/2, step 5384/7134 completed (loss: 0.08218413591384888, acc: 0.9796954393386841)
[2025-02-13 20:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47][root][INFO] - Training Epoch: 1/2, step 5385/7134 completed (loss: 0.10776527225971222, acc: 0.9709302186965942)
[2025-02-13 20:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:48][root][INFO] - Training Epoch: 1/2, step 5386/7134 completed (loss: 0.0455535426735878, acc: 0.9890109896659851)
[2025-02-13 20:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:48][root][INFO] - Training Epoch: 1/2, step 5387/7134 completed (loss: 0.05484702065587044, acc: 0.9754098653793335)
[2025-02-13 20:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49][root][INFO] - Training Epoch: 1/2, step 5388/7134 completed (loss: 0.048235226422548294, acc: 0.9888268113136292)
[2025-02-13 20:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49][root][INFO] - Training Epoch: 1/2, step 5389/7134 completed (loss: 0.03117034211754799, acc: 0.9931972622871399)
[2025-02-13 20:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49][root][INFO] - Training Epoch: 1/2, step 5390/7134 completed (loss: 0.20243506133556366, acc: 0.9496855139732361)
[2025-02-13 20:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50][root][INFO] - Training Epoch: 1/2, step 5391/7134 completed (loss: 0.1415717601776123, acc: 0.9640287756919861)
[2025-02-13 20:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50][root][INFO] - Training Epoch: 1/2, step 5392/7134 completed (loss: 0.20493732392787933, acc: 0.9659090638160706)
[2025-02-13 20:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50][root][INFO] - Training Epoch: 1/2, step 5393/7134 completed (loss: 0.18020179867744446, acc: 0.9666666388511658)
[2025-02-13 20:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51][root][INFO] - Training Epoch: 1/2, step 5394/7134 completed (loss: 0.507511556148529, acc: 0.8828125)
[2025-02-13 20:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51][root][INFO] - Training Epoch: 1/2, step 5395/7134 completed (loss: 0.10461235791444778, acc: 0.9864864945411682)
[2025-02-13 20:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51][root][INFO] - Training Epoch: 1/2, step 5396/7134 completed (loss: 0.10490693897008896, acc: 0.9802631735801697)
[2025-02-13 20:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52][root][INFO] - Training Epoch: 1/2, step 5397/7134 completed (loss: 0.09852370619773865, acc: 0.978723406791687)
[2025-02-13 20:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52][root][INFO] - Training Epoch: 1/2, step 5398/7134 completed (loss: 0.12310097366571426, acc: 0.9698795080184937)
[2025-02-13 20:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53][root][INFO] - Training Epoch: 1/2, step 5399/7134 completed (loss: 0.11179248243570328, acc: 0.9485714435577393)
[2025-02-13 20:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53][root][INFO] - Training Epoch: 1/2, step 5400/7134 completed (loss: 0.3144122362136841, acc: 0.9448275566101074)
[2025-02-13 20:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53][root][INFO] - Training Epoch: 1/2, step 5401/7134 completed (loss: 0.10576921701431274, acc: 0.9722222089767456)
[2025-02-13 20:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54][root][INFO] - Training Epoch: 1/2, step 5402/7134 completed (loss: 0.18866342306137085, acc: 0.9520547986030579)
[2025-02-13 20:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54][root][INFO] - Training Epoch: 1/2, step 5403/7134 completed (loss: 0.20156995952129364, acc: 0.9590163826942444)
[2025-02-13 20:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55][root][INFO] - Training Epoch: 1/2, step 5404/7134 completed (loss: 0.32006654143333435, acc: 0.926174521446228)
[2025-02-13 20:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55][root][INFO] - Training Epoch: 1/2, step 5405/7134 completed (loss: 0.14586526155471802, acc: 0.95652174949646)
[2025-02-13 20:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55][root][INFO] - Training Epoch: 1/2, step 5406/7134 completed (loss: 0.16822469234466553, acc: 0.9639639854431152)
[2025-02-13 20:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56][root][INFO] - Training Epoch: 1/2, step 5407/7134 completed (loss: 0.23479460179805756, acc: 0.9230769276618958)
[2025-02-13 20:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56][root][INFO] - Training Epoch: 1/2, step 5408/7134 completed (loss: 0.3102799952030182, acc: 0.9290780425071716)
[2025-02-13 20:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56][root][INFO] - Training Epoch: 1/2, step 5409/7134 completed (loss: 0.30892330408096313, acc: 0.9510489702224731)
[2025-02-13 20:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:57][root][INFO] - Training Epoch: 1/2, step 5410/7134 completed (loss: 0.0761217400431633, acc: 0.9759036302566528)
[2025-02-13 20:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:57][root][INFO] - Training Epoch: 1/2, step 5411/7134 completed (loss: 0.25548404455184937, acc: 0.9507042169570923)
[2025-02-13 20:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58][root][INFO] - Training Epoch: 1/2, step 5412/7134 completed (loss: 0.2342882752418518, acc: 0.9734513163566589)
[2025-02-13 20:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58][root][INFO] - Training Epoch: 1/2, step 5413/7134 completed (loss: 0.37618887424468994, acc: 0.8888888955116272)
[2025-02-13 20:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58][root][INFO] - Training Epoch: 1/2, step 5414/7134 completed (loss: 0.4430983364582062, acc: 0.8922155499458313)
[2025-02-13 20:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59][root][INFO] - Training Epoch: 1/2, step 5415/7134 completed (loss: 0.35434797406196594, acc: 0.9350649118423462)
[2025-02-13 20:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59][root][INFO] - Training Epoch: 1/2, step 5416/7134 completed (loss: 0.2784789502620697, acc: 0.9411764740943909)
[2025-02-13 20:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59][root][INFO] - Training Epoch: 1/2, step 5417/7134 completed (loss: 0.14619478583335876, acc: 0.9615384340286255)
[2025-02-13 20:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00][root][INFO] - Training Epoch: 1/2, step 5418/7134 completed (loss: 0.34124821424484253, acc: 0.9236111044883728)
[2025-02-13 20:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00][root][INFO] - Training Epoch: 1/2, step 5419/7134 completed (loss: 0.6249405741691589, acc: 0.8467153310775757)
[2025-02-13 20:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:01][root][INFO] - Training Epoch: 1/2, step 5420/7134 completed (loss: 0.6145024299621582, acc: 0.8270676732063293)
[2025-02-13 20:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:01][root][INFO] - Training Epoch: 1/2, step 5421/7134 completed (loss: 0.5109944343566895, acc: 0.8812500238418579)
[2025-02-13 20:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02][root][INFO] - Training Epoch: 1/2, step 5422/7134 completed (loss: 0.21214911341667175, acc: 0.9513888955116272)
[2025-02-13 20:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02][root][INFO] - Training Epoch: 1/2, step 5423/7134 completed (loss: 0.18693216145038605, acc: 0.9750000238418579)
[2025-02-13 20:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02][root][INFO] - Training Epoch: 1/2, step 5424/7134 completed (loss: 0.2193240374326706, acc: 0.9461538195610046)
[2025-02-13 20:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03][root][INFO] - Training Epoch: 1/2, step 5425/7134 completed (loss: 0.15663769841194153, acc: 0.953125)
[2025-02-13 20:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03][root][INFO] - Training Epoch: 1/2, step 5426/7134 completed (loss: 0.252259224653244, acc: 0.9590163826942444)
[2025-02-13 20:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03][root][INFO] - Training Epoch: 1/2, step 5427/7134 completed (loss: 0.12054426968097687, acc: 0.9803921580314636)
[2025-02-13 20:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04][root][INFO] - Training Epoch: 1/2, step 5428/7134 completed (loss: 0.15837769210338593, acc: 0.951724112033844)
[2025-02-13 20:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04][root][INFO] - Training Epoch: 1/2, step 5429/7134 completed (loss: 0.1877700686454773, acc: 0.9597315192222595)
[2025-02-13 20:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05][root][INFO] - Training Epoch: 1/2, step 5430/7134 completed (loss: 0.27532878518104553, acc: 0.9545454382896423)
[2025-02-13 20:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05][root][INFO] - Training Epoch: 1/2, step 5431/7134 completed (loss: 0.24729633331298828, acc: 0.9097744226455688)
[2025-02-13 20:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05][root][INFO] - Training Epoch: 1/2, step 5432/7134 completed (loss: 0.12529967725276947, acc: 0.96875)
[2025-02-13 20:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06][root][INFO] - Training Epoch: 1/2, step 5433/7134 completed (loss: 0.34005722403526306, acc: 0.9411764740943909)
[2025-02-13 20:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06][root][INFO] - Training Epoch: 1/2, step 5434/7134 completed (loss: 0.26243987679481506, acc: 0.9354838728904724)
[2025-02-13 20:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07][root][INFO] - Training Epoch: 1/2, step 5435/7134 completed (loss: 0.2233961671590805, acc: 0.9520000219345093)
[2025-02-13 20:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07][root][INFO] - Training Epoch: 1/2, step 5436/7134 completed (loss: 0.19445137679576874, acc: 0.9548386931419373)
[2025-02-13 20:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07][root][INFO] - Training Epoch: 1/2, step 5437/7134 completed (loss: 0.19388684630393982, acc: 0.969924807548523)
[2025-02-13 20:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08][root][INFO] - Training Epoch: 1/2, step 5438/7134 completed (loss: 0.26554521918296814, acc: 0.9277777671813965)
[2025-02-13 20:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08][root][INFO] - Training Epoch: 1/2, step 5439/7134 completed (loss: 0.3666648864746094, acc: 0.9151515364646912)
[2025-02-13 20:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09][root][INFO] - Training Epoch: 1/2, step 5440/7134 completed (loss: 0.20429539680480957, acc: 0.9166666865348816)
[2025-02-13 20:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09][root][INFO] - Training Epoch: 1/2, step 5441/7134 completed (loss: 0.26160213351249695, acc: 0.9189189076423645)
[2025-02-13 20:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09][root][INFO] - Training Epoch: 1/2, step 5442/7134 completed (loss: 0.14201200008392334, acc: 0.976190447807312)
[2025-02-13 20:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10][root][INFO] - Training Epoch: 1/2, step 5443/7134 completed (loss: 0.14701470732688904, acc: 0.9634146094322205)
[2025-02-13 20:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10][root][INFO] - Training Epoch: 1/2, step 5444/7134 completed (loss: 0.1637915074825287, acc: 0.9558823704719543)
[2025-02-13 20:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11][root][INFO] - Training Epoch: 1/2, step 5445/7134 completed (loss: 0.2807001769542694, acc: 0.9264705777168274)
[2025-02-13 20:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11][root][INFO] - Training Epoch: 1/2, step 5446/7134 completed (loss: 0.148493692278862, acc: 0.9509202241897583)
[2025-02-13 20:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11][root][INFO] - Training Epoch: 1/2, step 5447/7134 completed (loss: 0.26496028900146484, acc: 0.9178082346916199)
[2025-02-13 20:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12][root][INFO] - Training Epoch: 1/2, step 5448/7134 completed (loss: 0.21742112934589386, acc: 0.9340659379959106)
[2025-02-13 20:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12][root][INFO] - Training Epoch: 1/2, step 5449/7134 completed (loss: 0.21884191036224365, acc: 0.9672130942344666)
[2025-02-13 20:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12][root][INFO] - Training Epoch: 1/2, step 5450/7134 completed (loss: 0.1588025987148285, acc: 0.9611111283302307)
[2025-02-13 20:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:13][root][INFO] - Training Epoch: 1/2, step 5451/7134 completed (loss: 0.09500932693481445, acc: 0.9704142212867737)
[2025-02-13 20:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:13][root][INFO] - Training Epoch: 1/2, step 5452/7134 completed (loss: 0.1308816820383072, acc: 0.9644970297813416)
[2025-02-13 20:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14][root][INFO] - Training Epoch: 1/2, step 5453/7134 completed (loss: 0.2617757022380829, acc: 0.9277777671813965)
[2025-02-13 20:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14][root][INFO] - Training Epoch: 1/2, step 5454/7134 completed (loss: 0.1447826623916626, acc: 0.9836956262588501)
[2025-02-13 20:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14][root][INFO] - Training Epoch: 1/2, step 5455/7134 completed (loss: 0.17652344703674316, acc: 0.9572192430496216)
[2025-02-13 20:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15][root][INFO] - Training Epoch: 1/2, step 5456/7134 completed (loss: 0.14376287162303925, acc: 0.9569892287254333)
[2025-02-13 20:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15][root][INFO] - Training Epoch: 1/2, step 5457/7134 completed (loss: 0.18686193227767944, acc: 0.954023003578186)
[2025-02-13 20:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16][root][INFO] - Training Epoch: 1/2, step 5458/7134 completed (loss: 0.13015177845954895, acc: 0.9613259434700012)
[2025-02-13 20:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16][root][INFO] - Training Epoch: 1/2, step 5459/7134 completed (loss: 0.03549497574567795, acc: 0.987730085849762)
[2025-02-13 20:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16][root][INFO] - Training Epoch: 1/2, step 5460/7134 completed (loss: 0.10110890120267868, acc: 0.9822485446929932)
[2025-02-13 20:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17][root][INFO] - Training Epoch: 1/2, step 5461/7134 completed (loss: 0.2469615936279297, acc: 0.9174311757087708)
[2025-02-13 20:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17][root][INFO] - Training Epoch: 1/2, step 5462/7134 completed (loss: 0.0758855938911438, acc: 0.9754601120948792)
[2025-02-13 20:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17][root][INFO] - Training Epoch: 1/2, step 5463/7134 completed (loss: 0.24218401312828064, acc: 0.9567901492118835)
[2025-02-13 20:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18][root][INFO] - Training Epoch: 1/2, step 5464/7134 completed (loss: 0.2199886292219162, acc: 0.9453551769256592)
[2025-02-13 20:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18][root][INFO] - Training Epoch: 1/2, step 5465/7134 completed (loss: 0.08148057758808136, acc: 0.987500011920929)
[2025-02-13 20:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19][root][INFO] - Training Epoch: 1/2, step 5466/7134 completed (loss: 0.1284460425376892, acc: 0.96517413854599)
[2025-02-13 20:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19][root][INFO] - Training Epoch: 1/2, step 5467/7134 completed (loss: 0.08958166837692261, acc: 0.9701492786407471)
[2025-02-13 20:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19][root][INFO] - Training Epoch: 1/2, step 5468/7134 completed (loss: 0.06494484841823578, acc: 0.9842932224273682)
[2025-02-13 20:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20][root][INFO] - Training Epoch: 1/2, step 5469/7134 completed (loss: 0.2018340826034546, acc: 0.9417989253997803)
[2025-02-13 20:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20][root][INFO] - Training Epoch: 1/2, step 5470/7134 completed (loss: 0.052020855247974396, acc: 0.9900497794151306)
[2025-02-13 20:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20][root][INFO] - Training Epoch: 1/2, step 5471/7134 completed (loss: 0.044256582856178284, acc: 0.9882352948188782)
[2025-02-13 20:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21][root][INFO] - Training Epoch: 1/2, step 5472/7134 completed (loss: 0.09287410974502563, acc: 0.9832402467727661)
[2025-02-13 20:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21][root][INFO] - Training Epoch: 1/2, step 5473/7134 completed (loss: 0.02956099435687065, acc: 1.0)
[2025-02-13 20:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22][root][INFO] - Training Epoch: 1/2, step 5474/7134 completed (loss: 0.15184786915779114, acc: 0.9636363387107849)
[2025-02-13 20:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22][root][INFO] - Training Epoch: 1/2, step 5475/7134 completed (loss: 0.07054683566093445, acc: 0.9814814925193787)
[2025-02-13 20:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22][root][INFO] - Training Epoch: 1/2, step 5476/7134 completed (loss: 0.017070526257157326, acc: 0.9935064911842346)
[2025-02-13 20:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:23][root][INFO] - Training Epoch: 1/2, step 5477/7134 completed (loss: 0.10778900980949402, acc: 0.9846938848495483)
[2025-02-13 20:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:23][root][INFO] - Training Epoch: 1/2, step 5478/7134 completed (loss: 0.10546965897083282, acc: 0.9777777791023254)
[2025-02-13 20:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24][root][INFO] - Training Epoch: 1/2, step 5479/7134 completed (loss: 0.2210438996553421, acc: 0.9459459185600281)
[2025-02-13 20:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24][root][INFO] - Training Epoch: 1/2, step 5480/7134 completed (loss: 0.21800515055656433, acc: 0.9404761791229248)
[2025-02-13 20:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24][root][INFO] - Training Epoch: 1/2, step 5481/7134 completed (loss: 0.169908344745636, acc: 0.9754098653793335)
[2025-02-13 20:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:25][root][INFO] - Training Epoch: 1/2, step 5482/7134 completed (loss: 0.4595659077167511, acc: 0.9020618796348572)
[2025-02-13 20:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:25][root][INFO] - Training Epoch: 1/2, step 5483/7134 completed (loss: 0.2008650153875351, acc: 0.939393937587738)
[2025-02-13 20:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26][root][INFO] - Training Epoch: 1/2, step 5484/7134 completed (loss: 0.17643868923187256, acc: 0.9506173133850098)
[2025-02-13 20:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26][root][INFO] - Training Epoch: 1/2, step 5485/7134 completed (loss: 0.21781831979751587, acc: 0.9354838728904724)
[2025-02-13 20:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27][root][INFO] - Training Epoch: 1/2, step 5486/7134 completed (loss: 0.2790546119213104, acc: 0.9318181872367859)
[2025-02-13 20:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27][root][INFO] - Training Epoch: 1/2, step 5487/7134 completed (loss: 0.17154286801815033, acc: 0.9523809552192688)
[2025-02-13 20:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27][root][INFO] - Training Epoch: 1/2, step 5488/7134 completed (loss: 0.307009220123291, acc: 0.9281437397003174)
[2025-02-13 20:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28][root][INFO] - Training Epoch: 1/2, step 5489/7134 completed (loss: 0.516281247138977, acc: 0.8536585569381714)
[2025-02-13 20:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28][root][INFO] - Training Epoch: 1/2, step 5490/7134 completed (loss: 0.20558792352676392, acc: 0.939393937587738)
[2025-02-13 20:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29][root][INFO] - Training Epoch: 1/2, step 5491/7134 completed (loss: 0.14175164699554443, acc: 0.9590643048286438)
[2025-02-13 20:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29][root][INFO] - Training Epoch: 1/2, step 5492/7134 completed (loss: 0.19801278412342072, acc: 0.9552238583564758)
[2025-02-13 20:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29][root][INFO] - Training Epoch: 1/2, step 5493/7134 completed (loss: 0.3081459105014801, acc: 0.8958333134651184)
[2025-02-13 20:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30][root][INFO] - Training Epoch: 1/2, step 5494/7134 completed (loss: 0.16572394967079163, acc: 0.955974817276001)
[2025-02-13 20:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30][root][INFO] - Training Epoch: 1/2, step 5495/7134 completed (loss: 0.14096814393997192, acc: 0.9548872113227844)
[2025-02-13 20:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30][root][INFO] - Training Epoch: 1/2, step 5496/7134 completed (loss: 0.25091540813446045, acc: 0.9682539701461792)
[2025-02-13 20:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:31][root][INFO] - Training Epoch: 1/2, step 5497/7134 completed (loss: 0.26296839118003845, acc: 0.9235293865203857)
[2025-02-13 20:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:31][root][INFO] - Training Epoch: 1/2, step 5498/7134 completed (loss: 0.24037089943885803, acc: 0.9468085169792175)
[2025-02-13 20:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32][root][INFO] - Training Epoch: 1/2, step 5499/7134 completed (loss: 0.251129150390625, acc: 0.9279279112815857)
[2025-02-13 20:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32][root][INFO] - Training Epoch: 1/2, step 5500/7134 completed (loss: 0.12785986065864563, acc: 0.9689119458198547)
[2025-02-13 20:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32][root][INFO] - Training Epoch: 1/2, step 5501/7134 completed (loss: 0.23004741966724396, acc: 0.9343434572219849)
[2025-02-13 20:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33][root][INFO] - Training Epoch: 1/2, step 5502/7134 completed (loss: 0.16477851569652557, acc: 0.9604519605636597)
[2025-02-13 20:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33][root][INFO] - Training Epoch: 1/2, step 5503/7134 completed (loss: 0.41702955961227417, acc: 0.9230769276618958)
[2025-02-13 20:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34][root][INFO] - Training Epoch: 1/2, step 5504/7134 completed (loss: 0.21969084441661835, acc: 0.9277108311653137)
[2025-02-13 20:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34][root][INFO] - Training Epoch: 1/2, step 5505/7134 completed (loss: 0.3931456506252289, acc: 0.8863636255264282)
[2025-02-13 20:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34][root][INFO] - Training Epoch: 1/2, step 5506/7134 completed (loss: 0.26541727781295776, acc: 0.9242424368858337)
[2025-02-13 20:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:35][root][INFO] - Training Epoch: 1/2, step 5507/7134 completed (loss: 0.22141148149967194, acc: 0.9298245906829834)
[2025-02-13 20:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:35][root][INFO] - Training Epoch: 1/2, step 5508/7134 completed (loss: 0.18827837705612183, acc: 0.9425837397575378)
[2025-02-13 20:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36][root][INFO] - Training Epoch: 1/2, step 5509/7134 completed (loss: 0.26682403683662415, acc: 0.9478672742843628)
[2025-02-13 20:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36][root][INFO] - Training Epoch: 1/2, step 5510/7134 completed (loss: 0.2935502529144287, acc: 0.9247311949729919)
[2025-02-13 20:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36][root][INFO] - Training Epoch: 1/2, step 5511/7134 completed (loss: 0.2266090363264084, acc: 0.9417475461959839)
[2025-02-13 20:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37][root][INFO] - Training Epoch: 1/2, step 5512/7134 completed (loss: 0.28649431467056274, acc: 0.9230769276618958)
[2025-02-13 20:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37][root][INFO] - Training Epoch: 1/2, step 5513/7134 completed (loss: 0.2971961200237274, acc: 0.9053254723548889)
[2025-02-13 20:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37][root][INFO] - Training Epoch: 1/2, step 5514/7134 completed (loss: 0.36085185408592224, acc: 0.8981481194496155)
[2025-02-13 20:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:38][root][INFO] - Training Epoch: 1/2, step 5515/7134 completed (loss: 0.26078635454177856, acc: 0.9404761791229248)
[2025-02-13 20:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:38][root][INFO] - Training Epoch: 1/2, step 5516/7134 completed (loss: 0.19186481833457947, acc: 0.977011501789093)
[2025-02-13 20:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39][root][INFO] - Training Epoch: 1/2, step 5517/7134 completed (loss: 0.19738814234733582, acc: 0.9553072452545166)
[2025-02-13 20:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39][root][INFO] - Training Epoch: 1/2, step 5518/7134 completed (loss: 0.21086302399635315, acc: 0.9333333373069763)
[2025-02-13 20:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39][root][INFO] - Training Epoch: 1/2, step 5519/7134 completed (loss: 0.2096220850944519, acc: 0.9382715821266174)
[2025-02-13 20:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40][root][INFO] - Training Epoch: 1/2, step 5520/7134 completed (loss: 0.22265665233135223, acc: 0.9336493015289307)
[2025-02-13 20:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40][root][INFO] - Training Epoch: 1/2, step 5521/7134 completed (loss: 0.28677499294281006, acc: 0.9005235433578491)
[2025-02-13 20:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41][root][INFO] - Training Epoch: 1/2, step 5522/7134 completed (loss: 0.3009406328201294, acc: 0.8952879309654236)
[2025-02-13 20:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41][root][INFO] - Training Epoch: 1/2, step 5523/7134 completed (loss: 0.21645691990852356, acc: 0.9285714030265808)
[2025-02-13 20:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41][root][INFO] - Training Epoch: 1/2, step 5524/7134 completed (loss: 0.31326112151145935, acc: 0.9113923907279968)
[2025-02-13 20:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42][root][INFO] - Training Epoch: 1/2, step 5525/7134 completed (loss: 0.1279694139957428, acc: 0.9634146094322205)
[2025-02-13 20:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42][root][INFO] - Training Epoch: 1/2, step 5526/7134 completed (loss: 0.17023880779743195, acc: 0.95652174949646)
[2025-02-13 20:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42][root][INFO] - Training Epoch: 1/2, step 5527/7134 completed (loss: 0.13661527633666992, acc: 0.9720279574394226)
[2025-02-13 20:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:43][root][INFO] - Training Epoch: 1/2, step 5528/7134 completed (loss: 0.1796754151582718, acc: 0.9545454382896423)
[2025-02-13 20:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:43][root][INFO] - Training Epoch: 1/2, step 5529/7134 completed (loss: 0.15065449476242065, acc: 0.9636363387107849)
[2025-02-13 20:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44][root][INFO] - Training Epoch: 1/2, step 5530/7134 completed (loss: 0.09276608377695084, acc: 0.9727891087532043)
[2025-02-13 20:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44][root][INFO] - Training Epoch: 1/2, step 5531/7134 completed (loss: 0.1353265643119812, acc: 0.9759036302566528)
[2025-02-13 20:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44][root][INFO] - Training Epoch: 1/2, step 5532/7134 completed (loss: 0.19583271443843842, acc: 0.9435028433799744)
[2025-02-13 20:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45][root][INFO] - Training Epoch: 1/2, step 5533/7134 completed (loss: 0.05701589584350586, acc: 0.9944444298744202)
[2025-02-13 20:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45][root][INFO] - Training Epoch: 1/2, step 5534/7134 completed (loss: 0.3132345378398895, acc: 0.9382022619247437)
[2025-02-13 20:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45][root][INFO] - Training Epoch: 1/2, step 5535/7134 completed (loss: 0.12447647005319595, acc: 0.984455943107605)
[2025-02-13 20:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46][root][INFO] - Training Epoch: 1/2, step 5536/7134 completed (loss: 0.13700369000434875, acc: 0.9726775884628296)
[2025-02-13 20:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46][root][INFO] - Training Epoch: 1/2, step 5537/7134 completed (loss: 0.17859439551830292, acc: 0.959770143032074)
[2025-02-13 20:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46][root][INFO] - Training Epoch: 1/2, step 5538/7134 completed (loss: 0.14584824442863464, acc: 0.9685863852500916)
[2025-02-13 20:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:47][root][INFO] - Training Epoch: 1/2, step 5539/7134 completed (loss: 0.15805578231811523, acc: 0.9617486596107483)
[2025-02-13 20:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:47][root][INFO] - Training Epoch: 1/2, step 5540/7134 completed (loss: 0.19230249524116516, acc: 0.9518072009086609)
[2025-02-13 20:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48][root][INFO] - Training Epoch: 1/2, step 5541/7134 completed (loss: 0.11317285150289536, acc: 0.9883720874786377)
[2025-02-13 20:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48][root][INFO] - Training Epoch: 1/2, step 5542/7134 completed (loss: 0.05263623967766762, acc: 0.9806451797485352)
[2025-02-13 20:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48][root][INFO] - Training Epoch: 1/2, step 5543/7134 completed (loss: 0.10608688741922379, acc: 0.9839572310447693)
[2025-02-13 20:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49][root][INFO] - Training Epoch: 1/2, step 5544/7134 completed (loss: 0.0761144757270813, acc: 0.9890710115432739)
[2025-02-13 20:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49][root][INFO] - Training Epoch: 1/2, step 5545/7134 completed (loss: 0.12603861093521118, acc: 0.9642857313156128)
[2025-02-13 20:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49][root][INFO] - Training Epoch: 1/2, step 5546/7134 completed (loss: 0.10781259834766388, acc: 0.976047933101654)
[2025-02-13 20:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:50][root][INFO] - Training Epoch: 1/2, step 5547/7134 completed (loss: 0.11995340883731842, acc: 0.9723756909370422)
[2025-02-13 20:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:50][root][INFO] - Training Epoch: 1/2, step 5548/7134 completed (loss: 0.16368065774440765, acc: 0.970588207244873)
[2025-02-13 20:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51][root][INFO] - Training Epoch: 1/2, step 5549/7134 completed (loss: 0.13428747653961182, acc: 0.9597315192222595)
[2025-02-13 20:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51][root][INFO] - Training Epoch: 1/2, step 5550/7134 completed (loss: 0.08838321268558502, acc: 0.983146071434021)
[2025-02-13 20:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51][root][INFO] - Training Epoch: 1/2, step 5551/7134 completed (loss: 0.30417925119400024, acc: 0.9354838728904724)
[2025-02-13 20:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52][root][INFO] - Training Epoch: 1/2, step 5552/7134 completed (loss: 0.04246950149536133, acc: 0.9938650131225586)
[2025-02-13 20:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52][root][INFO] - Training Epoch: 1/2, step 5553/7134 completed (loss: 0.1758890599012375, acc: 0.9644970297813416)
[2025-02-13 20:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52][root][INFO] - Training Epoch: 1/2, step 5554/7134 completed (loss: 0.11038419604301453, acc: 0.9808917045593262)
[2025-02-13 20:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53][root][INFO] - Training Epoch: 1/2, step 5555/7134 completed (loss: 0.12985241413116455, acc: 0.976190447807312)
[2025-02-13 20:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53][root][INFO] - Training Epoch: 1/2, step 5556/7134 completed (loss: 0.24893981218338013, acc: 0.9553072452545166)
[2025-02-13 20:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53][root][INFO] - Training Epoch: 1/2, step 5557/7134 completed (loss: 0.17834946513175964, acc: 0.95652174949646)
[2025-02-13 20:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54][root][INFO] - Training Epoch: 1/2, step 5558/7134 completed (loss: 0.163113534450531, acc: 0.9444444179534912)
[2025-02-13 20:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54][root][INFO] - Training Epoch: 1/2, step 5559/7134 completed (loss: 0.24259012937545776, acc: 0.9608938694000244)
[2025-02-13 20:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55][root][INFO] - Training Epoch: 1/2, step 5560/7134 completed (loss: 0.3283454179763794, acc: 0.9388889074325562)
[2025-02-13 20:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55][root][INFO] - Training Epoch: 1/2, step 5561/7134 completed (loss: 0.21319179236888885, acc: 0.9590163826942444)
[2025-02-13 20:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55][root][INFO] - Training Epoch: 1/2, step 5562/7134 completed (loss: 0.24829623103141785, acc: 0.9496402740478516)
[2025-02-13 20:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56][root][INFO] - Training Epoch: 1/2, step 5563/7134 completed (loss: 0.11396785825490952, acc: 0.9722222089767456)
[2025-02-13 20:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56][root][INFO] - Training Epoch: 1/2, step 5564/7134 completed (loss: 0.06986065208911896, acc: 0.9750000238418579)
[2025-02-13 20:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56][root][INFO] - Training Epoch: 1/2, step 5565/7134 completed (loss: 0.06257082521915436, acc: 0.9800000190734863)
[2025-02-13 20:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57][root][INFO] - Training Epoch: 1/2, step 5566/7134 completed (loss: 0.06030897796154022, acc: 0.989847719669342)
[2025-02-13 20:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57][root][INFO] - Training Epoch: 1/2, step 5567/7134 completed (loss: 0.056099146604537964, acc: 0.9850746393203735)
[2025-02-13 20:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58][root][INFO] - Training Epoch: 1/2, step 5568/7134 completed (loss: 0.05488944426178932, acc: 0.9890109896659851)
[2025-02-13 20:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58][root][INFO] - Training Epoch: 1/2, step 5569/7134 completed (loss: 0.05536344647407532, acc: 0.9895833134651184)
[2025-02-13 20:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58][root][INFO] - Training Epoch: 1/2, step 5570/7134 completed (loss: 0.09840424358844757, acc: 0.9836956262588501)
[2025-02-13 20:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59][root][INFO] - Training Epoch: 1/2, step 5571/7134 completed (loss: 0.07227562367916107, acc: 0.9873417615890503)
[2025-02-13 20:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59][root][INFO] - Training Epoch: 1/2, step 5572/7134 completed (loss: 0.08381354063749313, acc: 0.987730085849762)
[2025-02-13 20:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59][root][INFO] - Training Epoch: 1/2, step 5573/7134 completed (loss: 0.10257529467344284, acc: 0.9803921580314636)
[2025-02-13 20:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:00][root][INFO] - Training Epoch: 1/2, step 5574/7134 completed (loss: 0.06433363258838654, acc: 0.9930555820465088)
[2025-02-13 20:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:00][root][INFO] - Training Epoch: 1/2, step 5575/7134 completed (loss: 0.27361300587654114, acc: 0.9399999976158142)
[2025-02-13 20:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01][root][INFO] - Training Epoch: 1/2, step 5576/7134 completed (loss: 0.07826937735080719, acc: 0.988095223903656)
[2025-02-13 20:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01][root][INFO] - Training Epoch: 1/2, step 5577/7134 completed (loss: 0.30081337690353394, acc: 0.9111111164093018)
[2025-02-13 20:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01][root][INFO] - Training Epoch: 1/2, step 5578/7134 completed (loss: 0.18678615987300873, acc: 0.9746835231781006)
[2025-02-13 20:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02][root][INFO] - Training Epoch: 1/2, step 5579/7134 completed (loss: 0.1428452879190445, acc: 0.9693251252174377)
[2025-02-13 20:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02][root][INFO] - Training Epoch: 1/2, step 5580/7134 completed (loss: 0.11589976400136948, acc: 0.9777777791023254)
[2025-02-13 20:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02][root][INFO] - Training Epoch: 1/2, step 5581/7134 completed (loss: 0.12710578739643097, acc: 0.9757575988769531)
[2025-02-13 20:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03][root][INFO] - Training Epoch: 1/2, step 5582/7134 completed (loss: 0.13364443182945251, acc: 0.9613259434700012)
[2025-02-13 20:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03][root][INFO] - Training Epoch: 1/2, step 5583/7134 completed (loss: 0.09689353406429291, acc: 0.9648241400718689)
[2025-02-13 20:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03][root][INFO] - Training Epoch: 1/2, step 5584/7134 completed (loss: 0.14281073212623596, acc: 0.9683544039726257)
[2025-02-13 20:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04][root][INFO] - Training Epoch: 1/2, step 5585/7134 completed (loss: 0.15320084989070892, acc: 0.9658536314964294)
[2025-02-13 20:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04][root][INFO] - Training Epoch: 1/2, step 5586/7134 completed (loss: 0.11155778169631958, acc: 0.9863945841789246)
[2025-02-13 20:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:05][root][INFO] - Training Epoch: 1/2, step 5587/7134 completed (loss: 0.08138803392648697, acc: 0.9714285731315613)
[2025-02-13 20:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:05][root][INFO] - Training Epoch: 1/2, step 5588/7134 completed (loss: 0.05164959281682968, acc: 0.9938271641731262)
[2025-02-13 20:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:05][root][INFO] - Training Epoch: 1/2, step 5589/7134 completed (loss: 0.18750494718551636, acc: 0.9689440727233887)
[2025-02-13 20:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06][root][INFO] - Training Epoch: 1/2, step 5590/7134 completed (loss: 0.17651912569999695, acc: 0.9653179049491882)
[2025-02-13 20:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06][root][INFO] - Training Epoch: 1/2, step 5591/7134 completed (loss: 0.0562526173889637, acc: 0.9826589822769165)
[2025-02-13 20:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06][root][INFO] - Training Epoch: 1/2, step 5592/7134 completed (loss: 0.08614551275968552, acc: 0.9825581312179565)
[2025-02-13 20:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07][root][INFO] - Training Epoch: 1/2, step 5593/7134 completed (loss: 0.15917880833148956, acc: 0.967391312122345)
[2025-02-13 20:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07][root][INFO] - Training Epoch: 1/2, step 5594/7134 completed (loss: 0.07141364365816116, acc: 0.9943181872367859)
[2025-02-13 20:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08][root][INFO] - Training Epoch: 1/2, step 5595/7134 completed (loss: 0.1451493501663208, acc: 0.9772727489471436)
[2025-02-13 20:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08][root][INFO] - Training Epoch: 1/2, step 5596/7134 completed (loss: 0.05514540523290634, acc: 0.9940119981765747)
[2025-02-13 20:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08][root][INFO] - Training Epoch: 1/2, step 5597/7134 completed (loss: 0.10577176511287689, acc: 0.9879518151283264)
[2025-02-13 20:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09][root][INFO] - Training Epoch: 1/2, step 5598/7134 completed (loss: 0.10121409595012665, acc: 0.9826589822769165)
[2025-02-13 20:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09][root][INFO] - Training Epoch: 1/2, step 5599/7134 completed (loss: 0.1819917857646942, acc: 0.9591836929321289)
[2025-02-13 20:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09][root][INFO] - Training Epoch: 1/2, step 5600/7134 completed (loss: 0.18946582078933716, acc: 0.9571428298950195)
[2025-02-13 20:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10][root][INFO] - Training Epoch: 1/2, step 5601/7134 completed (loss: 0.12834422290325165, acc: 0.9541984796524048)
[2025-02-13 20:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10][root][INFO] - Training Epoch: 1/2, step 5602/7134 completed (loss: 0.20083484053611755, acc: 0.9507042169570923)
[2025-02-13 20:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10][root][INFO] - Training Epoch: 1/2, step 5603/7134 completed (loss: 0.152133509516716, acc: 0.9508196711540222)
[2025-02-13 20:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:11][root][INFO] - Training Epoch: 1/2, step 5604/7134 completed (loss: 0.24640479683876038, acc: 0.9430894255638123)
[2025-02-13 20:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:11][root][INFO] - Training Epoch: 1/2, step 5605/7134 completed (loss: 0.23139740526676178, acc: 0.9291338324546814)
[2025-02-13 20:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12][root][INFO] - Training Epoch: 1/2, step 5606/7134 completed (loss: 0.2903675436973572, acc: 0.9345794320106506)
[2025-02-13 20:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12][root][INFO] - Training Epoch: 1/2, step 5607/7134 completed (loss: 0.08416116237640381, acc: 0.9784172773361206)
[2025-02-13 20:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12][root][INFO] - Training Epoch: 1/2, step 5608/7134 completed (loss: 0.17208582162857056, acc: 0.9534883499145508)
[2025-02-13 20:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13][root][INFO] - Training Epoch: 1/2, step 5609/7134 completed (loss: 0.23362095654010773, acc: 0.9166666865348816)
[2025-02-13 20:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13][root][INFO] - Training Epoch: 1/2, step 5610/7134 completed (loss: 0.0781744197010994, acc: 0.9727272987365723)
[2025-02-13 20:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13][root][INFO] - Training Epoch: 1/2, step 5611/7134 completed (loss: 0.15597201883792877, acc: 0.9626865386962891)
[2025-02-13 20:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14][root][INFO] - Training Epoch: 1/2, step 5612/7134 completed (loss: 0.23122964799404144, acc: 0.9507042169570923)
[2025-02-13 20:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14][root][INFO] - Training Epoch: 1/2, step 5613/7134 completed (loss: 0.24493266642093658, acc: 0.9553571343421936)
[2025-02-13 20:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14][root][INFO] - Training Epoch: 1/2, step 5614/7134 completed (loss: 0.31860220432281494, acc: 0.9463087320327759)
[2025-02-13 20:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15][root][INFO] - Training Epoch: 1/2, step 5615/7134 completed (loss: 0.12728917598724365, acc: 0.9714285731315613)
[2025-02-13 20:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15][root][INFO] - Training Epoch: 1/2, step 5616/7134 completed (loss: 0.13793249428272247, acc: 0.9629629850387573)
[2025-02-13 20:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16][root][INFO] - Training Epoch: 1/2, step 5617/7134 completed (loss: 0.06719893962144852, acc: 0.9844961166381836)
[2025-02-13 20:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16][root][INFO] - Training Epoch: 1/2, step 5618/7134 completed (loss: 0.14623229205608368, acc: 0.970588207244873)
[2025-02-13 20:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16][root][INFO] - Training Epoch: 1/2, step 5619/7134 completed (loss: 0.18587028980255127, acc: 0.9327731132507324)
[2025-02-13 20:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17][root][INFO] - Training Epoch: 1/2, step 5620/7134 completed (loss: 0.13292519748210907, acc: 0.95652174949646)
[2025-02-13 20:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17][root][INFO] - Training Epoch: 1/2, step 5621/7134 completed (loss: 0.10752485692501068, acc: 0.9806451797485352)
[2025-02-13 20:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17][root][INFO] - Training Epoch: 1/2, step 5622/7134 completed (loss: 0.15987400710582733, acc: 0.9518716335296631)
[2025-02-13 20:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18][root][INFO] - Training Epoch: 1/2, step 5623/7134 completed (loss: 0.1660805642604828, acc: 0.9513888955116272)
[2025-02-13 20:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18][root][INFO] - Training Epoch: 1/2, step 5624/7134 completed (loss: 0.10216239839792252, acc: 0.9738562107086182)
[2025-02-13 20:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18][root][INFO] - Training Epoch: 1/2, step 5625/7134 completed (loss: 0.054169122129678726, acc: 0.9901960492134094)
[2025-02-13 20:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19][root][INFO] - Training Epoch: 1/2, step 5626/7134 completed (loss: 0.08095338940620422, acc: 0.9724137783050537)
[2025-02-13 20:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19][root][INFO] - Training Epoch: 1/2, step 5627/7134 completed (loss: 0.271420955657959, acc: 0.9583333134651184)
[2025-02-13 20:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20][root][INFO] - Training Epoch: 1/2, step 5628/7134 completed (loss: 0.15354730188846588, acc: 0.9732142686843872)
[2025-02-13 20:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20][root][INFO] - Training Epoch: 1/2, step 5629/7134 completed (loss: 0.1706048548221588, acc: 0.9508196711540222)
[2025-02-13 20:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20][root][INFO] - Training Epoch: 1/2, step 5630/7134 completed (loss: 0.2957043945789337, acc: 0.9411764740943909)
[2025-02-13 20:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21][root][INFO] - Training Epoch: 1/2, step 5631/7134 completed (loss: 0.08858340978622437, acc: 0.9918699264526367)
[2025-02-13 20:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21][root][INFO] - Training Epoch: 1/2, step 5632/7134 completed (loss: 0.2296581268310547, acc: 0.9519230723381042)
[2025-02-13 20:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21][root][INFO] - Training Epoch: 1/2, step 5633/7134 completed (loss: 0.11333408206701279, acc: 0.9732142686843872)
[2025-02-13 20:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22][root][INFO] - Training Epoch: 1/2, step 5634/7134 completed (loss: 0.10422339290380478, acc: 0.9557521939277649)
[2025-02-13 20:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22][root][INFO] - Training Epoch: 1/2, step 5635/7134 completed (loss: 0.07518851011991501, acc: 0.984375)
[2025-02-13 20:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22][root][INFO] - Training Epoch: 1/2, step 5636/7134 completed (loss: 0.1292867213487625, acc: 0.9692307710647583)
[2025-02-13 20:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:23][root][INFO] - Training Epoch: 1/2, step 5637/7134 completed (loss: 0.12126310169696808, acc: 0.9772727489471436)
[2025-02-13 20:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:23][root][INFO] - Training Epoch: 1/2, step 5638/7134 completed (loss: 0.13266275823116302, acc: 0.9736841917037964)
[2025-02-13 20:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24][root][INFO] - Training Epoch: 1/2, step 5639/7134 completed (loss: 0.10148116201162338, acc: 0.9740259647369385)
[2025-02-13 20:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24][root][INFO] - Training Epoch: 1/2, step 5640/7134 completed (loss: 0.16199155151844025, acc: 0.9503546357154846)
[2025-02-13 20:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24][root][INFO] - Training Epoch: 1/2, step 5641/7134 completed (loss: 0.16362611949443817, acc: 0.9550561904907227)
[2025-02-13 20:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25][root][INFO] - Training Epoch: 1/2, step 5642/7134 completed (loss: 0.19617204368114471, acc: 0.9510489702224731)
[2025-02-13 20:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25][root][INFO] - Training Epoch: 1/2, step 5643/7134 completed (loss: 0.2370678186416626, acc: 0.9368420839309692)
[2025-02-13 20:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25][root][INFO] - Training Epoch: 1/2, step 5644/7134 completed (loss: 0.19015862047672272, acc: 0.9428571462631226)
[2025-02-13 20:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26][root][INFO] - Training Epoch: 1/2, step 5645/7134 completed (loss: 0.10009317100048065, acc: 0.9652777910232544)
[2025-02-13 20:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26][root][INFO] - Training Epoch: 1/2, step 5646/7134 completed (loss: 0.0872952863574028, acc: 0.9785714149475098)
[2025-02-13 20:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26][root][INFO] - Training Epoch: 1/2, step 5647/7134 completed (loss: 0.17128294706344604, acc: 0.9513888955116272)
[2025-02-13 20:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27][root][INFO] - Training Epoch: 1/2, step 5648/7134 completed (loss: 0.26874715089797974, acc: 0.9333333373069763)
[2025-02-13 20:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27][root][INFO] - Training Epoch: 1/2, step 5649/7134 completed (loss: 0.2901812791824341, acc: 0.912162184715271)
[2025-02-13 20:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27][root][INFO] - Training Epoch: 1/2, step 5650/7134 completed (loss: 0.19713851809501648, acc: 0.9435483813285828)
[2025-02-13 20:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28][root][INFO] - Training Epoch: 1/2, step 5651/7134 completed (loss: 0.09487069398164749, acc: 0.9795918464660645)
[2025-02-13 20:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28][root][INFO] - Training Epoch: 1/2, step 5652/7134 completed (loss: 0.2033998817205429, acc: 0.9280575513839722)
[2025-02-13 20:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29][root][INFO] - Training Epoch: 1/2, step 5653/7134 completed (loss: 0.1906169354915619, acc: 0.948051929473877)
[2025-02-13 20:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29][root][INFO] - Training Epoch: 1/2, step 5654/7134 completed (loss: 0.05622067302465439, acc: 0.9921875)
[2025-02-13 20:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29][root][INFO] - Training Epoch: 1/2, step 5655/7134 completed (loss: 0.15999850630760193, acc: 0.9719626307487488)
[2025-02-13 20:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30][root][INFO] - Training Epoch: 1/2, step 5656/7134 completed (loss: 0.14694543182849884, acc: 0.9672130942344666)
[2025-02-13 20:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30][root][INFO] - Training Epoch: 1/2, step 5657/7134 completed (loss: 0.2101825624704361, acc: 0.9602649211883545)
[2025-02-13 20:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31][root][INFO] - Training Epoch: 1/2, step 5658/7134 completed (loss: 0.14834997057914734, acc: 0.9607843160629272)
[2025-02-13 20:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31][root][INFO] - Training Epoch: 1/2, step 5659/7134 completed (loss: 0.1415873020887375, acc: 0.9428571462631226)
[2025-02-13 20:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31][root][INFO] - Training Epoch: 1/2, step 5660/7134 completed (loss: 0.20514144003391266, acc: 0.9453125)
[2025-02-13 20:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32][root][INFO] - Training Epoch: 1/2, step 5661/7134 completed (loss: 0.13808004558086395, acc: 0.9747899174690247)
[2025-02-13 20:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32][root][INFO] - Training Epoch: 1/2, step 5662/7134 completed (loss: 0.15290382504463196, acc: 0.9640287756919861)
[2025-02-13 20:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32][root][INFO] - Training Epoch: 1/2, step 5663/7134 completed (loss: 0.057120345532894135, acc: 0.9927536249160767)
[2025-02-13 20:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33][root][INFO] - Training Epoch: 1/2, step 5664/7134 completed (loss: 0.3165273666381836, acc: 0.9383561611175537)
[2025-02-13 20:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33][root][INFO] - Training Epoch: 1/2, step 5665/7134 completed (loss: 0.06934812664985657, acc: 0.9931972622871399)
[2025-02-13 20:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33][root][INFO] - Training Epoch: 1/2, step 5666/7134 completed (loss: 0.0928390622138977, acc: 0.9803921580314636)
[2025-02-13 20:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34][root][INFO] - Training Epoch: 1/2, step 5667/7134 completed (loss: 0.22351449728012085, acc: 0.9512194991111755)
[2025-02-13 20:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34][root][INFO] - Training Epoch: 1/2, step 5668/7134 completed (loss: 0.18717224895954132, acc: 0.970370352268219)
[2025-02-13 20:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34][root][INFO] - Training Epoch: 1/2, step 5669/7134 completed (loss: 0.20785170793533325, acc: 0.9617834687232971)
[2025-02-13 20:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35][root][INFO] - Training Epoch: 1/2, step 5670/7134 completed (loss: 0.06737776845693588, acc: 0.9841269850730896)
[2025-02-13 20:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35][root][INFO] - Training Epoch: 1/2, step 5671/7134 completed (loss: 0.11807554960250854, acc: 0.966292142868042)
[2025-02-13 20:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36][root][INFO] - Training Epoch: 1/2, step 5672/7134 completed (loss: 0.11952316761016846, acc: 0.9777777791023254)
[2025-02-13 20:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36][root][INFO] - Training Epoch: 1/2, step 5673/7134 completed (loss: 0.11349393427371979, acc: 0.9794520735740662)
[2025-02-13 20:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36][root][INFO] - Training Epoch: 1/2, step 5674/7134 completed (loss: 0.1795782744884491, acc: 0.9591836929321289)
[2025-02-13 20:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37][root][INFO] - Training Epoch: 1/2, step 5675/7134 completed (loss: 0.2945833206176758, acc: 0.9066666960716248)
[2025-02-13 20:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37][root][INFO] - Training Epoch: 1/2, step 5676/7134 completed (loss: 0.3346406817436218, acc: 0.9022988677024841)
[2025-02-13 20:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37][root][INFO] - Training Epoch: 1/2, step 5677/7134 completed (loss: 0.31753212213516235, acc: 0.9120879173278809)
[2025-02-13 20:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38][root][INFO] - Training Epoch: 1/2, step 5678/7134 completed (loss: 0.14137309789657593, acc: 0.9691358208656311)
[2025-02-13 20:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38][root][INFO] - Training Epoch: 1/2, step 5679/7134 completed (loss: 0.46859100461006165, acc: 0.8860759735107422)
[2025-02-13 20:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39][root][INFO] - Training Epoch: 1/2, step 5680/7134 completed (loss: 0.5585342049598694, acc: 0.8913043737411499)
[2025-02-13 20:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39][root][INFO] - Training Epoch: 1/2, step 5681/7134 completed (loss: 0.41438597440719604, acc: 0.8787878751754761)
[2025-02-13 20:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39][root][INFO] - Training Epoch: 1/2, step 5682/7134 completed (loss: 0.2532518208026886, acc: 0.9337349534034729)
[2025-02-13 20:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40][root][INFO] - Training Epoch: 1/2, step 5683/7134 completed (loss: 0.3404325246810913, acc: 0.9222221970558167)
[2025-02-13 20:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40][root][INFO] - Training Epoch: 1/2, step 5684/7134 completed (loss: 0.4715718924999237, acc: 0.8902438879013062)
[2025-02-13 20:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41][root][INFO] - Training Epoch: 1/2, step 5685/7134 completed (loss: 0.2695091962814331, acc: 0.9212598204612732)
[2025-02-13 20:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41][root][INFO] - Training Epoch: 1/2, step 5686/7134 completed (loss: 0.2897931635379791, acc: 0.9064748287200928)
[2025-02-13 20:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41][root][INFO] - Training Epoch: 1/2, step 5687/7134 completed (loss: 0.30521026253700256, acc: 0.9242424368858337)
[2025-02-13 20:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42][root][INFO] - Training Epoch: 1/2, step 5688/7134 completed (loss: 0.4676159620285034, acc: 0.9277777671813965)
[2025-02-13 20:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42][root][INFO] - Training Epoch: 1/2, step 5689/7134 completed (loss: 0.19359742105007172, acc: 0.9634146094322205)
[2025-02-13 20:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43][root][INFO] - Training Epoch: 1/2, step 5690/7134 completed (loss: 0.13452361524105072, acc: 0.9742268323898315)
[2025-02-13 20:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43][root][INFO] - Training Epoch: 1/2, step 5691/7134 completed (loss: 0.25063881278038025, acc: 0.9453551769256592)
[2025-02-13 20:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43][root][INFO] - Training Epoch: 1/2, step 5692/7134 completed (loss: 0.19727230072021484, acc: 0.9599999785423279)
[2025-02-13 20:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44][root][INFO] - Training Epoch: 1/2, step 5693/7134 completed (loss: 0.06845200806856155, acc: 0.9874213933944702)
[2025-02-13 20:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44][root][INFO] - Training Epoch: 1/2, step 5694/7134 completed (loss: 0.10281403362751007, acc: 0.9825581312179565)
[2025-02-13 20:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44][root][INFO] - Training Epoch: 1/2, step 5695/7134 completed (loss: 0.06864862143993378, acc: 0.987730085849762)
[2025-02-13 20:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45][root][INFO] - Training Epoch: 1/2, step 5696/7134 completed (loss: 0.10780554264783859, acc: 0.9801980257034302)
[2025-02-13 20:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45][root][INFO] - Training Epoch: 1/2, step 5697/7134 completed (loss: 0.25454092025756836, acc: 0.9575757384300232)
[2025-02-13 20:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:46][root][INFO] - Training Epoch: 1/2, step 5698/7134 completed (loss: 0.09059974551200867, acc: 0.9886363744735718)
[2025-02-13 20:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:46][root][INFO] - Training Epoch: 1/2, step 5699/7134 completed (loss: 0.3328492045402527, acc: 0.9195402264595032)
[2025-02-13 20:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47][root][INFO] - Training Epoch: 1/2, step 5700/7134 completed (loss: 0.17352727055549622, acc: 0.9681528806686401)
[2025-02-13 20:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47][root][INFO] - Training Epoch: 1/2, step 5701/7134 completed (loss: 0.1857536882162094, acc: 0.9621621370315552)
[2025-02-13 20:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47][root][INFO] - Training Epoch: 1/2, step 5702/7134 completed (loss: 0.1268504559993744, acc: 0.9825581312179565)
[2025-02-13 20:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:48][root][INFO] - Training Epoch: 1/2, step 5703/7134 completed (loss: 0.03828717768192291, acc: 0.9937106966972351)
[2025-02-13 20:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:48][root][INFO] - Training Epoch: 1/2, step 5704/7134 completed (loss: 0.16424666345119476, acc: 0.9470587968826294)
[2025-02-13 20:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49][root][INFO] - Training Epoch: 1/2, step 5705/7134 completed (loss: 0.07064275443553925, acc: 0.9852941036224365)
[2025-02-13 20:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49][root][INFO] - Training Epoch: 1/2, step 5706/7134 completed (loss: 0.10723107308149338, acc: 0.9702970385551453)
[2025-02-13 20:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49][root][INFO] - Training Epoch: 1/2, step 5707/7134 completed (loss: 0.1525418907403946, acc: 0.9760000109672546)
[2025-02-13 20:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50][root][INFO] - Training Epoch: 1/2, step 5708/7134 completed (loss: 0.04284342750906944, acc: 0.9871794581413269)
[2025-02-13 20:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50][root][INFO] - Training Epoch: 1/2, step 5709/7134 completed (loss: 0.22232863306999207, acc: 0.95652174949646)
[2025-02-13 20:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51][root][INFO] - Training Epoch: 1/2, step 5710/7134 completed (loss: 0.19919681549072266, acc: 0.9428571462631226)
[2025-02-13 20:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51][root][INFO] - Training Epoch: 1/2, step 5711/7134 completed (loss: 0.21297603845596313, acc: 0.9408602118492126)
[2025-02-13 20:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51][root][INFO] - Training Epoch: 1/2, step 5712/7134 completed (loss: 0.0874280259013176, acc: 0.9677419066429138)
[2025-02-13 20:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52][root][INFO] - Training Epoch: 1/2, step 5713/7134 completed (loss: 0.09551414847373962, acc: 0.9775280952453613)
[2025-02-13 20:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52][root][INFO] - Training Epoch: 1/2, step 5714/7134 completed (loss: 0.08442520350217819, acc: 0.97826087474823)
[2025-02-13 20:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52][root][INFO] - Training Epoch: 1/2, step 5715/7134 completed (loss: 0.2934240996837616, acc: 0.9485294222831726)
[2025-02-13 20:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:53][root][INFO] - Training Epoch: 1/2, step 5716/7134 completed (loss: 0.2992182970046997, acc: 0.9305555820465088)
[2025-02-13 20:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:53][root][INFO] - Training Epoch: 1/2, step 5717/7134 completed (loss: 0.431259423494339, acc: 0.9181286692619324)
[2025-02-13 20:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54][root][INFO] - Training Epoch: 1/2, step 5718/7134 completed (loss: 0.15803959965705872, acc: 0.9539473652839661)
[2025-02-13 20:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54][root][INFO] - Training Epoch: 1/2, step 5719/7134 completed (loss: 0.223782479763031, acc: 0.9235293865203857)
[2025-02-13 20:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54][root][INFO] - Training Epoch: 1/2, step 5720/7134 completed (loss: 0.2566436231136322, acc: 0.9294871687889099)
[2025-02-13 20:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:55][root][INFO] - Training Epoch: 1/2, step 5721/7134 completed (loss: 0.19463253021240234, acc: 0.9326424598693848)
[2025-02-13 20:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:55][root][INFO] - Training Epoch: 1/2, step 5722/7134 completed (loss: 0.05912930890917778, acc: 0.9818181991577148)
[2025-02-13 20:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56][root][INFO] - Training Epoch: 1/2, step 5723/7134 completed (loss: 0.19800561666488647, acc: 0.9576719403266907)
[2025-02-13 20:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56][root][INFO] - Training Epoch: 1/2, step 5724/7134 completed (loss: 0.24554714560508728, acc: 0.939393937587738)
[2025-02-13 20:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56][root][INFO] - Training Epoch: 1/2, step 5725/7134 completed (loss: 0.17531242966651917, acc: 0.954285740852356)
[2025-02-13 20:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57][root][INFO] - Training Epoch: 1/2, step 5726/7134 completed (loss: 0.24533405900001526, acc: 0.946107804775238)
[2025-02-13 20:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57][root][INFO] - Training Epoch: 1/2, step 5727/7134 completed (loss: 0.06742201745510101, acc: 0.979899525642395)
[2025-02-13 20:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57][root][INFO] - Training Epoch: 1/2, step 5728/7134 completed (loss: 0.022270387038588524, acc: 1.0)
[2025-02-13 20:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58][root][INFO] - Training Epoch: 1/2, step 5729/7134 completed (loss: 0.07764722406864166, acc: 0.9805194735527039)
[2025-02-13 20:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58][root][INFO] - Training Epoch: 1/2, step 5730/7134 completed (loss: 0.17948278784751892, acc: 0.9481481313705444)
[2025-02-13 20:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59][root][INFO] - Training Epoch: 1/2, step 5731/7134 completed (loss: 0.23267099261283875, acc: 0.9375)
[2025-02-13 20:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59][root][INFO] - Training Epoch: 1/2, step 5732/7134 completed (loss: 0.8787912726402283, acc: 0.7891566157341003)
[2025-02-13 20:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59][root][INFO] - Training Epoch: 1/2, step 5733/7134 completed (loss: 0.29972097277641296, acc: 0.9240506291389465)
[2025-02-13 20:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00][root][INFO] - Training Epoch: 1/2, step 5734/7134 completed (loss: 0.3173983097076416, acc: 0.9107142686843872)
[2025-02-13 20:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00][root][INFO] - Training Epoch: 1/2, step 5735/7134 completed (loss: 0.33960390090942383, acc: 0.940397322177887)
[2025-02-13 20:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00][root][INFO] - Training Epoch: 1/2, step 5736/7134 completed (loss: 0.3575877845287323, acc: 0.9171270728111267)
[2025-02-13 20:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01][root][INFO] - Training Epoch: 1/2, step 5737/7134 completed (loss: 0.137833371758461, acc: 0.9473684430122375)
[2025-02-13 20:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01][root][INFO] - Training Epoch: 1/2, step 5738/7134 completed (loss: 0.11955419927835464, acc: 0.9595959782600403)
[2025-02-13 20:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02][root][INFO] - Training Epoch: 1/2, step 5739/7134 completed (loss: 0.24710462987422943, acc: 0.9416666626930237)
[2025-02-13 20:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02][root][INFO] - Training Epoch: 1/2, step 5740/7134 completed (loss: 0.12701423466205597, acc: 0.9583333134651184)
[2025-02-13 20:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02][root][INFO] - Training Epoch: 1/2, step 5741/7134 completed (loss: 0.12044143676757812, acc: 0.9659090638160706)
[2025-02-13 20:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03][root][INFO] - Training Epoch: 1/2, step 5742/7134 completed (loss: 0.07118429243564606, acc: 0.9887640476226807)
[2025-02-13 20:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03][root][INFO] - Training Epoch: 1/2, step 5743/7134 completed (loss: 0.09342432022094727, acc: 0.9739583134651184)
[2025-02-13 20:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03][root][INFO] - Training Epoch: 1/2, step 5744/7134 completed (loss: 0.042602475732564926, acc: 1.0)
[2025-02-13 20:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04][root][INFO] - Training Epoch: 1/2, step 5745/7134 completed (loss: 0.22540028393268585, acc: 0.9550561904907227)
[2025-02-13 20:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04][root][INFO] - Training Epoch: 1/2, step 5746/7134 completed (loss: 0.13120201230049133, acc: 0.9629629850387573)
[2025-02-13 20:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05][root][INFO] - Training Epoch: 1/2, step 5747/7134 completed (loss: 0.14604046940803528, acc: 0.9454545378684998)
[2025-02-13 20:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05][root][INFO] - Training Epoch: 1/2, step 5748/7134 completed (loss: 0.06602463126182556, acc: 0.9763779640197754)
[2025-02-13 20:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05][root][INFO] - Training Epoch: 1/2, step 5749/7134 completed (loss: 0.2083018571138382, acc: 0.9644970297813416)
[2025-02-13 20:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06][root][INFO] - Training Epoch: 1/2, step 5750/7134 completed (loss: 0.1712222397327423, acc: 0.9497206807136536)
[2025-02-13 20:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06][root][INFO] - Training Epoch: 1/2, step 5751/7134 completed (loss: 0.21962468326091766, acc: 0.940397322177887)
[2025-02-13 20:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06][root][INFO] - Training Epoch: 1/2, step 5752/7134 completed (loss: 0.7302670478820801, acc: 0.8244274854660034)
[2025-02-13 20:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:07][root][INFO] - Training Epoch: 1/2, step 5753/7134 completed (loss: 0.24100223183631897, acc: 0.926174521446228)
[2025-02-13 20:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:07][root][INFO] - Training Epoch: 1/2, step 5754/7134 completed (loss: 0.2601117491722107, acc: 0.9367815852165222)
[2025-02-13 20:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08][root][INFO] - Training Epoch: 1/2, step 5755/7134 completed (loss: 0.3362273871898651, acc: 0.9285714030265808)
[2025-02-13 20:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08][root][INFO] - Training Epoch: 1/2, step 5756/7134 completed (loss: 0.31792184710502625, acc: 0.9011628031730652)
[2025-02-13 20:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08][root][INFO] - Training Epoch: 1/2, step 5757/7134 completed (loss: 0.35154056549072266, acc: 0.9213483333587646)
[2025-02-13 20:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09][root][INFO] - Training Epoch: 1/2, step 5758/7134 completed (loss: 0.4085327982902527, acc: 0.8888888955116272)
[2025-02-13 20:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09][root][INFO] - Training Epoch: 1/2, step 5759/7134 completed (loss: 0.6394093632698059, acc: 0.8834951519966125)
[2025-02-13 20:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10][root][INFO] - Training Epoch: 1/2, step 5760/7134 completed (loss: 0.2004946768283844, acc: 0.9610389471054077)
[2025-02-13 20:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10][root][INFO] - Training Epoch: 1/2, step 5761/7134 completed (loss: 0.1711927205324173, acc: 0.9440000057220459)
[2025-02-13 20:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10][root][INFO] - Training Epoch: 1/2, step 5762/7134 completed (loss: 0.188357874751091, acc: 0.961240291595459)
[2025-02-13 20:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11][root][INFO] - Training Epoch: 1/2, step 5763/7134 completed (loss: 0.15720753371715546, acc: 0.9562841653823853)
[2025-02-13 20:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11][root][INFO] - Training Epoch: 1/2, step 5764/7134 completed (loss: 0.03604760766029358, acc: 0.9923664331436157)
[2025-02-13 20:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11][root][INFO] - Training Epoch: 1/2, step 5765/7134 completed (loss: 0.19895121455192566, acc: 0.955974817276001)
[2025-02-13 20:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12][root][INFO] - Training Epoch: 1/2, step 5766/7134 completed (loss: 0.09627090394496918, acc: 0.9870967864990234)
[2025-02-13 20:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12][root][INFO] - Training Epoch: 1/2, step 5767/7134 completed (loss: 0.07738662511110306, acc: 0.9875776171684265)
[2025-02-13 20:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13][root][INFO] - Training Epoch: 1/2, step 5768/7134 completed (loss: 0.0788561999797821, acc: 0.9750000238418579)
[2025-02-13 20:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13][root][INFO] - Training Epoch: 1/2, step 5769/7134 completed (loss: 0.06159265339374542, acc: 0.9928571581840515)
[2025-02-13 20:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13][root][INFO] - Training Epoch: 1/2, step 5770/7134 completed (loss: 0.11093632131814957, acc: 0.9783783555030823)
[2025-02-13 20:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14][root][INFO] - Training Epoch: 1/2, step 5771/7134 completed (loss: 0.24557946622371674, acc: 0.9386503100395203)
[2025-02-13 20:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14][root][INFO] - Training Epoch: 1/2, step 5772/7134 completed (loss: 0.053398486226797104, acc: 0.9833333492279053)
[2025-02-13 20:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14][root][INFO] - Training Epoch: 1/2, step 5773/7134 completed (loss: 0.12197573482990265, acc: 0.969924807548523)
[2025-02-13 20:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:15][root][INFO] - Training Epoch: 1/2, step 5774/7134 completed (loss: 0.09963548928499222, acc: 0.9753086566925049)
[2025-02-13 20:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:15][root][INFO] - Training Epoch: 1/2, step 5775/7134 completed (loss: 0.07961621135473251, acc: 0.9800000190734863)
[2025-02-13 20:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16][root][INFO] - Training Epoch: 1/2, step 5776/7134 completed (loss: 0.1654794067144394, acc: 0.9550561904907227)
[2025-02-13 20:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16][root][INFO] - Training Epoch: 1/2, step 5777/7134 completed (loss: 0.21672777831554413, acc: 0.9579439163208008)
[2025-02-13 20:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16][root][INFO] - Training Epoch: 1/2, step 5778/7134 completed (loss: 0.36583152413368225, acc: 0.9146341681480408)
[2025-02-13 20:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17][root][INFO] - Training Epoch: 1/2, step 5779/7134 completed (loss: 0.2580965757369995, acc: 0.9588235020637512)
[2025-02-13 20:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17][root][INFO] - Training Epoch: 1/2, step 5780/7134 completed (loss: 0.20033077895641327, acc: 0.9552238583564758)
[2025-02-13 20:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17][root][INFO] - Training Epoch: 1/2, step 5781/7134 completed (loss: 0.24927639961242676, acc: 0.9479768872261047)
[2025-02-13 20:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18][root][INFO] - Training Epoch: 1/2, step 5782/7134 completed (loss: 0.12018270045518875, acc: 0.9779005646705627)
[2025-02-13 20:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18][root][INFO] - Training Epoch: 1/2, step 5783/7134 completed (loss: 0.16389600932598114, acc: 0.9653179049491882)
[2025-02-13 20:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18][root][INFO] - Training Epoch: 1/2, step 5784/7134 completed (loss: 0.15601544082164764, acc: 0.9604519605636597)
[2025-02-13 20:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19][root][INFO] - Training Epoch: 1/2, step 5785/7134 completed (loss: 0.22717644274234772, acc: 0.9513513445854187)
[2025-02-13 20:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19][root][INFO] - Training Epoch: 1/2, step 5786/7134 completed (loss: 0.17269496619701385, acc: 0.9523809552192688)
[2025-02-13 20:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20][root][INFO] - Training Epoch: 1/2, step 5787/7134 completed (loss: 0.13818161189556122, acc: 0.9581151604652405)
[2025-02-13 20:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20][root][INFO] - Training Epoch: 1/2, step 5788/7134 completed (loss: 0.07357699424028397, acc: 0.9868420958518982)
[2025-02-13 20:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20][root][INFO] - Training Epoch: 1/2, step 5789/7134 completed (loss: 0.11633875221014023, acc: 0.9646464586257935)
[2025-02-13 20:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21][root][INFO] - Training Epoch: 1/2, step 5790/7134 completed (loss: 0.21849384903907776, acc: 0.9642857313156128)
[2025-02-13 20:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21][root][INFO] - Training Epoch: 1/2, step 5791/7134 completed (loss: 0.10712303221225739, acc: 0.9626865386962891)
[2025-02-13 20:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21][root][INFO] - Training Epoch: 1/2, step 5792/7134 completed (loss: 0.44569867849349976, acc: 0.8590604066848755)
[2025-02-13 20:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22][root][INFO] - Training Epoch: 1/2, step 5793/7134 completed (loss: 0.30317941308021545, acc: 0.9263803958892822)
[2025-02-13 20:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22][root][INFO] - Training Epoch: 1/2, step 5794/7134 completed (loss: 0.38246044516563416, acc: 0.9171974658966064)
[2025-02-13 20:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23][root][INFO] - Training Epoch: 1/2, step 5795/7134 completed (loss: 0.5039778351783752, acc: 0.8947368264198303)
[2025-02-13 20:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23][root][INFO] - Training Epoch: 1/2, step 5796/7134 completed (loss: 0.3669492304325104, acc: 0.9083969593048096)
[2025-02-13 20:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23][root][INFO] - Training Epoch: 1/2, step 5797/7134 completed (loss: 0.23709551990032196, acc: 0.9395973086357117)
[2025-02-13 20:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24][root][INFO] - Training Epoch: 1/2, step 5798/7134 completed (loss: 0.9440882205963135, acc: 0.8662790656089783)
[2025-02-13 20:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24][root][INFO] - Training Epoch: 1/2, step 5799/7134 completed (loss: 0.23562951385974884, acc: 0.9448819160461426)
[2025-02-13 20:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24][root][INFO] - Training Epoch: 1/2, step 5800/7134 completed (loss: 0.43640947341918945, acc: 0.9064748287200928)
[2025-02-13 20:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25][root][INFO] - Training Epoch: 1/2, step 5801/7134 completed (loss: 0.30611371994018555, acc: 0.9259259104728699)
[2025-02-13 20:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25][root][INFO] - Training Epoch: 1/2, step 5802/7134 completed (loss: 0.166427344083786, acc: 0.9558823704719543)
[2025-02-13 20:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26][root][INFO] - Training Epoch: 1/2, step 5803/7134 completed (loss: 0.07639476656913757, acc: 0.982758641242981)
[2025-02-13 20:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26][root][INFO] - Training Epoch: 1/2, step 5804/7134 completed (loss: 0.3269825875759125, acc: 0.9044585824012756)
[2025-02-13 20:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26][root][INFO] - Training Epoch: 1/2, step 5805/7134 completed (loss: 0.2455967515707016, acc: 0.940397322177887)
[2025-02-13 20:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27][root][INFO] - Training Epoch: 1/2, step 5806/7134 completed (loss: 0.15541458129882812, acc: 0.9636363387107849)
[2025-02-13 20:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27][root][INFO] - Training Epoch: 1/2, step 5807/7134 completed (loss: 0.2546252906322479, acc: 0.9507042169570923)
[2025-02-13 20:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28][root][INFO] - Training Epoch: 1/2, step 5808/7134 completed (loss: 0.13669556379318237, acc: 0.9683544039726257)
[2025-02-13 20:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28][root][INFO] - Training Epoch: 1/2, step 5809/7134 completed (loss: 0.13320474326610565, acc: 0.9509803652763367)
[2025-02-13 20:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28][root][INFO] - Training Epoch: 1/2, step 5810/7134 completed (loss: 0.1072879433631897, acc: 0.9669421315193176)
[2025-02-13 20:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29][root][INFO] - Training Epoch: 1/2, step 5811/7134 completed (loss: 0.11755713820457458, acc: 0.9780219793319702)
[2025-02-13 20:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29][root][INFO] - Training Epoch: 1/2, step 5812/7134 completed (loss: 0.08783150464296341, acc: 0.9779411554336548)
[2025-02-13 20:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30][root][INFO] - Training Epoch: 1/2, step 5813/7134 completed (loss: 0.11183309555053711, acc: 0.9836065769195557)
[2025-02-13 20:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30][root][INFO] - Training Epoch: 1/2, step 5814/7134 completed (loss: 0.139595165848732, acc: 0.9520000219345093)
[2025-02-13 20:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30][root][INFO] - Training Epoch: 1/2, step 5815/7134 completed (loss: 0.1620028167963028, acc: 0.9482758641242981)
[2025-02-13 20:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31][root][INFO] - Training Epoch: 1/2, step 5816/7134 completed (loss: 0.029371144250035286, acc: 0.9946523904800415)
[2025-02-13 20:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31][root][INFO] - Training Epoch: 1/2, step 5817/7134 completed (loss: 0.1652171015739441, acc: 0.9714285731315613)
[2025-02-13 20:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31][root][INFO] - Training Epoch: 1/2, step 5818/7134 completed (loss: 0.09025432914495468, acc: 0.9673202633857727)
[2025-02-13 20:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32][root][INFO] - Training Epoch: 1/2, step 5819/7134 completed (loss: 0.0760447084903717, acc: 0.9929577708244324)
[2025-02-13 20:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32][root][INFO] - Training Epoch: 1/2, step 5820/7134 completed (loss: 0.10995364189147949, acc: 0.9834710955619812)
[2025-02-13 20:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33][root][INFO] - Training Epoch: 1/2, step 5821/7134 completed (loss: 0.10590507090091705, acc: 0.9862068891525269)
[2025-02-13 20:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33][root][INFO] - Training Epoch: 1/2, step 5822/7134 completed (loss: 0.07066918909549713, acc: 0.9865771532058716)
[2025-02-13 20:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33][root][INFO] - Training Epoch: 1/2, step 5823/7134 completed (loss: 0.06424444913864136, acc: 0.9659863710403442)
[2025-02-13 20:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:34][root][INFO] - Training Epoch: 1/2, step 5824/7134 completed (loss: 0.049535688012838364, acc: 0.9939393997192383)
[2025-02-13 20:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:34][root][INFO] - Training Epoch: 1/2, step 5825/7134 completed (loss: 0.1787680834531784, acc: 0.9541984796524048)
[2025-02-13 20:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35][root][INFO] - Training Epoch: 1/2, step 5826/7134 completed (loss: 0.07233034074306488, acc: 0.9767441749572754)
[2025-02-13 20:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35][root][INFO] - Training Epoch: 1/2, step 5827/7134 completed (loss: 0.06947759538888931, acc: 1.0)
[2025-02-13 20:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35][root][INFO] - Training Epoch: 1/2, step 5828/7134 completed (loss: 0.06678907573223114, acc: 0.9803921580314636)
[2025-02-13 20:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36][root][INFO] - Training Epoch: 1/2, step 5829/7134 completed (loss: 0.06360267102718353, acc: 0.9817073345184326)
[2025-02-13 20:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36][root][INFO] - Training Epoch: 1/2, step 5830/7134 completed (loss: 0.22191734611988068, acc: 0.9440000057220459)
[2025-02-13 20:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37][root][INFO] - Training Epoch: 1/2, step 5831/7134 completed (loss: 0.11344525218009949, acc: 0.9591836929321289)
[2025-02-13 20:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37][root][INFO] - Training Epoch: 1/2, step 5832/7134 completed (loss: 0.24032944440841675, acc: 0.9389312863349915)
[2025-02-13 20:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37][root][INFO] - Training Epoch: 1/2, step 5833/7134 completed (loss: 0.059159766882658005, acc: 0.9849624037742615)
[2025-02-13 20:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38][root][INFO] - Training Epoch: 1/2, step 5834/7134 completed (loss: 0.21799473464488983, acc: 0.9298245906829834)
[2025-02-13 20:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38][root][INFO] - Training Epoch: 1/2, step 5835/7134 completed (loss: 0.1831480711698532, acc: 0.9599999785423279)
[2025-02-13 20:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38][root][INFO] - Training Epoch: 1/2, step 5836/7134 completed (loss: 0.2714309096336365, acc: 0.9490445852279663)
[2025-02-13 20:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39][root][INFO] - Training Epoch: 1/2, step 5837/7134 completed (loss: 0.5381166934967041, acc: 0.899328887462616)
[2025-02-13 20:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39][root][INFO] - Training Epoch: 1/2, step 5838/7134 completed (loss: 0.20164552330970764, acc: 0.9640287756919861)
[2025-02-13 20:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40][root][INFO] - Training Epoch: 1/2, step 5839/7134 completed (loss: 0.3436780869960785, acc: 0.9202898740768433)
[2025-02-13 20:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40][root][INFO] - Training Epoch: 1/2, step 5840/7134 completed (loss: 0.01886547915637493, acc: 1.0)
[2025-02-13 20:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40][root][INFO] - Training Epoch: 1/2, step 5841/7134 completed (loss: 0.31546634435653687, acc: 0.925000011920929)
[2025-02-13 20:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41][root][INFO] - Training Epoch: 1/2, step 5842/7134 completed (loss: 0.09646874666213989, acc: 0.9586777091026306)
[2025-02-13 20:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41][root][INFO] - Training Epoch: 1/2, step 5843/7134 completed (loss: 0.22803974151611328, acc: 0.9702380895614624)
[2025-02-13 20:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42][root][INFO] - Training Epoch: 1/2, step 5844/7134 completed (loss: 0.2159424126148224, acc: 0.9638554453849792)
[2025-02-13 20:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42][root][INFO] - Training Epoch: 1/2, step 5845/7134 completed (loss: 0.14193589985370636, acc: 0.9545454382896423)
[2025-02-13 20:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42][root][INFO] - Training Epoch: 1/2, step 5846/7134 completed (loss: 0.17087145149707794, acc: 0.9702380895614624)
[2025-02-13 20:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43][root][INFO] - Training Epoch: 1/2, step 5847/7134 completed (loss: 0.08545353263616562, acc: 0.9759036302566528)
[2025-02-13 20:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43][root][INFO] - Training Epoch: 1/2, step 5848/7134 completed (loss: 0.11059214919805527, acc: 0.9797297120094299)
[2025-02-13 20:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43][root][INFO] - Training Epoch: 1/2, step 5849/7134 completed (loss: 0.21533916890621185, acc: 0.9415584206581116)
[2025-02-13 20:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44][root][INFO] - Training Epoch: 1/2, step 5850/7134 completed (loss: 0.13126157224178314, acc: 0.9577465057373047)
[2025-02-13 20:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44][root][INFO] - Training Epoch: 1/2, step 5851/7134 completed (loss: 0.2388746738433838, acc: 0.9452054500579834)
[2025-02-13 20:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45][root][INFO] - Training Epoch: 1/2, step 5852/7134 completed (loss: 0.1511729508638382, acc: 0.9650349617004395)
[2025-02-13 20:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45][root][INFO] - Training Epoch: 1/2, step 5853/7134 completed (loss: 0.2086811065673828, acc: 0.9213483333587646)
[2025-02-13 20:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45][root][INFO] - Training Epoch: 1/2, step 5854/7134 completed (loss: 0.16303899884223938, acc: 0.9769230484962463)
[2025-02-13 20:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:46][root][INFO] - Training Epoch: 1/2, step 5855/7134 completed (loss: 0.1954634189605713, acc: 0.9661017060279846)
[2025-02-13 20:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:46][root][INFO] - Training Epoch: 1/2, step 5856/7134 completed (loss: 0.2814454138278961, acc: 0.9154929518699646)
[2025-02-13 20:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47][root][INFO] - Training Epoch: 1/2, step 5857/7134 completed (loss: 0.23044072091579437, acc: 0.9398496150970459)
[2025-02-13 20:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47][root][INFO] - Training Epoch: 1/2, step 5858/7134 completed (loss: 0.12561854720115662, acc: 0.963302731513977)
[2025-02-13 20:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47][root][INFO] - Training Epoch: 1/2, step 5859/7134 completed (loss: 0.14402484893798828, acc: 0.9568965435028076)
[2025-02-13 20:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:48][root][INFO] - Training Epoch: 1/2, step 5860/7134 completed (loss: 0.1667432188987732, acc: 0.9545454382896423)
[2025-02-13 20:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:48][root][INFO] - Training Epoch: 1/2, step 5861/7134 completed (loss: 0.10369652509689331, acc: 0.961904764175415)
[2025-02-13 20:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49][root][INFO] - Training Epoch: 1/2, step 5862/7134 completed (loss: 0.22082923352718353, acc: 0.9406779408454895)
[2025-02-13 20:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49][root][INFO] - Training Epoch: 1/2, step 5863/7134 completed (loss: 0.31320735812187195, acc: 0.9462365508079529)
[2025-02-13 20:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49][root][INFO] - Training Epoch: 1/2, step 5864/7134 completed (loss: 0.3708745539188385, acc: 0.9298245906829834)
[2025-02-13 20:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50][root][INFO] - Training Epoch: 1/2, step 5865/7134 completed (loss: 0.1758987307548523, acc: 0.9652777910232544)
[2025-02-13 20:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50][root][INFO] - Training Epoch: 1/2, step 5866/7134 completed (loss: 0.08586081117391586, acc: 0.9835164546966553)
[2025-02-13 20:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51][root][INFO] - Training Epoch: 1/2, step 5867/7134 completed (loss: 0.14470790326595306, acc: 0.9662162065505981)
[2025-02-13 20:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51][root][INFO] - Training Epoch: 1/2, step 5868/7134 completed (loss: 0.25877130031585693, acc: 0.9281045794487)
[2025-02-13 20:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51][root][INFO] - Training Epoch: 1/2, step 5869/7134 completed (loss: 0.149249866604805, acc: 0.9580838084220886)
[2025-02-13 20:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52][root][INFO] - Training Epoch: 1/2, step 5870/7134 completed (loss: 0.14337904751300812, acc: 0.9629629850387573)
[2025-02-13 20:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52][root][INFO] - Training Epoch: 1/2, step 5871/7134 completed (loss: 0.08940953016281128, acc: 0.9724770784378052)
[2025-02-13 20:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52][root][INFO] - Training Epoch: 1/2, step 5872/7134 completed (loss: 0.16708025336265564, acc: 0.9627329111099243)
[2025-02-13 20:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53][root][INFO] - Training Epoch: 1/2, step 5873/7134 completed (loss: 0.1942470371723175, acc: 0.9572192430496216)
[2025-02-13 20:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53][root][INFO] - Training Epoch: 1/2, step 5874/7134 completed (loss: 0.09264271706342697, acc: 0.9682539701461792)
[2025-02-13 20:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54][root][INFO] - Training Epoch: 1/2, step 5875/7134 completed (loss: 0.20955424010753632, acc: 0.9166666865348816)
[2025-02-13 20:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54][root][INFO] - Training Epoch: 1/2, step 5876/7134 completed (loss: 0.07726271450519562, acc: 0.966292142868042)
[2025-02-13 20:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55][root][INFO] - Training Epoch: 1/2, step 5877/7134 completed (loss: 0.1174808144569397, acc: 0.9553072452545166)
[2025-02-13 20:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55][root][INFO] - Training Epoch: 1/2, step 5878/7134 completed (loss: 0.1672239601612091, acc: 0.9545454382896423)
[2025-02-13 20:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55][root][INFO] - Training Epoch: 1/2, step 5879/7134 completed (loss: 0.07671378552913666, acc: 0.9777777791023254)
[2025-02-13 20:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56][root][INFO] - Training Epoch: 1/2, step 5880/7134 completed (loss: 0.15302038192749023, acc: 0.9642857313156128)
[2025-02-13 20:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56][root][INFO] - Training Epoch: 1/2, step 5881/7134 completed (loss: 0.11102336645126343, acc: 0.9658119678497314)
[2025-02-13 20:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56][root][INFO] - Training Epoch: 1/2, step 5882/7134 completed (loss: 0.06181083619594574, acc: 0.9803921580314636)
[2025-02-13 20:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57][root][INFO] - Training Epoch: 1/2, step 5883/7134 completed (loss: 0.06644289940595627, acc: 0.9769230484962463)
[2025-02-13 20:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57][root][INFO] - Training Epoch: 1/2, step 5884/7134 completed (loss: 0.26374098658561707, acc: 0.920634925365448)
[2025-02-13 20:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58][root][INFO] - Training Epoch: 1/2, step 5885/7134 completed (loss: 0.4158583879470825, acc: 0.9313725233078003)
[2025-02-13 20:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58][root][INFO] - Training Epoch: 1/2, step 5886/7134 completed (loss: 0.06988025456666946, acc: 0.976190447807312)
[2025-02-13 20:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58][root][INFO] - Training Epoch: 1/2, step 5887/7134 completed (loss: 0.04240823909640312, acc: 0.9916666746139526)
[2025-02-13 20:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59][root][INFO] - Training Epoch: 1/2, step 5888/7134 completed (loss: 0.3154487609863281, acc: 0.9290780425071716)
[2025-02-13 20:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59][root][INFO] - Training Epoch: 1/2, step 5889/7134 completed (loss: 0.15035419166088104, acc: 0.9807692170143127)
[2025-02-13 20:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59][root][INFO] - Training Epoch: 1/2, step 5890/7134 completed (loss: 0.05291453003883362, acc: 0.9864864945411682)
[2025-02-13 20:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00][root][INFO] - Training Epoch: 1/2, step 5891/7134 completed (loss: 0.2400178611278534, acc: 0.9571428298950195)
[2025-02-13 20:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00][root][INFO] - Training Epoch: 1/2, step 5892/7134 completed (loss: 0.14158709347248077, acc: 0.9767441749572754)
[2025-02-13 20:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01][root][INFO] - Training Epoch: 1/2, step 5893/7134 completed (loss: 0.08697731792926788, acc: 0.9754601120948792)
[2025-02-13 20:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01][root][INFO] - Training Epoch: 1/2, step 5894/7134 completed (loss: 0.2043270468711853, acc: 0.9594594836235046)
[2025-02-13 20:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01][root][INFO] - Training Epoch: 1/2, step 5895/7134 completed (loss: 0.10798248648643494, acc: 0.96875)
[2025-02-13 20:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02][root][INFO] - Training Epoch: 1/2, step 5896/7134 completed (loss: 0.10858874022960663, acc: 0.9748427867889404)
[2025-02-13 20:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02][root][INFO] - Training Epoch: 1/2, step 5897/7134 completed (loss: 0.11144619435071945, acc: 0.9772727489471436)
[2025-02-13 20:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02][root][INFO] - Training Epoch: 1/2, step 5898/7134 completed (loss: 0.12417346984148026, acc: 0.9642857313156128)
[2025-02-13 20:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03][root][INFO] - Training Epoch: 1/2, step 5899/7134 completed (loss: 0.14613890647888184, acc: 0.9743589758872986)
[2025-02-13 20:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03][root][INFO] - Training Epoch: 1/2, step 5900/7134 completed (loss: 0.1004386693239212, acc: 0.9629629850387573)
[2025-02-13 20:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03][root][INFO] - Training Epoch: 1/2, step 5901/7134 completed (loss: 0.1855054497718811, acc: 0.969072163105011)
[2025-02-13 20:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04][root][INFO] - Training Epoch: 1/2, step 5902/7134 completed (loss: 0.06853366643190384, acc: 0.9897959232330322)
[2025-02-13 20:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04][root][INFO] - Training Epoch: 1/2, step 5903/7134 completed (loss: 0.11896080523729324, acc: 0.9734513163566589)
[2025-02-13 20:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04][root][INFO] - Training Epoch: 1/2, step 5904/7134 completed (loss: 0.19395986199378967, acc: 0.9577465057373047)
[2025-02-13 20:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05][root][INFO] - Training Epoch: 1/2, step 5905/7134 completed (loss: 0.18853116035461426, acc: 0.9527027010917664)
[2025-02-13 20:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05][root][INFO] - Training Epoch: 1/2, step 5906/7134 completed (loss: 0.1402900367975235, acc: 0.9673202633857727)
[2025-02-13 20:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05][root][INFO] - Training Epoch: 1/2, step 5907/7134 completed (loss: 0.10690630227327347, acc: 0.9791666865348816)
[2025-02-13 20:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06][root][INFO] - Training Epoch: 1/2, step 5908/7134 completed (loss: 0.12452229857444763, acc: 0.9740259647369385)
[2025-02-13 20:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06][root][INFO] - Training Epoch: 1/2, step 5909/7134 completed (loss: 0.25714242458343506, acc: 0.956204354763031)
[2025-02-13 20:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07][root][INFO] - Training Epoch: 1/2, step 5910/7134 completed (loss: 0.27178603410720825, acc: 0.9337748289108276)
[2025-02-13 20:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07][root][INFO] - Training Epoch: 1/2, step 5911/7134 completed (loss: 0.11882534623146057, acc: 0.95652174949646)
[2025-02-13 20:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07][root][INFO] - Training Epoch: 1/2, step 5912/7134 completed (loss: 0.08191754668951035, acc: 0.984375)
[2025-02-13 20:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:08][root][INFO] - Training Epoch: 1/2, step 5913/7134 completed (loss: 0.18167683482170105, acc: 0.9526627063751221)
[2025-02-13 20:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:08][root][INFO] - Training Epoch: 1/2, step 5914/7134 completed (loss: 0.1990443468093872, acc: 0.9642857313156128)
[2025-02-13 20:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09][root][INFO] - Training Epoch: 1/2, step 5915/7134 completed (loss: 0.17264099419116974, acc: 0.9694656729698181)
[2025-02-13 20:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09][root][INFO] - Training Epoch: 1/2, step 5916/7134 completed (loss: 0.1728696972131729, acc: 0.9536423683166504)
[2025-02-13 20:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09][root][INFO] - Training Epoch: 1/2, step 5917/7134 completed (loss: 0.056221846491098404, acc: 0.9760000109672546)
[2025-02-13 20:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10][root][INFO] - Training Epoch: 1/2, step 5918/7134 completed (loss: 0.23765483498573303, acc: 0.9440559148788452)
[2025-02-13 20:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10][root][INFO] - Training Epoch: 1/2, step 5919/7134 completed (loss: 0.06898008286952972, acc: 0.9701492786407471)
[2025-02-13 20:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10][root][INFO] - Training Epoch: 1/2, step 5920/7134 completed (loss: 0.04883400350809097, acc: 1.0)
[2025-02-13 20:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11][root][INFO] - Training Epoch: 1/2, step 5921/7134 completed (loss: 0.15882830321788788, acc: 0.9482758641242981)
[2025-02-13 20:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11][root][INFO] - Training Epoch: 1/2, step 5922/7134 completed (loss: 0.6632906198501587, acc: 0.8653846383094788)
[2025-02-13 20:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12][root][INFO] - Training Epoch: 1/2, step 5923/7134 completed (loss: 0.24305397272109985, acc: 0.9556962251663208)
[2025-02-13 20:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12][root][INFO] - Training Epoch: 1/2, step 5924/7134 completed (loss: 0.16785341501235962, acc: 0.942307710647583)
[2025-02-13 20:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12][root][INFO] - Training Epoch: 1/2, step 5925/7134 completed (loss: 0.8365897536277771, acc: 0.8333333134651184)
[2025-02-13 20:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13][root][INFO] - Training Epoch: 1/2, step 5926/7134 completed (loss: 0.36711302399635315, acc: 0.8881579041481018)
[2025-02-13 20:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13][root][INFO] - Training Epoch: 1/2, step 5927/7134 completed (loss: 0.27022698521614075, acc: 0.9107142686843872)
[2025-02-13 20:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14][root][INFO] - Training Epoch: 1/2, step 5928/7134 completed (loss: 1.1044992208480835, acc: 0.837837815284729)
[2025-02-13 20:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14][root][INFO] - Training Epoch: 1/2, step 5929/7134 completed (loss: 0.2876260280609131, acc: 0.9354838728904724)
[2025-02-13 20:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14][root][INFO] - Training Epoch: 1/2, step 5930/7134 completed (loss: 0.7428902387619019, acc: 0.8429751992225647)
[2025-02-13 20:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15][root][INFO] - Training Epoch: 1/2, step 5931/7134 completed (loss: 0.23968231678009033, acc: 0.9230769276618958)
[2025-02-13 20:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15][root][INFO] - Training Epoch: 1/2, step 5932/7134 completed (loss: 0.5341504216194153, acc: 0.8848921060562134)
[2025-02-13 20:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15][root][INFO] - Training Epoch: 1/2, step 5933/7134 completed (loss: 0.5308579206466675, acc: 0.8823529481887817)
[2025-02-13 20:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:16][root][INFO] - Training Epoch: 1/2, step 5934/7134 completed (loss: 0.27137506008148193, acc: 0.9285714030265808)
[2025-02-13 20:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:16][root][INFO] - Training Epoch: 1/2, step 5935/7134 completed (loss: 0.07662349194288254, acc: 0.9754098653793335)
[2025-02-13 20:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17][root][INFO] - Training Epoch: 1/2, step 5936/7134 completed (loss: 0.5393194556236267, acc: 0.89570552110672)
[2025-02-13 20:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17][root][INFO] - Training Epoch: 1/2, step 5937/7134 completed (loss: 0.11534324288368225, acc: 0.9647887349128723)
[2025-02-13 20:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17][root][INFO] - Training Epoch: 1/2, step 5938/7134 completed (loss: 0.22311154007911682, acc: 0.9473684430122375)
[2025-02-13 20:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18][root][INFO] - Training Epoch: 1/2, step 5939/7134 completed (loss: 1.0043809413909912, acc: 0.8017241358757019)
[2025-02-13 20:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18][root][INFO] - Training Epoch: 1/2, step 5940/7134 completed (loss: 0.21387404203414917, acc: 0.9503105878829956)
[2025-02-13 20:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18][root][INFO] - Training Epoch: 1/2, step 5941/7134 completed (loss: 0.2548893988132477, acc: 0.9166666865348816)
[2025-02-13 20:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19][root][INFO] - Training Epoch: 1/2, step 5942/7134 completed (loss: 0.057648833841085434, acc: 0.9679999947547913)
[2025-02-13 20:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19][root][INFO] - Training Epoch: 1/2, step 5943/7134 completed (loss: 0.07697568088769913, acc: 0.9815950989723206)
[2025-02-13 20:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20][root][INFO] - Training Epoch: 1/2, step 5944/7134 completed (loss: 0.1457025557756424, acc: 0.9642857313156128)
[2025-02-13 20:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20][root][INFO] - Training Epoch: 1/2, step 5945/7134 completed (loss: 0.0719941183924675, acc: 0.9822485446929932)
[2025-02-13 20:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20][root][INFO] - Training Epoch: 1/2, step 5946/7134 completed (loss: 0.14085423946380615, acc: 0.9513888955116272)
[2025-02-13 20:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21][root][INFO] - Training Epoch: 1/2, step 5947/7134 completed (loss: 0.22805795073509216, acc: 0.9375)
[2025-02-13 20:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21][root][INFO] - Training Epoch: 1/2, step 5948/7134 completed (loss: 0.3288740813732147, acc: 0.934959352016449)
[2025-02-13 20:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22][root][INFO] - Training Epoch: 1/2, step 5949/7134 completed (loss: 0.08494681119918823, acc: 0.9888888597488403)
[2025-02-13 20:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22][root][INFO] - Training Epoch: 1/2, step 5950/7134 completed (loss: 0.2958437204360962, acc: 0.9306930899620056)
[2025-02-13 20:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22][root][INFO] - Training Epoch: 1/2, step 5951/7134 completed (loss: 0.277648389339447, acc: 0.9404761791229248)
[2025-02-13 20:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23][root][INFO] - Training Epoch: 1/2, step 5952/7134 completed (loss: 0.6247614026069641, acc: 0.9210526347160339)
[2025-02-13 20:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23][root][INFO] - Training Epoch: 1/2, step 5953/7134 completed (loss: 0.20334182679653168, acc: 0.9528301954269409)
[2025-02-13 20:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24][root][INFO] - Training Epoch: 1/2, step 5954/7134 completed (loss: 0.04752545803785324, acc: 0.9900990128517151)
[2025-02-13 20:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24][root][INFO] - Training Epoch: 1/2, step 5955/7134 completed (loss: 0.2740793824195862, acc: 0.9629629850387573)
[2025-02-13 20:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24][root][INFO] - Training Epoch: 1/2, step 5956/7134 completed (loss: 0.27694541215896606, acc: 0.9416666626930237)
[2025-02-13 20:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25][root][INFO] - Training Epoch: 1/2, step 5957/7134 completed (loss: 0.1406119465827942, acc: 0.9756097793579102)
[2025-02-13 20:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25][root][INFO] - Training Epoch: 1/2, step 5958/7134 completed (loss: 0.17844633758068085, acc: 0.9599999785423279)
[2025-02-13 20:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26][root][INFO] - Training Epoch: 1/2, step 5959/7134 completed (loss: 0.1988268345594406, acc: 0.9468085169792175)
[2025-02-13 20:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26][root][INFO] - Training Epoch: 1/2, step 5960/7134 completed (loss: 0.034447044134140015, acc: 0.988095223903656)
[2025-02-13 20:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26][root][INFO] - Training Epoch: 1/2, step 5961/7134 completed (loss: 0.2307378202676773, acc: 0.9259259104728699)
[2025-02-13 20:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27][root][INFO] - Training Epoch: 1/2, step 5962/7134 completed (loss: 0.09739470481872559, acc: 0.9793103337287903)
[2025-02-13 20:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27][root][INFO] - Training Epoch: 1/2, step 5963/7134 completed (loss: 0.06861773133277893, acc: 0.9885714054107666)
[2025-02-13 20:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27][root][INFO] - Training Epoch: 1/2, step 5964/7134 completed (loss: 0.05216198042035103, acc: 0.9921259880065918)
[2025-02-13 20:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28][root][INFO] - Training Epoch: 1/2, step 5965/7134 completed (loss: 0.145377054810524, acc: 0.9615384340286255)
[2025-02-13 20:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28][root][INFO] - Training Epoch: 1/2, step 5966/7134 completed (loss: 0.1594649702310562, acc: 0.967391312122345)
[2025-02-13 20:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29][root][INFO] - Training Epoch: 1/2, step 5967/7134 completed (loss: 0.1006758064031601, acc: 0.9583333134651184)
[2025-02-13 20:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29][root][INFO] - Training Epoch: 1/2, step 5968/7134 completed (loss: 0.20654210448265076, acc: 0.9599999785423279)
[2025-02-13 20:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29][root][INFO] - Training Epoch: 1/2, step 5969/7134 completed (loss: 0.16594631969928741, acc: 0.948387086391449)
[2025-02-13 20:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:30][root][INFO] - Training Epoch: 1/2, step 5970/7134 completed (loss: 0.15771332383155823, acc: 0.984375)
[2025-02-13 20:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:30][root][INFO] - Training Epoch: 1/2, step 5971/7134 completed (loss: 0.0793304517865181, acc: 0.9631578922271729)
[2025-02-13 20:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31][root][INFO] - Training Epoch: 1/2, step 5972/7134 completed (loss: 0.2594528794288635, acc: 0.9263803958892822)
[2025-02-13 20:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31][root][INFO] - Training Epoch: 1/2, step 5973/7134 completed (loss: 0.31543225049972534, acc: 0.9428571462631226)
[2025-02-13 20:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31][root][INFO] - Training Epoch: 1/2, step 5974/7134 completed (loss: 0.08770452439785004, acc: 0.9926470518112183)
[2025-02-13 20:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32][root][INFO] - Training Epoch: 1/2, step 5975/7134 completed (loss: 0.05918837711215019, acc: 0.9817073345184326)
[2025-02-13 20:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32][root][INFO] - Training Epoch: 1/2, step 5976/7134 completed (loss: 0.17338939011096954, acc: 0.9673202633857727)
[2025-02-13 20:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33][root][INFO] - Training Epoch: 1/2, step 5977/7134 completed (loss: 0.12188360840082169, acc: 0.9750000238418579)
[2025-02-13 20:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33][root][INFO] - Training Epoch: 1/2, step 5978/7134 completed (loss: 0.1691017746925354, acc: 0.9813664555549622)
[2025-02-13 20:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33][root][INFO] - Training Epoch: 1/2, step 5979/7134 completed (loss: 0.21841783821582794, acc: 0.9473684430122375)
[2025-02-13 20:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34][root][INFO] - Training Epoch: 1/2, step 5980/7134 completed (loss: 0.12012174725532532, acc: 0.9619565010070801)
[2025-02-13 20:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34][root][INFO] - Training Epoch: 1/2, step 5981/7134 completed (loss: 0.09519579261541367, acc: 0.989847719669342)
[2025-02-13 20:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34][root][INFO] - Training Epoch: 1/2, step 5982/7134 completed (loss: 0.0958128422498703, acc: 0.975806474685669)
[2025-02-13 20:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35][root][INFO] - Training Epoch: 1/2, step 5983/7134 completed (loss: 0.12876777350902557, acc: 0.9712643623352051)
[2025-02-13 20:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35][root][INFO] - Training Epoch: 1/2, step 5984/7134 completed (loss: 0.13992831110954285, acc: 0.9696969985961914)
[2025-02-13 20:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36][root][INFO] - Training Epoch: 1/2, step 5985/7134 completed (loss: 0.11986272782087326, acc: 0.9719101190567017)
[2025-02-13 20:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36][root][INFO] - Training Epoch: 1/2, step 5986/7134 completed (loss: 0.20400074124336243, acc: 0.9523809552192688)
[2025-02-13 20:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36][root][INFO] - Training Epoch: 1/2, step 5987/7134 completed (loss: 0.14829352498054504, acc: 0.9658536314964294)
[2025-02-13 20:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37][root][INFO] - Training Epoch: 1/2, step 5988/7134 completed (loss: 0.08375503122806549, acc: 0.9833333492279053)
[2025-02-13 20:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37][root][INFO] - Training Epoch: 1/2, step 5989/7134 completed (loss: 0.12945939600467682, acc: 0.9769230484962463)
[2025-02-13 20:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37][root][INFO] - Training Epoch: 1/2, step 5990/7134 completed (loss: 0.14683246612548828, acc: 0.9613259434700012)
[2025-02-13 20:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38][root][INFO] - Training Epoch: 1/2, step 5991/7134 completed (loss: 0.2464710772037506, acc: 0.9431818127632141)
[2025-02-13 20:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38][root][INFO] - Training Epoch: 1/2, step 5992/7134 completed (loss: 0.08156727254390717, acc: 0.9846938848495483)
[2025-02-13 20:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38][root][INFO] - Training Epoch: 1/2, step 5993/7134 completed (loss: 0.10694196075201035, acc: 0.9775280952453613)
[2025-02-13 20:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39][root][INFO] - Training Epoch: 1/2, step 5994/7134 completed (loss: 0.07615364342927933, acc: 0.9752066135406494)
[2025-02-13 20:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39][root][INFO] - Training Epoch: 1/2, step 5995/7134 completed (loss: 0.08445325493812561, acc: 0.9710982441902161)
[2025-02-13 20:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39][root][INFO] - Training Epoch: 1/2, step 5996/7134 completed (loss: 0.16056916117668152, acc: 0.9599999785423279)
[2025-02-13 20:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40][root][INFO] - Training Epoch: 1/2, step 5997/7134 completed (loss: 0.1371123194694519, acc: 0.9715909361839294)
[2025-02-13 20:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40][root][INFO] - Training Epoch: 1/2, step 5998/7134 completed (loss: 0.09858492761850357, acc: 0.9682539701461792)
[2025-02-13 20:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41][root][INFO] - Training Epoch: 1/2, step 5999/7134 completed (loss: 0.08004472404718399, acc: 0.9806763529777527)
[2025-02-13 20:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41][root][INFO] - Training Epoch: 1/2, step 6000/7134 completed (loss: 0.069303497672081, acc: 0.9929577708244324)
[2025-02-13 20:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41][root][INFO] - Training Epoch: 1/2, step 6001/7134 completed (loss: 0.06192293390631676, acc: 0.987730085849762)
[2025-02-13 20:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42][root][INFO] - Training Epoch: 1/2, step 6002/7134 completed (loss: 0.17492873966693878, acc: 0.9504132270812988)
[2025-02-13 20:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42][root][INFO] - Training Epoch: 1/2, step 6003/7134 completed (loss: 0.1510852724313736, acc: 0.9545454382896423)
[2025-02-13 20:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42][root][INFO] - Training Epoch: 1/2, step 6004/7134 completed (loss: 0.4729452431201935, acc: 0.9017341136932373)
[2025-02-13 20:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43][root][INFO] - Training Epoch: 1/2, step 6005/7134 completed (loss: 0.23995471000671387, acc: 0.9470198750495911)
[2025-02-13 20:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43][root][INFO] - Training Epoch: 1/2, step 6006/7134 completed (loss: 0.2404845803976059, acc: 0.9200000166893005)
[2025-02-13 20:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44][root][INFO] - Training Epoch: 1/2, step 6007/7134 completed (loss: 0.16187328100204468, acc: 0.9558823704719543)
[2025-02-13 20:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44][root][INFO] - Training Epoch: 1/2, step 6008/7134 completed (loss: 0.17444607615470886, acc: 0.9440559148788452)
[2025-02-13 20:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44][root][INFO] - Training Epoch: 1/2, step 6009/7134 completed (loss: 0.18484754860401154, acc: 0.9548872113227844)
[2025-02-13 20:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45][root][INFO] - Training Epoch: 1/2, step 6010/7134 completed (loss: 0.27563151717185974, acc: 0.9240506291389465)
[2025-02-13 20:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45][root][INFO] - Training Epoch: 1/2, step 6011/7134 completed (loss: 0.29723745584487915, acc: 0.9285714030265808)
[2025-02-13 20:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45][root][INFO] - Training Epoch: 1/2, step 6012/7134 completed (loss: 0.14464497566223145, acc: 0.9454545378684998)
[2025-02-13 20:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46][root][INFO] - Training Epoch: 1/2, step 6013/7134 completed (loss: 0.2281351238489151, acc: 0.9382022619247437)
[2025-02-13 20:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46][root][INFO] - Training Epoch: 1/2, step 6014/7134 completed (loss: 0.27379366755485535, acc: 0.9370078444480896)
[2025-02-13 20:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46][root][INFO] - Training Epoch: 1/2, step 6015/7134 completed (loss: 0.30252277851104736, acc: 0.9099099040031433)
[2025-02-13 20:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47][root][INFO] - Training Epoch: 1/2, step 6016/7134 completed (loss: 0.3424403965473175, acc: 0.9189189076423645)
[2025-02-13 20:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47][root][INFO] - Training Epoch: 1/2, step 6017/7134 completed (loss: 0.2816299796104431, acc: 0.9230769276618958)
[2025-02-13 20:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48][root][INFO] - Training Epoch: 1/2, step 6018/7134 completed (loss: 0.20200663805007935, acc: 0.9615384340286255)
[2025-02-13 20:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48][root][INFO] - Training Epoch: 1/2, step 6019/7134 completed (loss: 0.26336967945098877, acc: 0.9166666865348816)
[2025-02-13 20:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48][root][INFO] - Training Epoch: 1/2, step 6020/7134 completed (loss: 0.047284286469221115, acc: 0.9841269850730896)
[2025-02-13 20:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49][root][INFO] - Training Epoch: 1/2, step 6021/7134 completed (loss: 0.17477349936962128, acc: 0.9512194991111755)
[2025-02-13 20:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49][root][INFO] - Training Epoch: 1/2, step 6022/7134 completed (loss: 0.16321709752082825, acc: 0.9605262875556946)
[2025-02-13 20:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49][root][INFO] - Training Epoch: 1/2, step 6023/7134 completed (loss: 0.0694563090801239, acc: 0.9659090638160706)
[2025-02-13 20:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50][root][INFO] - Training Epoch: 1/2, step 6024/7134 completed (loss: 0.01853403076529503, acc: 1.0)
[2025-02-13 20:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50][root][INFO] - Training Epoch: 1/2, step 6025/7134 completed (loss: 0.1509360671043396, acc: 0.9508196711540222)
[2025-02-13 20:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50][root][INFO] - Training Epoch: 1/2, step 6026/7134 completed (loss: 0.5343902111053467, acc: 0.8474576473236084)
[2025-02-13 20:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51][root][INFO] - Training Epoch: 1/2, step 6027/7134 completed (loss: 0.08762016147375107, acc: 0.9775280952453613)
[2025-02-13 20:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51][root][INFO] - Training Epoch: 1/2, step 6028/7134 completed (loss: 0.21539945900440216, acc: 0.949999988079071)
[2025-02-13 20:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52][root][INFO] - Training Epoch: 1/2, step 6029/7134 completed (loss: 0.13033559918403625, acc: 0.9577465057373047)
[2025-02-13 20:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52][root][INFO] - Training Epoch: 1/2, step 6030/7134 completed (loss: 0.13540418446063995, acc: 0.9696969985961914)
[2025-02-13 20:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53][root][INFO] - Training Epoch: 1/2, step 6031/7134 completed (loss: 0.14943164587020874, acc: 0.9651162624359131)
[2025-02-13 20:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53][root][INFO] - Training Epoch: 1/2, step 6032/7134 completed (loss: 0.09056088328361511, acc: 0.9801980257034302)
[2025-02-13 20:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53][root][INFO] - Training Epoch: 1/2, step 6033/7134 completed (loss: 0.21798738837242126, acc: 0.9545454382896423)
[2025-02-13 20:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:54][root][INFO] - Training Epoch: 1/2, step 6034/7134 completed (loss: 0.32234686613082886, acc: 0.89552241563797)
[2025-02-13 20:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:54][root][INFO] - Training Epoch: 1/2, step 6035/7134 completed (loss: 0.40775904059410095, acc: 0.9061033129692078)
[2025-02-13 20:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55][root][INFO] - Training Epoch: 1/2, step 6036/7134 completed (loss: 0.33774110674858093, acc: 0.9269663095474243)
[2025-02-13 20:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55][root][INFO] - Training Epoch: 1/2, step 6037/7134 completed (loss: 0.21612657606601715, acc: 0.936170220375061)
[2025-02-13 20:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55][root][INFO] - Training Epoch: 1/2, step 6038/7134 completed (loss: 0.1633269190788269, acc: 0.9607843160629272)
[2025-02-13 20:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56][root][INFO] - Training Epoch: 1/2, step 6039/7134 completed (loss: 0.23570914566516876, acc: 0.9512194991111755)
[2025-02-13 20:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56][root][INFO] - Training Epoch: 1/2, step 6040/7134 completed (loss: 0.12301359325647354, acc: 0.9823788404464722)
[2025-02-13 20:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57][root][INFO] - Training Epoch: 1/2, step 6041/7134 completed (loss: 0.2995060980319977, acc: 0.9193548560142517)
[2025-02-13 20:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57][root][INFO] - Training Epoch: 1/2, step 6042/7134 completed (loss: 0.10580302029848099, acc: 0.9770641922950745)
[2025-02-13 20:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57][root][INFO] - Training Epoch: 1/2, step 6043/7134 completed (loss: 0.15734265744686127, acc: 0.9462365508079529)
[2025-02-13 20:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:58][root][INFO] - Training Epoch: 1/2, step 6044/7134 completed (loss: 0.09516637027263641, acc: 0.9666666388511658)
[2025-02-13 20:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:58][root][INFO] - Training Epoch: 1/2, step 6045/7134 completed (loss: 0.18306653201580048, acc: 0.9425837397575378)
[2025-02-13 20:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59][root][INFO] - Training Epoch: 1/2, step 6046/7134 completed (loss: 0.2505585849285126, acc: 0.931506872177124)
[2025-02-13 20:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59][root][INFO] - Training Epoch: 1/2, step 6047/7134 completed (loss: 0.3822845220565796, acc: 0.918367326259613)
[2025-02-13 20:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59][root][INFO] - Training Epoch: 1/2, step 6048/7134 completed (loss: 0.2697239816188812, acc: 0.9186992049217224)
[2025-02-13 20:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00][root][INFO] - Training Epoch: 1/2, step 6049/7134 completed (loss: 0.1412087231874466, acc: 0.9580419659614563)
[2025-02-13 20:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00][root][INFO] - Training Epoch: 1/2, step 6050/7134 completed (loss: 0.32687780261039734, acc: 0.9017341136932373)
[2025-02-13 20:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01][root][INFO] - Training Epoch: 1/2, step 6051/7134 completed (loss: 0.4377618730068207, acc: 0.8920454382896423)
[2025-02-13 20:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01][root][INFO] - Training Epoch: 1/2, step 6052/7134 completed (loss: 0.3739434480667114, acc: 0.8866666555404663)
[2025-02-13 20:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01][root][INFO] - Training Epoch: 1/2, step 6053/7134 completed (loss: 0.2755297124385834, acc: 0.9292035102844238)
[2025-02-13 20:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02][root][INFO] - Training Epoch: 1/2, step 6054/7134 completed (loss: 0.16817182302474976, acc: 0.9481865167617798)
[2025-02-13 20:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02][root][INFO] - Training Epoch: 1/2, step 6055/7134 completed (loss: 0.24511010944843292, acc: 0.9415584206581116)
[2025-02-13 20:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03][root][INFO] - Training Epoch: 1/2, step 6056/7134 completed (loss: 0.1749694049358368, acc: 0.9279279112815857)
[2025-02-13 20:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03][root][INFO] - Training Epoch: 1/2, step 6057/7134 completed (loss: 0.3032850921154022, acc: 0.9350649118423462)
[2025-02-13 20:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03][root][INFO] - Training Epoch: 1/2, step 6058/7134 completed (loss: 0.17149946093559265, acc: 0.9491525292396545)
[2025-02-13 20:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04][root][INFO] - Training Epoch: 1/2, step 6059/7134 completed (loss: 0.14166170358657837, acc: 0.9589040875434875)
[2025-02-13 20:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04][root][INFO] - Training Epoch: 1/2, step 6060/7134 completed (loss: 0.17366814613342285, acc: 0.9567901492118835)
[2025-02-13 20:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05][root][INFO] - Training Epoch: 1/2, step 6061/7134 completed (loss: 0.2822880148887634, acc: 0.922535240650177)
[2025-02-13 20:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05][root][INFO] - Training Epoch: 1/2, step 6062/7134 completed (loss: 0.30233460664749146, acc: 0.920634925365448)
[2025-02-13 20:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05][root][INFO] - Training Epoch: 1/2, step 6063/7134 completed (loss: 0.26149022579193115, acc: 0.9491525292396545)
[2025-02-13 20:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:06][root][INFO] - Training Epoch: 1/2, step 6064/7134 completed (loss: 0.2691181004047394, acc: 0.9378882050514221)
[2025-02-13 20:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:06][root][INFO] - Training Epoch: 1/2, step 6065/7134 completed (loss: 0.18774452805519104, acc: 0.9492385983467102)
[2025-02-13 20:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07][root][INFO] - Training Epoch: 1/2, step 6066/7134 completed (loss: 0.39138883352279663, acc: 0.8888888955116272)
[2025-02-13 20:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07][root][INFO] - Training Epoch: 1/2, step 6067/7134 completed (loss: 0.44795456528663635, acc: 0.8947368264198303)
[2025-02-13 20:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07][root][INFO] - Training Epoch: 1/2, step 6068/7134 completed (loss: 0.5164062976837158, acc: 0.8216215968132019)
[2025-02-13 20:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08][root][INFO] - Training Epoch: 1/2, step 6069/7134 completed (loss: 0.32476094365119934, acc: 0.9058823585510254)
[2025-02-13 20:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08][root][INFO] - Training Epoch: 1/2, step 6070/7134 completed (loss: 0.2343740016222, acc: 0.9257143139839172)
[2025-02-13 20:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08][root][INFO] - Training Epoch: 1/2, step 6071/7134 completed (loss: 0.38519471883773804, acc: 0.8994082808494568)
[2025-02-13 20:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09][root][INFO] - Training Epoch: 1/2, step 6072/7134 completed (loss: 0.4251069128513336, acc: 0.8583333492279053)
[2025-02-13 20:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09][root][INFO] - Training Epoch: 1/2, step 6073/7134 completed (loss: 0.2429903894662857, acc: 0.9299362897872925)
[2025-02-13 20:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10][root][INFO] - Training Epoch: 1/2, step 6074/7134 completed (loss: 0.38700151443481445, acc: 0.9354838728904724)
[2025-02-13 20:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10][root][INFO] - Training Epoch: 1/2, step 6075/7134 completed (loss: 0.2168910950422287, acc: 0.9368420839309692)
[2025-02-13 20:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10][root][INFO] - Training Epoch: 1/2, step 6076/7134 completed (loss: 0.3923982083797455, acc: 0.8877005577087402)
[2025-02-13 20:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11][root][INFO] - Training Epoch: 1/2, step 6077/7134 completed (loss: 0.20448237657546997, acc: 0.9467455744743347)
[2025-02-13 20:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11][root][INFO] - Training Epoch: 1/2, step 6078/7134 completed (loss: 0.4331166744232178, acc: 0.9152542352676392)
[2025-02-13 20:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12][root][INFO] - Training Epoch: 1/2, step 6079/7134 completed (loss: 0.18390265107154846, acc: 0.9545454382896423)
[2025-02-13 20:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12][root][INFO] - Training Epoch: 1/2, step 6080/7134 completed (loss: 0.15569370985031128, acc: 0.9482758641242981)
[2025-02-13 20:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12][root][INFO] - Training Epoch: 1/2, step 6081/7134 completed (loss: 0.2696501910686493, acc: 0.9459459185600281)
[2025-02-13 20:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:13][root][INFO] - Training Epoch: 1/2, step 6082/7134 completed (loss: 0.14506796002388, acc: 0.9597315192222595)
[2025-02-13 20:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:13][root][INFO] - Training Epoch: 1/2, step 6083/7134 completed (loss: 0.11474777013063431, acc: 0.9689119458198547)
[2025-02-13 20:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14][root][INFO] - Training Epoch: 1/2, step 6084/7134 completed (loss: 0.1364728808403015, acc: 0.9666666388511658)
[2025-02-13 20:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14][root][INFO] - Training Epoch: 1/2, step 6085/7134 completed (loss: 0.21834152936935425, acc: 0.9513888955116272)
[2025-02-13 20:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14][root][INFO] - Training Epoch: 1/2, step 6086/7134 completed (loss: 0.06578826904296875, acc: 0.9781420826911926)
[2025-02-13 20:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15][root][INFO] - Training Epoch: 1/2, step 6087/7134 completed (loss: 0.16981449723243713, acc: 0.9656862616539001)
[2025-02-13 20:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15][root][INFO] - Training Epoch: 1/2, step 6088/7134 completed (loss: 0.25362011790275574, acc: 0.929729700088501)
[2025-02-13 20:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15][root][INFO] - Training Epoch: 1/2, step 6089/7134 completed (loss: 0.13769973814487457, acc: 0.9485714435577393)
[2025-02-13 20:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:16][root][INFO] - Training Epoch: 1/2, step 6090/7134 completed (loss: 0.11684439331293106, acc: 0.9786096215248108)
[2025-02-13 20:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:16][root][INFO] - Training Epoch: 1/2, step 6091/7134 completed (loss: 0.09915915876626968, acc: 0.987261176109314)
[2025-02-13 20:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17][root][INFO] - Training Epoch: 1/2, step 6092/7134 completed (loss: 0.23171782493591309, acc: 0.9454545378684998)
[2025-02-13 20:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17][root][INFO] - Training Epoch: 1/2, step 6093/7134 completed (loss: 0.20455756783485413, acc: 0.9402984976768494)
[2025-02-13 20:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17][root][INFO] - Training Epoch: 1/2, step 6094/7134 completed (loss: 0.380403071641922, acc: 0.9034090638160706)
[2025-02-13 20:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18][root][INFO] - Training Epoch: 1/2, step 6095/7134 completed (loss: 0.16617651283740997, acc: 0.9570552110671997)
[2025-02-13 20:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18][root][INFO] - Training Epoch: 1/2, step 6096/7134 completed (loss: 0.31560254096984863, acc: 0.903553307056427)
[2025-02-13 20:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19][root][INFO] - Training Epoch: 1/2, step 6097/7134 completed (loss: 0.244008868932724, acc: 0.9503105878829956)
[2025-02-13 20:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19][root][INFO] - Training Epoch: 1/2, step 6098/7134 completed (loss: 0.20469965040683746, acc: 0.9356725215911865)
[2025-02-13 20:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19][root][INFO] - Training Epoch: 1/2, step 6099/7134 completed (loss: 0.185115247964859, acc: 0.9591836929321289)
[2025-02-13 20:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:20][root][INFO] - Training Epoch: 1/2, step 6100/7134 completed (loss: 0.24898558855056763, acc: 0.9615384340286255)
[2025-02-13 20:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:20][root][INFO] - Training Epoch: 1/2, step 6101/7134 completed (loss: 0.12392988801002502, acc: 0.9583333134651184)
[2025-02-13 20:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21][root][INFO] - Training Epoch: 1/2, step 6102/7134 completed (loss: 0.2627103328704834, acc: 0.9296875)
[2025-02-13 20:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21][root][INFO] - Training Epoch: 1/2, step 6103/7134 completed (loss: 0.13934597373008728, acc: 0.9727272987365723)
[2025-02-13 20:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21][root][INFO] - Training Epoch: 1/2, step 6104/7134 completed (loss: 0.4552594721317291, acc: 0.8783783912658691)
[2025-02-13 20:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22][root][INFO] - Training Epoch: 1/2, step 6105/7134 completed (loss: 0.613179624080658, acc: 0.8666666746139526)
[2025-02-13 20:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22][root][INFO] - Training Epoch: 1/2, step 6106/7134 completed (loss: 0.2382243275642395, acc: 0.9370078444480896)
[2025-02-13 20:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22][root][INFO] - Training Epoch: 1/2, step 6107/7134 completed (loss: 0.05242801085114479, acc: 0.9914529919624329)
[2025-02-13 20:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23][root][INFO] - Training Epoch: 1/2, step 6108/7134 completed (loss: 0.27406957745552063, acc: 0.93388432264328)
[2025-02-13 20:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23][root][INFO] - Training Epoch: 1/2, step 6109/7134 completed (loss: 0.14956168830394745, acc: 0.95652174949646)
[2025-02-13 20:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24][root][INFO] - Training Epoch: 1/2, step 6110/7134 completed (loss: 0.5517566204071045, acc: 0.875)
[2025-02-13 20:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24][root][INFO] - Training Epoch: 1/2, step 6111/7134 completed (loss: 0.3746437132358551, acc: 0.9239130616188049)
[2025-02-13 20:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24][root][INFO] - Training Epoch: 1/2, step 6112/7134 completed (loss: 0.33596792817115784, acc: 0.9281045794487)
[2025-02-13 20:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25][root][INFO] - Training Epoch: 1/2, step 6113/7134 completed (loss: 0.24852317571640015, acc: 0.957317054271698)
[2025-02-13 20:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25][root][INFO] - Training Epoch: 1/2, step 6114/7134 completed (loss: 0.1738167554140091, acc: 0.9650349617004395)
[2025-02-13 20:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25][root][INFO] - Training Epoch: 1/2, step 6115/7134 completed (loss: 0.1388862133026123, acc: 0.9679999947547913)
[2025-02-13 20:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26][root][INFO] - Training Epoch: 1/2, step 6116/7134 completed (loss: 0.39931046962738037, acc: 0.9354838728904724)
[2025-02-13 20:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26][root][INFO] - Training Epoch: 1/2, step 6117/7134 completed (loss: 0.2437938004732132, acc: 0.9341317415237427)
[2025-02-13 20:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27][root][INFO] - Training Epoch: 1/2, step 6118/7134 completed (loss: 0.09487316757440567, acc: 0.9682539701461792)
[2025-02-13 20:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27][root][INFO] - Training Epoch: 1/2, step 6119/7134 completed (loss: 0.12122157216072083, acc: 0.9621621370315552)
[2025-02-13 20:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27][root][INFO] - Training Epoch: 1/2, step 6120/7134 completed (loss: 0.08951529115438461, acc: 0.9772727489471436)
[2025-02-13 20:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28][root][INFO] - Training Epoch: 1/2, step 6121/7134 completed (loss: 0.202142596244812, acc: 0.9527559280395508)
[2025-02-13 20:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28][root][INFO] - Training Epoch: 1/2, step 6122/7134 completed (loss: 0.09502233564853668, acc: 0.9838709831237793)
[2025-02-13 20:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29][root][INFO] - Training Epoch: 1/2, step 6123/7134 completed (loss: 0.21363230049610138, acc: 0.9658119678497314)
[2025-02-13 20:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29][root][INFO] - Training Epoch: 1/2, step 6124/7134 completed (loss: 0.10902418941259384, acc: 0.9663865566253662)
[2025-02-13 20:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29][root][INFO] - Training Epoch: 1/2, step 6125/7134 completed (loss: 0.3710745573043823, acc: 0.9532710313796997)
[2025-02-13 20:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30][root][INFO] - Training Epoch: 1/2, step 6126/7134 completed (loss: 0.18898701667785645, acc: 0.9735099077224731)
[2025-02-13 20:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30][root][INFO] - Training Epoch: 1/2, step 6127/7134 completed (loss: 0.051318567246198654, acc: 1.0)
[2025-02-13 20:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30][root][INFO] - Training Epoch: 1/2, step 6128/7134 completed (loss: 0.18853236734867096, acc: 0.9495798349380493)
[2025-02-13 20:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31][root][INFO] - Training Epoch: 1/2, step 6129/7134 completed (loss: 0.23089782893657684, acc: 0.9467455744743347)
[2025-02-13 20:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31][root][INFO] - Training Epoch: 1/2, step 6130/7134 completed (loss: 0.38036635518074036, acc: 0.9179104566574097)
[2025-02-13 20:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31][root][INFO] - Training Epoch: 1/2, step 6131/7134 completed (loss: 0.3328631520271301, acc: 0.9223300814628601)
[2025-02-13 20:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:32][root][INFO] - Training Epoch: 1/2, step 6132/7134 completed (loss: 0.2041672170162201, acc: 0.9358974099159241)
[2025-02-13 20:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:32][root][INFO] - Training Epoch: 1/2, step 6133/7134 completed (loss: 0.18730305135250092, acc: 0.9663461446762085)
[2025-02-13 20:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33][root][INFO] - Training Epoch: 1/2, step 6134/7134 completed (loss: 0.33727219700813293, acc: 0.9075144529342651)
[2025-02-13 20:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33][root][INFO] - Training Epoch: 1/2, step 6135/7134 completed (loss: 0.12045493721961975, acc: 0.9767441749572754)
[2025-02-13 20:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33][root][INFO] - Training Epoch: 1/2, step 6136/7134 completed (loss: 0.1799454540014267, acc: 0.9580838084220886)
[2025-02-13 20:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34][root][INFO] - Training Epoch: 1/2, step 6137/7134 completed (loss: 0.19747784733772278, acc: 0.9740259647369385)
[2025-02-13 20:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34][root][INFO] - Training Epoch: 1/2, step 6138/7134 completed (loss: 0.2682894766330719, acc: 0.9509202241897583)
[2025-02-13 20:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34][root][INFO] - Training Epoch: 1/2, step 6139/7134 completed (loss: 0.296496719121933, acc: 0.9047619104385376)
[2025-02-13 20:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35][root][INFO] - Training Epoch: 1/2, step 6140/7134 completed (loss: 0.17890848219394684, acc: 0.9615384340286255)
[2025-02-13 20:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35][root][INFO] - Training Epoch: 1/2, step 6141/7134 completed (loss: 0.357915997505188, acc: 0.9130434989929199)
[2025-02-13 20:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36][root][INFO] - Training Epoch: 1/2, step 6142/7134 completed (loss: 0.1669652909040451, acc: 0.9599999785423279)
[2025-02-13 20:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36][root][INFO] - Training Epoch: 1/2, step 6143/7134 completed (loss: 0.18499290943145752, acc: 0.9696969985961914)
[2025-02-13 20:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36][root][INFO] - Training Epoch: 1/2, step 6144/7134 completed (loss: 0.1365503966808319, acc: 0.9589040875434875)
[2025-02-13 20:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37][root][INFO] - Training Epoch: 1/2, step 6145/7134 completed (loss: 0.2841542065143585, acc: 0.9398906826972961)
[2025-02-13 20:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37][root][INFO] - Training Epoch: 1/2, step 6146/7134 completed (loss: 0.28631392121315, acc: 0.9397590160369873)
[2025-02-13 20:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37][root][INFO] - Training Epoch: 1/2, step 6147/7134 completed (loss: 0.18659569323062897, acc: 0.9527559280395508)
[2025-02-13 20:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38][root][INFO] - Training Epoch: 1/2, step 6148/7134 completed (loss: 0.1632700264453888, acc: 0.9448275566101074)
[2025-02-13 20:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38][root][INFO] - Training Epoch: 1/2, step 6149/7134 completed (loss: 0.20032069087028503, acc: 0.9602649211883545)
[2025-02-13 20:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39][root][INFO] - Training Epoch: 1/2, step 6150/7134 completed (loss: 0.13258123397827148, acc: 0.9510489702224731)
[2025-02-13 20:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39][root][INFO] - Training Epoch: 1/2, step 6151/7134 completed (loss: 0.13015583157539368, acc: 0.9681528806686401)
[2025-02-13 20:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39][root][INFO] - Training Epoch: 1/2, step 6152/7134 completed (loss: 0.2140999436378479, acc: 0.977142870426178)
[2025-02-13 20:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40][root][INFO] - Training Epoch: 1/2, step 6153/7134 completed (loss: 0.0850580632686615, acc: 0.9876543283462524)
[2025-02-13 20:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40][root][INFO] - Training Epoch: 1/2, step 6154/7134 completed (loss: 0.17468388378620148, acc: 0.9613259434700012)
[2025-02-13 20:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40][root][INFO] - Training Epoch: 1/2, step 6155/7134 completed (loss: 0.19193249940872192, acc: 0.9644970297813416)
[2025-02-13 20:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41][root][INFO] - Training Epoch: 1/2, step 6156/7134 completed (loss: 0.1589285135269165, acc: 0.9556962251663208)
[2025-02-13 20:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41][root][INFO] - Training Epoch: 1/2, step 6157/7134 completed (loss: 0.1537143439054489, acc: 0.9683544039726257)
[2025-02-13 20:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41][root][INFO] - Training Epoch: 1/2, step 6158/7134 completed (loss: 0.2012028992176056, acc: 0.9473684430122375)
[2025-02-13 20:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:42][root][INFO] - Training Epoch: 1/2, step 6159/7134 completed (loss: 0.15616285800933838, acc: 0.9702380895614624)
[2025-02-13 20:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:42][root][INFO] - Training Epoch: 1/2, step 6160/7134 completed (loss: 0.318733274936676, acc: 0.9235668778419495)
[2025-02-13 20:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43][root][INFO] - Training Epoch: 1/2, step 6161/7134 completed (loss: 0.2796298563480377, acc: 0.9527559280395508)
[2025-02-13 20:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43][root][INFO] - Training Epoch: 1/2, step 6162/7134 completed (loss: 0.48644304275512695, acc: 0.9038461446762085)
[2025-02-13 20:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43][root][INFO] - Training Epoch: 1/2, step 6163/7134 completed (loss: 0.4364906847476959, acc: 0.9224806427955627)
[2025-02-13 20:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44][root][INFO] - Training Epoch: 1/2, step 6164/7134 completed (loss: 0.24972064793109894, acc: 0.9451219439506531)
[2025-02-13 20:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44][root][INFO] - Training Epoch: 1/2, step 6165/7134 completed (loss: 0.29610973596572876, acc: 0.9312977194786072)
[2025-02-13 20:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45][root][INFO] - Training Epoch: 1/2, step 6166/7134 completed (loss: 0.14674295485019684, acc: 0.970802903175354)
[2025-02-13 20:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45][root][INFO] - Training Epoch: 1/2, step 6167/7134 completed (loss: 0.06653118878602982, acc: 0.9709302186965942)
[2025-02-13 20:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45][root][INFO] - Training Epoch: 1/2, step 6168/7134 completed (loss: 0.10007624328136444, acc: 0.9838709831237793)
[2025-02-13 20:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46][root][INFO] - Training Epoch: 1/2, step 6169/7134 completed (loss: 0.14839617908000946, acc: 0.9879518151283264)
[2025-02-13 20:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46][root][INFO] - Training Epoch: 1/2, step 6170/7134 completed (loss: 0.13432389497756958, acc: 0.9496402740478516)
[2025-02-13 20:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46][root][INFO] - Training Epoch: 1/2, step 6171/7134 completed (loss: 0.04687366262078285, acc: 0.9945054650306702)
[2025-02-13 20:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47][root][INFO] - Training Epoch: 1/2, step 6172/7134 completed (loss: 0.09815017879009247, acc: 0.96875)
[2025-02-13 20:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47][root][INFO] - Training Epoch: 1/2, step 6173/7134 completed (loss: 0.11385553330183029, acc: 0.9551281929016113)
[2025-02-13 20:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48][root][INFO] - Training Epoch: 1/2, step 6174/7134 completed (loss: 0.06331341713666916, acc: 0.9818181991577148)
[2025-02-13 20:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48][root][INFO] - Training Epoch: 1/2, step 6175/7134 completed (loss: 0.13288578391075134, acc: 0.9629629850387573)
[2025-02-13 20:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48][root][INFO] - Training Epoch: 1/2, step 6176/7134 completed (loss: 0.10497380793094635, acc: 0.9878048896789551)
[2025-02-13 20:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49][root][INFO] - Training Epoch: 1/2, step 6177/7134 completed (loss: 0.06587774306535721, acc: 0.9890109896659851)
[2025-02-13 20:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49][root][INFO] - Training Epoch: 1/2, step 6178/7134 completed (loss: 0.13401249051094055, acc: 0.9738562107086182)
[2025-02-13 20:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49][root][INFO] - Training Epoch: 1/2, step 6179/7134 completed (loss: 0.26416948437690735, acc: 0.9333333373069763)
[2025-02-13 20:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:50][root][INFO] - Training Epoch: 1/2, step 6180/7134 completed (loss: 0.8801756501197815, acc: 0.8085106611251831)
[2025-02-13 20:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:50][root][INFO] - Training Epoch: 1/2, step 6181/7134 completed (loss: 1.0090773105621338, acc: 0.7777777910232544)
[2025-02-13 20:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51][root][INFO] - Training Epoch: 1/2, step 6182/7134 completed (loss: 0.4746568202972412, acc: 0.8837209343910217)
[2025-02-13 20:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51][root][INFO] - Training Epoch: 1/2, step 6183/7134 completed (loss: 0.6561386585235596, acc: 0.8395061492919922)
[2025-02-13 20:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51][root][INFO] - Training Epoch: 1/2, step 6184/7134 completed (loss: 0.7801652550697327, acc: 0.843478262424469)
[2025-02-13 20:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52][root][INFO] - Training Epoch: 1/2, step 6185/7134 completed (loss: 0.2635704576969147, acc: 0.9274193644523621)
[2025-02-13 20:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52][root][INFO] - Training Epoch: 1/2, step 6186/7134 completed (loss: 0.1807437390089035, acc: 0.9694656729698181)
[2025-02-13 20:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52][root][INFO] - Training Epoch: 1/2, step 6187/7134 completed (loss: 0.23301884531974792, acc: 0.9415204524993896)
[2025-02-13 20:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:53][root][INFO] - Training Epoch: 1/2, step 6188/7134 completed (loss: 0.26446956396102905, acc: 0.9411764740943909)
[2025-02-13 20:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:53][root][INFO] - Training Epoch: 1/2, step 6189/7134 completed (loss: 0.3960663974285126, acc: 0.9230769276618958)
[2025-02-13 20:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54][root][INFO] - Training Epoch: 1/2, step 6190/7134 completed (loss: 0.4816876947879791, acc: 0.9071428775787354)
[2025-02-13 20:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54][root][INFO] - Training Epoch: 1/2, step 6191/7134 completed (loss: 0.22131645679473877, acc: 0.934959352016449)
[2025-02-13 20:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54][root][INFO] - Training Epoch: 1/2, step 6192/7134 completed (loss: 0.2117232233285904, acc: 0.9463414549827576)
[2025-02-13 20:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55][root][INFO] - Training Epoch: 1/2, step 6193/7134 completed (loss: 0.23114927113056183, acc: 0.9292929172515869)
[2025-02-13 20:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55][root][INFO] - Training Epoch: 1/2, step 6194/7134 completed (loss: 0.23307885229587555, acc: 0.9262295365333557)
[2025-02-13 20:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56][root][INFO] - Training Epoch: 1/2, step 6195/7134 completed (loss: 0.22510161995887756, acc: 0.9150943160057068)
[2025-02-13 20:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56][root][INFO] - Training Epoch: 1/2, step 6196/7134 completed (loss: 0.11022945493459702, acc: 0.9567901492118835)
[2025-02-13 20:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56][root][INFO] - Training Epoch: 1/2, step 6197/7134 completed (loss: 0.26183098554611206, acc: 0.9488636255264282)
[2025-02-13 20:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57][root][INFO] - Training Epoch: 1/2, step 6198/7134 completed (loss: 0.11024429649114609, acc: 0.9734513163566589)
[2025-02-13 20:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57][root][INFO] - Training Epoch: 1/2, step 6199/7134 completed (loss: 0.20730805397033691, acc: 0.9682539701461792)
[2025-02-13 20:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58][root][INFO] - Training Epoch: 1/2, step 6200/7134 completed (loss: 0.0646514967083931, acc: 0.9897435903549194)
[2025-02-13 20:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58][root][INFO] - Training Epoch: 1/2, step 6201/7134 completed (loss: 0.09149499982595444, acc: 0.9760765433311462)
[2025-02-13 20:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58][root][INFO] - Training Epoch: 1/2, step 6202/7134 completed (loss: 0.19271452724933624, acc: 0.961904764175415)
[2025-02-13 20:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59][root][INFO] - Training Epoch: 1/2, step 6203/7134 completed (loss: 0.09199804812669754, acc: 0.9593023061752319)
[2025-02-13 20:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59][root][INFO] - Training Epoch: 1/2, step 6204/7134 completed (loss: 0.03542345017194748, acc: 1.0)
[2025-02-13 20:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59][root][INFO] - Training Epoch: 1/2, step 6205/7134 completed (loss: 0.08463849127292633, acc: 0.9928057789802551)
[2025-02-13 20:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00][root][INFO] - Training Epoch: 1/2, step 6206/7134 completed (loss: 0.14862050116062164, acc: 0.9627329111099243)
[2025-02-13 20:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00][root][INFO] - Training Epoch: 1/2, step 6207/7134 completed (loss: 0.4841483235359192, acc: 0.8909090757369995)
[2025-02-13 20:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01][root][INFO] - Training Epoch: 1/2, step 6208/7134 completed (loss: 0.08755552768707275, acc: 0.9635416865348816)
[2025-02-13 20:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01][root][INFO] - Training Epoch: 1/2, step 6209/7134 completed (loss: 0.12193859368562698, acc: 0.9702380895614624)
[2025-02-13 20:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02][root][INFO] - Training Epoch: 1/2, step 6210/7134 completed (loss: 0.1783885508775711, acc: 0.9411764740943909)
[2025-02-13 20:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02][root][INFO] - Training Epoch: 1/2, step 6211/7134 completed (loss: 0.17798584699630737, acc: 0.9515151381492615)
[2025-02-13 20:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02][root][INFO] - Training Epoch: 1/2, step 6212/7134 completed (loss: 0.14163227379322052, acc: 0.9572649598121643)
[2025-02-13 20:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03][root][INFO] - Training Epoch: 1/2, step 6213/7134 completed (loss: 0.07093685120344162, acc: 0.9909090995788574)
[2025-02-13 20:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03][root][INFO] - Training Epoch: 1/2, step 6214/7134 completed (loss: 0.09238924086093903, acc: 0.9830508232116699)
[2025-02-13 20:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03][root][INFO] - Training Epoch: 1/2, step 6215/7134 completed (loss: 0.20832647383213043, acc: 0.9468598961830139)
[2025-02-13 20:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04][root][INFO] - Training Epoch: 1/2, step 6216/7134 completed (loss: 0.06028808280825615, acc: 1.0)
[2025-02-13 20:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04][root][INFO] - Training Epoch: 1/2, step 6217/7134 completed (loss: 0.1010090634226799, acc: 0.9746835231781006)
[2025-02-13 20:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05][root][INFO] - Training Epoch: 1/2, step 6218/7134 completed (loss: 0.11074372380971909, acc: 0.9741935729980469)
[2025-02-13 20:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05][root][INFO] - Training Epoch: 1/2, step 6219/7134 completed (loss: 0.16945059597492218, acc: 0.9477124214172363)
[2025-02-13 20:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05][root][INFO] - Training Epoch: 1/2, step 6220/7134 completed (loss: 0.21310187876224518, acc: 0.9655172228813171)
[2025-02-13 20:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06][root][INFO] - Training Epoch: 1/2, step 6221/7134 completed (loss: 0.03519885614514351, acc: 0.9925373196601868)
[2025-02-13 20:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06][root][INFO] - Training Epoch: 1/2, step 6222/7134 completed (loss: 0.3352525234222412, acc: 0.9036144614219666)
[2025-02-13 20:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07][root][INFO] - Training Epoch: 1/2, step 6223/7134 completed (loss: 0.3252774477005005, acc: 0.9270833134651184)
[2025-02-13 20:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07][root][INFO] - Training Epoch: 1/2, step 6224/7134 completed (loss: 0.08301857113838196, acc: 0.9729729890823364)
[2025-02-13 20:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07][root][INFO] - Training Epoch: 1/2, step 6225/7134 completed (loss: 0.1812080293893814, acc: 0.9553072452545166)
[2025-02-13 20:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08][root][INFO] - Training Epoch: 1/2, step 6226/7134 completed (loss: 0.3919679522514343, acc: 0.9021739363670349)
[2025-02-13 20:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08][root][INFO] - Training Epoch: 1/2, step 6227/7134 completed (loss: 0.1080569177865982, acc: 0.956250011920929)
[2025-02-13 20:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09][root][INFO] - Training Epoch: 1/2, step 6228/7134 completed (loss: 0.2026301473379135, acc: 0.936170220375061)
[2025-02-13 20:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09][root][INFO] - Training Epoch: 1/2, step 6229/7134 completed (loss: 0.33913150429725647, acc: 0.9073171019554138)
[2025-02-13 20:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09][root][INFO] - Training Epoch: 1/2, step 6230/7134 completed (loss: 0.24939526617527008, acc: 0.9308510422706604)
[2025-02-13 20:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:10][root][INFO] - Training Epoch: 1/2, step 6231/7134 completed (loss: 0.1018882542848587, acc: 0.9649122953414917)
[2025-02-13 20:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:10][root][INFO] - Training Epoch: 1/2, step 6232/7134 completed (loss: 0.18018190562725067, acc: 0.9450549483299255)
[2025-02-13 20:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11][root][INFO] - Training Epoch: 1/2, step 6233/7134 completed (loss: 0.12016282975673676, acc: 0.977011501789093)
[2025-02-13 20:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11][root][INFO] - Training Epoch: 1/2, step 6234/7134 completed (loss: 0.27881842851638794, acc: 0.9388889074325562)
[2025-02-13 20:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11][root][INFO] - Training Epoch: 1/2, step 6235/7134 completed (loss: 0.0645918995141983, acc: 0.9821428656578064)
[2025-02-13 20:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:12][root][INFO] - Training Epoch: 1/2, step 6236/7134 completed (loss: 0.1798442155122757, acc: 0.9497206807136536)
[2025-02-13 20:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:12][root][INFO] - Training Epoch: 1/2, step 6237/7134 completed (loss: 0.3073683977127075, acc: 0.9265536665916443)
[2025-02-13 20:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13][root][INFO] - Training Epoch: 1/2, step 6238/7134 completed (loss: 0.1155935600399971, acc: 0.9735099077224731)
[2025-02-13 20:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13][root][INFO] - Training Epoch: 1/2, step 6239/7134 completed (loss: 0.1406128704547882, acc: 0.9533678889274597)
[2025-02-13 20:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14][root][INFO] - Training Epoch: 1/2, step 6240/7134 completed (loss: 0.04756767302751541, acc: 0.9819276928901672)
[2025-02-13 20:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14][root][INFO] - Training Epoch: 1/2, step 6241/7134 completed (loss: 0.09482333809137344, acc: 0.9800000190734863)
[2025-02-13 20:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14][root][INFO] - Training Epoch: 1/2, step 6242/7134 completed (loss: 0.11592443287372589, acc: 0.9591836929321289)
[2025-02-13 20:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15][root][INFO] - Training Epoch: 1/2, step 6243/7134 completed (loss: 0.17330749332904816, acc: 0.9463087320327759)
[2025-02-13 20:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15][root][INFO] - Training Epoch: 1/2, step 6244/7134 completed (loss: 0.1363924741744995, acc: 0.9580838084220886)
[2025-02-13 20:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15][root][INFO] - Training Epoch: 1/2, step 6245/7134 completed (loss: 0.1953369677066803, acc: 0.9585798978805542)
[2025-02-13 20:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16][root][INFO] - Training Epoch: 1/2, step 6246/7134 completed (loss: 0.06296698749065399, acc: 0.9786096215248108)
[2025-02-13 20:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16][root][INFO] - Training Epoch: 1/2, step 6247/7134 completed (loss: 0.1817173808813095, acc: 0.95652174949646)
[2025-02-13 20:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17][root][INFO] - Training Epoch: 1/2, step 6248/7134 completed (loss: 0.3890683352947235, acc: 0.9221556782722473)
[2025-02-13 20:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17][root][INFO] - Training Epoch: 1/2, step 6249/7134 completed (loss: 0.03141973912715912, acc: 0.9933775067329407)
[2025-02-13 20:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17][root][INFO] - Training Epoch: 1/2, step 6250/7134 completed (loss: 0.02167261764407158, acc: 1.0)
[2025-02-13 20:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18][root][INFO] - Training Epoch: 1/2, step 6251/7134 completed (loss: 0.13990892469882965, acc: 0.9602649211883545)
[2025-02-13 20:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18][root][INFO] - Training Epoch: 1/2, step 6252/7134 completed (loss: 0.060499560087919235, acc: 0.9857142567634583)
[2025-02-13 20:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19][root][INFO] - Training Epoch: 1/2, step 6253/7134 completed (loss: 0.12650494277477264, acc: 0.9720279574394226)
[2025-02-13 20:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19][root][INFO] - Training Epoch: 1/2, step 6254/7134 completed (loss: 0.23537081480026245, acc: 0.9552238583564758)
[2025-02-13 20:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19][root][INFO] - Training Epoch: 1/2, step 6255/7134 completed (loss: 0.15844736993312836, acc: 0.9715909361839294)
[2025-02-13 20:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20][root][INFO] - Training Epoch: 1/2, step 6256/7134 completed (loss: 0.14693352580070496, acc: 0.9717513918876648)
[2025-02-13 20:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20][root][INFO] - Training Epoch: 1/2, step 6257/7134 completed (loss: 0.12512660026550293, acc: 0.976331353187561)
[2025-02-13 20:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21][root][INFO] - Training Epoch: 1/2, step 6258/7134 completed (loss: 0.09533185511827469, acc: 0.9666666388511658)
[2025-02-13 20:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21][root][INFO] - Training Epoch: 1/2, step 6259/7134 completed (loss: 0.04613586887717247, acc: 0.9937888383865356)
[2025-02-13 20:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21][root][INFO] - Training Epoch: 1/2, step 6260/7134 completed (loss: 0.10554248094558716, acc: 0.9745222926139832)
[2025-02-13 20:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22][root][INFO] - Training Epoch: 1/2, step 6261/7134 completed (loss: 0.12659607827663422, acc: 0.9627329111099243)
[2025-02-13 20:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22][root][INFO] - Training Epoch: 1/2, step 6262/7134 completed (loss: 0.0363435298204422, acc: 0.9876543283462524)
[2025-02-13 20:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22][root][INFO] - Training Epoch: 1/2, step 6263/7134 completed (loss: 0.04447174072265625, acc: 0.9871794581413269)
[2025-02-13 20:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23][root][INFO] - Training Epoch: 1/2, step 6264/7134 completed (loss: 0.029141690582036972, acc: 0.9931507110595703)
[2025-02-13 20:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23][root][INFO] - Training Epoch: 1/2, step 6265/7134 completed (loss: 0.03612560033798218, acc: 0.9938650131225586)
[2025-02-13 20:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24][root][INFO] - Training Epoch: 1/2, step 6266/7134 completed (loss: 0.03184933960437775, acc: 0.9923076629638672)
[2025-02-13 20:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24][root][INFO] - Training Epoch: 1/2, step 6267/7134 completed (loss: 0.059503354132175446, acc: 0.9802631735801697)
[2025-02-13 20:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24][root][INFO] - Training Epoch: 1/2, step 6268/7134 completed (loss: 0.0644739419221878, acc: 0.9875776171684265)
[2025-02-13 20:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:25][root][INFO] - Training Epoch: 1/2, step 6269/7134 completed (loss: 0.023222725838422775, acc: 0.9910714030265808)
[2025-02-13 20:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:25][root][INFO] - Training Epoch: 1/2, step 6270/7134 completed (loss: 0.033589091151952744, acc: 0.9868420958518982)
[2025-02-13 20:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26][root][INFO] - Training Epoch: 1/2, step 6271/7134 completed (loss: 0.07313168793916702, acc: 0.9886363744735718)
[2025-02-13 20:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26][root][INFO] - Training Epoch: 1/2, step 6272/7134 completed (loss: 0.012630018405616283, acc: 0.9938650131225586)
[2025-02-13 20:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26][root][INFO] - Training Epoch: 1/2, step 6273/7134 completed (loss: 0.03086109831929207, acc: 0.9881656765937805)
[2025-02-13 20:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27][root][INFO] - Training Epoch: 1/2, step 6274/7134 completed (loss: 0.03426153212785721, acc: 0.9940828680992126)
[2025-02-13 20:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27][root][INFO] - Training Epoch: 1/2, step 6275/7134 completed (loss: 0.08267868310213089, acc: 0.9865771532058716)
[2025-02-13 20:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27][root][INFO] - Training Epoch: 1/2, step 6276/7134 completed (loss: 0.12726721167564392, acc: 0.9593495726585388)
[2025-02-13 20:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:28][root][INFO] - Training Epoch: 1/2, step 6277/7134 completed (loss: 0.20557689666748047, acc: 0.9245283007621765)
[2025-02-13 20:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:28][root][INFO] - Training Epoch: 1/2, step 6278/7134 completed (loss: 0.04676234722137451, acc: 0.9926470518112183)
[2025-02-13 20:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29][root][INFO] - Training Epoch: 1/2, step 6279/7134 completed (loss: 0.09510084241628647, acc: 0.9923076629638672)
[2025-02-13 20:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29][root][INFO] - Training Epoch: 1/2, step 6280/7134 completed (loss: 0.0885872021317482, acc: 0.9776119589805603)
[2025-02-13 20:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29][root][INFO] - Training Epoch: 1/2, step 6281/7134 completed (loss: 0.04664716497063637, acc: 0.9926470518112183)
[2025-02-13 20:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30][root][INFO] - Training Epoch: 1/2, step 6282/7134 completed (loss: 0.13461613655090332, acc: 0.9652777910232544)
[2025-02-13 20:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30][root][INFO] - Training Epoch: 1/2, step 6283/7134 completed (loss: 0.10268806666135788, acc: 0.9739130139350891)
[2025-02-13 20:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30][root][INFO] - Training Epoch: 1/2, step 6284/7134 completed (loss: 0.23802612721920013, acc: 0.9454545378684998)
[2025-02-13 20:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31][root][INFO] - Training Epoch: 1/2, step 6285/7134 completed (loss: 0.29212886095046997, acc: 0.9052631855010986)
[2025-02-13 20:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31][root][INFO] - Training Epoch: 1/2, step 6286/7134 completed (loss: 0.09021176397800446, acc: 0.9722222089767456)
[2025-02-13 20:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31][root][INFO] - Training Epoch: 1/2, step 6287/7134 completed (loss: 0.1508013904094696, acc: 0.9545454382896423)
[2025-02-13 20:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32][root][INFO] - Training Epoch: 1/2, step 6288/7134 completed (loss: 0.03880701959133148, acc: 0.9929078221321106)
[2025-02-13 20:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32][root][INFO] - Training Epoch: 1/2, step 6289/7134 completed (loss: 0.10812993347644806, acc: 0.9621211886405945)
[2025-02-13 20:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33][root][INFO] - Training Epoch: 1/2, step 6290/7134 completed (loss: 0.12650537490844727, acc: 0.9701492786407471)
[2025-02-13 20:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33][root][INFO] - Training Epoch: 1/2, step 6291/7134 completed (loss: 0.04044385626912117, acc: 0.9848484992980957)
[2025-02-13 20:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33][root][INFO] - Training Epoch: 1/2, step 6292/7134 completed (loss: 0.17902064323425293, acc: 0.9375)
[2025-02-13 20:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34][root][INFO] - Training Epoch: 1/2, step 6293/7134 completed (loss: 0.22311775386333466, acc: 0.939393937587738)
[2025-02-13 20:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34][root][INFO] - Training Epoch: 1/2, step 6294/7134 completed (loss: 0.2626853287220001, acc: 0.9407894611358643)
[2025-02-13 20:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34][root][INFO] - Training Epoch: 1/2, step 6295/7134 completed (loss: 0.06723085790872574, acc: 0.9848484992980957)
[2025-02-13 20:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35][root][INFO] - Training Epoch: 1/2, step 6296/7134 completed (loss: 0.24187837541103363, acc: 0.9424460530281067)
[2025-02-13 20:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35][root][INFO] - Training Epoch: 1/2, step 6297/7134 completed (loss: 0.13488854467868805, acc: 0.9774436354637146)
[2025-02-13 20:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35][root][INFO] - Training Epoch: 1/2, step 6298/7134 completed (loss: 0.22469796240329742, acc: 0.9375)
[2025-02-13 20:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:36][root][INFO] - Training Epoch: 1/2, step 6299/7134 completed (loss: 0.15234623849391937, acc: 0.9736841917037964)
[2025-02-13 20:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:36][root][INFO] - Training Epoch: 1/2, step 6300/7134 completed (loss: 0.178641676902771, acc: 0.9605262875556946)
[2025-02-13 20:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37][root][INFO] - Training Epoch: 1/2, step 6301/7134 completed (loss: 0.03431433066725731, acc: 0.9953703880310059)
[2025-02-13 20:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37][root][INFO] - Training Epoch: 1/2, step 6302/7134 completed (loss: 0.0750010684132576, acc: 0.9720670580863953)
[2025-02-13 20:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37][root][INFO] - Training Epoch: 1/2, step 6303/7134 completed (loss: 0.04182806238532066, acc: 0.9820627570152283)
[2025-02-13 20:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38][root][INFO] - Training Epoch: 1/2, step 6304/7134 completed (loss: 0.07852960377931595, acc: 0.9829545617103577)
[2025-02-13 20:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38][root][INFO] - Training Epoch: 1/2, step 6305/7134 completed (loss: 0.14483045041561127, acc: 0.9589040875434875)
[2025-02-13 20:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38][root][INFO] - Training Epoch: 1/2, step 6306/7134 completed (loss: 0.21809335052967072, acc: 0.9620853066444397)
[2025-02-13 20:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39][root][INFO] - Training Epoch: 1/2, step 6307/7134 completed (loss: 0.28556686639785767, acc: 0.9618320465087891)
[2025-02-13 20:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39][root][INFO] - Training Epoch: 1/2, step 6308/7134 completed (loss: 0.1471736580133438, acc: 0.9637305736541748)
[2025-02-13 20:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39][root][INFO] - Training Epoch: 1/2, step 6309/7134 completed (loss: 0.14376267790794373, acc: 0.9585798978805542)
[2025-02-13 20:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:40][root][INFO] - Training Epoch: 1/2, step 6310/7134 completed (loss: 0.24890506267547607, acc: 0.9385964870452881)
[2025-02-13 20:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:40][root][INFO] - Training Epoch: 1/2, step 6311/7134 completed (loss: 0.1436062604188919, acc: 0.9479166865348816)
[2025-02-13 20:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41][root][INFO] - Training Epoch: 1/2, step 6312/7134 completed (loss: 0.07285244017839432, acc: 0.993630588054657)
[2025-02-13 20:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41][root][INFO] - Training Epoch: 1/2, step 6313/7134 completed (loss: 0.038494523614645004, acc: 0.9891892075538635)
[2025-02-13 20:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41][root][INFO] - Training Epoch: 1/2, step 6314/7134 completed (loss: 0.08913757652044296, acc: 0.9677419066429138)
[2025-02-13 20:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42][root][INFO] - Training Epoch: 1/2, step 6315/7134 completed (loss: 0.2118149697780609, acc: 0.9798657894134521)
[2025-02-13 20:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42][root][INFO] - Training Epoch: 1/2, step 6316/7134 completed (loss: 0.052870046347379684, acc: 0.9862068891525269)
[2025-02-13 20:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42][root][INFO] - Training Epoch: 1/2, step 6317/7134 completed (loss: 0.054319433867931366, acc: 0.9797297120094299)
[2025-02-13 20:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43][root][INFO] - Training Epoch: 1/2, step 6318/7134 completed (loss: 0.05052504688501358, acc: 0.9736841917037964)
[2025-02-13 20:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43][root][INFO] - Training Epoch: 1/2, step 6319/7134 completed (loss: 0.06788279861211777, acc: 0.9733333587646484)
[2025-02-13 20:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43][root][INFO] - Training Epoch: 1/2, step 6320/7134 completed (loss: 0.08196204155683517, acc: 0.9797297120094299)
[2025-02-13 20:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44][root][INFO] - Training Epoch: 1/2, step 6321/7134 completed (loss: 0.04018436744809151, acc: 0.9904761910438538)
[2025-02-13 20:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44][root][INFO] - Training Epoch: 1/2, step 6322/7134 completed (loss: 0.13269928097724915, acc: 0.9554139971733093)
[2025-02-13 20:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:45][root][INFO] - Training Epoch: 1/2, step 6323/7134 completed (loss: 0.13412721455097198, acc: 0.949999988079071)
[2025-02-13 20:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:45][root][INFO] - Training Epoch: 1/2, step 6324/7134 completed (loss: 0.20887453854084015, acc: 0.9562841653823853)
[2025-02-13 20:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46][root][INFO] - Training Epoch: 1/2, step 6325/7134 completed (loss: 0.15128998458385468, acc: 0.9596773982048035)
[2025-02-13 20:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46][root][INFO] - Training Epoch: 1/2, step 6326/7134 completed (loss: 0.21387933194637299, acc: 0.9308176040649414)
[2025-02-13 20:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46][root][INFO] - Training Epoch: 1/2, step 6327/7134 completed (loss: 0.025072619318962097, acc: 1.0)
[2025-02-13 20:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47][root][INFO] - Training Epoch: 1/2, step 6328/7134 completed (loss: 0.24115034937858582, acc: 0.9254658222198486)
[2025-02-13 20:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47][root][INFO] - Training Epoch: 1/2, step 6329/7134 completed (loss: 0.12066090106964111, acc: 0.9615384340286255)
[2025-02-13 20:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47][root][INFO] - Training Epoch: 1/2, step 6330/7134 completed (loss: 0.07883434742689133, acc: 0.9811320900917053)
[2025-02-13 20:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48][root][INFO] - Training Epoch: 1/2, step 6331/7134 completed (loss: 0.03894360363483429, acc: 0.9925373196601868)
[2025-02-13 20:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48][root][INFO] - Training Epoch: 1/2, step 6332/7134 completed (loss: 0.21996404230594635, acc: 0.9503546357154846)
[2025-02-13 20:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49][root][INFO] - Training Epoch: 1/2, step 6333/7134 completed (loss: 0.13563072681427002, acc: 0.9655172228813171)
[2025-02-13 20:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49][root][INFO] - Training Epoch: 1/2, step 6334/7134 completed (loss: 0.06920206546783447, acc: 0.9779411554336548)
[2025-02-13 20:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49][root][INFO] - Training Epoch: 1/2, step 6335/7134 completed (loss: 0.03498537093400955, acc: 0.993630588054657)
[2025-02-13 20:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50][root][INFO] - Training Epoch: 1/2, step 6336/7134 completed (loss: 0.14944149553775787, acc: 0.9736841917037964)
[2025-02-13 20:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50][root][INFO] - Training Epoch: 1/2, step 6337/7134 completed (loss: 0.0370379276573658, acc: 0.9930070042610168)
[2025-02-13 20:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50][root][INFO] - Training Epoch: 1/2, step 6338/7134 completed (loss: 0.12964387238025665, acc: 0.9685534834861755)
[2025-02-13 20:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51][root][INFO] - Training Epoch: 1/2, step 6339/7134 completed (loss: 0.040962591767311096, acc: 0.9869281053543091)
[2025-02-13 20:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51][root][INFO] - Training Epoch: 1/2, step 6340/7134 completed (loss: 0.055828072130680084, acc: 0.9754601120948792)
[2025-02-13 20:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51][root][INFO] - Training Epoch: 1/2, step 6341/7134 completed (loss: 0.10227545350790024, acc: 0.960629940032959)
[2025-02-13 20:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:52][root][INFO] - Training Epoch: 1/2, step 6342/7134 completed (loss: 0.12208051234483719, acc: 0.9710982441902161)
[2025-02-13 20:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:52][root][INFO] - Training Epoch: 1/2, step 6343/7134 completed (loss: 0.13244076073169708, acc: 0.9726027250289917)
[2025-02-13 20:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53][root][INFO] - Training Epoch: 1/2, step 6344/7134 completed (loss: 0.16396230459213257, acc: 0.950276255607605)
[2025-02-13 20:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53][root][INFO] - Training Epoch: 1/2, step 6345/7134 completed (loss: 0.10193481296300888, acc: 0.9836956262588501)
[2025-02-13 20:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53][root][INFO] - Training Epoch: 1/2, step 6346/7134 completed (loss: 0.2468319982290268, acc: 0.9418604373931885)
[2025-02-13 20:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54][root][INFO] - Training Epoch: 1/2, step 6347/7134 completed (loss: 0.19171154499053955, acc: 0.9663865566253662)
[2025-02-13 20:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54][root][INFO] - Training Epoch: 1/2, step 6348/7134 completed (loss: 0.11558420211076736, acc: 0.9652777910232544)
[2025-02-13 20:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54][root][INFO] - Training Epoch: 1/2, step 6349/7134 completed (loss: 0.1450827419757843, acc: 0.9621211886405945)
[2025-02-13 20:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55][root][INFO] - Training Epoch: 1/2, step 6350/7134 completed (loss: 0.2340613454580307, acc: 0.9365079402923584)
[2025-02-13 20:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55][root][INFO] - Training Epoch: 1/2, step 6351/7134 completed (loss: 0.26659563183784485, acc: 0.9341317415237427)
[2025-02-13 20:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56][root][INFO] - Training Epoch: 1/2, step 6352/7134 completed (loss: 0.3067793548107147, acc: 0.9017341136932373)
[2025-02-13 20:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56][root][INFO] - Training Epoch: 1/2, step 6353/7134 completed (loss: 0.15536943078041077, acc: 0.9386503100395203)
[2025-02-13 20:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56][root][INFO] - Training Epoch: 1/2, step 6354/7134 completed (loss: 0.09035276621580124, acc: 0.977011501789093)
[2025-02-13 20:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57][root][INFO] - Training Epoch: 1/2, step 6355/7134 completed (loss: 0.05749480053782463, acc: 0.9800000190734863)
[2025-02-13 20:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57][root][INFO] - Training Epoch: 1/2, step 6356/7134 completed (loss: 0.09595917910337448, acc: 0.9745222926139832)
[2025-02-13 20:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57][root][INFO] - Training Epoch: 1/2, step 6357/7134 completed (loss: 0.10124868899583817, acc: 0.9757575988769531)
[2025-02-13 20:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58][root][INFO] - Training Epoch: 1/2, step 6358/7134 completed (loss: 0.1275709867477417, acc: 0.9649122953414917)
[2025-02-13 20:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58][root][INFO] - Training Epoch: 1/2, step 6359/7134 completed (loss: 0.17603906989097595, acc: 0.9608938694000244)
[2025-02-13 20:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58][root][INFO] - Training Epoch: 1/2, step 6360/7134 completed (loss: 0.13793468475341797, acc: 0.9727891087532043)
[2025-02-13 20:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:59][root][INFO] - Training Epoch: 1/2, step 6361/7134 completed (loss: 0.2135322540998459, acc: 0.935251772403717)
[2025-02-13 20:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:59][root][INFO] - Training Epoch: 1/2, step 6362/7134 completed (loss: 0.09344343841075897, acc: 0.9741935729980469)
[2025-02-13 20:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00][root][INFO] - Training Epoch: 1/2, step 6363/7134 completed (loss: 0.18643492460250854, acc: 0.9704142212867737)
[2025-02-13 20:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00][root][INFO] - Training Epoch: 1/2, step 6364/7134 completed (loss: 0.10297852754592896, acc: 0.9681528806686401)
[2025-02-13 20:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00][root][INFO] - Training Epoch: 1/2, step 6365/7134 completed (loss: 0.10885989665985107, acc: 0.9745762944221497)
[2025-02-13 20:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01][root][INFO] - Training Epoch: 1/2, step 6366/7134 completed (loss: 0.1737024486064911, acc: 0.9704142212867737)
[2025-02-13 20:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01][root][INFO] - Training Epoch: 1/2, step 6367/7134 completed (loss: 0.09896767884492874, acc: 0.9767441749572754)
[2025-02-13 20:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02][root][INFO] - Training Epoch: 1/2, step 6368/7134 completed (loss: 0.1426495462656021, acc: 0.9638554453849792)
[2025-02-13 20:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02][root][INFO] - Training Epoch: 1/2, step 6369/7134 completed (loss: 0.18573683500289917, acc: 0.9464285969734192)
[2025-02-13 20:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02][root][INFO] - Training Epoch: 1/2, step 6370/7134 completed (loss: 0.46649080514907837, acc: 0.9069767594337463)
[2025-02-13 20:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03][root][INFO] - Training Epoch: 1/2, step 6371/7134 completed (loss: 0.23877353966236115, acc: 0.9281045794487)
[2025-02-13 20:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03][root][INFO] - Training Epoch: 1/2, step 6372/7134 completed (loss: 0.30669254064559937, acc: 0.9224137663841248)
[2025-02-13 20:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03][root][INFO] - Training Epoch: 1/2, step 6373/7134 completed (loss: 0.30371856689453125, acc: 0.9274193644523621)
[2025-02-13 20:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04][root][INFO] - Training Epoch: 1/2, step 6374/7134 completed (loss: 0.10411915183067322, acc: 0.9831932783126831)
[2025-02-13 20:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04][root][INFO] - Training Epoch: 1/2, step 6375/7134 completed (loss: 0.19689764082431793, acc: 0.9447513818740845)
[2025-02-13 20:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05][root][INFO] - Training Epoch: 1/2, step 6376/7134 completed (loss: 0.12160512059926987, acc: 0.9512194991111755)
[2025-02-13 20:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05][root][INFO] - Training Epoch: 1/2, step 6377/7134 completed (loss: 0.1556200236082077, acc: 0.9440000057220459)
[2025-02-13 20:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05][root][INFO] - Training Epoch: 1/2, step 6378/7134 completed (loss: 0.31019097566604614, acc: 0.8930232524871826)
[2025-02-13 20:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06][root][INFO] - Training Epoch: 1/2, step 6379/7134 completed (loss: 0.2541496455669403, acc: 0.9210526347160339)
[2025-02-13 20:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06][root][INFO] - Training Epoch: 1/2, step 6380/7134 completed (loss: 0.23282046616077423, acc: 0.956250011920929)
[2025-02-13 20:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06][root][INFO] - Training Epoch: 1/2, step 6381/7134 completed (loss: 0.050477370619773865, acc: 0.9864864945411682)
[2025-02-13 20:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07][root][INFO] - Training Epoch: 1/2, step 6382/7134 completed (loss: 0.07598178833723068, acc: 0.9835164546966553)
[2025-02-13 20:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07][root][INFO] - Training Epoch: 1/2, step 6383/7134 completed (loss: 0.2825981378555298, acc: 0.9479768872261047)
[2025-02-13 20:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08][root][INFO] - Training Epoch: 1/2, step 6384/7134 completed (loss: 0.11746202409267426, acc: 0.9556962251663208)
[2025-02-13 20:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08][root][INFO] - Training Epoch: 1/2, step 6385/7134 completed (loss: 0.2526046335697174, acc: 0.9414634108543396)
[2025-02-13 20:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08][root][INFO] - Training Epoch: 1/2, step 6386/7134 completed (loss: 0.1423846185207367, acc: 0.9462365508079529)
[2025-02-13 20:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09][root][INFO] - Training Epoch: 1/2, step 6387/7134 completed (loss: 0.23891684412956238, acc: 0.9696969985961914)
[2025-02-13 20:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09][root][INFO] - Training Epoch: 1/2, step 6388/7134 completed (loss: 0.17415830492973328, acc: 0.9556962251663208)
[2025-02-13 20:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09][root][INFO] - Training Epoch: 1/2, step 6389/7134 completed (loss: 0.15816131234169006, acc: 0.9519230723381042)
[2025-02-13 20:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:10][root][INFO] - Training Epoch: 1/2, step 6390/7134 completed (loss: 0.1677127629518509, acc: 0.9434782862663269)
[2025-02-13 20:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:10][root][INFO] - Training Epoch: 1/2, step 6391/7134 completed (loss: 0.20557913184165955, acc: 0.9593023061752319)
[2025-02-13 20:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11][root][INFO] - Training Epoch: 1/2, step 6392/7134 completed (loss: 0.2674761414527893, acc: 0.9347826242446899)
[2025-02-13 20:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11][root][INFO] - Training Epoch: 1/2, step 6393/7134 completed (loss: 0.09957187622785568, acc: 0.9861111044883728)
[2025-02-13 20:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11][root][INFO] - Training Epoch: 1/2, step 6394/7134 completed (loss: 0.26581358909606934, acc: 0.9611111283302307)
[2025-02-13 20:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12][root][INFO] - Training Epoch: 1/2, step 6395/7134 completed (loss: 0.0987231656908989, acc: 0.9801980257034302)
[2025-02-13 20:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12][root][INFO] - Training Epoch: 1/2, step 6396/7134 completed (loss: 0.16017791628837585, acc: 0.9617224931716919)
[2025-02-13 20:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12][root][INFO] - Training Epoch: 1/2, step 6397/7134 completed (loss: 0.07380978763103485, acc: 0.9751243591308594)
[2025-02-13 20:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13][root][INFO] - Training Epoch: 1/2, step 6398/7134 completed (loss: 0.12392684817314148, acc: 0.9655172228813171)
[2025-02-13 20:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13][root][INFO] - Training Epoch: 1/2, step 6399/7134 completed (loss: 0.16464969515800476, acc: 0.9719101190567017)
[2025-02-13 20:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13][root][INFO] - Training Epoch: 1/2, step 6400/7134 completed (loss: 0.059399232268333435, acc: 0.9819276928901672)
[2025-02-13 20:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:14][root][INFO] - Training Epoch: 1/2, step 6401/7134 completed (loss: 0.08940310776233673, acc: 0.976190447807312)
[2025-02-13 20:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:14][root][INFO] - Training Epoch: 1/2, step 6402/7134 completed (loss: 0.18046189844608307, acc: 0.9696969985961914)
[2025-02-13 20:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15][root][INFO] - Training Epoch: 1/2, step 6403/7134 completed (loss: 0.04535533860325813, acc: 0.9896373152732849)
[2025-02-13 20:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15][root][INFO] - Training Epoch: 1/2, step 6404/7134 completed (loss: 0.06312768906354904, acc: 0.9822485446929932)
[2025-02-13 20:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15][root][INFO] - Training Epoch: 1/2, step 6405/7134 completed (loss: 0.08095476031303406, acc: 0.9855769276618958)
[2025-02-13 20:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16][root][INFO] - Training Epoch: 1/2, step 6406/7134 completed (loss: 0.21997974812984467, acc: 0.9449999928474426)
[2025-02-13 20:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16][root][INFO] - Training Epoch: 1/2, step 6407/7134 completed (loss: 0.07827789336442947, acc: 0.9722222089767456)
[2025-02-13 20:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16][root][INFO] - Training Epoch: 1/2, step 6408/7134 completed (loss: 0.2931567430496216, acc: 0.9166666865348816)
[2025-02-13 20:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17][root][INFO] - Training Epoch: 1/2, step 6409/7134 completed (loss: 0.1729796826839447, acc: 0.9756097793579102)
[2025-02-13 20:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17][root][INFO] - Training Epoch: 1/2, step 6410/7134 completed (loss: 0.25702518224716187, acc: 0.9166666865348816)
[2025-02-13 20:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18][root][INFO] - Training Epoch: 1/2, step 6411/7134 completed (loss: 0.23024390637874603, acc: 0.9457364082336426)
[2025-02-13 20:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18][root][INFO] - Training Epoch: 1/2, step 6412/7134 completed (loss: 0.05612222105264664, acc: 0.9806451797485352)
[2025-02-13 20:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18][root][INFO] - Training Epoch: 1/2, step 6413/7134 completed (loss: 0.23630401492118835, acc: 0.9597315192222595)
[2025-02-13 20:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19][root][INFO] - Training Epoch: 1/2, step 6414/7134 completed (loss: 0.08642511814832687, acc: 0.9736841917037964)
[2025-02-13 20:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19][root][INFO] - Training Epoch: 1/2, step 6415/7134 completed (loss: 0.10301756858825684, acc: 0.9715909361839294)
[2025-02-13 20:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19][root][INFO] - Training Epoch: 1/2, step 6416/7134 completed (loss: 0.16991040110588074, acc: 0.9710144996643066)
[2025-02-13 20:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:20][root][INFO] - Training Epoch: 1/2, step 6417/7134 completed (loss: 0.32558760046958923, acc: 0.9224806427955627)
[2025-02-13 20:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:20][root][INFO] - Training Epoch: 1/2, step 6418/7134 completed (loss: 0.6077844500541687, acc: 0.8456375598907471)
[2025-02-13 20:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21][root][INFO] - Training Epoch: 1/2, step 6419/7134 completed (loss: 0.3412172794342041, acc: 0.9285714030265808)
[2025-02-13 20:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21][root][INFO] - Training Epoch: 1/2, step 6420/7134 completed (loss: 0.28298458456993103, acc: 0.9240506291389465)
[2025-02-13 20:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21][root][INFO] - Training Epoch: 1/2, step 6421/7134 completed (loss: 0.2250019907951355, acc: 0.9457831382751465)
[2025-02-13 20:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22][root][INFO] - Training Epoch: 1/2, step 6422/7134 completed (loss: 0.19181402027606964, acc: 0.9608938694000244)
[2025-02-13 20:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22][root][INFO] - Training Epoch: 1/2, step 6423/7134 completed (loss: 0.2233327180147171, acc: 0.942307710647583)
[2025-02-13 20:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22][root][INFO] - Training Epoch: 1/2, step 6424/7134 completed (loss: 0.13034480810165405, acc: 0.9734042286872864)
[2025-02-13 20:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:23][root][INFO] - Training Epoch: 1/2, step 6425/7134 completed (loss: 0.12300684303045273, acc: 0.9553072452545166)
[2025-02-13 20:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:23][root][INFO] - Training Epoch: 1/2, step 6426/7134 completed (loss: 0.2256108969449997, acc: 0.9553072452545166)
[2025-02-13 20:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24][root][INFO] - Training Epoch: 1/2, step 6427/7134 completed (loss: 0.16796272993087769, acc: 0.9426751732826233)
[2025-02-13 20:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24][root][INFO] - Training Epoch: 1/2, step 6428/7134 completed (loss: 0.1057085394859314, acc: 0.9808917045593262)
[2025-02-13 20:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24][root][INFO] - Training Epoch: 1/2, step 6429/7134 completed (loss: 0.11275167018175125, acc: 0.9677419066429138)
[2025-02-13 20:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:25][root][INFO] - Training Epoch: 1/2, step 6430/7134 completed (loss: 0.3265465199947357, acc: 0.9312499761581421)
[2025-02-13 20:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:25][root][INFO] - Training Epoch: 1/2, step 6431/7134 completed (loss: 0.37754544615745544, acc: 0.8918918967247009)
[2025-02-13 20:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26][root][INFO] - Training Epoch: 1/2, step 6432/7134 completed (loss: 0.8529845476150513, acc: 0.8322580456733704)
[2025-02-13 20:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26][root][INFO] - Training Epoch: 1/2, step 6433/7134 completed (loss: 0.35520699620246887, acc: 0.9261363744735718)
[2025-02-13 20:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26][root][INFO] - Training Epoch: 1/2, step 6434/7134 completed (loss: 0.08413416892290115, acc: 0.9653179049491882)
[2025-02-13 20:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:27][root][INFO] - Training Epoch: 1/2, step 6435/7134 completed (loss: 0.12525776028633118, acc: 0.9637681245803833)
[2025-02-13 20:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:27][root][INFO] - Training Epoch: 1/2, step 6436/7134 completed (loss: 0.18638651072978973, acc: 0.9530201554298401)
[2025-02-13 20:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28][root][INFO] - Training Epoch: 1/2, step 6437/7134 completed (loss: 0.35362935066223145, acc: 0.9407894611358643)
[2025-02-13 20:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28][root][INFO] - Training Epoch: 1/2, step 6438/7134 completed (loss: 0.3059524893760681, acc: 0.9368932247161865)
[2025-02-13 20:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28][root][INFO] - Training Epoch: 1/2, step 6439/7134 completed (loss: 0.21542872488498688, acc: 0.9503105878829956)
[2025-02-13 20:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29][root][INFO] - Training Epoch: 1/2, step 6440/7134 completed (loss: 0.21278709173202515, acc: 0.9519230723381042)
[2025-02-13 20:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29][root][INFO] - Training Epoch: 1/2, step 6441/7134 completed (loss: 0.11233725398778915, acc: 0.9811320900917053)
[2025-02-13 20:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29][root][INFO] - Training Epoch: 1/2, step 6442/7134 completed (loss: 0.1627333015203476, acc: 0.9660193920135498)
[2025-02-13 20:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30][root][INFO] - Training Epoch: 1/2, step 6443/7134 completed (loss: 0.07581332325935364, acc: 0.9852941036224365)
[2025-02-13 20:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30][root][INFO] - Training Epoch: 1/2, step 6444/7134 completed (loss: 0.17969724535942078, acc: 0.9444444179534912)
[2025-02-13 20:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31][root][INFO] - Training Epoch: 1/2, step 6445/7134 completed (loss: 0.3001975119113922, acc: 0.9458128213882446)
[2025-02-13 20:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31][root][INFO] - Training Epoch: 1/2, step 6446/7134 completed (loss: 0.0822221040725708, acc: 0.9803921580314636)
[2025-02-13 20:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31][root][INFO] - Training Epoch: 1/2, step 6447/7134 completed (loss: 0.17205096781253815, acc: 0.9583333134651184)
[2025-02-13 20:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32][root][INFO] - Training Epoch: 1/2, step 6448/7134 completed (loss: 0.4535680413246155, acc: 0.9096774458885193)
[2025-02-13 20:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32][root][INFO] - Training Epoch: 1/2, step 6449/7134 completed (loss: 0.17725147306919098, acc: 0.9637305736541748)
[2025-02-13 20:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33][root][INFO] - Training Epoch: 1/2, step 6450/7134 completed (loss: 0.15807418525218964, acc: 0.9653179049491882)
[2025-02-13 20:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33][root][INFO] - Training Epoch: 1/2, step 6451/7134 completed (loss: 0.20815427601337433, acc: 0.9579831957817078)
[2025-02-13 20:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33][root][INFO] - Training Epoch: 1/2, step 6452/7134 completed (loss: 0.10173813253641129, acc: 0.9795918464660645)
[2025-02-13 20:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34][root][INFO] - Training Epoch: 1/2, step 6453/7134 completed (loss: 0.1477060616016388, acc: 0.9754601120948792)
[2025-02-13 20:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34][root][INFO] - Training Epoch: 1/2, step 6454/7134 completed (loss: 0.2669714391231537, acc: 0.9398906826972961)
[2025-02-13 20:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34][root][INFO] - Training Epoch: 1/2, step 6455/7134 completed (loss: 0.32805630564689636, acc: 0.9266666769981384)
[2025-02-13 20:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:35][root][INFO] - Training Epoch: 1/2, step 6456/7134 completed (loss: 0.5214253664016724, acc: 0.8922155499458313)
[2025-02-13 20:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:35][root][INFO] - Training Epoch: 1/2, step 6457/7134 completed (loss: 0.20286411046981812, acc: 0.9541284441947937)
[2025-02-13 20:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36][root][INFO] - Training Epoch: 1/2, step 6458/7134 completed (loss: 0.1834743618965149, acc: 0.9527027010917664)
[2025-02-13 20:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36][root][INFO] - Training Epoch: 1/2, step 6459/7134 completed (loss: 0.023286966606974602, acc: 1.0)
[2025-02-13 20:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36][root][INFO] - Training Epoch: 1/2, step 6460/7134 completed (loss: 0.07328029721975327, acc: 0.9795918464660645)
[2025-02-13 20:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37][root][INFO] - Training Epoch: 1/2, step 6461/7134 completed (loss: 0.06411528587341309, acc: 0.9710982441902161)
[2025-02-13 20:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37][root][INFO] - Training Epoch: 1/2, step 6462/7134 completed (loss: 0.11337078362703323, acc: 0.978723406791687)
[2025-02-13 20:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37][root][INFO] - Training Epoch: 1/2, step 6463/7134 completed (loss: 0.15198180079460144, acc: 0.9611650705337524)
[2025-02-13 20:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38][root][INFO] - Training Epoch: 1/2, step 6464/7134 completed (loss: 0.1362592577934265, acc: 0.9664804339408875)
[2025-02-13 20:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38][root][INFO] - Training Epoch: 1/2, step 6465/7134 completed (loss: 0.03980163484811783, acc: 0.9890710115432739)
[2025-02-13 20:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38][root][INFO] - Training Epoch: 1/2, step 6466/7134 completed (loss: 0.16492284834384918, acc: 0.9629629850387573)
[2025-02-13 20:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:39][root][INFO] - Training Epoch: 1/2, step 6467/7134 completed (loss: 0.24512693285942078, acc: 0.9277108311653137)
[2025-02-13 20:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:39][root][INFO] - Training Epoch: 1/2, step 6468/7134 completed (loss: 0.11963273584842682, acc: 0.9632353186607361)
[2025-02-13 20:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40][root][INFO] - Training Epoch: 1/2, step 6469/7134 completed (loss: 0.22768127918243408, acc: 0.9579439163208008)
[2025-02-13 20:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40][root][INFO] - Training Epoch: 1/2, step 6470/7134 completed (loss: 0.15264272689819336, acc: 0.9509803652763367)
[2025-02-13 20:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41][root][INFO] - Training Epoch: 1/2, step 6471/7134 completed (loss: 0.07841651886701584, acc: 0.9784172773361206)
[2025-02-13 20:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41][root][INFO] - Training Epoch: 1/2, step 6472/7134 completed (loss: 0.1568184345960617, acc: 0.9599999785423279)
[2025-02-13 20:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41][root][INFO] - Training Epoch: 1/2, step 6473/7134 completed (loss: 0.066752128303051, acc: 0.9777777791023254)
[2025-02-13 20:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42][root][INFO] - Training Epoch: 1/2, step 6474/7134 completed (loss: 0.079250268638134, acc: 0.9892473220825195)
[2025-02-13 20:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42][root][INFO] - Training Epoch: 1/2, step 6475/7134 completed (loss: 0.06958334892988205, acc: 0.9785714149475098)
[2025-02-13 20:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42][root][INFO] - Training Epoch: 1/2, step 6476/7134 completed (loss: 0.1150045245885849, acc: 0.9666666388511658)
[2025-02-13 20:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43][root][INFO] - Training Epoch: 1/2, step 6477/7134 completed (loss: 0.0794224664568901, acc: 0.9826086759567261)
[2025-02-13 20:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43][root][INFO] - Training Epoch: 1/2, step 6478/7134 completed (loss: 0.1194915920495987, acc: 0.9710144996643066)
[2025-02-13 20:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44][root][INFO] - Training Epoch: 1/2, step 6479/7134 completed (loss: 0.09185582399368286, acc: 0.9798657894134521)
[2025-02-13 20:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44][root][INFO] - Training Epoch: 1/2, step 6480/7134 completed (loss: 0.11068523675203323, acc: 0.9857142567634583)
[2025-02-13 20:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44][root][INFO] - Training Epoch: 1/2, step 6481/7134 completed (loss: 0.040918394923210144, acc: 0.9857142567634583)
[2025-02-13 20:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:45][root][INFO] - Training Epoch: 1/2, step 6482/7134 completed (loss: 0.030303971841931343, acc: 0.9900000095367432)
[2025-02-13 20:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:45][root][INFO] - Training Epoch: 1/2, step 6483/7134 completed (loss: 0.20678098499774933, acc: 0.9652777910232544)
[2025-02-13 20:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46][root][INFO] - Training Epoch: 1/2, step 6484/7134 completed (loss: 0.11187601089477539, acc: 0.9707602262496948)
[2025-02-13 20:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46][root][INFO] - Training Epoch: 1/2, step 6485/7134 completed (loss: 0.05846310034394264, acc: 0.9831932783126831)
[2025-02-13 20:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46][root][INFO] - Training Epoch: 1/2, step 6486/7134 completed (loss: 0.3771530091762543, acc: 0.9202898740768433)
[2025-02-13 20:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:47][root][INFO] - Training Epoch: 1/2, step 6487/7134 completed (loss: 0.7612811923027039, acc: 0.8529411554336548)
[2025-02-13 20:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:47][root][INFO] - Training Epoch: 1/2, step 6488/7134 completed (loss: 0.2754533886909485, acc: 0.9090909361839294)
[2025-02-13 20:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48][root][INFO] - Training Epoch: 1/2, step 6489/7134 completed (loss: 0.3219994902610779, acc: 0.9316239356994629)
[2025-02-13 20:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48][root][INFO] - Training Epoch: 1/2, step 6490/7134 completed (loss: 0.5714382529258728, acc: 0.8656716346740723)
[2025-02-13 20:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48][root][INFO] - Training Epoch: 1/2, step 6491/7134 completed (loss: 0.25326502323150635, acc: 0.948051929473877)
[2025-02-13 20:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49][root][INFO] - Training Epoch: 1/2, step 6492/7134 completed (loss: 0.17630743980407715, acc: 0.9677419066429138)
[2025-02-13 20:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49][root][INFO] - Training Epoch: 1/2, step 6493/7134 completed (loss: 0.2217264473438263, acc: 0.9523809552192688)
[2025-02-13 20:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49][root][INFO] - Training Epoch: 1/2, step 6494/7134 completed (loss: 0.34132370352745056, acc: 0.9380530714988708)
[2025-02-13 20:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:50][root][INFO] - Training Epoch: 1/2, step 6495/7134 completed (loss: 0.3364231586456299, acc: 0.9074074029922485)
[2025-02-13 20:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:50][root][INFO] - Training Epoch: 1/2, step 6496/7134 completed (loss: 0.2688113749027252, acc: 0.9248554706573486)
[2025-02-13 20:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51][root][INFO] - Training Epoch: 1/2, step 6497/7134 completed (loss: 0.31505441665649414, acc: 0.9305555820465088)
[2025-02-13 20:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51][root][INFO] - Training Epoch: 1/2, step 6498/7134 completed (loss: 0.24281425774097443, acc: 0.9352940917015076)
[2025-02-13 20:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51][root][INFO] - Training Epoch: 1/2, step 6499/7134 completed (loss: 0.3348434865474701, acc: 0.9095744490623474)
[2025-02-13 20:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:52][root][INFO] - Training Epoch: 1/2, step 6500/7134 completed (loss: 0.48296549916267395, acc: 0.9137930870056152)
[2025-02-13 20:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:52][root][INFO] - Training Epoch: 1/2, step 6501/7134 completed (loss: 0.6410004496574402, acc: 0.8556700944900513)
[2025-02-13 20:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53][root][INFO] - Training Epoch: 1/2, step 6502/7134 completed (loss: 0.17508822679519653, acc: 0.939393937587738)
[2025-02-13 20:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53][root][INFO] - Training Epoch: 1/2, step 6503/7134 completed (loss: 0.2874385416507721, acc: 0.9130434989929199)
[2025-02-13 20:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53][root][INFO] - Training Epoch: 1/2, step 6504/7134 completed (loss: 0.2172514945268631, acc: 0.9520547986030579)
[2025-02-13 20:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54][root][INFO] - Training Epoch: 1/2, step 6505/7134 completed (loss: 0.2963923513889313, acc: 0.9266666769981384)
[2025-02-13 20:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54][root][INFO] - Training Epoch: 1/2, step 6506/7134 completed (loss: 0.2818722426891327, acc: 0.938144326210022)
[2025-02-13 20:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55][root][INFO] - Training Epoch: 1/2, step 6507/7134 completed (loss: 0.3769679069519043, acc: 0.907975435256958)
[2025-02-13 20:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55][root][INFO] - Training Epoch: 1/2, step 6508/7134 completed (loss: 0.1316632479429245, acc: 0.9611111283302307)
[2025-02-13 20:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55][root][INFO] - Training Epoch: 1/2, step 6509/7134 completed (loss: 0.14829270541667938, acc: 0.9587628841400146)
[2025-02-13 20:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56][root][INFO] - Training Epoch: 1/2, step 6510/7134 completed (loss: 0.1942967176437378, acc: 0.9563106894493103)
[2025-02-13 20:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56][root][INFO] - Training Epoch: 1/2, step 6511/7134 completed (loss: 0.18765227496623993, acc: 0.9477611780166626)
[2025-02-13 20:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57][root][INFO] - Training Epoch: 1/2, step 6512/7134 completed (loss: 0.20734931528568268, acc: 0.9636363387107849)
[2025-02-13 20:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57][root][INFO] - Training Epoch: 1/2, step 6513/7134 completed (loss: 0.13040335476398468, acc: 0.9777777791023254)
[2025-02-13 20:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57][root][INFO] - Training Epoch: 1/2, step 6514/7134 completed (loss: 0.3262965977191925, acc: 0.9506173133850098)
[2025-02-13 20:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58][root][INFO] - Training Epoch: 1/2, step 6515/7134 completed (loss: 0.12897755205631256, acc: 0.9670329689979553)
[2025-02-13 20:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58][root][INFO] - Training Epoch: 1/2, step 6516/7134 completed (loss: 0.26868489384651184, acc: 0.9411764740943909)
[2025-02-13 20:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59][root][INFO] - Training Epoch: 1/2, step 6517/7134 completed (loss: 0.08391787111759186, acc: 0.9937888383865356)
[2025-02-13 20:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59][root][INFO] - Training Epoch: 1/2, step 6518/7134 completed (loss: 0.08536186814308167, acc: 0.976047933101654)
[2025-02-13 20:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59][root][INFO] - Training Epoch: 1/2, step 6519/7134 completed (loss: 0.11724766343832016, acc: 0.9763779640197754)
[2025-02-13 20:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00][root][INFO] - Training Epoch: 1/2, step 6520/7134 completed (loss: 0.1431063413619995, acc: 0.9675324559211731)
[2025-02-13 20:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00][root][INFO] - Training Epoch: 1/2, step 6521/7134 completed (loss: 0.14153940975666046, acc: 0.9675675630569458)
[2025-02-13 20:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01][root][INFO] - Training Epoch: 1/2, step 6522/7134 completed (loss: 0.21020101010799408, acc: 0.9411764740943909)
[2025-02-13 20:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01][root][INFO] - Training Epoch: 1/2, step 6523/7134 completed (loss: 0.47525250911712646, acc: 0.9027777910232544)
[2025-02-13 20:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01][root][INFO] - Training Epoch: 1/2, step 6524/7134 completed (loss: 0.21670226752758026, acc: 0.9613259434700012)
[2025-02-13 20:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02][root][INFO] - Training Epoch: 1/2, step 6525/7134 completed (loss: 0.09817097336053848, acc: 0.9752066135406494)
[2025-02-13 20:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02][root][INFO] - Training Epoch: 1/2, step 6526/7134 completed (loss: 0.21663081645965576, acc: 0.9593908786773682)
[2025-02-13 20:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03][root][INFO] - Training Epoch: 1/2, step 6527/7134 completed (loss: 0.19015812873840332, acc: 0.9620253443717957)
[2025-02-13 20:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03][root][INFO] - Training Epoch: 1/2, step 6528/7134 completed (loss: 0.2998424768447876, acc: 0.9275362491607666)
[2025-02-13 20:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03][root][INFO] - Training Epoch: 1/2, step 6529/7134 completed (loss: 0.4616408050060272, acc: 0.9236640930175781)
[2025-02-13 20:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04][root][INFO] - Training Epoch: 1/2, step 6530/7134 completed (loss: 0.1558360904455185, acc: 0.9668508172035217)
[2025-02-13 20:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04][root][INFO] - Training Epoch: 1/2, step 6531/7134 completed (loss: 0.16093191504478455, acc: 0.957317054271698)
[2025-02-13 20:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04][root][INFO] - Training Epoch: 1/2, step 6532/7134 completed (loss: 0.27802953124046326, acc: 0.9560439586639404)
[2025-02-13 20:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:05][root][INFO] - Training Epoch: 1/2, step 6533/7134 completed (loss: 0.04786432161927223, acc: 0.9805194735527039)
[2025-02-13 20:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:05][root][INFO] - Training Epoch: 1/2, step 6534/7134 completed (loss: 0.2836083769798279, acc: 0.9537572264671326)
[2025-02-13 20:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06][root][INFO] - Training Epoch: 1/2, step 6535/7134 completed (loss: 0.1761619746685028, acc: 0.9433962106704712)
[2025-02-13 20:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06][root][INFO] - Training Epoch: 1/2, step 6536/7134 completed (loss: 0.06396712362766266, acc: 0.9826086759567261)
[2025-02-13 20:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06][root][INFO] - Training Epoch: 1/2, step 6537/7134 completed (loss: 0.2547438442707062, acc: 0.9107142686843872)
[2025-02-13 20:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07][root][INFO] - Training Epoch: 1/2, step 6538/7134 completed (loss: 0.2892901599407196, acc: 0.9383561611175537)
[2025-02-13 20:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07][root][INFO] - Training Epoch: 1/2, step 6539/7134 completed (loss: 0.04772767424583435, acc: 0.9740259647369385)
[2025-02-13 20:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08][root][INFO] - Training Epoch: 1/2, step 6540/7134 completed (loss: 0.8837946057319641, acc: 0.8409090638160706)
[2025-02-13 20:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08][root][INFO] - Training Epoch: 1/2, step 6541/7134 completed (loss: 0.13111120462417603, acc: 0.9558011293411255)
[2025-02-13 20:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08][root][INFO] - Training Epoch: 1/2, step 6542/7134 completed (loss: 0.20740287005901337, acc: 0.9571428298950195)
[2025-02-13 20:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:09][root][INFO] - Training Epoch: 1/2, step 6543/7134 completed (loss: 0.10454785823822021, acc: 0.9736841917037964)
[2025-02-13 20:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:09][root][INFO] - Training Epoch: 1/2, step 6544/7134 completed (loss: 0.24827036261558533, acc: 0.9369369149208069)
[2025-02-13 20:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10][root][INFO] - Training Epoch: 1/2, step 6545/7134 completed (loss: 0.06147807091474533, acc: 0.9917355179786682)
[2025-02-13 20:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10][root][INFO] - Training Epoch: 1/2, step 6546/7134 completed (loss: 0.13909856975078583, acc: 0.97826087474823)
[2025-02-13 20:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10][root][INFO] - Training Epoch: 1/2, step 6547/7134 completed (loss: 0.2883313298225403, acc: 0.9492753744125366)
[2025-02-13 20:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11][root][INFO] - Training Epoch: 1/2, step 6548/7134 completed (loss: 0.10932092368602753, acc: 0.9714285731315613)
[2025-02-13 20:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11][root][INFO] - Training Epoch: 1/2, step 6549/7134 completed (loss: 0.448246031999588, acc: 0.9236640930175781)
[2025-02-13 20:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11][root][INFO] - Training Epoch: 1/2, step 6550/7134 completed (loss: 0.22593311965465546, acc: 0.9426751732826233)
[2025-02-13 20:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:12][root][INFO] - Training Epoch: 1/2, step 6551/7134 completed (loss: 0.16205795109272003, acc: 0.9644669890403748)
[2025-02-13 20:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:12][root][INFO] - Training Epoch: 1/2, step 6552/7134 completed (loss: 0.28191033005714417, acc: 0.933920681476593)
[2025-02-13 20:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13][root][INFO] - Training Epoch: 1/2, step 6553/7134 completed (loss: 0.19465981423854828, acc: 0.9494949579238892)
[2025-02-13 20:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13][root][INFO] - Training Epoch: 1/2, step 6554/7134 completed (loss: 0.16912424564361572, acc: 0.9672897458076477)
[2025-02-13 20:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13][root][INFO] - Training Epoch: 1/2, step 6555/7134 completed (loss: 0.2702941298484802, acc: 0.9353448152542114)
[2025-02-13 20:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14][root][INFO] - Training Epoch: 1/2, step 6556/7134 completed (loss: 0.2631676197052002, acc: 0.9638009071350098)
[2025-02-13 20:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14][root][INFO] - Training Epoch: 1/2, step 6557/7134 completed (loss: 0.2045729011297226, acc: 0.9504504799842834)
[2025-02-13 20:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15][root][INFO] - Training Epoch: 1/2, step 6558/7134 completed (loss: 0.19798226654529572, acc: 0.9672897458076477)
[2025-02-13 20:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15][root][INFO] - Training Epoch: 1/2, step 6559/7134 completed (loss: 0.2381199300289154, acc: 0.9638554453849792)
[2025-02-13 20:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15][root][INFO] - Training Epoch: 1/2, step 6560/7134 completed (loss: 0.10930280387401581, acc: 0.9790576100349426)
[2025-02-13 20:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16][root][INFO] - Training Epoch: 1/2, step 6561/7134 completed (loss: 0.12200578302145004, acc: 0.9581151604652405)
[2025-02-13 20:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16][root][INFO] - Training Epoch: 1/2, step 6562/7134 completed (loss: 0.1863543838262558, acc: 0.9371069073677063)
[2025-02-13 20:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17][root][INFO] - Training Epoch: 1/2, step 6563/7134 completed (loss: 0.17012879252433777, acc: 0.9444444179534912)
[2025-02-13 20:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17][root][INFO] - Training Epoch: 1/2, step 6564/7134 completed (loss: 0.15224725008010864, acc: 0.9698795080184937)
[2025-02-13 20:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17][root][INFO] - Training Epoch: 1/2, step 6565/7134 completed (loss: 0.3067956566810608, acc: 0.9279279112815857)
[2025-02-13 20:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18][root][INFO] - Training Epoch: 1/2, step 6566/7134 completed (loss: 0.14537861943244934, acc: 0.9596773982048035)
[2025-02-13 20:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18][root][INFO] - Training Epoch: 1/2, step 6567/7134 completed (loss: 0.17256461083889008, acc: 0.9466666579246521)
[2025-02-13 20:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19][root][INFO] - Training Epoch: 1/2, step 6568/7134 completed (loss: 0.14021804928779602, acc: 0.9725274443626404)
[2025-02-13 20:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19][root][INFO] - Training Epoch: 1/2, step 6569/7134 completed (loss: 0.11609704047441483, acc: 0.9543147087097168)
[2025-02-13 20:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19][root][INFO] - Training Epoch: 1/2, step 6570/7134 completed (loss: 0.15924718976020813, acc: 0.9507389068603516)
[2025-02-13 20:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20][root][INFO] - Training Epoch: 1/2, step 6571/7134 completed (loss: 0.08483950793743134, acc: 0.9818181991577148)
[2025-02-13 20:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20][root][INFO] - Training Epoch: 1/2, step 6572/7134 completed (loss: 0.13122007250785828, acc: 0.9756097793579102)
[2025-02-13 20:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20][root][INFO] - Training Epoch: 1/2, step 6573/7134 completed (loss: 0.12437152862548828, acc: 0.9611650705337524)
[2025-02-13 20:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:21][root][INFO] - Training Epoch: 1/2, step 6574/7134 completed (loss: 0.08194677531719208, acc: 0.982758641242981)
[2025-02-13 20:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:21][root][INFO] - Training Epoch: 1/2, step 6575/7134 completed (loss: 0.1543085128068924, acc: 0.9530516266822815)
[2025-02-13 20:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22][root][INFO] - Training Epoch: 1/2, step 6576/7134 completed (loss: 0.12440498918294907, acc: 0.9678899049758911)
[2025-02-13 20:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22][root][INFO] - Training Epoch: 1/2, step 6577/7134 completed (loss: 0.22660702466964722, acc: 0.9272727370262146)
[2025-02-13 20:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23][root][INFO] - Training Epoch: 1/2, step 6578/7134 completed (loss: 0.10508758574724197, acc: 0.9713114500045776)
[2025-02-13 20:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23][root][INFO] - Training Epoch: 1/2, step 6579/7134 completed (loss: 0.4412241280078888, acc: 0.9097744226455688)
[2025-02-13 20:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23][root][INFO] - Training Epoch: 1/2, step 6580/7134 completed (loss: 0.1681511551141739, acc: 0.9526315927505493)
[2025-02-13 20:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24][root][INFO] - Training Epoch: 1/2, step 6581/7134 completed (loss: 0.2373044192790985, acc: 0.9459459185600281)
[2025-02-13 20:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24][root][INFO] - Training Epoch: 1/2, step 6582/7134 completed (loss: 0.26108208298683167, acc: 0.9473684430122375)
[2025-02-13 20:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24][root][INFO] - Training Epoch: 1/2, step 6583/7134 completed (loss: 0.1310446709394455, acc: 0.9748743772506714)
[2025-02-13 20:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25][root][INFO] - Training Epoch: 1/2, step 6584/7134 completed (loss: 0.17710046470165253, acc: 0.9677419066429138)
[2025-02-13 20:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25][root][INFO] - Training Epoch: 1/2, step 6585/7134 completed (loss: 0.16798633337020874, acc: 0.9459459185600281)
[2025-02-13 20:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26][root][INFO] - Training Epoch: 1/2, step 6586/7134 completed (loss: 0.19303859770298004, acc: 0.9631901979446411)
[2025-02-13 20:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26][root][INFO] - Training Epoch: 1/2, step 6587/7134 completed (loss: 0.24563272297382355, acc: 0.969072163105011)
[2025-02-13 20:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26][root][INFO] - Training Epoch: 1/2, step 6588/7134 completed (loss: 0.16964000463485718, acc: 0.9593023061752319)
[2025-02-13 20:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:27][root][INFO] - Training Epoch: 1/2, step 6589/7134 completed (loss: 0.2173255980014801, acc: 0.9440993666648865)
[2025-02-13 20:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:27][root][INFO] - Training Epoch: 1/2, step 6590/7134 completed (loss: 0.20052240788936615, acc: 0.9521276354789734)
[2025-02-13 20:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28][root][INFO] - Training Epoch: 1/2, step 6591/7134 completed (loss: 0.15796397626399994, acc: 0.9590643048286438)
[2025-02-13 20:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28][root][INFO] - Training Epoch: 1/2, step 6592/7134 completed (loss: 0.19004298746585846, acc: 0.9458128213882446)
[2025-02-13 20:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28][root][INFO] - Training Epoch: 1/2, step 6593/7134 completed (loss: 0.15731216967105865, acc: 0.9666666388511658)
[2025-02-13 20:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:29][root][INFO] - Training Epoch: 1/2, step 6594/7134 completed (loss: 0.1679200381040573, acc: 0.9605911374092102)
[2025-02-13 20:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:29][root][INFO] - Training Epoch: 1/2, step 6595/7134 completed (loss: 0.13543541729450226, acc: 0.9691358208656311)
[2025-02-13 20:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30][root][INFO] - Training Epoch: 1/2, step 6596/7134 completed (loss: 0.16887259483337402, acc: 0.9578313231468201)
[2025-02-13 20:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30][root][INFO] - Training Epoch: 1/2, step 6597/7134 completed (loss: 0.1276543140411377, acc: 0.9476439952850342)
[2025-02-13 20:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30][root][INFO] - Training Epoch: 1/2, step 6598/7134 completed (loss: 0.1812397688627243, acc: 0.9617486596107483)
[2025-02-13 20:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:31][root][INFO] - Training Epoch: 1/2, step 6599/7134 completed (loss: 0.16836804151535034, acc: 0.9463414549827576)
[2025-02-13 20:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:31][root][INFO] - Training Epoch: 1/2, step 6600/7134 completed (loss: 0.32034197449684143, acc: 0.9101123809814453)
[2025-02-13 20:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32][root][INFO] - Training Epoch: 1/2, step 6601/7134 completed (loss: 0.2612632215023041, acc: 0.9313725233078003)
[2025-02-13 20:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32][root][INFO] - Training Epoch: 1/2, step 6602/7134 completed (loss: 0.19129334390163422, acc: 0.9504950642585754)
[2025-02-13 20:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32][root][INFO] - Training Epoch: 1/2, step 6603/7134 completed (loss: 0.14959369599819183, acc: 0.955974817276001)
[2025-02-13 20:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:33][root][INFO] - Training Epoch: 1/2, step 6604/7134 completed (loss: 0.328751802444458, acc: 0.9139785170555115)
[2025-02-13 20:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:33][root][INFO] - Training Epoch: 1/2, step 6605/7134 completed (loss: 0.3342815041542053, acc: 0.9032257795333862)
[2025-02-13 20:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34][root][INFO] - Training Epoch: 1/2, step 6606/7134 completed (loss: 0.1619012951850891, acc: 0.9553072452545166)
[2025-02-13 20:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34][root][INFO] - Training Epoch: 1/2, step 6607/7134 completed (loss: 0.3015470504760742, acc: 0.9419354796409607)
[2025-02-13 20:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35][root][INFO] - Training Epoch: 1/2, step 6608/7134 completed (loss: 0.12585890293121338, acc: 0.965753436088562)
[2025-02-13 20:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35][root][INFO] - Training Epoch: 1/2, step 6609/7134 completed (loss: 0.19036197662353516, acc: 0.9496855139732361)
[2025-02-13 20:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35][root][INFO] - Training Epoch: 1/2, step 6610/7134 completed (loss: 0.27002665400505066, acc: 0.9333333373069763)
[2025-02-13 20:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36][root][INFO] - Training Epoch: 1/2, step 6611/7134 completed (loss: 0.21043729782104492, acc: 0.9246575236320496)
[2025-02-13 20:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36][root][INFO] - Training Epoch: 1/2, step 6612/7134 completed (loss: 0.31949925422668457, acc: 0.9384615421295166)
[2025-02-13 20:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36][root][INFO] - Training Epoch: 1/2, step 6613/7134 completed (loss: 0.172223761677742, acc: 0.9664429426193237)
[2025-02-13 20:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:37][root][INFO] - Training Epoch: 1/2, step 6614/7134 completed (loss: 0.15097209811210632, acc: 0.9670329689979553)
[2025-02-13 20:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:37][root][INFO] - Training Epoch: 1/2, step 6615/7134 completed (loss: 0.2088935673236847, acc: 0.9527027010917664)
[2025-02-13 20:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38][root][INFO] - Training Epoch: 1/2, step 6616/7134 completed (loss: 0.14610427618026733, acc: 0.9757575988769531)
[2025-02-13 20:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38][root][INFO] - Training Epoch: 1/2, step 6617/7134 completed (loss: 0.14905036985874176, acc: 0.970370352268219)
[2025-02-13 20:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38][root][INFO] - Training Epoch: 1/2, step 6618/7134 completed (loss: 0.04520281404256821, acc: 0.9927536249160767)
[2025-02-13 20:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39][root][INFO] - Training Epoch: 1/2, step 6619/7134 completed (loss: 0.07838578522205353, acc: 0.9887005686759949)
[2025-02-13 20:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39][root][INFO] - Training Epoch: 1/2, step 6620/7134 completed (loss: 0.23901303112506866, acc: 0.9305555820465088)
[2025-02-13 20:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40][root][INFO] - Training Epoch: 1/2, step 6621/7134 completed (loss: 0.16800403594970703, acc: 0.9599999785423279)
[2025-02-13 20:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40][root][INFO] - Training Epoch: 1/2, step 6622/7134 completed (loss: 0.12215261906385422, acc: 0.9615384340286255)
[2025-02-13 20:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40][root][INFO] - Training Epoch: 1/2, step 6623/7134 completed (loss: 0.12825435400009155, acc: 0.9640718698501587)
[2025-02-13 20:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41][root][INFO] - Training Epoch: 1/2, step 6624/7134 completed (loss: 0.08988448232412338, acc: 0.987261176109314)
[2025-02-13 20:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41][root][INFO] - Training Epoch: 1/2, step 6625/7134 completed (loss: 0.10896240919828415, acc: 0.9668874144554138)
[2025-02-13 20:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42][root][INFO] - Training Epoch: 1/2, step 6626/7134 completed (loss: 0.12595734000205994, acc: 0.954285740852356)
[2025-02-13 20:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42][root][INFO] - Training Epoch: 1/2, step 6627/7134 completed (loss: 0.1448289006948471, acc: 0.9623655676841736)
[2025-02-13 20:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42][root][INFO] - Training Epoch: 1/2, step 6628/7134 completed (loss: 0.13251745700836182, acc: 0.9642857313156128)
[2025-02-13 20:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:43][root][INFO] - Training Epoch: 1/2, step 6629/7134 completed (loss: 0.2265925407409668, acc: 0.9624999761581421)
[2025-02-13 20:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:43][root][INFO] - Training Epoch: 1/2, step 6630/7134 completed (loss: 0.12694968283176422, acc: 0.9739583134651184)
[2025-02-13 20:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44][root][INFO] - Training Epoch: 1/2, step 6631/7134 completed (loss: 0.09867363423109055, acc: 0.9813664555549622)
[2025-02-13 20:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44][root][INFO] - Training Epoch: 1/2, step 6632/7134 completed (loss: 0.10008414089679718, acc: 0.9695431590080261)
[2025-02-13 20:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44][root][INFO] - Training Epoch: 1/2, step 6633/7134 completed (loss: 0.12327191233634949, acc: 0.9835164546966553)
[2025-02-13 20:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45][root][INFO] - Training Epoch: 1/2, step 6634/7134 completed (loss: 0.13975095748901367, acc: 0.9548872113227844)
[2025-02-13 20:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45][root][INFO] - Training Epoch: 1/2, step 6635/7134 completed (loss: 0.33744800090789795, acc: 0.9350000023841858)
[2025-02-13 20:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46][root][INFO] - Training Epoch: 1/2, step 6636/7134 completed (loss: 0.2087973654270172, acc: 0.9317073225975037)
[2025-02-13 20:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46][root][INFO] - Training Epoch: 1/2, step 6637/7134 completed (loss: 0.13049757480621338, acc: 0.9624999761581421)
[2025-02-13 20:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46][root][INFO] - Training Epoch: 1/2, step 6638/7134 completed (loss: 0.15111176669597626, acc: 0.9670329689979553)
[2025-02-13 20:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47][root][INFO] - Training Epoch: 1/2, step 6639/7134 completed (loss: 0.30158257484436035, acc: 0.9281437397003174)
[2025-02-13 20:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47][root][INFO] - Training Epoch: 1/2, step 6640/7134 completed (loss: 0.11287851631641388, acc: 0.9808917045593262)
[2025-02-13 20:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:48][root][INFO] - Training Epoch: 1/2, step 6641/7134 completed (loss: 0.3303983509540558, acc: 0.9336283206939697)
[2025-02-13 20:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:48][root][INFO] - Training Epoch: 1/2, step 6642/7134 completed (loss: 0.21562433242797852, acc: 0.9277108311653137)
[2025-02-13 20:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49][root][INFO] - Training Epoch: 1/2, step 6643/7134 completed (loss: 0.1573558896780014, acc: 0.9623655676841736)
[2025-02-13 20:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49][root][INFO] - Training Epoch: 1/2, step 6644/7134 completed (loss: 0.24124890565872192, acc: 0.9336099624633789)
[2025-02-13 20:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49][root][INFO] - Training Epoch: 1/2, step 6645/7134 completed (loss: 0.10884464532136917, acc: 0.9682539701461792)
[2025-02-13 20:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50][root][INFO] - Training Epoch: 1/2, step 6646/7134 completed (loss: 0.19129520654678345, acc: 0.9650654792785645)
[2025-02-13 20:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50][root][INFO] - Training Epoch: 1/2, step 6647/7134 completed (loss: 0.08615157753229141, acc: 0.9764705896377563)
[2025-02-13 20:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50][root][INFO] - Training Epoch: 1/2, step 6648/7134 completed (loss: 0.22825519740581512, acc: 0.948387086391449)
[2025-02-13 20:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:51][root][INFO] - Training Epoch: 1/2, step 6649/7134 completed (loss: 0.1970311403274536, acc: 0.9468598961830139)
[2025-02-13 20:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:51][root][INFO] - Training Epoch: 1/2, step 6650/7134 completed (loss: 0.14425556361675262, acc: 0.9545454382896423)
[2025-02-13 20:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52][root][INFO] - Training Epoch: 1/2, step 6651/7134 completed (loss: 0.1914801150560379, acc: 0.9488636255264282)
[2025-02-13 20:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52][root][INFO] - Training Epoch: 1/2, step 6652/7134 completed (loss: 0.2789318561553955, acc: 0.9215686321258545)
[2025-02-13 20:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52][root][INFO] - Training Epoch: 1/2, step 6653/7134 completed (loss: 0.38098540902137756, acc: 0.8909090757369995)
[2025-02-13 20:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:53][root][INFO] - Training Epoch: 1/2, step 6654/7134 completed (loss: 0.39611873030662537, acc: 0.9177215099334717)
[2025-02-13 20:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:53][root][INFO] - Training Epoch: 1/2, step 6655/7134 completed (loss: 0.13568519055843353, acc: 0.9714285731315613)
[2025-02-13 20:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54][root][INFO] - Training Epoch: 1/2, step 6656/7134 completed (loss: 0.2865672707557678, acc: 0.9210526347160339)
[2025-02-13 20:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54][root][INFO] - Training Epoch: 1/2, step 6657/7134 completed (loss: 0.2412971556186676, acc: 0.936170220375061)
[2025-02-13 20:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54][root][INFO] - Training Epoch: 1/2, step 6658/7134 completed (loss: 0.15402869880199432, acc: 0.9543147087097168)
[2025-02-13 20:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:55][root][INFO] - Training Epoch: 1/2, step 6659/7134 completed (loss: 0.14183448255062103, acc: 0.9726775884628296)
[2025-02-13 20:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:55][root][INFO] - Training Epoch: 1/2, step 6660/7134 completed (loss: 0.10240821540355682, acc: 0.9836065769195557)
[2025-02-13 20:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56][root][INFO] - Training Epoch: 1/2, step 6661/7134 completed (loss: 0.23435567319393158, acc: 0.9402984976768494)
[2025-02-13 20:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56][root][INFO] - Training Epoch: 1/2, step 6662/7134 completed (loss: 0.41727060079574585, acc: 0.8633093237876892)
[2025-02-13 20:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56][root][INFO] - Training Epoch: 1/2, step 6663/7134 completed (loss: 0.17949558794498444, acc: 0.948051929473877)
[2025-02-13 20:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57][root][INFO] - Training Epoch: 1/2, step 6664/7134 completed (loss: 0.2189904898405075, acc: 0.9357143044471741)
[2025-02-13 20:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57][root][INFO] - Training Epoch: 1/2, step 6665/7134 completed (loss: 0.34382081031799316, acc: 0.9119496941566467)
[2025-02-13 20:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58][root][INFO] - Training Epoch: 1/2, step 6666/7134 completed (loss: 0.09684644639492035, acc: 0.9696969985961914)
[2025-02-13 20:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58][root][INFO] - Training Epoch: 1/2, step 6667/7134 completed (loss: 0.2722523510456085, acc: 0.8979591727256775)
[2025-02-13 20:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58][root][INFO] - Training Epoch: 1/2, step 6668/7134 completed (loss: 0.06852996349334717, acc: 0.985401451587677)
[2025-02-13 20:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59][root][INFO] - Training Epoch: 1/2, step 6669/7134 completed (loss: 0.08455568552017212, acc: 0.9791666865348816)
[2025-02-13 20:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59][root][INFO] - Training Epoch: 1/2, step 6670/7134 completed (loss: 0.12610583007335663, acc: 0.9640287756919861)
[2025-02-13 20:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59][root][INFO] - Training Epoch: 1/2, step 6671/7134 completed (loss: 0.06900608539581299, acc: 0.9817073345184326)
[2025-02-13 20:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:00][root][INFO] - Training Epoch: 1/2, step 6672/7134 completed (loss: 0.0831698477268219, acc: 0.9760000109672546)
[2025-02-13 20:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:00][root][INFO] - Training Epoch: 1/2, step 6673/7134 completed (loss: 0.08438819646835327, acc: 0.9685534834861755)
[2025-02-13 20:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01][root][INFO] - Training Epoch: 1/2, step 6674/7134 completed (loss: 0.04343391954898834, acc: 0.991525411605835)
[2025-02-13 20:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01][root][INFO] - Training Epoch: 1/2, step 6675/7134 completed (loss: 0.2210482656955719, acc: 0.9509803652763367)
[2025-02-13 20:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01][root][INFO] - Training Epoch: 1/2, step 6676/7134 completed (loss: 0.0344444140791893, acc: 0.9900000095367432)
[2025-02-13 20:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:02][root][INFO] - Training Epoch: 1/2, step 6677/7134 completed (loss: 0.07692841440439224, acc: 0.9923664331436157)
[2025-02-13 20:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:02][root][INFO] - Training Epoch: 1/2, step 6678/7134 completed (loss: 0.08439698070287704, acc: 0.9851852059364319)
[2025-02-13 20:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03][root][INFO] - Training Epoch: 1/2, step 6679/7134 completed (loss: 0.042435210198163986, acc: 0.991150438785553)
[2025-02-13 20:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03][root][INFO] - Training Epoch: 1/2, step 6680/7134 completed (loss: 0.03550906479358673, acc: 0.9885057210922241)
[2025-02-13 20:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03][root][INFO] - Training Epoch: 1/2, step 6681/7134 completed (loss: 0.01153819914907217, acc: 1.0)
[2025-02-13 20:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04][root][INFO] - Training Epoch: 1/2, step 6682/7134 completed (loss: 0.214344784617424, acc: 0.9611650705337524)
[2025-02-13 20:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04][root][INFO] - Training Epoch: 1/2, step 6683/7134 completed (loss: 0.2212887704372406, acc: 0.977011501789093)
[2025-02-13 20:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04][root][INFO] - Training Epoch: 1/2, step 6684/7134 completed (loss: 0.14785218238830566, acc: 0.960629940032959)
[2025-02-13 20:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:05][root][INFO] - Training Epoch: 1/2, step 6685/7134 completed (loss: 0.13928140699863434, acc: 0.9359999895095825)
[2025-02-13 20:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:05][root][INFO] - Training Epoch: 1/2, step 6686/7134 completed (loss: 0.22969241440296173, acc: 0.9568965435028076)
[2025-02-13 20:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06][root][INFO] - Training Epoch: 1/2, step 6687/7134 completed (loss: 0.2052774429321289, acc: 0.9651162624359131)
[2025-02-13 20:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06][root][INFO] - Training Epoch: 1/2, step 6688/7134 completed (loss: 0.4503091871738434, acc: 0.8848921060562134)
[2025-02-13 20:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06][root][INFO] - Training Epoch: 1/2, step 6689/7134 completed (loss: 0.2550901174545288, acc: 0.9259259104728699)
[2025-02-13 20:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07][root][INFO] - Training Epoch: 1/2, step 6690/7134 completed (loss: 0.3639076054096222, acc: 0.9019607901573181)
[2025-02-13 20:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07][root][INFO] - Training Epoch: 1/2, step 6691/7134 completed (loss: 0.15427003800868988, acc: 0.9583333134651184)
[2025-02-13 20:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08][root][INFO] - Training Epoch: 1/2, step 6692/7134 completed (loss: 0.34320345520973206, acc: 0.9158878326416016)
[2025-02-13 20:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08][root][INFO] - Training Epoch: 1/2, step 6693/7134 completed (loss: 0.20707759261131287, acc: 0.9555555582046509)
[2025-02-13 20:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08][root][INFO] - Training Epoch: 1/2, step 6694/7134 completed (loss: 0.086354561150074, acc: 0.9668874144554138)
[2025-02-13 20:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09][root][INFO] - Training Epoch: 1/2, step 6695/7134 completed (loss: 0.12058553844690323, acc: 0.9715909361839294)
[2025-02-13 20:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09][root][INFO] - Training Epoch: 1/2, step 6696/7134 completed (loss: 0.21001702547073364, acc: 0.9583333134651184)
[2025-02-13 20:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10][root][INFO] - Training Epoch: 1/2, step 6697/7134 completed (loss: 0.3497186005115509, acc: 0.9285714030265808)
[2025-02-13 20:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10][root][INFO] - Training Epoch: 1/2, step 6698/7134 completed (loss: 0.1403219848871231, acc: 0.9597315192222595)
[2025-02-13 20:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10][root][INFO] - Training Epoch: 1/2, step 6699/7134 completed (loss: 0.18604165315628052, acc: 0.961904764175415)
[2025-02-13 20:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11][root][INFO] - Training Epoch: 1/2, step 6700/7134 completed (loss: 0.39966604113578796, acc: 0.8888888955116272)
[2025-02-13 20:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11][root][INFO] - Training Epoch: 1/2, step 6701/7134 completed (loss: 0.6901951432228088, acc: 0.8134328126907349)
[2025-02-13 20:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11][root][INFO] - Training Epoch: 1/2, step 6702/7134 completed (loss: 0.2508508861064911, acc: 0.9304347634315491)
[2025-02-13 20:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:12][root][INFO] - Training Epoch: 1/2, step 6703/7134 completed (loss: 0.23007574677467346, acc: 0.9492753744125366)
[2025-02-13 20:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:12][root][INFO] - Training Epoch: 1/2, step 6704/7134 completed (loss: 0.3427896499633789, acc: 0.8850574493408203)
[2025-02-13 20:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13][root][INFO] - Training Epoch: 1/2, step 6705/7134 completed (loss: 0.15290199220180511, acc: 0.9421965479850769)
[2025-02-13 20:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13][root][INFO] - Training Epoch: 1/2, step 6706/7134 completed (loss: 0.17703816294670105, acc: 0.9636363387107849)
[2025-02-13 20:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13][root][INFO] - Training Epoch: 1/2, step 6707/7134 completed (loss: 0.17929479479789734, acc: 0.9487179517745972)
[2025-02-13 20:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14][root][INFO] - Training Epoch: 1/2, step 6708/7134 completed (loss: 0.19873318076133728, acc: 0.9389312863349915)
[2025-02-13 20:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14][root][INFO] - Training Epoch: 1/2, step 6709/7134 completed (loss: 0.22859403491020203, acc: 0.9634146094322205)
[2025-02-13 20:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14][root][INFO] - Training Epoch: 1/2, step 6710/7134 completed (loss: 0.2098073810338974, acc: 0.9473684430122375)
[2025-02-13 20:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:15][root][INFO] - Training Epoch: 1/2, step 6711/7134 completed (loss: 0.21362581849098206, acc: 0.9746835231781006)
[2025-02-13 20:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:15][root][INFO] - Training Epoch: 1/2, step 6712/7134 completed (loss: 0.31361123919487, acc: 0.8965517282485962)
[2025-02-13 20:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16][root][INFO] - Training Epoch: 1/2, step 6713/7134 completed (loss: 0.522320568561554, acc: 0.907975435256958)
[2025-02-13 20:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16][root][INFO] - Training Epoch: 1/2, step 6714/7134 completed (loss: 0.3075938820838928, acc: 0.9364162087440491)
[2025-02-13 20:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16][root][INFO] - Training Epoch: 1/2, step 6715/7134 completed (loss: 0.49102601408958435, acc: 0.8986486196517944)
[2025-02-13 20:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:17][root][INFO] - Training Epoch: 1/2, step 6716/7134 completed (loss: 0.16282221674919128, acc: 0.9452054500579834)
[2025-02-13 20:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:17][root][INFO] - Training Epoch: 1/2, step 6717/7134 completed (loss: 0.26753440499305725, acc: 0.9459459185600281)
[2025-02-13 20:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18][root][INFO] - Training Epoch: 1/2, step 6718/7134 completed (loss: 0.17217570543289185, acc: 0.9586206674575806)
[2025-02-13 20:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18][root][INFO] - Training Epoch: 1/2, step 6719/7134 completed (loss: 0.20811748504638672, acc: 0.9463087320327759)
[2025-02-13 20:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18][root][INFO] - Training Epoch: 1/2, step 6720/7134 completed (loss: 0.27734896540641785, acc: 0.931034505367279)
[2025-02-13 20:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:19][root][INFO] - Training Epoch: 1/2, step 6721/7134 completed (loss: 0.29666632413864136, acc: 0.9363057613372803)
[2025-02-13 20:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:19][root][INFO] - Training Epoch: 1/2, step 6722/7134 completed (loss: 0.37606188654899597, acc: 0.9300699234008789)
[2025-02-13 20:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20][root][INFO] - Training Epoch: 1/2, step 6723/7134 completed (loss: 0.2190527617931366, acc: 0.9234972596168518)
[2025-02-13 20:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20][root][INFO] - Training Epoch: 1/2, step 6724/7134 completed (loss: 0.2794899046421051, acc: 0.9496855139732361)
[2025-02-13 20:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20][root][INFO] - Training Epoch: 1/2, step 6725/7134 completed (loss: 0.18023979663848877, acc: 0.9539473652839661)
[2025-02-13 20:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21][root][INFO] - Training Epoch: 1/2, step 6726/7134 completed (loss: 0.39018186926841736, acc: 0.942148745059967)
[2025-02-13 20:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21][root][INFO] - Training Epoch: 1/2, step 6727/7134 completed (loss: 0.23116381466388702, acc: 0.9280575513839722)
[2025-02-13 20:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21][root][INFO] - Training Epoch: 1/2, step 6728/7134 completed (loss: 0.3607781231403351, acc: 0.8903225660324097)
[2025-02-13 20:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22][root][INFO] - Training Epoch: 1/2, step 6729/7134 completed (loss: 0.08633396774530411, acc: 0.9873417615890503)
[2025-02-13 20:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22][root][INFO] - Training Epoch: 1/2, step 6730/7134 completed (loss: 0.31061214208602905, acc: 0.9195402264595032)
[2025-02-13 20:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23][root][INFO] - Training Epoch: 1/2, step 6731/7134 completed (loss: 0.16179999709129333, acc: 0.9698795080184937)
[2025-02-13 20:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23][root][INFO] - Training Epoch: 1/2, step 6732/7134 completed (loss: 0.14972077310085297, acc: 0.9551281929016113)
[2025-02-13 20:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23][root][INFO] - Training Epoch: 1/2, step 6733/7134 completed (loss: 0.16161909699440002, acc: 0.9568345546722412)
[2025-02-13 20:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:24][root][INFO] - Training Epoch: 1/2, step 6734/7134 completed (loss: 0.26208731532096863, acc: 0.9440559148788452)
[2025-02-13 20:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:24][root][INFO] - Training Epoch: 1/2, step 6735/7134 completed (loss: 0.2563275992870331, acc: 0.9358974099159241)
[2025-02-13 20:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25][root][INFO] - Training Epoch: 1/2, step 6736/7134 completed (loss: 0.24979858100414276, acc: 0.9333333373069763)
[2025-02-13 20:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25][root][INFO] - Training Epoch: 1/2, step 6737/7134 completed (loss: 0.25627565383911133, acc: 0.9470198750495911)
[2025-02-13 20:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25][root][INFO] - Training Epoch: 1/2, step 6738/7134 completed (loss: 0.17735472321510315, acc: 0.9485294222831726)
[2025-02-13 20:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:26][root][INFO] - Training Epoch: 1/2, step 6739/7134 completed (loss: 0.12612974643707275, acc: 0.9642857313156128)
[2025-02-13 20:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:26][root][INFO] - Training Epoch: 1/2, step 6740/7134 completed (loss: 0.31622743606567383, acc: 0.9391891956329346)
[2025-02-13 20:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27][root][INFO] - Training Epoch: 1/2, step 6741/7134 completed (loss: 0.10043454170227051, acc: 0.9759036302566528)
[2025-02-13 20:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27][root][INFO] - Training Epoch: 1/2, step 6742/7134 completed (loss: 0.07224605977535248, acc: 0.9764705896377563)
[2025-02-13 20:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27][root][INFO] - Training Epoch: 1/2, step 6743/7134 completed (loss: 0.06082713603973389, acc: 0.9887005686759949)
[2025-02-13 20:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:28][root][INFO] - Training Epoch: 1/2, step 6744/7134 completed (loss: 0.10300514101982117, acc: 0.9662162065505981)
[2025-02-13 20:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:28][root][INFO] - Training Epoch: 1/2, step 6745/7134 completed (loss: 0.03840257227420807, acc: 0.9941860437393188)
[2025-02-13 20:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29][root][INFO] - Training Epoch: 1/2, step 6746/7134 completed (loss: 0.05853276699781418, acc: 0.9925373196601868)
[2025-02-13 20:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29][root][INFO] - Training Epoch: 1/2, step 6747/7134 completed (loss: 0.09005124121904373, acc: 0.9685863852500916)
[2025-02-13 20:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29][root][INFO] - Training Epoch: 1/2, step 6748/7134 completed (loss: 0.11945396661758423, acc: 0.9852941036224365)
[2025-02-13 20:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:30][root][INFO] - Training Epoch: 1/2, step 6749/7134 completed (loss: 0.1609116643667221, acc: 0.9511111378669739)
[2025-02-13 20:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:30][root][INFO] - Training Epoch: 1/2, step 6750/7134 completed (loss: 0.14070548117160797, acc: 0.9653465151786804)
[2025-02-13 20:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31][root][INFO] - Training Epoch: 1/2, step 6751/7134 completed (loss: 0.10629774630069733, acc: 0.9800994992256165)
[2025-02-13 20:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31][root][INFO] - Training Epoch: 1/2, step 6752/7134 completed (loss: 0.17414703965187073, acc: 0.9693251252174377)
[2025-02-13 20:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31][root][INFO] - Training Epoch: 1/2, step 6753/7134 completed (loss: 0.05630648881196976, acc: 0.9797979593276978)
[2025-02-13 20:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32][root][INFO] - Training Epoch: 1/2, step 6754/7134 completed (loss: 0.13460755348205566, acc: 0.9738219976425171)
[2025-02-13 20:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32][root][INFO] - Training Epoch: 1/2, step 6755/7134 completed (loss: 0.16203556954860687, acc: 0.963350772857666)
[2025-02-13 20:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33][root][INFO] - Training Epoch: 1/2, step 6756/7134 completed (loss: 0.041221946477890015, acc: 0.983146071434021)
[2025-02-13 20:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33][root][INFO] - Training Epoch: 1/2, step 6757/7134 completed (loss: 0.047685541212558746, acc: 0.9935483932495117)
[2025-02-13 20:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33][root][INFO] - Training Epoch: 1/2, step 6758/7134 completed (loss: 0.0609988272190094, acc: 0.9896373152732849)
[2025-02-13 20:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:34][root][INFO] - Training Epoch: 1/2, step 6759/7134 completed (loss: 0.07168907672166824, acc: 0.9776785969734192)
[2025-02-13 20:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:34][root][INFO] - Training Epoch: 1/2, step 6760/7134 completed (loss: 0.2642480134963989, acc: 0.9680851101875305)
[2025-02-13 20:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35][root][INFO] - Training Epoch: 1/2, step 6761/7134 completed (loss: 0.2076931744813919, acc: 0.9714285731315613)
[2025-02-13 20:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35][root][INFO] - Training Epoch: 1/2, step 6762/7134 completed (loss: 0.048367906361818314, acc: 0.9850000143051147)
[2025-02-13 20:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35][root][INFO] - Training Epoch: 1/2, step 6763/7134 completed (loss: 0.06030064821243286, acc: 0.9946523904800415)
[2025-02-13 20:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:36][root][INFO] - Training Epoch: 1/2, step 6764/7134 completed (loss: 0.08844148367643356, acc: 0.9823529124259949)
[2025-02-13 20:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:36][root][INFO] - Training Epoch: 1/2, step 6765/7134 completed (loss: 0.052408117800951004, acc: 0.9890109896659851)
[2025-02-13 20:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37][root][INFO] - Training Epoch: 1/2, step 6766/7134 completed (loss: 0.1041344478726387, acc: 0.9828571677207947)
[2025-02-13 20:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37][root][INFO] - Training Epoch: 1/2, step 6767/7134 completed (loss: 0.18795092403888702, acc: 0.9553571343421936)
[2025-02-13 20:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37][root][INFO] - Training Epoch: 1/2, step 6768/7134 completed (loss: 0.05655105412006378, acc: 0.976190447807312)
[2025-02-13 20:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:38][root][INFO] - Training Epoch: 1/2, step 6769/7134 completed (loss: 0.20664231479167938, acc: 0.9551281929016113)
[2025-02-13 20:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:38][root][INFO] - Training Epoch: 1/2, step 6770/7134 completed (loss: 0.16104035079479218, acc: 0.9684210419654846)
[2025-02-13 20:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39][root][INFO] - Training Epoch: 1/2, step 6771/7134 completed (loss: 0.06117497384548187, acc: 0.9941520690917969)
[2025-02-13 20:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39][root][INFO] - Training Epoch: 1/2, step 6772/7134 completed (loss: 0.2984682619571686, acc: 0.9230769276618958)
[2025-02-13 20:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39][root][INFO] - Training Epoch: 1/2, step 6773/7134 completed (loss: 0.13020873069763184, acc: 0.9691358208656311)
[2025-02-13 20:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40][root][INFO] - Training Epoch: 1/2, step 6774/7134 completed (loss: 0.18887558579444885, acc: 0.9526627063751221)
[2025-02-13 20:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40][root][INFO] - Training Epoch: 1/2, step 6775/7134 completed (loss: 0.14958657324314117, acc: 0.9655172228813171)
[2025-02-13 20:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40][root][INFO] - Training Epoch: 1/2, step 6776/7134 completed (loss: 0.10677629709243774, acc: 0.984000027179718)
[2025-02-13 20:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:41][root][INFO] - Training Epoch: 1/2, step 6777/7134 completed (loss: 0.19180160760879517, acc: 0.977142870426178)
[2025-02-13 20:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:41][root][INFO] - Training Epoch: 1/2, step 6778/7134 completed (loss: 0.06960117071866989, acc: 0.9886363744735718)
[2025-02-13 20:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42][root][INFO] - Training Epoch: 1/2, step 6779/7134 completed (loss: 0.09298785775899887, acc: 0.976190447807312)
[2025-02-13 20:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42][root][INFO] - Training Epoch: 1/2, step 6780/7134 completed (loss: 0.051328230649232864, acc: 0.9940119981765747)
[2025-02-13 20:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42][root][INFO] - Training Epoch: 1/2, step 6781/7134 completed (loss: 0.040383003652095795, acc: 0.9941520690917969)
[2025-02-13 20:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43][root][INFO] - Training Epoch: 1/2, step 6782/7134 completed (loss: 0.04440658539533615, acc: 0.9935483932495117)
[2025-02-13 20:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43][root][INFO] - Training Epoch: 1/2, step 6783/7134 completed (loss: 0.12370628863573074, acc: 0.9655172228813171)
[2025-02-13 20:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43][root][INFO] - Training Epoch: 1/2, step 6784/7134 completed (loss: 0.04443508759140968, acc: 0.9873417615890503)
[2025-02-13 20:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:44][root][INFO] - Training Epoch: 1/2, step 6785/7134 completed (loss: 0.05460642650723457, acc: 0.9803921580314636)
[2025-02-13 20:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:44][root][INFO] - Training Epoch: 1/2, step 6786/7134 completed (loss: 0.05646616593003273, acc: 0.9887005686759949)
[2025-02-13 20:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45][root][INFO] - Training Epoch: 1/2, step 6787/7134 completed (loss: 0.05648575350642204, acc: 0.9836065769195557)
[2025-02-13 20:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45][root][INFO] - Training Epoch: 1/2, step 6788/7134 completed (loss: 0.09851237386465073, acc: 0.971222996711731)
[2025-02-13 20:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45][root][INFO] - Training Epoch: 1/2, step 6789/7134 completed (loss: 0.050495900213718414, acc: 0.978723406791687)
[2025-02-13 20:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:46][root][INFO] - Training Epoch: 1/2, step 6790/7134 completed (loss: 0.1484643816947937, acc: 0.9659090638160706)
[2025-02-13 20:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:46][root][INFO] - Training Epoch: 1/2, step 6791/7134 completed (loss: 0.10234414041042328, acc: 0.9551281929016113)
[2025-02-13 20:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47][root][INFO] - Training Epoch: 1/2, step 6792/7134 completed (loss: 0.032939981669187546, acc: 1.0)
[2025-02-13 20:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47][root][INFO] - Training Epoch: 1/2, step 6793/7134 completed (loss: 0.05097740888595581, acc: 0.9824561476707458)
[2025-02-13 20:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47][root][INFO] - Training Epoch: 1/2, step 6794/7134 completed (loss: 0.0840175524353981, acc: 0.9863945841789246)
[2025-02-13 20:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:48][root][INFO] - Training Epoch: 1/2, step 6795/7134 completed (loss: 0.05447862297296524, acc: 0.9942857027053833)
[2025-02-13 20:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:48][root][INFO] - Training Epoch: 1/2, step 6796/7134 completed (loss: 0.027117162942886353, acc: 0.9883720874786377)
[2025-02-13 20:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49][root][INFO] - Training Epoch: 1/2, step 6797/7134 completed (loss: 0.18722674250602722, acc: 0.9634146094322205)
[2025-02-13 20:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49][root][INFO] - Training Epoch: 1/2, step 6798/7134 completed (loss: 0.0884893462061882, acc: 0.9879518151283264)
[2025-02-13 20:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49][root][INFO] - Training Epoch: 1/2, step 6799/7134 completed (loss: 0.06009684503078461, acc: 0.9820359349250793)
[2025-02-13 20:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50][root][INFO] - Training Epoch: 1/2, step 6800/7134 completed (loss: 0.11152045428752899, acc: 0.9620253443717957)
[2025-02-13 20:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50][root][INFO] - Training Epoch: 1/2, step 6801/7134 completed (loss: 0.08855731040239334, acc: 0.9784172773361206)
[2025-02-13 20:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50][root][INFO] - Training Epoch: 1/2, step 6802/7134 completed (loss: 0.06673014909029007, acc: 0.9860140085220337)
[2025-02-13 20:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51][root][INFO] - Training Epoch: 1/2, step 6803/7134 completed (loss: 0.1157887876033783, acc: 0.9666666388511658)
[2025-02-13 20:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51][root][INFO] - Training Epoch: 1/2, step 6804/7134 completed (loss: 0.17337258160114288, acc: 0.9597315192222595)
[2025-02-13 20:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52][root][INFO] - Training Epoch: 1/2, step 6805/7134 completed (loss: 0.19879576563835144, acc: 0.9407407641410828)
[2025-02-13 20:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52][root][INFO] - Training Epoch: 1/2, step 6806/7134 completed (loss: 0.11616550385951996, acc: 0.9719101190567017)
[2025-02-13 20:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52][root][INFO] - Training Epoch: 1/2, step 6807/7134 completed (loss: 0.0639999657869339, acc: 0.981249988079071)
[2025-02-13 20:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53][root][INFO] - Training Epoch: 1/2, step 6808/7134 completed (loss: 0.12371952831745148, acc: 0.9664429426193237)
[2025-02-13 20:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53][root][INFO] - Training Epoch: 1/2, step 6809/7134 completed (loss: 0.17669068276882172, acc: 0.9615384340286255)
[2025-02-13 20:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53][root][INFO] - Training Epoch: 1/2, step 6810/7134 completed (loss: 0.09229018539190292, acc: 0.9545454382896423)
[2025-02-13 20:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54][root][INFO] - Training Epoch: 1/2, step 6811/7134 completed (loss: 0.08963677287101746, acc: 0.9767441749572754)
[2025-02-13 20:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54][root][INFO] - Training Epoch: 1/2, step 6812/7134 completed (loss: 0.0635271817445755, acc: 1.0)
[2025-02-13 20:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:55][root][INFO] - Training Epoch: 1/2, step 6813/7134 completed (loss: 0.07540526986122131, acc: 0.987500011920929)
[2025-02-13 20:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:55][root][INFO] - Training Epoch: 1/2, step 6814/7134 completed (loss: 0.052224528044462204, acc: 0.9942857027053833)
[2025-02-13 20:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56][root][INFO] - Training Epoch: 1/2, step 6815/7134 completed (loss: 0.08736351132392883, acc: 0.9764705896377563)
[2025-02-13 20:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56][root][INFO] - Training Epoch: 1/2, step 6816/7134 completed (loss: 0.13168659806251526, acc: 0.9652777910232544)
[2025-02-13 20:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56][root][INFO] - Training Epoch: 1/2, step 6817/7134 completed (loss: 0.19341520965099335, acc: 0.9577465057373047)
[2025-02-13 20:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57][root][INFO] - Training Epoch: 1/2, step 6818/7134 completed (loss: 0.056044138967990875, acc: 0.9931972622871399)
[2025-02-13 20:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57][root][INFO] - Training Epoch: 1/2, step 6819/7134 completed (loss: 0.10744896531105042, acc: 0.9781022071838379)
[2025-02-13 20:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57][root][INFO] - Training Epoch: 1/2, step 6820/7134 completed (loss: 0.06280112266540527, acc: 0.9781022071838379)
[2025-02-13 20:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58][root][INFO] - Training Epoch: 1/2, step 6821/7134 completed (loss: 0.01624401845037937, acc: 1.0)
[2025-02-13 20:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58][root][INFO] - Training Epoch: 1/2, step 6822/7134 completed (loss: 0.3105320930480957, acc: 0.930232584476471)
[2025-02-13 20:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59][root][INFO] - Training Epoch: 1/2, step 6823/7134 completed (loss: 0.15030701458454132, acc: 0.9457831382751465)
[2025-02-13 20:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59][root][INFO] - Training Epoch: 1/2, step 6824/7134 completed (loss: 0.30724072456359863, acc: 0.8780487775802612)
[2025-02-13 20:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59][root][INFO] - Training Epoch: 1/2, step 6825/7134 completed (loss: 0.14958442747592926, acc: 0.9569892287254333)
[2025-02-13 20:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00][root][INFO] - Training Epoch: 1/2, step 6826/7134 completed (loss: 0.26362723112106323, acc: 0.9057971239089966)
[2025-02-13 20:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00][root][INFO] - Training Epoch: 1/2, step 6827/7134 completed (loss: 0.5047502517700195, acc: 0.90625)
[2025-02-13 20:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00][root][INFO] - Training Epoch: 1/2, step 6828/7134 completed (loss: 0.4180763363838196, acc: 0.8943662047386169)
[2025-02-13 20:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01][root][INFO] - Training Epoch: 1/2, step 6829/7134 completed (loss: 0.35119229555130005, acc: 0.9171597361564636)
[2025-02-13 20:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01][root][INFO] - Training Epoch: 1/2, step 6830/7134 completed (loss: 0.214164137840271, acc: 0.9459459185600281)
[2025-02-13 20:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02][root][INFO] - Training Epoch: 1/2, step 6831/7134 completed (loss: 0.26062148809432983, acc: 0.9385474920272827)
[2025-02-13 20:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02][root][INFO] - Training Epoch: 1/2, step 6832/7134 completed (loss: 0.2460840493440628, acc: 0.9305555820465088)
[2025-02-13 20:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02][root][INFO] - Training Epoch: 1/2, step 6833/7134 completed (loss: 0.22990849614143372, acc: 0.9363057613372803)
[2025-02-13 20:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03][root][INFO] - Training Epoch: 1/2, step 6834/7134 completed (loss: 0.2998310923576355, acc: 0.9328858852386475)
[2025-02-13 20:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03][root][INFO] - Training Epoch: 1/2, step 6835/7134 completed (loss: 0.2515541911125183, acc: 0.9477124214172363)
[2025-02-13 20:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03][root][INFO] - Training Epoch: 1/2, step 6836/7134 completed (loss: 0.18074296414852142, acc: 0.9431279897689819)
[2025-02-13 20:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04][root][INFO] - Training Epoch: 1/2, step 6837/7134 completed (loss: 0.1913311630487442, acc: 0.9646464586257935)
[2025-02-13 20:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04][root][INFO] - Training Epoch: 1/2, step 6838/7134 completed (loss: 0.3274846374988556, acc: 0.9096385836601257)
[2025-02-13 20:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04][root][INFO] - Training Epoch: 1/2, step 6839/7134 completed (loss: 0.5170289874076843, acc: 0.8933333158493042)
[2025-02-13 20:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05][root][INFO] - Training Epoch: 1/2, step 6840/7134 completed (loss: 0.33230307698249817, acc: 0.9299362897872925)
[2025-02-13 20:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05][root][INFO] - Training Epoch: 1/2, step 6841/7134 completed (loss: 0.1080833151936531, acc: 0.9756097793579102)
[2025-02-13 20:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06][root][INFO] - Training Epoch: 1/2, step 6842/7134 completed (loss: 0.19480514526367188, acc: 0.9395604133605957)
[2025-02-13 20:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06][root][INFO] - Training Epoch: 1/2, step 6843/7134 completed (loss: 0.18406569957733154, acc: 0.9481865167617798)
[2025-02-13 20:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06][root][INFO] - Training Epoch: 1/2, step 6844/7134 completed (loss: 0.21630585193634033, acc: 0.9328858852386475)
[2025-02-13 20:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07][root][INFO] - Training Epoch: 1/2, step 6845/7134 completed (loss: 0.1679336130619049, acc: 0.9461538195610046)
[2025-02-13 20:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07][root][INFO] - Training Epoch: 1/2, step 6846/7134 completed (loss: 0.14157195389270782, acc: 0.9629629850387573)
[2025-02-13 20:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07][root][INFO] - Training Epoch: 1/2, step 6847/7134 completed (loss: 0.17764458060264587, acc: 0.9437500238418579)
[2025-02-13 20:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:08][root][INFO] - Training Epoch: 1/2, step 6848/7134 completed (loss: 0.17175737023353577, acc: 0.966292142868042)
[2025-02-13 20:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:08][root][INFO] - Training Epoch: 1/2, step 6849/7134 completed (loss: 0.0961720272898674, acc: 0.9785714149475098)
[2025-02-13 20:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09][root][INFO] - Training Epoch: 1/2, step 6850/7134 completed (loss: 0.1338805854320526, acc: 0.9523809552192688)
[2025-02-13 20:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09][root][INFO] - Training Epoch: 1/2, step 6851/7134 completed (loss: 0.16800400614738464, acc: 0.950276255607605)
[2025-02-13 20:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09][root][INFO] - Training Epoch: 1/2, step 6852/7134 completed (loss: 0.1738201379776001, acc: 0.9571428298950195)
[2025-02-13 20:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10][root][INFO] - Training Epoch: 1/2, step 6853/7134 completed (loss: 0.1733105331659317, acc: 0.9479768872261047)
[2025-02-13 20:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10][root][INFO] - Training Epoch: 1/2, step 6854/7134 completed (loss: 0.1104448139667511, acc: 0.9745762944221497)
[2025-02-13 20:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10][root][INFO] - Training Epoch: 1/2, step 6855/7134 completed (loss: 0.044679511338472366, acc: 0.9906542301177979)
[2025-02-13 20:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:11][root][INFO] - Training Epoch: 1/2, step 6856/7134 completed (loss: 0.03072257712483406, acc: 0.9939758777618408)
[2025-02-13 20:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:11][root][INFO] - Training Epoch: 1/2, step 6857/7134 completed (loss: 0.031242744997143745, acc: 0.9940476417541504)
[2025-02-13 20:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12][root][INFO] - Training Epoch: 1/2, step 6858/7134 completed (loss: 0.03882096707820892, acc: 0.9848484992980957)
[2025-02-13 20:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12][root][INFO] - Training Epoch: 1/2, step 6859/7134 completed (loss: 0.03931907191872597, acc: 0.9887640476226807)
[2025-02-13 20:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12][root][INFO] - Training Epoch: 1/2, step 6860/7134 completed (loss: 0.05935472249984741, acc: 0.9862068891525269)
[2025-02-13 20:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:13][root][INFO] - Training Epoch: 1/2, step 6861/7134 completed (loss: 0.0829387903213501, acc: 0.9790209531784058)
[2025-02-13 20:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:13][root][INFO] - Training Epoch: 1/2, step 6862/7134 completed (loss: 0.1365058273077011, acc: 0.9696969985961914)
[2025-02-13 20:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14][root][INFO] - Training Epoch: 1/2, step 6863/7134 completed (loss: 0.06804364919662476, acc: 0.9882352948188782)
[2025-02-13 20:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14][root][INFO] - Training Epoch: 1/2, step 6864/7134 completed (loss: 0.06528908014297485, acc: 0.978723406791687)
[2025-02-13 20:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14][root][INFO] - Training Epoch: 1/2, step 6865/7134 completed (loss: 0.13755464553833008, acc: 0.9674796462059021)
[2025-02-13 20:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15][root][INFO] - Training Epoch: 1/2, step 6866/7134 completed (loss: 0.03256634622812271, acc: 0.9934640526771545)
[2025-02-13 20:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15][root][INFO] - Training Epoch: 1/2, step 6867/7134 completed (loss: 0.01113909762352705, acc: 1.0)
[2025-02-13 20:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15][root][INFO] - Training Epoch: 1/2, step 6868/7134 completed (loss: 0.04346991330385208, acc: 0.987500011920929)
[2025-02-13 20:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:16][root][INFO] - Training Epoch: 1/2, step 6869/7134 completed (loss: 0.020263681188225746, acc: 1.0)
[2025-02-13 20:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:16][root][INFO] - Training Epoch: 1/2, step 6870/7134 completed (loss: 0.09885169565677643, acc: 0.9873417615890503)
[2025-02-13 20:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17][root][INFO] - Training Epoch: 1/2, step 6871/7134 completed (loss: 0.03443550691008568, acc: 0.9920634627342224)
[2025-02-13 20:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17][root][INFO] - Training Epoch: 1/2, step 6872/7134 completed (loss: 0.08606012910604477, acc: 0.9943181872367859)
[2025-02-13 20:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17][root][INFO] - Training Epoch: 1/2, step 6873/7134 completed (loss: 0.10718023777008057, acc: 0.9823529124259949)
[2025-02-13 20:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18][root][INFO] - Training Epoch: 1/2, step 6874/7134 completed (loss: 0.04427697882056236, acc: 0.984375)
[2025-02-13 20:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18][root][INFO] - Training Epoch: 1/2, step 6875/7134 completed (loss: 0.15053638815879822, acc: 0.9597315192222595)
[2025-02-13 20:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18][root][INFO] - Training Epoch: 1/2, step 6876/7134 completed (loss: 0.05893857032060623, acc: 0.9770992398262024)
[2025-02-13 20:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:19][root][INFO] - Training Epoch: 1/2, step 6877/7134 completed (loss: 0.0645647868514061, acc: 0.9823529124259949)
[2025-02-13 20:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:19][root][INFO] - Training Epoch: 1/2, step 6878/7134 completed (loss: 0.16436666250228882, acc: 0.9542483687400818)
[2025-02-13 20:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20][root][INFO] - Training Epoch: 1/2, step 6879/7134 completed (loss: 0.1592807173728943, acc: 0.9741379022598267)
[2025-02-13 20:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20][root][INFO] - Training Epoch: 1/2, step 6880/7134 completed (loss: 0.04862276092171669, acc: 0.989847719669342)
[2025-02-13 20:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20][root][INFO] - Training Epoch: 1/2, step 6881/7134 completed (loss: 0.1705673635005951, acc: 0.9680851101875305)
[2025-02-13 20:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21][root][INFO] - Training Epoch: 1/2, step 6882/7134 completed (loss: 0.3086310923099518, acc: 0.8943662047386169)
[2025-02-13 20:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21][root][INFO] - Training Epoch: 1/2, step 6883/7134 completed (loss: 0.09212426096200943, acc: 0.9632353186607361)
[2025-02-13 20:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21][root][INFO] - Training Epoch: 1/2, step 6884/7134 completed (loss: 0.14014945924282074, acc: 0.9659090638160706)
[2025-02-13 20:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22][root][INFO] - Training Epoch: 1/2, step 6885/7134 completed (loss: 0.06781511753797531, acc: 0.9719626307487488)
[2025-02-13 20:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22][root][INFO] - Training Epoch: 1/2, step 6886/7134 completed (loss: 0.10996894538402557, acc: 0.9594594836235046)
[2025-02-13 20:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22][root][INFO] - Training Epoch: 1/2, step 6887/7134 completed (loss: 0.19034183025360107, acc: 0.9304347634315491)
[2025-02-13 20:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23][root][INFO] - Training Epoch: 1/2, step 6888/7134 completed (loss: 0.16441041231155396, acc: 0.9466666579246521)
[2025-02-13 20:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23][root][INFO] - Training Epoch: 1/2, step 6889/7134 completed (loss: 0.10341436415910721, acc: 0.9803921580314636)
[2025-02-13 20:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24][root][INFO] - Training Epoch: 1/2, step 6890/7134 completed (loss: 0.23179560899734497, acc: 0.9408602118492126)
[2025-02-13 20:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24][root][INFO] - Training Epoch: 1/2, step 6891/7134 completed (loss: 0.12290425598621368, acc: 0.9629629850387573)
[2025-02-13 20:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24][root][INFO] - Training Epoch: 1/2, step 6892/7134 completed (loss: 0.13512741029262543, acc: 0.9677419066429138)
[2025-02-13 20:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25][root][INFO] - Training Epoch: 1/2, step 6893/7134 completed (loss: 0.17361965775489807, acc: 0.942307710647583)
[2025-02-13 20:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25][root][INFO] - Training Epoch: 1/2, step 6894/7134 completed (loss: 0.1206262931227684, acc: 0.9791666865348816)
[2025-02-13 20:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26][root][INFO] - Training Epoch: 1/2, step 6895/7134 completed (loss: 0.12571211159229279, acc: 0.9704433679580688)
[2025-02-13 20:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26][root][INFO] - Training Epoch: 1/2, step 6896/7134 completed (loss: 0.20327381789684296, acc: 0.9479768872261047)
[2025-02-13 20:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26][root][INFO] - Training Epoch: 1/2, step 6897/7134 completed (loss: 0.15963177382946014, acc: 0.9702970385551453)
[2025-02-13 20:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27][root][INFO] - Training Epoch: 1/2, step 6898/7134 completed (loss: 0.134402796626091, acc: 0.9768785834312439)
[2025-02-13 20:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27][root][INFO] - Training Epoch: 1/2, step 6899/7134 completed (loss: 0.13860084116458893, acc: 0.9627659320831299)
[2025-02-13 20:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28][root][INFO] - Training Epoch: 1/2, step 6900/7134 completed (loss: 0.3930590748786926, acc: 0.9248554706573486)
[2025-02-13 20:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28][root][INFO] - Training Epoch: 1/2, step 6901/7134 completed (loss: 0.1971861571073532, acc: 0.9567567706108093)
[2025-02-13 20:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28][root][INFO] - Training Epoch: 1/2, step 6902/7134 completed (loss: 0.03280647099018097, acc: 0.9943820238113403)
[2025-02-13 20:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29][root][INFO] - Training Epoch: 1/2, step 6903/7134 completed (loss: 0.08663684874773026, acc: 0.9756097793579102)
[2025-02-13 20:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29][root][INFO] - Training Epoch: 1/2, step 6904/7134 completed (loss: 0.098810113966465, acc: 0.9740932583808899)
[2025-02-13 20:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30][root][INFO] - Training Epoch: 1/2, step 6905/7134 completed (loss: 0.15477992594242096, acc: 0.9552238583564758)
[2025-02-13 20:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30][root][INFO] - Training Epoch: 1/2, step 6906/7134 completed (loss: 0.18985776603221893, acc: 0.970588207244873)
[2025-02-13 20:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30][root][INFO] - Training Epoch: 1/2, step 6907/7134 completed (loss: 0.12715768814086914, acc: 0.9740932583808899)
[2025-02-13 20:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31][root][INFO] - Training Epoch: 1/2, step 6908/7134 completed (loss: 0.16390997171401978, acc: 0.9714285731315613)
[2025-02-13 20:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31][root][INFO] - Training Epoch: 1/2, step 6909/7134 completed (loss: 0.14755721390247345, acc: 0.9672130942344666)
[2025-02-13 20:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31][root][INFO] - Training Epoch: 1/2, step 6910/7134 completed (loss: 0.07364213466644287, acc: 0.9791666865348816)
[2025-02-13 20:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32][root][INFO] - Training Epoch: 1/2, step 6911/7134 completed (loss: 0.3543236553668976, acc: 0.9337016344070435)
[2025-02-13 20:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32][root][INFO] - Training Epoch: 1/2, step 6912/7134 completed (loss: 0.15422190725803375, acc: 0.9707317352294922)
[2025-02-13 20:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33][root][INFO] - Training Epoch: 1/2, step 6913/7134 completed (loss: 0.16207218170166016, acc: 0.9710144996643066)
[2025-02-13 20:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33][root][INFO] - Training Epoch: 1/2, step 6914/7134 completed (loss: 0.32621756196022034, acc: 0.9281045794487)
[2025-02-13 20:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33][root][INFO] - Training Epoch: 1/2, step 6915/7134 completed (loss: 0.1790170818567276, acc: 0.9567567706108093)
[2025-02-13 20:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34][root][INFO] - Training Epoch: 1/2, step 6916/7134 completed (loss: 0.19048915803432465, acc: 0.9512194991111755)
[2025-02-13 20:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34][root][INFO] - Training Epoch: 1/2, step 6917/7134 completed (loss: 0.22633102536201477, acc: 0.9484536051750183)
[2025-02-13 20:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35][root][INFO] - Training Epoch: 1/2, step 6918/7134 completed (loss: 0.12741345167160034, acc: 0.9742268323898315)
[2025-02-13 20:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35][root][INFO] - Training Epoch: 1/2, step 6919/7134 completed (loss: 0.2099463939666748, acc: 0.9593908786773682)
[2025-02-13 20:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35][root][INFO] - Training Epoch: 1/2, step 6920/7134 completed (loss: 0.06307796388864517, acc: 0.976047933101654)
[2025-02-13 20:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36][root][INFO] - Training Epoch: 1/2, step 6921/7134 completed (loss: 0.15783622860908508, acc: 0.9558823704719543)
[2025-02-13 20:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36][root][INFO] - Training Epoch: 1/2, step 6922/7134 completed (loss: 0.20788100361824036, acc: 0.9551281929016113)
[2025-02-13 20:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36][root][INFO] - Training Epoch: 1/2, step 6923/7134 completed (loss: 0.1511783003807068, acc: 0.9561403393745422)
[2025-02-13 20:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:37][root][INFO] - Training Epoch: 1/2, step 6924/7134 completed (loss: 0.11646687239408493, acc: 0.9696969985961914)
[2025-02-13 20:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:37][root][INFO] - Training Epoch: 1/2, step 6925/7134 completed (loss: 0.18095941841602325, acc: 0.9529411792755127)
[2025-02-13 20:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38][root][INFO] - Training Epoch: 1/2, step 6926/7134 completed (loss: 0.23530781269073486, acc: 0.9365079402923584)
[2025-02-13 20:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38][root][INFO] - Training Epoch: 1/2, step 6927/7134 completed (loss: 0.15181830525398254, acc: 0.9702970385551453)
[2025-02-13 20:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38][root][INFO] - Training Epoch: 1/2, step 6928/7134 completed (loss: 0.21014606952667236, acc: 0.9609375)
[2025-02-13 20:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39][root][INFO] - Training Epoch: 1/2, step 6929/7134 completed (loss: 0.09657415747642517, acc: 0.9659090638160706)
[2025-02-13 20:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39][root][INFO] - Training Epoch: 1/2, step 6930/7134 completed (loss: 0.1621045172214508, acc: 0.96875)
[2025-02-13 20:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40][root][INFO] - Training Epoch: 1/2, step 6931/7134 completed (loss: 0.15980730950832367, acc: 0.9470899701118469)
[2025-02-13 20:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40][root][INFO] - Training Epoch: 1/2, step 6932/7134 completed (loss: 0.20708906650543213, acc: 0.9520958065986633)
[2025-02-13 20:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40][root][INFO] - Training Epoch: 1/2, step 6933/7134 completed (loss: 0.23703449964523315, acc: 0.946107804775238)
[2025-02-13 20:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41][root][INFO] - Training Epoch: 1/2, step 6934/7134 completed (loss: 0.19852541387081146, acc: 0.9679487347602844)
[2025-02-13 20:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41][root][INFO] - Training Epoch: 1/2, step 6935/7134 completed (loss: 0.13924385607242584, acc: 0.978723406791687)
[2025-02-13 20:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42][root][INFO] - Training Epoch: 1/2, step 6936/7134 completed (loss: 0.10229835659265518, acc: 0.9784946441650391)
[2025-02-13 20:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42][root][INFO] - Training Epoch: 1/2, step 6937/7134 completed (loss: 0.24219176173210144, acc: 0.9520547986030579)
[2025-02-13 20:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42][root][INFO] - Training Epoch: 1/2, step 6938/7134 completed (loss: 0.547214925289154, acc: 0.8678160905838013)
[2025-02-13 20:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43][root][INFO] - Training Epoch: 1/2, step 6939/7134 completed (loss: 0.2607412338256836, acc: 0.9345238208770752)
[2025-02-13 20:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43][root][INFO] - Training Epoch: 1/2, step 6940/7134 completed (loss: 0.3659331500530243, acc: 0.9034482836723328)
[2025-02-13 20:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44][root][INFO] - Training Epoch: 1/2, step 6941/7134 completed (loss: 0.07748708128929138, acc: 0.9852941036224365)
[2025-02-13 20:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44][root][INFO] - Training Epoch: 1/2, step 6942/7134 completed (loss: 0.22553007304668427, acc: 0.9554139971733093)
[2025-02-13 20:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44][root][INFO] - Training Epoch: 1/2, step 6943/7134 completed (loss: 0.15349435806274414, acc: 0.9292035102844238)
[2025-02-13 20:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:45][root][INFO] - Training Epoch: 1/2, step 6944/7134 completed (loss: 0.16239523887634277, acc: 0.9586206674575806)
[2025-02-13 20:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:45][root][INFO] - Training Epoch: 1/2, step 6945/7134 completed (loss: 0.2568158805370331, acc: 0.96875)
[2025-02-13 20:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46][root][INFO] - Training Epoch: 1/2, step 6946/7134 completed (loss: 0.3537663221359253, acc: 0.9032257795333862)
[2025-02-13 20:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46][root][INFO] - Training Epoch: 1/2, step 6947/7134 completed (loss: 0.23143377900123596, acc: 0.9557521939277649)
[2025-02-13 20:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46][root][INFO] - Training Epoch: 1/2, step 6948/7134 completed (loss: 0.30702799558639526, acc: 0.9142857193946838)
[2025-02-13 20:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:47][root][INFO] - Training Epoch: 1/2, step 6949/7134 completed (loss: 0.22589850425720215, acc: 0.9217391014099121)
[2025-02-13 20:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:47][root][INFO] - Training Epoch: 1/2, step 6950/7134 completed (loss: 0.16290514171123505, acc: 0.9620253443717957)
[2025-02-13 20:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48][root][INFO] - Training Epoch: 1/2, step 6951/7134 completed (loss: 0.2761479616165161, acc: 0.9097222089767456)
[2025-02-13 20:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48][root][INFO] - Training Epoch: 1/2, step 6952/7134 completed (loss: 0.22058755159378052, acc: 0.9238095283508301)
[2025-02-13 20:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48][root][INFO] - Training Epoch: 1/2, step 6953/7134 completed (loss: 0.1968141794204712, acc: 0.9473684430122375)
[2025-02-13 20:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49][root][INFO] - Training Epoch: 1/2, step 6954/7134 completed (loss: 0.3695699870586395, acc: 0.9396551847457886)
[2025-02-13 20:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49][root][INFO] - Training Epoch: 1/2, step 6955/7134 completed (loss: 0.13887837529182434, acc: 0.9375)
[2025-02-13 20:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49][root][INFO] - Training Epoch: 1/2, step 6956/7134 completed (loss: 0.14155270159244537, acc: 0.9635036587715149)
[2025-02-13 20:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50][root][INFO] - Training Epoch: 1/2, step 6957/7134 completed (loss: 0.1581997573375702, acc: 0.9599999785423279)
[2025-02-13 20:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50][root][INFO] - Training Epoch: 1/2, step 6958/7134 completed (loss: 0.3618256449699402, acc: 0.9145299196243286)
[2025-02-13 20:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50][root][INFO] - Training Epoch: 1/2, step 6959/7134 completed (loss: 0.39658859372138977, acc: 0.8990825414657593)
[2025-02-13 20:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:51][root][INFO] - Training Epoch: 1/2, step 6960/7134 completed (loss: 0.20998093485832214, acc: 0.9444444179534912)
[2025-02-13 20:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:51][root][INFO] - Training Epoch: 1/2, step 6961/7134 completed (loss: 0.13807718455791473, acc: 0.9658119678497314)
[2025-02-13 20:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52][root][INFO] - Training Epoch: 1/2, step 6962/7134 completed (loss: 0.17236174643039703, acc: 0.9492753744125366)
[2025-02-13 20:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52][root][INFO] - Training Epoch: 1/2, step 6963/7134 completed (loss: 0.14445513486862183, acc: 0.9668874144554138)
[2025-02-13 20:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52][root][INFO] - Training Epoch: 1/2, step 6964/7134 completed (loss: 0.05541643872857094, acc: 0.9848484992980957)
[2025-02-13 20:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:53][root][INFO] - Training Epoch: 1/2, step 6965/7134 completed (loss: 0.036121778190135956, acc: 1.0)
[2025-02-13 20:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:53][root][INFO] - Training Epoch: 1/2, step 6966/7134 completed (loss: 0.13008421659469604, acc: 0.9523809552192688)
[2025-02-13 20:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54][root][INFO] - Training Epoch: 1/2, step 6967/7134 completed (loss: 0.05772556737065315, acc: 0.9863945841789246)
[2025-02-13 20:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54][root][INFO] - Training Epoch: 1/2, step 6968/7134 completed (loss: 0.11628981679677963, acc: 0.9798657894134521)
[2025-02-13 20:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54][root][INFO] - Training Epoch: 1/2, step 6969/7134 completed (loss: 0.19637219607830048, acc: 0.9516128897666931)
[2025-02-13 20:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55][root][INFO] - Training Epoch: 1/2, step 6970/7134 completed (loss: 0.31363552808761597, acc: 0.9396551847457886)
[2025-02-13 20:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55][root][INFO] - Training Epoch: 1/2, step 6971/7134 completed (loss: 0.24065852165222168, acc: 0.9444444179534912)
[2025-02-13 20:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56][root][INFO] - Training Epoch: 1/2, step 6972/7134 completed (loss: 0.2089245468378067, acc: 0.961240291595459)
[2025-02-13 20:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56][root][INFO] - Training Epoch: 1/2, step 6973/7134 completed (loss: 0.25727203488349915, acc: 0.9741379022598267)
[2025-02-13 20:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56][root][INFO] - Training Epoch: 1/2, step 6974/7134 completed (loss: 0.23702044785022736, acc: 0.9411764740943909)
[2025-02-13 20:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57][root][INFO] - Training Epoch: 1/2, step 6975/7134 completed (loss: 0.23371803760528564, acc: 0.9658119678497314)
[2025-02-13 20:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57][root][INFO] - Training Epoch: 1/2, step 6976/7134 completed (loss: 0.18381650745868683, acc: 0.9571428298950195)
[2025-02-13 20:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58][root][INFO] - Training Epoch: 1/2, step 6977/7134 completed (loss: 0.15539854764938354, acc: 0.9389312863349915)
[2025-02-13 20:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58][root][INFO] - Training Epoch: 1/2, step 6978/7134 completed (loss: 0.12632080912590027, acc: 0.9683544039726257)
[2025-02-13 20:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58][root][INFO] - Training Epoch: 1/2, step 6979/7134 completed (loss: 0.23625223338603973, acc: 0.9271523356437683)
[2025-02-13 20:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59][root][INFO] - Training Epoch: 1/2, step 6980/7134 completed (loss: 0.4634791314601898, acc: 0.8757061958312988)
[2025-02-13 20:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59][root][INFO] - Training Epoch: 1/2, step 6981/7134 completed (loss: 0.3049238622188568, acc: 0.907216489315033)
[2025-02-13 20:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59][root][INFO] - Training Epoch: 1/2, step 6982/7134 completed (loss: 0.32545095682144165, acc: 0.926701545715332)
[2025-02-13 20:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:00][root][INFO] - Training Epoch: 1/2, step 6983/7134 completed (loss: 0.40884697437286377, acc: 0.8846153616905212)
[2025-02-13 20:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:00][root][INFO] - Training Epoch: 1/2, step 6984/7134 completed (loss: 0.2072332501411438, acc: 0.9488372206687927)
[2025-02-13 20:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01][root][INFO] - Training Epoch: 1/2, step 6985/7134 completed (loss: 0.4122457206249237, acc: 0.9141631126403809)
[2025-02-13 20:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01][root][INFO] - Training Epoch: 1/2, step 6986/7134 completed (loss: 0.23269179463386536, acc: 0.9197530746459961)
[2025-02-13 20:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02][root][INFO] - Training Epoch: 1/2, step 6987/7134 completed (loss: 0.20546916127204895, acc: 0.9462810158729553)
[2025-02-13 20:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02][root][INFO] - Training Epoch: 1/2, step 6988/7134 completed (loss: 0.10694966465234756, acc: 0.9842519760131836)
[2025-02-13 20:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02][root][INFO] - Training Epoch: 1/2, step 6989/7134 completed (loss: 0.3354046940803528, acc: 0.9189189076423645)
[2025-02-13 20:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03][root][INFO] - Training Epoch: 1/2, step 6990/7134 completed (loss: 0.05059901252388954, acc: 0.9907407164573669)
[2025-02-13 20:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03][root][INFO] - Training Epoch: 1/2, step 6991/7134 completed (loss: 0.4295251965522766, acc: 0.9207921028137207)
[2025-02-13 20:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03][root][INFO] - Training Epoch: 1/2, step 6992/7134 completed (loss: 0.18683414161205292, acc: 0.9369369149208069)
[2025-02-13 20:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04][root][INFO] - Training Epoch: 1/2, step 6993/7134 completed (loss: 0.26681992411613464, acc: 0.9426751732826233)
[2025-02-13 20:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04][root][INFO] - Training Epoch: 1/2, step 6994/7134 completed (loss: 0.484354704618454, acc: 0.9066666960716248)
[2025-02-13 20:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05][root][INFO] - Training Epoch: 1/2, step 6995/7134 completed (loss: 0.1451362818479538, acc: 0.9418604373931885)
[2025-02-13 20:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05][root][INFO] - Training Epoch: 1/2, step 6996/7134 completed (loss: 0.16931550204753876, acc: 0.931506872177124)
[2025-02-13 20:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05][root][INFO] - Training Epoch: 1/2, step 6997/7134 completed (loss: 0.1454371064901352, acc: 0.9612902998924255)
[2025-02-13 20:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06][root][INFO] - Training Epoch: 1/2, step 6998/7134 completed (loss: 0.3339456617832184, acc: 0.8947368264198303)
[2025-02-13 20:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06][root][INFO] - Training Epoch: 1/2, step 6999/7134 completed (loss: 0.1022295132279396, acc: 0.9491525292396545)
[2025-02-13 20:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06][root][INFO] - Training Epoch: 1/2, step 7000/7134 completed (loss: 0.10930456966161728, acc: 0.9636363387107849)
[2025-02-13 20:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07][root][INFO] - Training Epoch: 1/2, step 7001/7134 completed (loss: 0.3109344244003296, acc: 0.9740259647369385)
[2025-02-13 20:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07][root][INFO] - Training Epoch: 1/2, step 7002/7134 completed (loss: 0.12661141157150269, acc: 0.9663865566253662)
[2025-02-13 20:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07][root][INFO] - Training Epoch: 1/2, step 7003/7134 completed (loss: 0.048305802047252655, acc: 0.9792746305465698)
[2025-02-13 20:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08][root][INFO] - Training Epoch: 1/2, step 7004/7134 completed (loss: 0.15383775532245636, acc: 0.9634146094322205)
[2025-02-13 20:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08][root][INFO] - Training Epoch: 1/2, step 7005/7134 completed (loss: 0.1453232318162918, acc: 0.9698795080184937)
[2025-02-13 20:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09][root][INFO] - Training Epoch: 1/2, step 7006/7134 completed (loss: 0.08623483777046204, acc: 0.9813664555549622)
[2025-02-13 20:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09][root][INFO] - Training Epoch: 1/2, step 7007/7134 completed (loss: 0.16738393902778625, acc: 0.9548386931419373)
[2025-02-13 20:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09][root][INFO] - Training Epoch: 1/2, step 7008/7134 completed (loss: 0.07790037244558334, acc: 0.9834254384040833)
[2025-02-13 20:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10][root][INFO] - Training Epoch: 1/2, step 7009/7134 completed (loss: 0.2672492265701294, acc: 0.9375)
[2025-02-13 20:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10][root][INFO] - Training Epoch: 1/2, step 7010/7134 completed (loss: 0.07033095508813858, acc: 0.9870967864990234)
[2025-02-13 20:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11][root][INFO] - Training Epoch: 1/2, step 7011/7134 completed (loss: 0.20941750705242157, acc: 0.9476439952850342)
[2025-02-13 20:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11][root][INFO] - Training Epoch: 1/2, step 7012/7134 completed (loss: 0.07751496136188507, acc: 0.9879518151283264)
[2025-02-13 20:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11][root][INFO] - Training Epoch: 1/2, step 7013/7134 completed (loss: 0.0731060728430748, acc: 0.987261176109314)
[2025-02-13 20:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12][root][INFO] - Training Epoch: 1/2, step 7014/7134 completed (loss: 0.16580718755722046, acc: 0.9594594836235046)
[2025-02-13 20:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12][root][INFO] - Training Epoch: 1/2, step 7015/7134 completed (loss: 0.15295861661434174, acc: 0.9776119589805603)
[2025-02-13 20:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13][root][INFO] - Training Epoch: 1/2, step 7016/7134 completed (loss: 0.22409889101982117, acc: 0.9487179517745972)
[2025-02-13 20:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13][root][INFO] - Training Epoch: 1/2, step 7017/7134 completed (loss: 0.2741575241088867, acc: 0.9380530714988708)
[2025-02-13 20:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13][root][INFO] - Training Epoch: 1/2, step 7018/7134 completed (loss: 0.29305294156074524, acc: 0.9567901492118835)
[2025-02-13 20:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14][root][INFO] - Training Epoch: 1/2, step 7019/7134 completed (loss: 0.35590025782585144, acc: 0.9430379867553711)
[2025-02-13 20:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14][root][INFO] - Training Epoch: 1/2, step 7020/7134 completed (loss: 0.05357876792550087, acc: 0.9879518151283264)
[2025-02-13 20:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15][root][INFO] - Training Epoch: 1/2, step 7021/7134 completed (loss: 0.14298410713672638, acc: 0.9555555582046509)
[2025-02-13 20:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15][root][INFO] - Training Epoch: 1/2, step 7022/7134 completed (loss: 0.3746229112148285, acc: 0.9027777910232544)
[2025-02-13 20:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15][root][INFO] - Training Epoch: 1/2, step 7023/7134 completed (loss: 0.3610285818576813, acc: 0.891566276550293)
[2025-02-13 20:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:16][root][INFO] - Training Epoch: 1/2, step 7024/7134 completed (loss: 0.21222475171089172, acc: 0.95652174949646)
[2025-02-13 20:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:16][root][INFO] - Training Epoch: 1/2, step 7025/7134 completed (loss: 0.31959283351898193, acc: 0.9047619104385376)
[2025-02-13 20:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17][root][INFO] - Training Epoch: 1/2, step 7026/7134 completed (loss: 0.3959607183933258, acc: 0.90625)
[2025-02-13 20:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17][root][INFO] - Training Epoch: 1/2, step 7027/7134 completed (loss: 0.3245582580566406, acc: 0.9154929518699646)
[2025-02-13 20:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17][root][INFO] - Training Epoch: 1/2, step 7028/7134 completed (loss: 0.13399869203567505, acc: 0.9670329689979553)
[2025-02-13 20:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:18][root][INFO] - Training Epoch: 1/2, step 7029/7134 completed (loss: 0.12502175569534302, acc: 0.966292142868042)
[2025-02-13 20:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:18][root][INFO] - Training Epoch: 1/2, step 7030/7134 completed (loss: 0.2992580533027649, acc: 0.9130434989929199)
[2025-02-13 20:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19][root][INFO] - Training Epoch: 1/2, step 7031/7134 completed (loss: 0.28010621666908264, acc: 0.9382715821266174)
[2025-02-13 20:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19][root][INFO] - Training Epoch: 1/2, step 7032/7134 completed (loss: 0.33420124650001526, acc: 0.9117646813392639)
[2025-02-13 20:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20][root][INFO] - Training Epoch: 1/2, step 7033/7134 completed (loss: 0.4510413706302643, acc: 0.8765432238578796)
[2025-02-13 20:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20][root][INFO] - Training Epoch: 1/2, step 7034/7134 completed (loss: 0.364717572927475, acc: 0.9375)
[2025-02-13 20:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20][root][INFO] - Training Epoch: 1/2, step 7035/7134 completed (loss: 0.35336828231811523, acc: 0.920634925365448)
[2025-02-13 20:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21][root][INFO] - Training Epoch: 1/2, step 7036/7134 completed (loss: 0.34941717982292175, acc: 0.8888888955116272)
[2025-02-13 20:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21][root][INFO] - Training Epoch: 1/2, step 7037/7134 completed (loss: 0.15025436878204346, acc: 0.9701492786407471)
[2025-02-13 20:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21][root][INFO] - Training Epoch: 1/2, step 7038/7134 completed (loss: 0.7512034177780151, acc: 0.8734177350997925)
[2025-02-13 20:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22][root][INFO] - Training Epoch: 1/2, step 7039/7134 completed (loss: 0.19397051632404327, acc: 0.9555555582046509)
[2025-02-13 20:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22][root][INFO] - Training Epoch: 1/2, step 7040/7134 completed (loss: 0.37579962611198425, acc: 0.9200000166893005)
[2025-02-13 20:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23][root][INFO] - Training Epoch: 1/2, step 7041/7134 completed (loss: 0.3973511755466461, acc: 0.887499988079071)
[2025-02-13 20:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23][root][INFO] - Training Epoch: 1/2, step 7042/7134 completed (loss: 0.30695345997810364, acc: 0.9436619877815247)
[2025-02-13 20:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23][root][INFO] - Training Epoch: 1/2, step 7043/7134 completed (loss: 0.46718811988830566, acc: 0.8533333539962769)
[2025-02-13 20:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:24][root][INFO] - Training Epoch: 1/2, step 7044/7134 completed (loss: 0.30323636531829834, acc: 0.9154929518699646)
[2025-02-13 20:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:24][root][INFO] - Training Epoch: 1/2, step 7045/7134 completed (loss: 0.23808766901493073, acc: 0.9448275566101074)
[2025-02-13 20:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25][root][INFO] - Training Epoch: 1/2, step 7046/7134 completed (loss: 0.40480470657348633, acc: 0.916167676448822)
[2025-02-13 20:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25][root][INFO] - Training Epoch: 1/2, step 7047/7134 completed (loss: 0.32737547159194946, acc: 0.9248826503753662)
[2025-02-13 20:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25][root][INFO] - Training Epoch: 1/2, step 7048/7134 completed (loss: 0.04067762568593025, acc: 0.9948453903198242)
[2025-02-13 20:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:26][root][INFO] - Training Epoch: 1/2, step 7049/7134 completed (loss: 0.2327069789171219, acc: 0.9595375657081604)
[2025-02-13 20:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:26][root][INFO] - Training Epoch: 1/2, step 7050/7134 completed (loss: 0.07987676560878754, acc: 0.9828571677207947)
[2025-02-13 20:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27][root][INFO] - Training Epoch: 1/2, step 7051/7134 completed (loss: 0.08896379172801971, acc: 0.9728260636329651)
[2025-02-13 20:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27][root][INFO] - Training Epoch: 1/2, step 7052/7134 completed (loss: 0.09697645157575607, acc: 0.9768785834312439)
[2025-02-13 20:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27][root][INFO] - Training Epoch: 1/2, step 7053/7134 completed (loss: 0.13305017352104187, acc: 0.9672130942344666)
[2025-02-13 20:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28][root][INFO] - Training Epoch: 1/2, step 7054/7134 completed (loss: 0.2286531776189804, acc: 0.9548386931419373)
[2025-02-13 20:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28][root][INFO] - Training Epoch: 1/2, step 7055/7134 completed (loss: 0.19510500133037567, acc: 0.9560439586639404)
[2025-02-13 20:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28][root][INFO] - Training Epoch: 1/2, step 7056/7134 completed (loss: 0.18383005261421204, acc: 0.9611111283302307)
[2025-02-13 20:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29][root][INFO] - Training Epoch: 1/2, step 7057/7134 completed (loss: 0.5876258015632629, acc: 0.9171270728111267)
[2025-02-13 20:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29][root][INFO] - Training Epoch: 1/2, step 7058/7134 completed (loss: 0.26071324944496155, acc: 0.930232584476471)
[2025-02-13 20:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30][root][INFO] - Training Epoch: 1/2, step 7059/7134 completed (loss: 0.1389109492301941, acc: 0.9615384340286255)
[2025-02-13 20:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30][root][INFO] - Training Epoch: 1/2, step 7060/7134 completed (loss: 0.2653666138648987, acc: 0.9436619877815247)
[2025-02-13 20:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30][root][INFO] - Training Epoch: 1/2, step 7061/7134 completed (loss: 0.21295678615570068, acc: 0.9354838728904724)
[2025-02-13 20:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31][root][INFO] - Training Epoch: 1/2, step 7062/7134 completed (loss: 0.17999637126922607, acc: 0.9491525292396545)
[2025-02-13 20:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31][root][INFO] - Training Epoch: 1/2, step 7063/7134 completed (loss: 0.3767143189907074, acc: 0.929729700088501)
[2025-02-13 20:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31][root][INFO] - Training Epoch: 1/2, step 7064/7134 completed (loss: 0.21656575798988342, acc: 0.9578313231468201)
[2025-02-13 20:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32][root][INFO] - Training Epoch: 1/2, step 7065/7134 completed (loss: 0.11945406347513199, acc: 0.9848484992980957)
[2025-02-13 20:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32][root][INFO] - Training Epoch: 1/2, step 7066/7134 completed (loss: 0.17548593878746033, acc: 0.9528796076774597)
[2025-02-13 20:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33][root][INFO] - Training Epoch: 1/2, step 7067/7134 completed (loss: 0.18190410733222961, acc: 0.9576719403266907)
[2025-02-13 20:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33][root][INFO] - Training Epoch: 1/2, step 7068/7134 completed (loss: 0.0433591790497303, acc: 0.995121955871582)
[2025-02-13 20:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33][root][INFO] - Training Epoch: 1/2, step 7069/7134 completed (loss: 0.11255253851413727, acc: 0.9870129823684692)
[2025-02-13 20:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34][root][INFO] - Training Epoch: 1/2, step 7070/7134 completed (loss: 0.10194813460111618, acc: 0.9710982441902161)
[2025-02-13 20:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34][root][INFO] - Training Epoch: 1/2, step 7071/7134 completed (loss: 0.1317615807056427, acc: 0.9747899174690247)
[2025-02-13 20:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34][root][INFO] - Training Epoch: 1/2, step 7072/7134 completed (loss: 0.1042301207780838, acc: 0.9595959782600403)
[2025-02-13 20:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35][root][INFO] - Training Epoch: 1/2, step 7073/7134 completed (loss: 0.13965262472629547, acc: 0.9597989916801453)
[2025-02-13 20:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35][root][INFO] - Training Epoch: 1/2, step 7074/7134 completed (loss: 0.43011683225631714, acc: 0.8943089246749878)
[2025-02-13 20:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36][root][INFO] - Training Epoch: 1/2, step 7075/7134 completed (loss: 0.26933568716049194, acc: 0.9363057613372803)
[2025-02-13 20:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36][root][INFO] - Training Epoch: 1/2, step 7076/7134 completed (loss: 0.33750659227371216, acc: 0.9213483333587646)
[2025-02-13 20:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36][root][INFO] - Training Epoch: 1/2, step 7077/7134 completed (loss: 0.3687364161014557, acc: 0.9109588861465454)
[2025-02-13 20:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37][root][INFO] - Training Epoch: 1/2, step 7078/7134 completed (loss: 0.1882932335138321, acc: 0.9724137783050537)
[2025-02-13 20:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37][root][INFO] - Training Epoch: 1/2, step 7079/7134 completed (loss: 0.12635751068592072, acc: 0.9605262875556946)
[2025-02-13 20:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37][root][INFO] - Training Epoch: 1/2, step 7080/7134 completed (loss: 0.24069911241531372, acc: 0.9319728016853333)
[2025-02-13 20:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:38][root][INFO] - Training Epoch: 1/2, step 7081/7134 completed (loss: 0.1313670426607132, acc: 0.9595375657081604)
[2025-02-13 20:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:38][root][INFO] - Training Epoch: 1/2, step 7082/7134 completed (loss: 0.3218110501766205, acc: 0.9276315569877625)
[2025-02-13 20:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39][root][INFO] - Training Epoch: 1/2, step 7083/7134 completed (loss: 0.09031042456626892, acc: 0.9716312289237976)
[2025-02-13 20:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39][root][INFO] - Training Epoch: 1/2, step 7084/7134 completed (loss: 0.14825038611888885, acc: 0.954023003578186)
[2025-02-13 20:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39][root][INFO] - Training Epoch: 1/2, step 7085/7134 completed (loss: 0.2287401258945465, acc: 0.9466666579246521)
[2025-02-13 20:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40][root][INFO] - Training Epoch: 1/2, step 7086/7134 completed (loss: 0.12732180953025818, acc: 0.9741935729980469)
[2025-02-13 20:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40][root][INFO] - Training Epoch: 1/2, step 7087/7134 completed (loss: 0.19883906841278076, acc: 0.9534883499145508)
[2025-02-13 20:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40][root][INFO] - Training Epoch: 1/2, step 7088/7134 completed (loss: 0.21013565361499786, acc: 0.9503546357154846)
[2025-02-13 20:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:41][root][INFO] - Training Epoch: 1/2, step 7089/7134 completed (loss: 0.2486906796693802, acc: 0.9419354796409607)
[2025-02-13 20:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:41][root][INFO] - Training Epoch: 1/2, step 7090/7134 completed (loss: 0.15504728257656097, acc: 0.9702380895614624)
[2025-02-13 20:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42][root][INFO] - Training Epoch: 1/2, step 7091/7134 completed (loss: 0.1142946183681488, acc: 0.9575757384300232)
[2025-02-13 20:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42][root][INFO] - Training Epoch: 1/2, step 7092/7134 completed (loss: 0.17055000364780426, acc: 0.9548872113227844)
[2025-02-13 20:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42][root][INFO] - Training Epoch: 1/2, step 7093/7134 completed (loss: 0.26846811175346375, acc: 0.8888888955116272)
[2025-02-13 20:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43][root][INFO] - Training Epoch: 1/2, step 7094/7134 completed (loss: 0.12332862615585327, acc: 0.9791666865348816)
[2025-02-13 20:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43][root][INFO] - Training Epoch: 1/2, step 7095/7134 completed (loss: 0.18938478827476501, acc: 0.9406779408454895)
[2025-02-13 20:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43][root][INFO] - Training Epoch: 1/2, step 7096/7134 completed (loss: 0.15372344851493835, acc: 0.9647887349128723)
[2025-02-13 20:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44][root][INFO] - Training Epoch: 1/2, step 7097/7134 completed (loss: 0.24764786660671234, acc: 0.9166666865348816)
[2025-02-13 20:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44][root][INFO] - Training Epoch: 1/2, step 7098/7134 completed (loss: 0.09038130193948746, acc: 0.9790209531784058)
[2025-02-13 20:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45][root][INFO] - Training Epoch: 1/2, step 7099/7134 completed (loss: 0.2245844006538391, acc: 0.9347826242446899)
[2025-02-13 20:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45][root][INFO] - Training Epoch: 1/2, step 7100/7134 completed (loss: 0.186005637049675, acc: 0.9520958065986633)
[2025-02-13 20:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45][root][INFO] - Training Epoch: 1/2, step 7101/7134 completed (loss: 0.08961964398622513, acc: 0.9856114983558655)
[2025-02-13 20:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46][root][INFO] - Training Epoch: 1/2, step 7102/7134 completed (loss: 0.16599304974079132, acc: 0.9576271176338196)
[2025-02-13 20:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46][root][INFO] - Training Epoch: 1/2, step 7103/7134 completed (loss: 0.13297554850578308, acc: 0.96875)
[2025-02-13 20:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46][root][INFO] - Training Epoch: 1/2, step 7104/7134 completed (loss: 0.10843916237354279, acc: 0.9649122953414917)
[2025-02-13 20:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47][root][INFO] - Training Epoch: 1/2, step 7105/7134 completed (loss: 0.10548266768455505, acc: 0.9791666865348816)
[2025-02-13 20:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47][root][INFO] - Training Epoch: 1/2, step 7106/7134 completed (loss: 0.05004303902387619, acc: 0.9851852059364319)
[2025-02-13 20:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48][root][INFO] - Training Epoch: 1/2, step 7107/7134 completed (loss: 0.10854891687631607, acc: 0.9689440727233887)
[2025-02-13 20:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48][root][INFO] - Training Epoch: 1/2, step 7108/7134 completed (loss: 0.13667058944702148, acc: 0.9635036587715149)
[2025-02-13 20:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48][root][INFO] - Training Epoch: 1/2, step 7109/7134 completed (loss: 0.27505314350128174, acc: 0.9037036895751953)
[2025-02-13 20:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49][root][INFO] - Training Epoch: 1/2, step 7110/7134 completed (loss: 0.05604492127895355, acc: 0.9924812316894531)
[2025-02-13 20:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49][root][INFO] - Training Epoch: 1/2, step 7111/7134 completed (loss: 0.21579761803150177, acc: 0.9351851940155029)
[2025-02-13 20:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49][root][INFO] - Training Epoch: 1/2, step 7112/7134 completed (loss: 0.16258199512958527, acc: 0.954954981803894)
[2025-02-13 20:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:50][root][INFO] - Training Epoch: 1/2, step 7113/7134 completed (loss: 0.3016678988933563, acc: 0.9299362897872925)
[2025-02-13 20:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:50][root][INFO] - Training Epoch: 1/2, step 7114/7134 completed (loss: 0.1667231321334839, acc: 0.9541284441947937)
[2025-02-13 20:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51][root][INFO] - Training Epoch: 1/2, step 7115/7134 completed (loss: 0.23390521109104156, acc: 0.9160305261611938)
[2025-02-13 20:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51][root][INFO] - Training Epoch: 1/2, step 7116/7134 completed (loss: 0.34358254075050354, acc: 0.9117646813392639)
[2025-02-13 20:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51][root][INFO] - Training Epoch: 1/2, step 7117/7134 completed (loss: 0.2553677260875702, acc: 0.9448819160461426)
[2025-02-13 20:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52][root][INFO] - Training Epoch: 1/2, step 7118/7134 completed (loss: 0.16748853027820587, acc: 0.9642857313156128)
[2025-02-13 20:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52][root][INFO] - Training Epoch: 1/2, step 7119/7134 completed (loss: 0.1332518756389618, acc: 0.9586206674575806)
[2025-02-13 20:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52][root][INFO] - Training Epoch: 1/2, step 7120/7134 completed (loss: 0.18437954783439636, acc: 0.9514563083648682)
[2025-02-13 20:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53][root][INFO] - Training Epoch: 1/2, step 7121/7134 completed (loss: 0.0345289520919323, acc: 1.0)
[2025-02-13 20:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53][root][INFO] - Training Epoch: 1/2, step 7122/7134 completed (loss: 0.17728954553604126, acc: 0.9569892287254333)
[2025-02-13 20:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54][root][INFO] - Training Epoch: 1/2, step 7123/7134 completed (loss: 0.11641890555620193, acc: 0.9595959782600403)
[2025-02-13 20:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54][root][INFO] - Training Epoch: 1/2, step 7124/7134 completed (loss: 0.16556371748447418, acc: 0.9672130942344666)
[2025-02-13 20:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54][root][INFO] - Training Epoch: 1/2, step 7125/7134 completed (loss: 0.08133949339389801, acc: 0.9694656729698181)
[2025-02-13 20:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:55][root][INFO] - Training Epoch: 1/2, step 7126/7134 completed (loss: 0.31359240412712097, acc: 0.9051094651222229)
[2025-02-13 20:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:55][root][INFO] - Training Epoch: 1/2, step 7127/7134 completed (loss: 0.1431387960910797, acc: 0.9506173133850098)
[2025-02-13 20:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56][root][INFO] - Training Epoch: 1/2, step 7128/7134 completed (loss: 0.1460019201040268, acc: 0.9589040875434875)
[2025-02-13 20:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56][root][INFO] - Training Epoch: 1/2, step 7129/7134 completed (loss: 0.49487948417663574, acc: 0.8682170510292053)
[2025-02-13 20:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56][root][INFO] - Training Epoch: 1/2, step 7130/7134 completed (loss: 0.8376147150993347, acc: 0.8199999928474426)
[2025-02-13 20:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:57][root][INFO] - Training Epoch: 1/2, step 7131/7134 completed (loss: 0.22695739567279816, acc: 0.9433962106704712)
[2025-02-13 20:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:48][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2487, device='cuda:0') eval_epoch_loss=tensor(0.2221, device='cuda:0') eval_epoch_acc=tensor(0.9471, device='cuda:0')
[2025-02-13 20:19:48][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:19:48][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:19:48][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_7132_loss_0.22211353480815887/model.pt
[2025-02-13 20:19:48][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:19:48][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.22211353480815887
[2025-02-13 20:19:48][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9471306204795837
[2025-02-13 20:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:49][root][INFO] - Training Epoch: 1/2, step 7132/7134 completed (loss: 0.07953780889511108, acc: 0.9852941036224365)
[2025-02-13 20:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:49][root][INFO] - Training Epoch: 1/2, step 7133/7134 completed (loss: 0.16313418745994568, acc: 0.9624060392379761)
[2025-02-13 20:19:49][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=1.3058, train_epoch_loss=0.2668, epoch time 3655.8783842939883s
[2025-02-13 20:19:49][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-13 20:19:49][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 21 GB
[2025-02-13 20:19:49][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-13 20:19:49][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-13 20:19:49][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-13 20:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51][root][INFO] - Training Epoch: 2/2, step 0/7134 completed (loss: 0.20229710638523102, acc: 0.9551281929016113)
[2025-02-13 20:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51][root][INFO] - Training Epoch: 2/2, step 1/7134 completed (loss: 0.16660600900650024, acc: 0.9617834687232971)
[2025-02-13 20:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51][root][INFO] - Training Epoch: 2/2, step 2/7134 completed (loss: 0.07281479984521866, acc: 0.9772727489471436)
[2025-02-13 20:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:52][root][INFO] - Training Epoch: 2/2, step 3/7134 completed (loss: 0.1317954808473587, acc: 0.9593023061752319)
[2025-02-13 20:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:52][root][INFO] - Training Epoch: 2/2, step 4/7134 completed (loss: 0.0946822464466095, acc: 0.9811320900917053)
[2025-02-13 20:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53][root][INFO] - Training Epoch: 2/2, step 5/7134 completed (loss: 0.09116395562887192, acc: 0.9664804339408875)
[2025-02-13 20:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53][root][INFO] - Training Epoch: 2/2, step 6/7134 completed (loss: 0.03811586648225784, acc: 0.9929577708244324)
[2025-02-13 20:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53][root][INFO] - Training Epoch: 2/2, step 7/7134 completed (loss: 0.08625024557113647, acc: 0.9838709831237793)
[2025-02-13 20:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54][root][INFO] - Training Epoch: 2/2, step 8/7134 completed (loss: 0.188359797000885, acc: 0.9512194991111755)
[2025-02-13 20:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54][root][INFO] - Training Epoch: 2/2, step 9/7134 completed (loss: 0.061296142637729645, acc: 0.9867549538612366)
[2025-02-13 20:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55][root][INFO] - Training Epoch: 2/2, step 10/7134 completed (loss: 0.15667468309402466, acc: 0.9467455744743347)
[2025-02-13 20:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55][root][INFO] - Training Epoch: 2/2, step 11/7134 completed (loss: 0.1071561798453331, acc: 0.9583333134651184)
[2025-02-13 20:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55][root][INFO] - Training Epoch: 2/2, step 12/7134 completed (loss: 0.0667104423046112, acc: 0.9942196607589722)
[2025-02-13 20:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:56][root][INFO] - Training Epoch: 2/2, step 13/7134 completed (loss: 0.08947067707777023, acc: 0.9719101190567017)
[2025-02-13 20:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:56][root][INFO] - Training Epoch: 2/2, step 14/7134 completed (loss: 0.021239086985588074, acc: 1.0)
[2025-02-13 20:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57][root][INFO] - Training Epoch: 2/2, step 15/7134 completed (loss: 0.032532576471567154, acc: 0.9927536249160767)
[2025-02-13 20:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57][root][INFO] - Training Epoch: 2/2, step 16/7134 completed (loss: 0.007996313273906708, acc: 1.0)
[2025-02-13 20:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57][root][INFO] - Training Epoch: 2/2, step 17/7134 completed (loss: 0.15283049643039703, acc: 0.9680851101875305)
[2025-02-13 20:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:58][root][INFO] - Training Epoch: 2/2, step 18/7134 completed (loss: 0.017766354605555534, acc: 1.0)
[2025-02-13 20:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:58][root][INFO] - Training Epoch: 2/2, step 19/7134 completed (loss: 0.08292397111654282, acc: 0.9710982441902161)
[2025-02-13 20:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59][root][INFO] - Training Epoch: 2/2, step 20/7134 completed (loss: 0.02908077836036682, acc: 0.9886363744735718)
[2025-02-13 20:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59][root][INFO] - Training Epoch: 2/2, step 21/7134 completed (loss: 0.10734985023736954, acc: 0.9772727489471436)
[2025-02-13 20:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59][root][INFO] - Training Epoch: 2/2, step 22/7134 completed (loss: 0.007793718948960304, acc: 1.0)
[2025-02-13 20:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00][root][INFO] - Training Epoch: 2/2, step 23/7134 completed (loss: 0.03504405543208122, acc: 0.994350254535675)
[2025-02-13 20:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00][root][INFO] - Training Epoch: 2/2, step 24/7134 completed (loss: 0.02595541812479496, acc: 0.9944444298744202)
[2025-02-13 20:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01][root][INFO] - Training Epoch: 2/2, step 25/7134 completed (loss: 0.10125169157981873, acc: 0.9704142212867737)
[2025-02-13 20:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01][root][INFO] - Training Epoch: 2/2, step 26/7134 completed (loss: 0.12907518446445465, acc: 0.9693251252174377)
[2025-02-13 20:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01][root][INFO] - Training Epoch: 2/2, step 27/7134 completed (loss: 0.020123908296227455, acc: 0.9935897588729858)
[2025-02-13 20:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:02][root][INFO] - Training Epoch: 2/2, step 28/7134 completed (loss: 0.21963593363761902, acc: 0.9543147087097168)
[2025-02-13 20:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:02][root][INFO] - Training Epoch: 2/2, step 29/7134 completed (loss: 0.15388809144496918, acc: 0.976331353187561)
[2025-02-13 20:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03][root][INFO] - Training Epoch: 2/2, step 30/7134 completed (loss: 0.11439217627048492, acc: 0.9758453965187073)
[2025-02-13 20:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03][root][INFO] - Training Epoch: 2/2, step 31/7134 completed (loss: 0.18922898173332214, acc: 0.9627906680107117)
[2025-02-13 20:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03][root][INFO] - Training Epoch: 2/2, step 32/7134 completed (loss: 0.2827909290790558, acc: 0.9402173757553101)
[2025-02-13 20:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:04][root][INFO] - Training Epoch: 2/2, step 33/7134 completed (loss: 0.23664623498916626, acc: 0.9538461565971375)
[2025-02-13 20:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:04][root][INFO] - Training Epoch: 2/2, step 34/7134 completed (loss: 0.11896858364343643, acc: 0.9714285731315613)
[2025-02-13 20:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05][root][INFO] - Training Epoch: 2/2, step 35/7134 completed (loss: 0.14877000451087952, acc: 0.9686274528503418)
[2025-02-13 20:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05][root][INFO] - Training Epoch: 2/2, step 36/7134 completed (loss: 0.18825267255306244, acc: 0.9677419066429138)
[2025-02-13 20:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05][root][INFO] - Training Epoch: 2/2, step 37/7134 completed (loss: 0.16478772461414337, acc: 0.9629629850387573)
[2025-02-13 20:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:06][root][INFO] - Training Epoch: 2/2, step 38/7134 completed (loss: 0.12380462884902954, acc: 0.9685534834861755)
[2025-02-13 20:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:06][root][INFO] - Training Epoch: 2/2, step 39/7134 completed (loss: 0.07964816689491272, acc: 0.9766082167625427)
[2025-02-13 20:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07][root][INFO] - Training Epoch: 2/2, step 40/7134 completed (loss: 0.13247466087341309, acc: 0.9910714030265808)
[2025-02-13 20:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07][root][INFO] - Training Epoch: 2/2, step 41/7134 completed (loss: 0.14796443283557892, acc: 0.9734042286872864)
[2025-02-13 20:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07][root][INFO] - Training Epoch: 2/2, step 42/7134 completed (loss: 0.1030639037489891, acc: 0.9876543283462524)
[2025-02-13 20:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08][root][INFO] - Training Epoch: 2/2, step 43/7134 completed (loss: 0.03267728537321091, acc: 0.994413435459137)
[2025-02-13 20:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08][root][INFO] - Training Epoch: 2/2, step 44/7134 completed (loss: 0.11893368512392044, acc: 0.9679144620895386)
[2025-02-13 20:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09][root][INFO] - Training Epoch: 2/2, step 45/7134 completed (loss: 0.12160108983516693, acc: 0.9811320900917053)
[2025-02-13 20:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09][root][INFO] - Training Epoch: 2/2, step 46/7134 completed (loss: 0.11609154939651489, acc: 0.9651162624359131)
[2025-02-13 20:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09][root][INFO] - Training Epoch: 2/2, step 47/7134 completed (loss: 0.12408925592899323, acc: 0.9664804339408875)
[2025-02-13 20:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10][root][INFO] - Training Epoch: 2/2, step 48/7134 completed (loss: 0.10483172535896301, acc: 0.9758453965187073)
[2025-02-13 20:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10][root][INFO] - Training Epoch: 2/2, step 49/7134 completed (loss: 0.14522621035575867, acc: 0.9823529124259949)
[2025-02-13 20:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11][root][INFO] - Training Epoch: 2/2, step 50/7134 completed (loss: 0.07842987775802612, acc: 0.9805825352668762)
[2025-02-13 20:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11][root][INFO] - Training Epoch: 2/2, step 51/7134 completed (loss: 0.09639421105384827, acc: 0.9744898080825806)
[2025-02-13 20:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12][root][INFO] - Training Epoch: 2/2, step 52/7134 completed (loss: 0.05655250325798988, acc: 0.9801324605941772)
[2025-02-13 20:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12][root][INFO] - Training Epoch: 2/2, step 53/7134 completed (loss: 0.05358165502548218, acc: 0.9783783555030823)
[2025-02-13 20:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12][root][INFO] - Training Epoch: 2/2, step 54/7134 completed (loss: 0.08659154921770096, acc: 0.976190447807312)
[2025-02-13 20:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13][root][INFO] - Training Epoch: 2/2, step 55/7134 completed (loss: 0.18999522924423218, acc: 0.9510869383811951)
[2025-02-13 20:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13][root][INFO] - Training Epoch: 2/2, step 56/7134 completed (loss: 0.03951753303408623, acc: 0.9846153855323792)
[2025-02-13 20:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13][root][INFO] - Training Epoch: 2/2, step 57/7134 completed (loss: 0.231705904006958, acc: 0.9505494236946106)
[2025-02-13 20:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:14][root][INFO] - Training Epoch: 2/2, step 58/7134 completed (loss: 0.17533542215824127, acc: 0.9497206807136536)
[2025-02-13 20:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:14][root][INFO] - Training Epoch: 2/2, step 59/7134 completed (loss: 0.07076536118984222, acc: 0.9839572310447693)
[2025-02-13 20:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15][root][INFO] - Training Epoch: 2/2, step 60/7134 completed (loss: 0.14509348571300507, acc: 0.9620253443717957)
[2025-02-13 20:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15][root][INFO] - Training Epoch: 2/2, step 61/7134 completed (loss: 0.13836698234081268, acc: 0.9664804339408875)
[2025-02-13 20:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16][root][INFO] - Training Epoch: 2/2, step 62/7134 completed (loss: 0.24209706485271454, acc: 0.9550561904907227)
[2025-02-13 20:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16][root][INFO] - Training Epoch: 2/2, step 63/7134 completed (loss: 0.23532569408416748, acc: 0.9450549483299255)
[2025-02-13 20:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16][root][INFO] - Training Epoch: 2/2, step 64/7134 completed (loss: 0.1805747151374817, acc: 0.9529411792755127)
[2025-02-13 20:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17][root][INFO] - Training Epoch: 2/2, step 65/7134 completed (loss: 0.27228718996047974, acc: 0.914893627166748)
[2025-02-13 20:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17][root][INFO] - Training Epoch: 2/2, step 66/7134 completed (loss: 0.18304301798343658, acc: 0.9242424368858337)
[2025-02-13 20:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18][root][INFO] - Training Epoch: 2/2, step 67/7134 completed (loss: 0.24954254925251007, acc: 0.9252873659133911)
[2025-02-13 20:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18][root][INFO] - Training Epoch: 2/2, step 68/7134 completed (loss: 0.24231117963790894, acc: 0.9382022619247437)
[2025-02-13 20:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18][root][INFO] - Training Epoch: 2/2, step 69/7134 completed (loss: 0.19889602065086365, acc: 0.9629629850387573)
[2025-02-13 20:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19][root][INFO] - Training Epoch: 2/2, step 70/7134 completed (loss: 0.2041080892086029, acc: 0.953125)
[2025-02-13 20:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19][root][INFO] - Training Epoch: 2/2, step 71/7134 completed (loss: 0.11758624017238617, acc: 0.9757575988769531)
[2025-02-13 20:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19][root][INFO] - Training Epoch: 2/2, step 72/7134 completed (loss: 0.10744491964578629, acc: 0.9644970297813416)
[2025-02-13 20:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20][root][INFO] - Training Epoch: 2/2, step 73/7134 completed (loss: 0.09002699702978134, acc: 0.9826589822769165)
[2025-02-13 20:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20][root][INFO] - Training Epoch: 2/2, step 74/7134 completed (loss: 0.05570494756102562, acc: 0.9825581312179565)
[2025-02-13 20:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21][root][INFO] - Training Epoch: 2/2, step 75/7134 completed (loss: 0.2846885919570923, acc: 0.9345238208770752)
[2025-02-13 20:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21][root][INFO] - Training Epoch: 2/2, step 76/7134 completed (loss: 0.1591901332139969, acc: 0.9619565010070801)
[2025-02-13 20:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21][root][INFO] - Training Epoch: 2/2, step 77/7134 completed (loss: 0.16351261734962463, acc: 0.9613259434700012)
[2025-02-13 20:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22][root][INFO] - Training Epoch: 2/2, step 78/7134 completed (loss: 0.29105669260025024, acc: 0.8940397500991821)
[2025-02-13 20:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22][root][INFO] - Training Epoch: 2/2, step 79/7134 completed (loss: 0.30412840843200684, acc: 0.9266666769981384)
[2025-02-13 20:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23][root][INFO] - Training Epoch: 2/2, step 80/7134 completed (loss: 0.09651513397693634, acc: 0.983146071434021)
[2025-02-13 20:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23][root][INFO] - Training Epoch: 2/2, step 81/7134 completed (loss: 0.15319068729877472, acc: 0.9576719403266907)
[2025-02-13 20:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23][root][INFO] - Training Epoch: 2/2, step 82/7134 completed (loss: 0.1824571043252945, acc: 0.9518072009086609)
[2025-02-13 20:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24][root][INFO] - Training Epoch: 2/2, step 83/7134 completed (loss: 0.1437956988811493, acc: 0.9734042286872864)
[2025-02-13 20:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24][root][INFO] - Training Epoch: 2/2, step 84/7134 completed (loss: 0.19759786128997803, acc: 0.9640287756919861)
[2025-02-13 20:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25][root][INFO] - Training Epoch: 2/2, step 85/7134 completed (loss: 0.19264942407608032, acc: 0.949999988079071)
[2025-02-13 20:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25][root][INFO] - Training Epoch: 2/2, step 86/7134 completed (loss: 0.29561641812324524, acc: 0.9285714030265808)
[2025-02-13 20:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26][root][INFO] - Training Epoch: 2/2, step 87/7134 completed (loss: 0.31957516074180603, acc: 0.9220778942108154)
[2025-02-13 20:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26][root][INFO] - Training Epoch: 2/2, step 88/7134 completed (loss: 0.24388492107391357, acc: 0.9397590160369873)
[2025-02-13 20:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26][root][INFO] - Training Epoch: 2/2, step 89/7134 completed (loss: 0.20891651511192322, acc: 0.9485714435577393)
[2025-02-13 20:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27][root][INFO] - Training Epoch: 2/2, step 90/7134 completed (loss: 0.13181182742118835, acc: 0.9579831957817078)
[2025-02-13 20:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27][root][INFO] - Training Epoch: 2/2, step 91/7134 completed (loss: 0.2438657283782959, acc: 0.9256198406219482)
[2025-02-13 20:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28][root][INFO] - Training Epoch: 2/2, step 92/7134 completed (loss: 0.16669484972953796, acc: 0.9610389471054077)
[2025-02-13 20:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28][root][INFO] - Training Epoch: 2/2, step 93/7134 completed (loss: 0.17565907537937164, acc: 0.9657142758369446)
[2025-02-13 20:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28][root][INFO] - Training Epoch: 2/2, step 94/7134 completed (loss: 0.2608449459075928, acc: 0.9433962106704712)
[2025-02-13 20:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29][root][INFO] - Training Epoch: 2/2, step 95/7134 completed (loss: 0.09774753451347351, acc: 0.9695122241973877)
[2025-02-13 20:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29][root][INFO] - Training Epoch: 2/2, step 96/7134 completed (loss: 0.0705566480755806, acc: 0.9906542301177979)
[2025-02-13 20:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29][root][INFO] - Training Epoch: 2/2, step 97/7134 completed (loss: 0.9729387164115906, acc: 0.8333333134651184)
[2025-02-13 20:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:30][root][INFO] - Training Epoch: 2/2, step 98/7134 completed (loss: 0.28706082701683044, acc: 0.908450722694397)
[2025-02-13 20:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:30][root][INFO] - Training Epoch: 2/2, step 99/7134 completed (loss: 0.19056664407253265, acc: 0.9461538195610046)
[2025-02-13 20:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31][root][INFO] - Training Epoch: 2/2, step 100/7134 completed (loss: 0.2598534822463989, acc: 0.950276255607605)
[2025-02-13 20:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31][root][INFO] - Training Epoch: 2/2, step 101/7134 completed (loss: 0.2806164026260376, acc: 0.9296875)
[2025-02-13 20:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31][root][INFO] - Training Epoch: 2/2, step 102/7134 completed (loss: 0.061524733901023865, acc: 0.9836065769195557)
[2025-02-13 20:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:32][root][INFO] - Training Epoch: 2/2, step 103/7134 completed (loss: 0.21833616495132446, acc: 0.9589040875434875)
[2025-02-13 20:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:32][root][INFO] - Training Epoch: 2/2, step 104/7134 completed (loss: 0.1021207943558693, acc: 0.9866666793823242)
[2025-02-13 20:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33][root][INFO] - Training Epoch: 2/2, step 105/7134 completed (loss: 0.05799730867147446, acc: 0.9776119589805603)
[2025-02-13 20:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33][root][INFO] - Training Epoch: 2/2, step 106/7134 completed (loss: 0.09441617131233215, acc: 0.9751552939414978)
[2025-02-13 20:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33][root][INFO] - Training Epoch: 2/2, step 107/7134 completed (loss: 0.4598120450973511, acc: 0.8662790656089783)
[2025-02-13 20:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34][root][INFO] - Training Epoch: 2/2, step 108/7134 completed (loss: 0.19524258375167847, acc: 0.9513513445854187)
[2025-02-13 20:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34][root][INFO] - Training Epoch: 2/2, step 109/7134 completed (loss: 0.19152767956256866, acc: 0.9603174328804016)
[2025-02-13 20:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34][root][INFO] - Training Epoch: 2/2, step 110/7134 completed (loss: 0.1456291228532791, acc: 0.9553072452545166)
[2025-02-13 20:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35][root][INFO] - Training Epoch: 2/2, step 111/7134 completed (loss: 0.2719128429889679, acc: 0.939393937587738)
[2025-02-13 20:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35][root][INFO] - Training Epoch: 2/2, step 112/7134 completed (loss: 0.19039510190486908, acc: 0.9375)
[2025-02-13 20:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36][root][INFO] - Training Epoch: 2/2, step 113/7134 completed (loss: 0.0890955850481987, acc: 0.9664804339408875)
[2025-02-13 20:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36][root][INFO] - Training Epoch: 2/2, step 114/7134 completed (loss: 0.16754688322544098, acc: 0.9428571462631226)
[2025-02-13 20:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36][root][INFO] - Training Epoch: 2/2, step 115/7134 completed (loss: 0.3650292456150055, acc: 0.9009901285171509)
[2025-02-13 20:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37][root][INFO] - Training Epoch: 2/2, step 116/7134 completed (loss: 0.14974169433116913, acc: 0.959770143032074)
[2025-02-13 20:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37][root][INFO] - Training Epoch: 2/2, step 117/7134 completed (loss: 0.21104063093662262, acc: 0.9454545378684998)
[2025-02-13 20:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38][root][INFO] - Training Epoch: 2/2, step 118/7134 completed (loss: 0.23987196385860443, acc: 0.9388889074325562)
[2025-02-13 20:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38][root][INFO] - Training Epoch: 2/2, step 119/7134 completed (loss: 0.17315280437469482, acc: 0.9414893388748169)
[2025-02-13 20:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38][root][INFO] - Training Epoch: 2/2, step 120/7134 completed (loss: 0.0953308492898941, acc: 0.9702380895614624)
[2025-02-13 20:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:39][root][INFO] - Training Epoch: 2/2, step 121/7134 completed (loss: 0.12422901391983032, acc: 0.9608938694000244)
[2025-02-13 20:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:39][root][INFO] - Training Epoch: 2/2, step 122/7134 completed (loss: 0.1073177382349968, acc: 0.9780219793319702)
[2025-02-13 20:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40][root][INFO] - Training Epoch: 2/2, step 123/7134 completed (loss: 0.211453378200531, acc: 0.9622641801834106)
[2025-02-13 20:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40][root][INFO] - Training Epoch: 2/2, step 124/7134 completed (loss: 0.19111473858356476, acc: 0.9583333134651184)
[2025-02-13 20:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40][root][INFO] - Training Epoch: 2/2, step 125/7134 completed (loss: 0.20142967998981476, acc: 0.9607843160629272)
[2025-02-13 20:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41][root][INFO] - Training Epoch: 2/2, step 126/7134 completed (loss: 0.2055382877588272, acc: 0.9468085169792175)
[2025-02-13 20:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41][root][INFO] - Training Epoch: 2/2, step 127/7134 completed (loss: 0.24319975078105927, acc: 0.9375)
[2025-02-13 20:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41][root][INFO] - Training Epoch: 2/2, step 128/7134 completed (loss: 0.22372914850711823, acc: 0.9665071964263916)
[2025-02-13 20:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:42][root][INFO] - Training Epoch: 2/2, step 129/7134 completed (loss: 0.19594436883926392, acc: 0.9441340565681458)
[2025-02-13 20:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:42][root][INFO] - Training Epoch: 2/2, step 130/7134 completed (loss: 0.10410982370376587, acc: 0.9588235020637512)
[2025-02-13 20:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43][root][INFO] - Training Epoch: 2/2, step 131/7134 completed (loss: 0.0489288754761219, acc: 0.9824561476707458)
[2025-02-13 20:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43][root][INFO] - Training Epoch: 2/2, step 132/7134 completed (loss: 0.20522409677505493, acc: 0.955974817276001)
[2025-02-13 20:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43][root][INFO] - Training Epoch: 2/2, step 133/7134 completed (loss: 0.10887807607650757, acc: 0.970588207244873)
[2025-02-13 20:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:44][root][INFO] - Training Epoch: 2/2, step 134/7134 completed (loss: 0.06924021244049072, acc: 0.9774436354637146)
[2025-02-13 20:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:44][root][INFO] - Training Epoch: 2/2, step 135/7134 completed (loss: 0.1920718103647232, acc: 0.9470899701118469)
[2025-02-13 20:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45][root][INFO] - Training Epoch: 2/2, step 136/7134 completed (loss: 0.11207006126642227, acc: 0.97826087474823)
[2025-02-13 20:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45][root][INFO] - Training Epoch: 2/2, step 137/7134 completed (loss: 0.1465246081352234, acc: 0.9597989916801453)
[2025-02-13 20:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45][root][INFO] - Training Epoch: 2/2, step 138/7134 completed (loss: 0.1189977377653122, acc: 0.9623655676841736)
[2025-02-13 20:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:46][root][INFO] - Training Epoch: 2/2, step 139/7134 completed (loss: 0.1490178406238556, acc: 0.9680851101875305)
[2025-02-13 20:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:46][root][INFO] - Training Epoch: 2/2, step 140/7134 completed (loss: 0.11276969313621521, acc: 0.9815950989723206)
[2025-02-13 20:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47][root][INFO] - Training Epoch: 2/2, step 141/7134 completed (loss: 0.09821094572544098, acc: 0.9800000190734863)
[2025-02-13 20:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47][root][INFO] - Training Epoch: 2/2, step 142/7134 completed (loss: 0.16993799805641174, acc: 0.9604519605636597)
[2025-02-13 20:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47][root][INFO] - Training Epoch: 2/2, step 143/7134 completed (loss: 0.2509186863899231, acc: 0.9171270728111267)
[2025-02-13 20:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48][root][INFO] - Training Epoch: 2/2, step 144/7134 completed (loss: 0.2819156050682068, acc: 0.9527027010917664)
[2025-02-13 20:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48][root][INFO] - Training Epoch: 2/2, step 145/7134 completed (loss: 0.599429726600647, acc: 0.8497409224510193)
[2025-02-13 20:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48][root][INFO] - Training Epoch: 2/2, step 146/7134 completed (loss: 0.32383251190185547, acc: 0.903954803943634)
[2025-02-13 20:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49][root][INFO] - Training Epoch: 2/2, step 147/7134 completed (loss: 0.45296168327331543, acc: 0.9103448390960693)
[2025-02-13 20:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49][root][INFO] - Training Epoch: 2/2, step 148/7134 completed (loss: 0.581105649471283, acc: 0.8306451439857483)
[2025-02-13 20:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50][root][INFO] - Training Epoch: 2/2, step 149/7134 completed (loss: 0.3741338551044464, acc: 0.8883248567581177)
[2025-02-13 20:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50][root][INFO] - Training Epoch: 2/2, step 150/7134 completed (loss: 0.3693749010562897, acc: 0.9057591557502747)
[2025-02-13 20:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51][root][INFO] - Training Epoch: 2/2, step 151/7134 completed (loss: 0.21543878316879272, acc: 0.9408602118492126)
[2025-02-13 20:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51][root][INFO] - Training Epoch: 2/2, step 152/7134 completed (loss: 0.15834081172943115, acc: 0.9712918400764465)
[2025-02-13 20:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51][root][INFO] - Training Epoch: 2/2, step 153/7134 completed (loss: 0.5241596102714539, acc: 0.9060773253440857)
[2025-02-13 20:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52][root][INFO] - Training Epoch: 2/2, step 154/7134 completed (loss: 0.42599251866340637, acc: 0.9281045794487)
[2025-02-13 20:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52][root][INFO] - Training Epoch: 2/2, step 155/7134 completed (loss: 0.3560062646865845, acc: 0.9508196711540222)
[2025-02-13 20:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53][root][INFO] - Training Epoch: 2/2, step 156/7134 completed (loss: 0.1391267329454422, acc: 0.9704433679580688)
[2025-02-13 20:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53][root][INFO] - Training Epoch: 2/2, step 157/7134 completed (loss: 0.38095539808273315, acc: 0.9153439402580261)
[2025-02-13 20:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53][root][INFO] - Training Epoch: 2/2, step 158/7134 completed (loss: 0.15367558598518372, acc: 0.9649122953414917)
[2025-02-13 20:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54][root][INFO] - Training Epoch: 2/2, step 159/7134 completed (loss: 0.27584680914878845, acc: 0.9479768872261047)
[2025-02-13 20:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54][root][INFO] - Training Epoch: 2/2, step 160/7134 completed (loss: 0.1354062408208847, acc: 0.9661017060279846)
[2025-02-13 20:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55][root][INFO] - Training Epoch: 2/2, step 161/7134 completed (loss: 0.035611823201179504, acc: 0.9864864945411682)
[2025-02-13 20:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55][root][INFO] - Training Epoch: 2/2, step 162/7134 completed (loss: 0.09731264412403107, acc: 0.9506173133850098)
[2025-02-13 20:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56][root][INFO] - Training Epoch: 2/2, step 163/7134 completed (loss: 0.05839167907834053, acc: 0.9856114983558655)
[2025-02-13 20:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56][root][INFO] - Training Epoch: 2/2, step 164/7134 completed (loss: 0.2594635784626007, acc: 0.9595959782600403)
[2025-02-13 20:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56][root][INFO] - Training Epoch: 2/2, step 165/7134 completed (loss: 0.20491579174995422, acc: 0.9700000286102295)
[2025-02-13 20:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57][root][INFO] - Training Epoch: 2/2, step 166/7134 completed (loss: 0.09665396809577942, acc: 0.9736841917037964)
[2025-02-13 20:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57][root][INFO] - Training Epoch: 2/2, step 167/7134 completed (loss: 1.3696544170379639, acc: 0.7289156913757324)
[2025-02-13 20:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57][root][INFO] - Training Epoch: 2/2, step 168/7134 completed (loss: 2.61856746673584, acc: 0.4727272689342499)
[2025-02-13 20:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:58][root][INFO] - Training Epoch: 2/2, step 169/7134 completed (loss: 2.3591151237487793, acc: 0.5859375)
[2025-02-13 20:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:58][root][INFO] - Training Epoch: 2/2, step 170/7134 completed (loss: 1.3763761520385742, acc: 0.726190447807312)
[2025-02-13 20:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59][root][INFO] - Training Epoch: 2/2, step 171/7134 completed (loss: 1.366296648979187, acc: 0.7425742745399475)
[2025-02-13 20:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59][root][INFO] - Training Epoch: 2/2, step 172/7134 completed (loss: 0.5844526886940002, acc: 0.8759689927101135)
[2025-02-13 20:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59][root][INFO] - Training Epoch: 2/2, step 173/7134 completed (loss: 0.6247660517692566, acc: 0.8479999899864197)
[2025-02-13 20:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00][root][INFO] - Training Epoch: 2/2, step 174/7134 completed (loss: 0.9749202728271484, acc: 0.7631579041481018)
[2025-02-13 20:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00][root][INFO] - Training Epoch: 2/2, step 175/7134 completed (loss: 0.6596230864524841, acc: 0.8344370722770691)
[2025-02-13 20:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00][root][INFO] - Training Epoch: 2/2, step 176/7134 completed (loss: 0.5965004563331604, acc: 0.8251366019248962)
[2025-02-13 20:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:01][root][INFO] - Training Epoch: 2/2, step 177/7134 completed (loss: 0.7264479398727417, acc: 0.8203592896461487)
[2025-02-13 20:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:01][root][INFO] - Training Epoch: 2/2, step 178/7134 completed (loss: 0.5321686267852783, acc: 0.8441558480262756)
[2025-02-13 20:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02][root][INFO] - Training Epoch: 2/2, step 179/7134 completed (loss: 0.22745400667190552, acc: 0.9124087691307068)
[2025-02-13 20:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02][root][INFO] - Training Epoch: 2/2, step 180/7134 completed (loss: 0.21941319108009338, acc: 0.9597315192222595)
[2025-02-13 20:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03][root][INFO] - Training Epoch: 2/2, step 181/7134 completed (loss: 0.042559217661619186, acc: 1.0)
[2025-02-13 20:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03][root][INFO] - Training Epoch: 2/2, step 182/7134 completed (loss: 0.07847930490970612, acc: 0.9875776171684265)
[2025-02-13 20:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03][root][INFO] - Training Epoch: 2/2, step 183/7134 completed (loss: 0.18289951980113983, acc: 0.9452054500579834)
[2025-02-13 20:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04][root][INFO] - Training Epoch: 2/2, step 184/7134 completed (loss: 0.17389602959156036, acc: 0.9453125)
[2025-02-13 20:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04][root][INFO] - Training Epoch: 2/2, step 185/7134 completed (loss: 0.0751333013176918, acc: 0.9795918464660645)
[2025-02-13 20:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05][root][INFO] - Training Epoch: 2/2, step 186/7134 completed (loss: 0.1875281035900116, acc: 0.9701492786407471)
[2025-02-13 20:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05][root][INFO] - Training Epoch: 2/2, step 187/7134 completed (loss: 0.26059162616729736, acc: 0.9281045794487)
[2025-02-13 20:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05][root][INFO] - Training Epoch: 2/2, step 188/7134 completed (loss: 0.28332430124282837, acc: 0.9146341681480408)
[2025-02-13 20:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06][root][INFO] - Training Epoch: 2/2, step 189/7134 completed (loss: 0.2920302748680115, acc: 0.9180327653884888)
[2025-02-13 20:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06][root][INFO] - Training Epoch: 2/2, step 190/7134 completed (loss: 0.5608032941818237, acc: 0.8805969953536987)
[2025-02-13 20:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06][root][INFO] - Training Epoch: 2/2, step 191/7134 completed (loss: 0.4006336033344269, acc: 0.942307710647583)
[2025-02-13 20:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07][root][INFO] - Training Epoch: 2/2, step 192/7134 completed (loss: 0.13186921179294586, acc: 0.9651162624359131)
[2025-02-13 20:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07][root][INFO] - Training Epoch: 2/2, step 193/7134 completed (loss: 0.26978012919425964, acc: 0.910179615020752)
[2025-02-13 20:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:08][root][INFO] - Training Epoch: 2/2, step 194/7134 completed (loss: 0.3669496476650238, acc: 0.893081784248352)
[2025-02-13 20:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:08][root][INFO] - Training Epoch: 2/2, step 195/7134 completed (loss: 0.22961382567882538, acc: 0.9375)
[2025-02-13 20:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09][root][INFO] - Training Epoch: 2/2, step 196/7134 completed (loss: 0.33295705914497375, acc: 0.8938547372817993)
[2025-02-13 20:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09][root][INFO] - Training Epoch: 2/2, step 197/7134 completed (loss: 0.1309303492307663, acc: 0.9729729890823364)
[2025-02-13 20:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09][root][INFO] - Training Epoch: 2/2, step 198/7134 completed (loss: 0.1592983454465866, acc: 0.9554139971733093)
[2025-02-13 20:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:10][root][INFO] - Training Epoch: 2/2, step 199/7134 completed (loss: 0.3474261164665222, acc: 0.9264705777168274)
[2025-02-13 20:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:10][root][INFO] - Training Epoch: 2/2, step 200/7134 completed (loss: 0.06514602899551392, acc: 0.9931972622871399)
[2025-02-13 20:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11][root][INFO] - Training Epoch: 2/2, step 201/7134 completed (loss: 0.062379006296396255, acc: 0.9929078221321106)
[2025-02-13 20:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11][root][INFO] - Training Epoch: 2/2, step 202/7134 completed (loss: 0.23613241314888, acc: 0.9419354796409607)
[2025-02-13 20:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11][root][INFO] - Training Epoch: 2/2, step 203/7134 completed (loss: 0.29953911900520325, acc: 0.9289940595626831)
[2025-02-13 20:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12][root][INFO] - Training Epoch: 2/2, step 204/7134 completed (loss: 0.3731593191623688, acc: 0.893081784248352)
[2025-02-13 20:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12][root][INFO] - Training Epoch: 2/2, step 205/7134 completed (loss: 0.40452656149864197, acc: 0.9049999713897705)
[2025-02-13 20:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13][root][INFO] - Training Epoch: 2/2, step 206/7134 completed (loss: 0.1320592761039734, acc: 0.9861111044883728)
[2025-02-13 20:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13][root][INFO] - Training Epoch: 2/2, step 207/7134 completed (loss: 0.31272971630096436, acc: 0.918367326259613)
[2025-02-13 20:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13][root][INFO] - Training Epoch: 2/2, step 208/7134 completed (loss: 0.6458314061164856, acc: 0.8928571343421936)
[2025-02-13 20:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14][root][INFO] - Training Epoch: 2/2, step 209/7134 completed (loss: 0.5262611508369446, acc: 0.8715083599090576)
[2025-02-13 20:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14][root][INFO] - Training Epoch: 2/2, step 210/7134 completed (loss: 0.35111188888549805, acc: 0.9251101613044739)
[2025-02-13 20:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15][root][INFO] - Training Epoch: 2/2, step 211/7134 completed (loss: 0.31917694211006165, acc: 0.9256756901741028)
[2025-02-13 20:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15][root][INFO] - Training Epoch: 2/2, step 212/7134 completed (loss: 0.5469958186149597, acc: 0.8925619721412659)
[2025-02-13 20:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15][root][INFO] - Training Epoch: 2/2, step 213/7134 completed (loss: 0.5751621127128601, acc: 0.8666666746139526)
[2025-02-13 20:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16][root][INFO] - Training Epoch: 2/2, step 214/7134 completed (loss: 0.1678144633769989, acc: 0.9603960514068604)
[2025-02-13 20:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16][root][INFO] - Training Epoch: 2/2, step 215/7134 completed (loss: 0.17251303791999817, acc: 0.9571428298950195)
[2025-02-13 20:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17][root][INFO] - Training Epoch: 2/2, step 216/7134 completed (loss: 0.14791764318943024, acc: 0.9775280952453613)
[2025-02-13 20:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17][root][INFO] - Training Epoch: 2/2, step 217/7134 completed (loss: 0.15273118019104004, acc: 0.9791666865348816)
[2025-02-13 20:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17][root][INFO] - Training Epoch: 2/2, step 218/7134 completed (loss: 0.09832699596881866, acc: 0.9893048405647278)
[2025-02-13 20:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18][root][INFO] - Training Epoch: 2/2, step 219/7134 completed (loss: 0.07641056925058365, acc: 0.9736841917037964)
[2025-02-13 20:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18][root][INFO] - Training Epoch: 2/2, step 220/7134 completed (loss: 0.10097906738519669, acc: 0.9739130139350891)
[2025-02-13 20:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19][root][INFO] - Training Epoch: 2/2, step 221/7134 completed (loss: 0.14955805242061615, acc: 0.96875)
[2025-02-13 20:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19][root][INFO] - Training Epoch: 2/2, step 222/7134 completed (loss: 0.1330561488866806, acc: 0.9710982441902161)
[2025-02-13 20:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19][root][INFO] - Training Epoch: 2/2, step 223/7134 completed (loss: 0.2719779312610626, acc: 0.9351351261138916)
[2025-02-13 20:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:20][root][INFO] - Training Epoch: 2/2, step 224/7134 completed (loss: 0.3506017029285431, acc: 0.9329608678817749)
[2025-02-13 20:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:20][root][INFO] - Training Epoch: 2/2, step 225/7134 completed (loss: 0.3245895802974701, acc: 0.9171597361564636)
[2025-02-13 20:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21][root][INFO] - Training Epoch: 2/2, step 226/7134 completed (loss: 0.170435830950737, acc: 0.9428571462631226)
[2025-02-13 20:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21][root][INFO] - Training Epoch: 2/2, step 227/7134 completed (loss: 0.3168156147003174, acc: 0.9468085169792175)
[2025-02-13 20:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21][root][INFO] - Training Epoch: 2/2, step 228/7134 completed (loss: 0.33913081884384155, acc: 0.9281437397003174)
[2025-02-13 20:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:22][root][INFO] - Training Epoch: 2/2, step 229/7134 completed (loss: 0.3059593737125397, acc: 0.9142857193946838)
[2025-02-13 20:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:22][root][INFO] - Training Epoch: 2/2, step 230/7134 completed (loss: 0.19019867479801178, acc: 0.950276255607605)
[2025-02-13 20:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23][root][INFO] - Training Epoch: 2/2, step 231/7134 completed (loss: 0.18535216152668, acc: 0.9736841917037964)
[2025-02-13 20:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23][root][INFO] - Training Epoch: 2/2, step 232/7134 completed (loss: 0.29940342903137207, acc: 0.9300699234008789)
[2025-02-13 20:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23][root][INFO] - Training Epoch: 2/2, step 233/7134 completed (loss: 0.2165079116821289, acc: 0.9489051103591919)
[2025-02-13 20:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24][root][INFO] - Training Epoch: 2/2, step 234/7134 completed (loss: 0.2200879603624344, acc: 0.9436619877815247)
[2025-02-13 20:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24][root][INFO] - Training Epoch: 2/2, step 235/7134 completed (loss: 0.2816372513771057, acc: 0.9390243887901306)
[2025-02-13 20:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25][root][INFO] - Training Epoch: 2/2, step 236/7134 completed (loss: 0.12486068904399872, acc: 0.9602649211883545)
[2025-02-13 20:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25][root][INFO] - Training Epoch: 2/2, step 237/7134 completed (loss: 0.3087123930454254, acc: 0.9171974658966064)
[2025-02-13 20:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25][root][INFO] - Training Epoch: 2/2, step 238/7134 completed (loss: 0.34843844175338745, acc: 0.9312977194786072)
[2025-02-13 20:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26][root][INFO] - Training Epoch: 2/2, step 239/7134 completed (loss: 0.21301807463169098, acc: 0.9528301954269409)
[2025-02-13 20:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26][root][INFO] - Training Epoch: 2/2, step 240/7134 completed (loss: 0.20481915771961212, acc: 0.9805825352668762)
[2025-02-13 20:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26][root][INFO] - Training Epoch: 2/2, step 241/7134 completed (loss: 0.4164702296257019, acc: 0.9104477763175964)
[2025-02-13 20:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27][root][INFO] - Training Epoch: 2/2, step 242/7134 completed (loss: 0.3908917307853699, acc: 0.9060402512550354)
[2025-02-13 20:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27][root][INFO] - Training Epoch: 2/2, step 243/7134 completed (loss: 0.6003291606903076, acc: 0.8620689511299133)
[2025-02-13 20:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28][root][INFO] - Training Epoch: 2/2, step 244/7134 completed (loss: 0.1917162537574768, acc: 0.9568345546722412)
[2025-02-13 20:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28][root][INFO] - Training Epoch: 2/2, step 245/7134 completed (loss: 0.1832663118839264, acc: 0.9508196711540222)
[2025-02-13 20:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28][root][INFO] - Training Epoch: 2/2, step 246/7134 completed (loss: 0.18484866619110107, acc: 0.9710144996643066)
[2025-02-13 20:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29][root][INFO] - Training Epoch: 2/2, step 247/7134 completed (loss: 0.19844338297843933, acc: 0.9523809552192688)
[2025-02-13 20:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29][root][INFO] - Training Epoch: 2/2, step 248/7134 completed (loss: 0.15964898467063904, acc: 0.9756097793579102)
[2025-02-13 20:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30][root][INFO] - Training Epoch: 2/2, step 249/7134 completed (loss: 0.16071434319019318, acc: 0.95652174949646)
[2025-02-13 20:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30][root][INFO] - Training Epoch: 2/2, step 250/7134 completed (loss: 0.21423554420471191, acc: 0.940397322177887)
[2025-02-13 20:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30][root][INFO] - Training Epoch: 2/2, step 251/7134 completed (loss: 0.21812672913074493, acc: 0.971222996711731)
[2025-02-13 20:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31][root][INFO] - Training Epoch: 2/2, step 252/7134 completed (loss: 0.08785734325647354, acc: 0.971222996711731)
[2025-02-13 20:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31][root][INFO] - Training Epoch: 2/2, step 253/7134 completed (loss: 0.05727604031562805, acc: 0.9919999837875366)
[2025-02-13 20:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31][root][INFO] - Training Epoch: 2/2, step 254/7134 completed (loss: 0.08529488742351532, acc: 0.9824561476707458)
[2025-02-13 20:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32][root][INFO] - Training Epoch: 2/2, step 255/7134 completed (loss: 0.12404777109622955, acc: 0.9679487347602844)
[2025-02-13 20:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32][root][INFO] - Training Epoch: 2/2, step 256/7134 completed (loss: 0.5035958290100098, acc: 0.8648648858070374)
[2025-02-13 20:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33][root][INFO] - Training Epoch: 2/2, step 257/7134 completed (loss: 0.21778230369091034, acc: 0.9512194991111755)
[2025-02-13 20:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33][root][INFO] - Training Epoch: 2/2, step 258/7134 completed (loss: 0.1022171750664711, acc: 0.975806474685669)
[2025-02-13 20:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33][root][INFO] - Training Epoch: 2/2, step 259/7134 completed (loss: 0.1303766369819641, acc: 0.9696969985961914)
[2025-02-13 20:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34][root][INFO] - Training Epoch: 2/2, step 260/7134 completed (loss: 0.11399220675230026, acc: 0.9774436354637146)
[2025-02-13 20:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34][root][INFO] - Training Epoch: 2/2, step 261/7134 completed (loss: 0.19929568469524384, acc: 0.9351851940155029)
[2025-02-13 20:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35][root][INFO] - Training Epoch: 2/2, step 262/7134 completed (loss: 0.03487817198038101, acc: 1.0)
[2025-02-13 20:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35][root][INFO] - Training Epoch: 2/2, step 263/7134 completed (loss: 0.16781429946422577, acc: 0.9320987462997437)
[2025-02-13 20:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35][root][INFO] - Training Epoch: 2/2, step 264/7134 completed (loss: 0.10829532891511917, acc: 0.9764705896377563)
[2025-02-13 20:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36][root][INFO] - Training Epoch: 2/2, step 265/7134 completed (loss: 0.17216947674751282, acc: 0.9438202381134033)
[2025-02-13 20:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36][root][INFO] - Training Epoch: 2/2, step 266/7134 completed (loss: 0.1729031205177307, acc: 0.9583333134651184)
[2025-02-13 20:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37][root][INFO] - Training Epoch: 2/2, step 267/7134 completed (loss: 0.15939012169837952, acc: 0.9521276354789734)
[2025-02-13 20:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37][root][INFO] - Training Epoch: 2/2, step 268/7134 completed (loss: 0.16462324559688568, acc: 0.9575757384300232)
[2025-02-13 20:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37][root][INFO] - Training Epoch: 2/2, step 269/7134 completed (loss: 0.2577424943447113, acc: 0.9345238208770752)
[2025-02-13 20:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38][root][INFO] - Training Epoch: 2/2, step 270/7134 completed (loss: 0.24652227759361267, acc: 0.9624999761581421)
[2025-02-13 20:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38][root][INFO] - Training Epoch: 2/2, step 271/7134 completed (loss: 0.15423215925693512, acc: 0.9666666388511658)
[2025-02-13 20:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38][root][INFO] - Training Epoch: 2/2, step 272/7134 completed (loss: 0.23752468824386597, acc: 0.9441340565681458)
[2025-02-13 20:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39][root][INFO] - Training Epoch: 2/2, step 273/7134 completed (loss: 0.14095765352249146, acc: 0.9680851101875305)
[2025-02-13 20:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39][root][INFO] - Training Epoch: 2/2, step 274/7134 completed (loss: 0.24328775703907013, acc: 0.9473684430122375)
[2025-02-13 20:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40][root][INFO] - Training Epoch: 2/2, step 275/7134 completed (loss: 0.07339493185281754, acc: 0.9644669890403748)
[2025-02-13 20:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40][root][INFO] - Training Epoch: 2/2, step 276/7134 completed (loss: 0.09026411175727844, acc: 0.9742268323898315)
[2025-02-13 20:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40][root][INFO] - Training Epoch: 2/2, step 277/7134 completed (loss: 0.20589378476142883, acc: 0.9567567706108093)
[2025-02-13 20:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:41][root][INFO] - Training Epoch: 2/2, step 278/7134 completed (loss: 0.15794318914413452, acc: 0.9635416865348816)
[2025-02-13 20:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:41][root][INFO] - Training Epoch: 2/2, step 279/7134 completed (loss: 0.20415550470352173, acc: 0.9567567706108093)
[2025-02-13 20:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42][root][INFO] - Training Epoch: 2/2, step 280/7134 completed (loss: 0.20048782229423523, acc: 0.9359999895095825)
[2025-02-13 20:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42][root][INFO] - Training Epoch: 2/2, step 281/7134 completed (loss: 0.11057968437671661, acc: 0.9745762944221497)
[2025-02-13 20:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42][root][INFO] - Training Epoch: 2/2, step 282/7134 completed (loss: 0.19245290756225586, acc: 0.9367088675498962)
[2025-02-13 20:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43][root][INFO] - Training Epoch: 2/2, step 283/7134 completed (loss: 0.05325641855597496, acc: 0.9923664331436157)
[2025-02-13 20:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43][root][INFO] - Training Epoch: 2/2, step 284/7134 completed (loss: 0.05760380998253822, acc: 0.9930070042610168)
[2025-02-13 20:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44][root][INFO] - Training Epoch: 2/2, step 285/7134 completed (loss: 0.21118228137493134, acc: 0.9481481313705444)
[2025-02-13 20:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44][root][INFO] - Training Epoch: 2/2, step 286/7134 completed (loss: 0.15984588861465454, acc: 0.9583333134651184)
[2025-02-13 20:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44][root][INFO] - Training Epoch: 2/2, step 287/7134 completed (loss: 0.19357304275035858, acc: 0.9798657894134521)
[2025-02-13 20:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45][root][INFO] - Training Epoch: 2/2, step 288/7134 completed (loss: 0.10725419223308563, acc: 0.960629940032959)
[2025-02-13 20:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45][root][INFO] - Training Epoch: 2/2, step 289/7134 completed (loss: 0.0829186886548996, acc: 0.9767441749572754)
[2025-02-13 20:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45][root][INFO] - Training Epoch: 2/2, step 290/7134 completed (loss: 0.31260234117507935, acc: 0.9285714030265808)
[2025-02-13 20:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46][root][INFO] - Training Epoch: 2/2, step 291/7134 completed (loss: 0.3353383541107178, acc: 0.9251700639724731)
[2025-02-13 20:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46][root][INFO] - Training Epoch: 2/2, step 292/7134 completed (loss: 0.4542402923107147, acc: 0.9012345671653748)
[2025-02-13 20:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47][root][INFO] - Training Epoch: 2/2, step 293/7134 completed (loss: 0.30164802074432373, acc: 0.9292035102844238)
[2025-02-13 20:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47][root][INFO] - Training Epoch: 2/2, step 294/7134 completed (loss: 0.21818530559539795, acc: 0.9237288236618042)
[2025-02-13 20:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47][root][INFO] - Training Epoch: 2/2, step 295/7134 completed (loss: 0.44005388021469116, acc: 0.8974359035491943)
[2025-02-13 20:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48][root][INFO] - Training Epoch: 2/2, step 296/7134 completed (loss: 0.06787879019975662, acc: 0.9924242496490479)
[2025-02-13 20:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48][root][INFO] - Training Epoch: 2/2, step 297/7134 completed (loss: 0.1558821052312851, acc: 0.9586777091026306)
[2025-02-13 20:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49][root][INFO] - Training Epoch: 2/2, step 298/7134 completed (loss: 0.07965726405382156, acc: 0.9846153855323792)
[2025-02-13 20:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49][root][INFO] - Training Epoch: 2/2, step 299/7134 completed (loss: 0.06172468885779381, acc: 0.985401451587677)
[2025-02-13 20:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49][root][INFO] - Training Epoch: 2/2, step 300/7134 completed (loss: 0.059671394526958466, acc: 0.9855072498321533)
[2025-02-13 20:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50][root][INFO] - Training Epoch: 2/2, step 301/7134 completed (loss: 0.08026177436113358, acc: 0.9720279574394226)
[2025-02-13 20:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50][root][INFO] - Training Epoch: 2/2, step 302/7134 completed (loss: 0.10592398047447205, acc: 0.9900990128517151)
[2025-02-13 20:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50][root][INFO] - Training Epoch: 2/2, step 303/7134 completed (loss: 0.11426952481269836, acc: 0.9729729890823364)
[2025-02-13 20:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:51][root][INFO] - Training Epoch: 2/2, step 304/7134 completed (loss: 0.0878000259399414, acc: 0.9900990128517151)
[2025-02-13 20:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:51][root][INFO] - Training Epoch: 2/2, step 305/7134 completed (loss: 0.16364194452762604, acc: 0.9609375)
[2025-02-13 20:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52][root][INFO] - Training Epoch: 2/2, step 306/7134 completed (loss: 0.06305134296417236, acc: 0.9739130139350891)
[2025-02-13 20:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52][root][INFO] - Training Epoch: 2/2, step 307/7134 completed (loss: 0.16101601719856262, acc: 0.9345794320106506)
[2025-02-13 20:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52][root][INFO] - Training Epoch: 2/2, step 308/7134 completed (loss: 0.10110428929328918, acc: 0.9685039520263672)
[2025-02-13 20:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53][root][INFO] - Training Epoch: 2/2, step 309/7134 completed (loss: 0.10624852776527405, acc: 0.9734513163566589)
[2025-02-13 20:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53][root][INFO] - Training Epoch: 2/2, step 310/7134 completed (loss: 0.10009045153856277, acc: 0.9844961166381836)
[2025-02-13 20:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53][root][INFO] - Training Epoch: 2/2, step 311/7134 completed (loss: 0.11014329642057419, acc: 0.9624060392379761)
[2025-02-13 20:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54][root][INFO] - Training Epoch: 2/2, step 312/7134 completed (loss: 0.022538864985108376, acc: 1.0)
[2025-02-13 20:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54][root][INFO] - Training Epoch: 2/2, step 313/7134 completed (loss: 0.02681405283510685, acc: 1.0)
[2025-02-13 20:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:55][root][INFO] - Training Epoch: 2/2, step 314/7134 completed (loss: 0.05120488256216049, acc: 0.9925925731658936)
[2025-02-13 20:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:55][root][INFO] - Training Epoch: 2/2, step 315/7134 completed (loss: 0.20598334074020386, acc: 0.949999988079071)
[2025-02-13 20:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:55][root][INFO] - Training Epoch: 2/2, step 316/7134 completed (loss: 0.08834730833768845, acc: 0.9819819927215576)
[2025-02-13 20:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56][root][INFO] - Training Epoch: 2/2, step 317/7134 completed (loss: 0.023112274706363678, acc: 1.0)
[2025-02-13 20:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56][root][INFO] - Training Epoch: 2/2, step 318/7134 completed (loss: 0.0962919294834137, acc: 0.9731183052062988)
[2025-02-13 20:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56][root][INFO] - Training Epoch: 2/2, step 319/7134 completed (loss: 0.14654628932476044, acc: 0.9627329111099243)
[2025-02-13 20:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57][root][INFO] - Training Epoch: 2/2, step 320/7134 completed (loss: 0.1533568799495697, acc: 0.9642857313156128)
[2025-02-13 20:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57][root][INFO] - Training Epoch: 2/2, step 321/7134 completed (loss: 0.10607884079217911, acc: 0.9655172228813171)
[2025-02-13 20:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58][root][INFO] - Training Epoch: 2/2, step 322/7134 completed (loss: 0.07128437608480453, acc: 0.9803921580314636)
[2025-02-13 20:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58][root][INFO] - Training Epoch: 2/2, step 323/7134 completed (loss: 0.2616930305957794, acc: 0.9285714030265808)
[2025-02-13 20:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58][root][INFO] - Training Epoch: 2/2, step 324/7134 completed (loss: 0.20097331702709198, acc: 0.9398148059844971)
[2025-02-13 20:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59][root][INFO] - Training Epoch: 2/2, step 325/7134 completed (loss: 0.13156946003437042, acc: 0.9659090638160706)
[2025-02-13 20:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59][root][INFO] - Training Epoch: 2/2, step 326/7134 completed (loss: 0.09688068926334381, acc: 0.9775280952453613)
[2025-02-13 20:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59][root][INFO] - Training Epoch: 2/2, step 327/7134 completed (loss: 0.07320304960012436, acc: 0.9747474789619446)
[2025-02-13 20:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00][root][INFO] - Training Epoch: 2/2, step 328/7134 completed (loss: 0.1085018664598465, acc: 0.9750000238418579)
[2025-02-13 20:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00][root][INFO] - Training Epoch: 2/2, step 329/7134 completed (loss: 0.07836935669183731, acc: 0.9800994992256165)
[2025-02-13 20:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01][root][INFO] - Training Epoch: 2/2, step 330/7134 completed (loss: 0.0392305888235569, acc: 0.9912280440330505)
[2025-02-13 20:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01][root][INFO] - Training Epoch: 2/2, step 331/7134 completed (loss: 0.06336815655231476, acc: 0.9791666865348816)
[2025-02-13 20:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01][root][INFO] - Training Epoch: 2/2, step 332/7134 completed (loss: 0.033779989928007126, acc: 0.9950248599052429)
[2025-02-13 20:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02][root][INFO] - Training Epoch: 2/2, step 333/7134 completed (loss: 0.190729558467865, acc: 0.9408602118492126)
[2025-02-13 20:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02][root][INFO] - Training Epoch: 2/2, step 334/7134 completed (loss: 0.136475071310997, acc: 0.9560439586639404)
[2025-02-13 20:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02][root][INFO] - Training Epoch: 2/2, step 335/7134 completed (loss: 0.11937128752470016, acc: 0.9729729890823364)
[2025-02-13 20:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03][root][INFO] - Training Epoch: 2/2, step 336/7134 completed (loss: 0.272967666387558, acc: 0.9585492014884949)
[2025-02-13 20:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03][root][INFO] - Training Epoch: 2/2, step 337/7134 completed (loss: 0.1367468386888504, acc: 0.9689440727233887)
[2025-02-13 20:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04][root][INFO] - Training Epoch: 2/2, step 338/7134 completed (loss: 0.20513246953487396, acc: 0.9581151604652405)
[2025-02-13 20:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04][root][INFO] - Training Epoch: 2/2, step 339/7134 completed (loss: 0.1646742969751358, acc: 0.9572192430496216)
[2025-02-13 20:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04][root][INFO] - Training Epoch: 2/2, step 340/7134 completed (loss: 0.1181185394525528, acc: 0.9696969985961914)
[2025-02-13 20:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:05][root][INFO] - Training Epoch: 2/2, step 341/7134 completed (loss: 0.09934872388839722, acc: 0.9846938848495483)
[2025-02-13 20:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:05][root][INFO] - Training Epoch: 2/2, step 342/7134 completed (loss: 0.11535603553056717, acc: 0.9811320900917053)
[2025-02-13 20:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06][root][INFO] - Training Epoch: 2/2, step 343/7134 completed (loss: 0.2001117467880249, acc: 0.9384615421295166)
[2025-02-13 20:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06][root][INFO] - Training Epoch: 2/2, step 344/7134 completed (loss: 0.13007774949073792, acc: 0.9534883499145508)
[2025-02-13 20:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06][root][INFO] - Training Epoch: 2/2, step 345/7134 completed (loss: 0.11983165144920349, acc: 0.9645389914512634)
[2025-02-13 20:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07][root][INFO] - Training Epoch: 2/2, step 346/7134 completed (loss: 0.07170427590608597, acc: 0.9802631735801697)
[2025-02-13 20:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07][root][INFO] - Training Epoch: 2/2, step 347/7134 completed (loss: 0.14835289120674133, acc: 0.9624999761581421)
[2025-02-13 20:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08][root][INFO] - Training Epoch: 2/2, step 348/7134 completed (loss: 0.12405704706907272, acc: 0.9585798978805542)
[2025-02-13 20:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08][root][INFO] - Training Epoch: 2/2, step 349/7134 completed (loss: 0.18651853501796722, acc: 0.9585798978805542)
[2025-02-13 20:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08][root][INFO] - Training Epoch: 2/2, step 350/7134 completed (loss: 0.11334935575723648, acc: 0.9631578922271729)
[2025-02-13 20:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09][root][INFO] - Training Epoch: 2/2, step 351/7134 completed (loss: 0.13410702347755432, acc: 0.97826087474823)
[2025-02-13 20:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09][root][INFO] - Training Epoch: 2/2, step 352/7134 completed (loss: 0.0892031267285347, acc: 0.9707602262496948)
[2025-02-13 20:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10][root][INFO] - Training Epoch: 2/2, step 353/7134 completed (loss: 0.1063670888543129, acc: 0.9754601120948792)
[2025-02-13 20:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10][root][INFO] - Training Epoch: 2/2, step 354/7134 completed (loss: 0.08967996388673782, acc: 0.9597989916801453)
[2025-02-13 20:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10][root][INFO] - Training Epoch: 2/2, step 355/7134 completed (loss: 0.2140585035085678, acc: 0.9467455744743347)
[2025-02-13 20:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11][root][INFO] - Training Epoch: 2/2, step 356/7134 completed (loss: 0.09749796241521835, acc: 0.9640718698501587)
[2025-02-13 20:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11][root][INFO] - Training Epoch: 2/2, step 357/7134 completed (loss: 0.07354845106601715, acc: 0.9767441749572754)
[2025-02-13 20:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11][root][INFO] - Training Epoch: 2/2, step 358/7134 completed (loss: 0.0916961207985878, acc: 0.9772727489471436)
[2025-02-13 20:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:12][root][INFO] - Training Epoch: 2/2, step 359/7134 completed (loss: 0.10270750522613525, acc: 0.970588207244873)
[2025-02-13 20:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:12][root][INFO] - Training Epoch: 2/2, step 360/7134 completed (loss: 0.0807967558503151, acc: 0.9673202633857727)
[2025-02-13 20:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13][root][INFO] - Training Epoch: 2/2, step 361/7134 completed (loss: 0.045455362647771835, acc: 0.987730085849762)
[2025-02-13 20:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13][root][INFO] - Training Epoch: 2/2, step 362/7134 completed (loss: 0.05442372336983681, acc: 0.9873417615890503)
[2025-02-13 20:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13][root][INFO] - Training Epoch: 2/2, step 363/7134 completed (loss: 0.1414613425731659, acc: 0.9726775884628296)
[2025-02-13 20:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14][root][INFO] - Training Epoch: 2/2, step 364/7134 completed (loss: 0.08356965333223343, acc: 0.9805825352668762)
[2025-02-13 20:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14][root][INFO] - Training Epoch: 2/2, step 365/7134 completed (loss: 0.05866507813334465, acc: 0.9873417615890503)
[2025-02-13 20:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14][root][INFO] - Training Epoch: 2/2, step 366/7134 completed (loss: 0.056493766605854034, acc: 0.9745222926139832)
[2025-02-13 20:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15][root][INFO] - Training Epoch: 2/2, step 367/7134 completed (loss: 0.021591268479824066, acc: 1.0)
[2025-02-13 20:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15][root][INFO] - Training Epoch: 2/2, step 368/7134 completed (loss: 0.032957904040813446, acc: 0.9881656765937805)
[2025-02-13 20:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16][root][INFO] - Training Epoch: 2/2, step 369/7134 completed (loss: 0.09118152409791946, acc: 0.9815950989723206)
[2025-02-13 20:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16][root][INFO] - Training Epoch: 2/2, step 370/7134 completed (loss: 0.08994059264659882, acc: 0.9746835231781006)
[2025-02-13 20:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16][root][INFO] - Training Epoch: 2/2, step 371/7134 completed (loss: 0.057462964206933975, acc: 0.9813664555549622)
[2025-02-13 20:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17][root][INFO] - Training Epoch: 2/2, step 372/7134 completed (loss: 0.060946643352508545, acc: 0.9811320900917053)
[2025-02-13 20:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17][root][INFO] - Training Epoch: 2/2, step 373/7134 completed (loss: 0.03781931474804878, acc: 0.9934640526771545)
[2025-02-13 20:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17][root][INFO] - Training Epoch: 2/2, step 374/7134 completed (loss: 0.105082668364048, acc: 0.9729729890823364)
[2025-02-13 20:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18][root][INFO] - Training Epoch: 2/2, step 375/7134 completed (loss: 0.24452628195285797, acc: 0.9521276354789734)
[2025-02-13 20:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18][root][INFO] - Training Epoch: 2/2, step 376/7134 completed (loss: 0.15717843174934387, acc: 0.9555555582046509)
[2025-02-13 20:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19][root][INFO] - Training Epoch: 2/2, step 377/7134 completed (loss: 0.10213208198547363, acc: 0.9608938694000244)
[2025-02-13 20:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19][root][INFO] - Training Epoch: 2/2, step 378/7134 completed (loss: 0.16493703424930573, acc: 0.9435028433799744)
[2025-02-13 20:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19][root][INFO] - Training Epoch: 2/2, step 379/7134 completed (loss: 0.04310791194438934, acc: 0.9754902124404907)
[2025-02-13 20:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20][root][INFO] - Training Epoch: 2/2, step 380/7134 completed (loss: 0.24817237257957458, acc: 0.9484536051750183)
[2025-02-13 20:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20][root][INFO] - Training Epoch: 2/2, step 381/7134 completed (loss: 0.14911986887454987, acc: 0.9631578922271729)
[2025-02-13 20:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20][root][INFO] - Training Epoch: 2/2, step 382/7134 completed (loss: 0.08204759657382965, acc: 0.9692307710647583)
[2025-02-13 20:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21][root][INFO] - Training Epoch: 2/2, step 383/7134 completed (loss: 0.08670864254236221, acc: 0.9689922332763672)
[2025-02-13 20:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21][root][INFO] - Training Epoch: 2/2, step 384/7134 completed (loss: 0.14060471951961517, acc: 0.9558011293411255)
[2025-02-13 20:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22][root][INFO] - Training Epoch: 2/2, step 385/7134 completed (loss: 0.04928690940141678, acc: 0.9803921580314636)
[2025-02-13 20:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22][root][INFO] - Training Epoch: 2/2, step 386/7134 completed (loss: 0.029473254457116127, acc: 0.9909909963607788)
[2025-02-13 20:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22][root][INFO] - Training Epoch: 2/2, step 387/7134 completed (loss: 0.08975358307361603, acc: 0.9644970297813416)
[2025-02-13 20:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23][root][INFO] - Training Epoch: 2/2, step 388/7134 completed (loss: 0.06837157160043716, acc: 0.980861246585846)
[2025-02-13 20:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23][root][INFO] - Training Epoch: 2/2, step 389/7134 completed (loss: 0.1448143571615219, acc: 0.9428571462631226)
[2025-02-13 20:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23][root][INFO] - Training Epoch: 2/2, step 390/7134 completed (loss: 0.08092739433050156, acc: 0.9797297120094299)
[2025-02-13 20:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24][root][INFO] - Training Epoch: 2/2, step 391/7134 completed (loss: 0.09244923293590546, acc: 0.9800000190734863)
[2025-02-13 20:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24][root][INFO] - Training Epoch: 2/2, step 392/7134 completed (loss: 0.08690600842237473, acc: 0.9767441749572754)
[2025-02-13 20:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24][root][INFO] - Training Epoch: 2/2, step 393/7134 completed (loss: 0.086051806807518, acc: 0.9679144620895386)
[2025-02-13 20:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25][root][INFO] - Training Epoch: 2/2, step 394/7134 completed (loss: 0.09072891622781754, acc: 0.9770992398262024)
[2025-02-13 20:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25][root][INFO] - Training Epoch: 2/2, step 395/7134 completed (loss: 0.18957608938217163, acc: 0.9734042286872864)
[2025-02-13 20:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26][root][INFO] - Training Epoch: 2/2, step 396/7134 completed (loss: 0.05443214252591133, acc: 0.982758641242981)
[2025-02-13 20:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26][root][INFO] - Training Epoch: 2/2, step 397/7134 completed (loss: 0.2226942926645279, acc: 0.95652174949646)
[2025-02-13 20:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26][root][INFO] - Training Epoch: 2/2, step 398/7134 completed (loss: 0.06551653146743774, acc: 0.9797979593276978)
[2025-02-13 20:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27][root][INFO] - Training Epoch: 2/2, step 399/7134 completed (loss: 0.07310522347688675, acc: 0.9810126423835754)
[2025-02-13 20:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27][root][INFO] - Training Epoch: 2/2, step 400/7134 completed (loss: 0.29710420966148376, acc: 0.9303797483444214)
[2025-02-13 20:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27][root][INFO] - Training Epoch: 2/2, step 401/7134 completed (loss: 0.112198106944561, acc: 0.9509803652763367)
[2025-02-13 20:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28][root][INFO] - Training Epoch: 2/2, step 402/7134 completed (loss: 0.1375369131565094, acc: 0.9648241400718689)
[2025-02-13 20:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28][root][INFO] - Training Epoch: 2/2, step 403/7134 completed (loss: 0.08200757205486298, acc: 0.9781659245491028)
[2025-02-13 20:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28][root][INFO] - Training Epoch: 2/2, step 404/7134 completed (loss: 0.08350706100463867, acc: 0.9731183052062988)
[2025-02-13 20:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29][root][INFO] - Training Epoch: 2/2, step 405/7134 completed (loss: 0.3118084967136383, acc: 0.895348846912384)
[2025-02-13 20:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29][root][INFO] - Training Epoch: 2/2, step 406/7134 completed (loss: 0.42271724343299866, acc: 0.8972973227500916)
[2025-02-13 20:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30][root][INFO] - Training Epoch: 2/2, step 407/7134 completed (loss: 0.4131337106227875, acc: 0.9171974658966064)
[2025-02-13 20:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30][root][INFO] - Training Epoch: 2/2, step 408/7134 completed (loss: 0.20652751624584198, acc: 0.9385474920272827)
[2025-02-13 20:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30][root][INFO] - Training Epoch: 2/2, step 409/7134 completed (loss: 0.13917198777198792, acc: 0.9631578922271729)
[2025-02-13 20:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31][root][INFO] - Training Epoch: 2/2, step 410/7134 completed (loss: 0.18224963545799255, acc: 0.9636363387107849)
[2025-02-13 20:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31][root][INFO] - Training Epoch: 2/2, step 411/7134 completed (loss: 0.11994216591119766, acc: 0.9684684872627258)
[2025-02-13 20:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31][root][INFO] - Training Epoch: 2/2, step 412/7134 completed (loss: 0.08924488723278046, acc: 0.9730941653251648)
[2025-02-13 20:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32][root][INFO] - Training Epoch: 2/2, step 413/7134 completed (loss: 0.13253049552440643, acc: 0.9685863852500916)
[2025-02-13 20:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32][root][INFO] - Training Epoch: 2/2, step 414/7134 completed (loss: 0.14072436094284058, acc: 0.9595375657081604)
[2025-02-13 20:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33][root][INFO] - Training Epoch: 2/2, step 415/7134 completed (loss: 0.10557717829942703, acc: 0.976190447807312)
[2025-02-13 20:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33][root][INFO] - Training Epoch: 2/2, step 416/7134 completed (loss: 0.0776049792766571, acc: 0.9807692170143127)
[2025-02-13 20:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33][root][INFO] - Training Epoch: 2/2, step 417/7134 completed (loss: 0.14465054869651794, acc: 0.9505494236946106)
[2025-02-13 20:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34][root][INFO] - Training Epoch: 2/2, step 418/7134 completed (loss: 0.1002921536564827, acc: 0.9710144996643066)
[2025-02-13 20:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34][root][INFO] - Training Epoch: 2/2, step 419/7134 completed (loss: 0.07041622698307037, acc: 0.9900990128517151)
[2025-02-13 20:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34][root][INFO] - Training Epoch: 2/2, step 420/7134 completed (loss: 0.12712106108665466, acc: 0.949999988079071)
[2025-02-13 20:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35][root][INFO] - Training Epoch: 2/2, step 421/7134 completed (loss: 0.07327357679605484, acc: 0.9874213933944702)
[2025-02-13 20:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35][root][INFO] - Training Epoch: 2/2, step 422/7134 completed (loss: 0.08259277790784836, acc: 0.9526627063751221)
[2025-02-13 20:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36][root][INFO] - Training Epoch: 2/2, step 423/7134 completed (loss: 0.023045381531119347, acc: 1.0)
[2025-02-13 20:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36][root][INFO] - Training Epoch: 2/2, step 424/7134 completed (loss: 0.051300447434186935, acc: 0.9938271641731262)
[2025-02-13 20:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36][root][INFO] - Training Epoch: 2/2, step 425/7134 completed (loss: 0.017496369779109955, acc: 1.0)
[2025-02-13 20:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37][root][INFO] - Training Epoch: 2/2, step 426/7134 completed (loss: 0.023335224017500877, acc: 0.9941176176071167)
[2025-02-13 20:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37][root][INFO] - Training Epoch: 2/2, step 427/7134 completed (loss: 0.04130847379565239, acc: 1.0)
[2025-02-13 20:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38][root][INFO] - Training Epoch: 2/2, step 428/7134 completed (loss: 0.038934942334890366, acc: 0.9940119981765747)
[2025-02-13 20:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38][root][INFO] - Training Epoch: 2/2, step 429/7134 completed (loss: 0.17736367881298065, acc: 0.9407894611358643)
[2025-02-13 20:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38][root][INFO] - Training Epoch: 2/2, step 430/7134 completed (loss: 0.06554052978754044, acc: 0.9882352948188782)
[2025-02-13 20:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39][root][INFO] - Training Epoch: 2/2, step 431/7134 completed (loss: 0.09259849786758423, acc: 0.9940828680992126)
[2025-02-13 20:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39][root][INFO] - Training Epoch: 2/2, step 432/7134 completed (loss: 0.10398252308368683, acc: 0.9772727489471436)
[2025-02-13 20:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39][root][INFO] - Training Epoch: 2/2, step 433/7134 completed (loss: 0.10563378781080246, acc: 0.978723406791687)
[2025-02-13 20:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40][root][INFO] - Training Epoch: 2/2, step 434/7134 completed (loss: 0.06815174967050552, acc: 0.9938271641731262)
[2025-02-13 20:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40][root][INFO] - Training Epoch: 2/2, step 435/7134 completed (loss: 0.06745489686727524, acc: 0.9722222089767456)
[2025-02-13 20:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41][root][INFO] - Training Epoch: 2/2, step 436/7134 completed (loss: 0.13073840737342834, acc: 0.9791666865348816)
[2025-02-13 20:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41][root][INFO] - Training Epoch: 2/2, step 437/7134 completed (loss: 0.16767188906669617, acc: 0.9718309640884399)
[2025-02-13 20:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41][root][INFO] - Training Epoch: 2/2, step 438/7134 completed (loss: 0.04323667660355568, acc: 0.9921259880065918)
[2025-02-13 20:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42][root][INFO] - Training Epoch: 2/2, step 439/7134 completed (loss: 0.05664704740047455, acc: 0.9885057210922241)
[2025-02-13 20:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42][root][INFO] - Training Epoch: 2/2, step 440/7134 completed (loss: 0.13281847536563873, acc: 0.9538461565971375)
[2025-02-13 20:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43][root][INFO] - Training Epoch: 2/2, step 441/7134 completed (loss: 0.05221012607216835, acc: 0.9931507110595703)
[2025-02-13 20:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43][root][INFO] - Training Epoch: 2/2, step 442/7134 completed (loss: 0.01634170114994049, acc: 1.0)
[2025-02-13 20:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43][root][INFO] - Training Epoch: 2/2, step 443/7134 completed (loss: 0.021639840677380562, acc: 0.9931972622871399)
[2025-02-13 20:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44][root][INFO] - Training Epoch: 2/2, step 444/7134 completed (loss: 0.02522789128124714, acc: 0.9922480583190918)
[2025-02-13 20:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44][root][INFO] - Training Epoch: 2/2, step 445/7134 completed (loss: 0.14579494297504425, acc: 0.9622641801834106)
[2025-02-13 20:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45][root][INFO] - Training Epoch: 2/2, step 446/7134 completed (loss: 0.054387759417295456, acc: 0.9881656765937805)
[2025-02-13 20:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45][root][INFO] - Training Epoch: 2/2, step 447/7134 completed (loss: 0.04450463876128197, acc: 0.987730085849762)
[2025-02-13 20:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45][root][INFO] - Training Epoch: 2/2, step 448/7134 completed (loss: 0.15437237918376923, acc: 0.9682539701461792)
[2025-02-13 20:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46][root][INFO] - Training Epoch: 2/2, step 449/7134 completed (loss: 0.17203393578529358, acc: 0.9468085169792175)
[2025-02-13 20:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46][root][INFO] - Training Epoch: 2/2, step 450/7134 completed (loss: 0.0999617874622345, acc: 0.9764150977134705)
[2025-02-13 20:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46][root][INFO] - Training Epoch: 2/2, step 451/7134 completed (loss: 0.2648794949054718, acc: 0.9364407062530518)
[2025-02-13 20:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47][root][INFO] - Training Epoch: 2/2, step 452/7134 completed (loss: 0.2581627368927002, acc: 0.9213483333587646)
[2025-02-13 20:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47][root][INFO] - Training Epoch: 2/2, step 453/7134 completed (loss: 0.117888905107975, acc: 0.9716312289237976)
[2025-02-13 20:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48][root][INFO] - Training Epoch: 2/2, step 454/7134 completed (loss: 0.23681002855300903, acc: 0.949367105960846)
[2025-02-13 20:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48][root][INFO] - Training Epoch: 2/2, step 455/7134 completed (loss: 0.20656555891036987, acc: 0.9487179517745972)
[2025-02-13 20:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48][root][INFO] - Training Epoch: 2/2, step 456/7134 completed (loss: 0.082065649330616, acc: 0.9693251252174377)
[2025-02-13 20:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49][root][INFO] - Training Epoch: 2/2, step 457/7134 completed (loss: 0.21181148290634155, acc: 0.9655172228813171)
[2025-02-13 20:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49][root][INFO] - Training Epoch: 2/2, step 458/7134 completed (loss: 0.1350676715373993, acc: 0.9596773982048035)
[2025-02-13 20:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49][root][INFO] - Training Epoch: 2/2, step 459/7134 completed (loss: 0.1805286705493927, acc: 0.9397590160369873)
[2025-02-13 20:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50][root][INFO] - Training Epoch: 2/2, step 460/7134 completed (loss: 0.15238739550113678, acc: 0.9466666579246521)
[2025-02-13 20:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50][root][INFO] - Training Epoch: 2/2, step 461/7134 completed (loss: 0.17870208621025085, acc: 0.9647058844566345)
[2025-02-13 20:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51][root][INFO] - Training Epoch: 2/2, step 462/7134 completed (loss: 0.12071940302848816, acc: 0.9814814925193787)
[2025-02-13 20:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51][root][INFO] - Training Epoch: 2/2, step 463/7134 completed (loss: 0.25289636850357056, acc: 0.9402984976768494)
[2025-02-13 20:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51][root][INFO] - Training Epoch: 2/2, step 464/7134 completed (loss: 0.14251889288425446, acc: 0.9715909361839294)
[2025-02-13 20:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52][root][INFO] - Training Epoch: 2/2, step 465/7134 completed (loss: 0.24284891784191132, acc: 0.9608938694000244)
[2025-02-13 20:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52][root][INFO] - Training Epoch: 2/2, step 466/7134 completed (loss: 0.21850813925266266, acc: 0.9532163739204407)
[2025-02-13 20:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53][root][INFO] - Training Epoch: 2/2, step 467/7134 completed (loss: 0.19235700368881226, acc: 0.9523809552192688)
[2025-02-13 20:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53][root][INFO] - Training Epoch: 2/2, step 468/7134 completed (loss: 0.21285782754421234, acc: 0.9466666579246521)
[2025-02-13 20:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53][root][INFO] - Training Epoch: 2/2, step 469/7134 completed (loss: 0.2519063949584961, acc: 0.9444444179534912)
[2025-02-13 20:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54][root][INFO] - Training Epoch: 2/2, step 470/7134 completed (loss: 0.18657450377941132, acc: 0.9594594836235046)
[2025-02-13 20:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54][root][INFO] - Training Epoch: 2/2, step 471/7134 completed (loss: 0.2019256204366684, acc: 0.9670329689979553)
[2025-02-13 20:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54][root][INFO] - Training Epoch: 2/2, step 472/7134 completed (loss: 0.1496819257736206, acc: 0.9599999785423279)
[2025-02-13 20:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:55][root][INFO] - Training Epoch: 2/2, step 473/7134 completed (loss: 0.15715622901916504, acc: 0.9772727489471436)
[2025-02-13 20:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:55][root][INFO] - Training Epoch: 2/2, step 474/7134 completed (loss: 0.19612117111682892, acc: 0.9515151381492615)
[2025-02-13 20:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56][root][INFO] - Training Epoch: 2/2, step 475/7134 completed (loss: 0.2583417594432831, acc: 0.9440559148788452)
[2025-02-13 20:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56][root][INFO] - Training Epoch: 2/2, step 476/7134 completed (loss: 0.22076353430747986, acc: 0.956250011920929)
[2025-02-13 20:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56][root][INFO] - Training Epoch: 2/2, step 477/7134 completed (loss: 0.20938600599765778, acc: 0.9465240836143494)
[2025-02-13 20:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57][root][INFO] - Training Epoch: 2/2, step 478/7134 completed (loss: 0.3230658173561096, acc: 0.9304347634315491)
[2025-02-13 20:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57][root][INFO] - Training Epoch: 2/2, step 479/7134 completed (loss: 0.2009543925523758, acc: 0.9470198750495911)
[2025-02-13 20:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57][root][INFO] - Training Epoch: 2/2, step 480/7134 completed (loss: 0.12646590173244476, acc: 0.9624999761581421)
[2025-02-13 20:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58][root][INFO] - Training Epoch: 2/2, step 481/7134 completed (loss: 0.09982086718082428, acc: 0.9813664555549622)
[2025-02-13 20:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58][root][INFO] - Training Epoch: 2/2, step 482/7134 completed (loss: 0.11624782532453537, acc: 0.9622641801834106)
[2025-02-13 20:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59][root][INFO] - Training Epoch: 2/2, step 483/7134 completed (loss: 0.10190049558877945, acc: 0.9615384340286255)
[2025-02-13 20:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59][root][INFO] - Training Epoch: 2/2, step 484/7134 completed (loss: 0.22759774327278137, acc: 0.9387755393981934)
[2025-02-13 20:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59][root][INFO] - Training Epoch: 2/2, step 485/7134 completed (loss: 0.3134891390800476, acc: 0.9130434989929199)
[2025-02-13 20:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00][root][INFO] - Training Epoch: 2/2, step 486/7134 completed (loss: 0.16843266785144806, acc: 0.9612902998924255)
[2025-02-13 20:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00][root][INFO] - Training Epoch: 2/2, step 487/7134 completed (loss: 0.1710902750492096, acc: 0.9753086566925049)
[2025-02-13 20:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00][root][INFO] - Training Epoch: 2/2, step 488/7134 completed (loss: 0.30241963267326355, acc: 0.9552238583564758)
[2025-02-13 20:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01][root][INFO] - Training Epoch: 2/2, step 489/7134 completed (loss: 0.4555690884590149, acc: 0.935251772403717)
[2025-02-13 20:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01][root][INFO] - Training Epoch: 2/2, step 490/7134 completed (loss: 0.13606634736061096, acc: 0.9810126423835754)
[2025-02-13 20:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02][root][INFO] - Training Epoch: 2/2, step 491/7134 completed (loss: 0.11356372386217117, acc: 0.9722222089767456)
[2025-02-13 20:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02][root][INFO] - Training Epoch: 2/2, step 492/7134 completed (loss: 0.05359647795557976, acc: 0.9935483932495117)
[2025-02-13 20:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02][root][INFO] - Training Epoch: 2/2, step 493/7134 completed (loss: 0.08234728872776031, acc: 0.9881656765937805)
[2025-02-13 20:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03][root][INFO] - Training Epoch: 2/2, step 494/7134 completed (loss: 0.04141125828027725, acc: 0.9866666793823242)
[2025-02-13 20:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03][root][INFO] - Training Epoch: 2/2, step 495/7134 completed (loss: 0.1132110059261322, acc: 0.9834710955619812)
[2025-02-13 20:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04][root][INFO] - Training Epoch: 2/2, step 496/7134 completed (loss: 0.11493553221225739, acc: 0.9684210419654846)
[2025-02-13 20:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04][root][INFO] - Training Epoch: 2/2, step 497/7134 completed (loss: 0.1169842854142189, acc: 0.9615384340286255)
[2025-02-13 20:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04][root][INFO] - Training Epoch: 2/2, step 498/7134 completed (loss: 0.07548098266124725, acc: 0.9932885766029358)
[2025-02-13 20:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05][root][INFO] - Training Epoch: 2/2, step 499/7134 completed (loss: 0.0735817551612854, acc: 0.9764705896377563)
[2025-02-13 20:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05][root][INFO] - Training Epoch: 2/2, step 500/7134 completed (loss: 0.09230447560548782, acc: 0.9651162624359131)
[2025-02-13 20:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06][root][INFO] - Training Epoch: 2/2, step 501/7134 completed (loss: 0.07949007302522659, acc: 0.9826589822769165)
[2025-02-13 20:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06][root][INFO] - Training Epoch: 2/2, step 502/7134 completed (loss: 0.05904625728726387, acc: 0.9935064911842346)
[2025-02-13 20:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06][root][INFO] - Training Epoch: 2/2, step 503/7134 completed (loss: 0.12637081742286682, acc: 0.9772727489471436)
[2025-02-13 20:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07][root][INFO] - Training Epoch: 2/2, step 504/7134 completed (loss: 0.08517204970121384, acc: 0.9875776171684265)
[2025-02-13 20:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07][root][INFO] - Training Epoch: 2/2, step 505/7134 completed (loss: 0.058012086898088455, acc: 0.988095223903656)
[2025-02-13 20:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08][root][INFO] - Training Epoch: 2/2, step 506/7134 completed (loss: 0.07370208203792572, acc: 0.9829545617103577)
[2025-02-13 20:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08][root][INFO] - Training Epoch: 2/2, step 507/7134 completed (loss: 0.08456495404243469, acc: 0.9836065769195557)
[2025-02-13 20:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08][root][INFO] - Training Epoch: 2/2, step 508/7134 completed (loss: 0.13234208524227142, acc: 0.9647058844566345)
[2025-02-13 20:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09][root][INFO] - Training Epoch: 2/2, step 509/7134 completed (loss: 0.12679821252822876, acc: 0.9594594836235046)
[2025-02-13 20:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09][root][INFO] - Training Epoch: 2/2, step 510/7134 completed (loss: 0.3671860694885254, acc: 0.934959352016449)
[2025-02-13 20:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09][root][INFO] - Training Epoch: 2/2, step 511/7134 completed (loss: 0.17806456983089447, acc: 0.9428571462631226)
[2025-02-13 20:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10][root][INFO] - Training Epoch: 2/2, step 512/7134 completed (loss: 0.2790123522281647, acc: 0.9389312863349915)
[2025-02-13 20:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10][root][INFO] - Training Epoch: 2/2, step 513/7134 completed (loss: 0.19720973074436188, acc: 0.9731543660163879)
[2025-02-13 20:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11][root][INFO] - Training Epoch: 2/2, step 514/7134 completed (loss: 0.31479403376579285, acc: 0.9166666865348816)
[2025-02-13 20:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11][root][INFO] - Training Epoch: 2/2, step 515/7134 completed (loss: 0.11540400236845016, acc: 0.9583333134651184)
[2025-02-13 20:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11][root][INFO] - Training Epoch: 2/2, step 516/7134 completed (loss: 0.1657598614692688, acc: 0.9491525292396545)
[2025-02-13 20:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12][root][INFO] - Training Epoch: 2/2, step 517/7134 completed (loss: 0.22639811038970947, acc: 0.9637681245803833)
[2025-02-13 20:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12][root][INFO] - Training Epoch: 2/2, step 518/7134 completed (loss: 0.19444240629673004, acc: 0.9801324605941772)
[2025-02-13 20:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13][root][INFO] - Training Epoch: 2/2, step 519/7134 completed (loss: 0.07828164845705032, acc: 0.9739130139350891)
[2025-02-13 20:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13][root][INFO] - Training Epoch: 2/2, step 520/7134 completed (loss: 0.06431257724761963, acc: 0.9736841917037964)
[2025-02-13 20:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13][root][INFO] - Training Epoch: 2/2, step 521/7134 completed (loss: 0.11816512048244476, acc: 0.9824561476707458)
[2025-02-13 20:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14][root][INFO] - Training Epoch: 2/2, step 522/7134 completed (loss: 0.08034798502922058, acc: 0.983146071434021)
[2025-02-13 20:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14][root][INFO] - Training Epoch: 2/2, step 523/7134 completed (loss: 0.06546653807163239, acc: 0.9825581312179565)
[2025-02-13 20:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14][root][INFO] - Training Epoch: 2/2, step 524/7134 completed (loss: 0.1277257800102234, acc: 0.9717513918876648)
[2025-02-13 20:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15][root][INFO] - Training Epoch: 2/2, step 525/7134 completed (loss: 0.12355247139930725, acc: 0.9815950989723206)
[2025-02-13 20:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15][root][INFO] - Training Epoch: 2/2, step 526/7134 completed (loss: 0.1254693865776062, acc: 0.9659090638160706)
[2025-02-13 20:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15][root][INFO] - Training Epoch: 2/2, step 527/7134 completed (loss: 0.15634271502494812, acc: 0.984375)
[2025-02-13 20:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16][root][INFO] - Training Epoch: 2/2, step 528/7134 completed (loss: 0.05026436969637871, acc: 0.982758641242981)
[2025-02-13 20:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16][root][INFO] - Training Epoch: 2/2, step 529/7134 completed (loss: 0.1707002967596054, acc: 0.9426751732826233)
[2025-02-13 20:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17][root][INFO] - Training Epoch: 2/2, step 530/7134 completed (loss: 0.1639164239168167, acc: 0.9538461565971375)
[2025-02-13 20:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17][root][INFO] - Training Epoch: 2/2, step 531/7134 completed (loss: 0.264803946018219, acc: 0.9452054500579834)
[2025-02-13 20:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17][root][INFO] - Training Epoch: 2/2, step 532/7134 completed (loss: 0.02137967385351658, acc: 1.0)
[2025-02-13 20:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18][root][INFO] - Training Epoch: 2/2, step 533/7134 completed (loss: 0.3471096456050873, acc: 0.9047619104385376)
[2025-02-13 20:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18][root][INFO] - Training Epoch: 2/2, step 534/7134 completed (loss: 0.07536540180444717, acc: 0.9880239367485046)
[2025-02-13 20:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19][root][INFO] - Training Epoch: 2/2, step 535/7134 completed (loss: 0.08445774018764496, acc: 0.9781022071838379)
[2025-02-13 20:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19][root][INFO] - Training Epoch: 2/2, step 536/7134 completed (loss: 0.06208360567688942, acc: 0.9939024448394775)
[2025-02-13 20:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19][root][INFO] - Training Epoch: 2/2, step 537/7134 completed (loss: 0.09610621631145477, acc: 0.9768785834312439)
[2025-02-13 20:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:20][root][INFO] - Training Epoch: 2/2, step 538/7134 completed (loss: 0.08700495958328247, acc: 0.9759036302566528)
[2025-02-13 20:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:20][root][INFO] - Training Epoch: 2/2, step 539/7134 completed (loss: 0.12018635869026184, acc: 0.9694656729698181)
[2025-02-13 20:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21][root][INFO] - Training Epoch: 2/2, step 540/7134 completed (loss: 0.1340097337961197, acc: 0.9622641801834106)
[2025-02-13 20:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21][root][INFO] - Training Epoch: 2/2, step 541/7134 completed (loss: 0.2297222763299942, acc: 0.9404761791229248)
[2025-02-13 20:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21][root][INFO] - Training Epoch: 2/2, step 542/7134 completed (loss: 0.29253172874450684, acc: 0.9599999785423279)
[2025-02-13 20:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22][root][INFO] - Training Epoch: 2/2, step 543/7134 completed (loss: 0.3035285770893097, acc: 0.9276315569877625)
[2025-02-13 20:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22][root][INFO] - Training Epoch: 2/2, step 544/7134 completed (loss: 0.17234008014202118, acc: 0.9642857313156128)
[2025-02-13 20:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23][root][INFO] - Training Epoch: 2/2, step 545/7134 completed (loss: 0.3190200924873352, acc: 0.9398496150970459)
[2025-02-13 20:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23][root][INFO] - Training Epoch: 2/2, step 546/7134 completed (loss: 0.3261515200138092, acc: 0.9074074029922485)
[2025-02-13 20:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23][root][INFO] - Training Epoch: 2/2, step 547/7134 completed (loss: 0.13689184188842773, acc: 0.9801324605941772)
[2025-02-13 20:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24][root][INFO] - Training Epoch: 2/2, step 548/7134 completed (loss: 0.1038074642419815, acc: 0.9802631735801697)
[2025-02-13 20:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24][root][INFO] - Training Epoch: 2/2, step 549/7134 completed (loss: 0.3277418315410614, acc: 0.9142857193946838)
[2025-02-13 20:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25][root][INFO] - Training Epoch: 2/2, step 550/7134 completed (loss: 0.14983409643173218, acc: 0.960629940032959)
[2025-02-13 20:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25][root][INFO] - Training Epoch: 2/2, step 551/7134 completed (loss: 0.1960369348526001, acc: 0.96875)
[2025-02-13 20:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25][root][INFO] - Training Epoch: 2/2, step 552/7134 completed (loss: 0.10257631540298462, acc: 0.9727891087532043)
[2025-02-13 20:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26][root][INFO] - Training Epoch: 2/2, step 553/7134 completed (loss: 0.15882080793380737, acc: 0.9578947424888611)
[2025-02-13 20:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26][root][INFO] - Training Epoch: 2/2, step 554/7134 completed (loss: 0.19756169617176056, acc: 0.9617834687232971)
[2025-02-13 20:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26][root][INFO] - Training Epoch: 2/2, step 555/7134 completed (loss: 0.3006752133369446, acc: 0.9220778942108154)
[2025-02-13 20:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27][root][INFO] - Training Epoch: 2/2, step 556/7134 completed (loss: 0.12595604360103607, acc: 0.96875)
[2025-02-13 20:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27][root][INFO] - Training Epoch: 2/2, step 557/7134 completed (loss: 0.18206821382045746, acc: 0.9503546357154846)
[2025-02-13 20:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28][root][INFO] - Training Epoch: 2/2, step 558/7134 completed (loss: 0.2556053698062897, acc: 0.9679487347602844)
[2025-02-13 20:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28][root][INFO] - Training Epoch: 2/2, step 559/7134 completed (loss: 0.24230916798114777, acc: 0.926174521446228)
[2025-02-13 20:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28][root][INFO] - Training Epoch: 2/2, step 560/7134 completed (loss: 0.05551542714238167, acc: 0.9837398529052734)
[2025-02-13 20:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:29][root][INFO] - Training Epoch: 2/2, step 561/7134 completed (loss: 0.2029167115688324, acc: 0.9437500238418579)
[2025-02-13 20:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:29][root][INFO] - Training Epoch: 2/2, step 562/7134 completed (loss: 0.10764380544424057, acc: 0.9740259647369385)
[2025-02-13 20:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30][root][INFO] - Training Epoch: 2/2, step 563/7134 completed (loss: 0.37863776087760925, acc: 0.9020978808403015)
[2025-02-13 20:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30][root][INFO] - Training Epoch: 2/2, step 564/7134 completed (loss: 0.2238633930683136, acc: 0.9473684430122375)
[2025-02-13 20:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30][root][INFO] - Training Epoch: 2/2, step 565/7134 completed (loss: 0.12263017147779465, acc: 0.960629940032959)
[2025-02-13 20:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31][root][INFO] - Training Epoch: 2/2, step 566/7134 completed (loss: 0.1647178828716278, acc: 0.9729729890823364)
[2025-02-13 20:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31][root][INFO] - Training Epoch: 2/2, step 567/7134 completed (loss: 0.19493107497692108, acc: 0.940119743347168)
[2025-02-13 20:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32][root][INFO] - Training Epoch: 2/2, step 568/7134 completed (loss: 0.1847875714302063, acc: 0.9578947424888611)
[2025-02-13 20:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32][root][INFO] - Training Epoch: 2/2, step 569/7134 completed (loss: 0.12049629539251328, acc: 0.9776536226272583)
[2025-02-13 20:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32][root][INFO] - Training Epoch: 2/2, step 570/7134 completed (loss: 0.19735415279865265, acc: 0.9402984976768494)
[2025-02-13 20:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33][root][INFO] - Training Epoch: 2/2, step 571/7134 completed (loss: 0.07709775120019913, acc: 0.9835164546966553)
[2025-02-13 20:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33][root][INFO] - Training Epoch: 2/2, step 572/7134 completed (loss: 0.08054255694150925, acc: 0.9828571677207947)
[2025-02-13 20:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34][root][INFO] - Training Epoch: 2/2, step 573/7134 completed (loss: 0.12594519555568695, acc: 0.9757575988769531)
[2025-02-13 20:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34][root][INFO] - Training Epoch: 2/2, step 574/7134 completed (loss: 0.07361564040184021, acc: 0.9881656765937805)
[2025-02-13 20:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34][root][INFO] - Training Epoch: 2/2, step 575/7134 completed (loss: 0.09706389904022217, acc: 0.9707602262496948)
[2025-02-13 20:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35][root][INFO] - Training Epoch: 2/2, step 576/7134 completed (loss: 0.08080818504095078, acc: 0.9838709831237793)
[2025-02-13 20:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35][root][INFO] - Training Epoch: 2/2, step 577/7134 completed (loss: 0.13960987329483032, acc: 0.9454545378684998)
[2025-02-13 20:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36][root][INFO] - Training Epoch: 2/2, step 578/7134 completed (loss: 0.11092791706323624, acc: 0.9751243591308594)
[2025-02-13 20:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36][root][INFO] - Training Epoch: 2/2, step 579/7134 completed (loss: 0.15588682889938354, acc: 0.9731183052062988)
[2025-02-13 20:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36][root][INFO] - Training Epoch: 2/2, step 580/7134 completed (loss: 0.3603293001651764, acc: 0.9388889074325562)
[2025-02-13 20:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37][root][INFO] - Training Epoch: 2/2, step 581/7134 completed (loss: 0.06590677797794342, acc: 0.9934210777282715)
[2025-02-13 20:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37][root][INFO] - Training Epoch: 2/2, step 582/7134 completed (loss: 0.10860180854797363, acc: 0.97826087474823)
[2025-02-13 20:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38][root][INFO] - Training Epoch: 2/2, step 583/7134 completed (loss: 0.08392449468374252, acc: 0.9820359349250793)
[2025-02-13 20:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38][root][INFO] - Training Epoch: 2/2, step 584/7134 completed (loss: 0.05671695992350578, acc: 0.9836065769195557)
[2025-02-13 20:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38][root][INFO] - Training Epoch: 2/2, step 585/7134 completed (loss: 0.08066824078559875, acc: 0.9729729890823364)
[2025-02-13 20:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39][root][INFO] - Training Epoch: 2/2, step 586/7134 completed (loss: 0.0746978297829628, acc: 0.9734042286872864)
[2025-02-13 20:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39][root][INFO] - Training Epoch: 2/2, step 587/7134 completed (loss: 0.0739167258143425, acc: 0.9779005646705627)
[2025-02-13 20:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40][root][INFO] - Training Epoch: 2/2, step 588/7134 completed (loss: 0.06416556984186172, acc: 0.9797297120094299)
[2025-02-13 20:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40][root][INFO] - Training Epoch: 2/2, step 589/7134 completed (loss: 0.12323024123907089, acc: 0.9693251252174377)
[2025-02-13 20:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40][root][INFO] - Training Epoch: 2/2, step 590/7134 completed (loss: 0.0967748835682869, acc: 0.9735099077224731)
[2025-02-13 20:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41][root][INFO] - Training Epoch: 2/2, step 591/7134 completed (loss: 0.08960124105215073, acc: 0.994350254535675)
[2025-02-13 20:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41][root][INFO] - Training Epoch: 2/2, step 592/7134 completed (loss: 0.0620562806725502, acc: 0.9886363744735718)
[2025-02-13 20:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42][root][INFO] - Training Epoch: 2/2, step 593/7134 completed (loss: 0.09806248545646667, acc: 0.9649122953414917)
[2025-02-13 20:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42][root][INFO] - Training Epoch: 2/2, step 594/7134 completed (loss: 0.18586833775043488, acc: 0.9640718698501587)
[2025-02-13 20:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42][root][INFO] - Training Epoch: 2/2, step 595/7134 completed (loss: 0.1212722435593605, acc: 0.9784946441650391)
[2025-02-13 20:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43][root][INFO] - Training Epoch: 2/2, step 596/7134 completed (loss: 0.08432134985923767, acc: 0.9833333492279053)
[2025-02-13 20:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43][root][INFO] - Training Epoch: 2/2, step 597/7134 completed (loss: 0.1175713986158371, acc: 0.9664804339408875)
[2025-02-13 20:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44][root][INFO] - Training Epoch: 2/2, step 598/7134 completed (loss: 0.21617630124092102, acc: 0.9510869383811951)
[2025-02-13 20:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44][root][INFO] - Training Epoch: 2/2, step 599/7134 completed (loss: 0.11497162282466888, acc: 0.9838709831237793)
[2025-02-13 20:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44][root][INFO] - Training Epoch: 2/2, step 600/7134 completed (loss: 0.0942508801817894, acc: 0.9672130942344666)
[2025-02-13 20:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45][root][INFO] - Training Epoch: 2/2, step 601/7134 completed (loss: 0.13063067197799683, acc: 0.9659090638160706)
[2025-02-13 20:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45][root][INFO] - Training Epoch: 2/2, step 602/7134 completed (loss: 0.11931660771369934, acc: 0.9567567706108093)
[2025-02-13 20:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46][root][INFO] - Training Epoch: 2/2, step 603/7134 completed (loss: 0.18746596574783325, acc: 0.9470899701118469)
[2025-02-13 20:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46][root][INFO] - Training Epoch: 2/2, step 604/7134 completed (loss: 0.1405591517686844, acc: 0.9696969985961914)
[2025-02-13 20:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46][root][INFO] - Training Epoch: 2/2, step 605/7134 completed (loss: 0.08105319738388062, acc: 0.9839572310447693)
[2025-02-13 20:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47][root][INFO] - Training Epoch: 2/2, step 606/7134 completed (loss: 0.13388952612876892, acc: 0.9649999737739563)
[2025-02-13 20:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47][root][INFO] - Training Epoch: 2/2, step 607/7134 completed (loss: 0.07258236408233643, acc: 0.9712643623352051)
[2025-02-13 20:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48][root][INFO] - Training Epoch: 2/2, step 608/7134 completed (loss: 0.06363168358802795, acc: 0.9783783555030823)
[2025-02-13 20:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48][root][INFO] - Training Epoch: 2/2, step 609/7134 completed (loss: 0.10330548137426376, acc: 0.9846153855323792)
[2025-02-13 20:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48][root][INFO] - Training Epoch: 2/2, step 610/7134 completed (loss: 0.057333528995513916, acc: 0.9852216839790344)
[2025-02-13 20:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49][root][INFO] - Training Epoch: 2/2, step 611/7134 completed (loss: 0.12829537689685822, acc: 0.9661017060279846)
[2025-02-13 20:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49][root][INFO] - Training Epoch: 2/2, step 612/7134 completed (loss: 0.16448037326335907, acc: 0.9619565010070801)
[2025-02-13 20:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50][root][INFO] - Training Epoch: 2/2, step 613/7134 completed (loss: 0.1089840829372406, acc: 0.9646464586257935)
[2025-02-13 20:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50][root][INFO] - Training Epoch: 2/2, step 614/7134 completed (loss: 0.02537282183766365, acc: 1.0)
[2025-02-13 20:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50][root][INFO] - Training Epoch: 2/2, step 615/7134 completed (loss: 0.044801078736782074, acc: 0.9894737005233765)
[2025-02-13 20:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:51][root][INFO] - Training Epoch: 2/2, step 616/7134 completed (loss: 0.06570760905742645, acc: 0.9723502397537231)
[2025-02-13 20:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:51][root][INFO] - Training Epoch: 2/2, step 617/7134 completed (loss: 0.07983456552028656, acc: 0.980861246585846)
[2025-02-13 20:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52][root][INFO] - Training Epoch: 2/2, step 618/7134 completed (loss: 0.046896591782569885, acc: 0.9946523904800415)
[2025-02-13 20:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52][root][INFO] - Training Epoch: 2/2, step 619/7134 completed (loss: 0.11985576897859573, acc: 0.9754098653793335)
[2025-02-13 20:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52][root][INFO] - Training Epoch: 2/2, step 620/7134 completed (loss: 0.10803860425949097, acc: 0.9794520735740662)
[2025-02-13 20:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53][root][INFO] - Training Epoch: 2/2, step 621/7134 completed (loss: 0.1739863008260727, acc: 0.95333331823349)
[2025-02-13 20:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53][root][INFO] - Training Epoch: 2/2, step 622/7134 completed (loss: 0.10032077878713608, acc: 0.9736841917037964)
[2025-02-13 20:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53][root][INFO] - Training Epoch: 2/2, step 623/7134 completed (loss: 0.35144880414009094, acc: 0.935251772403717)
[2025-02-13 20:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54][root][INFO] - Training Epoch: 2/2, step 624/7134 completed (loss: 0.06850520521402359, acc: 0.9927007555961609)
[2025-02-13 20:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54][root][INFO] - Training Epoch: 2/2, step 625/7134 completed (loss: 0.17553693056106567, acc: 0.949999988079071)
[2025-02-13 20:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55][root][INFO] - Training Epoch: 2/2, step 626/7134 completed (loss: 0.1280466914176941, acc: 0.9760000109672546)
[2025-02-13 20:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55][root][INFO] - Training Epoch: 2/2, step 627/7134 completed (loss: 0.14333680272102356, acc: 0.9863945841789246)
[2025-02-13 20:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55][root][INFO] - Training Epoch: 2/2, step 628/7134 completed (loss: 0.04674888774752617, acc: 1.0)
[2025-02-13 20:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56][root][INFO] - Training Epoch: 2/2, step 629/7134 completed (loss: 0.23533755540847778, acc: 0.957446813583374)
[2025-02-13 20:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56][root][INFO] - Training Epoch: 2/2, step 630/7134 completed (loss: 0.226899191737175, acc: 0.9299362897872925)
[2025-02-13 20:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57][root][INFO] - Training Epoch: 2/2, step 631/7134 completed (loss: 0.11197792738676071, acc: 0.9610389471054077)
[2025-02-13 20:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57][root][INFO] - Training Epoch: 2/2, step 632/7134 completed (loss: 0.06811212748289108, acc: 0.98591548204422)
[2025-02-13 20:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57][root][INFO] - Training Epoch: 2/2, step 633/7134 completed (loss: 0.056782376021146774, acc: 0.987500011920929)
[2025-02-13 20:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58][root][INFO] - Training Epoch: 2/2, step 634/7134 completed (loss: 0.09978292882442474, acc: 0.9740259647369385)
[2025-02-13 20:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58][root][INFO] - Training Epoch: 2/2, step 635/7134 completed (loss: 0.11893671005964279, acc: 0.9545454382896423)
[2025-02-13 20:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58][root][INFO] - Training Epoch: 2/2, step 636/7134 completed (loss: 0.11320306360721588, acc: 0.9820359349250793)
[2025-02-13 20:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:59][root][INFO] - Training Epoch: 2/2, step 637/7134 completed (loss: 0.09573729336261749, acc: 0.971222996711731)
[2025-02-13 20:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:59][root][INFO] - Training Epoch: 2/2, step 638/7134 completed (loss: 0.06318072229623795, acc: 0.9865771532058716)
[2025-02-13 20:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00][root][INFO] - Training Epoch: 2/2, step 639/7134 completed (loss: 0.06022753193974495, acc: 0.9857142567634583)
[2025-02-13 20:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00][root][INFO] - Training Epoch: 2/2, step 640/7134 completed (loss: 0.1833924949169159, acc: 0.9710144996643066)
[2025-02-13 20:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00][root][INFO] - Training Epoch: 2/2, step 641/7134 completed (loss: 0.12204564362764359, acc: 0.9637681245803833)
[2025-02-13 20:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:01][root][INFO] - Training Epoch: 2/2, step 642/7134 completed (loss: 0.08092659711837769, acc: 0.9779411554336548)
[2025-02-13 20:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:01][root][INFO] - Training Epoch: 2/2, step 643/7134 completed (loss: 0.06743736565113068, acc: 0.983146071434021)
[2025-02-13 20:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02][root][INFO] - Training Epoch: 2/2, step 644/7134 completed (loss: 0.04912359267473221, acc: 0.984375)
[2025-02-13 20:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02][root][INFO] - Training Epoch: 2/2, step 645/7134 completed (loss: 0.04749387502670288, acc: 0.9793103337287903)
[2025-02-13 20:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02][root][INFO] - Training Epoch: 2/2, step 646/7134 completed (loss: 0.10510629415512085, acc: 0.9810126423835754)
[2025-02-13 20:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03][root][INFO] - Training Epoch: 2/2, step 647/7134 completed (loss: 0.01992672123014927, acc: 0.9925925731658936)
[2025-02-13 20:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03][root][INFO] - Training Epoch: 2/2, step 648/7134 completed (loss: 0.2153073102235794, acc: 0.9461538195610046)
[2025-02-13 20:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04][root][INFO] - Training Epoch: 2/2, step 649/7134 completed (loss: 0.21034042537212372, acc: 0.9351351261138916)
[2025-02-13 20:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04][root][INFO] - Training Epoch: 2/2, step 650/7134 completed (loss: 0.2043028473854065, acc: 0.9452054500579834)
[2025-02-13 20:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04][root][INFO] - Training Epoch: 2/2, step 651/7134 completed (loss: 0.16962580382823944, acc: 0.9675324559211731)
[2025-02-13 20:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05][root][INFO] - Training Epoch: 2/2, step 652/7134 completed (loss: 0.12122812122106552, acc: 0.9647887349128723)
[2025-02-13 20:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05][root][INFO] - Training Epoch: 2/2, step 653/7134 completed (loss: 0.042552463710308075, acc: 1.0)
[2025-02-13 20:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05][root][INFO] - Training Epoch: 2/2, step 654/7134 completed (loss: 0.23735634982585907, acc: 0.9774436354637146)
[2025-02-13 20:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06][root][INFO] - Training Epoch: 2/2, step 655/7134 completed (loss: 0.05815839394927025, acc: 0.982758641242981)
[2025-02-13 20:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06][root][INFO] - Training Epoch: 2/2, step 656/7134 completed (loss: 0.15192951261997223, acc: 0.939393937587738)
[2025-02-13 20:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07][root][INFO] - Training Epoch: 2/2, step 657/7134 completed (loss: 0.18066389858722687, acc: 0.9647887349128723)
[2025-02-13 20:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07][root][INFO] - Training Epoch: 2/2, step 658/7134 completed (loss: 0.09934577345848083, acc: 0.9830508232116699)
[2025-02-13 20:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07][root][INFO] - Training Epoch: 2/2, step 659/7134 completed (loss: 0.07536694407463074, acc: 0.976047933101654)
[2025-02-13 20:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08][root][INFO] - Training Epoch: 2/2, step 660/7134 completed (loss: 0.11605732887983322, acc: 0.970588207244873)
[2025-02-13 20:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08][root][INFO] - Training Epoch: 2/2, step 661/7134 completed (loss: 0.14325422048568726, acc: 0.9576271176338196)
[2025-02-13 20:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09][root][INFO] - Training Epoch: 2/2, step 662/7134 completed (loss: 0.07104989886283875, acc: 0.9833333492279053)
[2025-02-13 20:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09][root][INFO] - Training Epoch: 2/2, step 663/7134 completed (loss: 0.10557230561971664, acc: 0.9854369163513184)
[2025-02-13 20:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09][root][INFO] - Training Epoch: 2/2, step 664/7134 completed (loss: 0.15519596636295319, acc: 0.9580419659614563)
[2025-02-13 20:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10][root][INFO] - Training Epoch: 2/2, step 665/7134 completed (loss: 0.253960520029068, acc: 0.9453551769256592)
[2025-02-13 20:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10][root][INFO] - Training Epoch: 2/2, step 666/7134 completed (loss: 0.1317766159772873, acc: 0.9657142758369446)
[2025-02-13 20:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10][root][INFO] - Training Epoch: 2/2, step 667/7134 completed (loss: 0.15871767699718475, acc: 0.9640718698501587)
[2025-02-13 20:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11][root][INFO] - Training Epoch: 2/2, step 668/7134 completed (loss: 0.11688391864299774, acc: 0.9781420826911926)
[2025-02-13 20:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11][root][INFO] - Training Epoch: 2/2, step 669/7134 completed (loss: 0.1720409244298935, acc: 0.9720670580863953)
[2025-02-13 20:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12][root][INFO] - Training Epoch: 2/2, step 670/7134 completed (loss: 0.1762700378894806, acc: 0.9497717022895813)
[2025-02-13 20:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12][root][INFO] - Training Epoch: 2/2, step 671/7134 completed (loss: 0.09005893766880035, acc: 0.9773755669593811)
[2025-02-13 20:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12][root][INFO] - Training Epoch: 2/2, step 672/7134 completed (loss: 0.14808252453804016, acc: 0.9624999761581421)
[2025-02-13 20:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13][root][INFO] - Training Epoch: 2/2, step 673/7134 completed (loss: 0.1747339367866516, acc: 0.9552238583564758)
[2025-02-13 20:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13][root][INFO] - Training Epoch: 2/2, step 674/7134 completed (loss: 0.1324552744626999, acc: 0.9634146094322205)
[2025-02-13 20:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13][root][INFO] - Training Epoch: 2/2, step 675/7134 completed (loss: 0.10989260673522949, acc: 0.9723756909370422)
[2025-02-13 20:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:14][root][INFO] - Training Epoch: 2/2, step 676/7134 completed (loss: 0.07034170627593994, acc: 0.9836956262588501)
[2025-02-13 20:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:14][root][INFO] - Training Epoch: 2/2, step 677/7134 completed (loss: 0.056158993393182755, acc: 0.9894179701805115)
[2025-02-13 20:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15][root][INFO] - Training Epoch: 2/2, step 678/7134 completed (loss: 0.13967204093933105, acc: 0.9390243887901306)
[2025-02-13 20:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15][root][INFO] - Training Epoch: 2/2, step 679/7134 completed (loss: 0.04070878028869629, acc: 0.9924812316894531)
[2025-02-13 20:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15][root][INFO] - Training Epoch: 2/2, step 680/7134 completed (loss: 0.13159672915935516, acc: 0.9698492288589478)
[2025-02-13 20:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16][root][INFO] - Training Epoch: 2/2, step 681/7134 completed (loss: 0.1198643296957016, acc: 0.9736841917037964)
[2025-02-13 20:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16][root][INFO] - Training Epoch: 2/2, step 682/7134 completed (loss: 0.12541745603084564, acc: 0.9830508232116699)
[2025-02-13 20:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16][root][INFO] - Training Epoch: 2/2, step 683/7134 completed (loss: 0.09183481335639954, acc: 0.9783783555030823)
[2025-02-13 20:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17][root][INFO] - Training Epoch: 2/2, step 684/7134 completed (loss: 0.07598406821489334, acc: 0.9777777791023254)
[2025-02-13 20:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17][root][INFO] - Training Epoch: 2/2, step 685/7134 completed (loss: 0.08290491253137589, acc: 0.9748743772506714)
[2025-02-13 20:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18][root][INFO] - Training Epoch: 2/2, step 686/7134 completed (loss: 0.03122427687048912, acc: 0.994535505771637)
[2025-02-13 20:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18][root][INFO] - Training Epoch: 2/2, step 687/7134 completed (loss: 0.053284548223018646, acc: 0.9921259880065918)
[2025-02-13 20:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18][root][INFO] - Training Epoch: 2/2, step 688/7134 completed (loss: 0.021187392994761467, acc: 0.9936708807945251)
[2025-02-13 20:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19][root][INFO] - Training Epoch: 2/2, step 689/7134 completed (loss: 0.04419687017798424, acc: 0.9925925731658936)
[2025-02-13 20:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19][root][INFO] - Training Epoch: 2/2, step 690/7134 completed (loss: 0.10070536285638809, acc: 0.9677419066429138)
[2025-02-13 20:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20][root][INFO] - Training Epoch: 2/2, step 691/7134 completed (loss: 0.14746172726154327, acc: 0.9675324559211731)
[2025-02-13 20:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20][root][INFO] - Training Epoch: 2/2, step 692/7134 completed (loss: 0.1394374817609787, acc: 0.9644669890403748)
[2025-02-13 20:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20][root][INFO] - Training Epoch: 2/2, step 693/7134 completed (loss: 0.12335062026977539, acc: 0.9716312289237976)
[2025-02-13 20:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21][root][INFO] - Training Epoch: 2/2, step 694/7134 completed (loss: 0.18669018149375916, acc: 0.9757575988769531)
[2025-02-13 20:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21][root][INFO] - Training Epoch: 2/2, step 695/7134 completed (loss: 0.18161366879940033, acc: 0.9402984976768494)
[2025-02-13 20:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21][root][INFO] - Training Epoch: 2/2, step 696/7134 completed (loss: 0.13726045191287994, acc: 0.9622641801834106)
[2025-02-13 20:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22][root][INFO] - Training Epoch: 2/2, step 697/7134 completed (loss: 0.11389194428920746, acc: 0.9619565010070801)
[2025-02-13 20:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22][root][INFO] - Training Epoch: 2/2, step 698/7134 completed (loss: 0.05122552439570427, acc: 0.9848484992980957)
[2025-02-13 20:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23][root][INFO] - Training Epoch: 2/2, step 699/7134 completed (loss: 0.0882822647690773, acc: 0.9863013625144958)
[2025-02-13 20:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23][root][INFO] - Training Epoch: 2/2, step 700/7134 completed (loss: 0.13588349521160126, acc: 0.9520000219345093)
[2025-02-13 20:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23][root][INFO] - Training Epoch: 2/2, step 701/7134 completed (loss: 0.2515506148338318, acc: 0.9304347634315491)
[2025-02-13 20:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24][root][INFO] - Training Epoch: 2/2, step 702/7134 completed (loss: 0.15665054321289062, acc: 0.9536423683166504)
[2025-02-13 20:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24][root][INFO] - Training Epoch: 2/2, step 703/7134 completed (loss: 0.1834137737751007, acc: 0.9527027010917664)
[2025-02-13 20:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24][root][INFO] - Training Epoch: 2/2, step 704/7134 completed (loss: 0.1525341421365738, acc: 0.9675324559211731)
[2025-02-13 20:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25][root][INFO] - Training Epoch: 2/2, step 705/7134 completed (loss: 0.18390344083309174, acc: 0.9587156176567078)
[2025-02-13 20:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25][root][INFO] - Training Epoch: 2/2, step 706/7134 completed (loss: 0.043317802250385284, acc: 0.9952830076217651)
[2025-02-13 20:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26][root][INFO] - Training Epoch: 2/2, step 707/7134 completed (loss: 0.1320100575685501, acc: 0.9702970385551453)
[2025-02-13 20:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26][root][INFO] - Training Epoch: 2/2, step 708/7134 completed (loss: 0.09115514159202576, acc: 0.9719101190567017)
[2025-02-13 20:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26][root][INFO] - Training Epoch: 2/2, step 709/7134 completed (loss: 0.11651364713907242, acc: 0.9599999785423279)
[2025-02-13 20:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27][root][INFO] - Training Epoch: 2/2, step 710/7134 completed (loss: 0.07470214366912842, acc: 0.9791666865348816)
[2025-02-13 20:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27][root][INFO] - Training Epoch: 2/2, step 711/7134 completed (loss: 0.09241129457950592, acc: 0.9656862616539001)
[2025-02-13 20:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27][root][INFO] - Training Epoch: 2/2, step 712/7134 completed (loss: 0.041561782360076904, acc: 0.9895833134651184)
[2025-02-13 20:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28][root][INFO] - Training Epoch: 2/2, step 713/7134 completed (loss: 0.06703199446201324, acc: 0.9849246144294739)
[2025-02-13 20:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28][root][INFO] - Training Epoch: 2/2, step 714/7134 completed (loss: 0.08943802118301392, acc: 0.9700000286102295)
[2025-02-13 20:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29][root][INFO] - Training Epoch: 2/2, step 715/7134 completed (loss: 0.07544007897377014, acc: 0.989130437374115)
[2025-02-13 20:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29][root][INFO] - Training Epoch: 2/2, step 716/7134 completed (loss: 0.0342901386320591, acc: 0.9950980544090271)
[2025-02-13 20:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29][root][INFO] - Training Epoch: 2/2, step 717/7134 completed (loss: 0.11792685091495514, acc: 0.9786096215248108)
[2025-02-13 20:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30][root][INFO] - Training Epoch: 2/2, step 718/7134 completed (loss: 0.05540235713124275, acc: 0.9894179701805115)
[2025-02-13 20:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30][root][INFO] - Training Epoch: 2/2, step 719/7134 completed (loss: 0.08662386238574982, acc: 0.9929078221321106)
[2025-02-13 20:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31][root][INFO] - Training Epoch: 2/2, step 720/7134 completed (loss: 0.06807779520750046, acc: 0.9837837815284729)
[2025-02-13 20:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31][root][INFO] - Training Epoch: 2/2, step 721/7134 completed (loss: 0.04065524786710739, acc: 0.9842105507850647)
[2025-02-13 20:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31][root][INFO] - Training Epoch: 2/2, step 722/7134 completed (loss: 0.05717865005135536, acc: 0.9949495196342468)
[2025-02-13 20:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32][root][INFO] - Training Epoch: 2/2, step 723/7134 completed (loss: 0.028154751285910606, acc: 0.9945651888847351)
[2025-02-13 20:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32][root][INFO] - Training Epoch: 2/2, step 724/7134 completed (loss: 0.07075055688619614, acc: 0.9748427867889404)
[2025-02-13 20:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33][root][INFO] - Training Epoch: 2/2, step 725/7134 completed (loss: 0.014672631397843361, acc: 1.0)
[2025-02-13 20:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33][root][INFO] - Training Epoch: 2/2, step 726/7134 completed (loss: 0.012522787787020206, acc: 1.0)
[2025-02-13 20:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33][root][INFO] - Training Epoch: 2/2, step 727/7134 completed (loss: 0.16027484834194183, acc: 0.9653179049491882)
[2025-02-13 20:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34][root][INFO] - Training Epoch: 2/2, step 728/7134 completed (loss: 0.022651104256510735, acc: 0.9942857027053833)
[2025-02-13 20:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34][root][INFO] - Training Epoch: 2/2, step 729/7134 completed (loss: 0.03312808275222778, acc: 0.9936708807945251)
[2025-02-13 20:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34][root][INFO] - Training Epoch: 2/2, step 730/7134 completed (loss: 0.04441694915294647, acc: 0.9893617033958435)
[2025-02-13 20:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35][root][INFO] - Training Epoch: 2/2, step 731/7134 completed (loss: 0.014255963265895844, acc: 1.0)
[2025-02-13 20:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35][root][INFO] - Training Epoch: 2/2, step 732/7134 completed (loss: 0.145094633102417, acc: 0.9754098653793335)
[2025-02-13 20:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36][root][INFO] - Training Epoch: 2/2, step 733/7134 completed (loss: 0.09956327080726624, acc: 0.9769230484962463)
[2025-02-13 20:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36][root][INFO] - Training Epoch: 2/2, step 734/7134 completed (loss: 0.05191556364297867, acc: 0.9919354915618896)
[2025-02-13 20:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36][root][INFO] - Training Epoch: 2/2, step 735/7134 completed (loss: 0.07092408835887909, acc: 0.9647887349128723)
[2025-02-13 20:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37][root][INFO] - Training Epoch: 2/2, step 736/7134 completed (loss: 0.17954592406749725, acc: 0.9504950642585754)
[2025-02-13 20:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37][root][INFO] - Training Epoch: 2/2, step 737/7134 completed (loss: 0.16129790246486664, acc: 0.9322034120559692)
[2025-02-13 20:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37][root][INFO] - Training Epoch: 2/2, step 738/7134 completed (loss: 0.03559854254126549, acc: 0.9905660152435303)
[2025-02-13 20:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38][root][INFO] - Training Epoch: 2/2, step 739/7134 completed (loss: 0.09980832040309906, acc: 0.9652174115180969)
[2025-02-13 20:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38][root][INFO] - Training Epoch: 2/2, step 740/7134 completed (loss: 0.06579954922199249, acc: 0.9837398529052734)
[2025-02-13 20:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39][root][INFO] - Training Epoch: 2/2, step 741/7134 completed (loss: 0.10577474534511566, acc: 0.9629629850387573)
[2025-02-13 20:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39][root][INFO] - Training Epoch: 2/2, step 742/7134 completed (loss: 0.06851574033498764, acc: 0.9894737005233765)
[2025-02-13 20:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39][root][INFO] - Training Epoch: 2/2, step 743/7134 completed (loss: 0.18977373838424683, acc: 0.9387755393981934)
[2025-02-13 20:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40][root][INFO] - Training Epoch: 2/2, step 744/7134 completed (loss: 0.04078911244869232, acc: 1.0)
[2025-02-13 20:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40][root][INFO] - Training Epoch: 2/2, step 745/7134 completed (loss: 0.11125431954860687, acc: 0.9779411554336548)
[2025-02-13 20:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40][root][INFO] - Training Epoch: 2/2, step 746/7134 completed (loss: 0.12104520946741104, acc: 0.9618320465087891)
[2025-02-13 20:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41][root][INFO] - Training Epoch: 2/2, step 747/7134 completed (loss: 0.1584090143442154, acc: 0.9370078444480896)
[2025-02-13 20:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41][root][INFO] - Training Epoch: 2/2, step 748/7134 completed (loss: 0.0910838320851326, acc: 0.9701492786407471)
[2025-02-13 20:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42][root][INFO] - Training Epoch: 2/2, step 749/7134 completed (loss: 0.06274557113647461, acc: 0.9857142567634583)
[2025-02-13 20:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42][root][INFO] - Training Epoch: 2/2, step 750/7134 completed (loss: 0.0871465876698494, acc: 0.9917355179786682)
[2025-02-13 20:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42][root][INFO] - Training Epoch: 2/2, step 751/7134 completed (loss: 0.12978991866111755, acc: 0.9599999785423279)
[2025-02-13 20:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43][root][INFO] - Training Epoch: 2/2, step 752/7134 completed (loss: 0.2783021032810211, acc: 0.9459459185600281)
[2025-02-13 20:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43][root][INFO] - Training Epoch: 2/2, step 753/7134 completed (loss: 0.07668997347354889, acc: 0.984375)
[2025-02-13 20:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44][root][INFO] - Training Epoch: 2/2, step 754/7134 completed (loss: 0.1009424552321434, acc: 0.97826087474823)
[2025-02-13 20:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44][root][INFO] - Training Epoch: 2/2, step 755/7134 completed (loss: 0.17753298580646515, acc: 0.9760000109672546)
[2025-02-13 20:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44][root][INFO] - Training Epoch: 2/2, step 756/7134 completed (loss: 0.036372870206832886, acc: 0.9910714030265808)
[2025-02-13 20:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45][root][INFO] - Training Epoch: 2/2, step 757/7134 completed (loss: 0.038180120289325714, acc: 0.9903846383094788)
[2025-02-13 20:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45][root][INFO] - Training Epoch: 2/2, step 758/7134 completed (loss: 0.041118260473012924, acc: 1.0)
[2025-02-13 20:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45][root][INFO] - Training Epoch: 2/2, step 759/7134 completed (loss: 0.09813112765550613, acc: 0.954954981803894)
[2025-02-13 20:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:46][root][INFO] - Training Epoch: 2/2, step 760/7134 completed (loss: 0.04150819033384323, acc: 0.9924242496490479)
[2025-02-13 20:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:46][root][INFO] - Training Epoch: 2/2, step 761/7134 completed (loss: 0.10912769287824631, acc: 0.9640718698501587)
[2025-02-13 20:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47][root][INFO] - Training Epoch: 2/2, step 762/7134 completed (loss: 0.09631886333227158, acc: 0.9710144996643066)
[2025-02-13 20:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47][root][INFO] - Training Epoch: 2/2, step 763/7134 completed (loss: 0.15320304036140442, acc: 0.9736841917037964)
[2025-02-13 20:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47][root][INFO] - Training Epoch: 2/2, step 764/7134 completed (loss: 0.0428459458053112, acc: 0.9881656765937805)
[2025-02-13 20:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48][root][INFO] - Training Epoch: 2/2, step 765/7134 completed (loss: 0.026722105219960213, acc: 0.9937106966972351)
[2025-02-13 20:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48][root][INFO] - Training Epoch: 2/2, step 766/7134 completed (loss: 0.09090624749660492, acc: 0.96875)
[2025-02-13 20:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48][root][INFO] - Training Epoch: 2/2, step 767/7134 completed (loss: 0.08425155282020569, acc: 0.976190447807312)
[2025-02-13 20:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49][root][INFO] - Training Epoch: 2/2, step 768/7134 completed (loss: 0.1866942197084427, acc: 0.9672130942344666)
[2025-02-13 20:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49][root][INFO] - Training Epoch: 2/2, step 769/7134 completed (loss: 0.07387005537748337, acc: 0.9875776171684265)
[2025-02-13 20:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50][root][INFO] - Training Epoch: 2/2, step 770/7134 completed (loss: 0.06329518556594849, acc: 0.9814814925193787)
[2025-02-13 20:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50][root][INFO] - Training Epoch: 2/2, step 771/7134 completed (loss: 0.03885125368833542, acc: 0.9927007555961609)
[2025-02-13 20:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50][root][INFO] - Training Epoch: 2/2, step 772/7134 completed (loss: 0.08305760473012924, acc: 0.9727891087532043)
[2025-02-13 20:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51][root][INFO] - Training Epoch: 2/2, step 773/7134 completed (loss: 0.13403573632240295, acc: 0.9631578922271729)
[2025-02-13 20:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51][root][INFO] - Training Epoch: 2/2, step 774/7134 completed (loss: 0.12864188849925995, acc: 0.9577465057373047)
[2025-02-13 20:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51][root][INFO] - Training Epoch: 2/2, step 775/7134 completed (loss: 0.024316158145666122, acc: 0.9946808218955994)
[2025-02-13 20:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52][root][INFO] - Training Epoch: 2/2, step 776/7134 completed (loss: 0.022250818088650703, acc: 0.993630588054657)
[2025-02-13 20:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52][root][INFO] - Training Epoch: 2/2, step 777/7134 completed (loss: 0.05751204863190651, acc: 0.9733333587646484)
[2025-02-13 20:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53][root][INFO] - Training Epoch: 2/2, step 778/7134 completed (loss: 0.07062534987926483, acc: 0.9937106966972351)
[2025-02-13 20:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53][root][INFO] - Training Epoch: 2/2, step 779/7134 completed (loss: 0.06397230923175812, acc: 0.988304078578949)
[2025-02-13 20:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53][root][INFO] - Training Epoch: 2/2, step 780/7134 completed (loss: 0.1436944454908371, acc: 0.9459459185600281)
[2025-02-13 20:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54][root][INFO] - Training Epoch: 2/2, step 781/7134 completed (loss: 0.12893353402614594, acc: 0.9754902124404907)
[2025-02-13 20:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54][root][INFO] - Training Epoch: 2/2, step 782/7134 completed (loss: 0.1172887459397316, acc: 0.9683544039726257)
[2025-02-13 20:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55][root][INFO] - Training Epoch: 2/2, step 783/7134 completed (loss: 0.10977989435195923, acc: 0.9719626307487488)
[2025-02-13 20:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55][root][INFO] - Training Epoch: 2/2, step 784/7134 completed (loss: 0.05219579115509987, acc: 0.9900497794151306)
[2025-02-13 20:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55][root][INFO] - Training Epoch: 2/2, step 785/7134 completed (loss: 0.18519212305545807, acc: 0.9466666579246521)
[2025-02-13 20:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56][root][INFO] - Training Epoch: 2/2, step 786/7134 completed (loss: 0.12196612358093262, acc: 0.9701492786407471)
[2025-02-13 20:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56][root][INFO] - Training Epoch: 2/2, step 787/7134 completed (loss: 0.02070002816617489, acc: 1.0)
[2025-02-13 20:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56][root][INFO] - Training Epoch: 2/2, step 788/7134 completed (loss: 0.18276026844978333, acc: 0.9441340565681458)
[2025-02-13 20:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57][root][INFO] - Training Epoch: 2/2, step 789/7134 completed (loss: 0.07494615763425827, acc: 0.9743589758872986)
[2025-02-13 20:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57][root][INFO] - Training Epoch: 2/2, step 790/7134 completed (loss: 0.12941202521324158, acc: 0.9729729890823364)
[2025-02-13 20:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58][root][INFO] - Training Epoch: 2/2, step 791/7134 completed (loss: 0.23257826268672943, acc: 0.934959352016449)
[2025-02-13 20:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58][root][INFO] - Training Epoch: 2/2, step 792/7134 completed (loss: 0.11476520448923111, acc: 0.9620253443717957)
[2025-02-13 20:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58][root][INFO] - Training Epoch: 2/2, step 793/7134 completed (loss: 0.0553240105509758, acc: 0.9937499761581421)
[2025-02-13 20:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59][root][INFO] - Training Epoch: 2/2, step 794/7134 completed (loss: 0.12135285139083862, acc: 0.9568965435028076)
[2025-02-13 20:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59][root][INFO] - Training Epoch: 2/2, step 795/7134 completed (loss: 0.06782186031341553, acc: 0.9873417615890503)
[2025-02-13 20:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00][root][INFO] - Training Epoch: 2/2, step 796/7134 completed (loss: 0.11038754880428314, acc: 0.9809523820877075)
[2025-02-13 20:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00][root][INFO] - Training Epoch: 2/2, step 797/7134 completed (loss: 0.15313921868801117, acc: 0.960629940032959)
[2025-02-13 20:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00][root][INFO] - Training Epoch: 2/2, step 798/7134 completed (loss: 0.058681920170784, acc: 0.9898989796638489)
[2025-02-13 20:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01][root][INFO] - Training Epoch: 2/2, step 799/7134 completed (loss: 0.0754241943359375, acc: 0.9752066135406494)
[2025-02-13 20:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01][root][INFO] - Training Epoch: 2/2, step 800/7134 completed (loss: 0.12002193182706833, acc: 0.970370352268219)
[2025-02-13 20:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01][root][INFO] - Training Epoch: 2/2, step 801/7134 completed (loss: 0.25876855850219727, acc: 0.9548386931419373)
[2025-02-13 20:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:02][root][INFO] - Training Epoch: 2/2, step 802/7134 completed (loss: 0.10872849076986313, acc: 0.9716312289237976)
[2025-02-13 20:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:02][root][INFO] - Training Epoch: 2/2, step 803/7134 completed (loss: 0.07253793627023697, acc: 0.97826087474823)
[2025-02-13 20:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03][root][INFO] - Training Epoch: 2/2, step 804/7134 completed (loss: 0.2380560040473938, acc: 0.9534883499145508)
[2025-02-13 20:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03][root][INFO] - Training Epoch: 2/2, step 805/7134 completed (loss: 0.208060622215271, acc: 0.9370629191398621)
[2025-02-13 20:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03][root][INFO] - Training Epoch: 2/2, step 806/7134 completed (loss: 0.06672538816928864, acc: 0.9814814925193787)
[2025-02-13 20:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04][root][INFO] - Training Epoch: 2/2, step 807/7134 completed (loss: 0.13411462306976318, acc: 0.9599999785423279)
[2025-02-13 20:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04][root][INFO] - Training Epoch: 2/2, step 808/7134 completed (loss: 0.05812570080161095, acc: 0.9796954393386841)
[2025-02-13 20:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04][root][INFO] - Training Epoch: 2/2, step 809/7134 completed (loss: 0.04822955280542374, acc: 0.9866666793823242)
[2025-02-13 20:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05][root][INFO] - Training Epoch: 2/2, step 810/7134 completed (loss: 0.14017724990844727, acc: 0.9682539701461792)
[2025-02-13 20:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05][root][INFO] - Training Epoch: 2/2, step 811/7134 completed (loss: 0.23684774339199066, acc: 0.9448275566101074)
[2025-02-13 20:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06][root][INFO] - Training Epoch: 2/2, step 812/7134 completed (loss: 0.0777573361992836, acc: 0.9789473414421082)
[2025-02-13 20:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06][root][INFO] - Training Epoch: 2/2, step 813/7134 completed (loss: 0.15206278860569, acc: 0.9588235020637512)
[2025-02-13 20:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06][root][INFO] - Training Epoch: 2/2, step 814/7134 completed (loss: 0.10561054199934006, acc: 0.9774011373519897)
[2025-02-13 20:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07][root][INFO] - Training Epoch: 2/2, step 815/7134 completed (loss: 0.0669998750090599, acc: 0.9871794581413269)
[2025-02-13 20:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07][root][INFO] - Training Epoch: 2/2, step 816/7134 completed (loss: 0.1622539609670639, acc: 0.9645389914512634)
[2025-02-13 20:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08][root][INFO] - Training Epoch: 2/2, step 817/7134 completed (loss: 0.15517298877239227, acc: 0.9568345546722412)
[2025-02-13 20:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08][root][INFO] - Training Epoch: 2/2, step 818/7134 completed (loss: 0.2984262704849243, acc: 0.9281045794487)
[2025-02-13 20:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08][root][INFO] - Training Epoch: 2/2, step 819/7134 completed (loss: 0.24877069890499115, acc: 0.931506872177124)
[2025-02-13 20:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09][root][INFO] - Training Epoch: 2/2, step 820/7134 completed (loss: 0.1729714721441269, acc: 0.9415584206581116)
[2025-02-13 20:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09][root][INFO] - Training Epoch: 2/2, step 821/7134 completed (loss: 0.11319240927696228, acc: 0.9710982441902161)
[2025-02-13 20:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09][root][INFO] - Training Epoch: 2/2, step 822/7134 completed (loss: 0.0802338719367981, acc: 0.9810126423835754)
[2025-02-13 20:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:10][root][INFO] - Training Epoch: 2/2, step 823/7134 completed (loss: 0.08839186280965805, acc: 0.9756097793579102)
[2025-02-13 20:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:10][root][INFO] - Training Epoch: 2/2, step 824/7134 completed (loss: 0.14008334279060364, acc: 0.9621211886405945)
[2025-02-13 20:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11][root][INFO] - Training Epoch: 2/2, step 825/7134 completed (loss: 0.10050084441900253, acc: 0.9772727489471436)
[2025-02-13 20:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11][root][INFO] - Training Epoch: 2/2, step 826/7134 completed (loss: 0.06508450955152512, acc: 0.9824561476707458)
[2025-02-13 20:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11][root][INFO] - Training Epoch: 2/2, step 827/7134 completed (loss: 0.08371028304100037, acc: 0.9766082167625427)
[2025-02-13 20:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12][root][INFO] - Training Epoch: 2/2, step 828/7134 completed (loss: 0.09356318414211273, acc: 0.9805194735527039)
[2025-02-13 20:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12][root][INFO] - Training Epoch: 2/2, step 829/7134 completed (loss: 0.13372060656547546, acc: 0.9711538553237915)
[2025-02-13 20:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12][root][INFO] - Training Epoch: 2/2, step 830/7134 completed (loss: 0.06841816008090973, acc: 0.9784172773361206)
[2025-02-13 20:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13][root][INFO] - Training Epoch: 2/2, step 831/7134 completed (loss: 0.0806489884853363, acc: 0.9819276928901672)
[2025-02-13 20:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13][root][INFO] - Training Epoch: 2/2, step 832/7134 completed (loss: 0.19877870380878448, acc: 0.9591836929321289)
[2025-02-13 20:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14][root][INFO] - Training Epoch: 2/2, step 833/7134 completed (loss: 0.03069429099559784, acc: 0.9940476417541504)
[2025-02-13 20:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14][root][INFO] - Training Epoch: 2/2, step 834/7134 completed (loss: 0.08811035007238388, acc: 0.970588207244873)
[2025-02-13 20:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14][root][INFO] - Training Epoch: 2/2, step 835/7134 completed (loss: 0.14993008971214294, acc: 0.9649122953414917)
[2025-02-13 20:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:15][root][INFO] - Training Epoch: 2/2, step 836/7134 completed (loss: 0.1081685870885849, acc: 0.9746192693710327)
[2025-02-13 20:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:15][root][INFO] - Training Epoch: 2/2, step 837/7134 completed (loss: 0.13447345793247223, acc: 0.9774011373519897)
[2025-02-13 20:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16][root][INFO] - Training Epoch: 2/2, step 838/7134 completed (loss: 0.08043433725833893, acc: 0.9738562107086182)
[2025-02-13 20:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16][root][INFO] - Training Epoch: 2/2, step 839/7134 completed (loss: 0.08012969046831131, acc: 0.9504950642585754)
[2025-02-13 20:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16][root][INFO] - Training Epoch: 2/2, step 840/7134 completed (loss: 0.06264091283082962, acc: 0.9820359349250793)
[2025-02-13 20:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17][root][INFO] - Training Epoch: 2/2, step 841/7134 completed (loss: 0.0403706394135952, acc: 0.9941520690917969)
[2025-02-13 20:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17][root][INFO] - Training Epoch: 2/2, step 842/7134 completed (loss: 0.03953992947936058, acc: 0.9823529124259949)
[2025-02-13 20:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18][root][INFO] - Training Epoch: 2/2, step 843/7134 completed (loss: 0.03946191817522049, acc: 0.988304078578949)
[2025-02-13 20:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18][root][INFO] - Training Epoch: 2/2, step 844/7134 completed (loss: 0.07951198518276215, acc: 0.9863013625144958)
[2025-02-13 20:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18][root][INFO] - Training Epoch: 2/2, step 845/7134 completed (loss: 0.046349212527275085, acc: 0.9777777791023254)
[2025-02-13 20:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19][root][INFO] - Training Epoch: 2/2, step 846/7134 completed (loss: 0.028492676094174385, acc: 1.0)
[2025-02-13 20:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19][root][INFO] - Training Epoch: 2/2, step 847/7134 completed (loss: 0.15653598308563232, acc: 0.970059871673584)
[2025-02-13 20:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19][root][INFO] - Training Epoch: 2/2, step 848/7134 completed (loss: 0.1342361569404602, acc: 0.9707602262496948)
[2025-02-13 20:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:20][root][INFO] - Training Epoch: 2/2, step 849/7134 completed (loss: 0.05986601486802101, acc: 0.9817073345184326)
[2025-02-13 20:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:20][root][INFO] - Training Epoch: 2/2, step 850/7134 completed (loss: 0.03528032451868057, acc: 0.9932885766029358)
[2025-02-13 20:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21][root][INFO] - Training Epoch: 2/2, step 851/7134 completed (loss: 0.11949639022350311, acc: 0.9709302186965942)
[2025-02-13 20:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21][root][INFO] - Training Epoch: 2/2, step 852/7134 completed (loss: 0.06960295140743256, acc: 0.9947643876075745)
[2025-02-13 20:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21][root][INFO] - Training Epoch: 2/2, step 853/7134 completed (loss: 0.0984286442399025, acc: 0.9738219976425171)
[2025-02-13 20:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22][root][INFO] - Training Epoch: 2/2, step 854/7134 completed (loss: 0.07494422048330307, acc: 0.9878048896789551)
[2025-02-13 20:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22][root][INFO] - Training Epoch: 2/2, step 855/7134 completed (loss: 0.06870923936367035, acc: 0.9768785834312439)
[2025-02-13 20:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22][root][INFO] - Training Epoch: 2/2, step 856/7134 completed (loss: 0.034524064511060715, acc: 1.0)
[2025-02-13 20:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23][root][INFO] - Training Epoch: 2/2, step 857/7134 completed (loss: 0.08528240025043488, acc: 0.9714285731315613)
[2025-02-13 20:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23][root][INFO] - Training Epoch: 2/2, step 858/7134 completed (loss: 0.12971261143684387, acc: 0.9702380895614624)
[2025-02-13 20:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24][root][INFO] - Training Epoch: 2/2, step 859/7134 completed (loss: 0.10482438653707504, acc: 0.9830508232116699)
[2025-02-13 20:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24][root][INFO] - Training Epoch: 2/2, step 860/7134 completed (loss: 0.14669091999530792, acc: 0.9735099077224731)
[2025-02-13 20:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24][root][INFO] - Training Epoch: 2/2, step 861/7134 completed (loss: 0.10431158542633057, acc: 0.9784946441650391)
[2025-02-13 20:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25][root][INFO] - Training Epoch: 2/2, step 862/7134 completed (loss: 0.1380055844783783, acc: 0.9657142758369446)
[2025-02-13 20:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25][root][INFO] - Training Epoch: 2/2, step 863/7134 completed (loss: 0.14197750389575958, acc: 0.9617486596107483)
[2025-02-13 20:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25][root][INFO] - Training Epoch: 2/2, step 864/7134 completed (loss: 0.14362485706806183, acc: 0.9682539701461792)
[2025-02-13 20:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:26][root][INFO] - Training Epoch: 2/2, step 865/7134 completed (loss: 0.18273603916168213, acc: 0.9651162624359131)
[2025-02-13 20:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:26][root][INFO] - Training Epoch: 2/2, step 866/7134 completed (loss: 0.20996496081352234, acc: 0.949999988079071)
[2025-02-13 20:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27][root][INFO] - Training Epoch: 2/2, step 867/7134 completed (loss: 0.17178860306739807, acc: 0.96875)
[2025-02-13 20:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27][root][INFO] - Training Epoch: 2/2, step 868/7134 completed (loss: 0.3459755778312683, acc: 0.930232584476471)
[2025-02-13 20:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27][root][INFO] - Training Epoch: 2/2, step 869/7134 completed (loss: 0.17282646894454956, acc: 0.9397590160369873)
[2025-02-13 20:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28][root][INFO] - Training Epoch: 2/2, step 870/7134 completed (loss: 0.0833704024553299, acc: 0.9766082167625427)
[2025-02-13 20:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28][root][INFO] - Training Epoch: 2/2, step 871/7134 completed (loss: 0.1505650281906128, acc: 0.9589040875434875)
[2025-02-13 20:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28][root][INFO] - Training Epoch: 2/2, step 872/7134 completed (loss: 0.04425148665904999, acc: 0.9930555820465088)
[2025-02-13 20:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:29][root][INFO] - Training Epoch: 2/2, step 873/7134 completed (loss: 0.16861487925052643, acc: 0.9454545378684998)
[2025-02-13 20:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:29][root][INFO] - Training Epoch: 2/2, step 874/7134 completed (loss: 0.17222356796264648, acc: 0.9583333134651184)
[2025-02-13 20:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30][root][INFO] - Training Epoch: 2/2, step 875/7134 completed (loss: 0.030498819425702095, acc: 0.9928057789802551)
[2025-02-13 20:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30][root][INFO] - Training Epoch: 2/2, step 876/7134 completed (loss: 0.08077367395162582, acc: 0.9802631735801697)
[2025-02-13 20:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30][root][INFO] - Training Epoch: 2/2, step 877/7134 completed (loss: 0.2182818055152893, acc: 0.9447852969169617)
[2025-02-13 20:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31][root][INFO] - Training Epoch: 2/2, step 878/7134 completed (loss: 0.12001631408929825, acc: 0.96875)
[2025-02-13 20:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31][root][INFO] - Training Epoch: 2/2, step 879/7134 completed (loss: 0.13651028275489807, acc: 0.9681528806686401)
[2025-02-13 20:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31][root][INFO] - Training Epoch: 2/2, step 880/7134 completed (loss: 0.11071129888296127, acc: 0.9693251252174377)
[2025-02-13 20:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:32][root][INFO] - Training Epoch: 2/2, step 881/7134 completed (loss: 0.07735268026590347, acc: 0.9837398529052734)
[2025-02-13 20:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:32][root][INFO] - Training Epoch: 2/2, step 882/7134 completed (loss: 0.09489016234874725, acc: 0.9838709831237793)
[2025-02-13 20:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33][root][INFO] - Training Epoch: 2/2, step 883/7134 completed (loss: 0.13282066583633423, acc: 0.9736841917037964)
[2025-02-13 20:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33][root][INFO] - Training Epoch: 2/2, step 884/7134 completed (loss: 0.043997034430503845, acc: 0.9940119981765747)
[2025-02-13 20:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33][root][INFO] - Training Epoch: 2/2, step 885/7134 completed (loss: 0.022663664072752, acc: 0.9939024448394775)
[2025-02-13 20:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34][root][INFO] - Training Epoch: 2/2, step 886/7134 completed (loss: 0.1074366495013237, acc: 0.9915966391563416)
[2025-02-13 20:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34][root][INFO] - Training Epoch: 2/2, step 887/7134 completed (loss: 0.08360664546489716, acc: 0.9850746393203735)
[2025-02-13 20:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34][root][INFO] - Training Epoch: 2/2, step 888/7134 completed (loss: 0.21387545764446259, acc: 0.9865771532058716)
[2025-02-13 20:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:35][root][INFO] - Training Epoch: 2/2, step 889/7134 completed (loss: 0.029368853196501732, acc: 1.0)
[2025-02-13 20:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:35][root][INFO] - Training Epoch: 2/2, step 890/7134 completed (loss: 0.14232981204986572, acc: 0.9463087320327759)
[2025-02-13 20:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36][root][INFO] - Training Epoch: 2/2, step 891/7134 completed (loss: 0.1819738745689392, acc: 0.9664804339408875)
[2025-02-13 20:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36][root][INFO] - Training Epoch: 2/2, step 892/7134 completed (loss: 0.09923696517944336, acc: 0.976190447807312)
[2025-02-13 20:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36][root][INFO] - Training Epoch: 2/2, step 893/7134 completed (loss: 0.1653391271829605, acc: 0.9673202633857727)
[2025-02-13 20:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37][root][INFO] - Training Epoch: 2/2, step 894/7134 completed (loss: 0.035052429884672165, acc: 0.994350254535675)
[2025-02-13 20:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37][root][INFO] - Training Epoch: 2/2, step 895/7134 completed (loss: 0.04027695581316948, acc: 0.9863013625144958)
[2025-02-13 20:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37][root][INFO] - Training Epoch: 2/2, step 896/7134 completed (loss: 0.028937775641679764, acc: 1.0)
[2025-02-13 20:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38][root][INFO] - Training Epoch: 2/2, step 897/7134 completed (loss: 0.09570403397083282, acc: 0.9832402467727661)
[2025-02-13 20:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38][root][INFO] - Training Epoch: 2/2, step 898/7134 completed (loss: 0.1669411063194275, acc: 0.9602272510528564)
[2025-02-13 20:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39][root][INFO] - Training Epoch: 2/2, step 899/7134 completed (loss: 0.09453658014535904, acc: 0.9625668525695801)
[2025-02-13 20:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39][root][INFO] - Training Epoch: 2/2, step 900/7134 completed (loss: 0.05290168151259422, acc: 0.9939024448394775)
[2025-02-13 20:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39][root][INFO] - Training Epoch: 2/2, step 901/7134 completed (loss: 0.12715719640254974, acc: 0.9642857313156128)
[2025-02-13 20:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40][root][INFO] - Training Epoch: 2/2, step 902/7134 completed (loss: 0.0725819319486618, acc: 0.9805194735527039)
[2025-02-13 20:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40][root][INFO] - Training Epoch: 2/2, step 903/7134 completed (loss: 0.028273548930883408, acc: 0.9942196607589722)
[2025-02-13 20:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40][root][INFO] - Training Epoch: 2/2, step 904/7134 completed (loss: 0.10810113698244095, acc: 0.9830508232116699)
[2025-02-13 20:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:41][root][INFO] - Training Epoch: 2/2, step 905/7134 completed (loss: 0.03986860439181328, acc: 0.9942528605461121)
[2025-02-13 20:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:41][root][INFO] - Training Epoch: 2/2, step 906/7134 completed (loss: 0.07265444844961166, acc: 0.9779005646705627)
[2025-02-13 20:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42][root][INFO] - Training Epoch: 2/2, step 907/7134 completed (loss: 0.07079494744539261, acc: 0.9784946441650391)
[2025-02-13 20:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42][root][INFO] - Training Epoch: 2/2, step 908/7134 completed (loss: 0.04111652448773384, acc: 0.9888888597488403)
[2025-02-13 20:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42][root][INFO] - Training Epoch: 2/2, step 909/7134 completed (loss: 0.05915386602282524, acc: 0.9802631735801697)
[2025-02-13 20:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43][root][INFO] - Training Epoch: 2/2, step 910/7134 completed (loss: 0.07242672145366669, acc: 0.981249988079071)
[2025-02-13 20:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43][root][INFO] - Training Epoch: 2/2, step 911/7134 completed (loss: 0.07255567610263824, acc: 0.9870967864990234)
[2025-02-13 20:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43][root][INFO] - Training Epoch: 2/2, step 912/7134 completed (loss: 0.11432112753391266, acc: 0.9736841917037964)
[2025-02-13 20:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:44][root][INFO] - Training Epoch: 2/2, step 913/7134 completed (loss: 0.042611267417669296, acc: 0.9890109896659851)
[2025-02-13 20:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:44][root][INFO] - Training Epoch: 2/2, step 914/7134 completed (loss: 0.0965077206492424, acc: 0.9757575988769531)
[2025-02-13 20:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45][root][INFO] - Training Epoch: 2/2, step 915/7134 completed (loss: 0.08440003544092178, acc: 0.9857142567634583)
[2025-02-13 20:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45][root][INFO] - Training Epoch: 2/2, step 916/7134 completed (loss: 0.05861185863614082, acc: 0.9844961166381836)
[2025-02-13 20:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45][root][INFO] - Training Epoch: 2/2, step 917/7134 completed (loss: 0.1499166488647461, acc: 0.9545454382896423)
[2025-02-13 20:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:46][root][INFO] - Training Epoch: 2/2, step 918/7134 completed (loss: 0.09005466848611832, acc: 0.9735099077224731)
[2025-02-13 20:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:46][root][INFO] - Training Epoch: 2/2, step 919/7134 completed (loss: 0.05332866311073303, acc: 0.9870129823684692)
[2025-02-13 20:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47][root][INFO] - Training Epoch: 2/2, step 920/7134 completed (loss: 0.08721690624952316, acc: 0.9767441749572754)
[2025-02-13 20:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47][root][INFO] - Training Epoch: 2/2, step 921/7134 completed (loss: 0.058857399970293045, acc: 0.988304078578949)
[2025-02-13 20:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47][root][INFO] - Training Epoch: 2/2, step 922/7134 completed (loss: 0.029800020158290863, acc: 0.9925373196601868)
[2025-02-13 20:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48][root][INFO] - Training Epoch: 2/2, step 923/7134 completed (loss: 0.07792629301548004, acc: 0.9766082167625427)
[2025-02-13 20:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48][root][INFO] - Training Epoch: 2/2, step 924/7134 completed (loss: 0.0561080239713192, acc: 0.9830508232116699)
[2025-02-13 20:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48][root][INFO] - Training Epoch: 2/2, step 925/7134 completed (loss: 0.29844123125076294, acc: 0.9419354796409607)
[2025-02-13 20:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:49][root][INFO] - Training Epoch: 2/2, step 926/7134 completed (loss: 0.11434691399335861, acc: 0.9722222089767456)
[2025-02-13 20:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:49][root][INFO] - Training Epoch: 2/2, step 927/7134 completed (loss: 0.19155049324035645, acc: 0.9496402740478516)
[2025-02-13 20:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50][root][INFO] - Training Epoch: 2/2, step 928/7134 completed (loss: 0.01408995222300291, acc: 1.0)
[2025-02-13 20:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50][root][INFO] - Training Epoch: 2/2, step 929/7134 completed (loss: 0.05936617776751518, acc: 0.9868420958518982)
[2025-02-13 20:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50][root][INFO] - Training Epoch: 2/2, step 930/7134 completed (loss: 0.16011150181293488, acc: 0.9596773982048035)
[2025-02-13 20:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:51][root][INFO] - Training Epoch: 2/2, step 931/7134 completed (loss: 0.09258828312158585, acc: 0.9852941036224365)
[2025-02-13 20:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:51][root][INFO] - Training Epoch: 2/2, step 932/7134 completed (loss: 0.0632498487830162, acc: 0.988095223903656)
[2025-02-13 20:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:52][root][INFO] - Training Epoch: 2/2, step 933/7134 completed (loss: 0.13291531801223755, acc: 0.9640718698501587)
[2025-02-13 20:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:52][root][INFO] - Training Epoch: 2/2, step 934/7134 completed (loss: 0.05341552570462227, acc: 0.9825581312179565)
[2025-02-13 20:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:52][root][INFO] - Training Epoch: 2/2, step 935/7134 completed (loss: 0.019435426220297813, acc: 1.0)
[2025-02-13 20:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53][root][INFO] - Training Epoch: 2/2, step 936/7134 completed (loss: 0.08927302807569504, acc: 0.9921259880065918)
[2025-02-13 20:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53][root][INFO] - Training Epoch: 2/2, step 937/7134 completed (loss: 0.05805601179599762, acc: 0.9918699264526367)
[2025-02-13 20:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53][root][INFO] - Training Epoch: 2/2, step 938/7134 completed (loss: 0.07137658447027206, acc: 0.9807692170143127)
[2025-02-13 20:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54][root][INFO] - Training Epoch: 2/2, step 939/7134 completed (loss: 0.12704400718212128, acc: 0.9695122241973877)
[2025-02-13 20:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54][root][INFO] - Training Epoch: 2/2, step 940/7134 completed (loss: 0.04852156713604927, acc: 1.0)
[2025-02-13 20:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55][root][INFO] - Training Epoch: 2/2, step 941/7134 completed (loss: 0.11854308843612671, acc: 0.9775280952453613)
[2025-02-13 20:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55][root][INFO] - Training Epoch: 2/2, step 942/7134 completed (loss: 0.20242589712142944, acc: 0.9651162624359131)
[2025-02-13 20:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55][root][INFO] - Training Epoch: 2/2, step 943/7134 completed (loss: 0.07971303164958954, acc: 0.9938650131225586)
[2025-02-13 20:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:56][root][INFO] - Training Epoch: 2/2, step 944/7134 completed (loss: 0.07348725944757462, acc: 0.9808917045593262)
[2025-02-13 20:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:56][root][INFO] - Training Epoch: 2/2, step 945/7134 completed (loss: 0.0893375501036644, acc: 0.9735099077224731)
[2025-02-13 20:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57][root][INFO] - Training Epoch: 2/2, step 946/7134 completed (loss: 0.03747578710317612, acc: 0.9902912378311157)
[2025-02-13 20:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57][root][INFO] - Training Epoch: 2/2, step 947/7134 completed (loss: 0.16987299919128418, acc: 0.9558823704719543)
[2025-02-13 20:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57][root][INFO] - Training Epoch: 2/2, step 948/7134 completed (loss: 0.039790909737348557, acc: 0.9939758777618408)
[2025-02-13 20:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58][root][INFO] - Training Epoch: 2/2, step 949/7134 completed (loss: 0.0540798082947731, acc: 0.981249988079071)
[2025-02-13 20:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58][root][INFO] - Training Epoch: 2/2, step 950/7134 completed (loss: 0.07800506800413132, acc: 0.9740259647369385)
[2025-02-13 20:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58][root][INFO] - Training Epoch: 2/2, step 951/7134 completed (loss: 0.0920695960521698, acc: 0.9770992398262024)
[2025-02-13 20:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59][root][INFO] - Training Epoch: 2/2, step 952/7134 completed (loss: 0.13111650943756104, acc: 0.9558823704719543)
[2025-02-13 20:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59][root][INFO] - Training Epoch: 2/2, step 953/7134 completed (loss: 0.1140645295381546, acc: 0.9756097793579102)
[2025-02-13 20:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00][root][INFO] - Training Epoch: 2/2, step 954/7134 completed (loss: 0.13336504995822906, acc: 0.9586206674575806)
[2025-02-13 20:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00][root][INFO] - Training Epoch: 2/2, step 955/7134 completed (loss: 0.14320151507854462, acc: 0.9604519605636597)
[2025-02-13 20:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00][root][INFO] - Training Epoch: 2/2, step 956/7134 completed (loss: 0.12157119065523148, acc: 0.9728260636329651)
[2025-02-13 20:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01][root][INFO] - Training Epoch: 2/2, step 957/7134 completed (loss: 0.12811283767223358, acc: 0.9754902124404907)
[2025-02-13 20:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01][root][INFO] - Training Epoch: 2/2, step 958/7134 completed (loss: 0.11174841970205307, acc: 0.9552238583564758)
[2025-02-13 20:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01][root][INFO] - Training Epoch: 2/2, step 959/7134 completed (loss: 0.1818573772907257, acc: 0.9484536051750183)
[2025-02-13 20:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:02][root][INFO] - Training Epoch: 2/2, step 960/7134 completed (loss: 0.24079598486423492, acc: 0.9477611780166626)
[2025-02-13 20:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:02][root][INFO] - Training Epoch: 2/2, step 961/7134 completed (loss: 0.1492651253938675, acc: 0.9450549483299255)
[2025-02-13 20:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03][root][INFO] - Training Epoch: 2/2, step 962/7134 completed (loss: 0.2240048050880432, acc: 0.9239766001701355)
[2025-02-13 20:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03][root][INFO] - Training Epoch: 2/2, step 963/7134 completed (loss: 0.15410052239894867, acc: 0.9534883499145508)
[2025-02-13 20:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03][root][INFO] - Training Epoch: 2/2, step 964/7134 completed (loss: 0.3197857439517975, acc: 0.9512194991111755)
[2025-02-13 20:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:04][root][INFO] - Training Epoch: 2/2, step 965/7134 completed (loss: 0.20094475150108337, acc: 0.9572192430496216)
[2025-02-13 20:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:04][root][INFO] - Training Epoch: 2/2, step 966/7134 completed (loss: 0.07995288074016571, acc: 0.9754601120948792)
[2025-02-13 20:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05][root][INFO] - Training Epoch: 2/2, step 967/7134 completed (loss: 0.0766868069767952, acc: 0.978723406791687)
[2025-02-13 20:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05][root][INFO] - Training Epoch: 2/2, step 968/7134 completed (loss: 0.18181516230106354, acc: 0.9398496150970459)
[2025-02-13 20:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05][root][INFO] - Training Epoch: 2/2, step 969/7134 completed (loss: 0.18394945561885834, acc: 0.9485714435577393)
[2025-02-13 20:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06][root][INFO] - Training Epoch: 2/2, step 970/7134 completed (loss: 0.2632947564125061, acc: 0.9411764740943909)
[2025-02-13 20:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06][root][INFO] - Training Epoch: 2/2, step 971/7134 completed (loss: 0.15367750823497772, acc: 0.9573459625244141)
[2025-02-13 20:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06][root][INFO] - Training Epoch: 2/2, step 972/7134 completed (loss: 0.19724491238594055, acc: 0.9384615421295166)
[2025-02-13 20:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:07][root][INFO] - Training Epoch: 2/2, step 973/7134 completed (loss: 0.1777358055114746, acc: 0.9560439586639404)
[2025-02-13 20:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:07][root][INFO] - Training Epoch: 2/2, step 974/7134 completed (loss: 0.12082936614751816, acc: 0.9752475023269653)
[2025-02-13 20:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08][root][INFO] - Training Epoch: 2/2, step 975/7134 completed (loss: 0.17962545156478882, acc: 0.978723406791687)
[2025-02-13 20:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08][root][INFO] - Training Epoch: 2/2, step 976/7134 completed (loss: 0.08606996387243271, acc: 0.9668874144554138)
[2025-02-13 20:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08][root][INFO] - Training Epoch: 2/2, step 977/7134 completed (loss: 0.25831952691078186, acc: 0.915730357170105)
[2025-02-13 20:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:09][root][INFO] - Training Epoch: 2/2, step 978/7134 completed (loss: 0.07328880578279495, acc: 0.9639175534248352)
[2025-02-13 20:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:09][root][INFO] - Training Epoch: 2/2, step 979/7134 completed (loss: 0.10385588556528091, acc: 0.9621621370315552)
[2025-02-13 20:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10][root][INFO] - Training Epoch: 2/2, step 980/7134 completed (loss: 0.09501683712005615, acc: 0.9718309640884399)
[2025-02-13 20:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10][root][INFO] - Training Epoch: 2/2, step 981/7134 completed (loss: 0.2496379166841507, acc: 0.9748427867889404)
[2025-02-13 20:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10][root][INFO] - Training Epoch: 2/2, step 982/7134 completed (loss: 0.2758321166038513, acc: 0.954285740852356)
[2025-02-13 20:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11][root][INFO] - Training Epoch: 2/2, step 983/7134 completed (loss: 0.1279595047235489, acc: 0.9774436354637146)
[2025-02-13 20:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11][root][INFO] - Training Epoch: 2/2, step 984/7134 completed (loss: 0.02658066712319851, acc: 0.9929078221321106)
[2025-02-13 20:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11][root][INFO] - Training Epoch: 2/2, step 985/7134 completed (loss: 0.10751888155937195, acc: 0.9777777791023254)
[2025-02-13 20:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12][root][INFO] - Training Epoch: 2/2, step 986/7134 completed (loss: 0.07584114372730255, acc: 0.9776536226272583)
[2025-02-13 20:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12][root][INFO] - Training Epoch: 2/2, step 987/7134 completed (loss: 0.031101493164896965, acc: 1.0)
[2025-02-13 20:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12][root][INFO] - Training Epoch: 2/2, step 988/7134 completed (loss: 0.27226054668426514, acc: 0.9491525292396545)
[2025-02-13 20:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:13][root][INFO] - Training Epoch: 2/2, step 989/7134 completed (loss: 0.20023350417613983, acc: 0.9386503100395203)
[2025-02-13 20:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:13][root][INFO] - Training Epoch: 2/2, step 990/7134 completed (loss: 0.13345885276794434, acc: 0.9725274443626404)
[2025-02-13 20:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14][root][INFO] - Training Epoch: 2/2, step 991/7134 completed (loss: 0.36447903513908386, acc: 0.9308176040649414)
[2025-02-13 20:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14][root][INFO] - Training Epoch: 2/2, step 992/7134 completed (loss: 0.08555734902620316, acc: 0.9826589822769165)
[2025-02-13 20:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14][root][INFO] - Training Epoch: 2/2, step 993/7134 completed (loss: 0.12371090799570084, acc: 0.971222996711731)
[2025-02-13 20:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15][root][INFO] - Training Epoch: 2/2, step 994/7134 completed (loss: 0.14177659153938293, acc: 0.9660193920135498)
[2025-02-13 20:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15][root][INFO] - Training Epoch: 2/2, step 995/7134 completed (loss: 0.10037779808044434, acc: 0.9727891087532043)
[2025-02-13 20:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15][root][INFO] - Training Epoch: 2/2, step 996/7134 completed (loss: 0.18810708820819855, acc: 0.9593023061752319)
[2025-02-13 20:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16][root][INFO] - Training Epoch: 2/2, step 997/7134 completed (loss: 0.09397739171981812, acc: 0.9764705896377563)
[2025-02-13 20:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16][root][INFO] - Training Epoch: 2/2, step 998/7134 completed (loss: 0.12558795511722565, acc: 0.976047933101654)
[2025-02-13 20:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17][root][INFO] - Training Epoch: 2/2, step 999/7134 completed (loss: 0.07393474131822586, acc: 0.9769230484962463)
[2025-02-13 20:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17][root][INFO] - Training Epoch: 2/2, step 1000/7134 completed (loss: 0.07383746653795242, acc: 0.9852941036224365)
[2025-02-13 20:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17][root][INFO] - Training Epoch: 2/2, step 1001/7134 completed (loss: 0.050027795135974884, acc: 0.9818181991577148)
[2025-02-13 20:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18][root][INFO] - Training Epoch: 2/2, step 1002/7134 completed (loss: 0.05088819935917854, acc: 0.9811320900917053)
[2025-02-13 20:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18][root][INFO] - Training Epoch: 2/2, step 1003/7134 completed (loss: 0.03502087667584419, acc: 0.9930555820465088)
[2025-02-13 20:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18][root][INFO] - Training Epoch: 2/2, step 1004/7134 completed (loss: 0.1622694432735443, acc: 0.9557521939277649)
[2025-02-13 20:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19][root][INFO] - Training Epoch: 2/2, step 1005/7134 completed (loss: 0.11160984635353088, acc: 0.9645389914512634)
[2025-02-13 20:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19][root][INFO] - Training Epoch: 2/2, step 1006/7134 completed (loss: 0.13223588466644287, acc: 0.9490445852279663)
[2025-02-13 20:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20][root][INFO] - Training Epoch: 2/2, step 1007/7134 completed (loss: 0.3658304810523987, acc: 0.922535240650177)
[2025-02-13 20:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20][root][INFO] - Training Epoch: 2/2, step 1008/7134 completed (loss: 0.0937342569231987, acc: 0.9753086566925049)
[2025-02-13 20:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20][root][INFO] - Training Epoch: 2/2, step 1009/7134 completed (loss: 0.08800125122070312, acc: 0.9922480583190918)
[2025-02-13 20:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21][root][INFO] - Training Epoch: 2/2, step 1010/7134 completed (loss: 0.11678696423768997, acc: 0.9572649598121643)
[2025-02-13 20:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21][root][INFO] - Training Epoch: 2/2, step 1011/7134 completed (loss: 0.05502290651202202, acc: 0.9937106966972351)
[2025-02-13 20:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21][root][INFO] - Training Epoch: 2/2, step 1012/7134 completed (loss: 0.25358399748802185, acc: 0.9437500238418579)
[2025-02-13 20:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:22][root][INFO] - Training Epoch: 2/2, step 1013/7134 completed (loss: 0.21671795845031738, acc: 0.9578313231468201)
[2025-02-13 20:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:22][root][INFO] - Training Epoch: 2/2, step 1014/7134 completed (loss: 0.11654291301965714, acc: 0.9716312289237976)
[2025-02-13 20:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23][root][INFO] - Training Epoch: 2/2, step 1015/7134 completed (loss: 0.06272577494382858, acc: 0.9933775067329407)
[2025-02-13 20:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23][root][INFO] - Training Epoch: 2/2, step 1016/7134 completed (loss: 0.04524178430438042, acc: 0.9831932783126831)
[2025-02-13 20:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23][root][INFO] - Training Epoch: 2/2, step 1017/7134 completed (loss: 0.0245501808822155, acc: 1.0)
[2025-02-13 20:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:24][root][INFO] - Training Epoch: 2/2, step 1018/7134 completed (loss: 0.10385187715291977, acc: 0.9751552939414978)
[2025-02-13 20:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:24][root][INFO] - Training Epoch: 2/2, step 1019/7134 completed (loss: 0.08109406381845474, acc: 0.9805194735527039)
[2025-02-13 20:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25][root][INFO] - Training Epoch: 2/2, step 1020/7134 completed (loss: 0.029234355315566063, acc: 1.0)
[2025-02-13 20:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25][root][INFO] - Training Epoch: 2/2, step 1021/7134 completed (loss: 0.05508754402399063, acc: 0.9869281053543091)
[2025-02-13 20:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25][root][INFO] - Training Epoch: 2/2, step 1022/7134 completed (loss: 0.05296926945447922, acc: 0.9768785834312439)
[2025-02-13 20:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26][root][INFO] - Training Epoch: 2/2, step 1023/7134 completed (loss: 0.07456326484680176, acc: 0.994413435459137)
[2025-02-13 20:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26][root][INFO] - Training Epoch: 2/2, step 1024/7134 completed (loss: 0.12033791095018387, acc: 0.9701492786407471)
[2025-02-13 20:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26][root][INFO] - Training Epoch: 2/2, step 1025/7134 completed (loss: 0.05602632090449333, acc: 0.978723406791687)
[2025-02-13 20:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:27][root][INFO] - Training Epoch: 2/2, step 1026/7134 completed (loss: 0.16326904296875, acc: 0.9646017551422119)
[2025-02-13 20:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:27][root][INFO] - Training Epoch: 2/2, step 1027/7134 completed (loss: 0.04212191700935364, acc: 0.9882352948188782)
[2025-02-13 20:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28][root][INFO] - Training Epoch: 2/2, step 1028/7134 completed (loss: 0.027362719178199768, acc: 1.0)
[2025-02-13 20:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28][root][INFO] - Training Epoch: 2/2, step 1029/7134 completed (loss: 0.019144881516695023, acc: 1.0)
[2025-02-13 20:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28][root][INFO] - Training Epoch: 2/2, step 1030/7134 completed (loss: 0.054339781403541565, acc: 0.977011501789093)
[2025-02-13 20:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29][root][INFO] - Training Epoch: 2/2, step 1031/7134 completed (loss: 0.16175620257854462, acc: 0.95652174949646)
[2025-02-13 20:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29][root][INFO] - Training Epoch: 2/2, step 1032/7134 completed (loss: 0.060176096856594086, acc: 0.9840425252914429)
[2025-02-13 20:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29][root][INFO] - Training Epoch: 2/2, step 1033/7134 completed (loss: 0.027291318401694298, acc: 1.0)
[2025-02-13 20:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30][root][INFO] - Training Epoch: 2/2, step 1034/7134 completed (loss: 0.22402629256248474, acc: 0.9285714030265808)
[2025-02-13 20:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30][root][INFO] - Training Epoch: 2/2, step 1035/7134 completed (loss: 0.3356735110282898, acc: 0.9189189076423645)
[2025-02-13 20:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31][root][INFO] - Training Epoch: 2/2, step 1036/7134 completed (loss: 0.4119979739189148, acc: 0.8813559412956238)
[2025-02-13 20:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31][root][INFO] - Training Epoch: 2/2, step 1037/7134 completed (loss: 0.045732948929071426, acc: 0.9840425252914429)
[2025-02-13 20:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31][root][INFO] - Training Epoch: 2/2, step 1038/7134 completed (loss: 0.06681106239557266, acc: 0.984000027179718)
[2025-02-13 20:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32][root][INFO] - Training Epoch: 2/2, step 1039/7134 completed (loss: 0.14260134100914001, acc: 0.963302731513977)
[2025-02-13 20:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32][root][INFO] - Training Epoch: 2/2, step 1040/7134 completed (loss: 0.03693602234125137, acc: 0.9927536249160767)
[2025-02-13 20:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32][root][INFO] - Training Epoch: 2/2, step 1041/7134 completed (loss: 0.0310499370098114, acc: 1.0)
[2025-02-13 20:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:33][root][INFO] - Training Epoch: 2/2, step 1042/7134 completed (loss: 0.1142432913184166, acc: 0.9553072452545166)
[2025-02-13 20:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:33][root][INFO] - Training Epoch: 2/2, step 1043/7134 completed (loss: 0.1019287109375, acc: 0.9752475023269653)
[2025-02-13 20:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34][root][INFO] - Training Epoch: 2/2, step 1044/7134 completed (loss: 0.032248303294181824, acc: 0.9937888383865356)
[2025-02-13 20:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34][root][INFO] - Training Epoch: 2/2, step 1045/7134 completed (loss: 0.2715124189853668, acc: 0.9192546606063843)
[2025-02-13 20:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34][root][INFO] - Training Epoch: 2/2, step 1046/7134 completed (loss: 0.2122807800769806, acc: 0.9548872113227844)
[2025-02-13 20:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35][root][INFO] - Training Epoch: 2/2, step 1047/7134 completed (loss: 0.2538892924785614, acc: 0.9314285516738892)
[2025-02-13 20:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35][root][INFO] - Training Epoch: 2/2, step 1048/7134 completed (loss: 0.1442350447177887, acc: 0.9745222926139832)
[2025-02-13 20:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36][root][INFO] - Training Epoch: 2/2, step 1049/7134 completed (loss: 0.11404332518577576, acc: 0.9802631735801697)
[2025-02-13 20:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36][root][INFO] - Training Epoch: 2/2, step 1050/7134 completed (loss: 0.23476870357990265, acc: 0.9375)
[2025-02-13 20:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36][root][INFO] - Training Epoch: 2/2, step 1051/7134 completed (loss: 0.06910481303930283, acc: 0.9828571677207947)
[2025-02-13 20:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37][root][INFO] - Training Epoch: 2/2, step 1052/7134 completed (loss: 0.10185062140226364, acc: 0.9578313231468201)
[2025-02-13 20:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37][root][INFO] - Training Epoch: 2/2, step 1053/7134 completed (loss: 0.15378475189208984, acc: 0.970059871673584)
[2025-02-13 20:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37][root][INFO] - Training Epoch: 2/2, step 1054/7134 completed (loss: 0.06571807712316513, acc: 0.984375)
[2025-02-13 20:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:38][root][INFO] - Training Epoch: 2/2, step 1055/7134 completed (loss: 0.04580620676279068, acc: 0.9847328066825867)
[2025-02-13 20:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:38][root][INFO] - Training Epoch: 2/2, step 1056/7134 completed (loss: 0.14526699483394623, acc: 0.9709302186965942)
[2025-02-13 20:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39][root][INFO] - Training Epoch: 2/2, step 1057/7134 completed (loss: 0.10813300311565399, acc: 0.9792746305465698)
[2025-02-13 20:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39][root][INFO] - Training Epoch: 2/2, step 1058/7134 completed (loss: 0.09314439445734024, acc: 0.9736841917037964)
[2025-02-13 20:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39][root][INFO] - Training Epoch: 2/2, step 1059/7134 completed (loss: 0.08126664161682129, acc: 0.9810426831245422)
[2025-02-13 20:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40][root][INFO] - Training Epoch: 2/2, step 1060/7134 completed (loss: 0.11671854555606842, acc: 0.9743589758872986)
[2025-02-13 20:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40][root][INFO] - Training Epoch: 2/2, step 1061/7134 completed (loss: 0.09984895586967468, acc: 0.9742268323898315)
[2025-02-13 20:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41][root][INFO] - Training Epoch: 2/2, step 1062/7134 completed (loss: 0.09124541282653809, acc: 0.9759036302566528)
[2025-02-13 20:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41][root][INFO] - Training Epoch: 2/2, step 1063/7134 completed (loss: 0.03218221664428711, acc: 1.0)
[2025-02-13 20:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41][root][INFO] - Training Epoch: 2/2, step 1064/7134 completed (loss: 0.13751435279846191, acc: 0.9567567706108093)
[2025-02-13 20:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42][root][INFO] - Training Epoch: 2/2, step 1065/7134 completed (loss: 0.10067509114742279, acc: 0.9779005646705627)
[2025-02-13 20:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42][root][INFO] - Training Epoch: 2/2, step 1066/7134 completed (loss: 0.1447698324918747, acc: 0.9618320465087891)
[2025-02-13 20:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42][root][INFO] - Training Epoch: 2/2, step 1067/7134 completed (loss: 0.1032501831650734, acc: 0.9673202633857727)
[2025-02-13 20:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:43][root][INFO] - Training Epoch: 2/2, step 1068/7134 completed (loss: 0.1636057198047638, acc: 0.9453125)
[2025-02-13 20:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:43][root][INFO] - Training Epoch: 2/2, step 1069/7134 completed (loss: 0.11375585943460464, acc: 0.9583333134651184)
[2025-02-13 20:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44][root][INFO] - Training Epoch: 2/2, step 1070/7134 completed (loss: 0.10706629604101181, acc: 0.9726027250289917)
[2025-02-13 20:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44][root][INFO] - Training Epoch: 2/2, step 1071/7134 completed (loss: 0.05611153319478035, acc: 0.981249988079071)
[2025-02-13 20:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44][root][INFO] - Training Epoch: 2/2, step 1072/7134 completed (loss: 0.01783393882215023, acc: 0.9939024448394775)
[2025-02-13 20:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45][root][INFO] - Training Epoch: 2/2, step 1073/7134 completed (loss: 0.04453875869512558, acc: 0.9813664555549622)
[2025-02-13 20:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45][root][INFO] - Training Epoch: 2/2, step 1074/7134 completed (loss: 0.11104162037372589, acc: 0.9716312289237976)
[2025-02-13 20:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45][root][INFO] - Training Epoch: 2/2, step 1075/7134 completed (loss: 0.27335676550865173, acc: 0.9635036587715149)
[2025-02-13 20:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46][root][INFO] - Training Epoch: 2/2, step 1076/7134 completed (loss: 0.1991080492734909, acc: 0.9468085169792175)
[2025-02-13 20:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46][root][INFO] - Training Epoch: 2/2, step 1077/7134 completed (loss: 0.15700788795948029, acc: 0.9646017551422119)
[2025-02-13 20:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47][root][INFO] - Training Epoch: 2/2, step 1078/7134 completed (loss: 0.2836835980415344, acc: 0.9308176040649414)
[2025-02-13 20:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47][root][INFO] - Training Epoch: 2/2, step 1079/7134 completed (loss: 0.13481946289539337, acc: 0.9723502397537231)
[2025-02-13 20:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47][root][INFO] - Training Epoch: 2/2, step 1080/7134 completed (loss: 0.17798155546188354, acc: 0.9767441749572754)
[2025-02-13 20:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:48][root][INFO] - Training Epoch: 2/2, step 1081/7134 completed (loss: 0.13247151672840118, acc: 0.9578313231468201)
[2025-02-13 20:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:48][root][INFO] - Training Epoch: 2/2, step 1082/7134 completed (loss: 0.030788714066147804, acc: 0.9910314083099365)
[2025-02-13 20:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49][root][INFO] - Training Epoch: 2/2, step 1083/7134 completed (loss: 0.10753501206636429, acc: 0.9746192693710327)
[2025-02-13 20:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49][root][INFO] - Training Epoch: 2/2, step 1084/7134 completed (loss: 0.07567690312862396, acc: 0.9734042286872864)
[2025-02-13 20:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49][root][INFO] - Training Epoch: 2/2, step 1085/7134 completed (loss: 0.06129191815853119, acc: 0.9748427867889404)
[2025-02-13 20:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:50][root][INFO] - Training Epoch: 2/2, step 1086/7134 completed (loss: 0.16007117927074432, acc: 0.9596773982048035)
[2025-02-13 20:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:50][root][INFO] - Training Epoch: 2/2, step 1087/7134 completed (loss: 0.10119558870792389, acc: 0.9784946441650391)
[2025-02-13 20:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51][root][INFO] - Training Epoch: 2/2, step 1088/7134 completed (loss: 0.06835276633501053, acc: 0.9886363744735718)
[2025-02-13 20:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51][root][INFO] - Training Epoch: 2/2, step 1089/7134 completed (loss: 0.1384691745042801, acc: 0.9693251252174377)
[2025-02-13 20:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51][root][INFO] - Training Epoch: 2/2, step 1090/7134 completed (loss: 0.1160271167755127, acc: 0.9583333134651184)
[2025-02-13 20:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52][root][INFO] - Training Epoch: 2/2, step 1091/7134 completed (loss: 0.08568558096885681, acc: 0.9689440727233887)
[2025-02-13 20:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52][root][INFO] - Training Epoch: 2/2, step 1092/7134 completed (loss: 0.10554641485214233, acc: 0.970059871673584)
[2025-02-13 20:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52][root][INFO] - Training Epoch: 2/2, step 1093/7134 completed (loss: 0.09673650562763214, acc: 0.9863945841789246)
[2025-02-13 20:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53][root][INFO] - Training Epoch: 2/2, step 1094/7134 completed (loss: 0.258099228143692, acc: 0.9473684430122375)
[2025-02-13 20:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53][root][INFO] - Training Epoch: 2/2, step 1095/7134 completed (loss: 0.1163221001625061, acc: 0.9894179701805115)
[2025-02-13 20:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54][root][INFO] - Training Epoch: 2/2, step 1096/7134 completed (loss: 0.22941796481609344, acc: 0.9247311949729919)
[2025-02-13 20:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54][root][INFO] - Training Epoch: 2/2, step 1097/7134 completed (loss: 0.18459220230579376, acc: 0.9453551769256592)
[2025-02-13 20:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54][root][INFO] - Training Epoch: 2/2, step 1098/7134 completed (loss: 0.04823562875390053, acc: 0.9888888597488403)
[2025-02-13 20:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55][root][INFO] - Training Epoch: 2/2, step 1099/7134 completed (loss: 0.12475500255823135, acc: 0.9791666865348816)
[2025-02-13 20:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55][root][INFO] - Training Epoch: 2/2, step 1100/7134 completed (loss: 0.13471494615077972, acc: 0.9649122953414917)
[2025-02-13 20:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56][root][INFO] - Training Epoch: 2/2, step 1101/7134 completed (loss: 0.20320385694503784, acc: 0.9487179517745972)
[2025-02-13 20:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56][root][INFO] - Training Epoch: 2/2, step 1102/7134 completed (loss: 0.32898104190826416, acc: 0.9097222089767456)
[2025-02-13 20:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56][root][INFO] - Training Epoch: 2/2, step 1103/7134 completed (loss: 0.31964364647865295, acc: 0.9523809552192688)
[2025-02-13 20:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57][root][INFO] - Training Epoch: 2/2, step 1104/7134 completed (loss: 0.09708519279956818, acc: 0.9800000190734863)
[2025-02-13 20:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57][root][INFO] - Training Epoch: 2/2, step 1105/7134 completed (loss: 0.10144657641649246, acc: 0.9753086566925049)
[2025-02-13 20:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58][root][INFO] - Training Epoch: 2/2, step 1106/7134 completed (loss: 0.05059479549527168, acc: 0.9933775067329407)
[2025-02-13 20:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58][root][INFO] - Training Epoch: 2/2, step 1107/7134 completed (loss: 0.024502823129296303, acc: 1.0)
[2025-02-13 20:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58][root][INFO] - Training Epoch: 2/2, step 1108/7134 completed (loss: 0.044038139283657074, acc: 0.9940828680992126)
[2025-02-13 20:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59][root][INFO] - Training Epoch: 2/2, step 1109/7134 completed (loss: 0.10798102617263794, acc: 0.9768785834312439)
[2025-02-13 20:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59][root][INFO] - Training Epoch: 2/2, step 1110/7134 completed (loss: 0.1288733333349228, acc: 0.9731183052062988)
[2025-02-13 20:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59][root][INFO] - Training Epoch: 2/2, step 1111/7134 completed (loss: 0.07096656411886215, acc: 0.981249988079071)
[2025-02-13 20:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00][root][INFO] - Training Epoch: 2/2, step 1112/7134 completed (loss: 0.06017354130744934, acc: 0.9860140085220337)
[2025-02-13 20:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00][root][INFO] - Training Epoch: 2/2, step 1113/7134 completed (loss: 0.09461943060159683, acc: 0.9748427867889404)
[2025-02-13 20:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00][root][INFO] - Training Epoch: 2/2, step 1114/7134 completed (loss: 0.1891608089208603, acc: 0.9523809552192688)
[2025-02-13 20:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:01][root][INFO] - Training Epoch: 2/2, step 1115/7134 completed (loss: 0.07017870992422104, acc: 0.9806451797485352)
[2025-02-13 20:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:01][root][INFO] - Training Epoch: 2/2, step 1116/7134 completed (loss: 0.06161191314458847, acc: 0.9674796462059021)
[2025-02-13 20:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02][root][INFO] - Training Epoch: 2/2, step 1117/7134 completed (loss: 0.06890762597322464, acc: 0.9802631735801697)
[2025-02-13 20:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02][root][INFO] - Training Epoch: 2/2, step 1118/7134 completed (loss: 0.05143670365214348, acc: 0.9847328066825867)
[2025-02-13 20:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02][root][INFO] - Training Epoch: 2/2, step 1119/7134 completed (loss: 0.08537063747644424, acc: 0.9724137783050537)
[2025-02-13 20:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03][root][INFO] - Training Epoch: 2/2, step 1120/7134 completed (loss: 0.12237991392612457, acc: 0.9770992398262024)
[2025-02-13 20:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03][root][INFO] - Training Epoch: 2/2, step 1121/7134 completed (loss: 0.11754824966192245, acc: 0.9677419066429138)
[2025-02-13 20:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04][root][INFO] - Training Epoch: 2/2, step 1122/7134 completed (loss: 0.01752694696187973, acc: 1.0)
[2025-02-13 20:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04][root][INFO] - Training Epoch: 2/2, step 1123/7134 completed (loss: 0.07453791052103043, acc: 0.9724137783050537)
[2025-02-13 20:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04][root][INFO] - Training Epoch: 2/2, step 1124/7134 completed (loss: 0.09333401918411255, acc: 0.9729729890823364)
[2025-02-13 20:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05][root][INFO] - Training Epoch: 2/2, step 1125/7134 completed (loss: 0.076431043446064, acc: 0.9821428656578064)
[2025-02-13 20:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05][root][INFO] - Training Epoch: 2/2, step 1126/7134 completed (loss: 0.10360224545001984, acc: 0.966292142868042)
[2025-02-13 20:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05][root][INFO] - Training Epoch: 2/2, step 1127/7134 completed (loss: 0.03919381648302078, acc: 0.9852941036224365)
[2025-02-13 20:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06][root][INFO] - Training Epoch: 2/2, step 1128/7134 completed (loss: 0.08461873978376389, acc: 0.9847328066825867)
[2025-02-13 20:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06][root][INFO] - Training Epoch: 2/2, step 1129/7134 completed (loss: 0.08475100249052048, acc: 0.9750000238418579)
[2025-02-13 20:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07][root][INFO] - Training Epoch: 2/2, step 1130/7134 completed (loss: 0.0334639735519886, acc: 0.9856114983558655)
[2025-02-13 20:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07][root][INFO] - Training Epoch: 2/2, step 1131/7134 completed (loss: 0.10520678013563156, acc: 0.9848484992980957)
[2025-02-13 20:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07][root][INFO] - Training Epoch: 2/2, step 1132/7134 completed (loss: 0.28165993094444275, acc: 0.9395973086357117)
[2025-02-13 20:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08][root][INFO] - Training Epoch: 2/2, step 1133/7134 completed (loss: 0.2054135799407959, acc: 0.9595375657081604)
[2025-02-13 20:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08][root][INFO] - Training Epoch: 2/2, step 1134/7134 completed (loss: 0.10573463886976242, acc: 0.9725274443626404)
[2025-02-13 20:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08][root][INFO] - Training Epoch: 2/2, step 1135/7134 completed (loss: 0.07104494422674179, acc: 0.9798657894134521)
[2025-02-13 20:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09][root][INFO] - Training Epoch: 2/2, step 1136/7134 completed (loss: 0.22299696505069733, acc: 0.9387755393981934)
[2025-02-13 20:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09][root][INFO] - Training Epoch: 2/2, step 1137/7134 completed (loss: 0.16151712834835052, acc: 0.9528301954269409)
[2025-02-13 20:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10][root][INFO] - Training Epoch: 2/2, step 1138/7134 completed (loss: 0.11363052576780319, acc: 0.9700000286102295)
[2025-02-13 20:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10][root][INFO] - Training Epoch: 2/2, step 1139/7134 completed (loss: 0.10725103318691254, acc: 0.9725274443626404)
[2025-02-13 20:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10][root][INFO] - Training Epoch: 2/2, step 1140/7134 completed (loss: 0.05215020477771759, acc: 0.9894737005233765)
[2025-02-13 20:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11][root][INFO] - Training Epoch: 2/2, step 1141/7134 completed (loss: 0.056801557540893555, acc: 0.9909090995788574)
[2025-02-13 20:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11][root][INFO] - Training Epoch: 2/2, step 1142/7134 completed (loss: 0.11825644969940186, acc: 0.9777777791023254)
[2025-02-13 20:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11][root][INFO] - Training Epoch: 2/2, step 1143/7134 completed (loss: 0.23295500874519348, acc: 0.9578313231468201)
[2025-02-13 20:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12][root][INFO] - Training Epoch: 2/2, step 1144/7134 completed (loss: 0.16415084898471832, acc: 0.9702380895614624)
[2025-02-13 20:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12][root][INFO] - Training Epoch: 2/2, step 1145/7134 completed (loss: 0.07812807708978653, acc: 0.9888268113136292)
[2025-02-13 20:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12][root][INFO] - Training Epoch: 2/2, step 1146/7134 completed (loss: 0.03346193954348564, acc: 0.9924242496490479)
[2025-02-13 20:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:13][root][INFO] - Training Epoch: 2/2, step 1147/7134 completed (loss: 0.08884959667921066, acc: 0.9837837815284729)
[2025-02-13 20:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:13][root][INFO] - Training Epoch: 2/2, step 1148/7134 completed (loss: 0.04310857877135277, acc: 0.9928057789802551)
[2025-02-13 20:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14][root][INFO] - Training Epoch: 2/2, step 1149/7134 completed (loss: 0.1539493054151535, acc: 0.9583333134651184)
[2025-02-13 20:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14][root][INFO] - Training Epoch: 2/2, step 1150/7134 completed (loss: 0.06818384677171707, acc: 0.9803921580314636)
[2025-02-13 20:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14][root][INFO] - Training Epoch: 2/2, step 1151/7134 completed (loss: 0.07306346297264099, acc: 0.9747899174690247)
[2025-02-13 20:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15][root][INFO] - Training Epoch: 2/2, step 1152/7134 completed (loss: 0.04961993917822838, acc: 0.9919354915618896)
[2025-02-13 20:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15][root][INFO] - Training Epoch: 2/2, step 1153/7134 completed (loss: 0.10297638177871704, acc: 0.9803921580314636)
[2025-02-13 20:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15][root][INFO] - Training Epoch: 2/2, step 1154/7134 completed (loss: 0.08225101977586746, acc: 0.9765625)
[2025-02-13 20:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16][root][INFO] - Training Epoch: 2/2, step 1155/7134 completed (loss: 0.09709611535072327, acc: 0.9914529919624329)
[2025-02-13 20:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16][root][INFO] - Training Epoch: 2/2, step 1156/7134 completed (loss: 0.02491830475628376, acc: 1.0)
[2025-02-13 20:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17][root][INFO] - Training Epoch: 2/2, step 1157/7134 completed (loss: 0.29361459612846375, acc: 0.931034505367279)
[2025-02-13 20:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17][root][INFO] - Training Epoch: 2/2, step 1158/7134 completed (loss: 0.10370543599128723, acc: 0.9739130139350891)
[2025-02-13 20:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17][root][INFO] - Training Epoch: 2/2, step 1159/7134 completed (loss: 0.08387216925621033, acc: 0.9820359349250793)
[2025-02-13 20:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18][root][INFO] - Training Epoch: 2/2, step 1160/7134 completed (loss: 0.02960813418030739, acc: 0.9938271641731262)
[2025-02-13 20:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18][root][INFO] - Training Epoch: 2/2, step 1161/7134 completed (loss: 0.059412699192762375, acc: 0.9710144996643066)
[2025-02-13 20:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19][root][INFO] - Training Epoch: 2/2, step 1162/7134 completed (loss: 0.1445392221212387, acc: 0.9620253443717957)
[2025-02-13 20:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19][root][INFO] - Training Epoch: 2/2, step 1163/7134 completed (loss: 0.06263168156147003, acc: 0.9793103337287903)
[2025-02-13 20:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19][root][INFO] - Training Epoch: 2/2, step 1164/7134 completed (loss: 0.11975473165512085, acc: 0.9723756909370422)
[2025-02-13 20:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20][root][INFO] - Training Epoch: 2/2, step 1165/7134 completed (loss: 0.05170240253210068, acc: 0.9794871807098389)
[2025-02-13 20:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20][root][INFO] - Training Epoch: 2/2, step 1166/7134 completed (loss: 0.029701458290219307, acc: 0.9946808218955994)
[2025-02-13 20:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20][root][INFO] - Training Epoch: 2/2, step 1167/7134 completed (loss: 0.027666615322232246, acc: 0.9919999837875366)
[2025-02-13 20:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21][root][INFO] - Training Epoch: 2/2, step 1168/7134 completed (loss: 0.06621987372636795, acc: 0.9841269850730896)
[2025-02-13 20:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21][root][INFO] - Training Epoch: 2/2, step 1169/7134 completed (loss: 0.10906101763248444, acc: 0.957446813583374)
[2025-02-13 20:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22][root][INFO] - Training Epoch: 2/2, step 1170/7134 completed (loss: 0.19186100363731384, acc: 0.9604519605636597)
[2025-02-13 20:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22][root][INFO] - Training Epoch: 2/2, step 1171/7134 completed (loss: 0.04833782836794853, acc: 0.9933333396911621)
[2025-02-13 20:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22][root][INFO] - Training Epoch: 2/2, step 1172/7134 completed (loss: 0.06926809251308441, acc: 0.9821428656578064)
[2025-02-13 20:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:23][root][INFO] - Training Epoch: 2/2, step 1173/7134 completed (loss: 0.06904907524585724, acc: 0.9841269850730896)
[2025-02-13 20:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:23][root][INFO] - Training Epoch: 2/2, step 1174/7134 completed (loss: 0.04184075817465782, acc: 0.987730085849762)
[2025-02-13 20:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24][root][INFO] - Training Epoch: 2/2, step 1175/7134 completed (loss: 0.056978948414325714, acc: 0.9882352948188782)
[2025-02-13 20:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24][root][INFO] - Training Epoch: 2/2, step 1176/7134 completed (loss: 0.0348123162984848, acc: 0.9930555820465088)
[2025-02-13 20:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24][root][INFO] - Training Epoch: 2/2, step 1177/7134 completed (loss: 0.012641878798604012, acc: 1.0)
[2025-02-13 20:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25][root][INFO] - Training Epoch: 2/2, step 1178/7134 completed (loss: 0.038538046181201935, acc: 0.9903846383094788)
[2025-02-13 20:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25][root][INFO] - Training Epoch: 2/2, step 1179/7134 completed (loss: 0.038573045283555984, acc: 0.9838709831237793)
[2025-02-13 20:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25][root][INFO] - Training Epoch: 2/2, step 1180/7134 completed (loss: 0.05803874507546425, acc: 0.9729729890823364)
[2025-02-13 20:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26][root][INFO] - Training Epoch: 2/2, step 1181/7134 completed (loss: 0.03959598019719124, acc: 0.9945054650306702)
[2025-02-13 20:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26][root][INFO] - Training Epoch: 2/2, step 1182/7134 completed (loss: 0.023060064762830734, acc: 1.0)
[2025-02-13 20:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27][root][INFO] - Training Epoch: 2/2, step 1183/7134 completed (loss: 0.014162275940179825, acc: 1.0)
[2025-02-13 20:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27][root][INFO] - Training Epoch: 2/2, step 1184/7134 completed (loss: 0.11640918999910355, acc: 0.9768785834312439)
[2025-02-13 20:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27][root][INFO] - Training Epoch: 2/2, step 1185/7134 completed (loss: 0.01630137860774994, acc: 1.0)
[2025-02-13 20:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28][root][INFO] - Training Epoch: 2/2, step 1186/7134 completed (loss: 0.03182819113135338, acc: 0.9945651888847351)
[2025-02-13 20:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28][root][INFO] - Training Epoch: 2/2, step 1187/7134 completed (loss: 0.06907875835895538, acc: 0.976190447807312)
[2025-02-13 20:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28][root][INFO] - Training Epoch: 2/2, step 1188/7134 completed (loss: 0.028931202366948128, acc: 0.9892473220825195)
[2025-02-13 20:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:29][root][INFO] - Training Epoch: 2/2, step 1189/7134 completed (loss: 0.05936240032315254, acc: 0.9822485446929932)
[2025-02-13 20:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:29][root][INFO] - Training Epoch: 2/2, step 1190/7134 completed (loss: 0.035432055592536926, acc: 0.9888888597488403)
[2025-02-13 20:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30][root][INFO] - Training Epoch: 2/2, step 1191/7134 completed (loss: 0.1470487415790558, acc: 0.9659863710403442)
[2025-02-13 20:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30][root][INFO] - Training Epoch: 2/2, step 1192/7134 completed (loss: 0.04082576185464859, acc: 0.9924812316894531)
[2025-02-13 20:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30][root][INFO] - Training Epoch: 2/2, step 1193/7134 completed (loss: 0.08920572698116302, acc: 0.9763779640197754)
[2025-02-13 20:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31][root][INFO] - Training Epoch: 2/2, step 1194/7134 completed (loss: 0.03073306381702423, acc: 0.9932885766029358)
[2025-02-13 20:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31][root][INFO] - Training Epoch: 2/2, step 1195/7134 completed (loss: 0.04405442997813225, acc: 0.9846153855323792)
[2025-02-13 20:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32][root][INFO] - Training Epoch: 2/2, step 1196/7134 completed (loss: 0.14451588690280914, acc: 0.9645389914512634)
[2025-02-13 20:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32][root][INFO] - Training Epoch: 2/2, step 1197/7134 completed (loss: 0.07263879477977753, acc: 0.9863013625144958)
[2025-02-13 20:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32][root][INFO] - Training Epoch: 2/2, step 1198/7134 completed (loss: 0.12019693851470947, acc: 0.9750000238418579)
[2025-02-13 20:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33][root][INFO] - Training Epoch: 2/2, step 1199/7134 completed (loss: 0.09429571032524109, acc: 0.9805194735527039)
[2025-02-13 20:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33][root][INFO] - Training Epoch: 2/2, step 1200/7134 completed (loss: 0.023683013394474983, acc: 1.0)
[2025-02-13 20:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33][root][INFO] - Training Epoch: 2/2, step 1201/7134 completed (loss: 0.0678485557436943, acc: 0.9874213933944702)
[2025-02-13 20:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34][root][INFO] - Training Epoch: 2/2, step 1202/7134 completed (loss: 0.08872858434915543, acc: 0.9836956262588501)
[2025-02-13 20:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34][root][INFO] - Training Epoch: 2/2, step 1203/7134 completed (loss: 0.04574466124176979, acc: 0.9933333396911621)
[2025-02-13 20:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35][root][INFO] - Training Epoch: 2/2, step 1204/7134 completed (loss: 0.1499740183353424, acc: 0.9779411554336548)
[2025-02-13 20:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35][root][INFO] - Training Epoch: 2/2, step 1205/7134 completed (loss: 0.29604607820510864, acc: 0.931034505367279)
[2025-02-13 20:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35][root][INFO] - Training Epoch: 2/2, step 1206/7134 completed (loss: 0.1586369127035141, acc: 0.9698492288589478)
[2025-02-13 20:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36][root][INFO] - Training Epoch: 2/2, step 1207/7134 completed (loss: 0.07638459652662277, acc: 0.9838709831237793)
[2025-02-13 20:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36][root][INFO] - Training Epoch: 2/2, step 1208/7134 completed (loss: 0.18297487497329712, acc: 0.9694656729698181)
[2025-02-13 20:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36][root][INFO] - Training Epoch: 2/2, step 1209/7134 completed (loss: 0.14129169285297394, acc: 0.9672130942344666)
[2025-02-13 20:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37][root][INFO] - Training Epoch: 2/2, step 1210/7134 completed (loss: 0.1896895319223404, acc: 0.9466666579246521)
[2025-02-13 20:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37][root][INFO] - Training Epoch: 2/2, step 1211/7134 completed (loss: 0.4086362421512604, acc: 0.8920863270759583)
[2025-02-13 20:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38][root][INFO] - Training Epoch: 2/2, step 1212/7134 completed (loss: 0.2749442756175995, acc: 0.9489051103591919)
[2025-02-13 20:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38][root][INFO] - Training Epoch: 2/2, step 1213/7134 completed (loss: 0.2175751030445099, acc: 0.9467455744743347)
[2025-02-13 20:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38][root][INFO] - Training Epoch: 2/2, step 1214/7134 completed (loss: 0.2295813262462616, acc: 0.9200000166893005)
[2025-02-13 20:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39][root][INFO] - Training Epoch: 2/2, step 1215/7134 completed (loss: 0.21571053564548492, acc: 0.9411764740943909)
[2025-02-13 20:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39][root][INFO] - Training Epoch: 2/2, step 1216/7134 completed (loss: 0.07820035517215729, acc: 0.9867549538612366)
[2025-02-13 20:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39][root][INFO] - Training Epoch: 2/2, step 1217/7134 completed (loss: 0.129612997174263, acc: 0.9671052694320679)
[2025-02-13 20:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:40][root][INFO] - Training Epoch: 2/2, step 1218/7134 completed (loss: 0.19246232509613037, acc: 0.949999988079071)
[2025-02-13 20:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:40][root][INFO] - Training Epoch: 2/2, step 1219/7134 completed (loss: 0.19316312670707703, acc: 0.9466666579246521)
[2025-02-13 20:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41][root][INFO] - Training Epoch: 2/2, step 1220/7134 completed (loss: 0.16577492654323578, acc: 0.9639175534248352)
[2025-02-13 20:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41][root][INFO] - Training Epoch: 2/2, step 1221/7134 completed (loss: 0.17494605481624603, acc: 0.9547511339187622)
[2025-02-13 20:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41][root][INFO] - Training Epoch: 2/2, step 1222/7134 completed (loss: 0.22945046424865723, acc: 0.9611650705337524)
[2025-02-13 20:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:42][root][INFO] - Training Epoch: 2/2, step 1223/7134 completed (loss: 0.3526664972305298, acc: 0.8920454382896423)
[2025-02-13 20:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:42][root][INFO] - Training Epoch: 2/2, step 1224/7134 completed (loss: 0.14625637233257294, acc: 0.9622641801834106)
[2025-02-13 20:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43][root][INFO] - Training Epoch: 2/2, step 1225/7134 completed (loss: 0.2938470244407654, acc: 0.9452054500579834)
[2025-02-13 20:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43][root][INFO] - Training Epoch: 2/2, step 1226/7134 completed (loss: 0.3613623380661011, acc: 0.89682537317276)
[2025-02-13 20:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43][root][INFO] - Training Epoch: 2/2, step 1227/7134 completed (loss: 0.3766740560531616, acc: 0.9175257682800293)
[2025-02-13 20:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44][root][INFO] - Training Epoch: 2/2, step 1228/7134 completed (loss: 0.23403695225715637, acc: 0.9593023061752319)
[2025-02-13 20:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44][root][INFO] - Training Epoch: 2/2, step 1229/7134 completed (loss: 0.10392776131629944, acc: 0.9851484894752502)
[2025-02-13 20:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44][root][INFO] - Training Epoch: 2/2, step 1230/7134 completed (loss: 0.23136192560195923, acc: 0.9433962106704712)
[2025-02-13 20:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45][root][INFO] - Training Epoch: 2/2, step 1231/7134 completed (loss: 0.20706689357757568, acc: 0.9482758641242981)
[2025-02-13 20:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45][root][INFO] - Training Epoch: 2/2, step 1232/7134 completed (loss: 0.20968802273273468, acc: 0.9417040348052979)
[2025-02-13 20:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46][root][INFO] - Training Epoch: 2/2, step 1233/7134 completed (loss: 0.2908422350883484, acc: 0.9140271544456482)
[2025-02-13 20:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46][root][INFO] - Training Epoch: 2/2, step 1234/7134 completed (loss: 0.10489076375961304, acc: 0.9743589758872986)
[2025-02-13 20:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46][root][INFO] - Training Epoch: 2/2, step 1235/7134 completed (loss: 0.16524410247802734, acc: 0.9744898080825806)
[2025-02-13 20:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47][root][INFO] - Training Epoch: 2/2, step 1236/7134 completed (loss: 0.2346438467502594, acc: 0.9352940917015076)
[2025-02-13 20:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47][root][INFO] - Training Epoch: 2/2, step 1237/7134 completed (loss: 0.17468580603599548, acc: 0.9735099077224731)
[2025-02-13 20:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47][root][INFO] - Training Epoch: 2/2, step 1238/7134 completed (loss: 0.6008497476577759, acc: 0.8651162981987)
[2025-02-13 20:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48][root][INFO] - Training Epoch: 2/2, step 1239/7134 completed (loss: 0.2374669313430786, acc: 0.9512194991111755)
[2025-02-13 20:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48][root][INFO] - Training Epoch: 2/2, step 1240/7134 completed (loss: 0.20226219296455383, acc: 0.9476743936538696)
[2025-02-13 20:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49][root][INFO] - Training Epoch: 2/2, step 1241/7134 completed (loss: 0.06636994332075119, acc: 0.9836065769195557)
[2025-02-13 20:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49][root][INFO] - Training Epoch: 2/2, step 1242/7134 completed (loss: 0.2788337767124176, acc: 0.9162303805351257)
[2025-02-13 20:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49][root][INFO] - Training Epoch: 2/2, step 1243/7134 completed (loss: 0.226931631565094, acc: 0.9429824352264404)
[2025-02-13 20:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:50][root][INFO] - Training Epoch: 2/2, step 1244/7134 completed (loss: 0.18848562240600586, acc: 0.9647887349128723)
[2025-02-13 20:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:50][root][INFO] - Training Epoch: 2/2, step 1245/7134 completed (loss: 0.16587600111961365, acc: 0.9494949579238892)
[2025-02-13 20:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51][root][INFO] - Training Epoch: 2/2, step 1246/7134 completed (loss: 0.32052716612815857, acc: 0.9105263352394104)
[2025-02-13 20:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51][root][INFO] - Training Epoch: 2/2, step 1247/7134 completed (loss: 0.12646472454071045, acc: 0.9638554453849792)
[2025-02-13 20:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51][root][INFO] - Training Epoch: 2/2, step 1248/7134 completed (loss: 0.10717736929655075, acc: 0.9800000190734863)
[2025-02-13 20:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52][root][INFO] - Training Epoch: 2/2, step 1249/7134 completed (loss: 0.07454658299684525, acc: 0.9913793206214905)
[2025-02-13 20:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52][root][INFO] - Training Epoch: 2/2, step 1250/7134 completed (loss: 0.04600705951452255, acc: 0.9933333396911621)
[2025-02-13 20:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52][root][INFO] - Training Epoch: 2/2, step 1251/7134 completed (loss: 0.12121739238500595, acc: 0.9757575988769531)
[2025-02-13 20:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53][root][INFO] - Training Epoch: 2/2, step 1252/7134 completed (loss: 0.08820764720439911, acc: 0.9842519760131836)
[2025-02-13 20:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53][root][INFO] - Training Epoch: 2/2, step 1253/7134 completed (loss: 0.17455950379371643, acc: 0.9572649598121643)
[2025-02-13 20:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54][root][INFO] - Training Epoch: 2/2, step 1254/7134 completed (loss: 0.14556190371513367, acc: 0.9599999785423279)
[2025-02-13 20:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54][root][INFO] - Training Epoch: 2/2, step 1255/7134 completed (loss: 0.08132865279912949, acc: 0.9793103337287903)
[2025-02-13 20:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54][root][INFO] - Training Epoch: 2/2, step 1256/7134 completed (loss: 0.09807956218719482, acc: 0.9770992398262024)
[2025-02-13 20:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55][root][INFO] - Training Epoch: 2/2, step 1257/7134 completed (loss: 0.0829630196094513, acc: 0.9770992398262024)
[2025-02-13 20:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55][root][INFO] - Training Epoch: 2/2, step 1258/7134 completed (loss: 0.15161113440990448, acc: 0.9583333134651184)
[2025-02-13 20:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56][root][INFO] - Training Epoch: 2/2, step 1259/7134 completed (loss: 0.21181713044643402, acc: 0.9615384340286255)
[2025-02-13 20:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56][root][INFO] - Training Epoch: 2/2, step 1260/7134 completed (loss: 0.09817666560411453, acc: 0.9798657894134521)
[2025-02-13 20:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56][root][INFO] - Training Epoch: 2/2, step 1261/7134 completed (loss: 0.1004989966750145, acc: 0.9729729890823364)
[2025-02-13 20:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57][root][INFO] - Training Epoch: 2/2, step 1262/7134 completed (loss: 0.020587865263223648, acc: 0.9922480583190918)
[2025-02-13 20:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57][root][INFO] - Training Epoch: 2/2, step 1263/7134 completed (loss: 0.04828009009361267, acc: 0.9870967864990234)
[2025-02-13 20:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57][root][INFO] - Training Epoch: 2/2, step 1264/7134 completed (loss: 0.054848987609148026, acc: 0.9851852059364319)
[2025-02-13 20:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58][root][INFO] - Training Epoch: 2/2, step 1265/7134 completed (loss: 0.051037225872278214, acc: 0.9942857027053833)
[2025-02-13 20:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58][root][INFO] - Training Epoch: 2/2, step 1266/7134 completed (loss: 0.09324329346418381, acc: 0.9770992398262024)
[2025-02-13 20:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59][root][INFO] - Training Epoch: 2/2, step 1267/7134 completed (loss: 0.04670456424355507, acc: 1.0)
[2025-02-13 20:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59][root][INFO] - Training Epoch: 2/2, step 1268/7134 completed (loss: 0.08187773078680038, acc: 0.970370352268219)
[2025-02-13 20:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59][root][INFO] - Training Epoch: 2/2, step 1269/7134 completed (loss: 0.22411957383155823, acc: 0.939393937587738)
[2025-02-13 20:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00][root][INFO] - Training Epoch: 2/2, step 1270/7134 completed (loss: 0.11022954434156418, acc: 0.965753436088562)
[2025-02-13 20:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00][root][INFO] - Training Epoch: 2/2, step 1271/7134 completed (loss: 0.12085293978452682, acc: 0.9916666746139526)
[2025-02-13 20:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00][root][INFO] - Training Epoch: 2/2, step 1272/7134 completed (loss: 0.0330059789121151, acc: 1.0)
[2025-02-13 20:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:01][root][INFO] - Training Epoch: 2/2, step 1273/7134 completed (loss: 0.11592166870832443, acc: 0.9777777791023254)
[2025-02-13 20:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:01][root][INFO] - Training Epoch: 2/2, step 1274/7134 completed (loss: 0.028959738090634346, acc: 0.9917355179786682)
[2025-02-13 20:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02][root][INFO] - Training Epoch: 2/2, step 1275/7134 completed (loss: 0.0719725638628006, acc: 0.9826086759567261)
[2025-02-13 20:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02][root][INFO] - Training Epoch: 2/2, step 1276/7134 completed (loss: 0.08798719197511673, acc: 0.9696969985961914)
[2025-02-13 20:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02][root][INFO] - Training Epoch: 2/2, step 1277/7134 completed (loss: 0.029906587675213814, acc: 1.0)
[2025-02-13 20:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03][root][INFO] - Training Epoch: 2/2, step 1278/7134 completed (loss: 0.15835878252983093, acc: 0.96875)
[2025-02-13 20:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03][root][INFO] - Training Epoch: 2/2, step 1279/7134 completed (loss: 0.12365195900201797, acc: 0.9619565010070801)
[2025-02-13 20:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03][root][INFO] - Training Epoch: 2/2, step 1280/7134 completed (loss: 0.07357899844646454, acc: 0.9786096215248108)
[2025-02-13 20:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:04][root][INFO] - Training Epoch: 2/2, step 1281/7134 completed (loss: 0.08717814087867737, acc: 0.9772727489471436)
[2025-02-13 20:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:04][root][INFO] - Training Epoch: 2/2, step 1282/7134 completed (loss: 0.3293967843055725, acc: 0.9289617538452148)
[2025-02-13 20:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05][root][INFO] - Training Epoch: 2/2, step 1283/7134 completed (loss: 0.0591326579451561, acc: 0.9922480583190918)
[2025-02-13 20:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05][root][INFO] - Training Epoch: 2/2, step 1284/7134 completed (loss: 0.14921975135803223, acc: 0.9580838084220886)
[2025-02-13 20:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05][root][INFO] - Training Epoch: 2/2, step 1285/7134 completed (loss: 0.172406405210495, acc: 0.9750000238418579)
[2025-02-13 20:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06][root][INFO] - Training Epoch: 2/2, step 1286/7134 completed (loss: 0.087883859872818, acc: 0.9756097793579102)
[2025-02-13 20:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06][root][INFO] - Training Epoch: 2/2, step 1287/7134 completed (loss: 0.06742307543754578, acc: 0.9777777791023254)
[2025-02-13 20:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06][root][INFO] - Training Epoch: 2/2, step 1288/7134 completed (loss: 0.05901763215661049, acc: 0.984455943107605)
[2025-02-13 20:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07][root][INFO] - Training Epoch: 2/2, step 1289/7134 completed (loss: 0.19655005633831024, acc: 0.9448275566101074)
[2025-02-13 20:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07][root][INFO] - Training Epoch: 2/2, step 1290/7134 completed (loss: 0.05375940352678299, acc: 0.9880239367485046)
[2025-02-13 20:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08][root][INFO] - Training Epoch: 2/2, step 1291/7134 completed (loss: 0.15037879347801208, acc: 0.9647058844566345)
[2025-02-13 20:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08][root][INFO] - Training Epoch: 2/2, step 1292/7134 completed (loss: 0.09145943075418472, acc: 0.9670329689979553)
[2025-02-13 20:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08][root][INFO] - Training Epoch: 2/2, step 1293/7134 completed (loss: 0.20477426052093506, acc: 0.964102566242218)
[2025-02-13 20:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:09][root][INFO] - Training Epoch: 2/2, step 1294/7134 completed (loss: 0.1283247470855713, acc: 0.9562841653823853)
[2025-02-13 20:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:09][root][INFO] - Training Epoch: 2/2, step 1295/7134 completed (loss: 0.0744854211807251, acc: 0.9701492786407471)
[2025-02-13 20:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10][root][INFO] - Training Epoch: 2/2, step 1296/7134 completed (loss: 0.03167757764458656, acc: 1.0)
[2025-02-13 20:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10][root][INFO] - Training Epoch: 2/2, step 1297/7134 completed (loss: 0.13182322680950165, acc: 0.9732142686843872)
[2025-02-13 20:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10][root][INFO] - Training Epoch: 2/2, step 1298/7134 completed (loss: 0.060393597930669785, acc: 0.9917355179786682)
[2025-02-13 20:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11][root][INFO] - Training Epoch: 2/2, step 1299/7134 completed (loss: 0.038000404834747314, acc: 0.9879518151283264)
[2025-02-13 20:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11][root][INFO] - Training Epoch: 2/2, step 1300/7134 completed (loss: 0.04861210659146309, acc: 0.9950494766235352)
[2025-02-13 20:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11][root][INFO] - Training Epoch: 2/2, step 1301/7134 completed (loss: 0.0927330031991005, acc: 0.9736841917037964)
[2025-02-13 20:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12][root][INFO] - Training Epoch: 2/2, step 1302/7134 completed (loss: 0.12739257514476776, acc: 0.9603960514068604)
[2025-02-13 20:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12][root][INFO] - Training Epoch: 2/2, step 1303/7134 completed (loss: 0.06174503639340401, acc: 0.9900000095367432)
[2025-02-13 20:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13][root][INFO] - Training Epoch: 2/2, step 1304/7134 completed (loss: 0.13125145435333252, acc: 0.9830508232116699)
[2025-02-13 20:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13][root][INFO] - Training Epoch: 2/2, step 1305/7134 completed (loss: 0.15637701749801636, acc: 0.9536082744598389)
[2025-02-13 20:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13][root][INFO] - Training Epoch: 2/2, step 1306/7134 completed (loss: 0.14381437003612518, acc: 0.9605262875556946)
[2025-02-13 20:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:14][root][INFO] - Training Epoch: 2/2, step 1307/7134 completed (loss: 0.03592894598841667, acc: 0.9928057789802551)
[2025-02-13 20:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:14][root][INFO] - Training Epoch: 2/2, step 1308/7134 completed (loss: 0.061502888798713684, acc: 0.9935483932495117)
[2025-02-13 20:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15][root][INFO] - Training Epoch: 2/2, step 1309/7134 completed (loss: 0.1176246702671051, acc: 0.9710144996643066)
[2025-02-13 20:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15][root][INFO] - Training Epoch: 2/2, step 1310/7134 completed (loss: 0.14301243424415588, acc: 0.9723756909370422)
[2025-02-13 20:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15][root][INFO] - Training Epoch: 2/2, step 1311/7134 completed (loss: 0.07459883391857147, acc: 0.9807692170143127)
[2025-02-13 20:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16][root][INFO] - Training Epoch: 2/2, step 1312/7134 completed (loss: 0.255074679851532, acc: 0.9503105878829956)
[2025-02-13 20:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16][root][INFO] - Training Epoch: 2/2, step 1313/7134 completed (loss: 0.18834476172924042, acc: 0.9567901492118835)
[2025-02-13 20:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16][root][INFO] - Training Epoch: 2/2, step 1314/7134 completed (loss: 0.08693987131118774, acc: 0.9710144996643066)
[2025-02-13 20:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17][root][INFO] - Training Epoch: 2/2, step 1315/7134 completed (loss: 0.26276895403862, acc: 0.9243243336677551)
[2025-02-13 20:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17][root][INFO] - Training Epoch: 2/2, step 1316/7134 completed (loss: 0.17680728435516357, acc: 0.9512194991111755)
[2025-02-13 20:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18][root][INFO] - Training Epoch: 2/2, step 1317/7134 completed (loss: 0.14277003705501556, acc: 0.9552238583564758)
[2025-02-13 20:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18][root][INFO] - Training Epoch: 2/2, step 1318/7134 completed (loss: 0.0718713328242302, acc: 0.9780219793319702)
[2025-02-13 20:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18][root][INFO] - Training Epoch: 2/2, step 1319/7134 completed (loss: 0.13513386249542236, acc: 0.9818181991577148)
[2025-02-13 20:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:19][root][INFO] - Training Epoch: 2/2, step 1320/7134 completed (loss: 0.2289707362651825, acc: 0.940397322177887)
[2025-02-13 20:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:19][root][INFO] - Training Epoch: 2/2, step 1321/7134 completed (loss: 0.10920830070972443, acc: 0.9585492014884949)
[2025-02-13 20:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20][root][INFO] - Training Epoch: 2/2, step 1322/7134 completed (loss: 0.13246959447860718, acc: 0.9591836929321289)
[2025-02-13 20:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20][root][INFO] - Training Epoch: 2/2, step 1323/7134 completed (loss: 0.2210996448993683, acc: 0.9507042169570923)
[2025-02-13 20:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20][root][INFO] - Training Epoch: 2/2, step 1324/7134 completed (loss: 0.06931041926145554, acc: 0.9869281053543091)
[2025-02-13 20:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21][root][INFO] - Training Epoch: 2/2, step 1325/7134 completed (loss: 0.1006205677986145, acc: 0.9805825352668762)
[2025-02-13 20:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21][root][INFO] - Training Epoch: 2/2, step 1326/7134 completed (loss: 0.25593966245651245, acc: 0.9452054500579834)
[2025-02-13 20:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21][root][INFO] - Training Epoch: 2/2, step 1327/7134 completed (loss: 0.08753282576799393, acc: 0.9822485446929932)
[2025-02-13 20:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22][root][INFO] - Training Epoch: 2/2, step 1328/7134 completed (loss: 0.09775067120790482, acc: 0.9603960514068604)
[2025-02-13 20:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22][root][INFO] - Training Epoch: 2/2, step 1329/7134 completed (loss: 0.15130051970481873, acc: 0.9599999785423279)
[2025-02-13 20:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23][root][INFO] - Training Epoch: 2/2, step 1330/7134 completed (loss: 0.07457740604877472, acc: 0.9784172773361206)
[2025-02-13 20:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23][root][INFO] - Training Epoch: 2/2, step 1331/7134 completed (loss: 0.04937959834933281, acc: 0.98591548204422)
[2025-02-13 20:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23][root][INFO] - Training Epoch: 2/2, step 1332/7134 completed (loss: 0.06852135807275772, acc: 0.9736841917037964)
[2025-02-13 20:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24][root][INFO] - Training Epoch: 2/2, step 1333/7134 completed (loss: 0.12646915018558502, acc: 0.9736841917037964)
[2025-02-13 20:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24][root][INFO] - Training Epoch: 2/2, step 1334/7134 completed (loss: 0.27477505803108215, acc: 0.9382715821266174)
[2025-02-13 20:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24][root][INFO] - Training Epoch: 2/2, step 1335/7134 completed (loss: 0.23892457783222198, acc: 0.9528796076774597)
[2025-02-13 20:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25][root][INFO] - Training Epoch: 2/2, step 1336/7134 completed (loss: 0.14090470969676971, acc: 0.9518716335296631)
[2025-02-13 20:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25][root][INFO] - Training Epoch: 2/2, step 1337/7134 completed (loss: 0.11657131463289261, acc: 0.9612902998924255)
[2025-02-13 20:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25][root][INFO] - Training Epoch: 2/2, step 1338/7134 completed (loss: 0.290290892124176, acc: 0.953125)
[2025-02-13 20:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26][root][INFO] - Training Epoch: 2/2, step 1339/7134 completed (loss: 0.24686264991760254, acc: 0.9387755393981934)
[2025-02-13 20:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26][root][INFO] - Training Epoch: 2/2, step 1340/7134 completed (loss: 0.16591013967990875, acc: 0.9805194735527039)
[2025-02-13 20:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27][root][INFO] - Training Epoch: 2/2, step 1341/7134 completed (loss: 0.04850006103515625, acc: 0.9890109896659851)
[2025-02-13 20:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27][root][INFO] - Training Epoch: 2/2, step 1342/7134 completed (loss: 0.05960587039589882, acc: 0.9801324605941772)
[2025-02-13 20:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27][root][INFO] - Training Epoch: 2/2, step 1343/7134 completed (loss: 0.16873715817928314, acc: 0.9411764740943909)
[2025-02-13 20:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:28][root][INFO] - Training Epoch: 2/2, step 1344/7134 completed (loss: 0.0514950156211853, acc: 0.9908257126808167)
[2025-02-13 20:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:28][root][INFO] - Training Epoch: 2/2, step 1345/7134 completed (loss: 0.056085024029016495, acc: 1.0)
[2025-02-13 20:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29][root][INFO] - Training Epoch: 2/2, step 1346/7134 completed (loss: 0.10021361708641052, acc: 0.9836065769195557)
[2025-02-13 20:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29][root][INFO] - Training Epoch: 2/2, step 1347/7134 completed (loss: 0.10301279276609421, acc: 0.9722222089767456)
[2025-02-13 20:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29][root][INFO] - Training Epoch: 2/2, step 1348/7134 completed (loss: 0.05394122004508972, acc: 0.988950252532959)
[2025-02-13 20:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30][root][INFO] - Training Epoch: 2/2, step 1349/7134 completed (loss: 0.02496408298611641, acc: 1.0)
[2025-02-13 20:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30][root][INFO] - Training Epoch: 2/2, step 1350/7134 completed (loss: 0.05707661807537079, acc: 0.9820359349250793)
[2025-02-13 20:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30][root][INFO] - Training Epoch: 2/2, step 1351/7134 completed (loss: 0.021669037640094757, acc: 1.0)
[2025-02-13 20:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:31][root][INFO] - Training Epoch: 2/2, step 1352/7134 completed (loss: 0.08108599483966827, acc: 0.9817073345184326)
[2025-02-13 20:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:31][root][INFO] - Training Epoch: 2/2, step 1353/7134 completed (loss: 0.06028358265757561, acc: 0.9916666746139526)
[2025-02-13 20:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32][root][INFO] - Training Epoch: 2/2, step 1354/7134 completed (loss: 0.012620688416063786, acc: 1.0)
[2025-02-13 20:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32][root][INFO] - Training Epoch: 2/2, step 1355/7134 completed (loss: 0.02112802304327488, acc: 0.9870967864990234)
[2025-02-13 20:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32][root][INFO] - Training Epoch: 2/2, step 1356/7134 completed (loss: 0.054474566131830215, acc: 0.9883720874786377)
[2025-02-13 20:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33][root][INFO] - Training Epoch: 2/2, step 1357/7134 completed (loss: 0.021472960710525513, acc: 1.0)
[2025-02-13 20:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33][root][INFO] - Training Epoch: 2/2, step 1358/7134 completed (loss: 0.051200103014707565, acc: 0.9886363744735718)
[2025-02-13 20:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33][root][INFO] - Training Epoch: 2/2, step 1359/7134 completed (loss: 0.015251608565449715, acc: 1.0)
[2025-02-13 20:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34][root][INFO] - Training Epoch: 2/2, step 1360/7134 completed (loss: 0.014252032153308392, acc: 1.0)
[2025-02-13 20:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34][root][INFO] - Training Epoch: 2/2, step 1361/7134 completed (loss: 0.03986736014485359, acc: 0.9940828680992126)
[2025-02-13 20:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34][root][INFO] - Training Epoch: 2/2, step 1362/7134 completed (loss: 0.0310048945248127, acc: 0.994535505771637)
[2025-02-13 20:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35][root][INFO] - Training Epoch: 2/2, step 1363/7134 completed (loss: 0.03392241522669792, acc: 0.982758641242981)
[2025-02-13 20:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35][root][INFO] - Training Epoch: 2/2, step 1364/7134 completed (loss: 0.032799892127513885, acc: 0.9882352948188782)
[2025-02-13 20:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36][root][INFO] - Training Epoch: 2/2, step 1365/7134 completed (loss: 0.012400038540363312, acc: 1.0)
[2025-02-13 20:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36][root][INFO] - Training Epoch: 2/2, step 1366/7134 completed (loss: 0.009223476983606815, acc: 1.0)
[2025-02-13 20:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36][root][INFO] - Training Epoch: 2/2, step 1367/7134 completed (loss: 0.021845225244760513, acc: 0.9926470518112183)
[2025-02-13 20:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37][root][INFO] - Training Epoch: 2/2, step 1368/7134 completed (loss: 0.037643224000930786, acc: 0.9942528605461121)
[2025-02-13 20:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37][root][INFO] - Training Epoch: 2/2, step 1369/7134 completed (loss: 0.050407953560352325, acc: 0.9927007555961609)
[2025-02-13 20:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38][root][INFO] - Training Epoch: 2/2, step 1370/7134 completed (loss: 0.0886976346373558, acc: 0.9897959232330322)
[2025-02-13 20:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38][root][INFO] - Training Epoch: 2/2, step 1371/7134 completed (loss: 0.06231861189007759, acc: 0.9814814925193787)
[2025-02-13 20:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38][root][INFO] - Training Epoch: 2/2, step 1372/7134 completed (loss: 0.08001788705587387, acc: 0.9696969985961914)
[2025-02-13 20:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:39][root][INFO] - Training Epoch: 2/2, step 1373/7134 completed (loss: 0.06487379968166351, acc: 0.9844961166381836)
[2025-02-13 20:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:39][root][INFO] - Training Epoch: 2/2, step 1374/7134 completed (loss: 0.11373031139373779, acc: 0.9666666388511658)
[2025-02-13 20:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40][root][INFO] - Training Epoch: 2/2, step 1375/7134 completed (loss: 0.10706739127635956, acc: 0.970588207244873)
[2025-02-13 20:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40][root][INFO] - Training Epoch: 2/2, step 1376/7134 completed (loss: 0.17066612839698792, acc: 0.9642857313156128)
[2025-02-13 20:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40][root][INFO] - Training Epoch: 2/2, step 1377/7134 completed (loss: 0.1490670144557953, acc: 0.9411764740943909)
[2025-02-13 20:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41][root][INFO] - Training Epoch: 2/2, step 1378/7134 completed (loss: 0.12598411738872528, acc: 0.9903846383094788)
[2025-02-13 20:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41][root][INFO] - Training Epoch: 2/2, step 1379/7134 completed (loss: 0.14516682922840118, acc: 0.9504950642585754)
[2025-02-13 20:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41][root][INFO] - Training Epoch: 2/2, step 1380/7134 completed (loss: 0.07884277403354645, acc: 0.9629629850387573)
[2025-02-13 20:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42][root][INFO] - Training Epoch: 2/2, step 1381/7134 completed (loss: 0.07065735757350922, acc: 0.9855072498321533)
[2025-02-13 20:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42][root][INFO] - Training Epoch: 2/2, step 1382/7134 completed (loss: 0.10984120517969131, acc: 0.9769230484962463)
[2025-02-13 20:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43][root][INFO] - Training Epoch: 2/2, step 1383/7134 completed (loss: 0.2257215529680252, acc: 0.9558823704719543)
[2025-02-13 20:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43][root][INFO] - Training Epoch: 2/2, step 1384/7134 completed (loss: 0.1766931265592575, acc: 0.9645389914512634)
[2025-02-13 20:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43][root][INFO] - Training Epoch: 2/2, step 1385/7134 completed (loss: 0.25652143359184265, acc: 0.9440000057220459)
[2025-02-13 20:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44][root][INFO] - Training Epoch: 2/2, step 1386/7134 completed (loss: 0.07143310457468033, acc: 0.98591548204422)
[2025-02-13 20:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44][root][INFO] - Training Epoch: 2/2, step 1387/7134 completed (loss: 0.065921351313591, acc: 0.9930070042610168)
[2025-02-13 20:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45][root][INFO] - Training Epoch: 2/2, step 1388/7134 completed (loss: 0.14140678942203522, acc: 0.9599999785423279)
[2025-02-13 20:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45][root][INFO] - Training Epoch: 2/2, step 1389/7134 completed (loss: 0.10528314858675003, acc: 0.9850746393203735)
[2025-02-13 20:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45][root][INFO] - Training Epoch: 2/2, step 1390/7134 completed (loss: 0.11644694954156876, acc: 0.970802903175354)
[2025-02-13 20:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46][root][INFO] - Training Epoch: 2/2, step 1391/7134 completed (loss: 0.2342909276485443, acc: 0.9264705777168274)
[2025-02-13 20:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46][root][INFO] - Training Epoch: 2/2, step 1392/7134 completed (loss: 0.11581696569919586, acc: 0.9756097793579102)
[2025-02-13 20:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46][root][INFO] - Training Epoch: 2/2, step 1393/7134 completed (loss: 0.10303095728158951, acc: 0.96875)
[2025-02-13 20:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:47][root][INFO] - Training Epoch: 2/2, step 1394/7134 completed (loss: 0.08674443513154984, acc: 0.9905660152435303)
[2025-02-13 20:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:47][root][INFO] - Training Epoch: 2/2, step 1395/7134 completed (loss: 0.1735733300447464, acc: 0.9765625)
[2025-02-13 20:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48][root][INFO] - Training Epoch: 2/2, step 1396/7134 completed (loss: 0.062156595289707184, acc: 0.9760000109672546)
[2025-02-13 20:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48][root][INFO] - Training Epoch: 2/2, step 1397/7134 completed (loss: 0.38179656863212585, acc: 0.912162184715271)
[2025-02-13 20:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48][root][INFO] - Training Epoch: 2/2, step 1398/7134 completed (loss: 0.059159837663173676, acc: 0.9801980257034302)
[2025-02-13 20:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49][root][INFO] - Training Epoch: 2/2, step 1399/7134 completed (loss: 0.1774023473262787, acc: 0.9424460530281067)
[2025-02-13 20:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49][root][INFO] - Training Epoch: 2/2, step 1400/7134 completed (loss: 0.04831541329622269, acc: 0.9927536249160767)
[2025-02-13 20:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50][root][INFO] - Training Epoch: 2/2, step 1401/7134 completed (loss: 0.06874649226665497, acc: 0.9851852059364319)
[2025-02-13 20:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50][root][INFO] - Training Epoch: 2/2, step 1402/7134 completed (loss: 0.05609792843461037, acc: 0.9776119589805603)
[2025-02-13 20:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50][root][INFO] - Training Epoch: 2/2, step 1403/7134 completed (loss: 0.10079829394817352, acc: 0.9704142212867737)
[2025-02-13 20:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51][root][INFO] - Training Epoch: 2/2, step 1404/7134 completed (loss: 0.059581268578767776, acc: 0.988095223903656)
[2025-02-13 20:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51][root][INFO] - Training Epoch: 2/2, step 1405/7134 completed (loss: 0.01855507120490074, acc: 1.0)
[2025-02-13 20:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51][root][INFO] - Training Epoch: 2/2, step 1406/7134 completed (loss: 0.02654310129582882, acc: 0.9937888383865356)
[2025-02-13 20:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:52][root][INFO] - Training Epoch: 2/2, step 1407/7134 completed (loss: 0.013069704174995422, acc: 1.0)
[2025-02-13 20:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:52][root][INFO] - Training Epoch: 2/2, step 1408/7134 completed (loss: 0.020667361095547676, acc: 1.0)
[2025-02-13 20:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53][root][INFO] - Training Epoch: 2/2, step 1409/7134 completed (loss: 0.010761226527392864, acc: 1.0)
[2025-02-13 20:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53][root][INFO] - Training Epoch: 2/2, step 1410/7134 completed (loss: 0.13453812897205353, acc: 0.9834254384040833)
[2025-02-13 20:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53][root][INFO] - Training Epoch: 2/2, step 1411/7134 completed (loss: 0.042694613337516785, acc: 0.9885714054107666)
[2025-02-13 20:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54][root][INFO] - Training Epoch: 2/2, step 1412/7134 completed (loss: 0.08388730138540268, acc: 0.9736841917037964)
[2025-02-13 20:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54][root][INFO] - Training Epoch: 2/2, step 1413/7134 completed (loss: 0.033736780285835266, acc: 0.9940119981765747)
[2025-02-13 20:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54][root][INFO] - Training Epoch: 2/2, step 1414/7134 completed (loss: 0.009006530977785587, acc: 1.0)
[2025-02-13 20:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55][root][INFO] - Training Epoch: 2/2, step 1415/7134 completed (loss: 0.09305188059806824, acc: 0.970802903175354)
[2025-02-13 20:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55][root][INFO] - Training Epoch: 2/2, step 1416/7134 completed (loss: 0.009318333119153976, acc: 1.0)
[2025-02-13 20:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56][root][INFO] - Training Epoch: 2/2, step 1417/7134 completed (loss: 0.02336398884654045, acc: 0.9930555820465088)
[2025-02-13 20:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56][root][INFO] - Training Epoch: 2/2, step 1418/7134 completed (loss: 0.011571762152016163, acc: 1.0)
[2025-02-13 20:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56][root][INFO] - Training Epoch: 2/2, step 1419/7134 completed (loss: 0.05166909843683243, acc: 0.9886363744735718)
[2025-02-13 20:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:57][root][INFO] - Training Epoch: 2/2, step 1420/7134 completed (loss: 0.05480831488966942, acc: 0.9789473414421082)
[2025-02-13 20:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:57][root][INFO] - Training Epoch: 2/2, step 1421/7134 completed (loss: 0.014263044111430645, acc: 1.0)
[2025-02-13 20:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58][root][INFO] - Training Epoch: 2/2, step 1422/7134 completed (loss: 0.09567821025848389, acc: 0.9698795080184937)
[2025-02-13 20:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58][root][INFO] - Training Epoch: 2/2, step 1423/7134 completed (loss: 0.29109668731689453, acc: 0.9312977194786072)
[2025-02-13 20:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58][root][INFO] - Training Epoch: 2/2, step 1424/7134 completed (loss: 0.14597639441490173, acc: 0.9593495726585388)
[2025-02-13 20:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59][root][INFO] - Training Epoch: 2/2, step 1425/7134 completed (loss: 0.1459614336490631, acc: 0.9479768872261047)
[2025-02-13 20:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59][root][INFO] - Training Epoch: 2/2, step 1426/7134 completed (loss: 0.06302957981824875, acc: 0.9896907210350037)
[2025-02-13 20:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00][root][INFO] - Training Epoch: 2/2, step 1427/7134 completed (loss: 0.18330027163028717, acc: 0.9322034120559692)
[2025-02-13 20:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00][root][INFO] - Training Epoch: 2/2, step 1428/7134 completed (loss: 0.5151417255401611, acc: 0.8548387289047241)
[2025-02-13 20:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00][root][INFO] - Training Epoch: 2/2, step 1429/7134 completed (loss: 0.1187279149889946, acc: 0.9722222089767456)
[2025-02-13 20:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01][root][INFO] - Training Epoch: 2/2, step 1430/7134 completed (loss: 0.14778371155261993, acc: 0.9640287756919861)
[2025-02-13 20:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01][root][INFO] - Training Epoch: 2/2, step 1431/7134 completed (loss: 0.05713821202516556, acc: 0.9784946441650391)
[2025-02-13 20:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01][root][INFO] - Training Epoch: 2/2, step 1432/7134 completed (loss: 0.043786998838186264, acc: 0.9899497628211975)
[2025-02-13 20:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:02][root][INFO] - Training Epoch: 2/2, step 1433/7134 completed (loss: 0.059723541140556335, acc: 0.9852941036224365)
[2025-02-13 20:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:02][root][INFO] - Training Epoch: 2/2, step 1434/7134 completed (loss: 0.057309895753860474, acc: 0.9950248599052429)
[2025-02-13 20:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03][root][INFO] - Training Epoch: 2/2, step 1435/7134 completed (loss: 0.0728677436709404, acc: 0.9838709831237793)
[2025-02-13 20:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03][root][INFO] - Training Epoch: 2/2, step 1436/7134 completed (loss: 0.07669796794652939, acc: 0.9861111044883728)
[2025-02-13 20:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03][root][INFO] - Training Epoch: 2/2, step 1437/7134 completed (loss: 0.061160795390605927, acc: 0.9892473220825195)
[2025-02-13 20:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:04][root][INFO] - Training Epoch: 2/2, step 1438/7134 completed (loss: 0.07343599945306778, acc: 0.9878787994384766)
[2025-02-13 20:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:04][root][INFO] - Training Epoch: 2/2, step 1439/7134 completed (loss: 0.03923368453979492, acc: 0.9925925731658936)
[2025-02-13 20:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05][root][INFO] - Training Epoch: 2/2, step 1440/7134 completed (loss: 0.059839170426130295, acc: 0.9813664555549622)
[2025-02-13 20:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05][root][INFO] - Training Epoch: 2/2, step 1441/7134 completed (loss: 0.018048657104372978, acc: 0.9942528605461121)
[2025-02-13 20:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05][root][INFO] - Training Epoch: 2/2, step 1442/7134 completed (loss: 0.02976338379085064, acc: 0.9821428656578064)
[2025-02-13 20:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06][root][INFO] - Training Epoch: 2/2, step 1443/7134 completed (loss: 0.13825587928295135, acc: 0.9814814925193787)
[2025-02-13 20:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06][root][INFO] - Training Epoch: 2/2, step 1444/7134 completed (loss: 0.09462360292673111, acc: 0.9791666865348816)
[2025-02-13 20:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06][root][INFO] - Training Epoch: 2/2, step 1445/7134 completed (loss: 0.07561475038528442, acc: 0.987261176109314)
[2025-02-13 20:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:07][root][INFO] - Training Epoch: 2/2, step 1446/7134 completed (loss: 0.05515080317854881, acc: 0.9898989796638489)
[2025-02-13 20:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:07][root][INFO] - Training Epoch: 2/2, step 1447/7134 completed (loss: 0.045934487134218216, acc: 0.9892473220825195)
[2025-02-13 20:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08][root][INFO] - Training Epoch: 2/2, step 1448/7134 completed (loss: 0.15090049803256989, acc: 0.9402984976768494)
[2025-02-13 20:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08][root][INFO] - Training Epoch: 2/2, step 1449/7134 completed (loss: 0.16958646476268768, acc: 0.9562841653823853)
[2025-02-13 20:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08][root][INFO] - Training Epoch: 2/2, step 1450/7134 completed (loss: 0.08446977287530899, acc: 0.9839572310447693)
[2025-02-13 20:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:09][root][INFO] - Training Epoch: 2/2, step 1451/7134 completed (loss: 0.05610313639044762, acc: 0.9842932224273682)
[2025-02-13 20:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:09][root][INFO] - Training Epoch: 2/2, step 1452/7134 completed (loss: 0.11127502471208572, acc: 0.9791666865348816)
[2025-02-13 20:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10][root][INFO] - Training Epoch: 2/2, step 1453/7134 completed (loss: 0.31910789012908936, acc: 0.9011628031730652)
[2025-02-13 20:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10][root][INFO] - Training Epoch: 2/2, step 1454/7134 completed (loss: 0.10966943949460983, acc: 0.96875)
[2025-02-13 20:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10][root][INFO] - Training Epoch: 2/2, step 1455/7134 completed (loss: 0.23301878571510315, acc: 0.931034505367279)
[2025-02-13 20:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11][root][INFO] - Training Epoch: 2/2, step 1456/7134 completed (loss: 0.15339446067810059, acc: 0.9586777091026306)
[2025-02-13 20:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11][root][INFO] - Training Epoch: 2/2, step 1457/7134 completed (loss: 0.06113176792860031, acc: 0.9813664555549622)
[2025-02-13 20:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11][root][INFO] - Training Epoch: 2/2, step 1458/7134 completed (loss: 0.04334333911538124, acc: 0.9940828680992126)
[2025-02-13 20:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12][root][INFO] - Training Epoch: 2/2, step 1459/7134 completed (loss: 0.03629367798566818, acc: 0.9925925731658936)
[2025-02-13 20:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12][root][INFO] - Training Epoch: 2/2, step 1460/7134 completed (loss: 0.1300448328256607, acc: 0.9704142212867737)
[2025-02-13 20:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13][root][INFO] - Training Epoch: 2/2, step 1461/7134 completed (loss: 0.07878542691469193, acc: 0.9602649211883545)
[2025-02-13 20:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13][root][INFO] - Training Epoch: 2/2, step 1462/7134 completed (loss: 0.06901717185974121, acc: 0.9850746393203735)
[2025-02-13 20:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13][root][INFO] - Training Epoch: 2/2, step 1463/7134 completed (loss: 0.09130100160837173, acc: 0.9803921580314636)
[2025-02-13 20:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14][root][INFO] - Training Epoch: 2/2, step 1464/7134 completed (loss: 0.17943882942199707, acc: 0.9578313231468201)
[2025-02-13 20:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14][root][INFO] - Training Epoch: 2/2, step 1465/7134 completed (loss: 0.13130298256874084, acc: 0.9743589758872986)
[2025-02-13 20:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15][root][INFO] - Training Epoch: 2/2, step 1466/7134 completed (loss: 0.02708544209599495, acc: 0.994413435459137)
[2025-02-13 20:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15][root][INFO] - Training Epoch: 2/2, step 1467/7134 completed (loss: 0.03576516732573509, acc: 0.9878787994384766)
[2025-02-13 20:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15][root][INFO] - Training Epoch: 2/2, step 1468/7134 completed (loss: 0.04730316251516342, acc: 0.9887640476226807)
[2025-02-13 20:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16][root][INFO] - Training Epoch: 2/2, step 1469/7134 completed (loss: 0.021539000794291496, acc: 1.0)
[2025-02-13 20:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16][root][INFO] - Training Epoch: 2/2, step 1470/7134 completed (loss: 0.044674504548311234, acc: 0.987261176109314)
[2025-02-13 20:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16][root][INFO] - Training Epoch: 2/2, step 1471/7134 completed (loss: 0.1802314817905426, acc: 0.9461538195610046)
[2025-02-13 20:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:17][root][INFO] - Training Epoch: 2/2, step 1472/7134 completed (loss: 0.03342225030064583, acc: 0.9941520690917969)
[2025-02-13 20:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:17][root][INFO] - Training Epoch: 2/2, step 1473/7134 completed (loss: 0.06629602611064911, acc: 0.9818181991577148)
[2025-02-13 20:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18][root][INFO] - Training Epoch: 2/2, step 1474/7134 completed (loss: 0.0765254944562912, acc: 0.9655172228813171)
[2025-02-13 20:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18][root][INFO] - Training Epoch: 2/2, step 1475/7134 completed (loss: 0.18312616646289825, acc: 0.9757575988769531)
[2025-02-13 20:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18][root][INFO] - Training Epoch: 2/2, step 1476/7134 completed (loss: 0.08706555515527725, acc: 0.9866666793823242)
[2025-02-13 20:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19][root][INFO] - Training Epoch: 2/2, step 1477/7134 completed (loss: 0.10102470964193344, acc: 0.9655172228813171)
[2025-02-13 20:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19][root][INFO] - Training Epoch: 2/2, step 1478/7134 completed (loss: 0.11425045877695084, acc: 0.9802631735801697)
[2025-02-13 20:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19][root][INFO] - Training Epoch: 2/2, step 1479/7134 completed (loss: 0.03299294784665108, acc: 0.9942528605461121)
[2025-02-13 20:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20][root][INFO] - Training Epoch: 2/2, step 1480/7134 completed (loss: 0.023342909291386604, acc: 0.9944751262664795)
[2025-02-13 20:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20][root][INFO] - Training Epoch: 2/2, step 1481/7134 completed (loss: 0.09103990346193314, acc: 0.9833333492279053)
[2025-02-13 20:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21][root][INFO] - Training Epoch: 2/2, step 1482/7134 completed (loss: 0.08951220661401749, acc: 0.976331353187561)
[2025-02-13 20:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21][root][INFO] - Training Epoch: 2/2, step 1483/7134 completed (loss: 0.02754245698451996, acc: 0.9923664331436157)
[2025-02-13 20:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21][root][INFO] - Training Epoch: 2/2, step 1484/7134 completed (loss: 0.17271369695663452, acc: 0.9759036302566528)
[2025-02-13 20:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22][root][INFO] - Training Epoch: 2/2, step 1485/7134 completed (loss: 0.07390830665826797, acc: 0.9824561476707458)
[2025-02-13 20:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22][root][INFO] - Training Epoch: 2/2, step 1486/7134 completed (loss: 0.036125730723142624, acc: 0.9937106966972351)
[2025-02-13 20:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22][root][INFO] - Training Epoch: 2/2, step 1487/7134 completed (loss: 0.03978005051612854, acc: 0.9783783555030823)
[2025-02-13 20:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:23][root][INFO] - Training Epoch: 2/2, step 1488/7134 completed (loss: 0.11127964407205582, acc: 0.9767441749572754)
[2025-02-13 20:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:23][root][INFO] - Training Epoch: 2/2, step 1489/7134 completed (loss: 0.09547750651836395, acc: 0.9795918464660645)
[2025-02-13 20:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24][root][INFO] - Training Epoch: 2/2, step 1490/7134 completed (loss: 0.07401800900697708, acc: 0.9646017551422119)
[2025-02-13 20:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24][root][INFO] - Training Epoch: 2/2, step 1491/7134 completed (loss: 0.031915463507175446, acc: 0.9952380657196045)
[2025-02-13 20:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24][root][INFO] - Training Epoch: 2/2, step 1492/7134 completed (loss: 0.09495415538549423, acc: 0.971563994884491)
[2025-02-13 20:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:25][root][INFO] - Training Epoch: 2/2, step 1493/7134 completed (loss: 0.06327608227729797, acc: 0.9906542301177979)
[2025-02-13 20:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:25][root][INFO] - Training Epoch: 2/2, step 1494/7134 completed (loss: 0.03594985604286194, acc: 0.9896373152732849)
[2025-02-13 20:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26][root][INFO] - Training Epoch: 2/2, step 1495/7134 completed (loss: 0.025576459243893623, acc: 1.0)
[2025-02-13 20:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26][root][INFO] - Training Epoch: 2/2, step 1496/7134 completed (loss: 0.07359844446182251, acc: 0.987500011920929)
[2025-02-13 20:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26][root][INFO] - Training Epoch: 2/2, step 1497/7134 completed (loss: 0.03045094758272171, acc: 1.0)
[2025-02-13 20:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27][root][INFO] - Training Epoch: 2/2, step 1498/7134 completed (loss: 0.0367966964840889, acc: 0.9891892075538635)
[2025-02-13 20:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27][root][INFO] - Training Epoch: 2/2, step 1499/7134 completed (loss: 0.06665224581956863, acc: 0.9720930457115173)
[2025-02-13 20:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27][root][INFO] - Training Epoch: 2/2, step 1500/7134 completed (loss: 0.12758015096187592, acc: 0.9520547986030579)
[2025-02-13 20:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:28][root][INFO] - Training Epoch: 2/2, step 1501/7134 completed (loss: 0.14385086297988892, acc: 0.9644970297813416)
[2025-02-13 20:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:28][root][INFO] - Training Epoch: 2/2, step 1502/7134 completed (loss: 0.12306376546621323, acc: 0.9727272987365723)
[2025-02-13 20:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29][root][INFO] - Training Epoch: 2/2, step 1503/7134 completed (loss: 0.10493694990873337, acc: 0.9828571677207947)
[2025-02-13 20:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29][root][INFO] - Training Epoch: 2/2, step 1504/7134 completed (loss: 0.048403266817331314, acc: 0.9894179701805115)
[2025-02-13 20:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29][root][INFO] - Training Epoch: 2/2, step 1505/7134 completed (loss: 0.07155560702085495, acc: 0.9746835231781006)
[2025-02-13 20:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30][root][INFO] - Training Epoch: 2/2, step 1506/7134 completed (loss: 0.09218529611825943, acc: 0.9685534834861755)
[2025-02-13 20:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30][root][INFO] - Training Epoch: 2/2, step 1507/7134 completed (loss: 0.10865798592567444, acc: 0.96875)
[2025-02-13 20:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30][root][INFO] - Training Epoch: 2/2, step 1508/7134 completed (loss: 0.05183256417512894, acc: 0.985981285572052)
[2025-02-13 20:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:31][root][INFO] - Training Epoch: 2/2, step 1509/7134 completed (loss: 0.057344477623701096, acc: 0.9833333492279053)
[2025-02-13 20:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:31][root][INFO] - Training Epoch: 2/2, step 1510/7134 completed (loss: 0.10000666230916977, acc: 0.9548386931419373)
[2025-02-13 20:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32][root][INFO] - Training Epoch: 2/2, step 1511/7134 completed (loss: 0.09121681749820709, acc: 0.9664804339408875)
[2025-02-13 20:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32][root][INFO] - Training Epoch: 2/2, step 1512/7134 completed (loss: 0.02313700132071972, acc: 1.0)
[2025-02-13 20:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32][root][INFO] - Training Epoch: 2/2, step 1513/7134 completed (loss: 0.03529628738760948, acc: 0.9944751262664795)
[2025-02-13 20:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33][root][INFO] - Training Epoch: 2/2, step 1514/7134 completed (loss: 0.08940227329730988, acc: 0.9637305736541748)
[2025-02-13 20:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33][root][INFO] - Training Epoch: 2/2, step 1515/7134 completed (loss: 0.10323118418455124, acc: 0.9819819927215576)
[2025-02-13 20:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34][root][INFO] - Training Epoch: 2/2, step 1516/7134 completed (loss: 0.1471766084432602, acc: 0.949999988079071)
[2025-02-13 20:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34][root][INFO] - Training Epoch: 2/2, step 1517/7134 completed (loss: 0.19633318483829498, acc: 0.9464285969734192)
[2025-02-13 20:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34][root][INFO] - Training Epoch: 2/2, step 1518/7134 completed (loss: 0.04103144630789757, acc: 0.9896907210350037)
[2025-02-13 20:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35][root][INFO] - Training Epoch: 2/2, step 1519/7134 completed (loss: 0.06820200383663177, acc: 0.9805825352668762)
[2025-02-13 20:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35][root][INFO] - Training Epoch: 2/2, step 1520/7134 completed (loss: 0.04817989096045494, acc: 0.985401451587677)
[2025-02-13 20:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35][root][INFO] - Training Epoch: 2/2, step 1521/7134 completed (loss: 0.38830074667930603, acc: 0.8823529481887817)
[2025-02-13 20:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:36][root][INFO] - Training Epoch: 2/2, step 1522/7134 completed (loss: 0.18733668327331543, acc: 0.9523809552192688)
[2025-02-13 20:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:36][root][INFO] - Training Epoch: 2/2, step 1523/7134 completed (loss: 0.3208089768886566, acc: 0.9144737124443054)
[2025-02-13 20:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37][root][INFO] - Training Epoch: 2/2, step 1524/7134 completed (loss: 0.05994215980172157, acc: 0.9726027250289917)
[2025-02-13 20:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37][root][INFO] - Training Epoch: 2/2, step 1525/7134 completed (loss: 0.1512427181005478, acc: 0.9700000286102295)
[2025-02-13 20:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37][root][INFO] - Training Epoch: 2/2, step 1526/7134 completed (loss: 0.17273424565792084, acc: 0.9387755393981934)
[2025-02-13 20:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38][root][INFO] - Training Epoch: 2/2, step 1527/7134 completed (loss: 0.29129353165626526, acc: 0.9292035102844238)
[2025-02-13 20:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38][root][INFO] - Training Epoch: 2/2, step 1528/7134 completed (loss: 0.1566501259803772, acc: 0.9420289993286133)
[2025-02-13 20:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39][root][INFO] - Training Epoch: 2/2, step 1529/7134 completed (loss: 0.08255542814731598, acc: 0.9734042286872864)
[2025-02-13 20:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39][root][INFO] - Training Epoch: 2/2, step 1530/7134 completed (loss: 0.11722897738218307, acc: 0.9642857313156128)
[2025-02-13 20:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39][root][INFO] - Training Epoch: 2/2, step 1531/7134 completed (loss: 0.3552708029747009, acc: 0.9166666865348816)
[2025-02-13 20:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40][root][INFO] - Training Epoch: 2/2, step 1532/7134 completed (loss: 0.04362818971276283, acc: 0.9897435903549194)
[2025-02-13 20:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40][root][INFO] - Training Epoch: 2/2, step 1533/7134 completed (loss: 0.17872576415538788, acc: 0.9518072009086609)
[2025-02-13 20:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40][root][INFO] - Training Epoch: 2/2, step 1534/7134 completed (loss: 0.14220915734767914, acc: 0.971222996711731)
[2025-02-13 20:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:41][root][INFO] - Training Epoch: 2/2, step 1535/7134 completed (loss: 0.12988530099391937, acc: 0.9599999785423279)
[2025-02-13 20:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:41][root][INFO] - Training Epoch: 2/2, step 1536/7134 completed (loss: 0.07537822425365448, acc: 0.9814814925193787)
[2025-02-13 20:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42][root][INFO] - Training Epoch: 2/2, step 1537/7134 completed (loss: 0.10670487582683563, acc: 0.9824561476707458)
[2025-02-13 20:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42][root][INFO] - Training Epoch: 2/2, step 1538/7134 completed (loss: 0.25076040625572205, acc: 0.9583333134651184)
[2025-02-13 20:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42][root][INFO] - Training Epoch: 2/2, step 1539/7134 completed (loss: 0.11795494705438614, acc: 0.9757575988769531)
[2025-02-13 20:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43][root][INFO] - Training Epoch: 2/2, step 1540/7134 completed (loss: 0.12106595933437347, acc: 0.9683544039726257)
[2025-02-13 20:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43][root][INFO] - Training Epoch: 2/2, step 1541/7134 completed (loss: 0.13405020534992218, acc: 0.9578947424888611)
[2025-02-13 20:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43][root][INFO] - Training Epoch: 2/2, step 1542/7134 completed (loss: 0.07812181860208511, acc: 0.983146071434021)
[2025-02-13 20:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:44][root][INFO] - Training Epoch: 2/2, step 1543/7134 completed (loss: 0.10124827921390533, acc: 0.9666666388511658)
[2025-02-13 20:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:44][root][INFO] - Training Epoch: 2/2, step 1544/7134 completed (loss: 0.05804426968097687, acc: 0.9750000238418579)
[2025-02-13 20:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45][root][INFO] - Training Epoch: 2/2, step 1545/7134 completed (loss: 0.11917007714509964, acc: 0.9523809552192688)
[2025-02-13 20:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45][root][INFO] - Training Epoch: 2/2, step 1546/7134 completed (loss: 0.30815842747688293, acc: 0.9411764740943909)
[2025-02-13 20:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45][root][INFO] - Training Epoch: 2/2, step 1547/7134 completed (loss: 0.16893643140792847, acc: 0.9444444179534912)
[2025-02-13 20:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46][root][INFO] - Training Epoch: 2/2, step 1548/7134 completed (loss: 0.09875789284706116, acc: 0.9736841917037964)
[2025-02-13 20:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46][root][INFO] - Training Epoch: 2/2, step 1549/7134 completed (loss: 0.02157234027981758, acc: 1.0)
[2025-02-13 20:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47][root][INFO] - Training Epoch: 2/2, step 1550/7134 completed (loss: 0.05248354747891426, acc: 0.9882352948188782)
[2025-02-13 20:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47][root][INFO] - Training Epoch: 2/2, step 1551/7134 completed (loss: 0.04139704629778862, acc: 0.9867549538612366)
[2025-02-13 20:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47][root][INFO] - Training Epoch: 2/2, step 1552/7134 completed (loss: 0.04039063677191734, acc: 0.9948979616165161)
[2025-02-13 20:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48][root][INFO] - Training Epoch: 2/2, step 1553/7134 completed (loss: 0.0685853436589241, acc: 0.9805194735527039)
[2025-02-13 20:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48][root][INFO] - Training Epoch: 2/2, step 1554/7134 completed (loss: 0.09248726814985275, acc: 0.9710982441902161)
[2025-02-13 20:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48][root][INFO] - Training Epoch: 2/2, step 1555/7134 completed (loss: 0.05095862224698067, acc: 0.987500011920929)
[2025-02-13 20:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49][root][INFO] - Training Epoch: 2/2, step 1556/7134 completed (loss: 0.04897641763091087, acc: 0.9887005686759949)
[2025-02-13 20:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49][root][INFO] - Training Epoch: 2/2, step 1557/7134 completed (loss: 0.053361717611551285, acc: 0.9888888597488403)
[2025-02-13 20:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50][root][INFO] - Training Epoch: 2/2, step 1558/7134 completed (loss: 0.11916928738355637, acc: 0.965753436088562)
[2025-02-13 20:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50][root][INFO] - Training Epoch: 2/2, step 1559/7134 completed (loss: 0.05777589976787567, acc: 0.977142870426178)
[2025-02-13 20:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50][root][INFO] - Training Epoch: 2/2, step 1560/7134 completed (loss: 0.07001496106386185, acc: 0.9847715497016907)
[2025-02-13 20:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51][root][INFO] - Training Epoch: 2/2, step 1561/7134 completed (loss: 0.16441667079925537, acc: 0.9525862336158752)
[2025-02-13 20:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51][root][INFO] - Training Epoch: 2/2, step 1562/7134 completed (loss: 0.08817814290523529, acc: 0.9822485446929932)
[2025-02-13 20:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51][root][INFO] - Training Epoch: 2/2, step 1563/7134 completed (loss: 0.09412997215986252, acc: 0.9656862616539001)
[2025-02-13 20:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52][root][INFO] - Training Epoch: 2/2, step 1564/7134 completed (loss: 0.18098683655261993, acc: 0.9405940771102905)
[2025-02-13 20:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52][root][INFO] - Training Epoch: 2/2, step 1565/7134 completed (loss: 0.0936143770813942, acc: 0.9874213933944702)
[2025-02-13 20:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53][root][INFO] - Training Epoch: 2/2, step 1566/7134 completed (loss: 0.0914001539349556, acc: 0.988950252532959)
[2025-02-13 20:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53][root][INFO] - Training Epoch: 2/2, step 1567/7134 completed (loss: 0.23978212475776672, acc: 0.9560975432395935)
[2025-02-13 20:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53][root][INFO] - Training Epoch: 2/2, step 1568/7134 completed (loss: 0.1602088063955307, acc: 0.9471153616905212)
[2025-02-13 20:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54][root][INFO] - Training Epoch: 2/2, step 1569/7134 completed (loss: 0.14637236297130585, acc: 0.9515151381492615)
[2025-02-13 20:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54][root][INFO] - Training Epoch: 2/2, step 1570/7134 completed (loss: 0.36271482706069946, acc: 0.9483568072319031)
[2025-02-13 20:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55][root][INFO] - Training Epoch: 2/2, step 1571/7134 completed (loss: 0.21939389407634735, acc: 0.9631336331367493)
[2025-02-13 20:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55][root][INFO] - Training Epoch: 2/2, step 1572/7134 completed (loss: 0.26187342405319214, acc: 0.9624413251876831)
[2025-02-13 20:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55][root][INFO] - Training Epoch: 2/2, step 1573/7134 completed (loss: 0.08826320618391037, acc: 0.9820359349250793)
[2025-02-13 20:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56][root][INFO] - Training Epoch: 2/2, step 1574/7134 completed (loss: 0.1977323591709137, acc: 0.9551569223403931)
[2025-02-13 20:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56][root][INFO] - Training Epoch: 2/2, step 1575/7134 completed (loss: 0.10711441189050674, acc: 0.9772727489471436)
[2025-02-13 20:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56][root][INFO] - Training Epoch: 2/2, step 1576/7134 completed (loss: 0.11803964525461197, acc: 0.9765258431434631)
[2025-02-13 20:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:57][root][INFO] - Training Epoch: 2/2, step 1577/7134 completed (loss: 0.12359154969453812, acc: 0.966183602809906)
[2025-02-13 20:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:57][root][INFO] - Training Epoch: 2/2, step 1578/7134 completed (loss: 0.11511614918708801, acc: 0.9659090638160706)
[2025-02-13 20:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58][root][INFO] - Training Epoch: 2/2, step 1579/7134 completed (loss: 0.04651212319731712, acc: 0.995192289352417)
[2025-02-13 20:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58][root][INFO] - Training Epoch: 2/2, step 1580/7134 completed (loss: 0.1783173531293869, acc: 0.9420289993286133)
[2025-02-13 20:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58][root][INFO] - Training Epoch: 2/2, step 1581/7134 completed (loss: 0.19632384181022644, acc: 0.9345238208770752)
[2025-02-13 20:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:59][root][INFO] - Training Epoch: 2/2, step 1582/7134 completed (loss: 0.1494670808315277, acc: 0.9666666388511658)
[2025-02-13 20:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:59][root][INFO] - Training Epoch: 2/2, step 1583/7134 completed (loss: 0.08507753908634186, acc: 0.9714285731315613)
[2025-02-13 20:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00][root][INFO] - Training Epoch: 2/2, step 1584/7134 completed (loss: 0.10762955248355865, acc: 0.9729729890823364)
[2025-02-13 20:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00][root][INFO] - Training Epoch: 2/2, step 1585/7134 completed (loss: 0.05792892351746559, acc: 0.9854369163513184)
[2025-02-13 20:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00][root][INFO] - Training Epoch: 2/2, step 1586/7134 completed (loss: 0.13531950116157532, acc: 0.95652174949646)
[2025-02-13 20:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01][root][INFO] - Training Epoch: 2/2, step 1587/7134 completed (loss: 0.05032862350344658, acc: 0.987261176109314)
[2025-02-13 20:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01][root][INFO] - Training Epoch: 2/2, step 1588/7134 completed (loss: 0.0874922052025795, acc: 0.9808917045593262)
[2025-02-13 20:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01][root][INFO] - Training Epoch: 2/2, step 1589/7134 completed (loss: 0.06539232283830643, acc: 0.9878048896789551)
[2025-02-13 20:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:02][root][INFO] - Training Epoch: 2/2, step 1590/7134 completed (loss: 0.07518003135919571, acc: 0.9857142567634583)
[2025-02-13 20:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:02][root][INFO] - Training Epoch: 2/2, step 1591/7134 completed (loss: 0.04607083275914192, acc: 0.9844961166381836)
[2025-02-13 20:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03][root][INFO] - Training Epoch: 2/2, step 1592/7134 completed (loss: 0.11081906408071518, acc: 0.9740259647369385)
[2025-02-13 20:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03][root][INFO] - Training Epoch: 2/2, step 1593/7134 completed (loss: 0.051571428775787354, acc: 0.9946523904800415)
[2025-02-13 20:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03][root][INFO] - Training Epoch: 2/2, step 1594/7134 completed (loss: 0.06053537130355835, acc: 0.9765625)
[2025-02-13 20:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04][root][INFO] - Training Epoch: 2/2, step 1595/7134 completed (loss: 0.019284849986433983, acc: 1.0)
[2025-02-13 20:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04][root][INFO] - Training Epoch: 2/2, step 1596/7134 completed (loss: 0.15755830705165863, acc: 0.9861111044883728)
[2025-02-13 20:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04][root][INFO] - Training Epoch: 2/2, step 1597/7134 completed (loss: 0.021423552185297012, acc: 1.0)
[2025-02-13 20:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:05][root][INFO] - Training Epoch: 2/2, step 1598/7134 completed (loss: 0.10252130776643753, acc: 0.9583333134651184)
[2025-02-13 20:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:05][root][INFO] - Training Epoch: 2/2, step 1599/7134 completed (loss: 0.11314129084348679, acc: 0.9793103337287903)
[2025-02-13 20:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06][root][INFO] - Training Epoch: 2/2, step 1600/7134 completed (loss: 0.11725005507469177, acc: 0.9744898080825806)
[2025-02-13 20:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06][root][INFO] - Training Epoch: 2/2, step 1601/7134 completed (loss: 0.11353439837694168, acc: 0.9818181991577148)
[2025-02-13 20:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06][root][INFO] - Training Epoch: 2/2, step 1602/7134 completed (loss: 0.10888515412807465, acc: 0.9807692170143127)
[2025-02-13 20:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07][root][INFO] - Training Epoch: 2/2, step 1603/7134 completed (loss: 0.09953484684228897, acc: 0.9879518151283264)
[2025-02-13 20:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07][root][INFO] - Training Epoch: 2/2, step 1604/7134 completed (loss: 0.06484193354845047, acc: 0.9757575988769531)
[2025-02-13 20:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07][root][INFO] - Training Epoch: 2/2, step 1605/7134 completed (loss: 0.07036146521568298, acc: 0.9823529124259949)
[2025-02-13 20:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08][root][INFO] - Training Epoch: 2/2, step 1606/7134 completed (loss: 0.07564102858304977, acc: 0.9863945841789246)
[2025-02-13 20:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08][root][INFO] - Training Epoch: 2/2, step 1607/7134 completed (loss: 0.017453210428357124, acc: 1.0)
[2025-02-13 20:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09][root][INFO] - Training Epoch: 2/2, step 1608/7134 completed (loss: 0.026843257248401642, acc: 0.9921259880065918)
[2025-02-13 20:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09][root][INFO] - Training Epoch: 2/2, step 1609/7134 completed (loss: 0.10650506615638733, acc: 0.9583333134651184)
[2025-02-13 20:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09][root][INFO] - Training Epoch: 2/2, step 1610/7134 completed (loss: 0.057098206132650375, acc: 0.9923664331436157)
[2025-02-13 20:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10][root][INFO] - Training Epoch: 2/2, step 1611/7134 completed (loss: 0.06551007926464081, acc: 0.9876543283462524)
[2025-02-13 20:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10][root][INFO] - Training Epoch: 2/2, step 1612/7134 completed (loss: 0.12617504596710205, acc: 0.9638554453849792)
[2025-02-13 20:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10][root][INFO] - Training Epoch: 2/2, step 1613/7134 completed (loss: 0.13550680875778198, acc: 0.9751552939414978)
[2025-02-13 20:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:11][root][INFO] - Training Epoch: 2/2, step 1614/7134 completed (loss: 0.09793303161859512, acc: 0.9671052694320679)
[2025-02-13 20:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:11][root][INFO] - Training Epoch: 2/2, step 1615/7134 completed (loss: 0.07242497056722641, acc: 0.9698795080184937)
[2025-02-13 20:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12][root][INFO] - Training Epoch: 2/2, step 1616/7134 completed (loss: 0.08174069970846176, acc: 0.9935483932495117)
[2025-02-13 20:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12][root][INFO] - Training Epoch: 2/2, step 1617/7134 completed (loss: 0.1053449884057045, acc: 0.9784172773361206)
[2025-02-13 20:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12][root][INFO] - Training Epoch: 2/2, step 1618/7134 completed (loss: 0.27195242047309875, acc: 0.9465649127960205)
[2025-02-13 20:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:13][root][INFO] - Training Epoch: 2/2, step 1619/7134 completed (loss: 0.1231820285320282, acc: 0.9756097793579102)
[2025-02-13 20:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:13][root][INFO] - Training Epoch: 2/2, step 1620/7134 completed (loss: 0.15414729714393616, acc: 0.9545454382896423)
[2025-02-13 20:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14][root][INFO] - Training Epoch: 2/2, step 1621/7134 completed (loss: 0.12007544934749603, acc: 0.9607843160629272)
[2025-02-13 20:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14][root][INFO] - Training Epoch: 2/2, step 1622/7134 completed (loss: 0.07705622911453247, acc: 0.9803921580314636)
[2025-02-13 20:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14][root][INFO] - Training Epoch: 2/2, step 1623/7134 completed (loss: 0.05889261141419411, acc: 0.9935897588729858)
[2025-02-13 20:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15][root][INFO] - Training Epoch: 2/2, step 1624/7134 completed (loss: 0.1691141128540039, acc: 0.9675324559211731)
[2025-02-13 20:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15][root][INFO] - Training Epoch: 2/2, step 1625/7134 completed (loss: 0.12016265094280243, acc: 0.956250011920929)
[2025-02-13 20:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15][root][INFO] - Training Epoch: 2/2, step 1626/7134 completed (loss: 0.12756486237049103, acc: 0.969072163105011)
[2025-02-13 20:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:16][root][INFO] - Training Epoch: 2/2, step 1627/7134 completed (loss: 0.08170318603515625, acc: 0.9593495726585388)
[2025-02-13 20:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:16][root][INFO] - Training Epoch: 2/2, step 1628/7134 completed (loss: 0.050310760736465454, acc: 0.9814814925193787)
[2025-02-13 20:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17][root][INFO] - Training Epoch: 2/2, step 1629/7134 completed (loss: 0.07490044087171555, acc: 0.9924242496490479)
[2025-02-13 20:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17][root][INFO] - Training Epoch: 2/2, step 1630/7134 completed (loss: 0.044465746730566025, acc: 1.0)
[2025-02-13 20:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17][root][INFO] - Training Epoch: 2/2, step 1631/7134 completed (loss: 0.15840116143226624, acc: 0.9640287756919861)
[2025-02-13 20:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:18][root][INFO] - Training Epoch: 2/2, step 1632/7134 completed (loss: 0.08812259137630463, acc: 0.9925373196601868)
[2025-02-13 20:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:18][root][INFO] - Training Epoch: 2/2, step 1633/7134 completed (loss: 0.061501454561948776, acc: 0.9785714149475098)
[2025-02-13 20:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19][root][INFO] - Training Epoch: 2/2, step 1634/7134 completed (loss: 0.08729232102632523, acc: 0.9748427867889404)
[2025-02-13 20:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19][root][INFO] - Training Epoch: 2/2, step 1635/7134 completed (loss: 0.11240953207015991, acc: 0.9722222089767456)
[2025-02-13 20:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19][root][INFO] - Training Epoch: 2/2, step 1636/7134 completed (loss: 0.1134125217795372, acc: 0.963302731513977)
[2025-02-13 20:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20][root][INFO] - Training Epoch: 2/2, step 1637/7134 completed (loss: 0.07768606394529343, acc: 0.9875776171684265)
[2025-02-13 20:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20][root][INFO] - Training Epoch: 2/2, step 1638/7134 completed (loss: 0.20390696823596954, acc: 0.9212598204612732)
[2025-02-13 20:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20][root][INFO] - Training Epoch: 2/2, step 1639/7134 completed (loss: 0.39009130001068115, acc: 0.8985507488250732)
[2025-02-13 20:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:21][root][INFO] - Training Epoch: 2/2, step 1640/7134 completed (loss: 0.1326839029788971, acc: 0.9492753744125366)
[2025-02-13 20:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:21][root][INFO] - Training Epoch: 2/2, step 1641/7134 completed (loss: 0.07802878320217133, acc: 0.9802631735801697)
[2025-02-13 20:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22][root][INFO] - Training Epoch: 2/2, step 1642/7134 completed (loss: 0.11047384887933731, acc: 0.955974817276001)
[2025-02-13 20:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22][root][INFO] - Training Epoch: 2/2, step 1643/7134 completed (loss: 0.22527670860290527, acc: 0.918181836605072)
[2025-02-13 20:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22][root][INFO] - Training Epoch: 2/2, step 1644/7134 completed (loss: 0.14056234061717987, acc: 0.9622641801834106)
[2025-02-13 20:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23][root][INFO] - Training Epoch: 2/2, step 1645/7134 completed (loss: 0.20085211098194122, acc: 0.9379310607910156)
[2025-02-13 20:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23][root][INFO] - Training Epoch: 2/2, step 1646/7134 completed (loss: 0.13254041969776154, acc: 0.9741935729980469)
[2025-02-13 20:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23][root][INFO] - Training Epoch: 2/2, step 1647/7134 completed (loss: 0.08663932979106903, acc: 0.9718309640884399)
[2025-02-13 20:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24][root][INFO] - Training Epoch: 2/2, step 1648/7134 completed (loss: 0.11338817328214645, acc: 0.9701492786407471)
[2025-02-13 20:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24][root][INFO] - Training Epoch: 2/2, step 1649/7134 completed (loss: 0.03847215697169304, acc: 0.991150438785553)
[2025-02-13 20:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25][root][INFO] - Training Epoch: 2/2, step 1650/7134 completed (loss: 0.053302474319934845, acc: 0.9949748516082764)
[2025-02-13 20:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25][root][INFO] - Training Epoch: 2/2, step 1651/7134 completed (loss: 0.12376397848129272, acc: 0.9636363387107849)
[2025-02-13 20:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25][root][INFO] - Training Epoch: 2/2, step 1652/7134 completed (loss: 0.16171583533287048, acc: 0.9529914259910583)
[2025-02-13 20:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26][root][INFO] - Training Epoch: 2/2, step 1653/7134 completed (loss: 0.07493841648101807, acc: 0.9781420826911926)
[2025-02-13 20:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26][root][INFO] - Training Epoch: 2/2, step 1654/7134 completed (loss: 0.0944431722164154, acc: 0.9742489457130432)
[2025-02-13 20:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26][root][INFO] - Training Epoch: 2/2, step 1655/7134 completed (loss: 0.061572443693876266, acc: 0.9897959232330322)
[2025-02-13 20:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:27][root][INFO] - Training Epoch: 2/2, step 1656/7134 completed (loss: 0.16745169460773468, acc: 0.9730941653251648)
[2025-02-13 20:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:27][root][INFO] - Training Epoch: 2/2, step 1657/7134 completed (loss: 0.11087857186794281, acc: 0.9689922332763672)
[2025-02-13 20:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28][root][INFO] - Training Epoch: 2/2, step 1658/7134 completed (loss: 0.09123087674379349, acc: 0.9751037359237671)
[2025-02-13 20:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28][root][INFO] - Training Epoch: 2/2, step 1659/7134 completed (loss: 0.08029531687498093, acc: 0.9800000190734863)
[2025-02-13 20:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28][root][INFO] - Training Epoch: 2/2, step 1660/7134 completed (loss: 0.08860365301370621, acc: 0.9855769276618958)
[2025-02-13 20:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:29][root][INFO] - Training Epoch: 2/2, step 1661/7134 completed (loss: 0.08295512199401855, acc: 0.9740259647369385)
[2025-02-13 20:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:29][root][INFO] - Training Epoch: 2/2, step 1662/7134 completed (loss: 0.0479443185031414, acc: 0.9905213117599487)
[2025-02-13 20:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30][root][INFO] - Training Epoch: 2/2, step 1663/7134 completed (loss: 0.05579139292240143, acc: 0.9872340559959412)
[2025-02-13 20:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30][root][INFO] - Training Epoch: 2/2, step 1664/7134 completed (loss: 0.05377854034304619, acc: 0.9800000190734863)
[2025-02-13 20:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30][root][INFO] - Training Epoch: 2/2, step 1665/7134 completed (loss: 0.0634964108467102, acc: 0.987730085849762)
[2025-02-13 20:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:31][root][INFO] - Training Epoch: 2/2, step 1666/7134 completed (loss: 0.03756099194288254, acc: 0.9915966391563416)
[2025-02-13 20:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:31][root][INFO] - Training Epoch: 2/2, step 1667/7134 completed (loss: 0.05211058631539345, acc: 0.9898989796638489)
[2025-02-13 20:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32][root][INFO] - Training Epoch: 2/2, step 1668/7134 completed (loss: 0.058447666466236115, acc: 0.9820627570152283)
[2025-02-13 20:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32][root][INFO] - Training Epoch: 2/2, step 1669/7134 completed (loss: 0.059462275356054306, acc: 0.9847328066825867)
[2025-02-13 20:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32][root][INFO] - Training Epoch: 2/2, step 1670/7134 completed (loss: 0.11437103152275085, acc: 0.9733840227127075)
[2025-02-13 20:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33][root][INFO] - Training Epoch: 2/2, step 1671/7134 completed (loss: 0.07834234833717346, acc: 0.9744898080825806)
[2025-02-13 20:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33][root][INFO] - Training Epoch: 2/2, step 1672/7134 completed (loss: 0.08848346769809723, acc: 0.9649122953414917)
[2025-02-13 20:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33][root][INFO] - Training Epoch: 2/2, step 1673/7134 completed (loss: 0.10864255577325821, acc: 0.9813664555549622)
[2025-02-13 20:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34][root][INFO] - Training Epoch: 2/2, step 1674/7134 completed (loss: 0.09634125232696533, acc: 0.9802631735801697)
[2025-02-13 20:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34][root][INFO] - Training Epoch: 2/2, step 1675/7134 completed (loss: 0.12154927849769592, acc: 0.965753436088562)
[2025-02-13 20:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35][root][INFO] - Training Epoch: 2/2, step 1676/7134 completed (loss: 0.11005362123250961, acc: 0.9807692170143127)
[2025-02-13 20:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35][root][INFO] - Training Epoch: 2/2, step 1677/7134 completed (loss: 0.2335422933101654, acc: 0.9503546357154846)
[2025-02-13 20:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35][root][INFO] - Training Epoch: 2/2, step 1678/7134 completed (loss: 0.17245498299598694, acc: 0.970059871673584)
[2025-02-13 20:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36][root][INFO] - Training Epoch: 2/2, step 1679/7134 completed (loss: 0.10182588547468185, acc: 0.9817073345184326)
[2025-02-13 20:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36][root][INFO] - Training Epoch: 2/2, step 1680/7134 completed (loss: 0.08597222715616226, acc: 0.9823529124259949)
[2025-02-13 20:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36][root][INFO] - Training Epoch: 2/2, step 1681/7134 completed (loss: 0.0880013108253479, acc: 0.9811320900917053)
[2025-02-13 20:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37][root][INFO] - Training Epoch: 2/2, step 1682/7134 completed (loss: 0.13695019483566284, acc: 0.965753436088562)
[2025-02-13 20:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37][root][INFO] - Training Epoch: 2/2, step 1683/7134 completed (loss: 0.07353787869215012, acc: 0.9793103337287903)
[2025-02-13 20:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38][root][INFO] - Training Epoch: 2/2, step 1684/7134 completed (loss: 0.05041491240262985, acc: 0.9858155846595764)
[2025-02-13 20:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38][root][INFO] - Training Epoch: 2/2, step 1685/7134 completed (loss: 0.11324264109134674, acc: 0.9575757384300232)
[2025-02-13 20:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38][root][INFO] - Training Epoch: 2/2, step 1686/7134 completed (loss: 0.026159213855862617, acc: 0.9849624037742615)
[2025-02-13 20:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39][root][INFO] - Training Epoch: 2/2, step 1687/7134 completed (loss: 0.032685671001672745, acc: 0.991150438785553)
[2025-02-13 20:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39][root][INFO] - Training Epoch: 2/2, step 1688/7134 completed (loss: 0.051798731088638306, acc: 0.9863013625144958)
[2025-02-13 20:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39][root][INFO] - Training Epoch: 2/2, step 1689/7134 completed (loss: 0.05621945858001709, acc: 0.9935483932495117)
[2025-02-13 20:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:40][root][INFO] - Training Epoch: 2/2, step 1690/7134 completed (loss: 0.06379597634077072, acc: 0.982300877571106)
[2025-02-13 20:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:40][root][INFO] - Training Epoch: 2/2, step 1691/7134 completed (loss: 0.03211129084229469, acc: 0.9929078221321106)
[2025-02-13 20:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41][root][INFO] - Training Epoch: 2/2, step 1692/7134 completed (loss: 0.062015973031520844, acc: 0.9674796462059021)
[2025-02-13 20:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41][root][INFO] - Training Epoch: 2/2, step 1693/7134 completed (loss: 0.03379041701555252, acc: 0.9931972622871399)
[2025-02-13 20:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41][root][INFO] - Training Epoch: 2/2, step 1694/7134 completed (loss: 0.10732776671648026, acc: 0.9793103337287903)
[2025-02-13 20:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:42][root][INFO] - Training Epoch: 2/2, step 1695/7134 completed (loss: 0.09843698143959045, acc: 0.9663865566253662)
[2025-02-13 20:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:42][root][INFO] - Training Epoch: 2/2, step 1696/7134 completed (loss: 0.131830096244812, acc: 0.9624060392379761)
[2025-02-13 20:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43][root][INFO] - Training Epoch: 2/2, step 1697/7134 completed (loss: 0.04002612084150314, acc: 1.0)
[2025-02-13 20:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43][root][INFO] - Training Epoch: 2/2, step 1698/7134 completed (loss: 0.01683684252202511, acc: 1.0)
[2025-02-13 20:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43][root][INFO] - Training Epoch: 2/2, step 1699/7134 completed (loss: 0.04383523389697075, acc: 0.9925373196601868)
[2025-02-13 20:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44][root][INFO] - Training Epoch: 2/2, step 1700/7134 completed (loss: 0.0827488824725151, acc: 0.9701492786407471)
[2025-02-13 20:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44][root][INFO] - Training Epoch: 2/2, step 1701/7134 completed (loss: 0.05814477428793907, acc: 1.0)
[2025-02-13 20:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44][root][INFO] - Training Epoch: 2/2, step 1702/7134 completed (loss: 0.10191837698221207, acc: 0.970059871673584)
[2025-02-13 20:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:45][root][INFO] - Training Epoch: 2/2, step 1703/7134 completed (loss: 0.2041560709476471, acc: 0.9470198750495911)
[2025-02-13 20:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:45][root][INFO] - Training Epoch: 2/2, step 1704/7134 completed (loss: 0.22054311633110046, acc: 0.9451219439506531)
[2025-02-13 20:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46][root][INFO] - Training Epoch: 2/2, step 1705/7134 completed (loss: 0.1472734808921814, acc: 0.9760000109672546)
[2025-02-13 20:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46][root][INFO] - Training Epoch: 2/2, step 1706/7134 completed (loss: 0.07637554407119751, acc: 0.9714285731315613)
[2025-02-13 20:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46][root][INFO] - Training Epoch: 2/2, step 1707/7134 completed (loss: 0.1760099232196808, acc: 0.9655172228813171)
[2025-02-13 20:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47][root][INFO] - Training Epoch: 2/2, step 1708/7134 completed (loss: 0.055883195251226425, acc: 0.9925925731658936)
[2025-02-13 20:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47][root][INFO] - Training Epoch: 2/2, step 1709/7134 completed (loss: 0.08704815804958344, acc: 0.9866666793823242)
[2025-02-13 20:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48][root][INFO] - Training Epoch: 2/2, step 1710/7134 completed (loss: 0.12022225558757782, acc: 0.9864864945411682)
[2025-02-13 20:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48][root][INFO] - Training Epoch: 2/2, step 1711/7134 completed (loss: 0.11062947660684586, acc: 0.9919354915618896)
[2025-02-13 20:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48][root][INFO] - Training Epoch: 2/2, step 1712/7134 completed (loss: 0.04964108765125275, acc: 0.9920634627342224)
[2025-02-13 20:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49][root][INFO] - Training Epoch: 2/2, step 1713/7134 completed (loss: 0.04447115212678909, acc: 1.0)
[2025-02-13 20:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49][root][INFO] - Training Epoch: 2/2, step 1714/7134 completed (loss: 0.08628429472446442, acc: 0.9716312289237976)
[2025-02-13 20:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50][root][INFO] - Training Epoch: 2/2, step 1715/7134 completed (loss: 0.04898032173514366, acc: 0.9905660152435303)
[2025-02-13 20:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50][root][INFO] - Training Epoch: 2/2, step 1716/7134 completed (loss: 0.15021105110645294, acc: 0.9671052694320679)
[2025-02-13 20:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50][root][INFO] - Training Epoch: 2/2, step 1717/7134 completed (loss: 0.08598224818706512, acc: 0.9922480583190918)
[2025-02-13 20:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51][root][INFO] - Training Epoch: 2/2, step 1718/7134 completed (loss: 0.08047878742218018, acc: 0.9914529919624329)
[2025-02-13 20:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51][root][INFO] - Training Epoch: 2/2, step 1719/7134 completed (loss: 0.1512233167886734, acc: 0.9704142212867737)
[2025-02-13 20:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51][root][INFO] - Training Epoch: 2/2, step 1720/7134 completed (loss: 0.14314071834087372, acc: 0.9692307710647583)
[2025-02-13 20:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52][root][INFO] - Training Epoch: 2/2, step 1721/7134 completed (loss: 0.07497603446245193, acc: 0.96875)
[2025-02-13 20:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52][root][INFO] - Training Epoch: 2/2, step 1722/7134 completed (loss: 0.07669419050216675, acc: 0.9696969985961914)
[2025-02-13 20:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53][root][INFO] - Training Epoch: 2/2, step 1723/7134 completed (loss: 0.18247562646865845, acc: 0.9567901492118835)
[2025-02-13 20:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53][root][INFO] - Training Epoch: 2/2, step 1724/7134 completed (loss: 0.10746873915195465, acc: 0.984375)
[2025-02-13 20:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53][root][INFO] - Training Epoch: 2/2, step 1725/7134 completed (loss: 0.037347570061683655, acc: 1.0)
[2025-02-13 20:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54][root][INFO] - Training Epoch: 2/2, step 1726/7134 completed (loss: 0.04129481315612793, acc: 0.9784946441650391)
[2025-02-13 20:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54][root][INFO] - Training Epoch: 2/2, step 1727/7134 completed (loss: 0.04704825207591057, acc: 0.991150438785553)
[2025-02-13 20:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54][root][INFO] - Training Epoch: 2/2, step 1728/7134 completed (loss: 0.060801513493061066, acc: 0.9846153855323792)
[2025-02-13 20:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:55][root][INFO] - Training Epoch: 2/2, step 1729/7134 completed (loss: 0.08825979381799698, acc: 0.9819819927215576)
[2025-02-13 20:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:55][root][INFO] - Training Epoch: 2/2, step 1730/7134 completed (loss: 0.060437221080064774, acc: 0.9813084006309509)
[2025-02-13 20:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56][root][INFO] - Training Epoch: 2/2, step 1731/7134 completed (loss: 0.22216638922691345, acc: 0.9518072009086609)
[2025-02-13 20:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56][root][INFO] - Training Epoch: 2/2, step 1732/7134 completed (loss: 0.04123555123806, acc: 1.0)
[2025-02-13 20:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56][root][INFO] - Training Epoch: 2/2, step 1733/7134 completed (loss: 0.13624435663223267, acc: 0.9541284441947937)
[2025-02-13 20:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57][root][INFO] - Training Epoch: 2/2, step 1734/7134 completed (loss: 0.155544251203537, acc: 0.9518072009086609)
[2025-02-13 20:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57][root][INFO] - Training Epoch: 2/2, step 1735/7134 completed (loss: 0.07772823423147202, acc: 0.9867549538612366)
[2025-02-13 20:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57][root][INFO] - Training Epoch: 2/2, step 1736/7134 completed (loss: 0.09497205913066864, acc: 0.9810126423835754)
[2025-02-13 20:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58][root][INFO] - Training Epoch: 2/2, step 1737/7134 completed (loss: 0.0752921774983406, acc: 0.9933333396911621)
[2025-02-13 20:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58][root][INFO] - Training Epoch: 2/2, step 1738/7134 completed (loss: 0.022860711440443993, acc: 0.994413435459137)
[2025-02-13 20:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59][root][INFO] - Training Epoch: 2/2, step 1739/7134 completed (loss: 0.09000111371278763, acc: 0.9624060392379761)
[2025-02-13 20:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59][root][INFO] - Training Epoch: 2/2, step 1740/7134 completed (loss: 0.03772512450814247, acc: 0.9864864945411682)
[2025-02-13 20:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59][root][INFO] - Training Epoch: 2/2, step 1741/7134 completed (loss: 0.07644634693861008, acc: 0.9751552939414978)
[2025-02-13 20:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00][root][INFO] - Training Epoch: 2/2, step 1742/7134 completed (loss: 0.07018385827541351, acc: 0.9801324605941772)
[2025-02-13 20:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00][root][INFO] - Training Epoch: 2/2, step 1743/7134 completed (loss: 0.08897239714860916, acc: 0.9863945841789246)
[2025-02-13 20:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00][root][INFO] - Training Epoch: 2/2, step 1744/7134 completed (loss: 0.03275449946522713, acc: 0.9924812316894531)
[2025-02-13 20:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01][root][INFO] - Training Epoch: 2/2, step 1745/7134 completed (loss: 0.07101590186357498, acc: 0.9764705896377563)
[2025-02-13 20:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01][root][INFO] - Training Epoch: 2/2, step 1746/7134 completed (loss: 0.033178407698869705, acc: 0.9858155846595764)
[2025-02-13 20:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02][root][INFO] - Training Epoch: 2/2, step 1747/7134 completed (loss: 0.11594164371490479, acc: 0.9634146094322205)
[2025-02-13 20:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02][root][INFO] - Training Epoch: 2/2, step 1748/7134 completed (loss: 0.07993406802415848, acc: 0.9882352948188782)
[2025-02-13 20:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02][root][INFO] - Training Epoch: 2/2, step 1749/7134 completed (loss: 0.12321748584508896, acc: 0.9685863852500916)
[2025-02-13 20:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:03][root][INFO] - Training Epoch: 2/2, step 1750/7134 completed (loss: 0.17344962060451508, acc: 0.9704142212867737)
[2025-02-13 20:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:03][root][INFO] - Training Epoch: 2/2, step 1751/7134 completed (loss: 0.18116477131843567, acc: 0.9702380895614624)
[2025-02-13 20:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04][root][INFO] - Training Epoch: 2/2, step 1752/7134 completed (loss: 0.2220337837934494, acc: 0.9696969985961914)
[2025-02-13 20:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04][root][INFO] - Training Epoch: 2/2, step 1753/7134 completed (loss: 0.16535431146621704, acc: 0.9735099077224731)
[2025-02-13 20:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04][root][INFO] - Training Epoch: 2/2, step 1754/7134 completed (loss: 0.12126903980970383, acc: 0.9803921580314636)
[2025-02-13 20:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05][root][INFO] - Training Epoch: 2/2, step 1755/7134 completed (loss: 0.08226751536130905, acc: 0.9738562107086182)
[2025-02-13 20:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05][root][INFO] - Training Epoch: 2/2, step 1756/7134 completed (loss: 0.1469520628452301, acc: 0.9666666388511658)
[2025-02-13 20:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05][root][INFO] - Training Epoch: 2/2, step 1757/7134 completed (loss: 0.1251254379749298, acc: 0.9752066135406494)
[2025-02-13 20:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06][root][INFO] - Training Epoch: 2/2, step 1758/7134 completed (loss: 0.05271497741341591, acc: 0.9882352948188782)
[2025-02-13 20:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06][root][INFO] - Training Epoch: 2/2, step 1759/7134 completed (loss: 0.047779228538274765, acc: 1.0)
[2025-02-13 20:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06][root][INFO] - Training Epoch: 2/2, step 1760/7134 completed (loss: 0.03942961245775223, acc: 1.0)
[2025-02-13 20:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:07][root][INFO] - Training Epoch: 2/2, step 1761/7134 completed (loss: 0.03889339789748192, acc: 1.0)
[2025-02-13 20:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:07][root][INFO] - Training Epoch: 2/2, step 1762/7134 completed (loss: 0.07083196192979813, acc: 0.9902912378311157)
[2025-02-13 20:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08][root][INFO] - Training Epoch: 2/2, step 1763/7134 completed (loss: 0.113422691822052, acc: 0.9726027250289917)
[2025-02-13 20:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08][root][INFO] - Training Epoch: 2/2, step 1764/7134 completed (loss: 0.1479131430387497, acc: 0.975806474685669)
[2025-02-13 20:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08][root][INFO] - Training Epoch: 2/2, step 1765/7134 completed (loss: 0.106679767370224, acc: 0.9673202633857727)
[2025-02-13 20:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09][root][INFO] - Training Epoch: 2/2, step 1766/7134 completed (loss: 0.10467637330293655, acc: 0.9740259647369385)
[2025-02-13 20:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09][root][INFO] - Training Epoch: 2/2, step 1767/7134 completed (loss: 0.2540464699268341, acc: 0.9435028433799744)
[2025-02-13 20:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09][root][INFO] - Training Epoch: 2/2, step 1768/7134 completed (loss: 0.026601780205965042, acc: 0.9910714030265808)
[2025-02-13 20:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:10][root][INFO] - Training Epoch: 2/2, step 1769/7134 completed (loss: 0.12467709928750992, acc: 0.9666666388511658)
[2025-02-13 20:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:10][root][INFO] - Training Epoch: 2/2, step 1770/7134 completed (loss: 0.08838587254285812, acc: 0.9794520735740662)
[2025-02-13 20:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11][root][INFO] - Training Epoch: 2/2, step 1771/7134 completed (loss: 0.10736642777919769, acc: 0.9740259647369385)
[2025-02-13 20:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11][root][INFO] - Training Epoch: 2/2, step 1772/7134 completed (loss: 0.12350154668092728, acc: 0.9785714149475098)
[2025-02-13 20:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11][root][INFO] - Training Epoch: 2/2, step 1773/7134 completed (loss: 0.05709484964609146, acc: 0.9858155846595764)
[2025-02-13 20:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12][root][INFO] - Training Epoch: 2/2, step 1774/7134 completed (loss: 0.02110971510410309, acc: 1.0)
[2025-02-13 20:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12][root][INFO] - Training Epoch: 2/2, step 1775/7134 completed (loss: 0.026882939040660858, acc: 0.9931507110595703)
[2025-02-13 20:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12][root][INFO] - Training Epoch: 2/2, step 1776/7134 completed (loss: 0.04188547283411026, acc: 1.0)
[2025-02-13 20:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13][root][INFO] - Training Epoch: 2/2, step 1777/7134 completed (loss: 0.023736368864774704, acc: 0.993630588054657)
[2025-02-13 20:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13][root][INFO] - Training Epoch: 2/2, step 1778/7134 completed (loss: 0.14313168823719025, acc: 0.955974817276001)
[2025-02-13 20:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13][root][INFO] - Training Epoch: 2/2, step 1779/7134 completed (loss: 0.06163373962044716, acc: 0.982300877571106)
[2025-02-13 20:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:14][root][INFO] - Training Epoch: 2/2, step 1780/7134 completed (loss: 0.03948263078927994, acc: 0.9940119981765747)
[2025-02-13 20:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:02][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2413, device='cuda:0') eval_epoch_loss=tensor(0.2162, device='cuda:0') eval_epoch_acc=tensor(0.9491, device='cuda:0')
[2025-02-13 20:35:02][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:35:02][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:35:02][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_1781_loss_0.21616549789905548/model.pt
[2025-02-13 20:35:02][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:35:02][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.21616549789905548
[2025-02-13 20:35:02][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9491442441940308
[2025-02-13 20:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:02][root][INFO] - Training Epoch: 2/2, step 1781/7134 completed (loss: 0.07574019581079483, acc: 0.9731543660163879)
[2025-02-13 20:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03][root][INFO] - Training Epoch: 2/2, step 1782/7134 completed (loss: 0.022250577807426453, acc: 0.9935897588729858)
[2025-02-13 20:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03][root][INFO] - Training Epoch: 2/2, step 1783/7134 completed (loss: 0.09535353630781174, acc: 0.9768785834312439)
[2025-02-13 20:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03][root][INFO] - Training Epoch: 2/2, step 1784/7134 completed (loss: 0.07002651691436768, acc: 0.9935897588729858)
[2025-02-13 20:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:04][root][INFO] - Training Epoch: 2/2, step 1785/7134 completed (loss: 0.12143029272556305, acc: 0.9638554453849792)
[2025-02-13 20:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:04][root][INFO] - Training Epoch: 2/2, step 1786/7134 completed (loss: 0.028621722012758255, acc: 1.0)
[2025-02-13 20:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05][root][INFO] - Training Epoch: 2/2, step 1787/7134 completed (loss: 0.11848493665456772, acc: 0.9477611780166626)
[2025-02-13 20:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05][root][INFO] - Training Epoch: 2/2, step 1788/7134 completed (loss: 0.11206266283988953, acc: 0.9793103337287903)
[2025-02-13 20:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05][root][INFO] - Training Epoch: 2/2, step 1789/7134 completed (loss: 0.08173343539237976, acc: 0.9802631735801697)
[2025-02-13 20:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06][root][INFO] - Training Epoch: 2/2, step 1790/7134 completed (loss: 0.39166903495788574, acc: 0.9161290526390076)
[2025-02-13 20:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06][root][INFO] - Training Epoch: 2/2, step 1791/7134 completed (loss: 0.34279462695121765, acc: 0.9386503100395203)
[2025-02-13 20:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06][root][INFO] - Training Epoch: 2/2, step 1792/7134 completed (loss: 0.6441323757171631, acc: 0.910614550113678)
[2025-02-13 20:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07][root][INFO] - Training Epoch: 2/2, step 1793/7134 completed (loss: 0.3056476414203644, acc: 0.9585798978805542)
[2025-02-13 20:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07][root][INFO] - Training Epoch: 2/2, step 1794/7134 completed (loss: 0.26083487272262573, acc: 0.9276315569877625)
[2025-02-13 20:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08][root][INFO] - Training Epoch: 2/2, step 1795/7134 completed (loss: 0.41824230551719666, acc: 0.887499988079071)
[2025-02-13 20:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08][root][INFO] - Training Epoch: 2/2, step 1796/7134 completed (loss: 0.3172526955604553, acc: 0.909604549407959)
[2025-02-13 20:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08][root][INFO] - Training Epoch: 2/2, step 1797/7134 completed (loss: 0.2647654116153717, acc: 0.9257143139839172)
[2025-02-13 20:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09][root][INFO] - Training Epoch: 2/2, step 1798/7134 completed (loss: 0.3105056583881378, acc: 0.9069767594337463)
[2025-02-13 20:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09][root][INFO] - Training Epoch: 2/2, step 1799/7134 completed (loss: 0.3023350238800049, acc: 0.9292929172515869)
[2025-02-13 20:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09][root][INFO] - Training Epoch: 2/2, step 1800/7134 completed (loss: 0.18818169832229614, acc: 0.9476190209388733)
[2025-02-13 20:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10][root][INFO] - Training Epoch: 2/2, step 1801/7134 completed (loss: 0.21303848922252655, acc: 0.9369369149208069)
[2025-02-13 20:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10][root][INFO] - Training Epoch: 2/2, step 1802/7134 completed (loss: 0.17343033850193024, acc: 0.9521276354789734)
[2025-02-13 20:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10][root][INFO] - Training Epoch: 2/2, step 1803/7134 completed (loss: 0.21578004956245422, acc: 0.9451219439506531)
[2025-02-13 20:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11][root][INFO] - Training Epoch: 2/2, step 1804/7134 completed (loss: 0.0920172929763794, acc: 0.9788359999656677)
[2025-02-13 20:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11][root][INFO] - Training Epoch: 2/2, step 1805/7134 completed (loss: 0.16100408136844635, acc: 0.9532163739204407)
[2025-02-13 20:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11][root][INFO] - Training Epoch: 2/2, step 1806/7134 completed (loss: 0.15845143795013428, acc: 0.9562841653823853)
[2025-02-13 20:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:12][root][INFO] - Training Epoch: 2/2, step 1807/7134 completed (loss: 0.1780662089586258, acc: 0.9457831382751465)
[2025-02-13 20:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:12][root][INFO] - Training Epoch: 2/2, step 1808/7134 completed (loss: 0.08979115635156631, acc: 0.9774011373519897)
[2025-02-13 20:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13][root][INFO] - Training Epoch: 2/2, step 1809/7134 completed (loss: 0.07883104681968689, acc: 0.9826589822769165)
[2025-02-13 20:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13][root][INFO] - Training Epoch: 2/2, step 1810/7134 completed (loss: 0.08028978109359741, acc: 0.9890109896659851)
[2025-02-13 20:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13][root][INFO] - Training Epoch: 2/2, step 1811/7134 completed (loss: 0.13497115671634674, acc: 0.9680851101875305)
[2025-02-13 20:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14][root][INFO] - Training Epoch: 2/2, step 1812/7134 completed (loss: 0.1018727645277977, acc: 0.9717513918876648)
[2025-02-13 20:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14][root][INFO] - Training Epoch: 2/2, step 1813/7134 completed (loss: 0.1273394227027893, acc: 0.9779005646705627)
[2025-02-13 20:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14][root][INFO] - Training Epoch: 2/2, step 1814/7134 completed (loss: 0.09807019680738449, acc: 0.9779005646705627)
[2025-02-13 20:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15][root][INFO] - Training Epoch: 2/2, step 1815/7134 completed (loss: 0.12204147130250931, acc: 0.9745222926139832)
[2025-02-13 20:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15][root][INFO] - Training Epoch: 2/2, step 1816/7134 completed (loss: 0.2176761031150818, acc: 0.9236111044883728)
[2025-02-13 20:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15][root][INFO] - Training Epoch: 2/2, step 1817/7134 completed (loss: 0.09256912022829056, acc: 0.96875)
[2025-02-13 20:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:16][root][INFO] - Training Epoch: 2/2, step 1818/7134 completed (loss: 0.11022277176380157, acc: 0.9790576100349426)
[2025-02-13 20:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:16][root][INFO] - Training Epoch: 2/2, step 1819/7134 completed (loss: 0.06614375114440918, acc: 0.9767441749572754)
[2025-02-13 20:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17][root][INFO] - Training Epoch: 2/2, step 1820/7134 completed (loss: 0.07510208338499069, acc: 0.9764150977134705)
[2025-02-13 20:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17][root][INFO] - Training Epoch: 2/2, step 1821/7134 completed (loss: 0.09132738411426544, acc: 0.9813084006309509)
[2025-02-13 20:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17][root][INFO] - Training Epoch: 2/2, step 1822/7134 completed (loss: 0.06129544600844383, acc: 0.9868420958518982)
[2025-02-13 20:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18][root][INFO] - Training Epoch: 2/2, step 1823/7134 completed (loss: 0.10454093664884567, acc: 0.9651162624359131)
[2025-02-13 20:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18][root][INFO] - Training Epoch: 2/2, step 1824/7134 completed (loss: 0.07715509086847305, acc: 0.9781420826911926)
[2025-02-13 20:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18][root][INFO] - Training Epoch: 2/2, step 1825/7134 completed (loss: 0.07750013470649719, acc: 0.9766355156898499)
[2025-02-13 20:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19][root][INFO] - Training Epoch: 2/2, step 1826/7134 completed (loss: 0.0545983724296093, acc: 0.988304078578949)
[2025-02-13 20:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19][root][INFO] - Training Epoch: 2/2, step 1827/7134 completed (loss: 0.10254906862974167, acc: 0.9810426831245422)
[2025-02-13 20:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20][root][INFO] - Training Epoch: 2/2, step 1828/7134 completed (loss: 0.062178775668144226, acc: 0.9833333492279053)
[2025-02-13 20:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20][root][INFO] - Training Epoch: 2/2, step 1829/7134 completed (loss: 0.06296277046203613, acc: 0.9838709831237793)
[2025-02-13 20:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20][root][INFO] - Training Epoch: 2/2, step 1830/7134 completed (loss: 0.07637689262628555, acc: 0.984455943107605)
[2025-02-13 20:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21][root][INFO] - Training Epoch: 2/2, step 1831/7134 completed (loss: 0.03387909382581711, acc: 1.0)
[2025-02-13 20:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21][root][INFO] - Training Epoch: 2/2, step 1832/7134 completed (loss: 0.18036748468875885, acc: 0.9534883499145508)
[2025-02-13 20:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21][root][INFO] - Training Epoch: 2/2, step 1833/7134 completed (loss: 0.4288063645362854, acc: 0.9024389982223511)
[2025-02-13 20:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22][root][INFO] - Training Epoch: 2/2, step 1834/7134 completed (loss: 0.04205871373414993, acc: 0.9871794581413269)
[2025-02-13 20:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22][root][INFO] - Training Epoch: 2/2, step 1835/7134 completed (loss: 0.168730691075325, acc: 0.9610389471054077)
[2025-02-13 20:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22][root][INFO] - Training Epoch: 2/2, step 1836/7134 completed (loss: 0.11219076812267303, acc: 0.9679144620895386)
[2025-02-13 20:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:23][root][INFO] - Training Epoch: 2/2, step 1837/7134 completed (loss: 0.12153021991252899, acc: 0.9780219793319702)
[2025-02-13 20:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:23][root][INFO] - Training Epoch: 2/2, step 1838/7134 completed (loss: 0.07318843901157379, acc: 0.9724770784378052)
[2025-02-13 20:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24][root][INFO] - Training Epoch: 2/2, step 1839/7134 completed (loss: 0.07463668286800385, acc: 0.9890109896659851)
[2025-02-13 20:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24][root][INFO] - Training Epoch: 2/2, step 1840/7134 completed (loss: 0.1460610330104828, acc: 0.9620253443717957)
[2025-02-13 20:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24][root][INFO] - Training Epoch: 2/2, step 1841/7134 completed (loss: 0.06993923336267471, acc: 0.9818181991577148)
[2025-02-13 20:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25][root][INFO] - Training Epoch: 2/2, step 1842/7134 completed (loss: 0.14139443635940552, acc: 0.9634146094322205)
[2025-02-13 20:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25][root][INFO] - Training Epoch: 2/2, step 1843/7134 completed (loss: 0.17071644961833954, acc: 0.9702380895614624)
[2025-02-13 20:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25][root][INFO] - Training Epoch: 2/2, step 1844/7134 completed (loss: 0.06406334042549133, acc: 0.978723406791687)
[2025-02-13 20:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26][root][INFO] - Training Epoch: 2/2, step 1845/7134 completed (loss: 0.049414120614528656, acc: 0.989130437374115)
[2025-02-13 20:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26][root][INFO] - Training Epoch: 2/2, step 1846/7134 completed (loss: 0.1313873529434204, acc: 0.9743589758872986)
[2025-02-13 20:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26][root][INFO] - Training Epoch: 2/2, step 1847/7134 completed (loss: 0.1335427314043045, acc: 0.9545454382896423)
[2025-02-13 20:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:27][root][INFO] - Training Epoch: 2/2, step 1848/7134 completed (loss: 0.2217937409877777, acc: 0.9414893388748169)
[2025-02-13 20:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:27][root][INFO] - Training Epoch: 2/2, step 1849/7134 completed (loss: 0.09136262536048889, acc: 0.9647887349128723)
[2025-02-13 20:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28][root][INFO] - Training Epoch: 2/2, step 1850/7134 completed (loss: 0.20976904034614563, acc: 0.9432989954948425)
[2025-02-13 20:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28][root][INFO] - Training Epoch: 2/2, step 1851/7134 completed (loss: 0.07392095774412155, acc: 0.9846153855323792)
[2025-02-13 20:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28][root][INFO] - Training Epoch: 2/2, step 1852/7134 completed (loss: 0.17831771075725555, acc: 0.925000011920929)
[2025-02-13 20:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29][root][INFO] - Training Epoch: 2/2, step 1853/7134 completed (loss: 0.16307571530342102, acc: 0.9547738432884216)
[2025-02-13 20:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29][root][INFO] - Training Epoch: 2/2, step 1854/7134 completed (loss: 0.15003487467765808, acc: 0.9638009071350098)
[2025-02-13 20:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29][root][INFO] - Training Epoch: 2/2, step 1855/7134 completed (loss: 0.10327684134244919, acc: 0.9810426831245422)
[2025-02-13 20:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30][root][INFO] - Training Epoch: 2/2, step 1856/7134 completed (loss: 0.1294029951095581, acc: 0.9591836929321289)
[2025-02-13 20:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30][root][INFO] - Training Epoch: 2/2, step 1857/7134 completed (loss: 0.29497942328453064, acc: 0.9264705777168274)
[2025-02-13 20:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30][root][INFO] - Training Epoch: 2/2, step 1858/7134 completed (loss: 0.12535898387432098, acc: 0.9642857313156128)
[2025-02-13 20:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:31][root][INFO] - Training Epoch: 2/2, step 1859/7134 completed (loss: 0.07988261431455612, acc: 0.980861246585846)
[2025-02-13 20:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:31][root][INFO] - Training Epoch: 2/2, step 1860/7134 completed (loss: 0.12338920682668686, acc: 0.9726775884628296)
[2025-02-13 20:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32][root][INFO] - Training Epoch: 2/2, step 1861/7134 completed (loss: 0.13311795890331268, acc: 0.9790576100349426)
[2025-02-13 20:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32][root][INFO] - Training Epoch: 2/2, step 1862/7134 completed (loss: 0.04994070902466774, acc: 0.9841269850730896)
[2025-02-13 20:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32][root][INFO] - Training Epoch: 2/2, step 1863/7134 completed (loss: 0.10013818740844727, acc: 0.9753694534301758)
[2025-02-13 20:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33][root][INFO] - Training Epoch: 2/2, step 1864/7134 completed (loss: 0.33876514434814453, acc: 0.903553307056427)
[2025-02-13 20:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33][root][INFO] - Training Epoch: 2/2, step 1865/7134 completed (loss: 0.15999779105186462, acc: 0.9731543660163879)
[2025-02-13 20:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33][root][INFO] - Training Epoch: 2/2, step 1866/7134 completed (loss: 0.05066157504916191, acc: 0.9882352948188782)
[2025-02-13 20:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34][root][INFO] - Training Epoch: 2/2, step 1867/7134 completed (loss: 0.052462127059698105, acc: 0.9886363744735718)
[2025-02-13 20:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34][root][INFO] - Training Epoch: 2/2, step 1868/7134 completed (loss: 0.24182501435279846, acc: 0.9447852969169617)
[2025-02-13 20:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34][root][INFO] - Training Epoch: 2/2, step 1869/7134 completed (loss: 0.08932619541883469, acc: 0.9794520735740662)
[2025-02-13 20:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35][root][INFO] - Training Epoch: 2/2, step 1870/7134 completed (loss: 0.0704478994011879, acc: 0.9675675630569458)
[2025-02-13 20:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35][root][INFO] - Training Epoch: 2/2, step 1871/7134 completed (loss: 0.26002034544944763, acc: 0.9612902998924255)
[2025-02-13 20:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36][root][INFO] - Training Epoch: 2/2, step 1872/7134 completed (loss: 1.225756287574768, acc: 0.7246376872062683)
[2025-02-13 20:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36][root][INFO] - Training Epoch: 2/2, step 1873/7134 completed (loss: 0.5129888653755188, acc: 0.8677685856819153)
[2025-02-13 20:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36][root][INFO] - Training Epoch: 2/2, step 1874/7134 completed (loss: 0.10456806421279907, acc: 0.977011501789093)
[2025-02-13 20:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37][root][INFO] - Training Epoch: 2/2, step 1875/7134 completed (loss: 0.08825763314962387, acc: 0.9732142686843872)
[2025-02-13 20:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37][root][INFO] - Training Epoch: 2/2, step 1876/7134 completed (loss: 0.3054509460926056, acc: 0.9257143139839172)
[2025-02-13 20:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38][root][INFO] - Training Epoch: 2/2, step 1877/7134 completed (loss: 0.09225017577409744, acc: 0.9830508232116699)
[2025-02-13 20:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38][root][INFO] - Training Epoch: 2/2, step 1878/7134 completed (loss: 0.10324069857597351, acc: 0.9673202633857727)
[2025-02-13 20:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38][root][INFO] - Training Epoch: 2/2, step 1879/7134 completed (loss: 0.47040069103240967, acc: 0.8888888955116272)
[2025-02-13 20:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39][root][INFO] - Training Epoch: 2/2, step 1880/7134 completed (loss: 0.22413787245750427, acc: 0.9593495726585388)
[2025-02-13 20:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39][root][INFO] - Training Epoch: 2/2, step 1881/7134 completed (loss: 0.15645889937877655, acc: 0.9577465057373047)
[2025-02-13 20:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39][root][INFO] - Training Epoch: 2/2, step 1882/7134 completed (loss: 0.15131238102912903, acc: 0.967391312122345)
[2025-02-13 20:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:40][root][INFO] - Training Epoch: 2/2, step 1883/7134 completed (loss: 0.13163937628269196, acc: 0.961240291595459)
[2025-02-13 20:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:40][root][INFO] - Training Epoch: 2/2, step 1884/7134 completed (loss: 0.16795554757118225, acc: 0.9594594836235046)
[2025-02-13 20:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41][root][INFO] - Training Epoch: 2/2, step 1885/7134 completed (loss: 0.29678821563720703, acc: 0.9014084339141846)
[2025-02-13 20:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41][root][INFO] - Training Epoch: 2/2, step 1886/7134 completed (loss: 0.3019702136516571, acc: 0.9444444179534912)
[2025-02-13 20:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41][root][INFO] - Training Epoch: 2/2, step 1887/7134 completed (loss: 0.17395754158496857, acc: 0.9536423683166504)
[2025-02-13 20:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:42][root][INFO] - Training Epoch: 2/2, step 1888/7134 completed (loss: 0.10071544349193573, acc: 0.9829059839248657)
[2025-02-13 20:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:42][root][INFO] - Training Epoch: 2/2, step 1889/7134 completed (loss: 0.0980871245265007, acc: 0.9590643048286438)
[2025-02-13 20:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43][root][INFO] - Training Epoch: 2/2, step 1890/7134 completed (loss: 0.05259091407060623, acc: 0.9801324605941772)
[2025-02-13 20:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43][root][INFO] - Training Epoch: 2/2, step 1891/7134 completed (loss: 0.19319970905780792, acc: 0.9709302186965942)
[2025-02-13 20:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43][root][INFO] - Training Epoch: 2/2, step 1892/7134 completed (loss: 0.13128581643104553, acc: 0.9820359349250793)
[2025-02-13 20:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44][root][INFO] - Training Epoch: 2/2, step 1893/7134 completed (loss: 0.14064404368400574, acc: 0.9444444179534912)
[2025-02-13 20:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44][root][INFO] - Training Epoch: 2/2, step 1894/7134 completed (loss: 0.17043127119541168, acc: 0.9662162065505981)
[2025-02-13 20:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44][root][INFO] - Training Epoch: 2/2, step 1895/7134 completed (loss: 0.5947397947311401, acc: 0.8314606547355652)
[2025-02-13 20:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45][root][INFO] - Training Epoch: 2/2, step 1896/7134 completed (loss: 0.27976876497268677, acc: 0.9090909361839294)
[2025-02-13 20:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45][root][INFO] - Training Epoch: 2/2, step 1897/7134 completed (loss: 0.12450242042541504, acc: 0.9647887349128723)
[2025-02-13 20:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46][root][INFO] - Training Epoch: 2/2, step 1898/7134 completed (loss: 0.0833907499909401, acc: 0.9586206674575806)
[2025-02-13 20:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46][root][INFO] - Training Epoch: 2/2, step 1899/7134 completed (loss: 0.07381630688905716, acc: 0.9848484992980957)
[2025-02-13 20:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46][root][INFO] - Training Epoch: 2/2, step 1900/7134 completed (loss: 0.06155337393283844, acc: 1.0)
[2025-02-13 20:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47][root][INFO] - Training Epoch: 2/2, step 1901/7134 completed (loss: 0.16244164109230042, acc: 0.9784172773361206)
[2025-02-13 20:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47][root][INFO] - Training Epoch: 2/2, step 1902/7134 completed (loss: 0.07422647625207901, acc: 0.9784172773361206)
[2025-02-13 20:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47][root][INFO] - Training Epoch: 2/2, step 1903/7134 completed (loss: 0.06734132766723633, acc: 0.9803921580314636)
[2025-02-13 20:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48][root][INFO] - Training Epoch: 2/2, step 1904/7134 completed (loss: 0.08161178976297379, acc: 0.9904761910438538)
[2025-02-13 20:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48][root][INFO] - Training Epoch: 2/2, step 1905/7134 completed (loss: 0.12130596488714218, acc: 0.978723406791687)
[2025-02-13 20:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49][root][INFO] - Training Epoch: 2/2, step 1906/7134 completed (loss: 0.10795316100120544, acc: 0.985401451587677)
[2025-02-13 20:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49][root][INFO] - Training Epoch: 2/2, step 1907/7134 completed (loss: 0.16812366247177124, acc: 0.9523809552192688)
[2025-02-13 20:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49][root][INFO] - Training Epoch: 2/2, step 1908/7134 completed (loss: 0.107975535094738, acc: 0.9719626307487488)
[2025-02-13 20:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50][root][INFO] - Training Epoch: 2/2, step 1909/7134 completed (loss: 0.1463344693183899, acc: 0.9387755393981934)
[2025-02-13 20:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50][root][INFO] - Training Epoch: 2/2, step 1910/7134 completed (loss: 0.10452061146497726, acc: 0.9627906680107117)
[2025-02-13 20:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50][root][INFO] - Training Epoch: 2/2, step 1911/7134 completed (loss: 0.07891132682561874, acc: 0.9847715497016907)
[2025-02-13 20:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:51][root][INFO] - Training Epoch: 2/2, step 1912/7134 completed (loss: 0.033680833876132965, acc: 0.9818181991577148)
[2025-02-13 20:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:51][root][INFO] - Training Epoch: 2/2, step 1913/7134 completed (loss: 0.11672613769769669, acc: 0.9763779640197754)
[2025-02-13 20:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52][root][INFO] - Training Epoch: 2/2, step 1914/7134 completed (loss: 0.15890781581401825, acc: 0.9508196711540222)
[2025-02-13 20:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52][root][INFO] - Training Epoch: 2/2, step 1915/7134 completed (loss: 0.07916862517595291, acc: 0.9860140085220337)
[2025-02-13 20:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52][root][INFO] - Training Epoch: 2/2, step 1916/7134 completed (loss: 0.061209216713905334, acc: 0.9835164546966553)
[2025-02-13 20:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53][root][INFO] - Training Epoch: 2/2, step 1917/7134 completed (loss: 0.11668724566698074, acc: 0.9818181991577148)
[2025-02-13 20:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53][root][INFO] - Training Epoch: 2/2, step 1918/7134 completed (loss: 0.11637633293867111, acc: 0.9801324605941772)
[2025-02-13 20:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53][root][INFO] - Training Epoch: 2/2, step 1919/7134 completed (loss: 0.0735335722565651, acc: 0.9878048896789551)
[2025-02-13 20:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:54][root][INFO] - Training Epoch: 2/2, step 1920/7134 completed (loss: 0.10132439434528351, acc: 0.9863945841789246)
[2025-02-13 20:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:54][root][INFO] - Training Epoch: 2/2, step 1921/7134 completed (loss: 0.09842915087938309, acc: 0.9766082167625427)
[2025-02-13 20:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55][root][INFO] - Training Epoch: 2/2, step 1922/7134 completed (loss: 0.026686063036322594, acc: 0.9959016442298889)
[2025-02-13 20:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55][root][INFO] - Training Epoch: 2/2, step 1923/7134 completed (loss: 0.05779210478067398, acc: 0.9790209531784058)
[2025-02-13 20:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55][root][INFO] - Training Epoch: 2/2, step 1924/7134 completed (loss: 0.14935840666294098, acc: 0.953125)
[2025-02-13 20:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:56][root][INFO] - Training Epoch: 2/2, step 1925/7134 completed (loss: 0.16897229850292206, acc: 0.9372385144233704)
[2025-02-13 20:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:56][root][INFO] - Training Epoch: 2/2, step 1926/7134 completed (loss: 0.049689341336488724, acc: 0.9866071343421936)
[2025-02-13 20:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57][root][INFO] - Training Epoch: 2/2, step 1927/7134 completed (loss: 0.12370247393846512, acc: 0.9716981053352356)
[2025-02-13 20:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57][root][INFO] - Training Epoch: 2/2, step 1928/7134 completed (loss: 0.08858736604452133, acc: 0.9702380895614624)
[2025-02-13 20:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57][root][INFO] - Training Epoch: 2/2, step 1929/7134 completed (loss: 0.03956922888755798, acc: 0.994413435459137)
[2025-02-13 20:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58][root][INFO] - Training Epoch: 2/2, step 1930/7134 completed (loss: 0.29160526394844055, acc: 0.9130434989929199)
[2025-02-13 20:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58][root][INFO] - Training Epoch: 2/2, step 1931/7134 completed (loss: 0.23748953640460968, acc: 0.9490740895271301)
[2025-02-13 20:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58][root][INFO] - Training Epoch: 2/2, step 1932/7134 completed (loss: 0.22194920480251312, acc: 0.9497206807136536)
[2025-02-13 20:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59][root][INFO] - Training Epoch: 2/2, step 1933/7134 completed (loss: 0.2908783257007599, acc: 0.9346733689308167)
[2025-02-13 20:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59][root][INFO] - Training Epoch: 2/2, step 1934/7134 completed (loss: 0.08596143126487732, acc: 0.9657142758369446)
[2025-02-13 20:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00][root][INFO] - Training Epoch: 2/2, step 1935/7134 completed (loss: 0.30071479082107544, acc: 0.9390243887901306)
[2025-02-13 20:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00][root][INFO] - Training Epoch: 2/2, step 1936/7134 completed (loss: 0.06533374637365341, acc: 0.9738562107086182)
[2025-02-13 20:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00][root][INFO] - Training Epoch: 2/2, step 1937/7134 completed (loss: 0.1386520117521286, acc: 0.9695122241973877)
[2025-02-13 20:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01][root][INFO] - Training Epoch: 2/2, step 1938/7134 completed (loss: 0.3505672514438629, acc: 0.929347813129425)
[2025-02-13 20:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01][root][INFO] - Training Epoch: 2/2, step 1939/7134 completed (loss: 0.06233195215463638, acc: 0.9913793206214905)
[2025-02-13 20:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02][root][INFO] - Training Epoch: 2/2, step 1940/7134 completed (loss: 0.2038959413766861, acc: 0.940119743347168)
[2025-02-13 20:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02][root][INFO] - Training Epoch: 2/2, step 1941/7134 completed (loss: 0.06805303692817688, acc: 0.976331353187561)
[2025-02-13 20:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02][root][INFO] - Training Epoch: 2/2, step 1942/7134 completed (loss: 0.059500839561223984, acc: 0.9814814925193787)
[2025-02-13 20:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03][root][INFO] - Training Epoch: 2/2, step 1943/7134 completed (loss: 0.17468400299549103, acc: 0.9684210419654846)
[2025-02-13 20:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03][root][INFO] - Training Epoch: 2/2, step 1944/7134 completed (loss: 0.06067603826522827, acc: 0.9836065769195557)
[2025-02-13 20:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03][root][INFO] - Training Epoch: 2/2, step 1945/7134 completed (loss: 0.057267479598522186, acc: 0.9919354915618896)
[2025-02-13 20:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04][root][INFO] - Training Epoch: 2/2, step 1946/7134 completed (loss: 0.08536549657583237, acc: 0.9785714149475098)
[2025-02-13 20:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04][root][INFO] - Training Epoch: 2/2, step 1947/7134 completed (loss: 0.1148313656449318, acc: 0.9537572264671326)
[2025-02-13 20:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04][root][INFO] - Training Epoch: 2/2, step 1948/7134 completed (loss: 0.09297840297222137, acc: 0.9769230484962463)
[2025-02-13 20:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05][root][INFO] - Training Epoch: 2/2, step 1949/7134 completed (loss: 0.027090156450867653, acc: 1.0)
[2025-02-13 20:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05][root][INFO] - Training Epoch: 2/2, step 1950/7134 completed (loss: 0.060405027121305466, acc: 0.9855072498321533)
[2025-02-13 20:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06][root][INFO] - Training Epoch: 2/2, step 1951/7134 completed (loss: 0.11439275741577148, acc: 0.969924807548523)
[2025-02-13 20:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06][root][INFO] - Training Epoch: 2/2, step 1952/7134 completed (loss: 0.08966665714979172, acc: 0.9759036302566528)
[2025-02-13 20:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06][root][INFO] - Training Epoch: 2/2, step 1953/7134 completed (loss: 0.04554009437561035, acc: 0.9873417615890503)
[2025-02-13 20:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07][root][INFO] - Training Epoch: 2/2, step 1954/7134 completed (loss: 0.06857583671808243, acc: 0.9803921580314636)
[2025-02-13 20:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07][root][INFO] - Training Epoch: 2/2, step 1955/7134 completed (loss: 0.021027380600571632, acc: 1.0)
[2025-02-13 20:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07][root][INFO] - Training Epoch: 2/2, step 1956/7134 completed (loss: 0.025827426463365555, acc: 0.989130437374115)
[2025-02-13 20:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08][root][INFO] - Training Epoch: 2/2, step 1957/7134 completed (loss: 0.08501958101987839, acc: 0.9849624037742615)
[2025-02-13 20:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08][root][INFO] - Training Epoch: 2/2, step 1958/7134 completed (loss: 0.0630260705947876, acc: 0.9888888597488403)
[2025-02-13 20:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09][root][INFO] - Training Epoch: 2/2, step 1959/7134 completed (loss: 0.1441996544599533, acc: 0.9642857313156128)
[2025-02-13 20:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09][root][INFO] - Training Epoch: 2/2, step 1960/7134 completed (loss: 0.08019410818815231, acc: 0.9833333492279053)
[2025-02-13 20:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09][root][INFO] - Training Epoch: 2/2, step 1961/7134 completed (loss: 0.08405578881502151, acc: 0.9746835231781006)
[2025-02-13 20:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10][root][INFO] - Training Epoch: 2/2, step 1962/7134 completed (loss: 0.03909534588456154, acc: 1.0)
[2025-02-13 20:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10][root][INFO] - Training Epoch: 2/2, step 1963/7134 completed (loss: 0.1593240201473236, acc: 0.9668874144554138)
[2025-02-13 20:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10][root][INFO] - Training Epoch: 2/2, step 1964/7134 completed (loss: 0.15481628477573395, acc: 0.9747474789619446)
[2025-02-13 20:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11][root][INFO] - Training Epoch: 2/2, step 1965/7134 completed (loss: 0.15299241244792938, acc: 0.9550561904907227)
[2025-02-13 20:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11][root][INFO] - Training Epoch: 2/2, step 1966/7134 completed (loss: 0.15787608921527863, acc: 0.9617834687232971)
[2025-02-13 20:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12][root][INFO] - Training Epoch: 2/2, step 1967/7134 completed (loss: 0.12201680988073349, acc: 0.9631578922271729)
[2025-02-13 20:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12][root][INFO] - Training Epoch: 2/2, step 1968/7134 completed (loss: 0.08790004998445511, acc: 0.9680851101875305)
[2025-02-13 20:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12][root][INFO] - Training Epoch: 2/2, step 1969/7134 completed (loss: 0.11085343360900879, acc: 0.9661017060279846)
[2025-02-13 20:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13][root][INFO] - Training Epoch: 2/2, step 1970/7134 completed (loss: 0.09445621073246002, acc: 0.9644970297813416)
[2025-02-13 20:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13][root][INFO] - Training Epoch: 2/2, step 1971/7134 completed (loss: 0.22706584632396698, acc: 0.9424083828926086)
[2025-02-13 20:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13][root][INFO] - Training Epoch: 2/2, step 1972/7134 completed (loss: 0.4053366482257843, acc: 0.8771929740905762)
[2025-02-13 20:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14][root][INFO] - Training Epoch: 2/2, step 1973/7134 completed (loss: 0.5826848149299622, acc: 0.875)
[2025-02-13 20:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14][root][INFO] - Training Epoch: 2/2, step 1974/7134 completed (loss: 0.07806592434644699, acc: 0.9845361113548279)
[2025-02-13 20:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15][root][INFO] - Training Epoch: 2/2, step 1975/7134 completed (loss: 0.04697706922888756, acc: 0.9937499761581421)
[2025-02-13 20:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15][root][INFO] - Training Epoch: 2/2, step 1976/7134 completed (loss: 0.2609497308731079, acc: 0.942307710647583)
[2025-02-13 20:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15][root][INFO] - Training Epoch: 2/2, step 1977/7134 completed (loss: 0.15764769911766052, acc: 0.970059871673584)
[2025-02-13 20:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16][root][INFO] - Training Epoch: 2/2, step 1978/7134 completed (loss: 0.0846719816327095, acc: 0.9883720874786377)
[2025-02-13 20:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16][root][INFO] - Training Epoch: 2/2, step 1979/7134 completed (loss: 0.058846328407526016, acc: 0.9835164546966553)
[2025-02-13 20:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17][root][INFO] - Training Epoch: 2/2, step 1980/7134 completed (loss: 0.10983496904373169, acc: 0.9842932224273682)
[2025-02-13 20:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17][root][INFO] - Training Epoch: 2/2, step 1981/7134 completed (loss: 0.08332031220197678, acc: 0.9890109896659851)
[2025-02-13 20:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17][root][INFO] - Training Epoch: 2/2, step 1982/7134 completed (loss: 0.09119099378585815, acc: 0.9743589758872986)
[2025-02-13 20:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18][root][INFO] - Training Epoch: 2/2, step 1983/7134 completed (loss: 0.09132164716720581, acc: 0.9838709831237793)
[2025-02-13 20:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18][root][INFO] - Training Epoch: 2/2, step 1984/7134 completed (loss: 0.0621686689555645, acc: 0.9767441749572754)
[2025-02-13 20:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18][root][INFO] - Training Epoch: 2/2, step 1985/7134 completed (loss: 0.10311441868543625, acc: 0.9751552939414978)
[2025-02-13 20:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19][root][INFO] - Training Epoch: 2/2, step 1986/7134 completed (loss: 0.03555221110582352, acc: 0.994413435459137)
[2025-02-13 20:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19][root][INFO] - Training Epoch: 2/2, step 1987/7134 completed (loss: 0.09098271280527115, acc: 0.966292142868042)
[2025-02-13 20:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20][root][INFO] - Training Epoch: 2/2, step 1988/7134 completed (loss: 0.07381458580493927, acc: 0.9939393997192383)
[2025-02-13 20:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20][root][INFO] - Training Epoch: 2/2, step 1989/7134 completed (loss: 0.0664009153842926, acc: 0.9726027250289917)
[2025-02-13 20:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20][root][INFO] - Training Epoch: 2/2, step 1990/7134 completed (loss: 0.08147589862346649, acc: 0.9779005646705627)
[2025-02-13 20:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21][root][INFO] - Training Epoch: 2/2, step 1991/7134 completed (loss: 0.07334043085575104, acc: 0.9677419066429138)
[2025-02-13 20:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21][root][INFO] - Training Epoch: 2/2, step 1992/7134 completed (loss: 0.21366479992866516, acc: 0.9347826242446899)
[2025-02-13 20:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22][root][INFO] - Training Epoch: 2/2, step 1993/7134 completed (loss: 0.0959801971912384, acc: 0.9739130139350891)
[2025-02-13 20:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22][root][INFO] - Training Epoch: 2/2, step 1994/7134 completed (loss: 0.041415877640247345, acc: 1.0)
[2025-02-13 20:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22][root][INFO] - Training Epoch: 2/2, step 1995/7134 completed (loss: 0.05902710556983948, acc: 0.9724137783050537)
[2025-02-13 20:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:23][root][INFO] - Training Epoch: 2/2, step 1996/7134 completed (loss: 0.07577797025442123, acc: 0.9781022071838379)
[2025-02-13 20:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:23][root][INFO] - Training Epoch: 2/2, step 1997/7134 completed (loss: 0.11194363981485367, acc: 0.9571428298950195)
[2025-02-13 20:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24][root][INFO] - Training Epoch: 2/2, step 1998/7134 completed (loss: 0.04626701772212982, acc: 0.9929577708244324)
[2025-02-13 20:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24][root][INFO] - Training Epoch: 2/2, step 1999/7134 completed (loss: 0.09775339812040329, acc: 0.9580419659614563)
[2025-02-13 20:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24][root][INFO] - Training Epoch: 2/2, step 2000/7134 completed (loss: 0.08186879754066467, acc: 0.9791666865348816)
[2025-02-13 20:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25][root][INFO] - Training Epoch: 2/2, step 2001/7134 completed (loss: 0.03717294707894325, acc: 0.9931034445762634)
[2025-02-13 20:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25][root][INFO] - Training Epoch: 2/2, step 2002/7134 completed (loss: 0.08164975792169571, acc: 0.9849624037742615)
[2025-02-13 20:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25][root][INFO] - Training Epoch: 2/2, step 2003/7134 completed (loss: 0.022978805005550385, acc: 0.9912280440330505)
[2025-02-13 20:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26][root][INFO] - Training Epoch: 2/2, step 2004/7134 completed (loss: 0.29309341311454773, acc: 0.9344262480735779)
[2025-02-13 20:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26][root][INFO] - Training Epoch: 2/2, step 2005/7134 completed (loss: 0.10743610560894012, acc: 0.9591836929321289)
[2025-02-13 20:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27][root][INFO] - Training Epoch: 2/2, step 2006/7134 completed (loss: 0.03224432095885277, acc: 0.9883720874786377)
[2025-02-13 20:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27][root][INFO] - Training Epoch: 2/2, step 2007/7134 completed (loss: 0.03573278337717056, acc: 0.9831932783126831)
[2025-02-13 20:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27][root][INFO] - Training Epoch: 2/2, step 2008/7134 completed (loss: 0.0737404152750969, acc: 0.9794520735740662)
[2025-02-13 20:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28][root][INFO] - Training Epoch: 2/2, step 2009/7134 completed (loss: 0.0531836561858654, acc: 0.9777777791023254)
[2025-02-13 20:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28][root][INFO] - Training Epoch: 2/2, step 2010/7134 completed (loss: 0.07698100805282593, acc: 0.970802903175354)
[2025-02-13 20:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29][root][INFO] - Training Epoch: 2/2, step 2011/7134 completed (loss: 0.05004284530878067, acc: 0.9841269850730896)
[2025-02-13 20:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29][root][INFO] - Training Epoch: 2/2, step 2012/7134 completed (loss: 0.09533721208572388, acc: 0.9745762944221497)
[2025-02-13 20:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29][root][INFO] - Training Epoch: 2/2, step 2013/7134 completed (loss: 0.07995302230119705, acc: 0.9836065769195557)
[2025-02-13 20:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30][root][INFO] - Training Epoch: 2/2, step 2014/7134 completed (loss: 0.06765351444482803, acc: 0.9876543283462524)
[2025-02-13 20:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30][root][INFO] - Training Epoch: 2/2, step 2015/7134 completed (loss: 0.04921690374612808, acc: 0.987500011920929)
[2025-02-13 20:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30][root][INFO] - Training Epoch: 2/2, step 2016/7134 completed (loss: 0.2291799783706665, acc: 0.9455782175064087)
[2025-02-13 20:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:31][root][INFO] - Training Epoch: 2/2, step 2017/7134 completed (loss: 0.12544068694114685, acc: 0.9536423683166504)
[2025-02-13 20:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:31][root][INFO] - Training Epoch: 2/2, step 2018/7134 completed (loss: 0.03189832717180252, acc: 1.0)
[2025-02-13 20:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32][root][INFO] - Training Epoch: 2/2, step 2019/7134 completed (loss: 0.0649770051240921, acc: 0.9735099077224731)
[2025-02-13 20:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32][root][INFO] - Training Epoch: 2/2, step 2020/7134 completed (loss: 0.06062600761651993, acc: 0.9834710955619812)
[2025-02-13 20:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32][root][INFO] - Training Epoch: 2/2, step 2021/7134 completed (loss: 0.10155188292264938, acc: 0.9621621370315552)
[2025-02-13 20:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33][root][INFO] - Training Epoch: 2/2, step 2022/7134 completed (loss: 0.16746918857097626, acc: 0.9552238583564758)
[2025-02-13 20:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33][root][INFO] - Training Epoch: 2/2, step 2023/7134 completed (loss: 0.12407917529344559, acc: 0.9754601120948792)
[2025-02-13 20:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33][root][INFO] - Training Epoch: 2/2, step 2024/7134 completed (loss: 0.11965454369783401, acc: 0.9767441749572754)
[2025-02-13 20:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:34][root][INFO] - Training Epoch: 2/2, step 2025/7134 completed (loss: 0.12369746714830399, acc: 0.9824561476707458)
[2025-02-13 20:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:34][root][INFO] - Training Epoch: 2/2, step 2026/7134 completed (loss: 0.05412048101425171, acc: 0.9941520690917969)
[2025-02-13 20:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35][root][INFO] - Training Epoch: 2/2, step 2027/7134 completed (loss: 0.05452492833137512, acc: 0.9924812316894531)
[2025-02-13 20:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35][root][INFO] - Training Epoch: 2/2, step 2028/7134 completed (loss: 0.08754585683345795, acc: 0.9693251252174377)
[2025-02-13 20:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35][root][INFO] - Training Epoch: 2/2, step 2029/7134 completed (loss: 0.1123122051358223, acc: 0.970588207244873)
[2025-02-13 20:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36][root][INFO] - Training Epoch: 2/2, step 2030/7134 completed (loss: 0.21231432259082794, acc: 0.9277108311653137)
[2025-02-13 20:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36][root][INFO] - Training Epoch: 2/2, step 2031/7134 completed (loss: 0.043931178748607635, acc: 0.9944751262664795)
[2025-02-13 20:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36][root][INFO] - Training Epoch: 2/2, step 2032/7134 completed (loss: 0.19195270538330078, acc: 0.9473684430122375)
[2025-02-13 20:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37][root][INFO] - Training Epoch: 2/2, step 2033/7134 completed (loss: 0.014815577305853367, acc: 1.0)
[2025-02-13 20:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37][root][INFO] - Training Epoch: 2/2, step 2034/7134 completed (loss: 0.19436179101467133, acc: 0.9328858852386475)
[2025-02-13 20:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38][root][INFO] - Training Epoch: 2/2, step 2035/7134 completed (loss: 0.041955817490816116, acc: 0.9838709831237793)
[2025-02-13 20:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38][root][INFO] - Training Epoch: 2/2, step 2036/7134 completed (loss: 0.07786212116479874, acc: 0.9770992398262024)
[2025-02-13 20:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38][root][INFO] - Training Epoch: 2/2, step 2037/7134 completed (loss: 0.0533176064491272, acc: 0.9754601120948792)
[2025-02-13 20:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39][root][INFO] - Training Epoch: 2/2, step 2038/7134 completed (loss: 0.03998776152729988, acc: 0.9945054650306702)
[2025-02-13 20:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39][root][INFO] - Training Epoch: 2/2, step 2039/7134 completed (loss: 0.02241263911128044, acc: 1.0)
[2025-02-13 20:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39][root][INFO] - Training Epoch: 2/2, step 2040/7134 completed (loss: 0.12938085198402405, acc: 0.9741379022598267)
[2025-02-13 20:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:40][root][INFO] - Training Epoch: 2/2, step 2041/7134 completed (loss: 0.04163240268826485, acc: 0.9878787994384766)
[2025-02-13 20:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:40][root][INFO] - Training Epoch: 2/2, step 2042/7134 completed (loss: 0.08458641171455383, acc: 0.9745762944221497)
[2025-02-13 20:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41][root][INFO] - Training Epoch: 2/2, step 2043/7134 completed (loss: 0.027961529791355133, acc: 0.9932885766029358)
[2025-02-13 20:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41][root][INFO] - Training Epoch: 2/2, step 2044/7134 completed (loss: 0.03994157910346985, acc: 0.9919999837875366)
[2025-02-13 20:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41][root][INFO] - Training Epoch: 2/2, step 2045/7134 completed (loss: 0.08215738087892532, acc: 0.9916666746139526)
[2025-02-13 20:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:42][root][INFO] - Training Epoch: 2/2, step 2046/7134 completed (loss: 0.13595077395439148, acc: 0.9663865566253662)
[2025-02-13 20:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:42][root][INFO] - Training Epoch: 2/2, step 2047/7134 completed (loss: 0.19185984134674072, acc: 0.9448275566101074)
[2025-02-13 20:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43][root][INFO] - Training Epoch: 2/2, step 2048/7134 completed (loss: 0.1950305551290512, acc: 0.9642857313156128)
[2025-02-13 20:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43][root][INFO] - Training Epoch: 2/2, step 2049/7134 completed (loss: 0.19219569861888885, acc: 0.9363636374473572)
[2025-02-13 20:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43][root][INFO] - Training Epoch: 2/2, step 2050/7134 completed (loss: 0.14431166648864746, acc: 0.965753436088562)
[2025-02-13 20:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44][root][INFO] - Training Epoch: 2/2, step 2051/7134 completed (loss: 0.11021436005830765, acc: 0.9646017551422119)
[2025-02-13 20:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44][root][INFO] - Training Epoch: 2/2, step 2052/7134 completed (loss: 0.19232651591300964, acc: 0.9583333134651184)
[2025-02-13 20:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44][root][INFO] - Training Epoch: 2/2, step 2053/7134 completed (loss: 0.2895621657371521, acc: 0.9465649127960205)
[2025-02-13 20:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45][root][INFO] - Training Epoch: 2/2, step 2054/7134 completed (loss: 0.2430313378572464, acc: 0.942307710647583)
[2025-02-13 20:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45][root][INFO] - Training Epoch: 2/2, step 2055/7134 completed (loss: 0.17687098681926727, acc: 0.9520547986030579)
[2025-02-13 20:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46][root][INFO] - Training Epoch: 2/2, step 2056/7134 completed (loss: 0.1943623125553131, acc: 0.9661017060279846)
[2025-02-13 20:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46][root][INFO] - Training Epoch: 2/2, step 2057/7134 completed (loss: 0.1338062286376953, acc: 0.9719101190567017)
[2025-02-13 20:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46][root][INFO] - Training Epoch: 2/2, step 2058/7134 completed (loss: 0.21718262135982513, acc: 0.9595375657081604)
[2025-02-13 20:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47][root][INFO] - Training Epoch: 2/2, step 2059/7134 completed (loss: 0.35782548785209656, acc: 0.936170220375061)
[2025-02-13 20:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47][root][INFO] - Training Epoch: 2/2, step 2060/7134 completed (loss: 0.13025717437267303, acc: 0.9745222926139832)
[2025-02-13 20:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47][root][INFO] - Training Epoch: 2/2, step 2061/7134 completed (loss: 0.167150616645813, acc: 0.9464285969734192)
[2025-02-13 20:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:48][root][INFO] - Training Epoch: 2/2, step 2062/7134 completed (loss: 0.17352734506130219, acc: 0.953125)
[2025-02-13 20:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:48][root][INFO] - Training Epoch: 2/2, step 2063/7134 completed (loss: 0.0952862948179245, acc: 0.9716312289237976)
[2025-02-13 20:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49][root][INFO] - Training Epoch: 2/2, step 2064/7134 completed (loss: 0.11317531019449234, acc: 0.970588207244873)
[2025-02-13 20:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49][root][INFO] - Training Epoch: 2/2, step 2065/7134 completed (loss: 0.12399609386920929, acc: 0.9714285731315613)
[2025-02-13 20:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49][root][INFO] - Training Epoch: 2/2, step 2066/7134 completed (loss: 0.08908108621835709, acc: 0.9756097793579102)
[2025-02-13 20:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:50][root][INFO] - Training Epoch: 2/2, step 2067/7134 completed (loss: 0.12122034281492233, acc: 0.9583333134651184)
[2025-02-13 20:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:50][root][INFO] - Training Epoch: 2/2, step 2068/7134 completed (loss: 0.08879449218511581, acc: 0.970588207244873)
[2025-02-13 20:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51][root][INFO] - Training Epoch: 2/2, step 2069/7134 completed (loss: 0.06768150627613068, acc: 0.9731543660163879)
[2025-02-13 20:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51][root][INFO] - Training Epoch: 2/2, step 2070/7134 completed (loss: 0.03451148048043251, acc: 1.0)
[2025-02-13 20:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51][root][INFO] - Training Epoch: 2/2, step 2071/7134 completed (loss: 0.01727174036204815, acc: 1.0)
[2025-02-13 20:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52][root][INFO] - Training Epoch: 2/2, step 2072/7134 completed (loss: 0.06573647260665894, acc: 0.9803921580314636)
[2025-02-13 20:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52][root][INFO] - Training Epoch: 2/2, step 2073/7134 completed (loss: 0.044456373900175095, acc: 0.9917355179786682)
[2025-02-13 20:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52][root][INFO] - Training Epoch: 2/2, step 2074/7134 completed (loss: 0.11980472505092621, acc: 0.9738219976425171)
[2025-02-13 20:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:53][root][INFO] - Training Epoch: 2/2, step 2075/7134 completed (loss: 0.12026539444923401, acc: 0.955974817276001)
[2025-02-13 20:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:53][root][INFO] - Training Epoch: 2/2, step 2076/7134 completed (loss: 0.08042513579130173, acc: 0.9863945841789246)
[2025-02-13 20:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54][root][INFO] - Training Epoch: 2/2, step 2077/7134 completed (loss: 0.07432994991540909, acc: 0.9830508232116699)
[2025-02-13 20:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54][root][INFO] - Training Epoch: 2/2, step 2078/7134 completed (loss: 0.06476149708032608, acc: 0.989847719669342)
[2025-02-13 20:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54][root][INFO] - Training Epoch: 2/2, step 2079/7134 completed (loss: 0.0571429468691349, acc: 0.9879518151283264)
[2025-02-13 20:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55][root][INFO] - Training Epoch: 2/2, step 2080/7134 completed (loss: 0.09622947871685028, acc: 0.9803921580314636)
[2025-02-13 20:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55][root][INFO] - Training Epoch: 2/2, step 2081/7134 completed (loss: 0.13658718764781952, acc: 0.987261176109314)
[2025-02-13 20:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55][root][INFO] - Training Epoch: 2/2, step 2082/7134 completed (loss: 0.14152681827545166, acc: 0.9491525292396545)
[2025-02-13 20:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56][root][INFO] - Training Epoch: 2/2, step 2083/7134 completed (loss: 0.11118229478597641, acc: 0.9844961166381836)
[2025-02-13 20:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56][root][INFO] - Training Epoch: 2/2, step 2084/7134 completed (loss: 0.08680947124958038, acc: 0.9800000190734863)
[2025-02-13 20:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57][root][INFO] - Training Epoch: 2/2, step 2085/7134 completed (loss: 0.021604018285870552, acc: 0.9918699264526367)
[2025-02-13 20:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57][root][INFO] - Training Epoch: 2/2, step 2086/7134 completed (loss: 0.04340027645230293, acc: 0.9901960492134094)
[2025-02-13 20:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57][root][INFO] - Training Epoch: 2/2, step 2087/7134 completed (loss: 0.02714763954281807, acc: 0.9940476417541504)
[2025-02-13 20:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58][root][INFO] - Training Epoch: 2/2, step 2088/7134 completed (loss: 0.1816859394311905, acc: 0.9644970297813416)
[2025-02-13 20:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58][root][INFO] - Training Epoch: 2/2, step 2089/7134 completed (loss: 0.12380111962556839, acc: 0.976190447807312)
[2025-02-13 20:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58][root][INFO] - Training Epoch: 2/2, step 2090/7134 completed (loss: 0.04106791317462921, acc: 0.9869281053543091)
[2025-02-13 20:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59][root][INFO] - Training Epoch: 2/2, step 2091/7134 completed (loss: 0.03692009299993515, acc: 0.9862068891525269)
[2025-02-13 20:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59][root][INFO] - Training Epoch: 2/2, step 2092/7134 completed (loss: 0.028587501496076584, acc: 0.9933775067329407)
[2025-02-13 20:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00][root][INFO] - Training Epoch: 2/2, step 2093/7134 completed (loss: 0.06940431147813797, acc: 0.9789473414421082)
[2025-02-13 20:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00][root][INFO] - Training Epoch: 2/2, step 2094/7134 completed (loss: 0.09965475648641586, acc: 0.9701492786407471)
[2025-02-13 20:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00][root][INFO] - Training Epoch: 2/2, step 2095/7134 completed (loss: 0.09641778469085693, acc: 0.9701492786407471)
[2025-02-13 20:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01][root][INFO] - Training Epoch: 2/2, step 2096/7134 completed (loss: 0.04836583882570267, acc: 0.9860140085220337)
[2025-02-13 20:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01][root][INFO] - Training Epoch: 2/2, step 2097/7134 completed (loss: 0.07989255338907242, acc: 0.9819276928901672)
[2025-02-13 20:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01][root][INFO] - Training Epoch: 2/2, step 2098/7134 completed (loss: 0.04177040234208107, acc: 0.9868420958518982)
[2025-02-13 20:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02][root][INFO] - Training Epoch: 2/2, step 2099/7134 completed (loss: 0.04688216745853424, acc: 0.9765625)
[2025-02-13 20:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02][root][INFO] - Training Epoch: 2/2, step 2100/7134 completed (loss: 0.060569360852241516, acc: 0.977142870426178)
[2025-02-13 20:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03][root][INFO] - Training Epoch: 2/2, step 2101/7134 completed (loss: 0.05485326796770096, acc: 0.9940476417541504)
[2025-02-13 20:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03][root][INFO] - Training Epoch: 2/2, step 2102/7134 completed (loss: 0.0650172010064125, acc: 0.9837837815284729)
[2025-02-13 20:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03][root][INFO] - Training Epoch: 2/2, step 2103/7134 completed (loss: 0.10125476121902466, acc: 0.987500011920929)
[2025-02-13 20:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04][root][INFO] - Training Epoch: 2/2, step 2104/7134 completed (loss: 0.024861155077815056, acc: 1.0)
[2025-02-13 20:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04][root][INFO] - Training Epoch: 2/2, step 2105/7134 completed (loss: 0.06361650675535202, acc: 0.9858155846595764)
[2025-02-13 20:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04][root][INFO] - Training Epoch: 2/2, step 2106/7134 completed (loss: 0.06520902365446091, acc: 0.9802631735801697)
[2025-02-13 20:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:05][root][INFO] - Training Epoch: 2/2, step 2107/7134 completed (loss: 0.06810179352760315, acc: 0.9779005646705627)
[2025-02-13 20:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:05][root][INFO] - Training Epoch: 2/2, step 2108/7134 completed (loss: 0.062388189136981964, acc: 0.9751552939414978)
[2025-02-13 20:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06][root][INFO] - Training Epoch: 2/2, step 2109/7134 completed (loss: 0.09610016644001007, acc: 0.97826087474823)
[2025-02-13 20:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06][root][INFO] - Training Epoch: 2/2, step 2110/7134 completed (loss: 0.08996519446372986, acc: 0.970059871673584)
[2025-02-13 20:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06][root][INFO] - Training Epoch: 2/2, step 2111/7134 completed (loss: 0.18828783929347992, acc: 0.9637305736541748)
[2025-02-13 20:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:07][root][INFO] - Training Epoch: 2/2, step 2112/7134 completed (loss: 0.06604819744825363, acc: 0.9733333587646484)
[2025-02-13 20:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:07][root][INFO] - Training Epoch: 2/2, step 2113/7134 completed (loss: 0.06727339327335358, acc: 0.9866666793823242)
[2025-02-13 20:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08][root][INFO] - Training Epoch: 2/2, step 2114/7134 completed (loss: 0.10301313549280167, acc: 0.976331353187561)
[2025-02-13 20:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08][root][INFO] - Training Epoch: 2/2, step 2115/7134 completed (loss: 0.09489147365093231, acc: 0.9712643623352051)
[2025-02-13 20:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08][root][INFO] - Training Epoch: 2/2, step 2116/7134 completed (loss: 0.08202628046274185, acc: 0.9727891087532043)
[2025-02-13 20:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09][root][INFO] - Training Epoch: 2/2, step 2117/7134 completed (loss: 0.10203515738248825, acc: 0.9717513918876648)
[2025-02-13 20:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09][root][INFO] - Training Epoch: 2/2, step 2118/7134 completed (loss: 0.1057540625333786, acc: 0.9585798978805542)
[2025-02-13 20:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09][root][INFO] - Training Epoch: 2/2, step 2119/7134 completed (loss: 0.06443415582180023, acc: 0.9826589822769165)
[2025-02-13 20:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10][root][INFO] - Training Epoch: 2/2, step 2120/7134 completed (loss: 0.07517867535352707, acc: 0.9753086566925049)
[2025-02-13 20:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10][root][INFO] - Training Epoch: 2/2, step 2121/7134 completed (loss: 0.16725392639636993, acc: 0.9731543660163879)
[2025-02-13 20:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11][root][INFO] - Training Epoch: 2/2, step 2122/7134 completed (loss: 0.07618676871061325, acc: 0.9774011373519897)
[2025-02-13 20:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11][root][INFO] - Training Epoch: 2/2, step 2123/7134 completed (loss: 0.16187232732772827, acc: 0.9802631735801697)
[2025-02-13 20:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11][root][INFO] - Training Epoch: 2/2, step 2124/7134 completed (loss: 0.05034255608916283, acc: 0.9864864945411682)
[2025-02-13 20:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12][root][INFO] - Training Epoch: 2/2, step 2125/7134 completed (loss: 0.07133429497480392, acc: 0.9820359349250793)
[2025-02-13 20:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12][root][INFO] - Training Epoch: 2/2, step 2126/7134 completed (loss: 0.08352271467447281, acc: 0.9858155846595764)
[2025-02-13 20:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12][root][INFO] - Training Epoch: 2/2, step 2127/7134 completed (loss: 0.03957681357860565, acc: 0.9879518151283264)
[2025-02-13 20:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13][root][INFO] - Training Epoch: 2/2, step 2128/7134 completed (loss: 0.031593576073646545, acc: 1.0)
[2025-02-13 20:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13][root][INFO] - Training Epoch: 2/2, step 2129/7134 completed (loss: 0.08696573227643967, acc: 0.976331353187561)
[2025-02-13 20:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14][root][INFO] - Training Epoch: 2/2, step 2130/7134 completed (loss: 0.08946561813354492, acc: 0.9729729890823364)
[2025-02-13 20:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14][root][INFO] - Training Epoch: 2/2, step 2131/7134 completed (loss: 0.04149569198489189, acc: 0.987730085849762)
[2025-02-13 20:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14][root][INFO] - Training Epoch: 2/2, step 2132/7134 completed (loss: 0.017716359347105026, acc: 0.9939024448394775)
[2025-02-13 20:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15][root][INFO] - Training Epoch: 2/2, step 2133/7134 completed (loss: 0.08983418345451355, acc: 0.9707602262496948)
[2025-02-13 20:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15][root][INFO] - Training Epoch: 2/2, step 2134/7134 completed (loss: 0.07383966445922852, acc: 0.977011501789093)
[2025-02-13 20:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15][root][INFO] - Training Epoch: 2/2, step 2135/7134 completed (loss: 0.03648552671074867, acc: 0.9867549538612366)
[2025-02-13 20:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16][root][INFO] - Training Epoch: 2/2, step 2136/7134 completed (loss: 0.05426521971821785, acc: 0.9945054650306702)
[2025-02-13 20:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16][root][INFO] - Training Epoch: 2/2, step 2137/7134 completed (loss: 0.07712094485759735, acc: 0.9893617033958435)
[2025-02-13 20:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17][root][INFO] - Training Epoch: 2/2, step 2138/7134 completed (loss: 0.06402166187763214, acc: 0.9820359349250793)
[2025-02-13 20:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17][root][INFO] - Training Epoch: 2/2, step 2139/7134 completed (loss: 0.04464925825595856, acc: 0.9883720874786377)
[2025-02-13 20:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17][root][INFO] - Training Epoch: 2/2, step 2140/7134 completed (loss: 0.11668284982442856, acc: 0.9548022747039795)
[2025-02-13 20:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18][root][INFO] - Training Epoch: 2/2, step 2141/7134 completed (loss: 0.06737704575061798, acc: 0.9893617033958435)
[2025-02-13 20:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18][root][INFO] - Training Epoch: 2/2, step 2142/7134 completed (loss: 0.12922130525112152, acc: 0.9774011373519897)
[2025-02-13 20:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18][root][INFO] - Training Epoch: 2/2, step 2143/7134 completed (loss: 0.02600196562707424, acc: 0.9931034445762634)
[2025-02-13 20:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19][root][INFO] - Training Epoch: 2/2, step 2144/7134 completed (loss: 0.05768568813800812, acc: 0.9886363744735718)
[2025-02-13 20:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19][root][INFO] - Training Epoch: 2/2, step 2145/7134 completed (loss: 0.08843820542097092, acc: 0.9941860437393188)
[2025-02-13 20:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20][root][INFO] - Training Epoch: 2/2, step 2146/7134 completed (loss: 0.038648780435323715, acc: 0.9932432174682617)
[2025-02-13 20:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20][root][INFO] - Training Epoch: 2/2, step 2147/7134 completed (loss: 0.11675727367401123, acc: 0.9545454382896423)
[2025-02-13 20:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20][root][INFO] - Training Epoch: 2/2, step 2148/7134 completed (loss: 0.06731554120779037, acc: 0.9772727489471436)
[2025-02-13 20:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21][root][INFO] - Training Epoch: 2/2, step 2149/7134 completed (loss: 0.02844255603849888, acc: 0.9941520690917969)
[2025-02-13 20:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21][root][INFO] - Training Epoch: 2/2, step 2150/7134 completed (loss: 0.06788899004459381, acc: 0.9774011373519897)
[2025-02-13 20:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21][root][INFO] - Training Epoch: 2/2, step 2151/7134 completed (loss: 0.07377512753009796, acc: 0.9819276928901672)
[2025-02-13 20:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:22][root][INFO] - Training Epoch: 2/2, step 2152/7134 completed (loss: 0.08397446572780609, acc: 0.9806451797485352)
[2025-02-13 20:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:22][root][INFO] - Training Epoch: 2/2, step 2153/7134 completed (loss: 0.07167372107505798, acc: 0.9901960492134094)
[2025-02-13 20:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23][root][INFO] - Training Epoch: 2/2, step 2154/7134 completed (loss: 0.10804656147956848, acc: 0.982758641242981)
[2025-02-13 20:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23][root][INFO] - Training Epoch: 2/2, step 2155/7134 completed (loss: 0.07903604954481125, acc: 0.9871794581413269)
[2025-02-13 20:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23][root][INFO] - Training Epoch: 2/2, step 2156/7134 completed (loss: 0.05339657887816429, acc: 0.9852941036224365)
[2025-02-13 20:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24][root][INFO] - Training Epoch: 2/2, step 2157/7134 completed (loss: 0.1260741651058197, acc: 0.9646464586257935)
[2025-02-13 20:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24][root][INFO] - Training Epoch: 2/2, step 2158/7134 completed (loss: 0.10442465543746948, acc: 0.9845361113548279)
[2025-02-13 20:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24][root][INFO] - Training Epoch: 2/2, step 2159/7134 completed (loss: 0.050853751599788666, acc: 0.9821428656578064)
[2025-02-13 20:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25][root][INFO] - Training Epoch: 2/2, step 2160/7134 completed (loss: 0.07646549493074417, acc: 0.9695122241973877)
[2025-02-13 20:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25][root][INFO] - Training Epoch: 2/2, step 2161/7134 completed (loss: 0.13508526980876923, acc: 0.9555555582046509)
[2025-02-13 20:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26][root][INFO] - Training Epoch: 2/2, step 2162/7134 completed (loss: 0.05558890849351883, acc: 0.9870967864990234)
[2025-02-13 20:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26][root][INFO] - Training Epoch: 2/2, step 2163/7134 completed (loss: 0.07578238099813461, acc: 0.9866666793823242)
[2025-02-13 20:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26][root][INFO] - Training Epoch: 2/2, step 2164/7134 completed (loss: 0.017081378027796745, acc: 1.0)
[2025-02-13 20:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27][root][INFO] - Training Epoch: 2/2, step 2165/7134 completed (loss: 0.08138323575258255, acc: 0.9878048896789551)
[2025-02-13 20:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27][root][INFO] - Training Epoch: 2/2, step 2166/7134 completed (loss: 0.024819042533636093, acc: 0.994350254535675)
[2025-02-13 20:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27][root][INFO] - Training Epoch: 2/2, step 2167/7134 completed (loss: 0.13412991166114807, acc: 0.9708737730979919)
[2025-02-13 20:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:28][root][INFO] - Training Epoch: 2/2, step 2168/7134 completed (loss: 0.038991738110780716, acc: 0.9928057789802551)
[2025-02-13 20:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:28][root][INFO] - Training Epoch: 2/2, step 2169/7134 completed (loss: 0.10867350548505783, acc: 0.9748743772506714)
[2025-02-13 20:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29][root][INFO] - Training Epoch: 2/2, step 2170/7134 completed (loss: 0.04830757528543472, acc: 0.9803921580314636)
[2025-02-13 20:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29][root][INFO] - Training Epoch: 2/2, step 2171/7134 completed (loss: 0.19414059817790985, acc: 0.9621621370315552)
[2025-02-13 20:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29][root][INFO] - Training Epoch: 2/2, step 2172/7134 completed (loss: 0.04035266488790512, acc: 0.9952606558799744)
[2025-02-13 20:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30][root][INFO] - Training Epoch: 2/2, step 2173/7134 completed (loss: 0.09142985194921494, acc: 0.9863013625144958)
[2025-02-13 20:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30][root][INFO] - Training Epoch: 2/2, step 2174/7134 completed (loss: 0.0568355917930603, acc: 0.9880239367485046)
[2025-02-13 20:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31][root][INFO] - Training Epoch: 2/2, step 2175/7134 completed (loss: 0.06542415171861649, acc: 0.9818181991577148)
[2025-02-13 20:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31][root][INFO] - Training Epoch: 2/2, step 2176/7134 completed (loss: 0.05020929500460625, acc: 0.9836956262588501)
[2025-02-13 20:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31][root][INFO] - Training Epoch: 2/2, step 2177/7134 completed (loss: 0.08833152800798416, acc: 0.9781420826911926)
[2025-02-13 20:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32][root][INFO] - Training Epoch: 2/2, step 2178/7134 completed (loss: 0.047468822449445724, acc: 0.984375)
[2025-02-13 20:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32][root][INFO] - Training Epoch: 2/2, step 2179/7134 completed (loss: 0.06420358270406723, acc: 0.9885057210922241)
[2025-02-13 20:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33][root][INFO] - Training Epoch: 2/2, step 2180/7134 completed (loss: 0.11278069764375687, acc: 0.9780219793319702)
[2025-02-13 20:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33][root][INFO] - Training Epoch: 2/2, step 2181/7134 completed (loss: 0.050702936947345734, acc: 0.9875776171684265)
[2025-02-13 20:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33][root][INFO] - Training Epoch: 2/2, step 2182/7134 completed (loss: 0.11993368715047836, acc: 0.9743589758872986)
[2025-02-13 20:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34][root][INFO] - Training Epoch: 2/2, step 2183/7134 completed (loss: 0.07736450433731079, acc: 0.988950252532959)
[2025-02-13 20:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34][root][INFO] - Training Epoch: 2/2, step 2184/7134 completed (loss: 0.04669904336333275, acc: 0.98591548204422)
[2025-02-13 20:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34][root][INFO] - Training Epoch: 2/2, step 2185/7134 completed (loss: 0.13672447204589844, acc: 0.955974817276001)
[2025-02-13 20:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:35][root][INFO] - Training Epoch: 2/2, step 2186/7134 completed (loss: 0.20192456245422363, acc: 0.9599999785423279)
[2025-02-13 20:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:35][root][INFO] - Training Epoch: 2/2, step 2187/7134 completed (loss: 0.14665718376636505, acc: 0.9496402740478516)
[2025-02-13 20:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36][root][INFO] - Training Epoch: 2/2, step 2188/7134 completed (loss: 0.03752652555704117, acc: 0.9939393997192383)
[2025-02-13 20:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36][root][INFO] - Training Epoch: 2/2, step 2189/7134 completed (loss: 0.08688965439796448, acc: 0.9870129823684692)
[2025-02-13 20:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36][root][INFO] - Training Epoch: 2/2, step 2190/7134 completed (loss: 0.1351102739572525, acc: 0.9664804339408875)
[2025-02-13 20:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37][root][INFO] - Training Epoch: 2/2, step 2191/7134 completed (loss: 0.08268409222364426, acc: 0.9578947424888611)
[2025-02-13 20:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37][root][INFO] - Training Epoch: 2/2, step 2192/7134 completed (loss: 0.03143773227930069, acc: 0.9925925731658936)
[2025-02-13 20:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38][root][INFO] - Training Epoch: 2/2, step 2193/7134 completed (loss: 0.1055726557970047, acc: 0.9781420826911926)
[2025-02-13 20:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38][root][INFO] - Training Epoch: 2/2, step 2194/7134 completed (loss: 0.08305700123310089, acc: 0.9671052694320679)
[2025-02-13 20:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38][root][INFO] - Training Epoch: 2/2, step 2195/7134 completed (loss: 0.0182347409427166, acc: 1.0)
[2025-02-13 20:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39][root][INFO] - Training Epoch: 2/2, step 2196/7134 completed (loss: 0.06188216060400009, acc: 0.9780219793319702)
[2025-02-13 20:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39][root][INFO] - Training Epoch: 2/2, step 2197/7134 completed (loss: 0.10397985577583313, acc: 0.960629940032959)
[2025-02-13 20:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39][root][INFO] - Training Epoch: 2/2, step 2198/7134 completed (loss: 0.039322059601545334, acc: 1.0)
[2025-02-13 20:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:40][root][INFO] - Training Epoch: 2/2, step 2199/7134 completed (loss: 0.012952371500432491, acc: 1.0)
[2025-02-13 20:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:40][root][INFO] - Training Epoch: 2/2, step 2200/7134 completed (loss: 0.029678313061594963, acc: 0.9921259880065918)
[2025-02-13 20:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41][root][INFO] - Training Epoch: 2/2, step 2201/7134 completed (loss: 0.027753887698054314, acc: 0.9932432174682617)
[2025-02-13 20:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41][root][INFO] - Training Epoch: 2/2, step 2202/7134 completed (loss: 0.01851562410593033, acc: 1.0)
[2025-02-13 20:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41][root][INFO] - Training Epoch: 2/2, step 2203/7134 completed (loss: 0.024124875664711, acc: 0.9940119981765747)
[2025-02-13 20:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42][root][INFO] - Training Epoch: 2/2, step 2204/7134 completed (loss: 0.06656447798013687, acc: 0.982758641242981)
[2025-02-13 20:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42][root][INFO] - Training Epoch: 2/2, step 2205/7134 completed (loss: 0.016145838424563408, acc: 1.0)
[2025-02-13 20:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42][root][INFO] - Training Epoch: 2/2, step 2206/7134 completed (loss: 0.08862751722335815, acc: 0.9865771532058716)
[2025-02-13 20:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43][root][INFO] - Training Epoch: 2/2, step 2207/7134 completed (loss: 0.022899437695741653, acc: 0.9931972622871399)
[2025-02-13 20:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43][root][INFO] - Training Epoch: 2/2, step 2208/7134 completed (loss: 0.0510161817073822, acc: 0.9935483932495117)
[2025-02-13 20:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44][root][INFO] - Training Epoch: 2/2, step 2209/7134 completed (loss: 0.03290652856230736, acc: 0.9939024448394775)
[2025-02-13 20:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44][root][INFO] - Training Epoch: 2/2, step 2210/7134 completed (loss: 0.04650656506419182, acc: 0.9817073345184326)
[2025-02-13 20:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44][root][INFO] - Training Epoch: 2/2, step 2211/7134 completed (loss: 0.03810344636440277, acc: 0.9939758777618408)
[2025-02-13 20:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45][root][INFO] - Training Epoch: 2/2, step 2212/7134 completed (loss: 0.02011147513985634, acc: 0.9942857027053833)
[2025-02-13 20:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45][root][INFO] - Training Epoch: 2/2, step 2213/7134 completed (loss: 0.13087016344070435, acc: 0.9647058844566345)
[2025-02-13 20:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45][root][INFO] - Training Epoch: 2/2, step 2214/7134 completed (loss: 0.0646766647696495, acc: 0.9864864945411682)
[2025-02-13 20:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46][root][INFO] - Training Epoch: 2/2, step 2215/7134 completed (loss: 0.022238338366150856, acc: 1.0)
[2025-02-13 20:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46][root][INFO] - Training Epoch: 2/2, step 2216/7134 completed (loss: 0.01768137887120247, acc: 1.0)
[2025-02-13 20:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46][root][INFO] - Training Epoch: 2/2, step 2217/7134 completed (loss: 0.044924862682819366, acc: 0.9880239367485046)
[2025-02-13 20:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:47][root][INFO] - Training Epoch: 2/2, step 2218/7134 completed (loss: 0.07259902358055115, acc: 0.9753086566925049)
[2025-02-13 20:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:47][root][INFO] - Training Epoch: 2/2, step 2219/7134 completed (loss: 0.04061251878738403, acc: 0.9928571581840515)
[2025-02-13 20:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:48][root][INFO] - Training Epoch: 2/2, step 2220/7134 completed (loss: 0.03633951395750046, acc: 0.9851852059364319)
[2025-02-13 20:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:48][root][INFO] - Training Epoch: 2/2, step 2221/7134 completed (loss: 0.06063321605324745, acc: 0.9813664555549622)
[2025-02-13 20:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:48][root][INFO] - Training Epoch: 2/2, step 2222/7134 completed (loss: 0.06772693991661072, acc: 0.9726027250289917)
[2025-02-13 20:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49][root][INFO] - Training Epoch: 2/2, step 2223/7134 completed (loss: 0.06175609678030014, acc: 0.9865771532058716)
[2025-02-13 20:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49][root][INFO] - Training Epoch: 2/2, step 2224/7134 completed (loss: 0.14130869507789612, acc: 0.9746835231781006)
[2025-02-13 20:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49][root][INFO] - Training Epoch: 2/2, step 2225/7134 completed (loss: 0.04917251318693161, acc: 0.9931972622871399)
[2025-02-13 20:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50][root][INFO] - Training Epoch: 2/2, step 2226/7134 completed (loss: 0.034537170082330704, acc: 0.9932885766029358)
[2025-02-13 20:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50][root][INFO] - Training Epoch: 2/2, step 2227/7134 completed (loss: 0.02805495820939541, acc: 0.9935064911842346)
[2025-02-13 20:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50][root][INFO] - Training Epoch: 2/2, step 2228/7134 completed (loss: 0.046489227563142776, acc: 0.9922480583190918)
[2025-02-13 20:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:51][root][INFO] - Training Epoch: 2/2, step 2229/7134 completed (loss: 0.09015820920467377, acc: 0.9902912378311157)
[2025-02-13 20:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:51][root][INFO] - Training Epoch: 2/2, step 2230/7134 completed (loss: 0.015382139012217522, acc: 1.0)
[2025-02-13 20:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52][root][INFO] - Training Epoch: 2/2, step 2231/7134 completed (loss: 0.07826364040374756, acc: 0.9915966391563416)
[2025-02-13 20:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52][root][INFO] - Training Epoch: 2/2, step 2232/7134 completed (loss: 0.07698355615139008, acc: 0.977011501789093)
[2025-02-13 20:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52][root][INFO] - Training Epoch: 2/2, step 2233/7134 completed (loss: 0.06989644467830658, acc: 0.9807692170143127)
[2025-02-13 20:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53][root][INFO] - Training Epoch: 2/2, step 2234/7134 completed (loss: 0.0237300805747509, acc: 0.9935064911842346)
[2025-02-13 20:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53][root][INFO] - Training Epoch: 2/2, step 2235/7134 completed (loss: 0.12331023812294006, acc: 0.9720279574394226)
[2025-02-13 20:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53][root][INFO] - Training Epoch: 2/2, step 2236/7134 completed (loss: 0.054563358426094055, acc: 0.9866666793823242)
[2025-02-13 20:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54][root][INFO] - Training Epoch: 2/2, step 2237/7134 completed (loss: 0.06661107391119003, acc: 0.9800000190734863)
[2025-02-13 20:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54][root][INFO] - Training Epoch: 2/2, step 2238/7134 completed (loss: 0.09019073843955994, acc: 0.9814814925193787)
[2025-02-13 20:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55][root][INFO] - Training Epoch: 2/2, step 2239/7134 completed (loss: 0.24229532480239868, acc: 0.9465240836143494)
[2025-02-13 20:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55][root][INFO] - Training Epoch: 2/2, step 2240/7134 completed (loss: 0.08830254524946213, acc: 0.9772727489471436)
[2025-02-13 20:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55][root][INFO] - Training Epoch: 2/2, step 2241/7134 completed (loss: 0.09787320345640182, acc: 0.9583333134651184)
[2025-02-13 20:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56][root][INFO] - Training Epoch: 2/2, step 2242/7134 completed (loss: 0.1797209084033966, acc: 0.9444444179534912)
[2025-02-13 20:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56][root][INFO] - Training Epoch: 2/2, step 2243/7134 completed (loss: 0.2361164391040802, acc: 0.9354838728904724)
[2025-02-13 20:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56][root][INFO] - Training Epoch: 2/2, step 2244/7134 completed (loss: 0.14021442830562592, acc: 0.9589040875434875)
[2025-02-13 20:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57][root][INFO] - Training Epoch: 2/2, step 2245/7134 completed (loss: 0.18920333683490753, acc: 0.9542483687400818)
[2025-02-13 20:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57][root][INFO] - Training Epoch: 2/2, step 2246/7134 completed (loss: 0.43104541301727295, acc: 0.868852436542511)
[2025-02-13 20:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57][root][INFO] - Training Epoch: 2/2, step 2247/7134 completed (loss: 0.24385106563568115, acc: 0.9421965479850769)
[2025-02-13 20:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58][root][INFO] - Training Epoch: 2/2, step 2248/7134 completed (loss: 0.3089987337589264, acc: 0.9248554706573486)
[2025-02-13 20:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58][root][INFO] - Training Epoch: 2/2, step 2249/7134 completed (loss: 0.17064343392848969, acc: 0.9613259434700012)
[2025-02-13 20:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59][root][INFO] - Training Epoch: 2/2, step 2250/7134 completed (loss: 0.18294447660446167, acc: 0.9521276354789734)
[2025-02-13 20:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59][root][INFO] - Training Epoch: 2/2, step 2251/7134 completed (loss: 0.09216949343681335, acc: 0.9754098653793335)
[2025-02-13 20:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59][root][INFO] - Training Epoch: 2/2, step 2252/7134 completed (loss: 0.05677160993218422, acc: 0.989130437374115)
[2025-02-13 20:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00][root][INFO] - Training Epoch: 2/2, step 2253/7134 completed (loss: 0.08197545260190964, acc: 0.9808917045593262)
[2025-02-13 20:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00][root][INFO] - Training Epoch: 2/2, step 2254/7134 completed (loss: 0.18849502503871918, acc: 0.9596773982048035)
[2025-02-13 20:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00][root][INFO] - Training Epoch: 2/2, step 2255/7134 completed (loss: 0.07626418024301529, acc: 0.9748427867889404)
[2025-02-13 20:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01][root][INFO] - Training Epoch: 2/2, step 2256/7134 completed (loss: 0.08226010203361511, acc: 0.9837398529052734)
[2025-02-13 20:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01][root][INFO] - Training Epoch: 2/2, step 2257/7134 completed (loss: 0.09040437638759613, acc: 0.9814814925193787)
[2025-02-13 20:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01][root][INFO] - Training Epoch: 2/2, step 2258/7134 completed (loss: 0.07674159854650497, acc: 0.9922480583190918)
[2025-02-13 20:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02][root][INFO] - Training Epoch: 2/2, step 2259/7134 completed (loss: 0.12038297206163406, acc: 0.9829059839248657)
[2025-02-13 20:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02][root][INFO] - Training Epoch: 2/2, step 2260/7134 completed (loss: 0.22896058857440948, acc: 0.9155844449996948)
[2025-02-13 20:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02][root][INFO] - Training Epoch: 2/2, step 2261/7134 completed (loss: 0.21691690385341644, acc: 0.9444444179534912)
[2025-02-13 20:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03][root][INFO] - Training Epoch: 2/2, step 2262/7134 completed (loss: 0.07155880331993103, acc: 0.9849624037742615)
[2025-02-13 20:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03][root][INFO] - Training Epoch: 2/2, step 2263/7134 completed (loss: 0.20774264633655548, acc: 0.9548872113227844)
[2025-02-13 20:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03][root][INFO] - Training Epoch: 2/2, step 2264/7134 completed (loss: 0.2919726073741913, acc: 0.9351351261138916)
[2025-02-13 20:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04][root][INFO] - Training Epoch: 2/2, step 2265/7134 completed (loss: 0.2284747213125229, acc: 0.9545454382896423)
[2025-02-13 20:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04][root][INFO] - Training Epoch: 2/2, step 2266/7134 completed (loss: 0.20065562427043915, acc: 0.939226508140564)
[2025-02-13 20:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05][root][INFO] - Training Epoch: 2/2, step 2267/7134 completed (loss: 0.0753902792930603, acc: 0.9674796462059021)
[2025-02-13 20:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05][root][INFO] - Training Epoch: 2/2, step 2268/7134 completed (loss: 0.0878402441740036, acc: 0.9863013625144958)
[2025-02-13 20:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05][root][INFO] - Training Epoch: 2/2, step 2269/7134 completed (loss: 0.04451720416545868, acc: 0.994413435459137)
[2025-02-13 20:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06][root][INFO] - Training Epoch: 2/2, step 2270/7134 completed (loss: 0.18436388671398163, acc: 0.96875)
[2025-02-13 20:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06][root][INFO] - Training Epoch: 2/2, step 2271/7134 completed (loss: 0.20586319267749786, acc: 0.9459459185600281)
[2025-02-13 20:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06][root][INFO] - Training Epoch: 2/2, step 2272/7134 completed (loss: 0.09228174388408661, acc: 0.9623655676841736)
[2025-02-13 20:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:07][root][INFO] - Training Epoch: 2/2, step 2273/7134 completed (loss: 0.05583509802818298, acc: 0.9888888597488403)
[2025-02-13 20:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:07][root][INFO] - Training Epoch: 2/2, step 2274/7134 completed (loss: 0.45723283290863037, acc: 0.9230769276618958)
[2025-02-13 20:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08][root][INFO] - Training Epoch: 2/2, step 2275/7134 completed (loss: 0.07377313077449799, acc: 0.9950980544090271)
[2025-02-13 20:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08][root][INFO] - Training Epoch: 2/2, step 2276/7134 completed (loss: 0.15400519967079163, acc: 0.9788359999656677)
[2025-02-13 20:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08][root][INFO] - Training Epoch: 2/2, step 2277/7134 completed (loss: 0.08314258605241776, acc: 0.9897435903549194)
[2025-02-13 20:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09][root][INFO] - Training Epoch: 2/2, step 2278/7134 completed (loss: 0.3050205409526825, acc: 0.9354838728904724)
[2025-02-13 20:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09][root][INFO] - Training Epoch: 2/2, step 2279/7134 completed (loss: 0.08779407292604446, acc: 0.9796954393386841)
[2025-02-13 20:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10][root][INFO] - Training Epoch: 2/2, step 2280/7134 completed (loss: 0.16669628024101257, acc: 0.9587156176567078)
[2025-02-13 20:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10][root][INFO] - Training Epoch: 2/2, step 2281/7134 completed (loss: 0.3304857015609741, acc: 0.936170220375061)
[2025-02-13 20:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10][root][INFO] - Training Epoch: 2/2, step 2282/7134 completed (loss: 0.02691163681447506, acc: 0.9947368502616882)
[2025-02-13 20:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11][root][INFO] - Training Epoch: 2/2, step 2283/7134 completed (loss: 0.06749889254570007, acc: 0.9807692170143127)
[2025-02-13 20:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11][root][INFO] - Training Epoch: 2/2, step 2284/7134 completed (loss: 0.046108394861221313, acc: 0.9870967864990234)
[2025-02-13 20:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11][root][INFO] - Training Epoch: 2/2, step 2285/7134 completed (loss: 0.0514225997030735, acc: 0.9900000095367432)
[2025-02-13 20:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12][root][INFO] - Training Epoch: 2/2, step 2286/7134 completed (loss: 0.1286020427942276, acc: 0.9655172228813171)
[2025-02-13 20:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12][root][INFO] - Training Epoch: 2/2, step 2287/7134 completed (loss: 0.10280732810497284, acc: 0.9693251252174377)
[2025-02-13 20:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12][root][INFO] - Training Epoch: 2/2, step 2288/7134 completed (loss: 0.015590700320899487, acc: 1.0)
[2025-02-13 20:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:13][root][INFO] - Training Epoch: 2/2, step 2289/7134 completed (loss: 0.10101348906755447, acc: 0.9679144620895386)
[2025-02-13 20:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:13][root][INFO] - Training Epoch: 2/2, step 2290/7134 completed (loss: 0.06040160357952118, acc: 0.9838709831237793)
[2025-02-13 20:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14][root][INFO] - Training Epoch: 2/2, step 2291/7134 completed (loss: 0.039638932794332504, acc: 1.0)
[2025-02-13 20:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14][root][INFO] - Training Epoch: 2/2, step 2292/7134 completed (loss: 0.10679829120635986, acc: 0.9797979593276978)
[2025-02-13 20:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14][root][INFO] - Training Epoch: 2/2, step 2293/7134 completed (loss: 0.07796543836593628, acc: 0.9817351698875427)
[2025-02-13 20:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15][root][INFO] - Training Epoch: 2/2, step 2294/7134 completed (loss: 0.06330154836177826, acc: 0.9856459498405457)
[2025-02-13 20:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15][root][INFO] - Training Epoch: 2/2, step 2295/7134 completed (loss: 0.12308892607688904, acc: 0.95703125)
[2025-02-13 20:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15][root][INFO] - Training Epoch: 2/2, step 2296/7134 completed (loss: 0.05238091200590134, acc: 0.9807692170143127)
[2025-02-13 20:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16][root][INFO] - Training Epoch: 2/2, step 2297/7134 completed (loss: 0.02938300557434559, acc: 1.0)
[2025-02-13 20:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16][root][INFO] - Training Epoch: 2/2, step 2298/7134 completed (loss: 0.06573515385389328, acc: 0.9809523820877075)
[2025-02-13 20:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16][root][INFO] - Training Epoch: 2/2, step 2299/7134 completed (loss: 0.10602100938558578, acc: 0.9784946441650391)
[2025-02-13 20:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17][root][INFO] - Training Epoch: 2/2, step 2300/7134 completed (loss: 0.09346672147512436, acc: 0.9747474789619446)
[2025-02-13 20:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17][root][INFO] - Training Epoch: 2/2, step 2301/7134 completed (loss: 0.09965589642524719, acc: 0.967391312122345)
[2025-02-13 20:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18][root][INFO] - Training Epoch: 2/2, step 2302/7134 completed (loss: 0.09911448508501053, acc: 0.9692307710647583)
[2025-02-13 20:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18][root][INFO] - Training Epoch: 2/2, step 2303/7134 completed (loss: 0.13018272817134857, acc: 0.981566846370697)
[2025-02-13 20:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18][root][INFO] - Training Epoch: 2/2, step 2304/7134 completed (loss: 0.14145290851593018, acc: 0.9471365809440613)
[2025-02-13 20:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19][root][INFO] - Training Epoch: 2/2, step 2305/7134 completed (loss: 0.10523355007171631, acc: 0.9716981053352356)
[2025-02-13 20:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19][root][INFO] - Training Epoch: 2/2, step 2306/7134 completed (loss: 0.05214553326368332, acc: 0.9860464930534363)
[2025-02-13 20:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19][root][INFO] - Training Epoch: 2/2, step 2307/7134 completed (loss: 0.04802856221795082, acc: 0.991525411605835)
[2025-02-13 20:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20][root][INFO] - Training Epoch: 2/2, step 2308/7134 completed (loss: 0.06391452252864838, acc: 0.9867841601371765)
[2025-02-13 20:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20][root][INFO] - Training Epoch: 2/2, step 2309/7134 completed (loss: 0.04711447283625603, acc: 0.9841269850730896)
[2025-02-13 20:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21][root][INFO] - Training Epoch: 2/2, step 2310/7134 completed (loss: 0.07371716946363449, acc: 0.9857142567634583)
[2025-02-13 20:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21][root][INFO] - Training Epoch: 2/2, step 2311/7134 completed (loss: 0.05797090753912926, acc: 0.9802955389022827)
[2025-02-13 20:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21][root][INFO] - Training Epoch: 2/2, step 2312/7134 completed (loss: 0.04870228469371796, acc: 0.9953051805496216)
[2025-02-13 20:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22][root][INFO] - Training Epoch: 2/2, step 2313/7134 completed (loss: 0.09035709500312805, acc: 0.9807692170143127)
[2025-02-13 20:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22][root][INFO] - Training Epoch: 2/2, step 2314/7134 completed (loss: 0.058340493589639664, acc: 0.9770641922950745)
[2025-02-13 20:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22][root][INFO] - Training Epoch: 2/2, step 2315/7134 completed (loss: 0.08958154171705246, acc: 0.9627906680107117)
[2025-02-13 20:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23][root][INFO] - Training Epoch: 2/2, step 2316/7134 completed (loss: 0.023914648219943047, acc: 1.0)
[2025-02-13 20:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23][root][INFO] - Training Epoch: 2/2, step 2317/7134 completed (loss: 0.11949370801448822, acc: 0.9621848464012146)
[2025-02-13 20:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23][root][INFO] - Training Epoch: 2/2, step 2318/7134 completed (loss: 0.029862847179174423, acc: 0.9956331849098206)
[2025-02-13 20:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24][root][INFO] - Training Epoch: 2/2, step 2319/7134 completed (loss: 0.026462778449058533, acc: 0.9954954981803894)
[2025-02-13 20:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24][root][INFO] - Training Epoch: 2/2, step 2320/7134 completed (loss: 0.028796490281820297, acc: 0.9948717951774597)
[2025-02-13 20:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25][root][INFO] - Training Epoch: 2/2, step 2321/7134 completed (loss: 0.06520278006792068, acc: 0.9882352948188782)
[2025-02-13 20:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25][root][INFO] - Training Epoch: 2/2, step 2322/7134 completed (loss: 0.06832096725702286, acc: 0.9693251252174377)
[2025-02-13 20:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25][root][INFO] - Training Epoch: 2/2, step 2323/7134 completed (loss: 0.05274941027164459, acc: 0.987261176109314)
[2025-02-13 20:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26][root][INFO] - Training Epoch: 2/2, step 2324/7134 completed (loss: 0.2337590456008911, acc: 0.9759036302566528)
[2025-02-13 20:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26][root][INFO] - Training Epoch: 2/2, step 2325/7134 completed (loss: 0.08841542154550552, acc: 0.9714285731315613)
[2025-02-13 20:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26][root][INFO] - Training Epoch: 2/2, step 2326/7134 completed (loss: 0.043059926480054855, acc: 0.9923076629638672)
[2025-02-13 20:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27][root][INFO] - Training Epoch: 2/2, step 2327/7134 completed (loss: 0.04877838119864464, acc: 0.9798657894134521)
[2025-02-13 20:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27][root][INFO] - Training Epoch: 2/2, step 2328/7134 completed (loss: 0.10350597649812698, acc: 0.9865771532058716)
[2025-02-13 20:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28][root][INFO] - Training Epoch: 2/2, step 2329/7134 completed (loss: 0.08452337235212326, acc: 0.9803921580314636)
[2025-02-13 20:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28][root][INFO] - Training Epoch: 2/2, step 2330/7134 completed (loss: 0.013184154406189919, acc: 1.0)
[2025-02-13 20:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28][root][INFO] - Training Epoch: 2/2, step 2331/7134 completed (loss: 0.029598534107208252, acc: 0.9870967864990234)
[2025-02-13 20:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29][root][INFO] - Training Epoch: 2/2, step 2332/7134 completed (loss: 0.041150614619255066, acc: 0.9939393997192383)
[2025-02-13 20:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29][root][INFO] - Training Epoch: 2/2, step 2333/7134 completed (loss: 0.08354304730892181, acc: 0.9817073345184326)
[2025-02-13 20:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29][root][INFO] - Training Epoch: 2/2, step 2334/7134 completed (loss: 0.15881487727165222, acc: 0.9681528806686401)
[2025-02-13 20:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30][root][INFO] - Training Epoch: 2/2, step 2335/7134 completed (loss: 0.0322946161031723, acc: 0.9873417615890503)
[2025-02-13 20:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30][root][INFO] - Training Epoch: 2/2, step 2336/7134 completed (loss: 0.020619375631213188, acc: 1.0)
[2025-02-13 20:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31][root][INFO] - Training Epoch: 2/2, step 2337/7134 completed (loss: 0.023626793175935745, acc: 0.9930555820465088)
[2025-02-13 20:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31][root][INFO] - Training Epoch: 2/2, step 2338/7134 completed (loss: 0.03287607803940773, acc: 1.0)
[2025-02-13 20:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31][root][INFO] - Training Epoch: 2/2, step 2339/7134 completed (loss: 0.12290889769792557, acc: 0.9615384340286255)
[2025-02-13 20:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32][root][INFO] - Training Epoch: 2/2, step 2340/7134 completed (loss: 0.016161417588591576, acc: 1.0)
[2025-02-13 20:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32][root][INFO] - Training Epoch: 2/2, step 2341/7134 completed (loss: 0.022710751742124557, acc: 1.0)
[2025-02-13 20:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32][root][INFO] - Training Epoch: 2/2, step 2342/7134 completed (loss: 0.024975482374429703, acc: 0.988095223903656)
[2025-02-13 20:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33][root][INFO] - Training Epoch: 2/2, step 2343/7134 completed (loss: 0.05146780610084534, acc: 0.987500011920929)
[2025-02-13 20:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33][root][INFO] - Training Epoch: 2/2, step 2344/7134 completed (loss: 0.050090543925762177, acc: 0.9876543283462524)
[2025-02-13 20:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34][root][INFO] - Training Epoch: 2/2, step 2345/7134 completed (loss: 0.051089249551296234, acc: 0.982758641242981)
[2025-02-13 20:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34][root][INFO] - Training Epoch: 2/2, step 2346/7134 completed (loss: 0.05134579539299011, acc: 0.988950252532959)
[2025-02-13 20:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34][root][INFO] - Training Epoch: 2/2, step 2347/7134 completed (loss: 0.11209019273519516, acc: 0.98591548204422)
[2025-02-13 20:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35][root][INFO] - Training Epoch: 2/2, step 2348/7134 completed (loss: 0.2469230592250824, acc: 0.938144326210022)
[2025-02-13 20:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35][root][INFO] - Training Epoch: 2/2, step 2349/7134 completed (loss: 0.14138631522655487, acc: 0.9624999761581421)
[2025-02-13 20:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36][root][INFO] - Training Epoch: 2/2, step 2350/7134 completed (loss: 0.20235353708267212, acc: 0.9430379867553711)
[2025-02-13 20:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36][root][INFO] - Training Epoch: 2/2, step 2351/7134 completed (loss: 0.10477498918771744, acc: 0.9726775884628296)
[2025-02-13 20:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36][root][INFO] - Training Epoch: 2/2, step 2352/7134 completed (loss: 0.16200324892997742, acc: 0.948387086391449)
[2025-02-13 20:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37][root][INFO] - Training Epoch: 2/2, step 2353/7134 completed (loss: 0.1730400025844574, acc: 0.9612902998924255)
[2025-02-13 20:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37][root][INFO] - Training Epoch: 2/2, step 2354/7134 completed (loss: 0.061363089829683304, acc: 0.9726775884628296)
[2025-02-13 20:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37][root][INFO] - Training Epoch: 2/2, step 2355/7134 completed (loss: 0.13507068157196045, acc: 0.9723756909370422)
[2025-02-13 20:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:38][root][INFO] - Training Epoch: 2/2, step 2356/7134 completed (loss: 0.14510558545589447, acc: 0.97826087474823)
[2025-02-13 20:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:38][root][INFO] - Training Epoch: 2/2, step 2357/7134 completed (loss: 0.0944036915898323, acc: 0.976190447807312)
[2025-02-13 20:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39][root][INFO] - Training Epoch: 2/2, step 2358/7134 completed (loss: 0.15122711658477783, acc: 0.9591836929321289)
[2025-02-13 20:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39][root][INFO] - Training Epoch: 2/2, step 2359/7134 completed (loss: 0.10645876824855804, acc: 0.987500011920929)
[2025-02-13 20:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39][root][INFO] - Training Epoch: 2/2, step 2360/7134 completed (loss: 0.11971405148506165, acc: 0.9810126423835754)
[2025-02-13 20:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40][root][INFO] - Training Epoch: 2/2, step 2361/7134 completed (loss: 0.09953277558088303, acc: 0.976190447807312)
[2025-02-13 20:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40][root][INFO] - Training Epoch: 2/2, step 2362/7134 completed (loss: 0.14639706909656525, acc: 0.9459459185600281)
[2025-02-13 20:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41][root][INFO] - Training Epoch: 2/2, step 2363/7134 completed (loss: 0.10552405565977097, acc: 0.9790576100349426)
[2025-02-13 20:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41][root][INFO] - Training Epoch: 2/2, step 2364/7134 completed (loss: 0.06335719674825668, acc: 0.9869281053543091)
[2025-02-13 20:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41][root][INFO] - Training Epoch: 2/2, step 2365/7134 completed (loss: 0.06339982151985168, acc: 0.9825581312179565)
[2025-02-13 20:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42][root][INFO] - Training Epoch: 2/2, step 2366/7134 completed (loss: 0.059637024998664856, acc: 0.9897435903549194)
[2025-02-13 20:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42][root][INFO] - Training Epoch: 2/2, step 2367/7134 completed (loss: 0.10033439099788666, acc: 0.9848484992980957)
[2025-02-13 20:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43][root][INFO] - Training Epoch: 2/2, step 2368/7134 completed (loss: 0.11532223969697952, acc: 0.979899525642395)
[2025-02-13 20:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43][root][INFO] - Training Epoch: 2/2, step 2369/7134 completed (loss: 0.09604640305042267, acc: 0.9858490824699402)
[2025-02-13 20:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43][root][INFO] - Training Epoch: 2/2, step 2370/7134 completed (loss: 0.019317401573061943, acc: 1.0)
[2025-02-13 20:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44][root][INFO] - Training Epoch: 2/2, step 2371/7134 completed (loss: 0.06894438713788986, acc: 0.9689440727233887)
[2025-02-13 20:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44][root][INFO] - Training Epoch: 2/2, step 2372/7134 completed (loss: 0.05432838946580887, acc: 0.9900000095367432)
[2025-02-13 20:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44][root][INFO] - Training Epoch: 2/2, step 2373/7134 completed (loss: 0.047389473766088486, acc: 0.984455943107605)
[2025-02-13 20:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45][root][INFO] - Training Epoch: 2/2, step 2374/7134 completed (loss: 0.04626866802573204, acc: 0.9779005646705627)
[2025-02-13 20:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45][root][INFO] - Training Epoch: 2/2, step 2375/7134 completed (loss: 0.11174428462982178, acc: 0.9696969985961914)
[2025-02-13 20:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46][root][INFO] - Training Epoch: 2/2, step 2376/7134 completed (loss: 0.179476797580719, acc: 0.9673202633857727)
[2025-02-13 20:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46][root][INFO] - Training Epoch: 2/2, step 2377/7134 completed (loss: 0.08068221062421799, acc: 0.9766355156898499)
[2025-02-13 20:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46][root][INFO] - Training Epoch: 2/2, step 2378/7134 completed (loss: 0.13164380192756653, acc: 0.9752475023269653)
[2025-02-13 20:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47][root][INFO] - Training Epoch: 2/2, step 2379/7134 completed (loss: 0.18859098851680756, acc: 0.9438775777816772)
[2025-02-13 20:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47][root][INFO] - Training Epoch: 2/2, step 2380/7134 completed (loss: 0.1327069252729416, acc: 0.9836956262588501)
[2025-02-13 20:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47][root][INFO] - Training Epoch: 2/2, step 2381/7134 completed (loss: 0.3246143162250519, acc: 0.9068322777748108)
[2025-02-13 20:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:48][root][INFO] - Training Epoch: 2/2, step 2382/7134 completed (loss: 0.11368589103221893, acc: 0.9691358208656311)
[2025-02-13 20:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:48][root][INFO] - Training Epoch: 2/2, step 2383/7134 completed (loss: 0.10611751675605774, acc: 0.9672130942344666)
[2025-02-13 20:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49][root][INFO] - Training Epoch: 2/2, step 2384/7134 completed (loss: 0.21264871954917908, acc: 0.9655172228813171)
[2025-02-13 20:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49][root][INFO] - Training Epoch: 2/2, step 2385/7134 completed (loss: 0.08875033259391785, acc: 0.9725274443626404)
[2025-02-13 20:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49][root][INFO] - Training Epoch: 2/2, step 2386/7134 completed (loss: 0.10324030369520187, acc: 0.9735449552536011)
[2025-02-13 20:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50][root][INFO] - Training Epoch: 2/2, step 2387/7134 completed (loss: 0.08898476511240005, acc: 0.9714285731315613)
[2025-02-13 20:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50][root][INFO] - Training Epoch: 2/2, step 2388/7134 completed (loss: 0.09297850728034973, acc: 0.9759615659713745)
[2025-02-13 20:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51][root][INFO] - Training Epoch: 2/2, step 2389/7134 completed (loss: 0.22228789329528809, acc: 0.9478672742843628)
[2025-02-13 20:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51][root][INFO] - Training Epoch: 2/2, step 2390/7134 completed (loss: 0.2814774215221405, acc: 0.9194312691688538)
[2025-02-13 20:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51][root][INFO] - Training Epoch: 2/2, step 2391/7134 completed (loss: 0.16585591435432434, acc: 0.9621621370315552)
[2025-02-13 20:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52][root][INFO] - Training Epoch: 2/2, step 2392/7134 completed (loss: 0.15031222999095917, acc: 0.9617486596107483)
[2025-02-13 20:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52][root][INFO] - Training Epoch: 2/2, step 2393/7134 completed (loss: 0.10145705193281174, acc: 0.9740932583808899)
[2025-02-13 20:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52][root][INFO] - Training Epoch: 2/2, step 2394/7134 completed (loss: 0.11829216778278351, acc: 0.9846938848495483)
[2025-02-13 20:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53][root][INFO] - Training Epoch: 2/2, step 2395/7134 completed (loss: 0.34681758284568787, acc: 0.9314285516738892)
[2025-02-13 20:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53][root][INFO] - Training Epoch: 2/2, step 2396/7134 completed (loss: 0.3306187391281128, acc: 0.9141414165496826)
[2025-02-13 20:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54][root][INFO] - Training Epoch: 2/2, step 2397/7134 completed (loss: 0.2154674082994461, acc: 0.9695122241973877)
[2025-02-13 20:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54][root][INFO] - Training Epoch: 2/2, step 2398/7134 completed (loss: 0.07649923861026764, acc: 0.9783783555030823)
[2025-02-13 20:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54][root][INFO] - Training Epoch: 2/2, step 2399/7134 completed (loss: 0.2592836618423462, acc: 0.9274611473083496)
[2025-02-13 20:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55][root][INFO] - Training Epoch: 2/2, step 2400/7134 completed (loss: 0.28156572580337524, acc: 0.9246231317520142)
[2025-02-13 20:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55][root][INFO] - Training Epoch: 2/2, step 2401/7134 completed (loss: 0.6225901246070862, acc: 0.8221153616905212)
[2025-02-13 20:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55][root][INFO] - Training Epoch: 2/2, step 2402/7134 completed (loss: 0.25115540623664856, acc: 0.9082125425338745)
[2025-02-13 20:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56][root][INFO] - Training Epoch: 2/2, step 2403/7134 completed (loss: 0.07565966993570328, acc: 0.9820359349250793)
[2025-02-13 20:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56][root][INFO] - Training Epoch: 2/2, step 2404/7134 completed (loss: 0.08586685359477997, acc: 0.9802631735801697)
[2025-02-13 20:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56][root][INFO] - Training Epoch: 2/2, step 2405/7134 completed (loss: 0.20555172860622406, acc: 0.9681528806686401)
[2025-02-13 20:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57][root][INFO] - Training Epoch: 2/2, step 2406/7134 completed (loss: 0.027241230010986328, acc: 1.0)
[2025-02-13 20:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57][root][INFO] - Training Epoch: 2/2, step 2407/7134 completed (loss: 0.2188822627067566, acc: 0.9607843160629272)
[2025-02-13 20:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58][root][INFO] - Training Epoch: 2/2, step 2408/7134 completed (loss: 0.22098581492900848, acc: 0.950276255607605)
[2025-02-13 20:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58][root][INFO] - Training Epoch: 2/2, step 2409/7134 completed (loss: 0.12886610627174377, acc: 0.9624999761581421)
[2025-02-13 20:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58][root][INFO] - Training Epoch: 2/2, step 2410/7134 completed (loss: 0.053756795823574066, acc: 0.9929577708244324)
[2025-02-13 20:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59][root][INFO] - Training Epoch: 2/2, step 2411/7134 completed (loss: 0.03660735860466957, acc: 0.988950252532959)
[2025-02-13 20:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59][root][INFO] - Training Epoch: 2/2, step 2412/7134 completed (loss: 0.042633768171072006, acc: 0.9943181872367859)
[2025-02-13 20:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59][root][INFO] - Training Epoch: 2/2, step 2413/7134 completed (loss: 0.034987300634384155, acc: 1.0)
[2025-02-13 20:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00][root][INFO] - Training Epoch: 2/2, step 2414/7134 completed (loss: 0.05312179774045944, acc: 0.9890710115432739)
[2025-02-13 20:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00][root][INFO] - Training Epoch: 2/2, step 2415/7134 completed (loss: 0.22559793293476105, acc: 0.9817073345184326)
[2025-02-13 20:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01][root][INFO] - Training Epoch: 2/2, step 2416/7134 completed (loss: 0.07128933072090149, acc: 0.9811320900917053)
[2025-02-13 20:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01][root][INFO] - Training Epoch: 2/2, step 2417/7134 completed (loss: 0.11547955125570297, acc: 0.9804878234863281)
[2025-02-13 20:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01][root][INFO] - Training Epoch: 2/2, step 2418/7134 completed (loss: 0.11279484629631042, acc: 0.9720279574394226)
[2025-02-13 20:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02][root][INFO] - Training Epoch: 2/2, step 2419/7134 completed (loss: 0.09489937871694565, acc: 0.9815950989723206)
[2025-02-13 20:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02][root][INFO] - Training Epoch: 2/2, step 2420/7134 completed (loss: 0.10570420324802399, acc: 0.9722222089767456)
[2025-02-13 20:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02][root][INFO] - Training Epoch: 2/2, step 2421/7134 completed (loss: 0.06669308245182037, acc: 0.9783783555030823)
[2025-02-13 20:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03][root][INFO] - Training Epoch: 2/2, step 2422/7134 completed (loss: 0.07629949599504471, acc: 0.9828571677207947)
[2025-02-13 20:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03][root][INFO] - Training Epoch: 2/2, step 2423/7134 completed (loss: 0.0875249058008194, acc: 0.988095223903656)
[2025-02-13 20:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04][root][INFO] - Training Epoch: 2/2, step 2424/7134 completed (loss: 0.10006535053253174, acc: 0.9772727489471436)
[2025-02-13 20:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04][root][INFO] - Training Epoch: 2/2, step 2425/7134 completed (loss: 0.09422608464956284, acc: 0.966292142868042)
[2025-02-13 20:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04][root][INFO] - Training Epoch: 2/2, step 2426/7134 completed (loss: 0.06320357322692871, acc: 0.982758641242981)
[2025-02-13 20:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05][root][INFO] - Training Epoch: 2/2, step 2427/7134 completed (loss: 0.04638237878680229, acc: 0.9791666865348816)
[2025-02-13 20:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05][root][INFO] - Training Epoch: 2/2, step 2428/7134 completed (loss: 0.03930094465613365, acc: 0.9947368502616882)
[2025-02-13 20:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06][root][INFO] - Training Epoch: 2/2, step 2429/7134 completed (loss: 0.0881194919347763, acc: 0.9746835231781006)
[2025-02-13 20:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06][root][INFO] - Training Epoch: 2/2, step 2430/7134 completed (loss: 0.07511167228221893, acc: 0.9800000190734863)
[2025-02-13 20:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06][root][INFO] - Training Epoch: 2/2, step 2431/7134 completed (loss: 0.04593789577484131, acc: 0.9860140085220337)
[2025-02-13 20:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07][root][INFO] - Training Epoch: 2/2, step 2432/7134 completed (loss: 0.19139310717582703, acc: 0.942148745059967)
[2025-02-13 20:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07][root][INFO] - Training Epoch: 2/2, step 2433/7134 completed (loss: 0.14249302446842194, acc: 0.971222996711731)
[2025-02-13 20:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07][root][INFO] - Training Epoch: 2/2, step 2434/7134 completed (loss: 0.14876040816307068, acc: 0.9597315192222595)
[2025-02-13 20:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:08][root][INFO] - Training Epoch: 2/2, step 2435/7134 completed (loss: 0.15458045899868011, acc: 0.9602649211883545)
[2025-02-13 20:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:08][root][INFO] - Training Epoch: 2/2, step 2436/7134 completed (loss: 0.05035283789038658, acc: 0.9847715497016907)
[2025-02-13 20:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09][root][INFO] - Training Epoch: 2/2, step 2437/7134 completed (loss: 0.11353867501020432, acc: 0.9718309640884399)
[2025-02-13 20:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09][root][INFO] - Training Epoch: 2/2, step 2438/7134 completed (loss: 0.11574223637580872, acc: 0.9814814925193787)
[2025-02-13 20:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09][root][INFO] - Training Epoch: 2/2, step 2439/7134 completed (loss: 0.09251084923744202, acc: 0.9893048405647278)
[2025-02-13 20:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10][root][INFO] - Training Epoch: 2/2, step 2440/7134 completed (loss: 0.05669538676738739, acc: 0.9886363744735718)
[2025-02-13 20:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10][root][INFO] - Training Epoch: 2/2, step 2441/7134 completed (loss: 0.2862533628940582, acc: 0.9520000219345093)
[2025-02-13 20:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10][root][INFO] - Training Epoch: 2/2, step 2442/7134 completed (loss: 0.13951575756072998, acc: 0.976331353187561)
[2025-02-13 20:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11][root][INFO] - Training Epoch: 2/2, step 2443/7134 completed (loss: 0.19136889278888702, acc: 0.9631901979446411)
[2025-02-13 20:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11][root][INFO] - Training Epoch: 2/2, step 2444/7134 completed (loss: 0.1844761222600937, acc: 0.9534883499145508)
[2025-02-13 20:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12][root][INFO] - Training Epoch: 2/2, step 2445/7134 completed (loss: 0.16766910254955292, acc: 0.9617834687232971)
[2025-02-13 20:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12][root][INFO] - Training Epoch: 2/2, step 2446/7134 completed (loss: 0.3183051347732544, acc: 0.9285714030265808)
[2025-02-13 20:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12][root][INFO] - Training Epoch: 2/2, step 2447/7134 completed (loss: 0.07277902960777283, acc: 0.9887640476226807)
[2025-02-13 20:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13][root][INFO] - Training Epoch: 2/2, step 2448/7134 completed (loss: 0.07138895243406296, acc: 0.9798657894134521)
[2025-02-13 20:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13][root][INFO] - Training Epoch: 2/2, step 2449/7134 completed (loss: 0.20825077593326569, acc: 0.9418604373931885)
[2025-02-13 20:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13][root][INFO] - Training Epoch: 2/2, step 2450/7134 completed (loss: 0.046063702553510666, acc: 0.9882352948188782)
[2025-02-13 20:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14][root][INFO] - Training Epoch: 2/2, step 2451/7134 completed (loss: 0.07766169309616089, acc: 0.9839572310447693)
[2025-02-13 20:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14][root][INFO] - Training Epoch: 2/2, step 2452/7134 completed (loss: 0.08636526018381119, acc: 0.9731183052062988)
[2025-02-13 20:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14][root][INFO] - Training Epoch: 2/2, step 2453/7134 completed (loss: 0.0856482982635498, acc: 0.9740932583808899)
[2025-02-13 20:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:15][root][INFO] - Training Epoch: 2/2, step 2454/7134 completed (loss: 0.07339641451835632, acc: 0.9716312289237976)
[2025-02-13 20:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:15][root][INFO] - Training Epoch: 2/2, step 2455/7134 completed (loss: 0.04882936552166939, acc: 0.987730085849762)
[2025-02-13 20:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16][root][INFO] - Training Epoch: 2/2, step 2456/7134 completed (loss: 0.13483041524887085, acc: 0.9481865167617798)
[2025-02-13 20:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16][root][INFO] - Training Epoch: 2/2, step 2457/7134 completed (loss: 0.14852869510650635, acc: 0.9738562107086182)
[2025-02-13 20:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16][root][INFO] - Training Epoch: 2/2, step 2458/7134 completed (loss: 0.281974732875824, acc: 0.9437500238418579)
[2025-02-13 20:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17][root][INFO] - Training Epoch: 2/2, step 2459/7134 completed (loss: 0.06097635254263878, acc: 0.9824561476707458)
[2025-02-13 20:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17][root][INFO] - Training Epoch: 2/2, step 2460/7134 completed (loss: 0.0976099893450737, acc: 0.9725274443626404)
[2025-02-13 20:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17][root][INFO] - Training Epoch: 2/2, step 2461/7134 completed (loss: 0.034329853951931, acc: 0.9937106966972351)
[2025-02-13 20:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18][root][INFO] - Training Epoch: 2/2, step 2462/7134 completed (loss: 0.16793277859687805, acc: 0.9636363387107849)
[2025-02-13 20:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18][root][INFO] - Training Epoch: 2/2, step 2463/7134 completed (loss: 0.01910456083714962, acc: 1.0)
[2025-02-13 20:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18][root][INFO] - Training Epoch: 2/2, step 2464/7134 completed (loss: 0.10387872159481049, acc: 0.9750000238418579)
[2025-02-13 20:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19][root][INFO] - Training Epoch: 2/2, step 2465/7134 completed (loss: 0.0824272409081459, acc: 0.9664804339408875)
[2025-02-13 20:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19][root][INFO] - Training Epoch: 2/2, step 2466/7134 completed (loss: 0.04038577899336815, acc: 0.9820359349250793)
[2025-02-13 20:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20][root][INFO] - Training Epoch: 2/2, step 2467/7134 completed (loss: 0.11899620294570923, acc: 0.966292142868042)
[2025-02-13 20:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20][root][INFO] - Training Epoch: 2/2, step 2468/7134 completed (loss: 0.06704005599021912, acc: 0.9709302186965942)
[2025-02-13 20:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20][root][INFO] - Training Epoch: 2/2, step 2469/7134 completed (loss: 0.018636072054505348, acc: 1.0)
[2025-02-13 20:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21][root][INFO] - Training Epoch: 2/2, step 2470/7134 completed (loss: 0.11132261157035828, acc: 0.9716312289237976)
[2025-02-13 20:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21][root][INFO] - Training Epoch: 2/2, step 2471/7134 completed (loss: 0.09581959247589111, acc: 0.957446813583374)
[2025-02-13 20:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22][root][INFO] - Training Epoch: 2/2, step 2472/7134 completed (loss: 0.019265752285718918, acc: 1.0)
[2025-02-13 20:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22][root][INFO] - Training Epoch: 2/2, step 2473/7134 completed (loss: 0.0420488566160202, acc: 0.9906542301177979)
[2025-02-13 20:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22][root][INFO] - Training Epoch: 2/2, step 2474/7134 completed (loss: 0.016163058578968048, acc: 1.0)
[2025-02-13 20:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23][root][INFO] - Training Epoch: 2/2, step 2475/7134 completed (loss: 0.02904493734240532, acc: 0.9918032884597778)
[2025-02-13 20:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23][root][INFO] - Training Epoch: 2/2, step 2476/7134 completed (loss: 0.03799529746174812, acc: 0.9824561476707458)
[2025-02-13 20:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23][root][INFO] - Training Epoch: 2/2, step 2477/7134 completed (loss: 0.013157207518815994, acc: 1.0)
[2025-02-13 20:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24][root][INFO] - Training Epoch: 2/2, step 2478/7134 completed (loss: 0.024406710639595985, acc: 1.0)
[2025-02-13 20:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24][root][INFO] - Training Epoch: 2/2, step 2479/7134 completed (loss: 0.15858761966228485, acc: 0.9591836929321289)
[2025-02-13 20:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25][root][INFO] - Training Epoch: 2/2, step 2480/7134 completed (loss: 0.06048274040222168, acc: 0.9807692170143127)
[2025-02-13 20:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25][root][INFO] - Training Epoch: 2/2, step 2481/7134 completed (loss: 0.026193767786026, acc: 1.0)
[2025-02-13 20:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25][root][INFO] - Training Epoch: 2/2, step 2482/7134 completed (loss: 0.06438374519348145, acc: 0.9805194735527039)
[2025-02-13 20:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:26][root][INFO] - Training Epoch: 2/2, step 2483/7134 completed (loss: 0.047441598027944565, acc: 0.9940119981765747)
[2025-02-13 20:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:26][root][INFO] - Training Epoch: 2/2, step 2484/7134 completed (loss: 0.10662846267223358, acc: 0.9819276928901672)
[2025-02-13 20:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27][root][INFO] - Training Epoch: 2/2, step 2485/7134 completed (loss: 0.04962143674492836, acc: 0.9925925731658936)
[2025-02-13 20:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27][root][INFO] - Training Epoch: 2/2, step 2486/7134 completed (loss: 0.07505276799201965, acc: 0.9931507110595703)
[2025-02-13 20:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27][root][INFO] - Training Epoch: 2/2, step 2487/7134 completed (loss: 0.20462584495544434, acc: 0.9583333134651184)
[2025-02-13 20:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28][root][INFO] - Training Epoch: 2/2, step 2488/7134 completed (loss: 0.07286573201417923, acc: 0.9818181991577148)
[2025-02-13 20:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28][root][INFO] - Training Epoch: 2/2, step 2489/7134 completed (loss: 0.10280707478523254, acc: 0.976331353187561)
[2025-02-13 20:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28][root][INFO] - Training Epoch: 2/2, step 2490/7134 completed (loss: 0.07266996800899506, acc: 0.9870129823684692)
[2025-02-13 20:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:29][root][INFO] - Training Epoch: 2/2, step 2491/7134 completed (loss: 0.15305280685424805, acc: 0.9701492786407471)
[2025-02-13 20:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:29][root][INFO] - Training Epoch: 2/2, step 2492/7134 completed (loss: 0.0646941065788269, acc: 0.9937106966972351)
[2025-02-13 20:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30][root][INFO] - Training Epoch: 2/2, step 2493/7134 completed (loss: 0.07618031650781631, acc: 0.988095223903656)
[2025-02-13 20:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30][root][INFO] - Training Epoch: 2/2, step 2494/7134 completed (loss: 0.05742722377181053, acc: 0.9876543283462524)
[2025-02-13 20:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30][root][INFO] - Training Epoch: 2/2, step 2495/7134 completed (loss: 0.08286537230014801, acc: 0.9882352948188782)
[2025-02-13 20:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:31][root][INFO] - Training Epoch: 2/2, step 2496/7134 completed (loss: 0.07570842653512955, acc: 0.9810126423835754)
[2025-02-13 20:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:31][root][INFO] - Training Epoch: 2/2, step 2497/7134 completed (loss: 0.0756172388792038, acc: 0.9885057210922241)
[2025-02-13 20:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32][root][INFO] - Training Epoch: 2/2, step 2498/7134 completed (loss: 0.059497103095054626, acc: 0.9828571677207947)
[2025-02-13 20:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32][root][INFO] - Training Epoch: 2/2, step 2499/7134 completed (loss: 0.03312436863780022, acc: 0.9939758777618408)
[2025-02-13 20:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32][root][INFO] - Training Epoch: 2/2, step 2500/7134 completed (loss: 0.059096354991197586, acc: 0.9932432174682617)
[2025-02-13 20:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33][root][INFO] - Training Epoch: 2/2, step 2501/7134 completed (loss: 0.09205345809459686, acc: 0.9775280952453613)
[2025-02-13 20:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33][root][INFO] - Training Epoch: 2/2, step 2502/7134 completed (loss: 0.04305935651063919, acc: 0.9914529919624329)
[2025-02-13 20:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33][root][INFO] - Training Epoch: 2/2, step 2503/7134 completed (loss: 0.03783832862973213, acc: 0.9927536249160767)
[2025-02-13 20:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34][root][INFO] - Training Epoch: 2/2, step 2504/7134 completed (loss: 0.10804783552885056, acc: 0.9683544039726257)
[2025-02-13 20:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34][root][INFO] - Training Epoch: 2/2, step 2505/7134 completed (loss: 0.051071859896183014, acc: 0.9825581312179565)
[2025-02-13 20:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35][root][INFO] - Training Epoch: 2/2, step 2506/7134 completed (loss: 0.021769866347312927, acc: 1.0)
[2025-02-13 20:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35][root][INFO] - Training Epoch: 2/2, step 2507/7134 completed (loss: 0.021230235695838928, acc: 0.9944444298744202)
[2025-02-13 20:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35][root][INFO] - Training Epoch: 2/2, step 2508/7134 completed (loss: 0.035526175051927567, acc: 0.9830508232116699)
[2025-02-13 20:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36][root][INFO] - Training Epoch: 2/2, step 2509/7134 completed (loss: 0.029779156669974327, acc: 0.9882352948188782)
[2025-02-13 20:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36][root][INFO] - Training Epoch: 2/2, step 2510/7134 completed (loss: 0.027163591235876083, acc: 0.9928571581840515)
[2025-02-13 20:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36][root][INFO] - Training Epoch: 2/2, step 2511/7134 completed (loss: 0.11696360260248184, acc: 0.9726775884628296)
[2025-02-13 20:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37][root][INFO] - Training Epoch: 2/2, step 2512/7134 completed (loss: 0.09130667895078659, acc: 0.9764705896377563)
[2025-02-13 20:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37][root][INFO] - Training Epoch: 2/2, step 2513/7134 completed (loss: 0.03219348564743996, acc: 1.0)
[2025-02-13 20:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38][root][INFO] - Training Epoch: 2/2, step 2514/7134 completed (loss: 0.07214875519275665, acc: 0.9808917045593262)
[2025-02-13 20:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38][root][INFO] - Training Epoch: 2/2, step 2515/7134 completed (loss: 0.04519590362906456, acc: 0.9870129823684692)
[2025-02-13 20:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38][root][INFO] - Training Epoch: 2/2, step 2516/7134 completed (loss: 0.09317527711391449, acc: 0.9888268113136292)
[2025-02-13 20:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39][root][INFO] - Training Epoch: 2/2, step 2517/7134 completed (loss: 0.08858238905668259, acc: 0.9833333492279053)
[2025-02-13 20:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39][root][INFO] - Training Epoch: 2/2, step 2518/7134 completed (loss: 0.04221625253558159, acc: 0.9869281053543091)
[2025-02-13 20:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40][root][INFO] - Training Epoch: 2/2, step 2519/7134 completed (loss: 0.01174253225326538, acc: 0.9935483932495117)
[2025-02-13 20:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40][root][INFO] - Training Epoch: 2/2, step 2520/7134 completed (loss: 0.0315842404961586, acc: 0.9934210777282715)
[2025-02-13 20:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40][root][INFO] - Training Epoch: 2/2, step 2521/7134 completed (loss: 0.09677775949239731, acc: 0.9890109896659851)
[2025-02-13 20:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41][root][INFO] - Training Epoch: 2/2, step 2522/7134 completed (loss: 0.041194476187229156, acc: 0.9931034445762634)
[2025-02-13 20:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41][root][INFO] - Training Epoch: 2/2, step 2523/7134 completed (loss: 0.097249336540699, acc: 0.9815950989723206)
[2025-02-13 20:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41][root][INFO] - Training Epoch: 2/2, step 2524/7134 completed (loss: 0.03241449594497681, acc: 0.9878048896789551)
[2025-02-13 20:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:42][root][INFO] - Training Epoch: 2/2, step 2525/7134 completed (loss: 0.06514962762594223, acc: 0.9794520735740662)
[2025-02-13 20:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:42][root][INFO] - Training Epoch: 2/2, step 2526/7134 completed (loss: 0.008621168322861195, acc: 1.0)
[2025-02-13 20:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43][root][INFO] - Training Epoch: 2/2, step 2527/7134 completed (loss: 0.025494113564491272, acc: 1.0)
[2025-02-13 20:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43][root][INFO] - Training Epoch: 2/2, step 2528/7134 completed (loss: 0.027557414025068283, acc: 0.9879518151283264)
[2025-02-13 20:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43][root][INFO] - Training Epoch: 2/2, step 2529/7134 completed (loss: 0.04347870871424675, acc: 0.988095223903656)
[2025-02-13 20:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44][root][INFO] - Training Epoch: 2/2, step 2530/7134 completed (loss: 0.0481424443423748, acc: 0.983146071434021)
[2025-02-13 20:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44][root][INFO] - Training Epoch: 2/2, step 2531/7134 completed (loss: 0.01632041297852993, acc: 1.0)
[2025-02-13 20:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44][root][INFO] - Training Epoch: 2/2, step 2532/7134 completed (loss: 0.03004462830722332, acc: 0.9933333396911621)
[2025-02-13 20:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45][root][INFO] - Training Epoch: 2/2, step 2533/7134 completed (loss: 0.046726495027542114, acc: 0.9935897588729858)
[2025-02-13 20:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45][root][INFO] - Training Epoch: 2/2, step 2534/7134 completed (loss: 0.08596759289503098, acc: 0.9832402467727661)
[2025-02-13 20:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46][root][INFO] - Training Epoch: 2/2, step 2535/7134 completed (loss: 0.011340070515871048, acc: 1.0)
[2025-02-13 20:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46][root][INFO] - Training Epoch: 2/2, step 2536/7134 completed (loss: 0.04528943821787834, acc: 0.9948186278343201)
[2025-02-13 20:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46][root][INFO] - Training Epoch: 2/2, step 2537/7134 completed (loss: 0.08497779071331024, acc: 0.965753436088562)
[2025-02-13 20:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47][root][INFO] - Training Epoch: 2/2, step 2538/7134 completed (loss: 0.12285424023866653, acc: 0.977011501789093)
[2025-02-13 20:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47][root][INFO] - Training Epoch: 2/2, step 2539/7134 completed (loss: 0.06382086127996445, acc: 0.9849624037742615)
[2025-02-13 20:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47][root][INFO] - Training Epoch: 2/2, step 2540/7134 completed (loss: 0.06528124958276749, acc: 0.9801324605941772)
[2025-02-13 20:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48][root][INFO] - Training Epoch: 2/2, step 2541/7134 completed (loss: 0.018031753599643707, acc: 1.0)
[2025-02-13 20:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48][root][INFO] - Training Epoch: 2/2, step 2542/7134 completed (loss: 0.06959360092878342, acc: 0.987500011920929)
[2025-02-13 20:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49][root][INFO] - Training Epoch: 2/2, step 2543/7134 completed (loss: 0.11807797849178314, acc: 0.9748427867889404)
[2025-02-13 20:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49][root][INFO] - Training Epoch: 2/2, step 2544/7134 completed (loss: 0.10717979073524475, acc: 0.9702380895614624)
[2025-02-13 20:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49][root][INFO] - Training Epoch: 2/2, step 2545/7134 completed (loss: 0.016294533386826515, acc: 1.0)
[2025-02-13 20:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50][root][INFO] - Training Epoch: 2/2, step 2546/7134 completed (loss: 0.195029616355896, acc: 0.9489796161651611)
[2025-02-13 20:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50][root][INFO] - Training Epoch: 2/2, step 2547/7134 completed (loss: 0.1354677379131317, acc: 0.9640287756919861)
[2025-02-13 20:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51][root][INFO] - Training Epoch: 2/2, step 2548/7134 completed (loss: 0.0802827924489975, acc: 0.991150438785553)
[2025-02-13 20:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51][root][INFO] - Training Epoch: 2/2, step 2549/7134 completed (loss: 0.0327993743121624, acc: 1.0)
[2025-02-13 20:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51][root][INFO] - Training Epoch: 2/2, step 2550/7134 completed (loss: 0.022681601345539093, acc: 0.9863013625144958)
[2025-02-13 20:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52][root][INFO] - Training Epoch: 2/2, step 2551/7134 completed (loss: 0.14466094970703125, acc: 0.9637681245803833)
[2025-02-13 20:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52][root][INFO] - Training Epoch: 2/2, step 2552/7134 completed (loss: 0.0294062327593565, acc: 0.9873417615890503)
[2025-02-13 20:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52][root][INFO] - Training Epoch: 2/2, step 2553/7134 completed (loss: 0.07473991811275482, acc: 0.9868420958518982)
[2025-02-13 20:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53][root][INFO] - Training Epoch: 2/2, step 2554/7134 completed (loss: 0.10874611884355545, acc: 0.9763779640197754)
[2025-02-13 20:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53][root][INFO] - Training Epoch: 2/2, step 2555/7134 completed (loss: 0.03634052351117134, acc: 0.9909909963607788)
[2025-02-13 20:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54][root][INFO] - Training Epoch: 2/2, step 2556/7134 completed (loss: 0.08945612609386444, acc: 0.9900000095367432)
[2025-02-13 20:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54][root][INFO] - Training Epoch: 2/2, step 2557/7134 completed (loss: 0.12189683318138123, acc: 0.9795918464660645)
[2025-02-13 20:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54][root][INFO] - Training Epoch: 2/2, step 2558/7134 completed (loss: 0.020297260954976082, acc: 1.0)
[2025-02-13 20:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55][root][INFO] - Training Epoch: 2/2, step 2559/7134 completed (loss: 0.03546352684497833, acc: 0.9937499761581421)
[2025-02-13 20:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55][root][INFO] - Training Epoch: 2/2, step 2560/7134 completed (loss: 0.0546465702354908, acc: 0.9797297120094299)
[2025-02-13 20:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55][root][INFO] - Training Epoch: 2/2, step 2561/7134 completed (loss: 0.025303438305854797, acc: 1.0)
[2025-02-13 20:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:56][root][INFO] - Training Epoch: 2/2, step 2562/7134 completed (loss: 0.022740615531802177, acc: 0.9919999837875366)
[2025-02-13 20:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:56][root][INFO] - Training Epoch: 2/2, step 2563/7134 completed (loss: 0.04831279069185257, acc: 0.9940828680992126)
[2025-02-13 20:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57][root][INFO] - Training Epoch: 2/2, step 2564/7134 completed (loss: 0.17789599299430847, acc: 0.9496402740478516)
[2025-02-13 20:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57][root][INFO] - Training Epoch: 2/2, step 2565/7134 completed (loss: 0.02948959916830063, acc: 0.9928057789802551)
[2025-02-13 20:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57][root][INFO] - Training Epoch: 2/2, step 2566/7134 completed (loss: 0.05803953483700752, acc: 0.9879518151283264)
[2025-02-13 20:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58][root][INFO] - Training Epoch: 2/2, step 2567/7134 completed (loss: 0.04436726123094559, acc: 0.9849624037742615)
[2025-02-13 20:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58][root][INFO] - Training Epoch: 2/2, step 2568/7134 completed (loss: 0.09389396756887436, acc: 0.9807692170143127)
[2025-02-13 20:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58][root][INFO] - Training Epoch: 2/2, step 2569/7134 completed (loss: 0.02132171392440796, acc: 1.0)
[2025-02-13 20:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59][root][INFO] - Training Epoch: 2/2, step 2570/7134 completed (loss: 0.011894789524376392, acc: 1.0)
[2025-02-13 20:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59][root][INFO] - Training Epoch: 2/2, step 2571/7134 completed (loss: 0.0773647129535675, acc: 0.9672130942344666)
[2025-02-13 20:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59][root][INFO] - Training Epoch: 2/2, step 2572/7134 completed (loss: 0.021519258618354797, acc: 1.0)
[2025-02-13 20:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:00][root][INFO] - Training Epoch: 2/2, step 2573/7134 completed (loss: 0.05598396435379982, acc: 0.9928057789802551)
[2025-02-13 20:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:00][root][INFO] - Training Epoch: 2/2, step 2574/7134 completed (loss: 0.038387615233659744, acc: 0.9898989796638489)
[2025-02-13 20:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01][root][INFO] - Training Epoch: 2/2, step 2575/7134 completed (loss: 0.10930906981229782, acc: 0.9754098653793335)
[2025-02-13 20:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01][root][INFO] - Training Epoch: 2/2, step 2576/7134 completed (loss: 0.02077564224600792, acc: 1.0)
[2025-02-13 20:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01][root][INFO] - Training Epoch: 2/2, step 2577/7134 completed (loss: 0.15344254672527313, acc: 0.9921259880065918)
[2025-02-13 20:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02][root][INFO] - Training Epoch: 2/2, step 2578/7134 completed (loss: 0.1569080799818039, acc: 0.9673202633857727)
[2025-02-13 20:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02][root][INFO] - Training Epoch: 2/2, step 2579/7134 completed (loss: 0.14301766455173492, acc: 0.9679487347602844)
[2025-02-13 20:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02][root][INFO] - Training Epoch: 2/2, step 2580/7134 completed (loss: 0.08004488050937653, acc: 0.9826589822769165)
[2025-02-13 20:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03][root][INFO] - Training Epoch: 2/2, step 2581/7134 completed (loss: 0.21545018255710602, acc: 0.9604519605636597)
[2025-02-13 20:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03][root][INFO] - Training Epoch: 2/2, step 2582/7134 completed (loss: 0.12381691485643387, acc: 0.9774011373519897)
[2025-02-13 20:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03][root][INFO] - Training Epoch: 2/2, step 2583/7134 completed (loss: 0.16661566495895386, acc: 0.931034505367279)
[2025-02-13 20:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04][root][INFO] - Training Epoch: 2/2, step 2584/7134 completed (loss: 0.173951655626297, acc: 0.9503546357154846)
[2025-02-13 20:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04][root][INFO] - Training Epoch: 2/2, step 2585/7134 completed (loss: 0.1000160425901413, acc: 0.9801324605941772)
[2025-02-13 20:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05][root][INFO] - Training Epoch: 2/2, step 2586/7134 completed (loss: 0.16300733387470245, acc: 0.9692307710647583)
[2025-02-13 20:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05][root][INFO] - Training Epoch: 2/2, step 2587/7134 completed (loss: 0.07197238504886627, acc: 0.9750000238418579)
[2025-02-13 20:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05][root][INFO] - Training Epoch: 2/2, step 2588/7134 completed (loss: 0.05752824246883392, acc: 0.9863945841789246)
[2025-02-13 20:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06][root][INFO] - Training Epoch: 2/2, step 2589/7134 completed (loss: 0.13625235855579376, acc: 0.9523809552192688)
[2025-02-13 20:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06][root][INFO] - Training Epoch: 2/2, step 2590/7134 completed (loss: 0.09522966295480728, acc: 0.9629629850387573)
[2025-02-13 20:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06][root][INFO] - Training Epoch: 2/2, step 2591/7134 completed (loss: 0.1705358922481537, acc: 0.9532163739204407)
[2025-02-13 20:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07][root][INFO] - Training Epoch: 2/2, step 2592/7134 completed (loss: 0.223329558968544, acc: 0.9441340565681458)
[2025-02-13 20:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07][root][INFO] - Training Epoch: 2/2, step 2593/7134 completed (loss: 0.07654263079166412, acc: 0.9939024448394775)
[2025-02-13 20:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08][root][INFO] - Training Epoch: 2/2, step 2594/7134 completed (loss: 0.11412589251995087, acc: 0.9607843160629272)
[2025-02-13 20:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08][root][INFO] - Training Epoch: 2/2, step 2595/7134 completed (loss: 0.0785137489438057, acc: 0.9817073345184326)
[2025-02-13 20:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08][root][INFO] - Training Epoch: 2/2, step 2596/7134 completed (loss: 0.08136207610368729, acc: 0.9823529124259949)
[2025-02-13 20:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09][root][INFO] - Training Epoch: 2/2, step 2597/7134 completed (loss: 0.10800833255052567, acc: 0.9717513918876648)
[2025-02-13 20:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09][root][INFO] - Training Epoch: 2/2, step 2598/7134 completed (loss: 0.147967129945755, acc: 0.9602649211883545)
[2025-02-13 20:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09][root][INFO] - Training Epoch: 2/2, step 2599/7134 completed (loss: 0.021012848243117332, acc: 0.9937106966972351)
[2025-02-13 20:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10][root][INFO] - Training Epoch: 2/2, step 2600/7134 completed (loss: 0.14038729667663574, acc: 0.9775280952453613)
[2025-02-13 20:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10][root][INFO] - Training Epoch: 2/2, step 2601/7134 completed (loss: 0.1453876942396164, acc: 0.948387086391449)
[2025-02-13 20:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11][root][INFO] - Training Epoch: 2/2, step 2602/7134 completed (loss: 0.08337663859128952, acc: 0.988095223903656)
[2025-02-13 20:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11][root][INFO] - Training Epoch: 2/2, step 2603/7134 completed (loss: 0.11881788074970245, acc: 0.9673202633857727)
[2025-02-13 20:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11][root][INFO] - Training Epoch: 2/2, step 2604/7134 completed (loss: 0.07991019636392593, acc: 0.9735449552536011)
[2025-02-13 20:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12][root][INFO] - Training Epoch: 2/2, step 2605/7134 completed (loss: 0.1124536320567131, acc: 0.9469026327133179)
[2025-02-13 20:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12][root][INFO] - Training Epoch: 2/2, step 2606/7134 completed (loss: 0.09048904478549957, acc: 0.9800000190734863)
[2025-02-13 20:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12][root][INFO] - Training Epoch: 2/2, step 2607/7134 completed (loss: 0.058313027024269104, acc: 0.983146071434021)
[2025-02-13 20:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:13][root][INFO] - Training Epoch: 2/2, step 2608/7134 completed (loss: 0.1430404633283615, acc: 0.9629629850387573)
[2025-02-13 20:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:13][root][INFO] - Training Epoch: 2/2, step 2609/7134 completed (loss: 0.07937180250883102, acc: 0.9738562107086182)
[2025-02-13 20:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14][root][INFO] - Training Epoch: 2/2, step 2610/7134 completed (loss: 0.13672977685928345, acc: 0.9640287756919861)
[2025-02-13 20:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14][root][INFO] - Training Epoch: 2/2, step 2611/7134 completed (loss: 0.20372989773750305, acc: 0.9441340565681458)
[2025-02-13 20:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14][root][INFO] - Training Epoch: 2/2, step 2612/7134 completed (loss: 0.1552315354347229, acc: 0.949999988079071)
[2025-02-13 20:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15][root][INFO] - Training Epoch: 2/2, step 2613/7134 completed (loss: 0.10586019605398178, acc: 0.9702380895614624)
[2025-02-13 20:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15][root][INFO] - Training Epoch: 2/2, step 2614/7134 completed (loss: 0.16750212013721466, acc: 0.9521276354789734)
[2025-02-13 20:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16][root][INFO] - Training Epoch: 2/2, step 2615/7134 completed (loss: 0.08604680001735687, acc: 0.983146071434021)
[2025-02-13 20:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16][root][INFO] - Training Epoch: 2/2, step 2616/7134 completed (loss: 0.11337202042341232, acc: 0.9747899174690247)
[2025-02-13 20:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16][root][INFO] - Training Epoch: 2/2, step 2617/7134 completed (loss: 0.2262839674949646, acc: 0.9441624283790588)
[2025-02-13 20:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17][root][INFO] - Training Epoch: 2/2, step 2618/7134 completed (loss: 0.052267517894506454, acc: 0.9914529919624329)
[2025-02-13 20:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17][root][INFO] - Training Epoch: 2/2, step 2619/7134 completed (loss: 0.040915295481681824, acc: 0.9927536249160767)
[2025-02-13 20:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17][root][INFO] - Training Epoch: 2/2, step 2620/7134 completed (loss: 0.07251483201980591, acc: 0.9894179701805115)
[2025-02-13 20:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18][root][INFO] - Training Epoch: 2/2, step 2621/7134 completed (loss: 0.15224166214466095, acc: 0.9679144620895386)
[2025-02-13 20:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18][root][INFO] - Training Epoch: 2/2, step 2622/7134 completed (loss: 0.14395646750926971, acc: 0.9589040875434875)
[2025-02-13 20:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18][root][INFO] - Training Epoch: 2/2, step 2623/7134 completed (loss: 0.13749435544013977, acc: 0.963302731513977)
[2025-02-13 20:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19][root][INFO] - Training Epoch: 2/2, step 2624/7134 completed (loss: 0.2045789211988449, acc: 0.9333333373069763)
[2025-02-13 20:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19][root][INFO] - Training Epoch: 2/2, step 2625/7134 completed (loss: 0.10036717355251312, acc: 0.9702380895614624)
[2025-02-13 20:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20][root][INFO] - Training Epoch: 2/2, step 2626/7134 completed (loss: 0.05438830330967903, acc: 0.9942528605461121)
[2025-02-13 20:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20][root][INFO] - Training Epoch: 2/2, step 2627/7134 completed (loss: 0.07417508214712143, acc: 0.9815950989723206)
[2025-02-13 20:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20][root][INFO] - Training Epoch: 2/2, step 2628/7134 completed (loss: 0.13275977969169617, acc: 0.9666666388511658)
[2025-02-13 20:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21][root][INFO] - Training Epoch: 2/2, step 2629/7134 completed (loss: 0.048129111528396606, acc: 0.9880239367485046)
[2025-02-13 20:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21][root][INFO] - Training Epoch: 2/2, step 2630/7134 completed (loss: 0.0927603617310524, acc: 0.9637681245803833)
[2025-02-13 20:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22][root][INFO] - Training Epoch: 2/2, step 2631/7134 completed (loss: 0.10009396821260452, acc: 0.9695431590080261)
[2025-02-13 20:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22][root][INFO] - Training Epoch: 2/2, step 2632/7134 completed (loss: 0.025814613327383995, acc: 0.991304337978363)
[2025-02-13 20:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22][root][INFO] - Training Epoch: 2/2, step 2633/7134 completed (loss: 0.15313299000263214, acc: 0.9615384340286255)
[2025-02-13 20:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23][root][INFO] - Training Epoch: 2/2, step 2634/7134 completed (loss: 0.027082713320851326, acc: 0.9940828680992126)
[2025-02-13 20:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23][root][INFO] - Training Epoch: 2/2, step 2635/7134 completed (loss: 0.14458106458187103, acc: 0.9611111283302307)
[2025-02-13 20:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23][root][INFO] - Training Epoch: 2/2, step 2636/7134 completed (loss: 0.16541284322738647, acc: 0.9608938694000244)
[2025-02-13 20:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:24][root][INFO] - Training Epoch: 2/2, step 2637/7134 completed (loss: 0.29501640796661377, acc: 0.9202454090118408)
[2025-02-13 20:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:24][root][INFO] - Training Epoch: 2/2, step 2638/7134 completed (loss: 0.21581779420375824, acc: 0.9402984976768494)
[2025-02-13 20:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25][root][INFO] - Training Epoch: 2/2, step 2639/7134 completed (loss: 0.32676783204078674, acc: 0.9248120188713074)
[2025-02-13 20:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25][root][INFO] - Training Epoch: 2/2, step 2640/7134 completed (loss: 0.35390543937683105, acc: 0.9402984976768494)
[2025-02-13 20:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25][root][INFO] - Training Epoch: 2/2, step 2641/7134 completed (loss: 0.1501055210828781, acc: 0.9599999785423279)
[2025-02-13 20:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26][root][INFO] - Training Epoch: 2/2, step 2642/7134 completed (loss: 0.432407021522522, acc: 0.9047619104385376)
[2025-02-13 20:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26][root][INFO] - Training Epoch: 2/2, step 2643/7134 completed (loss: 0.2732669711112976, acc: 0.9476743936538696)
[2025-02-13 20:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27][root][INFO] - Training Epoch: 2/2, step 2644/7134 completed (loss: 0.18681471049785614, acc: 0.9629629850387573)
[2025-02-13 20:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27][root][INFO] - Training Epoch: 2/2, step 2645/7134 completed (loss: 0.14502079784870148, acc: 0.954285740852356)
[2025-02-13 20:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27][root][INFO] - Training Epoch: 2/2, step 2646/7134 completed (loss: 0.23762966692447662, acc: 0.9378882050514221)
[2025-02-13 20:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28][root][INFO] - Training Epoch: 2/2, step 2647/7134 completed (loss: 0.17609666287899017, acc: 0.95652174949646)
[2025-02-13 20:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28][root][INFO] - Training Epoch: 2/2, step 2648/7134 completed (loss: 0.2646183669567108, acc: 0.9306930899620056)
[2025-02-13 20:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29][root][INFO] - Training Epoch: 2/2, step 2649/7134 completed (loss: 0.2761240303516388, acc: 0.908108115196228)
[2025-02-13 20:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29][root][INFO] - Training Epoch: 2/2, step 2650/7134 completed (loss: 0.24863873422145844, acc: 0.929411768913269)
[2025-02-13 20:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29][root][INFO] - Training Epoch: 2/2, step 2651/7134 completed (loss: 0.10736316442489624, acc: 0.9756097793579102)
[2025-02-13 20:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30][root][INFO] - Training Epoch: 2/2, step 2652/7134 completed (loss: 0.06245328485965729, acc: 0.9887640476226807)
[2025-02-13 20:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30][root][INFO] - Training Epoch: 2/2, step 2653/7134 completed (loss: 0.05556800961494446, acc: 0.9934640526771545)
[2025-02-13 20:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30][root][INFO] - Training Epoch: 2/2, step 2654/7134 completed (loss: 0.040451791137456894, acc: 0.9857142567634583)
[2025-02-13 20:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31][root][INFO] - Training Epoch: 2/2, step 2655/7134 completed (loss: 0.08726655691862106, acc: 0.9751552939414978)
[2025-02-13 20:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31][root][INFO] - Training Epoch: 2/2, step 2656/7134 completed (loss: 0.1814921349287033, acc: 0.9651162624359131)
[2025-02-13 20:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31][root][INFO] - Training Epoch: 2/2, step 2657/7134 completed (loss: 0.05923281982541084, acc: 0.9873417615890503)
[2025-02-13 20:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32][root][INFO] - Training Epoch: 2/2, step 2658/7134 completed (loss: 0.2627980709075928, acc: 0.939393937587738)
[2025-02-13 20:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32][root][INFO] - Training Epoch: 2/2, step 2659/7134 completed (loss: 0.18360166251659393, acc: 0.9477611780166626)
[2025-02-13 20:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33][root][INFO] - Training Epoch: 2/2, step 2660/7134 completed (loss: 0.17180779576301575, acc: 0.9658119678497314)
[2025-02-13 20:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33][root][INFO] - Training Epoch: 2/2, step 2661/7134 completed (loss: 0.03638608381152153, acc: 0.9940828680992126)
[2025-02-13 20:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33][root][INFO] - Training Epoch: 2/2, step 2662/7134 completed (loss: 0.05652377009391785, acc: 0.9814814925193787)
[2025-02-13 20:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34][root][INFO] - Training Epoch: 2/2, step 2663/7134 completed (loss: 0.08591345697641373, acc: 0.9740932583808899)
[2025-02-13 20:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34][root][INFO] - Training Epoch: 2/2, step 2664/7134 completed (loss: 0.0676824301481247, acc: 0.9883720874786377)
[2025-02-13 20:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35][root][INFO] - Training Epoch: 2/2, step 2665/7134 completed (loss: 0.03844771161675453, acc: 0.9939758777618408)
[2025-02-13 20:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35][root][INFO] - Training Epoch: 2/2, step 2666/7134 completed (loss: 0.016570627689361572, acc: 0.9941860437393188)
[2025-02-13 20:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35][root][INFO] - Training Epoch: 2/2, step 2667/7134 completed (loss: 0.03129943832755089, acc: 0.9936708807945251)
[2025-02-13 20:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36][root][INFO] - Training Epoch: 2/2, step 2668/7134 completed (loss: 0.011122827418148518, acc: 1.0)
[2025-02-13 20:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36][root][INFO] - Training Epoch: 2/2, step 2669/7134 completed (loss: 0.009109525941312313, acc: 1.0)
[2025-02-13 20:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36][root][INFO] - Training Epoch: 2/2, step 2670/7134 completed (loss: 0.053248949348926544, acc: 0.9828571677207947)
[2025-02-13 20:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37][root][INFO] - Training Epoch: 2/2, step 2671/7134 completed (loss: 0.05006275698542595, acc: 0.9828571677207947)
[2025-02-13 20:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37][root][INFO] - Training Epoch: 2/2, step 2672/7134 completed (loss: 0.030962156131863594, acc: 0.9939758777618408)
[2025-02-13 20:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38][root][INFO] - Training Epoch: 2/2, step 2673/7134 completed (loss: 0.018810104578733444, acc: 1.0)
[2025-02-13 20:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38][root][INFO] - Training Epoch: 2/2, step 2674/7134 completed (loss: 0.06810571998357773, acc: 0.9938271641731262)
[2025-02-13 20:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38][root][INFO] - Training Epoch: 2/2, step 2675/7134 completed (loss: 0.014297768473625183, acc: 1.0)
[2025-02-13 20:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39][root][INFO] - Training Epoch: 2/2, step 2676/7134 completed (loss: 0.02783256582915783, acc: 0.9886363744735718)
[2025-02-13 20:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39][root][INFO] - Training Epoch: 2/2, step 2677/7134 completed (loss: 0.024219345301389694, acc: 0.9938650131225586)
[2025-02-13 20:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39][root][INFO] - Training Epoch: 2/2, step 2678/7134 completed (loss: 0.012575282715260983, acc: 1.0)
[2025-02-13 20:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40][root][INFO] - Training Epoch: 2/2, step 2679/7134 completed (loss: 0.01347876712679863, acc: 1.0)
[2025-02-13 20:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40][root][INFO] - Training Epoch: 2/2, step 2680/7134 completed (loss: 0.04666643217206001, acc: 0.9780219793319702)
[2025-02-13 20:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40][root][INFO] - Training Epoch: 2/2, step 2681/7134 completed (loss: 0.012523040175437927, acc: 1.0)
[2025-02-13 20:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41][root][INFO] - Training Epoch: 2/2, step 2682/7134 completed (loss: 0.0350622721016407, acc: 0.9865771532058716)
[2025-02-13 20:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41][root][INFO] - Training Epoch: 2/2, step 2683/7134 completed (loss: 0.011458809487521648, acc: 1.0)
[2025-02-13 20:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41][root][INFO] - Training Epoch: 2/2, step 2684/7134 completed (loss: 0.09860671311616898, acc: 0.9921875)
[2025-02-13 20:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42][root][INFO] - Training Epoch: 2/2, step 2685/7134 completed (loss: 0.027461867779493332, acc: 1.0)
[2025-02-13 20:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42][root][INFO] - Training Epoch: 2/2, step 2686/7134 completed (loss: 0.13726940751075745, acc: 0.9599999785423279)
[2025-02-13 20:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43][root][INFO] - Training Epoch: 2/2, step 2687/7134 completed (loss: 0.0632500872015953, acc: 0.9831932783126831)
[2025-02-13 20:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43][root][INFO] - Training Epoch: 2/2, step 2688/7134 completed (loss: 0.1172771155834198, acc: 0.9624060392379761)
[2025-02-13 20:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43][root][INFO] - Training Epoch: 2/2, step 2689/7134 completed (loss: 0.09614215046167374, acc: 0.9837398529052734)
[2025-02-13 20:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44][root][INFO] - Training Epoch: 2/2, step 2690/7134 completed (loss: 0.0981319472193718, acc: 0.9784172773361206)
[2025-02-13 20:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44][root][INFO] - Training Epoch: 2/2, step 2691/7134 completed (loss: 0.1665487140417099, acc: 0.9469026327133179)
[2025-02-13 20:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44][root][INFO] - Training Epoch: 2/2, step 2692/7134 completed (loss: 0.1869201362133026, acc: 0.942148745059967)
[2025-02-13 20:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45][root][INFO] - Training Epoch: 2/2, step 2693/7134 completed (loss: 0.1566895693540573, acc: 0.9734513163566589)
[2025-02-13 20:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45][root][INFO] - Training Epoch: 2/2, step 2694/7134 completed (loss: 0.17759107053279877, acc: 0.9735099077224731)
[2025-02-13 20:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45][root][INFO] - Training Epoch: 2/2, step 2695/7134 completed (loss: 0.07365907728672028, acc: 0.970370352268219)
[2025-02-13 20:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46][root][INFO] - Training Epoch: 2/2, step 2696/7134 completed (loss: 0.1320313662290573, acc: 0.9642857313156128)
[2025-02-13 20:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46][root][INFO] - Training Epoch: 2/2, step 2697/7134 completed (loss: 0.1680176854133606, acc: 0.9610389471054077)
[2025-02-13 20:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46][root][INFO] - Training Epoch: 2/2, step 2698/7134 completed (loss: 0.10471721738576889, acc: 0.9659863710403442)
[2025-02-13 20:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47][root][INFO] - Training Epoch: 2/2, step 2699/7134 completed (loss: 0.0770149901509285, acc: 0.9741379022598267)
[2025-02-13 20:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47][root][INFO] - Training Epoch: 2/2, step 2700/7134 completed (loss: 0.07372809201478958, acc: 0.9857142567634583)
[2025-02-13 20:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48][root][INFO] - Training Epoch: 2/2, step 2701/7134 completed (loss: 0.11870437115430832, acc: 0.969924807548523)
[2025-02-13 20:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48][root][INFO] - Training Epoch: 2/2, step 2702/7134 completed (loss: 0.13893458247184753, acc: 0.9814814925193787)
[2025-02-13 20:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48][root][INFO] - Training Epoch: 2/2, step 2703/7134 completed (loss: 0.03813907131552696, acc: 0.9866666793823242)
[2025-02-13 20:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49][root][INFO] - Training Epoch: 2/2, step 2704/7134 completed (loss: 0.07733836770057678, acc: 0.988095223903656)
[2025-02-13 20:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49][root][INFO] - Training Epoch: 2/2, step 2705/7134 completed (loss: 0.1232042983174324, acc: 0.9718309640884399)
[2025-02-13 20:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49][root][INFO] - Training Epoch: 2/2, step 2706/7134 completed (loss: 0.09603758901357651, acc: 0.9794520735740662)
[2025-02-13 20:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50][root][INFO] - Training Epoch: 2/2, step 2707/7134 completed (loss: 0.13521911203861237, acc: 0.9583333134651184)
[2025-02-13 20:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50][root][INFO] - Training Epoch: 2/2, step 2708/7134 completed (loss: 0.09161845594644547, acc: 0.9828571677207947)
[2025-02-13 20:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50][root][INFO] - Training Epoch: 2/2, step 2709/7134 completed (loss: 0.04044753313064575, acc: 0.9929577708244324)
[2025-02-13 20:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51][root][INFO] - Training Epoch: 2/2, step 2710/7134 completed (loss: 0.058940671384334564, acc: 0.9900000095367432)
[2025-02-13 20:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51][root][INFO] - Training Epoch: 2/2, step 2711/7134 completed (loss: 0.022406764328479767, acc: 1.0)
[2025-02-13 20:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52][root][INFO] - Training Epoch: 2/2, step 2712/7134 completed (loss: 0.07002298533916473, acc: 0.978723406791687)
[2025-02-13 20:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52][root][INFO] - Training Epoch: 2/2, step 2713/7134 completed (loss: 0.03838133439421654, acc: 0.987500011920929)
[2025-02-13 20:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52][root][INFO] - Training Epoch: 2/2, step 2714/7134 completed (loss: 0.05886658653616905, acc: 0.9852941036224365)
[2025-02-13 20:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53][root][INFO] - Training Epoch: 2/2, step 2715/7134 completed (loss: 0.03211972862482071, acc: 0.9931034445762634)
[2025-02-13 20:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53][root][INFO] - Training Epoch: 2/2, step 2716/7134 completed (loss: 0.05153590440750122, acc: 0.9883720874786377)
[2025-02-13 20:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53][root][INFO] - Training Epoch: 2/2, step 2717/7134 completed (loss: 0.03602851182222366, acc: 1.0)
[2025-02-13 20:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54][root][INFO] - Training Epoch: 2/2, step 2718/7134 completed (loss: 0.15740469098091125, acc: 0.9513888955116272)
[2025-02-13 20:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54][root][INFO] - Training Epoch: 2/2, step 2719/7134 completed (loss: 0.1273951381444931, acc: 0.971222996711731)
[2025-02-13 20:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55][root][INFO] - Training Epoch: 2/2, step 2720/7134 completed (loss: 0.1687127947807312, acc: 0.956204354763031)
[2025-02-13 20:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55][root][INFO] - Training Epoch: 2/2, step 2721/7134 completed (loss: 0.13581934571266174, acc: 0.9801324605941772)
[2025-02-13 20:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55][root][INFO] - Training Epoch: 2/2, step 2722/7134 completed (loss: 0.10643883794546127, acc: 0.978723406791687)
[2025-02-13 20:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56][root][INFO] - Training Epoch: 2/2, step 2723/7134 completed (loss: 0.26598116755485535, acc: 0.9520958065986633)
[2025-02-13 20:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56][root][INFO] - Training Epoch: 2/2, step 2724/7134 completed (loss: 0.05258197709918022, acc: 0.9774011373519897)
[2025-02-13 20:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56][root][INFO] - Training Epoch: 2/2, step 2725/7134 completed (loss: 0.08139721304178238, acc: 0.9626865386962891)
[2025-02-13 20:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57][root][INFO] - Training Epoch: 2/2, step 2726/7134 completed (loss: 0.12717337906360626, acc: 0.9674796462059021)
[2025-02-13 20:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57][root][INFO] - Training Epoch: 2/2, step 2727/7134 completed (loss: 0.03309169039130211, acc: 0.9912280440330505)
[2025-02-13 20:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57][root][INFO] - Training Epoch: 2/2, step 2728/7134 completed (loss: 0.5830280780792236, acc: 0.8740741014480591)
[2025-02-13 20:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:58][root][INFO] - Training Epoch: 2/2, step 2729/7134 completed (loss: 0.2857707738876343, acc: 0.949999988079071)
[2025-02-13 20:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:58][root][INFO] - Training Epoch: 2/2, step 2730/7134 completed (loss: 0.18123915791511536, acc: 0.9469026327133179)
[2025-02-13 20:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59][root][INFO] - Training Epoch: 2/2, step 2731/7134 completed (loss: 0.15573081374168396, acc: 0.9514563083648682)
[2025-02-13 20:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59][root][INFO] - Training Epoch: 2/2, step 2732/7134 completed (loss: 0.21997390687465668, acc: 0.936274528503418)
[2025-02-13 20:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59][root][INFO] - Training Epoch: 2/2, step 2733/7134 completed (loss: 0.13608288764953613, acc: 0.9611111283302307)
[2025-02-13 20:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00][root][INFO] - Training Epoch: 2/2, step 2734/7134 completed (loss: 0.19707468152046204, acc: 0.9504132270812988)
[2025-02-13 20:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00][root][INFO] - Training Epoch: 2/2, step 2735/7134 completed (loss: 0.15922243893146515, acc: 0.9756097793579102)
[2025-02-13 20:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00][root][INFO] - Training Epoch: 2/2, step 2736/7134 completed (loss: 0.20612064003944397, acc: 0.9308176040649414)
[2025-02-13 20:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01][root][INFO] - Training Epoch: 2/2, step 2737/7134 completed (loss: 0.15462952852249146, acc: 0.9785714149475098)
[2025-02-13 20:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01][root][INFO] - Training Epoch: 2/2, step 2738/7134 completed (loss: 0.1759873926639557, acc: 0.9580838084220886)
[2025-02-13 20:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02][root][INFO] - Training Epoch: 2/2, step 2739/7134 completed (loss: 0.181313619017601, acc: 0.9420289993286133)
[2025-02-13 20:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02][root][INFO] - Training Epoch: 2/2, step 2740/7134 completed (loss: 0.1054740622639656, acc: 0.9727891087532043)
[2025-02-13 20:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02][root][INFO] - Training Epoch: 2/2, step 2741/7134 completed (loss: 0.07058238238096237, acc: 0.9741379022598267)
[2025-02-13 20:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03][root][INFO] - Training Epoch: 2/2, step 2742/7134 completed (loss: 0.08640501648187637, acc: 0.9747899174690247)
[2025-02-13 20:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03][root][INFO] - Training Epoch: 2/2, step 2743/7134 completed (loss: 0.048071496188640594, acc: 0.978723406791687)
[2025-02-13 20:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03][root][INFO] - Training Epoch: 2/2, step 2744/7134 completed (loss: 0.01953672058880329, acc: 1.0)
[2025-02-13 20:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04][root][INFO] - Training Epoch: 2/2, step 2745/7134 completed (loss: 0.35000863671302795, acc: 0.9107142686843872)
[2025-02-13 20:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04][root][INFO] - Training Epoch: 2/2, step 2746/7134 completed (loss: 0.1577812135219574, acc: 0.9772727489471436)
[2025-02-13 20:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04][root][INFO] - Training Epoch: 2/2, step 2747/7134 completed (loss: 0.17262105643749237, acc: 0.9629629850387573)
[2025-02-13 20:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05][root][INFO] - Training Epoch: 2/2, step 2748/7134 completed (loss: 0.04323224350810051, acc: 0.9870129823684692)
[2025-02-13 20:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05][root][INFO] - Training Epoch: 2/2, step 2749/7134 completed (loss: 0.14960090816020966, acc: 0.9793814420700073)
[2025-02-13 20:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06][root][INFO] - Training Epoch: 2/2, step 2750/7134 completed (loss: 0.13423337042331696, acc: 0.9578313231468201)
[2025-02-13 20:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06][root][INFO] - Training Epoch: 2/2, step 2751/7134 completed (loss: 0.1533680409193039, acc: 0.9639639854431152)
[2025-02-13 20:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06][root][INFO] - Training Epoch: 2/2, step 2752/7134 completed (loss: 0.1132204532623291, acc: 0.9781022071838379)
[2025-02-13 20:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07][root][INFO] - Training Epoch: 2/2, step 2753/7134 completed (loss: 0.2871388792991638, acc: 0.9495798349380493)
[2025-02-13 20:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07][root][INFO] - Training Epoch: 2/2, step 2754/7134 completed (loss: 0.11956901103258133, acc: 0.9750000238418579)
[2025-02-13 20:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07][root][INFO] - Training Epoch: 2/2, step 2755/7134 completed (loss: 0.1035676822066307, acc: 0.9921875)
[2025-02-13 20:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08][root][INFO] - Training Epoch: 2/2, step 2756/7134 completed (loss: 0.07869207113981247, acc: 0.9864864945411682)
[2025-02-13 20:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08][root][INFO] - Training Epoch: 2/2, step 2757/7134 completed (loss: 0.0805509090423584, acc: 0.97826087474823)
[2025-02-13 20:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09][root][INFO] - Training Epoch: 2/2, step 2758/7134 completed (loss: 0.14485079050064087, acc: 0.9590163826942444)
[2025-02-13 20:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09][root][INFO] - Training Epoch: 2/2, step 2759/7134 completed (loss: 0.053586073219776154, acc: 0.9857142567634583)
[2025-02-13 20:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09][root][INFO] - Training Epoch: 2/2, step 2760/7134 completed (loss: 0.09417364001274109, acc: 0.9777777791023254)
[2025-02-13 20:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10][root][INFO] - Training Epoch: 2/2, step 2761/7134 completed (loss: 0.18241992592811584, acc: 0.9578947424888611)
[2025-02-13 20:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10][root][INFO] - Training Epoch: 2/2, step 2762/7134 completed (loss: 0.15598410367965698, acc: 0.9556650519371033)
[2025-02-13 20:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10][root][INFO] - Training Epoch: 2/2, step 2763/7134 completed (loss: 0.09888183325529099, acc: 0.9781420826911926)
[2025-02-13 20:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11][root][INFO] - Training Epoch: 2/2, step 2764/7134 completed (loss: 0.11745131760835648, acc: 0.9739583134651184)
[2025-02-13 20:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11][root][INFO] - Training Epoch: 2/2, step 2765/7134 completed (loss: 0.05655568093061447, acc: 0.9823529124259949)
[2025-02-13 20:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12][root][INFO] - Training Epoch: 2/2, step 2766/7134 completed (loss: 0.021830623969435692, acc: 1.0)
[2025-02-13 20:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12][root][INFO] - Training Epoch: 2/2, step 2767/7134 completed (loss: 0.03242618218064308, acc: 0.9950000047683716)
[2025-02-13 20:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12][root][INFO] - Training Epoch: 2/2, step 2768/7134 completed (loss: 0.059288397431373596, acc: 0.9879518151283264)
[2025-02-13 20:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13][root][INFO] - Training Epoch: 2/2, step 2769/7134 completed (loss: 0.06818733364343643, acc: 0.9736841917037964)
[2025-02-13 20:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13][root][INFO] - Training Epoch: 2/2, step 2770/7134 completed (loss: 0.031499505043029785, acc: 0.9938271641731262)
[2025-02-13 20:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14][root][INFO] - Training Epoch: 2/2, step 2771/7134 completed (loss: 0.14207766950130463, acc: 0.9620853066444397)
[2025-02-13 20:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14][root][INFO] - Training Epoch: 2/2, step 2772/7134 completed (loss: 0.08777135610580444, acc: 0.9700000286102295)
[2025-02-13 20:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14][root][INFO] - Training Epoch: 2/2, step 2773/7134 completed (loss: 0.10985102504491806, acc: 0.9698492288589478)
[2025-02-13 20:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15][root][INFO] - Training Epoch: 2/2, step 2774/7134 completed (loss: 0.15594646334648132, acc: 0.9512194991111755)
[2025-02-13 20:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15][root][INFO] - Training Epoch: 2/2, step 2775/7134 completed (loss: 0.184650719165802, acc: 0.9398906826972961)
[2025-02-13 20:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15][root][INFO] - Training Epoch: 2/2, step 2776/7134 completed (loss: 0.09021047502756119, acc: 0.9826839566230774)
[2025-02-13 20:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16][root][INFO] - Training Epoch: 2/2, step 2777/7134 completed (loss: 0.025735866278409958, acc: 0.9885714054107666)
[2025-02-13 20:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16][root][INFO] - Training Epoch: 2/2, step 2778/7134 completed (loss: 0.12543794512748718, acc: 0.9595959782600403)
[2025-02-13 20:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17][root][INFO] - Training Epoch: 2/2, step 2779/7134 completed (loss: 0.13370868563652039, acc: 0.9634703397750854)
[2025-02-13 20:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17][root][INFO] - Training Epoch: 2/2, step 2780/7134 completed (loss: 0.17671823501586914, acc: 0.9513274431228638)
[2025-02-13 20:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17][root][INFO] - Training Epoch: 2/2, step 2781/7134 completed (loss: 0.07169492542743683, acc: 0.97826087474823)
[2025-02-13 20:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18][root][INFO] - Training Epoch: 2/2, step 2782/7134 completed (loss: 0.07318920642137527, acc: 0.9732142686843872)
[2025-02-13 20:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18][root][INFO] - Training Epoch: 2/2, step 2783/7134 completed (loss: 0.04855727031826973, acc: 0.9795918464660645)
[2025-02-13 20:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18][root][INFO] - Training Epoch: 2/2, step 2784/7134 completed (loss: 0.047961436212062836, acc: 0.9806763529777527)
[2025-02-13 20:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19][root][INFO] - Training Epoch: 2/2, step 2785/7134 completed (loss: 0.07499851286411285, acc: 0.9805825352668762)
[2025-02-13 20:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19][root][INFO] - Training Epoch: 2/2, step 2786/7134 completed (loss: 0.18559399247169495, acc: 0.9333333373069763)
[2025-02-13 20:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20][root][INFO] - Training Epoch: 2/2, step 2787/7134 completed (loss: 0.04349394515156746, acc: 0.9826589822769165)
[2025-02-13 20:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20][root][INFO] - Training Epoch: 2/2, step 2788/7134 completed (loss: 0.14125840365886688, acc: 0.9681528806686401)
[2025-02-13 20:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20][root][INFO] - Training Epoch: 2/2, step 2789/7134 completed (loss: 0.5366693139076233, acc: 0.8666666746139526)
[2025-02-13 20:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21][root][INFO] - Training Epoch: 2/2, step 2790/7134 completed (loss: 0.476826936006546, acc: 0.875)
[2025-02-13 20:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21][root][INFO] - Training Epoch: 2/2, step 2791/7134 completed (loss: 0.24267719686031342, acc: 0.940397322177887)
[2025-02-13 20:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22][root][INFO] - Training Epoch: 2/2, step 2792/7134 completed (loss: 0.13293133676052094, acc: 0.9709302186965942)
[2025-02-13 20:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22][root][INFO] - Training Epoch: 2/2, step 2793/7134 completed (loss: 0.07539626955986023, acc: 0.9820359349250793)
[2025-02-13 20:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22][root][INFO] - Training Epoch: 2/2, step 2794/7134 completed (loss: 0.016270413994789124, acc: 1.0)
[2025-02-13 20:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23][root][INFO] - Training Epoch: 2/2, step 2795/7134 completed (loss: 0.25139784812927246, acc: 0.9259259104728699)
[2025-02-13 20:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23][root][INFO] - Training Epoch: 2/2, step 2796/7134 completed (loss: 0.12234870344400406, acc: 0.9674796462059021)
[2025-02-13 20:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23][root][INFO] - Training Epoch: 2/2, step 2797/7134 completed (loss: 0.07291299104690552, acc: 0.9756097793579102)
[2025-02-13 20:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24][root][INFO] - Training Epoch: 2/2, step 2798/7134 completed (loss: 0.15363077819347382, acc: 0.949999988079071)
[2025-02-13 20:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24][root][INFO] - Training Epoch: 2/2, step 2799/7134 completed (loss: 0.1748102456331253, acc: 0.9296875)
[2025-02-13 20:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24][root][INFO] - Training Epoch: 2/2, step 2800/7134 completed (loss: 0.13637235760688782, acc: 0.9781022071838379)
[2025-02-13 20:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25][root][INFO] - Training Epoch: 2/2, step 2801/7134 completed (loss: 0.17115308344364166, acc: 0.9617486596107483)
[2025-02-13 20:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25][root][INFO] - Training Epoch: 2/2, step 2802/7134 completed (loss: 0.11802776902914047, acc: 0.9698492288589478)
[2025-02-13 20:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26][root][INFO] - Training Epoch: 2/2, step 2803/7134 completed (loss: 0.04932468757033348, acc: 0.9814814925193787)
[2025-02-13 20:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26][root][INFO] - Training Epoch: 2/2, step 2804/7134 completed (loss: 0.09993910044431686, acc: 0.9942196607589722)
[2025-02-13 20:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26][root][INFO] - Training Epoch: 2/2, step 2805/7134 completed (loss: 0.06999543309211731, acc: 0.9767441749572754)
[2025-02-13 20:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27][root][INFO] - Training Epoch: 2/2, step 2806/7134 completed (loss: 0.08684175461530685, acc: 0.9779411554336548)
[2025-02-13 20:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27][root][INFO] - Training Epoch: 2/2, step 2807/7134 completed (loss: 0.1000475287437439, acc: 0.9902912378311157)
[2025-02-13 20:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27][root][INFO] - Training Epoch: 2/2, step 2808/7134 completed (loss: 0.10829365998506546, acc: 0.9698492288589478)
[2025-02-13 20:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28][root][INFO] - Training Epoch: 2/2, step 2809/7134 completed (loss: 0.16928282380104065, acc: 0.960629940032959)
[2025-02-13 20:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28][root][INFO] - Training Epoch: 2/2, step 2810/7134 completed (loss: 0.044027235358953476, acc: 0.9890710115432739)
[2025-02-13 20:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29][root][INFO] - Training Epoch: 2/2, step 2811/7134 completed (loss: 0.05328042432665825, acc: 0.988950252532959)
[2025-02-13 20:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29][root][INFO] - Training Epoch: 2/2, step 2812/7134 completed (loss: 0.030561452731490135, acc: 1.0)
[2025-02-13 20:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29][root][INFO] - Training Epoch: 2/2, step 2813/7134 completed (loss: 0.09819873422384262, acc: 0.9885057210922241)
[2025-02-13 20:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30][root][INFO] - Training Epoch: 2/2, step 2814/7134 completed (loss: 0.16505950689315796, acc: 0.9709302186965942)
[2025-02-13 20:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30][root][INFO] - Training Epoch: 2/2, step 2815/7134 completed (loss: 0.07919833809137344, acc: 0.9835164546966553)
[2025-02-13 20:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30][root][INFO] - Training Epoch: 2/2, step 2816/7134 completed (loss: 0.06457086652517319, acc: 0.9869281053543091)
[2025-02-13 20:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31][root][INFO] - Training Epoch: 2/2, step 2817/7134 completed (loss: 0.06699034571647644, acc: 0.9847715497016907)
[2025-02-13 20:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31][root][INFO] - Training Epoch: 2/2, step 2818/7134 completed (loss: 0.053626649081707, acc: 0.9946808218955994)
[2025-02-13 20:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31][root][INFO] - Training Epoch: 2/2, step 2819/7134 completed (loss: 0.06481441855430603, acc: 0.9863013625144958)
[2025-02-13 20:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:32][root][INFO] - Training Epoch: 2/2, step 2820/7134 completed (loss: 0.2627902925014496, acc: 0.9386503100395203)
[2025-02-13 20:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:32][root][INFO] - Training Epoch: 2/2, step 2821/7134 completed (loss: 0.13646523654460907, acc: 0.9689119458198547)
[2025-02-13 20:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33][root][INFO] - Training Epoch: 2/2, step 2822/7134 completed (loss: 0.14158247411251068, acc: 0.9675675630569458)
[2025-02-13 20:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33][root][INFO] - Training Epoch: 2/2, step 2823/7134 completed (loss: 0.10081931948661804, acc: 0.9813664555549622)
[2025-02-13 20:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33][root][INFO] - Training Epoch: 2/2, step 2824/7134 completed (loss: 0.22409871220588684, acc: 0.9308176040649414)
[2025-02-13 20:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34][root][INFO] - Training Epoch: 2/2, step 2825/7134 completed (loss: 0.10177439451217651, acc: 0.9777777791023254)
[2025-02-13 20:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34][root][INFO] - Training Epoch: 2/2, step 2826/7134 completed (loss: 0.10981401056051254, acc: 0.977011501789093)
[2025-02-13 20:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34][root][INFO] - Training Epoch: 2/2, step 2827/7134 completed (loss: 0.10575374215841293, acc: 0.9904305934906006)
[2025-02-13 20:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35][root][INFO] - Training Epoch: 2/2, step 2828/7134 completed (loss: 0.1061965748667717, acc: 0.9746835231781006)
[2025-02-13 20:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35][root][INFO] - Training Epoch: 2/2, step 2829/7134 completed (loss: 0.12283492088317871, acc: 0.9583333134651184)
[2025-02-13 20:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36][root][INFO] - Training Epoch: 2/2, step 2830/7134 completed (loss: 0.04972356557846069, acc: 0.9701492786407471)
[2025-02-13 20:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36][root][INFO] - Training Epoch: 2/2, step 2831/7134 completed (loss: 0.060448408126831055, acc: 0.9797297120094299)
[2025-02-13 20:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36][root][INFO] - Training Epoch: 2/2, step 2832/7134 completed (loss: 0.1426895707845688, acc: 0.9784946441650391)
[2025-02-13 20:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37][root][INFO] - Training Epoch: 2/2, step 2833/7134 completed (loss: 0.1606634110212326, acc: 0.9841269850730896)
[2025-02-13 20:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37][root][INFO] - Training Epoch: 2/2, step 2834/7134 completed (loss: 0.1648804396390915, acc: 0.9611111283302307)
[2025-02-13 20:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37][root][INFO] - Training Epoch: 2/2, step 2835/7134 completed (loss: 0.023373117670416832, acc: 0.9934210777282715)
[2025-02-13 20:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:38][root][INFO] - Training Epoch: 2/2, step 2836/7134 completed (loss: 0.17375335097312927, acc: 0.9700000286102295)
[2025-02-13 20:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:38][root][INFO] - Training Epoch: 2/2, step 2837/7134 completed (loss: 0.04584962874650955, acc: 0.9824561476707458)
[2025-02-13 20:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39][root][INFO] - Training Epoch: 2/2, step 2838/7134 completed (loss: 0.1359047293663025, acc: 0.9704433679580688)
[2025-02-13 20:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39][root][INFO] - Training Epoch: 2/2, step 2839/7134 completed (loss: 0.15887562930583954, acc: 0.9850746393203735)
[2025-02-13 20:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39][root][INFO] - Training Epoch: 2/2, step 2840/7134 completed (loss: 0.05809012055397034, acc: 0.9882352948188782)
[2025-02-13 20:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40][root][INFO] - Training Epoch: 2/2, step 2841/7134 completed (loss: 0.06278099864721298, acc: 0.9905660152435303)
[2025-02-13 20:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40][root][INFO] - Training Epoch: 2/2, step 2842/7134 completed (loss: 0.12315502762794495, acc: 0.9791666865348816)
[2025-02-13 20:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40][root][INFO] - Training Epoch: 2/2, step 2843/7134 completed (loss: 0.037696968764066696, acc: 0.9937106966972351)
[2025-02-13 20:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41][root][INFO] - Training Epoch: 2/2, step 2844/7134 completed (loss: 0.1612221747636795, acc: 0.9728260636329651)
[2025-02-13 20:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41][root][INFO] - Training Epoch: 2/2, step 2845/7134 completed (loss: 0.11362165957689285, acc: 0.9797297120094299)
[2025-02-13 20:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42][root][INFO] - Training Epoch: 2/2, step 2846/7134 completed (loss: 0.10062415897846222, acc: 0.9738562107086182)
[2025-02-13 20:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42][root][INFO] - Training Epoch: 2/2, step 2847/7134 completed (loss: 0.07664553076028824, acc: 0.993630588054657)
[2025-02-13 20:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42][root][INFO] - Training Epoch: 2/2, step 2848/7134 completed (loss: 0.11576028168201447, acc: 0.9816513657569885)
[2025-02-13 20:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:43][root][INFO] - Training Epoch: 2/2, step 2849/7134 completed (loss: 0.14180396497249603, acc: 0.9657142758369446)
[2025-02-13 20:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:43][root][INFO] - Training Epoch: 2/2, step 2850/7134 completed (loss: 0.18076002597808838, acc: 0.9572649598121643)
[2025-02-13 20:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44][root][INFO] - Training Epoch: 2/2, step 2851/7134 completed (loss: 0.09414949268102646, acc: 0.970370352268219)
[2025-02-13 20:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44][root][INFO] - Training Epoch: 2/2, step 2852/7134 completed (loss: 0.0877164974808693, acc: 0.9837398529052734)
[2025-02-13 20:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44][root][INFO] - Training Epoch: 2/2, step 2853/7134 completed (loss: 0.06768559664487839, acc: 0.9795918464660645)
[2025-02-13 20:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45][root][INFO] - Training Epoch: 2/2, step 2854/7134 completed (loss: 0.09037677943706512, acc: 0.9659863710403442)
[2025-02-13 20:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45][root][INFO] - Training Epoch: 2/2, step 2855/7134 completed (loss: 0.09823737293481827, acc: 0.9662162065505981)
[2025-02-13 20:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45][root][INFO] - Training Epoch: 2/2, step 2856/7134 completed (loss: 0.03723720833659172, acc: 0.9937888383865356)
[2025-02-13 20:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46][root][INFO] - Training Epoch: 2/2, step 2857/7134 completed (loss: 0.07865642756223679, acc: 0.9880239367485046)
[2025-02-13 20:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46][root][INFO] - Training Epoch: 2/2, step 2858/7134 completed (loss: 0.04139072075486183, acc: 0.9848484992980957)
[2025-02-13 20:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46][root][INFO] - Training Epoch: 2/2, step 2859/7134 completed (loss: 0.1710330694913864, acc: 0.9464285969734192)
[2025-02-13 20:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47][root][INFO] - Training Epoch: 2/2, step 2860/7134 completed (loss: 0.13254691660404205, acc: 0.9765625)
[2025-02-13 20:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47][root][INFO] - Training Epoch: 2/2, step 2861/7134 completed (loss: 0.06417020410299301, acc: 0.9821428656578064)
[2025-02-13 20:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:48][root][INFO] - Training Epoch: 2/2, step 2862/7134 completed (loss: 0.07896813750267029, acc: 0.9849624037742615)
[2025-02-13 20:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:48][root][INFO] - Training Epoch: 2/2, step 2863/7134 completed (loss: 0.02602205239236355, acc: 1.0)
[2025-02-13 20:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:48][root][INFO] - Training Epoch: 2/2, step 2864/7134 completed (loss: 0.10013896226882935, acc: 0.9789473414421082)
[2025-02-13 20:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49][root][INFO] - Training Epoch: 2/2, step 2865/7134 completed (loss: 0.04176894947886467, acc: 0.991304337978363)
[2025-02-13 20:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49][root][INFO] - Training Epoch: 2/2, step 2866/7134 completed (loss: 0.06410785764455795, acc: 0.9870967864990234)
[2025-02-13 20:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49][root][INFO] - Training Epoch: 2/2, step 2867/7134 completed (loss: 0.034713014960289, acc: 0.9876543283462524)
[2025-02-13 20:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:50][root][INFO] - Training Epoch: 2/2, step 2868/7134 completed (loss: 0.01982731744647026, acc: 1.0)
[2025-02-13 20:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:50][root][INFO] - Training Epoch: 2/2, step 2869/7134 completed (loss: 0.1495843380689621, acc: 0.9793103337287903)
[2025-02-13 20:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51][root][INFO] - Training Epoch: 2/2, step 2870/7134 completed (loss: 0.2583758234977722, acc: 0.9636363387107849)
[2025-02-13 20:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51][root][INFO] - Training Epoch: 2/2, step 2871/7134 completed (loss: 0.09947335720062256, acc: 0.9862068891525269)
[2025-02-13 20:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51][root][INFO] - Training Epoch: 2/2, step 2872/7134 completed (loss: 0.03411749377846718, acc: 0.991150438785553)
[2025-02-13 20:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52][root][INFO] - Training Epoch: 2/2, step 2873/7134 completed (loss: 0.02839590236544609, acc: 1.0)
[2025-02-13 20:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52][root][INFO] - Training Epoch: 2/2, step 2874/7134 completed (loss: 0.07125288248062134, acc: 0.9791666865348816)
[2025-02-13 20:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52][root][INFO] - Training Epoch: 2/2, step 2875/7134 completed (loss: 0.14812219142913818, acc: 0.9464285969734192)
[2025-02-13 20:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53][root][INFO] - Training Epoch: 2/2, step 2876/7134 completed (loss: 0.25319328904151917, acc: 0.9339622855186462)
[2025-02-13 20:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53][root][INFO] - Training Epoch: 2/2, step 2877/7134 completed (loss: 0.04953429102897644, acc: 0.9914529919624329)
[2025-02-13 20:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54][root][INFO] - Training Epoch: 2/2, step 2878/7134 completed (loss: 0.058548346161842346, acc: 0.9912280440330505)
[2025-02-13 20:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54][root][INFO] - Training Epoch: 2/2, step 2879/7134 completed (loss: 0.038391903042793274, acc: 0.9931972622871399)
[2025-02-13 20:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54][root][INFO] - Training Epoch: 2/2, step 2880/7134 completed (loss: 0.014415242709219456, acc: 1.0)
[2025-02-13 20:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55][root][INFO] - Training Epoch: 2/2, step 2881/7134 completed (loss: 0.058750420808792114, acc: 0.9841269850730896)
[2025-02-13 20:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55][root][INFO] - Training Epoch: 2/2, step 2882/7134 completed (loss: 0.18314500153064728, acc: 0.9537572264671326)
[2025-02-13 20:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55][root][INFO] - Training Epoch: 2/2, step 2883/7134 completed (loss: 0.14536640048027039, acc: 0.9580838084220886)
[2025-02-13 20:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56][root][INFO] - Training Epoch: 2/2, step 2884/7134 completed (loss: 0.17439983785152435, acc: 0.9454545378684998)
[2025-02-13 20:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56][root][INFO] - Training Epoch: 2/2, step 2885/7134 completed (loss: 0.2661910653114319, acc: 0.9151515364646912)
[2025-02-13 20:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57][root][INFO] - Training Epoch: 2/2, step 2886/7134 completed (loss: 0.1849725842475891, acc: 0.9529411792755127)
[2025-02-13 20:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57][root][INFO] - Training Epoch: 2/2, step 2887/7134 completed (loss: 0.18398834764957428, acc: 0.9563318490982056)
[2025-02-13 20:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57][root][INFO] - Training Epoch: 2/2, step 2888/7134 completed (loss: 0.12355374544858932, acc: 0.9631578922271729)
[2025-02-13 20:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58][root][INFO] - Training Epoch: 2/2, step 2889/7134 completed (loss: 0.10165154933929443, acc: 0.9720670580863953)
[2025-02-13 20:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58][root][INFO] - Training Epoch: 2/2, step 2890/7134 completed (loss: 0.1185959130525589, acc: 0.9692307710647583)
[2025-02-13 20:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58][root][INFO] - Training Epoch: 2/2, step 2891/7134 completed (loss: 0.05780480057001114, acc: 0.970370352268219)
[2025-02-13 20:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59][root][INFO] - Training Epoch: 2/2, step 2892/7134 completed (loss: 0.12115829437971115, acc: 0.9526627063751221)
[2025-02-13 20:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59][root][INFO] - Training Epoch: 2/2, step 2893/7134 completed (loss: 0.13383041322231293, acc: 0.9672130942344666)
[2025-02-13 20:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00][root][INFO] - Training Epoch: 2/2, step 2894/7134 completed (loss: 0.1894632875919342, acc: 0.9481481313705444)
[2025-02-13 20:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00][root][INFO] - Training Epoch: 2/2, step 2895/7134 completed (loss: 0.04892369359731674, acc: 0.9745762944221497)
[2025-02-13 20:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00][root][INFO] - Training Epoch: 2/2, step 2896/7134 completed (loss: 0.12700895965099335, acc: 0.9820359349250793)
[2025-02-13 20:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01][root][INFO] - Training Epoch: 2/2, step 2897/7134 completed (loss: 0.19591893255710602, acc: 0.969924807548523)
[2025-02-13 20:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01][root][INFO] - Training Epoch: 2/2, step 2898/7134 completed (loss: 0.11141636967658997, acc: 0.97826087474823)
[2025-02-13 20:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02][root][INFO] - Training Epoch: 2/2, step 2899/7134 completed (loss: 0.17728564143180847, acc: 0.9520958065986633)
[2025-02-13 20:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02][root][INFO] - Training Epoch: 2/2, step 2900/7134 completed (loss: 0.08493853360414505, acc: 0.9893617033958435)
[2025-02-13 20:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02][root][INFO] - Training Epoch: 2/2, step 2901/7134 completed (loss: 0.07094654440879822, acc: 0.9864864945411682)
[2025-02-13 20:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03][root][INFO] - Training Epoch: 2/2, step 2902/7134 completed (loss: 0.07259742170572281, acc: 0.9829059839248657)
[2025-02-13 20:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03][root][INFO] - Training Epoch: 2/2, step 2903/7134 completed (loss: 0.05102632939815521, acc: 0.9943820238113403)
[2025-02-13 20:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03][root][INFO] - Training Epoch: 2/2, step 2904/7134 completed (loss: 0.025468377396464348, acc: 0.9945945739746094)
[2025-02-13 20:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:04][root][INFO] - Training Epoch: 2/2, step 2905/7134 completed (loss: 0.08353831619024277, acc: 0.9732620120048523)
[2025-02-13 20:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:04][root][INFO] - Training Epoch: 2/2, step 2906/7134 completed (loss: 0.07413029670715332, acc: 0.9842932224273682)
[2025-02-13 20:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05][root][INFO] - Training Epoch: 2/2, step 2907/7134 completed (loss: 0.0698765441775322, acc: 0.976047933101654)
[2025-02-13 20:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05][root][INFO] - Training Epoch: 2/2, step 2908/7134 completed (loss: 0.09085515886545181, acc: 0.977011501789093)
[2025-02-13 20:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05][root][INFO] - Training Epoch: 2/2, step 2909/7134 completed (loss: 0.05075974389910698, acc: 0.9890710115432739)
[2025-02-13 20:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06][root][INFO] - Training Epoch: 2/2, step 2910/7134 completed (loss: 0.07410064339637756, acc: 0.9839572310447693)
[2025-02-13 20:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06][root][INFO] - Training Epoch: 2/2, step 2911/7134 completed (loss: 0.11769698560237885, acc: 0.9650349617004395)
[2025-02-13 20:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06][root][INFO] - Training Epoch: 2/2, step 2912/7134 completed (loss: 0.030746104195713997, acc: 1.0)
[2025-02-13 20:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:07][root][INFO] - Training Epoch: 2/2, step 2913/7134 completed (loss: 0.11173039674758911, acc: 0.971222996711731)
[2025-02-13 20:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:07][root][INFO] - Training Epoch: 2/2, step 2914/7134 completed (loss: 0.04898791387677193, acc: 1.0)
[2025-02-13 20:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08][root][INFO] - Training Epoch: 2/2, step 2915/7134 completed (loss: 0.3077809810638428, acc: 0.931034505367279)
[2025-02-13 20:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08][root][INFO] - Training Epoch: 2/2, step 2916/7134 completed (loss: 0.04319697245955467, acc: 1.0)
[2025-02-13 20:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08][root][INFO] - Training Epoch: 2/2, step 2917/7134 completed (loss: 0.03607220947742462, acc: 0.9912280440330505)
[2025-02-13 20:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09][root][INFO] - Training Epoch: 2/2, step 2918/7134 completed (loss: 0.07853047549724579, acc: 0.9824561476707458)
[2025-02-13 20:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09][root][INFO] - Training Epoch: 2/2, step 2919/7134 completed (loss: 0.08820755779743195, acc: 0.9698795080184937)
[2025-02-13 20:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09][root][INFO] - Training Epoch: 2/2, step 2920/7134 completed (loss: 0.06676594167947769, acc: 0.9811320900917053)
[2025-02-13 20:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10][root][INFO] - Training Epoch: 2/2, step 2921/7134 completed (loss: 0.06338878720998764, acc: 0.9857142567634583)
[2025-02-13 20:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10][root][INFO] - Training Epoch: 2/2, step 2922/7134 completed (loss: 0.21491728723049164, acc: 0.9693251252174377)
[2025-02-13 20:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10][root][INFO] - Training Epoch: 2/2, step 2923/7134 completed (loss: 0.364265114068985, acc: 0.9436619877815247)
[2025-02-13 20:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11][root][INFO] - Training Epoch: 2/2, step 2924/7134 completed (loss: 0.10249578952789307, acc: 0.9878048896789551)
[2025-02-13 20:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11][root][INFO] - Training Epoch: 2/2, step 2925/7134 completed (loss: 0.05148661509156227, acc: 0.9941176176071167)
[2025-02-13 20:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12][root][INFO] - Training Epoch: 2/2, step 2926/7134 completed (loss: 0.2582254409790039, acc: 0.9698492288589478)
[2025-02-13 20:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12][root][INFO] - Training Epoch: 2/2, step 2927/7134 completed (loss: 0.0723271518945694, acc: 0.9826589822769165)
[2025-02-13 20:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12][root][INFO] - Training Epoch: 2/2, step 2928/7134 completed (loss: 0.15457528829574585, acc: 0.9652777910232544)
[2025-02-13 20:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13][root][INFO] - Training Epoch: 2/2, step 2929/7134 completed (loss: 0.05034996196627617, acc: 0.9940476417541504)
[2025-02-13 20:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13][root][INFO] - Training Epoch: 2/2, step 2930/7134 completed (loss: 0.04924982786178589, acc: 0.9931972622871399)
[2025-02-13 20:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14][root][INFO] - Training Epoch: 2/2, step 2931/7134 completed (loss: 0.19644270837306976, acc: 0.9358974099159241)
[2025-02-13 20:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14][root][INFO] - Training Epoch: 2/2, step 2932/7134 completed (loss: 0.04613376408815384, acc: 0.9946523904800415)
[2025-02-13 20:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14][root][INFO] - Training Epoch: 2/2, step 2933/7134 completed (loss: 0.07057465612888336, acc: 0.9781022071838379)
[2025-02-13 20:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15][root][INFO] - Training Epoch: 2/2, step 2934/7134 completed (loss: 0.04039740189909935, acc: 0.9928571581840515)
[2025-02-13 20:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15][root][INFO] - Training Epoch: 2/2, step 2935/7134 completed (loss: 0.024732772260904312, acc: 1.0)
[2025-02-13 20:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16][root][INFO] - Training Epoch: 2/2, step 2936/7134 completed (loss: 0.10673502832651138, acc: 0.9778761267662048)
[2025-02-13 20:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16][root][INFO] - Training Epoch: 2/2, step 2937/7134 completed (loss: 0.10422220826148987, acc: 0.970588207244873)
[2025-02-13 20:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16][root][INFO] - Training Epoch: 2/2, step 2938/7134 completed (loss: 0.0880543515086174, acc: 0.9672130942344666)
[2025-02-13 20:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17][root][INFO] - Training Epoch: 2/2, step 2939/7134 completed (loss: 0.06880128383636475, acc: 0.9867549538612366)
[2025-02-13 20:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17][root][INFO] - Training Epoch: 2/2, step 2940/7134 completed (loss: 0.12132430821657181, acc: 0.9617486596107483)
[2025-02-13 20:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17][root][INFO] - Training Epoch: 2/2, step 2941/7134 completed (loss: 0.09997604787349701, acc: 0.9719101190567017)
[2025-02-13 20:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18][root][INFO] - Training Epoch: 2/2, step 2942/7134 completed (loss: 0.07946613430976868, acc: 0.9878787994384766)
[2025-02-13 20:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18][root][INFO] - Training Epoch: 2/2, step 2943/7134 completed (loss: 0.08828337490558624, acc: 0.9864864945411682)
[2025-02-13 20:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18][root][INFO] - Training Epoch: 2/2, step 2944/7134 completed (loss: 0.10109400749206543, acc: 0.9698795080184937)
[2025-02-13 20:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19][root][INFO] - Training Epoch: 2/2, step 2945/7134 completed (loss: 0.11582259088754654, acc: 0.9731543660163879)
[2025-02-13 20:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19][root][INFO] - Training Epoch: 2/2, step 2946/7134 completed (loss: 0.14207379519939423, acc: 0.9629629850387573)
[2025-02-13 20:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20][root][INFO] - Training Epoch: 2/2, step 2947/7134 completed (loss: 0.12996654212474823, acc: 0.956250011920929)
[2025-02-13 20:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20][root][INFO] - Training Epoch: 2/2, step 2948/7134 completed (loss: 0.0875818058848381, acc: 0.9861111044883728)
[2025-02-13 20:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20][root][INFO] - Training Epoch: 2/2, step 2949/7134 completed (loss: 0.060585394501686096, acc: 0.9869281053543091)
[2025-02-13 20:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21][root][INFO] - Training Epoch: 2/2, step 2950/7134 completed (loss: 0.08224798738956451, acc: 0.9776536226272583)
[2025-02-13 20:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21][root][INFO] - Training Epoch: 2/2, step 2951/7134 completed (loss: 0.06547588109970093, acc: 0.9813664555549622)
[2025-02-13 20:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21][root][INFO] - Training Epoch: 2/2, step 2952/7134 completed (loss: 0.03561496362090111, acc: 0.987261176109314)
[2025-02-13 20:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22][root][INFO] - Training Epoch: 2/2, step 2953/7134 completed (loss: 0.06536003947257996, acc: 0.9795918464660645)
[2025-02-13 20:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22][root][INFO] - Training Epoch: 2/2, step 2954/7134 completed (loss: 0.08249630779027939, acc: 0.9742268323898315)
[2025-02-13 20:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23][root][INFO] - Training Epoch: 2/2, step 2955/7134 completed (loss: 0.057039473205804825, acc: 0.9939393997192383)
[2025-02-13 20:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23][root][INFO] - Training Epoch: 2/2, step 2956/7134 completed (loss: 0.058415092527866364, acc: 0.9815950989723206)
[2025-02-13 20:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23][root][INFO] - Training Epoch: 2/2, step 2957/7134 completed (loss: 0.04451214149594307, acc: 0.9942528605461121)
[2025-02-13 20:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:24][root][INFO] - Training Epoch: 2/2, step 2958/7134 completed (loss: 0.031058818101882935, acc: 1.0)
[2025-02-13 20:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:24][root][INFO] - Training Epoch: 2/2, step 2959/7134 completed (loss: 0.018084200099110603, acc: 1.0)
[2025-02-13 20:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25][root][INFO] - Training Epoch: 2/2, step 2960/7134 completed (loss: 0.061993956565856934, acc: 0.9835164546966553)
[2025-02-13 20:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25][root][INFO] - Training Epoch: 2/2, step 2961/7134 completed (loss: 0.04622156172990799, acc: 0.9820359349250793)
[2025-02-13 20:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25][root][INFO] - Training Epoch: 2/2, step 2962/7134 completed (loss: 0.034557051956653595, acc: 0.9940476417541504)
[2025-02-13 20:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26][root][INFO] - Training Epoch: 2/2, step 2963/7134 completed (loss: 0.05817480385303497, acc: 0.9832402467727661)
[2025-02-13 20:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26][root][INFO] - Training Epoch: 2/2, step 2964/7134 completed (loss: 0.026705602183938026, acc: 0.9933333396911621)
[2025-02-13 20:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26][root][INFO] - Training Epoch: 2/2, step 2965/7134 completed (loss: 0.08325305581092834, acc: 0.9943181872367859)
[2025-02-13 20:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27][root][INFO] - Training Epoch: 2/2, step 2966/7134 completed (loss: 0.007791853044182062, acc: 1.0)
[2025-02-13 20:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27][root][INFO] - Training Epoch: 2/2, step 2967/7134 completed (loss: 0.18927960097789764, acc: 0.9398906826972961)
[2025-02-13 20:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27][root][INFO] - Training Epoch: 2/2, step 2968/7134 completed (loss: 0.2560245990753174, acc: 0.918749988079071)
[2025-02-13 20:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:28][root][INFO] - Training Epoch: 2/2, step 2969/7134 completed (loss: 0.8125903010368347, acc: 0.8495145440101624)
[2025-02-13 20:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:28][root][INFO] - Training Epoch: 2/2, step 2970/7134 completed (loss: 0.6488251686096191, acc: 0.858208954334259)
[2025-02-13 20:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29][root][INFO] - Training Epoch: 2/2, step 2971/7134 completed (loss: 0.12752003967761993, acc: 0.9668508172035217)
[2025-02-13 20:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29][root][INFO] - Training Epoch: 2/2, step 2972/7134 completed (loss: 0.1716933399438858, acc: 0.9466666579246521)
[2025-02-13 20:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29][root][INFO] - Training Epoch: 2/2, step 2973/7134 completed (loss: 0.09321867674589157, acc: 0.9757575988769531)
[2025-02-13 20:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30][root][INFO] - Training Epoch: 2/2, step 2974/7134 completed (loss: 0.13295213878154755, acc: 0.9590643048286438)
[2025-02-13 20:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30][root][INFO] - Training Epoch: 2/2, step 2975/7134 completed (loss: 0.11858828365802765, acc: 0.9593023061752319)
[2025-02-13 20:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30][root][INFO] - Training Epoch: 2/2, step 2976/7134 completed (loss: 0.08594230562448502, acc: 0.9811320900917053)
[2025-02-13 20:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31][root][INFO] - Training Epoch: 2/2, step 2977/7134 completed (loss: 0.07720582187175751, acc: 0.9793103337287903)
[2025-02-13 20:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31][root][INFO] - Training Epoch: 2/2, step 2978/7134 completed (loss: 0.13096977770328522, acc: 0.9518072009086609)
[2025-02-13 20:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32][root][INFO] - Training Epoch: 2/2, step 2979/7134 completed (loss: 0.11240192502737045, acc: 0.9655172228813171)
[2025-02-13 20:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32][root][INFO] - Training Epoch: 2/2, step 2980/7134 completed (loss: 0.1586868017911911, acc: 0.9714285731315613)
[2025-02-13 20:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32][root][INFO] - Training Epoch: 2/2, step 2981/7134 completed (loss: 0.1309066265821457, acc: 0.9655172228813171)
[2025-02-13 20:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33][root][INFO] - Training Epoch: 2/2, step 2982/7134 completed (loss: 0.2093685269355774, acc: 0.9395604133605957)
[2025-02-13 20:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33][root][INFO] - Training Epoch: 2/2, step 2983/7134 completed (loss: 0.11203628033399582, acc: 0.9631901979446411)
[2025-02-13 20:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33][root][INFO] - Training Epoch: 2/2, step 2984/7134 completed (loss: 0.1887909173965454, acc: 0.9395604133605957)
[2025-02-13 20:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34][root][INFO] - Training Epoch: 2/2, step 2985/7134 completed (loss: 0.06972409039735794, acc: 0.9804878234863281)
[2025-02-13 20:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34][root][INFO] - Training Epoch: 2/2, step 2986/7134 completed (loss: 0.11132415384054184, acc: 0.9742268323898315)
[2025-02-13 20:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34][root][INFO] - Training Epoch: 2/2, step 2987/7134 completed (loss: 0.12072545289993286, acc: 0.97826087474823)
[2025-02-13 20:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:35][root][INFO] - Training Epoch: 2/2, step 2988/7134 completed (loss: 0.11996220797300339, acc: 0.9675675630569458)
[2025-02-13 20:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:35][root][INFO] - Training Epoch: 2/2, step 2989/7134 completed (loss: 0.0910428985953331, acc: 0.9820359349250793)
[2025-02-13 20:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36][root][INFO] - Training Epoch: 2/2, step 2990/7134 completed (loss: 0.26536205410957336, acc: 0.9527027010917664)
[2025-02-13 20:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36][root][INFO] - Training Epoch: 2/2, step 2991/7134 completed (loss: 0.36340251564979553, acc: 0.90625)
[2025-02-13 20:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36][root][INFO] - Training Epoch: 2/2, step 2992/7134 completed (loss: 0.19226230680942535, acc: 0.9496855139732361)
[2025-02-13 20:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37][root][INFO] - Training Epoch: 2/2, step 2993/7134 completed (loss: 0.08903768658638, acc: 0.9648241400718689)
[2025-02-13 20:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37][root][INFO] - Training Epoch: 2/2, step 2994/7134 completed (loss: 0.03902081400156021, acc: 0.9920634627342224)
[2025-02-13 20:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38][root][INFO] - Training Epoch: 2/2, step 2995/7134 completed (loss: 0.07310471683740616, acc: 0.9805825352668762)
[2025-02-13 20:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38][root][INFO] - Training Epoch: 2/2, step 2996/7134 completed (loss: 0.09424750506877899, acc: 0.9639175534248352)
[2025-02-13 20:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38][root][INFO] - Training Epoch: 2/2, step 2997/7134 completed (loss: 0.04229193925857544, acc: 0.9858490824699402)
[2025-02-13 20:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39][root][INFO] - Training Epoch: 2/2, step 2998/7134 completed (loss: 0.0661068931221962, acc: 0.9940476417541504)
[2025-02-13 20:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39][root][INFO] - Training Epoch: 2/2, step 2999/7134 completed (loss: 0.07162636518478394, acc: 0.987500011920929)
[2025-02-13 20:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39][root][INFO] - Training Epoch: 2/2, step 3000/7134 completed (loss: 0.1894669383764267, acc: 0.9398148059844971)
[2025-02-13 20:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:40][root][INFO] - Training Epoch: 2/2, step 3001/7134 completed (loss: 0.08392352610826492, acc: 0.9695431590080261)
[2025-02-13 20:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:40][root][INFO] - Training Epoch: 2/2, step 3002/7134 completed (loss: 0.1690673679113388, acc: 0.9723502397537231)
[2025-02-13 20:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41][root][INFO] - Training Epoch: 2/2, step 3003/7134 completed (loss: 0.01729242503643036, acc: 0.9949238300323486)
[2025-02-13 20:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41][root][INFO] - Training Epoch: 2/2, step 3004/7134 completed (loss: 0.015128086321055889, acc: 1.0)
[2025-02-13 20:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41][root][INFO] - Training Epoch: 2/2, step 3005/7134 completed (loss: 0.022989628836512566, acc: 0.9948186278343201)
[2025-02-13 20:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42][root][INFO] - Training Epoch: 2/2, step 3006/7134 completed (loss: 0.04641829803586006, acc: 0.9885714054107666)
[2025-02-13 20:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42][root][INFO] - Training Epoch: 2/2, step 3007/7134 completed (loss: 0.06289174407720566, acc: 0.9883720874786377)
[2025-02-13 20:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42][root][INFO] - Training Epoch: 2/2, step 3008/7134 completed (loss: 0.10102394968271255, acc: 0.9727891087532043)
[2025-02-13 20:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43][root][INFO] - Training Epoch: 2/2, step 3009/7134 completed (loss: 0.12475553154945374, acc: 0.9702380895614624)
[2025-02-13 20:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43][root][INFO] - Training Epoch: 2/2, step 3010/7134 completed (loss: 0.10237853229045868, acc: 0.9657142758369446)
[2025-02-13 20:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44][root][INFO] - Training Epoch: 2/2, step 3011/7134 completed (loss: 0.36297187209129333, acc: 0.9151515364646912)
[2025-02-13 20:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44][root][INFO] - Training Epoch: 2/2, step 3012/7134 completed (loss: 0.1624429076910019, acc: 0.971222996711731)
[2025-02-13 20:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44][root][INFO] - Training Epoch: 2/2, step 3013/7134 completed (loss: 0.019209392368793488, acc: 0.9941860437393188)
[2025-02-13 20:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45][root][INFO] - Training Epoch: 2/2, step 3014/7134 completed (loss: 0.07208050042390823, acc: 0.9810126423835754)
[2025-02-13 20:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45][root][INFO] - Training Epoch: 2/2, step 3015/7134 completed (loss: 0.07607370615005493, acc: 0.9788732528686523)
[2025-02-13 20:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45][root][INFO] - Training Epoch: 2/2, step 3016/7134 completed (loss: 0.06310727447271347, acc: 0.9794520735740662)
[2025-02-13 20:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46][root][INFO] - Training Epoch: 2/2, step 3017/7134 completed (loss: 0.025359129533171654, acc: 0.9917355179786682)
[2025-02-13 20:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46][root][INFO] - Training Epoch: 2/2, step 3018/7134 completed (loss: 0.048596207052469254, acc: 0.9847328066825867)
[2025-02-13 20:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47][root][INFO] - Training Epoch: 2/2, step 3019/7134 completed (loss: 0.09011338651180267, acc: 0.9926470518112183)
[2025-02-13 20:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47][root][INFO] - Training Epoch: 2/2, step 3020/7134 completed (loss: 0.03456299379467964, acc: 1.0)
[2025-02-13 20:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47][root][INFO] - Training Epoch: 2/2, step 3021/7134 completed (loss: 0.03815889358520508, acc: 0.9924242496490479)
[2025-02-13 20:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48][root][INFO] - Training Epoch: 2/2, step 3022/7134 completed (loss: 0.11493005603551865, acc: 0.9774436354637146)
[2025-02-13 20:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48][root][INFO] - Training Epoch: 2/2, step 3023/7134 completed (loss: 0.07661541551351547, acc: 0.9769230484962463)
[2025-02-13 20:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48][root][INFO] - Training Epoch: 2/2, step 3024/7134 completed (loss: 0.029320504516363144, acc: 1.0)
[2025-02-13 20:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49][root][INFO] - Training Epoch: 2/2, step 3025/7134 completed (loss: 0.13991393148899078, acc: 0.9923076629638672)
[2025-02-13 20:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49][root][INFO] - Training Epoch: 2/2, step 3026/7134 completed (loss: 0.12957437336444855, acc: 0.9590163826942444)
[2025-02-13 20:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49][root][INFO] - Training Epoch: 2/2, step 3027/7134 completed (loss: 0.10991856455802917, acc: 0.9895833134651184)
[2025-02-13 20:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50][root][INFO] - Training Epoch: 2/2, step 3028/7134 completed (loss: 0.021333932876586914, acc: 1.0)
[2025-02-13 20:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50][root][INFO] - Training Epoch: 2/2, step 3029/7134 completed (loss: 0.12199592590332031, acc: 0.9863013625144958)
[2025-02-13 20:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51][root][INFO] - Training Epoch: 2/2, step 3030/7134 completed (loss: 0.07033927738666534, acc: 0.9864864945411682)
[2025-02-13 20:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51][root][INFO] - Training Epoch: 2/2, step 3031/7134 completed (loss: 0.04973249137401581, acc: 0.9797979593276978)
[2025-02-13 20:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51][root][INFO] - Training Epoch: 2/2, step 3032/7134 completed (loss: 0.04530636966228485, acc: 0.9838709831237793)
[2025-02-13 20:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52][root][INFO] - Training Epoch: 2/2, step 3033/7134 completed (loss: 0.06884017586708069, acc: 0.96875)
[2025-02-13 20:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52][root][INFO] - Training Epoch: 2/2, step 3034/7134 completed (loss: 0.0657128393650055, acc: 0.9819819927215576)
[2025-02-13 20:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52][root][INFO] - Training Epoch: 2/2, step 3035/7134 completed (loss: 0.05291600897908211, acc: 0.9931972622871399)
[2025-02-13 20:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53][root][INFO] - Training Epoch: 2/2, step 3036/7134 completed (loss: 0.12820422649383545, acc: 0.9520000219345093)
[2025-02-13 20:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53][root][INFO] - Training Epoch: 2/2, step 3037/7134 completed (loss: 0.09706682711839676, acc: 0.9731543660163879)
[2025-02-13 20:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53][root][INFO] - Training Epoch: 2/2, step 3038/7134 completed (loss: 0.08060171455144882, acc: 0.9850746393203735)
[2025-02-13 20:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54][root][INFO] - Training Epoch: 2/2, step 3039/7134 completed (loss: 0.05860641226172447, acc: 0.9864864945411682)
[2025-02-13 20:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54][root][INFO] - Training Epoch: 2/2, step 3040/7134 completed (loss: 0.08216749131679535, acc: 0.9844961166381836)
[2025-02-13 20:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55][root][INFO] - Training Epoch: 2/2, step 3041/7134 completed (loss: 0.1879405826330185, acc: 0.9642857313156128)
[2025-02-13 20:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55][root][INFO] - Training Epoch: 2/2, step 3042/7134 completed (loss: 0.056723807007074356, acc: 0.9871794581413269)
[2025-02-13 20:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55][root][INFO] - Training Epoch: 2/2, step 3043/7134 completed (loss: 0.04121912270784378, acc: 0.9821428656578064)
[2025-02-13 20:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56][root][INFO] - Training Epoch: 2/2, step 3044/7134 completed (loss: 0.0488600991666317, acc: 0.9924812316894531)
[2025-02-13 20:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56][root][INFO] - Training Epoch: 2/2, step 3045/7134 completed (loss: 0.02384508214890957, acc: 0.9922480583190918)
[2025-02-13 20:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56][root][INFO] - Training Epoch: 2/2, step 3046/7134 completed (loss: 0.07799731194972992, acc: 0.9822485446929932)
[2025-02-13 20:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57][root][INFO] - Training Epoch: 2/2, step 3047/7134 completed (loss: 0.09371477365493774, acc: 0.9717513918876648)
[2025-02-13 20:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57][root][INFO] - Training Epoch: 2/2, step 3048/7134 completed (loss: 0.07048409432172775, acc: 0.9736841917037964)
[2025-02-13 20:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57][root][INFO] - Training Epoch: 2/2, step 3049/7134 completed (loss: 0.06348291784524918, acc: 0.981249988079071)
[2025-02-13 20:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58][root][INFO] - Training Epoch: 2/2, step 3050/7134 completed (loss: 0.08162350207567215, acc: 0.9796954393386841)
[2025-02-13 20:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58][root][INFO] - Training Epoch: 2/2, step 3051/7134 completed (loss: 0.10236531496047974, acc: 0.9642857313156128)
[2025-02-13 20:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58][root][INFO] - Training Epoch: 2/2, step 3052/7134 completed (loss: 0.06860852986574173, acc: 0.9783783555030823)
[2025-02-13 20:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59][root][INFO] - Training Epoch: 2/2, step 3053/7134 completed (loss: 0.03456909582018852, acc: 0.9950248599052429)
[2025-02-13 20:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59][root][INFO] - Training Epoch: 2/2, step 3054/7134 completed (loss: 0.032536689192056656, acc: 0.9946808218955994)
[2025-02-13 20:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00][root][INFO] - Training Epoch: 2/2, step 3055/7134 completed (loss: 0.07659522444009781, acc: 0.9864864945411682)
[2025-02-13 20:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00][root][INFO] - Training Epoch: 2/2, step 3056/7134 completed (loss: 0.05049964785575867, acc: 0.9945945739746094)
[2025-02-13 20:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00][root][INFO] - Training Epoch: 2/2, step 3057/7134 completed (loss: 0.05464646965265274, acc: 0.9815950989723206)
[2025-02-13 20:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01][root][INFO] - Training Epoch: 2/2, step 3058/7134 completed (loss: 0.09826017916202545, acc: 0.9736841917037964)
[2025-02-13 20:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01][root][INFO] - Training Epoch: 2/2, step 3059/7134 completed (loss: 0.06298530101776123, acc: 0.9885714054107666)
[2025-02-13 20:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01][root][INFO] - Training Epoch: 2/2, step 3060/7134 completed (loss: 0.08808556199073792, acc: 0.9685863852500916)
[2025-02-13 20:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02][root][INFO] - Training Epoch: 2/2, step 3061/7134 completed (loss: 0.06527305394411087, acc: 0.9750000238418579)
[2025-02-13 20:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02][root][INFO] - Training Epoch: 2/2, step 3062/7134 completed (loss: 0.027524052187800407, acc: 0.9937106966972351)
[2025-02-13 20:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02][root][INFO] - Training Epoch: 2/2, step 3063/7134 completed (loss: 0.07489601522684097, acc: 0.9659090638160706)
[2025-02-13 20:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:03][root][INFO] - Training Epoch: 2/2, step 3064/7134 completed (loss: 0.02964601293206215, acc: 1.0)
[2025-02-13 20:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:03][root][INFO] - Training Epoch: 2/2, step 3065/7134 completed (loss: 0.015975186601281166, acc: 1.0)
[2025-02-13 20:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04][root][INFO] - Training Epoch: 2/2, step 3066/7134 completed (loss: 0.02340780384838581, acc: 1.0)
[2025-02-13 20:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04][root][INFO] - Training Epoch: 2/2, step 3067/7134 completed (loss: 0.016865219920873642, acc: 1.0)
[2025-02-13 20:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04][root][INFO] - Training Epoch: 2/2, step 3068/7134 completed (loss: 0.11748023331165314, acc: 0.9553072452545166)
[2025-02-13 20:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05][root][INFO] - Training Epoch: 2/2, step 3069/7134 completed (loss: 0.013390569016337395, acc: 1.0)
[2025-02-13 20:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05][root][INFO] - Training Epoch: 2/2, step 3070/7134 completed (loss: 0.02217162773013115, acc: 0.9952152967453003)
[2025-02-13 20:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05][root][INFO] - Training Epoch: 2/2, step 3071/7134 completed (loss: 0.056983958929777145, acc: 0.9833333492279053)
[2025-02-13 20:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06][root][INFO] - Training Epoch: 2/2, step 3072/7134 completed (loss: 0.04523155093193054, acc: 0.98591548204422)
[2025-02-13 20:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06][root][INFO] - Training Epoch: 2/2, step 3073/7134 completed (loss: 0.02625887840986252, acc: 0.9887005686759949)
[2025-02-13 20:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06][root][INFO] - Training Epoch: 2/2, step 3074/7134 completed (loss: 0.025540702044963837, acc: 0.9933775067329407)
[2025-02-13 20:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07][root][INFO] - Training Epoch: 2/2, step 3075/7134 completed (loss: 0.036643173545598984, acc: 0.9938271641731262)
[2025-02-13 20:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07][root][INFO] - Training Epoch: 2/2, step 3076/7134 completed (loss: 0.037238623946905136, acc: 0.9946236610412598)
[2025-02-13 20:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08][root][INFO] - Training Epoch: 2/2, step 3077/7134 completed (loss: 0.12635141611099243, acc: 0.963350772857666)
[2025-02-13 20:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08][root][INFO] - Training Epoch: 2/2, step 3078/7134 completed (loss: 0.10833939909934998, acc: 0.9635416865348816)
[2025-02-13 20:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08][root][INFO] - Training Epoch: 2/2, step 3079/7134 completed (loss: 0.08634401857852936, acc: 0.9735449552536011)
[2025-02-13 20:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09][root][INFO] - Training Epoch: 2/2, step 3080/7134 completed (loss: 0.12710648775100708, acc: 0.9537037014961243)
[2025-02-13 20:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09][root][INFO] - Training Epoch: 2/2, step 3081/7134 completed (loss: 0.04989492893218994, acc: 0.9900990128517151)
[2025-02-13 20:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09][root][INFO] - Training Epoch: 2/2, step 3082/7134 completed (loss: 0.1900969296693802, acc: 0.9601989984512329)
[2025-02-13 20:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10][root][INFO] - Training Epoch: 2/2, step 3083/7134 completed (loss: 0.07296621799468994, acc: 0.9675675630569458)
[2025-02-13 20:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10][root][INFO] - Training Epoch: 2/2, step 3084/7134 completed (loss: 0.0898367315530777, acc: 0.9685863852500916)
[2025-02-13 20:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10][root][INFO] - Training Epoch: 2/2, step 3085/7134 completed (loss: 0.15970996022224426, acc: 0.9548386931419373)
[2025-02-13 20:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11][root][INFO] - Training Epoch: 2/2, step 3086/7134 completed (loss: 0.10747941583395004, acc: 0.9771689772605896)
[2025-02-13 20:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11][root][INFO] - Training Epoch: 2/2, step 3087/7134 completed (loss: 0.0895664170384407, acc: 0.976331353187561)
[2025-02-13 20:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12][root][INFO] - Training Epoch: 2/2, step 3088/7134 completed (loss: 0.11271904408931732, acc: 0.9672897458076477)
[2025-02-13 20:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12][root][INFO] - Training Epoch: 2/2, step 3089/7134 completed (loss: 0.05395197495818138, acc: 0.9860464930534363)
[2025-02-13 20:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12][root][INFO] - Training Epoch: 2/2, step 3090/7134 completed (loss: 0.2623184323310852, acc: 0.9378530979156494)
[2025-02-13 20:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13][root][INFO] - Training Epoch: 2/2, step 3091/7134 completed (loss: 0.29931238293647766, acc: 0.9270386099815369)
[2025-02-13 20:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13][root][INFO] - Training Epoch: 2/2, step 3092/7134 completed (loss: 0.20873785018920898, acc: 0.9618320465087891)
[2025-02-13 20:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13][root][INFO] - Training Epoch: 2/2, step 3093/7134 completed (loss: 0.14652444422245026, acc: 0.9577465057373047)
[2025-02-13 20:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14][root][INFO] - Training Epoch: 2/2, step 3094/7134 completed (loss: 0.0364106148481369, acc: 0.9942196607589722)
[2025-02-13 20:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14][root][INFO] - Training Epoch: 2/2, step 3095/7134 completed (loss: 0.13991647958755493, acc: 0.9722222089767456)
[2025-02-13 20:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14][root][INFO] - Training Epoch: 2/2, step 3096/7134 completed (loss: 0.10997775197029114, acc: 0.9842932224273682)
[2025-02-13 20:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15][root][INFO] - Training Epoch: 2/2, step 3097/7134 completed (loss: 0.08065816015005112, acc: 0.9817073345184326)
[2025-02-13 20:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15][root][INFO] - Training Epoch: 2/2, step 3098/7134 completed (loss: 0.12279605120420456, acc: 0.9673202633857727)
[2025-02-13 20:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16][root][INFO] - Training Epoch: 2/2, step 3099/7134 completed (loss: 0.07352419942617416, acc: 0.9635036587715149)
[2025-02-13 20:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16][root][INFO] - Training Epoch: 2/2, step 3100/7134 completed (loss: 0.06226605176925659, acc: 0.9820627570152283)
[2025-02-13 20:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16][root][INFO] - Training Epoch: 2/2, step 3101/7134 completed (loss: 0.07068569958209991, acc: 0.9847715497016907)
[2025-02-13 20:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17][root][INFO] - Training Epoch: 2/2, step 3102/7134 completed (loss: 0.0743074044585228, acc: 0.9815950989723206)
[2025-02-13 20:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17][root][INFO] - Training Epoch: 2/2, step 3103/7134 completed (loss: 0.09264252334833145, acc: 0.9784482717514038)
[2025-02-13 20:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17][root][INFO] - Training Epoch: 2/2, step 3104/7134 completed (loss: 0.07712315768003464, acc: 0.9844961166381836)
[2025-02-13 20:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18][root][INFO] - Training Epoch: 2/2, step 3105/7134 completed (loss: 0.148703470826149, acc: 0.969565212726593)
[2025-02-13 20:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18][root][INFO] - Training Epoch: 2/2, step 3106/7134 completed (loss: 0.06620505452156067, acc: 0.9798657894134521)
[2025-02-13 20:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19][root][INFO] - Training Epoch: 2/2, step 3107/7134 completed (loss: 0.056222666054964066, acc: 0.9883720874786377)
[2025-02-13 20:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19][root][INFO] - Training Epoch: 2/2, step 3108/7134 completed (loss: 0.08000790327787399, acc: 0.9795918464660645)
[2025-02-13 20:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19][root][INFO] - Training Epoch: 2/2, step 3109/7134 completed (loss: 0.1126713901758194, acc: 0.9692307710647583)
[2025-02-13 20:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20][root][INFO] - Training Epoch: 2/2, step 3110/7134 completed (loss: 0.11794139444828033, acc: 0.9611650705337524)
[2025-02-13 20:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20][root][INFO] - Training Epoch: 2/2, step 3111/7134 completed (loss: 0.31256377696990967, acc: 0.9130434989929199)
[2025-02-13 20:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20][root][INFO] - Training Epoch: 2/2, step 3112/7134 completed (loss: 0.21071898937225342, acc: 0.956204354763031)
[2025-02-13 20:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:21][root][INFO] - Training Epoch: 2/2, step 3113/7134 completed (loss: 0.0610065683722496, acc: 0.9817073345184326)
[2025-02-13 20:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:21][root][INFO] - Training Epoch: 2/2, step 3114/7134 completed (loss: 0.0555601641535759, acc: 0.9924812316894531)
[2025-02-13 20:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22][root][INFO] - Training Epoch: 2/2, step 3115/7134 completed (loss: 0.09331931173801422, acc: 0.9802631735801697)
[2025-02-13 20:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22][root][INFO] - Training Epoch: 2/2, step 3116/7134 completed (loss: 0.060535021126270294, acc: 0.9830508232116699)
[2025-02-13 20:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22][root][INFO] - Training Epoch: 2/2, step 3117/7134 completed (loss: 0.1268007755279541, acc: 0.9775280952453613)
[2025-02-13 20:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23][root][INFO] - Training Epoch: 2/2, step 3118/7134 completed (loss: 0.08017226308584213, acc: 0.9779005646705627)
[2025-02-13 20:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23][root][INFO] - Training Epoch: 2/2, step 3119/7134 completed (loss: 0.1583680659532547, acc: 0.9417989253997803)
[2025-02-13 20:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23][root][INFO] - Training Epoch: 2/2, step 3120/7134 completed (loss: 0.2277267426252365, acc: 0.9506173133850098)
[2025-02-13 20:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:24][root][INFO] - Training Epoch: 2/2, step 3121/7134 completed (loss: 0.08188562840223312, acc: 0.9886363744735718)
[2025-02-13 20:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:24][root][INFO] - Training Epoch: 2/2, step 3122/7134 completed (loss: 0.12202138453722, acc: 0.9789473414421082)
[2025-02-13 20:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25][root][INFO] - Training Epoch: 2/2, step 3123/7134 completed (loss: 0.10641977190971375, acc: 0.9675675630569458)
[2025-02-13 20:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25][root][INFO] - Training Epoch: 2/2, step 3124/7134 completed (loss: 0.15342670679092407, acc: 0.9611111283302307)
[2025-02-13 20:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25][root][INFO] - Training Epoch: 2/2, step 3125/7134 completed (loss: 0.0488709919154644, acc: 0.9863945841789246)
[2025-02-13 20:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26][root][INFO] - Training Epoch: 2/2, step 3126/7134 completed (loss: 0.10993468761444092, acc: 0.9776536226272583)
[2025-02-13 20:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26][root][INFO] - Training Epoch: 2/2, step 3127/7134 completed (loss: 0.10873639583587646, acc: 0.9765258431434631)
[2025-02-13 20:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26][root][INFO] - Training Epoch: 2/2, step 3128/7134 completed (loss: 0.08521095663309097, acc: 0.9839572310447693)
[2025-02-13 20:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27][root][INFO] - Training Epoch: 2/2, step 3129/7134 completed (loss: 0.08398061245679855, acc: 0.9886363744735718)
[2025-02-13 20:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27][root][INFO] - Training Epoch: 2/2, step 3130/7134 completed (loss: 0.051240548491477966, acc: 0.9847715497016907)
[2025-02-13 20:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28][root][INFO] - Training Epoch: 2/2, step 3131/7134 completed (loss: 0.1014726310968399, acc: 0.9604519605636597)
[2025-02-13 20:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28][root][INFO] - Training Epoch: 2/2, step 3132/7134 completed (loss: 0.03565334901213646, acc: 0.991150438785553)
[2025-02-13 20:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28][root][INFO] - Training Epoch: 2/2, step 3133/7134 completed (loss: 0.21509943902492523, acc: 0.9659090638160706)
[2025-02-13 20:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29][root][INFO] - Training Epoch: 2/2, step 3134/7134 completed (loss: 0.06139960139989853, acc: 0.9765625)
[2025-02-13 20:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29][root][INFO] - Training Epoch: 2/2, step 3135/7134 completed (loss: 0.1117214784026146, acc: 0.969072163105011)
[2025-02-13 20:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29][root][INFO] - Training Epoch: 2/2, step 3136/7134 completed (loss: 0.07626505196094513, acc: 0.9770992398262024)
[2025-02-13 20:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30][root][INFO] - Training Epoch: 2/2, step 3137/7134 completed (loss: 0.11180119216442108, acc: 0.957446813583374)
[2025-02-13 20:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30][root][INFO] - Training Epoch: 2/2, step 3138/7134 completed (loss: 0.22098541259765625, acc: 0.9666666388511658)
[2025-02-13 20:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30][root][INFO] - Training Epoch: 2/2, step 3139/7134 completed (loss: 0.13343581557273865, acc: 0.9640287756919861)
[2025-02-13 20:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:31][root][INFO] - Training Epoch: 2/2, step 3140/7134 completed (loss: 0.09735538065433502, acc: 0.9794520735740662)
[2025-02-13 20:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:31][root][INFO] - Training Epoch: 2/2, step 3141/7134 completed (loss: 0.14438046514987946, acc: 0.9797297120094299)
[2025-02-13 20:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32][root][INFO] - Training Epoch: 2/2, step 3142/7134 completed (loss: 0.0745864287018776, acc: 0.976047933101654)
[2025-02-13 20:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32][root][INFO] - Training Epoch: 2/2, step 3143/7134 completed (loss: 0.10789746791124344, acc: 0.9741935729980469)
[2025-02-13 20:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32][root][INFO] - Training Epoch: 2/2, step 3144/7134 completed (loss: 0.12140259146690369, acc: 0.9871794581413269)
[2025-02-13 20:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33][root][INFO] - Training Epoch: 2/2, step 3145/7134 completed (loss: 0.11477547883987427, acc: 0.9639639854431152)
[2025-02-13 20:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33][root][INFO] - Training Epoch: 2/2, step 3146/7134 completed (loss: 0.14594562351703644, acc: 0.9494949579238892)
[2025-02-13 20:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33][root][INFO] - Training Epoch: 2/2, step 3147/7134 completed (loss: 0.055721744894981384, acc: 0.9863013625144958)
[2025-02-13 20:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:34][root][INFO] - Training Epoch: 2/2, step 3148/7134 completed (loss: 0.03402040898799896, acc: 0.9888268113136292)
[2025-02-13 20:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:34][root][INFO] - Training Epoch: 2/2, step 3149/7134 completed (loss: 0.029717983677983284, acc: 1.0)
[2025-02-13 20:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35][root][INFO] - Training Epoch: 2/2, step 3150/7134 completed (loss: 0.40809521079063416, acc: 0.8857142925262451)
[2025-02-13 20:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35][root][INFO] - Training Epoch: 2/2, step 3151/7134 completed (loss: 0.34099194407463074, acc: 0.9465649127960205)
[2025-02-13 20:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35][root][INFO] - Training Epoch: 2/2, step 3152/7134 completed (loss: 0.09192188829183578, acc: 0.9788732528686523)
[2025-02-13 20:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36][root][INFO] - Training Epoch: 2/2, step 3153/7134 completed (loss: 0.09195686876773834, acc: 0.966292142868042)
[2025-02-13 20:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36][root][INFO] - Training Epoch: 2/2, step 3154/7134 completed (loss: 0.11256541311740875, acc: 0.969924807548523)
[2025-02-13 20:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36][root][INFO] - Training Epoch: 2/2, step 3155/7134 completed (loss: 0.29206332564353943, acc: 0.932330846786499)
[2025-02-13 20:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37][root][INFO] - Training Epoch: 2/2, step 3156/7134 completed (loss: 0.22583122551441193, acc: 0.9360465407371521)
[2025-02-13 20:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37][root][INFO] - Training Epoch: 2/2, step 3157/7134 completed (loss: 0.10800278186798096, acc: 0.9739130139350891)
[2025-02-13 20:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38][root][INFO] - Training Epoch: 2/2, step 3158/7134 completed (loss: 0.07543344795703888, acc: 0.9858155846595764)
[2025-02-13 20:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38][root][INFO] - Training Epoch: 2/2, step 3159/7134 completed (loss: 0.11166886985301971, acc: 0.9793814420700073)
[2025-02-13 20:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38][root][INFO] - Training Epoch: 2/2, step 3160/7134 completed (loss: 0.04528659209609032, acc: 0.9935897588729858)
[2025-02-13 20:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39][root][INFO] - Training Epoch: 2/2, step 3161/7134 completed (loss: 0.11344333738088608, acc: 0.9808917045593262)
[2025-02-13 20:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39][root][INFO] - Training Epoch: 2/2, step 3162/7134 completed (loss: 0.06062439829111099, acc: 0.9863013625144958)
[2025-02-13 20:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39][root][INFO] - Training Epoch: 2/2, step 3163/7134 completed (loss: 0.030049050226807594, acc: 0.9918032884597778)
[2025-02-13 20:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40][root][INFO] - Training Epoch: 2/2, step 3164/7134 completed (loss: 0.09025365114212036, acc: 0.9927536249160767)
[2025-02-13 20:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40][root][INFO] - Training Epoch: 2/2, step 3165/7134 completed (loss: 0.06362749636173248, acc: 0.9930070042610168)
[2025-02-13 20:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40][root][INFO] - Training Epoch: 2/2, step 3166/7134 completed (loss: 0.12623941898345947, acc: 0.9746835231781006)
[2025-02-13 20:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41][root][INFO] - Training Epoch: 2/2, step 3167/7134 completed (loss: 0.13045082986354828, acc: 0.9629629850387573)
[2025-02-13 20:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41][root][INFO] - Training Epoch: 2/2, step 3168/7134 completed (loss: 0.07762950658798218, acc: 0.9803921580314636)
[2025-02-13 20:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42][root][INFO] - Training Epoch: 2/2, step 3169/7134 completed (loss: 0.10058088600635529, acc: 0.9754098653793335)
[2025-02-13 20:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42][root][INFO] - Training Epoch: 2/2, step 3170/7134 completed (loss: 0.10559427738189697, acc: 0.9852941036224365)
[2025-02-13 20:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42][root][INFO] - Training Epoch: 2/2, step 3171/7134 completed (loss: 0.09440917521715164, acc: 0.9835164546966553)
[2025-02-13 20:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43][root][INFO] - Training Epoch: 2/2, step 3172/7134 completed (loss: 0.0716278925538063, acc: 0.9920634627342224)
[2025-02-13 20:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43][root][INFO] - Training Epoch: 2/2, step 3173/7134 completed (loss: 0.08186525106430054, acc: 0.9898989796638489)
[2025-02-13 20:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43][root][INFO] - Training Epoch: 2/2, step 3174/7134 completed (loss: 0.10333610326051712, acc: 0.9759036302566528)
[2025-02-13 20:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:44][root][INFO] - Training Epoch: 2/2, step 3175/7134 completed (loss: 0.07363688945770264, acc: 0.9825581312179565)
[2025-02-13 20:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:44][root][INFO] - Training Epoch: 2/2, step 3176/7134 completed (loss: 0.2191806435585022, acc: 0.9709302186965942)
[2025-02-13 20:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45][root][INFO] - Training Epoch: 2/2, step 3177/7134 completed (loss: 0.13531804084777832, acc: 0.9691358208656311)
[2025-02-13 20:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45][root][INFO] - Training Epoch: 2/2, step 3178/7134 completed (loss: 0.08475951850414276, acc: 0.9599999785423279)
[2025-02-13 20:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45][root][INFO] - Training Epoch: 2/2, step 3179/7134 completed (loss: 0.10601800680160522, acc: 0.9642857313156128)
[2025-02-13 20:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46][root][INFO] - Training Epoch: 2/2, step 3180/7134 completed (loss: 0.06800806522369385, acc: 0.9876543283462524)
[2025-02-13 20:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46][root][INFO] - Training Epoch: 2/2, step 3181/7134 completed (loss: 0.08444466441869736, acc: 0.9779005646705627)
[2025-02-13 20:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46][root][INFO] - Training Epoch: 2/2, step 3182/7134 completed (loss: 0.07610537111759186, acc: 0.9878048896789551)
[2025-02-13 20:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:47][root][INFO] - Training Epoch: 2/2, step 3183/7134 completed (loss: 0.08435853570699692, acc: 0.970802903175354)
[2025-02-13 20:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:47][root][INFO] - Training Epoch: 2/2, step 3184/7134 completed (loss: 0.07295244187116623, acc: 0.987500011920929)
[2025-02-13 20:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48][root][INFO] - Training Epoch: 2/2, step 3185/7134 completed (loss: 0.03466300666332245, acc: 0.9937106966972351)
[2025-02-13 20:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48][root][INFO] - Training Epoch: 2/2, step 3186/7134 completed (loss: 0.06257782131433487, acc: 0.9743589758872986)
[2025-02-13 20:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48][root][INFO] - Training Epoch: 2/2, step 3187/7134 completed (loss: 0.20753180980682373, acc: 0.9714285731315613)
[2025-02-13 20:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49][root][INFO] - Training Epoch: 2/2, step 3188/7134 completed (loss: 0.053357016295194626, acc: 0.9888268113136292)
[2025-02-13 20:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49][root][INFO] - Training Epoch: 2/2, step 3189/7134 completed (loss: 0.06944848597049713, acc: 0.9876543283462524)
[2025-02-13 20:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50][root][INFO] - Training Epoch: 2/2, step 3190/7134 completed (loss: 0.12341634929180145, acc: 0.9750000238418579)
[2025-02-13 20:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50][root][INFO] - Training Epoch: 2/2, step 3191/7134 completed (loss: 0.06134264916181564, acc: 0.9937888383865356)
[2025-02-13 20:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50][root][INFO] - Training Epoch: 2/2, step 3192/7134 completed (loss: 0.20052942633628845, acc: 0.970802903175354)
[2025-02-13 20:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51][root][INFO] - Training Epoch: 2/2, step 3193/7134 completed (loss: 0.0982622280716896, acc: 0.9783783555030823)
[2025-02-13 20:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51][root][INFO] - Training Epoch: 2/2, step 3194/7134 completed (loss: 0.10474678128957748, acc: 0.9757575988769531)
[2025-02-13 20:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51][root][INFO] - Training Epoch: 2/2, step 3195/7134 completed (loss: 0.12586674094200134, acc: 0.9689440727233887)
[2025-02-13 20:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52][root][INFO] - Training Epoch: 2/2, step 3196/7134 completed (loss: 0.07448913902044296, acc: 0.9855072498321533)
[2025-02-13 20:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52][root][INFO] - Training Epoch: 2/2, step 3197/7134 completed (loss: 0.11070077866315842, acc: 0.976047933101654)
[2025-02-13 20:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53][root][INFO] - Training Epoch: 2/2, step 3198/7134 completed (loss: 0.04024482145905495, acc: 1.0)
[2025-02-13 20:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53][root][INFO] - Training Epoch: 2/2, step 3199/7134 completed (loss: 0.05177770182490349, acc: 0.9883720874786377)
[2025-02-13 20:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53][root][INFO] - Training Epoch: 2/2, step 3200/7134 completed (loss: 0.11356168240308762, acc: 0.9858155846595764)
[2025-02-13 20:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54][root][INFO] - Training Epoch: 2/2, step 3201/7134 completed (loss: 0.11715402454137802, acc: 0.9763779640197754)
[2025-02-13 20:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54][root][INFO] - Training Epoch: 2/2, step 3202/7134 completed (loss: 0.0870356485247612, acc: 0.983146071434021)
[2025-02-13 20:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54][root][INFO] - Training Epoch: 2/2, step 3203/7134 completed (loss: 0.06796030700206757, acc: 0.987500011920929)
[2025-02-13 20:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55][root][INFO] - Training Epoch: 2/2, step 3204/7134 completed (loss: 0.0815814733505249, acc: 0.9726027250289917)
[2025-02-13 20:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55][root][INFO] - Training Epoch: 2/2, step 3205/7134 completed (loss: 0.02020665444433689, acc: 0.9929078221321106)
[2025-02-13 20:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55][root][INFO] - Training Epoch: 2/2, step 3206/7134 completed (loss: 0.07009438425302505, acc: 0.9696969985961914)
[2025-02-13 20:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:56][root][INFO] - Training Epoch: 2/2, step 3207/7134 completed (loss: 0.11264435946941376, acc: 0.9693251252174377)
[2025-02-13 20:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:56][root][INFO] - Training Epoch: 2/2, step 3208/7134 completed (loss: 0.08179516345262527, acc: 0.9928057789802551)
[2025-02-13 20:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57][root][INFO] - Training Epoch: 2/2, step 3209/7134 completed (loss: 0.07009715586900711, acc: 0.987500011920929)
[2025-02-13 20:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57][root][INFO] - Training Epoch: 2/2, step 3210/7134 completed (loss: 0.03579423204064369, acc: 0.9935483932495117)
[2025-02-13 20:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57][root][INFO] - Training Epoch: 2/2, step 3211/7134 completed (loss: 0.06099695339798927, acc: 0.9915966391563416)
[2025-02-13 20:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58][root][INFO] - Training Epoch: 2/2, step 3212/7134 completed (loss: 0.048281230032444, acc: 0.9824561476707458)
[2025-02-13 20:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58][root][INFO] - Training Epoch: 2/2, step 3213/7134 completed (loss: 0.11568021774291992, acc: 0.9583333134651184)
[2025-02-13 20:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58][root][INFO] - Training Epoch: 2/2, step 3214/7134 completed (loss: 0.07794269919395447, acc: 0.9707602262496948)
[2025-02-13 20:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59][root][INFO] - Training Epoch: 2/2, step 3215/7134 completed (loss: 0.08667368441820145, acc: 0.976331353187561)
[2025-02-13 20:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59][root][INFO] - Training Epoch: 2/2, step 3216/7134 completed (loss: 0.06537289172410965, acc: 0.9814814925193787)
[2025-02-13 20:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00][root][INFO] - Training Epoch: 2/2, step 3217/7134 completed (loss: 0.07896668463945389, acc: 0.9719101190567017)
[2025-02-13 20:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00][root][INFO] - Training Epoch: 2/2, step 3218/7134 completed (loss: 0.07033337652683258, acc: 0.9870967864990234)
[2025-02-13 20:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00][root][INFO] - Training Epoch: 2/2, step 3219/7134 completed (loss: 0.10123597830533981, acc: 0.9523809552192688)
[2025-02-13 20:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01][root][INFO] - Training Epoch: 2/2, step 3220/7134 completed (loss: 0.055970724672079086, acc: 0.9774011373519897)
[2025-02-13 20:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01][root][INFO] - Training Epoch: 2/2, step 3221/7134 completed (loss: 0.04435352236032486, acc: 0.9838709831237793)
[2025-02-13 20:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02][root][INFO] - Training Epoch: 2/2, step 3222/7134 completed (loss: 0.13731008768081665, acc: 0.9601989984512329)
[2025-02-13 20:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02][root][INFO] - Training Epoch: 2/2, step 3223/7134 completed (loss: 0.20052765309810638, acc: 0.9693251252174377)
[2025-02-13 20:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02][root][INFO] - Training Epoch: 2/2, step 3224/7134 completed (loss: 0.1306491196155548, acc: 0.9653179049491882)
[2025-02-13 20:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03][root][INFO] - Training Epoch: 2/2, step 3225/7134 completed (loss: 0.08911755681037903, acc: 0.9700000286102295)
[2025-02-13 20:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03][root][INFO] - Training Epoch: 2/2, step 3226/7134 completed (loss: 0.09378328919410706, acc: 0.9801324605941772)
[2025-02-13 20:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03][root][INFO] - Training Epoch: 2/2, step 3227/7134 completed (loss: 0.1982131153345108, acc: 0.9547325372695923)
[2025-02-13 20:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04][root][INFO] - Training Epoch: 2/2, step 3228/7134 completed (loss: 0.05763135850429535, acc: 0.9848484992980957)
[2025-02-13 20:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04][root][INFO] - Training Epoch: 2/2, step 3229/7134 completed (loss: 0.1795060932636261, acc: 0.9595959782600403)
[2025-02-13 20:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05][root][INFO] - Training Epoch: 2/2, step 3230/7134 completed (loss: 0.06678151339292526, acc: 0.981249988079071)
[2025-02-13 20:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05][root][INFO] - Training Epoch: 2/2, step 3231/7134 completed (loss: 0.1001780778169632, acc: 0.9725274443626404)
[2025-02-13 20:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05][root][INFO] - Training Epoch: 2/2, step 3232/7134 completed (loss: 0.028725016862154007, acc: 0.9953703880310059)
[2025-02-13 20:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:06][root][INFO] - Training Epoch: 2/2, step 3233/7134 completed (loss: 0.10732357949018478, acc: 0.9685039520263672)
[2025-02-13 20:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:06][root][INFO] - Training Epoch: 2/2, step 3234/7134 completed (loss: 0.03312673419713974, acc: 0.9937106966972351)
[2025-02-13 20:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07][root][INFO] - Training Epoch: 2/2, step 3235/7134 completed (loss: 0.03697998821735382, acc: 0.9929078221321106)
[2025-02-13 20:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07][root][INFO] - Training Epoch: 2/2, step 3236/7134 completed (loss: 0.18780748546123505, acc: 0.9604519605636597)
[2025-02-13 20:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07][root][INFO] - Training Epoch: 2/2, step 3237/7134 completed (loss: 0.07598919421434402, acc: 0.9740259647369385)
[2025-02-13 20:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08][root][INFO] - Training Epoch: 2/2, step 3238/7134 completed (loss: 0.1138424351811409, acc: 0.9611111283302307)
[2025-02-13 20:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08][root][INFO] - Training Epoch: 2/2, step 3239/7134 completed (loss: 0.0374247282743454, acc: 0.9939024448394775)
[2025-02-13 20:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08][root][INFO] - Training Epoch: 2/2, step 3240/7134 completed (loss: 0.15011928975582123, acc: 0.9776536226272583)
[2025-02-13 20:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:09][root][INFO] - Training Epoch: 2/2, step 3241/7134 completed (loss: 0.053433265537023544, acc: 0.9882352948188782)
[2025-02-13 20:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:09][root][INFO] - Training Epoch: 2/2, step 3242/7134 completed (loss: 0.027168171480298042, acc: 0.9935064911842346)
[2025-02-13 20:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10][root][INFO] - Training Epoch: 2/2, step 3243/7134 completed (loss: 0.06255492568016052, acc: 0.9839572310447693)
[2025-02-13 20:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10][root][INFO] - Training Epoch: 2/2, step 3244/7134 completed (loss: 0.08722086250782013, acc: 0.9826589822769165)
[2025-02-13 20:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10][root][INFO] - Training Epoch: 2/2, step 3245/7134 completed (loss: 0.06670138984918594, acc: 0.9846153855323792)
[2025-02-13 20:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11][root][INFO] - Training Epoch: 2/2, step 3246/7134 completed (loss: 0.09825574606657028, acc: 0.9794871807098389)
[2025-02-13 20:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11][root][INFO] - Training Epoch: 2/2, step 3247/7134 completed (loss: 0.043604303151369095, acc: 0.9807692170143127)
[2025-02-13 20:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12][root][INFO] - Training Epoch: 2/2, step 3248/7134 completed (loss: 0.013348824344575405, acc: 1.0)
[2025-02-13 20:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12][root][INFO] - Training Epoch: 2/2, step 3249/7134 completed (loss: 0.03411856293678284, acc: 0.9881656765937805)
[2025-02-13 20:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12][root][INFO] - Training Epoch: 2/2, step 3250/7134 completed (loss: 0.11679688096046448, acc: 0.9640718698501587)
[2025-02-13 20:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13][root][INFO] - Training Epoch: 2/2, step 3251/7134 completed (loss: 0.05690598487854004, acc: 0.9871794581413269)
[2025-02-13 20:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13][root][INFO] - Training Epoch: 2/2, step 3252/7134 completed (loss: 0.020387329161167145, acc: 1.0)
[2025-02-13 20:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14][root][INFO] - Training Epoch: 2/2, step 3253/7134 completed (loss: 0.05608440190553665, acc: 0.9802631735801697)
[2025-02-13 20:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14][root][INFO] - Training Epoch: 2/2, step 3254/7134 completed (loss: 0.05328822880983353, acc: 0.9870129823684692)
[2025-02-13 20:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14][root][INFO] - Training Epoch: 2/2, step 3255/7134 completed (loss: 0.05263930931687355, acc: 0.9886363744735718)
[2025-02-13 20:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15][root][INFO] - Training Epoch: 2/2, step 3256/7134 completed (loss: 0.06979303807020187, acc: 0.9937888383865356)
[2025-02-13 20:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15][root][INFO] - Training Epoch: 2/2, step 3257/7134 completed (loss: 0.1570240706205368, acc: 0.9757575988769531)
[2025-02-13 20:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:16][root][INFO] - Training Epoch: 2/2, step 3258/7134 completed (loss: 0.12924860417842865, acc: 0.9593495726585388)
[2025-02-13 20:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:16][root][INFO] - Training Epoch: 2/2, step 3259/7134 completed (loss: 0.11211445182561874, acc: 0.9689440727233887)
[2025-02-13 20:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17][root][INFO] - Training Epoch: 2/2, step 3260/7134 completed (loss: 0.20962919294834137, acc: 0.9586206674575806)
[2025-02-13 20:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17][root][INFO] - Training Epoch: 2/2, step 3261/7134 completed (loss: 0.15244485437870026, acc: 0.9489051103591919)
[2025-02-13 20:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17][root][INFO] - Training Epoch: 2/2, step 3262/7134 completed (loss: 0.09847167879343033, acc: 0.9691358208656311)
[2025-02-13 20:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:18][root][INFO] - Training Epoch: 2/2, step 3263/7134 completed (loss: 0.08618390560150146, acc: 0.9822485446929932)
[2025-02-13 20:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:18][root][INFO] - Training Epoch: 2/2, step 3264/7134 completed (loss: 0.0912100300192833, acc: 0.9695122241973877)
[2025-02-13 20:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19][root][INFO] - Training Epoch: 2/2, step 3265/7134 completed (loss: 0.12696535885334015, acc: 0.9808917045593262)
[2025-02-13 20:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19][root][INFO] - Training Epoch: 2/2, step 3266/7134 completed (loss: 0.06190064549446106, acc: 0.9862068891525269)
[2025-02-13 20:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19][root][INFO] - Training Epoch: 2/2, step 3267/7134 completed (loss: 0.13499325513839722, acc: 0.9575757384300232)
[2025-02-13 20:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20][root][INFO] - Training Epoch: 2/2, step 3268/7134 completed (loss: 0.0723167210817337, acc: 0.9794520735740662)
[2025-02-13 20:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20][root][INFO] - Training Epoch: 2/2, step 3269/7134 completed (loss: 0.06764617562294006, acc: 0.9893617033958435)
[2025-02-13 20:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:21][root][INFO] - Training Epoch: 2/2, step 3270/7134 completed (loss: 0.07798027992248535, acc: 0.9729729890823364)
[2025-02-13 20:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:21][root][INFO] - Training Epoch: 2/2, step 3271/7134 completed (loss: 0.016635678708553314, acc: 1.0)
[2025-02-13 20:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22][root][INFO] - Training Epoch: 2/2, step 3272/7134 completed (loss: 0.0715305283665657, acc: 0.9800000190734863)
[2025-02-13 20:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22][root][INFO] - Training Epoch: 2/2, step 3273/7134 completed (loss: 0.0777682363986969, acc: 0.9714285731315613)
[2025-02-13 20:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22][root][INFO] - Training Epoch: 2/2, step 3274/7134 completed (loss: 0.04793381690979004, acc: 0.9875776171684265)
[2025-02-13 20:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23][root][INFO] - Training Epoch: 2/2, step 3275/7134 completed (loss: 0.43116244673728943, acc: 0.9041916131973267)
[2025-02-13 20:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23][root][INFO] - Training Epoch: 2/2, step 3276/7134 completed (loss: 0.08480672538280487, acc: 0.9803921580314636)
[2025-02-13 20:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24][root][INFO] - Training Epoch: 2/2, step 3277/7134 completed (loss: 0.11274444311857224, acc: 0.9640287756919861)
[2025-02-13 20:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24][root][INFO] - Training Epoch: 2/2, step 3278/7134 completed (loss: 0.07377393543720245, acc: 0.9919354915618896)
[2025-02-13 20:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24][root][INFO] - Training Epoch: 2/2, step 3279/7134 completed (loss: 0.12422100454568863, acc: 0.9588235020637512)
[2025-02-13 20:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25][root][INFO] - Training Epoch: 2/2, step 3280/7134 completed (loss: 0.22068063914775848, acc: 0.932692289352417)
[2025-02-13 20:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25][root][INFO] - Training Epoch: 2/2, step 3281/7134 completed (loss: 0.11569129675626755, acc: 0.9727891087532043)
[2025-02-13 20:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26][root][INFO] - Training Epoch: 2/2, step 3282/7134 completed (loss: 0.04526330903172493, acc: 0.9923664331436157)
[2025-02-13 20:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26][root][INFO] - Training Epoch: 2/2, step 3283/7134 completed (loss: 0.046317651867866516, acc: 0.9901960492134094)
[2025-02-13 20:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26][root][INFO] - Training Epoch: 2/2, step 3284/7134 completed (loss: 0.046564992517232895, acc: 0.9862068891525269)
[2025-02-13 20:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27][root][INFO] - Training Epoch: 2/2, step 3285/7134 completed (loss: 0.09067708998918533, acc: 0.970588207244873)
[2025-02-13 20:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27][root][INFO] - Training Epoch: 2/2, step 3286/7134 completed (loss: 0.11256003379821777, acc: 0.9846153855323792)
[2025-02-13 20:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28][root][INFO] - Training Epoch: 2/2, step 3287/7134 completed (loss: 0.07008612155914307, acc: 0.9777777791023254)
[2025-02-13 20:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28][root][INFO] - Training Epoch: 2/2, step 3288/7134 completed (loss: 0.12514418363571167, acc: 0.9856114983558655)
[2025-02-13 20:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28][root][INFO] - Training Epoch: 2/2, step 3289/7134 completed (loss: 0.0821741595864296, acc: 0.9779411554336548)
[2025-02-13 20:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29][root][INFO] - Training Epoch: 2/2, step 3290/7134 completed (loss: 0.10795173794031143, acc: 0.9663865566253662)
[2025-02-13 20:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29][root][INFO] - Training Epoch: 2/2, step 3291/7134 completed (loss: 0.09142765402793884, acc: 0.9900990128517151)
[2025-02-13 20:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29][root][INFO] - Training Epoch: 2/2, step 3292/7134 completed (loss: 0.15544933080673218, acc: 0.9747899174690247)
[2025-02-13 20:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:30][root][INFO] - Training Epoch: 2/2, step 3293/7134 completed (loss: 0.09035235643386841, acc: 0.9848484992980957)
[2025-02-13 20:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:30][root][INFO] - Training Epoch: 2/2, step 3294/7134 completed (loss: 0.17559568583965302, acc: 0.9621211886405945)
[2025-02-13 20:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31][root][INFO] - Training Epoch: 2/2, step 3295/7134 completed (loss: 0.023217443376779556, acc: 0.9919354915618896)
[2025-02-13 20:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31][root][INFO] - Training Epoch: 2/2, step 3296/7134 completed (loss: 0.09699930250644684, acc: 0.9692307710647583)
[2025-02-13 20:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31][root][INFO] - Training Epoch: 2/2, step 3297/7134 completed (loss: 0.07316842675209045, acc: 0.9905660152435303)
[2025-02-13 20:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32][root][INFO] - Training Epoch: 2/2, step 3298/7134 completed (loss: 0.20313464105129242, acc: 0.9548872113227844)
[2025-02-13 20:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32][root][INFO] - Training Epoch: 2/2, step 3299/7134 completed (loss: 0.04723271355032921, acc: 0.9842519760131836)
[2025-02-13 20:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32][root][INFO] - Training Epoch: 2/2, step 3300/7134 completed (loss: 0.09965649992227554, acc: 0.9774436354637146)
[2025-02-13 20:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33][root][INFO] - Training Epoch: 2/2, step 3301/7134 completed (loss: 0.03816172853112221, acc: 0.9902912378311157)
[2025-02-13 20:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33][root][INFO] - Training Epoch: 2/2, step 3302/7134 completed (loss: 0.16532865166664124, acc: 0.9720279574394226)
[2025-02-13 20:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33][root][INFO] - Training Epoch: 2/2, step 3303/7134 completed (loss: 0.09830228984355927, acc: 0.9702970385551453)
[2025-02-13 20:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:34][root][INFO] - Training Epoch: 2/2, step 3304/7134 completed (loss: 0.059456732124090195, acc: 0.9862385392189026)
[2025-02-13 20:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:34][root][INFO] - Training Epoch: 2/2, step 3305/7134 completed (loss: 0.09973055869340897, acc: 0.9874213933944702)
[2025-02-13 20:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35][root][INFO] - Training Epoch: 2/2, step 3306/7134 completed (loss: 0.05938089266419411, acc: 0.982758641242981)
[2025-02-13 20:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35][root][INFO] - Training Epoch: 2/2, step 3307/7134 completed (loss: 0.04714284464716911, acc: 0.9851852059364319)
[2025-02-13 20:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35][root][INFO] - Training Epoch: 2/2, step 3308/7134 completed (loss: 0.15717089176177979, acc: 0.970588207244873)
[2025-02-13 20:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36][root][INFO] - Training Epoch: 2/2, step 3309/7134 completed (loss: 0.13844697177410126, acc: 0.9679487347602844)
[2025-02-13 20:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36][root][INFO] - Training Epoch: 2/2, step 3310/7134 completed (loss: 0.06381691247224808, acc: 0.9818181991577148)
[2025-02-13 20:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36][root][INFO] - Training Epoch: 2/2, step 3311/7134 completed (loss: 0.09762870520353317, acc: 0.9702970385551453)
[2025-02-13 20:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37][root][INFO] - Training Epoch: 2/2, step 3312/7134 completed (loss: 0.06290572881698608, acc: 0.9849624037742615)
[2025-02-13 20:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37][root][INFO] - Training Epoch: 2/2, step 3313/7134 completed (loss: 0.07416719198226929, acc: 0.9897959232330322)
[2025-02-13 20:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37][root][INFO] - Training Epoch: 2/2, step 3314/7134 completed (loss: 0.056284066289663315, acc: 0.9908257126808167)
[2025-02-13 20:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38][root][INFO] - Training Epoch: 2/2, step 3315/7134 completed (loss: 0.09325233101844788, acc: 0.9822485446929932)
[2025-02-13 20:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38][root][INFO] - Training Epoch: 2/2, step 3316/7134 completed (loss: 0.14990359544754028, acc: 0.9701492786407471)
[2025-02-13 20:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39][root][INFO] - Training Epoch: 2/2, step 3317/7134 completed (loss: 0.15255527198314667, acc: 0.9586777091026306)
[2025-02-13 20:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39][root][INFO] - Training Epoch: 2/2, step 3318/7134 completed (loss: 0.029597625136375427, acc: 1.0)
[2025-02-13 20:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39][root][INFO] - Training Epoch: 2/2, step 3319/7134 completed (loss: 0.13094741106033325, acc: 0.95652174949646)
[2025-02-13 20:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:40][root][INFO] - Training Epoch: 2/2, step 3320/7134 completed (loss: 0.12359365820884705, acc: 0.9830508232116699)
[2025-02-13 20:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:40][root][INFO] - Training Epoch: 2/2, step 3321/7134 completed (loss: 0.21457131206989288, acc: 0.9428571462631226)
[2025-02-13 20:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41][root][INFO] - Training Epoch: 2/2, step 3322/7134 completed (loss: 0.10147903114557266, acc: 0.9736841917037964)
[2025-02-13 20:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41][root][INFO] - Training Epoch: 2/2, step 3323/7134 completed (loss: 0.08695941418409348, acc: 0.9824561476707458)
[2025-02-13 20:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41][root][INFO] - Training Epoch: 2/2, step 3324/7134 completed (loss: 0.18927808105945587, acc: 0.9430052042007446)
[2025-02-13 20:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42][root][INFO] - Training Epoch: 2/2, step 3325/7134 completed (loss: 0.1341800093650818, acc: 0.9580838084220886)
[2025-02-13 20:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42][root][INFO] - Training Epoch: 2/2, step 3326/7134 completed (loss: 0.16796375811100006, acc: 0.9575757384300232)
[2025-02-13 20:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42][root][INFO] - Training Epoch: 2/2, step 3327/7134 completed (loss: 0.17146752774715424, acc: 0.950276255607605)
[2025-02-13 20:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43][root][INFO] - Training Epoch: 2/2, step 3328/7134 completed (loss: 0.07513175904750824, acc: 0.978723406791687)
[2025-02-13 20:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43][root][INFO] - Training Epoch: 2/2, step 3329/7134 completed (loss: 0.13557226955890656, acc: 0.96875)
[2025-02-13 20:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44][root][INFO] - Training Epoch: 2/2, step 3330/7134 completed (loss: 0.0884949341416359, acc: 0.9743589758872986)
[2025-02-13 20:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44][root][INFO] - Training Epoch: 2/2, step 3331/7134 completed (loss: 0.2655720114707947, acc: 0.9289940595626831)
[2025-02-13 20:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44][root][INFO] - Training Epoch: 2/2, step 3332/7134 completed (loss: 0.20223283767700195, acc: 0.9733333587646484)
[2025-02-13 20:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45][root][INFO] - Training Epoch: 2/2, step 3333/7134 completed (loss: 0.1358496993780136, acc: 0.9652174115180969)
[2025-02-13 20:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45][root][INFO] - Training Epoch: 2/2, step 3334/7134 completed (loss: 0.11397658288478851, acc: 0.967391312122345)
[2025-02-13 20:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45][root][INFO] - Training Epoch: 2/2, step 3335/7134 completed (loss: 0.04481762647628784, acc: 0.9933333396911621)
[2025-02-13 20:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46][root][INFO] - Training Epoch: 2/2, step 3336/7134 completed (loss: 0.029821626842021942, acc: 0.9940119981765747)
[2025-02-13 20:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46][root][INFO] - Training Epoch: 2/2, step 3337/7134 completed (loss: 0.03719022125005722, acc: 0.9915966391563416)
[2025-02-13 20:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46][root][INFO] - Training Epoch: 2/2, step 3338/7134 completed (loss: 0.05018482729792595, acc: 0.984455943107605)
[2025-02-13 20:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47][root][INFO] - Training Epoch: 2/2, step 3339/7134 completed (loss: 0.10301243513822556, acc: 0.966183602809906)
[2025-02-13 20:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47][root][INFO] - Training Epoch: 2/2, step 3340/7134 completed (loss: 0.05380893871188164, acc: 0.988095223903656)
[2025-02-13 20:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48][root][INFO] - Training Epoch: 2/2, step 3341/7134 completed (loss: 0.0622212179005146, acc: 0.9880239367485046)
[2025-02-13 20:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48][root][INFO] - Training Epoch: 2/2, step 3342/7134 completed (loss: 0.07613438367843628, acc: 0.9794520735740662)
[2025-02-13 20:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48][root][INFO] - Training Epoch: 2/2, step 3343/7134 completed (loss: 0.03624499589204788, acc: 0.9913793206214905)
[2025-02-13 20:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49][root][INFO] - Training Epoch: 2/2, step 3344/7134 completed (loss: 0.08754821866750717, acc: 0.9632353186607361)
[2025-02-13 20:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49][root][INFO] - Training Epoch: 2/2, step 3345/7134 completed (loss: 0.11129846423864365, acc: 0.9860140085220337)
[2025-02-13 20:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49][root][INFO] - Training Epoch: 2/2, step 3346/7134 completed (loss: 0.07474887371063232, acc: 0.9756097793579102)
[2025-02-13 20:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50][root][INFO] - Training Epoch: 2/2, step 3347/7134 completed (loss: 0.19895218312740326, acc: 0.9542483687400818)
[2025-02-13 20:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50][root][INFO] - Training Epoch: 2/2, step 3348/7134 completed (loss: 0.07290469855070114, acc: 0.9746835231781006)
[2025-02-13 20:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50][root][INFO] - Training Epoch: 2/2, step 3349/7134 completed (loss: 0.057570960372686386, acc: 0.9922480583190918)
[2025-02-13 20:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51][root][INFO] - Training Epoch: 2/2, step 3350/7134 completed (loss: 0.07288847118616104, acc: 0.9752066135406494)
[2025-02-13 20:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51][root][INFO] - Training Epoch: 2/2, step 3351/7134 completed (loss: 0.1490442007780075, acc: 0.957446813583374)
[2025-02-13 20:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52][root][INFO] - Training Epoch: 2/2, step 3352/7134 completed (loss: 0.04920891672372818, acc: 0.9851852059364319)
[2025-02-13 20:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52][root][INFO] - Training Epoch: 2/2, step 3353/7134 completed (loss: 0.13121606409549713, acc: 0.9593495726585388)
[2025-02-13 20:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52][root][INFO] - Training Epoch: 2/2, step 3354/7134 completed (loss: 0.056189339607954025, acc: 0.9683544039726257)
[2025-02-13 20:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53][root][INFO] - Training Epoch: 2/2, step 3355/7134 completed (loss: 0.030881918966770172, acc: 0.9878048896789551)
[2025-02-13 20:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53][root][INFO] - Training Epoch: 2/2, step 3356/7134 completed (loss: 0.0059102741070091724, acc: 1.0)
[2025-02-13 20:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53][root][INFO] - Training Epoch: 2/2, step 3357/7134 completed (loss: 0.06654733419418335, acc: 0.9880239367485046)
[2025-02-13 20:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54][root][INFO] - Training Epoch: 2/2, step 3358/7134 completed (loss: 0.14514325559139252, acc: 0.9411764740943909)
[2025-02-13 20:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54][root][INFO] - Training Epoch: 2/2, step 3359/7134 completed (loss: 0.057626187801361084, acc: 0.9931507110595703)
[2025-02-13 20:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54][root][INFO] - Training Epoch: 2/2, step 3360/7134 completed (loss: 0.12870946526527405, acc: 0.9739130139350891)
[2025-02-13 20:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55][root][INFO] - Training Epoch: 2/2, step 3361/7134 completed (loss: 0.06732060760259628, acc: 0.9910714030265808)
[2025-02-13 20:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55][root][INFO] - Training Epoch: 2/2, step 3362/7134 completed (loss: 0.10929806530475616, acc: 0.9727272987365723)
[2025-02-13 20:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55][root][INFO] - Training Epoch: 2/2, step 3363/7134 completed (loss: 0.05927407369017601, acc: 0.9813084006309509)
[2025-02-13 20:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56][root][INFO] - Training Epoch: 2/2, step 3364/7134 completed (loss: 0.19888408482074738, acc: 0.9346405267715454)
[2025-02-13 20:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56][root][INFO] - Training Epoch: 2/2, step 3365/7134 completed (loss: 0.05127452686429024, acc: 0.991150438785553)
[2025-02-13 20:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57][root][INFO] - Training Epoch: 2/2, step 3366/7134 completed (loss: 0.028376450762152672, acc: 1.0)
[2025-02-13 20:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57][root][INFO] - Training Epoch: 2/2, step 3367/7134 completed (loss: 0.05380072444677353, acc: 0.9839572310447693)
[2025-02-13 20:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57][root][INFO] - Training Epoch: 2/2, step 3368/7134 completed (loss: 0.06496059894561768, acc: 0.9813664555549622)
[2025-02-13 20:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58][root][INFO] - Training Epoch: 2/2, step 3369/7134 completed (loss: 0.03929818794131279, acc: 0.9945945739746094)
[2025-02-13 20:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58][root][INFO] - Training Epoch: 2/2, step 3370/7134 completed (loss: 0.03727858513593674, acc: 0.9938650131225586)
[2025-02-13 20:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58][root][INFO] - Training Epoch: 2/2, step 3371/7134 completed (loss: 0.0310402549803257, acc: 0.9930555820465088)
[2025-02-13 20:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59][root][INFO] - Training Epoch: 2/2, step 3372/7134 completed (loss: 0.08265457302331924, acc: 0.9931972622871399)
[2025-02-13 20:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59][root][INFO] - Training Epoch: 2/2, step 3373/7134 completed (loss: 0.029506368562579155, acc: 0.9934640526771545)
[2025-02-13 20:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59][root][INFO] - Training Epoch: 2/2, step 3374/7134 completed (loss: 0.0745396539568901, acc: 0.977142870426178)
[2025-02-13 20:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00][root][INFO] - Training Epoch: 2/2, step 3375/7134 completed (loss: 0.05573649704456329, acc: 0.9820359349250793)
[2025-02-13 20:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00][root][INFO] - Training Epoch: 2/2, step 3376/7134 completed (loss: 0.04689405858516693, acc: 0.9947090148925781)
[2025-02-13 20:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01][root][INFO] - Training Epoch: 2/2, step 3377/7134 completed (loss: 0.03503658249974251, acc: 0.9949748516082764)
[2025-02-13 20:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01][root][INFO] - Training Epoch: 2/2, step 3378/7134 completed (loss: 0.010998384095728397, acc: 1.0)
[2025-02-13 20:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01][root][INFO] - Training Epoch: 2/2, step 3379/7134 completed (loss: 0.06155378744006157, acc: 0.9864864945411682)
[2025-02-13 20:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02][root][INFO] - Training Epoch: 2/2, step 3380/7134 completed (loss: 0.08559897541999817, acc: 0.97826087474823)
[2025-02-13 20:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02][root][INFO] - Training Epoch: 2/2, step 3381/7134 completed (loss: 0.06046747416257858, acc: 0.9878048896789551)
[2025-02-13 20:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03][root][INFO] - Training Epoch: 2/2, step 3382/7134 completed (loss: 0.03943250700831413, acc: 0.984375)
[2025-02-13 20:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03][root][INFO] - Training Epoch: 2/2, step 3383/7134 completed (loss: 0.07468806952238083, acc: 0.9790209531784058)
[2025-02-13 20:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03][root][INFO] - Training Epoch: 2/2, step 3384/7134 completed (loss: 0.10433655977249146, acc: 0.970802903175354)
[2025-02-13 20:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04][root][INFO] - Training Epoch: 2/2, step 3385/7134 completed (loss: 0.035363417118787766, acc: 0.9861111044883728)
[2025-02-13 20:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04][root][INFO] - Training Epoch: 2/2, step 3386/7134 completed (loss: 0.06408185511827469, acc: 0.9704142212867737)
[2025-02-13 20:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04][root][INFO] - Training Epoch: 2/2, step 3387/7134 completed (loss: 0.08441591262817383, acc: 0.9825581312179565)
[2025-02-13 20:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05][root][INFO] - Training Epoch: 2/2, step 3388/7134 completed (loss: 0.053513478487730026, acc: 0.9743589758872986)
[2025-02-13 20:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05][root][INFO] - Training Epoch: 2/2, step 3389/7134 completed (loss: 0.08715783804655075, acc: 0.9781420826911926)
[2025-02-13 20:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06][root][INFO] - Training Epoch: 2/2, step 3390/7134 completed (loss: 0.05136996880173683, acc: 0.9829545617103577)
[2025-02-13 20:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06][root][INFO] - Training Epoch: 2/2, step 3391/7134 completed (loss: 0.07582752406597137, acc: 0.9817073345184326)
[2025-02-13 20:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06][root][INFO] - Training Epoch: 2/2, step 3392/7134 completed (loss: 0.02628568559885025, acc: 0.9923664331436157)
[2025-02-13 20:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07][root][INFO] - Training Epoch: 2/2, step 3393/7134 completed (loss: 0.09720370173454285, acc: 0.9720279574394226)
[2025-02-13 20:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07][root][INFO] - Training Epoch: 2/2, step 3394/7134 completed (loss: 0.04008263722062111, acc: 1.0)
[2025-02-13 20:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07][root][INFO] - Training Epoch: 2/2, step 3395/7134 completed (loss: 0.1604893058538437, acc: 0.9539473652839661)
[2025-02-13 20:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08][root][INFO] - Training Epoch: 2/2, step 3396/7134 completed (loss: 0.09492723643779755, acc: 0.9807692170143127)
[2025-02-13 20:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08][root][INFO] - Training Epoch: 2/2, step 3397/7134 completed (loss: 0.03893887624144554, acc: 0.9900497794151306)
[2025-02-13 20:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08][root][INFO] - Training Epoch: 2/2, step 3398/7134 completed (loss: 0.0666293278336525, acc: 0.97826087474823)
[2025-02-13 20:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09][root][INFO] - Training Epoch: 2/2, step 3399/7134 completed (loss: 0.16878966987133026, acc: 0.9583333134651184)
[2025-02-13 20:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09][root][INFO] - Training Epoch: 2/2, step 3400/7134 completed (loss: 0.0700409859418869, acc: 0.9685039520263672)
[2025-02-13 20:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10][root][INFO] - Training Epoch: 2/2, step 3401/7134 completed (loss: 0.08773138374090195, acc: 0.9863945841789246)
[2025-02-13 20:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10][root][INFO] - Training Epoch: 2/2, step 3402/7134 completed (loss: 0.2914786636829376, acc: 0.9518716335296631)
[2025-02-13 20:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10][root][INFO] - Training Epoch: 2/2, step 3403/7134 completed (loss: 0.1711721271276474, acc: 0.9595959782600403)
[2025-02-13 20:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11][root][INFO] - Training Epoch: 2/2, step 3404/7134 completed (loss: 0.03897722810506821, acc: 0.9874213933944702)
[2025-02-13 20:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11][root][INFO] - Training Epoch: 2/2, step 3405/7134 completed (loss: 0.21397699415683746, acc: 0.9485294222831726)
[2025-02-13 20:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12][root][INFO] - Training Epoch: 2/2, step 3406/7134 completed (loss: 0.21124190092086792, acc: 0.9350000023841858)
[2025-02-13 20:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12][root][INFO] - Training Epoch: 2/2, step 3407/7134 completed (loss: 0.08312831819057465, acc: 0.987730085849762)
[2025-02-13 20:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12][root][INFO] - Training Epoch: 2/2, step 3408/7134 completed (loss: 0.03452751040458679, acc: 0.9905660152435303)
[2025-02-13 20:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13][root][INFO] - Training Epoch: 2/2, step 3409/7134 completed (loss: 0.14473740756511688, acc: 0.977011501789093)
[2025-02-13 20:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13][root][INFO] - Training Epoch: 2/2, step 3410/7134 completed (loss: 0.17997632920742035, acc: 0.9634146094322205)
[2025-02-13 20:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13][root][INFO] - Training Epoch: 2/2, step 3411/7134 completed (loss: 0.07858452945947647, acc: 0.9835164546966553)
[2025-02-13 20:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14][root][INFO] - Training Epoch: 2/2, step 3412/7134 completed (loss: 0.1286897510290146, acc: 0.9750000238418579)
[2025-02-13 20:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14][root][INFO] - Training Epoch: 2/2, step 3413/7134 completed (loss: 0.08411779999732971, acc: 0.9914529919624329)
[2025-02-13 20:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14][root][INFO] - Training Epoch: 2/2, step 3414/7134 completed (loss: 0.0465460866689682, acc: 0.9791666865348816)
[2025-02-13 20:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15][root][INFO] - Training Epoch: 2/2, step 3415/7134 completed (loss: 0.03723416477441788, acc: 0.9928571581840515)
[2025-02-13 20:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15][root][INFO] - Training Epoch: 2/2, step 3416/7134 completed (loss: 0.13977651298046112, acc: 0.9617486596107483)
[2025-02-13 20:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16][root][INFO] - Training Epoch: 2/2, step 3417/7134 completed (loss: 0.05968242883682251, acc: 0.9880239367485046)
[2025-02-13 20:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16][root][INFO] - Training Epoch: 2/2, step 3418/7134 completed (loss: 0.0809803381562233, acc: 0.9754601120948792)
[2025-02-13 20:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16][root][INFO] - Training Epoch: 2/2, step 3419/7134 completed (loss: 0.08429911732673645, acc: 0.9759036302566528)
[2025-02-13 20:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17][root][INFO] - Training Epoch: 2/2, step 3420/7134 completed (loss: 0.01306402962654829, acc: 1.0)
[2025-02-13 20:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17][root][INFO] - Training Epoch: 2/2, step 3421/7134 completed (loss: 0.06920115649700165, acc: 0.9743589758872986)
[2025-02-13 20:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17][root][INFO] - Training Epoch: 2/2, step 3422/7134 completed (loss: 0.1386042833328247, acc: 0.9852941036224365)
[2025-02-13 20:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18][root][INFO] - Training Epoch: 2/2, step 3423/7134 completed (loss: 0.022165572270751, acc: 1.0)
[2025-02-13 20:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18][root][INFO] - Training Epoch: 2/2, step 3424/7134 completed (loss: 0.08519654721021652, acc: 0.9866666793823242)
[2025-02-13 20:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18][root][INFO] - Training Epoch: 2/2, step 3425/7134 completed (loss: 0.08305124938488007, acc: 0.9789473414421082)
[2025-02-13 20:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19][root][INFO] - Training Epoch: 2/2, step 3426/7134 completed (loss: 0.12913544476032257, acc: 0.988095223903656)
[2025-02-13 20:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19][root][INFO] - Training Epoch: 2/2, step 3427/7134 completed (loss: 0.0757022425532341, acc: 0.9935064911842346)
[2025-02-13 20:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20][root][INFO] - Training Epoch: 2/2, step 3428/7134 completed (loss: 0.09268045425415039, acc: 0.9764705896377563)
[2025-02-13 20:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20][root][INFO] - Training Epoch: 2/2, step 3429/7134 completed (loss: 0.05159979313611984, acc: 0.9929577708244324)
[2025-02-13 20:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20][root][INFO] - Training Epoch: 2/2, step 3430/7134 completed (loss: 0.1856611967086792, acc: 0.9589040875434875)
[2025-02-13 20:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21][root][INFO] - Training Epoch: 2/2, step 3431/7134 completed (loss: 0.11198337376117706, acc: 0.97826087474823)
[2025-02-13 20:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21][root][INFO] - Training Epoch: 2/2, step 3432/7134 completed (loss: 0.04888906702399254, acc: 0.987730085849762)
[2025-02-13 20:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21][root][INFO] - Training Epoch: 2/2, step 3433/7134 completed (loss: 0.09750320762395859, acc: 0.9632353186607361)
[2025-02-13 20:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22][root][INFO] - Training Epoch: 2/2, step 3434/7134 completed (loss: 0.07451024651527405, acc: 0.9857142567634583)
[2025-02-13 20:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22][root][INFO] - Training Epoch: 2/2, step 3435/7134 completed (loss: 0.08402799069881439, acc: 0.9793103337287903)
[2025-02-13 20:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22][root][INFO] - Training Epoch: 2/2, step 3436/7134 completed (loss: 0.09568030387163162, acc: 0.9781022071838379)
[2025-02-13 20:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23][root][INFO] - Training Epoch: 2/2, step 3437/7134 completed (loss: 0.0758277103304863, acc: 0.9834710955619812)
[2025-02-13 20:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23][root][INFO] - Training Epoch: 2/2, step 3438/7134 completed (loss: 0.13168969750404358, acc: 0.9824561476707458)
[2025-02-13 20:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24][root][INFO] - Training Epoch: 2/2, step 3439/7134 completed (loss: 0.13333620131015778, acc: 0.976190447807312)
[2025-02-13 20:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24][root][INFO] - Training Epoch: 2/2, step 3440/7134 completed (loss: 0.15128716826438904, acc: 0.9568965435028076)
[2025-02-13 20:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24][root][INFO] - Training Epoch: 2/2, step 3441/7134 completed (loss: 0.1520768404006958, acc: 0.9534883499145508)
[2025-02-13 20:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25][root][INFO] - Training Epoch: 2/2, step 3442/7134 completed (loss: 0.044105853885412216, acc: 0.9929078221321106)
[2025-02-13 20:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25][root][INFO] - Training Epoch: 2/2, step 3443/7134 completed (loss: 0.2485058456659317, acc: 0.966292142868042)
[2025-02-13 20:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25][root][INFO] - Training Epoch: 2/2, step 3444/7134 completed (loss: 0.12773582339286804, acc: 0.9490445852279663)
[2025-02-13 20:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:26][root][INFO] - Training Epoch: 2/2, step 3445/7134 completed (loss: 0.2477095127105713, acc: 0.9624060392379761)
[2025-02-13 20:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:26][root][INFO] - Training Epoch: 2/2, step 3446/7134 completed (loss: 0.15560747683048248, acc: 0.9599999785423279)
[2025-02-13 20:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27][root][INFO] - Training Epoch: 2/2, step 3447/7134 completed (loss: 0.1787751317024231, acc: 0.9506173133850098)
[2025-02-13 20:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27][root][INFO] - Training Epoch: 2/2, step 3448/7134 completed (loss: 0.1414620578289032, acc: 0.9595375657081604)
[2025-02-13 20:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27][root][INFO] - Training Epoch: 2/2, step 3449/7134 completed (loss: 0.049349766224622726, acc: 1.0)
[2025-02-13 20:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28][root][INFO] - Training Epoch: 2/2, step 3450/7134 completed (loss: 0.11212877184152603, acc: 0.9803921580314636)
[2025-02-13 20:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28][root][INFO] - Training Epoch: 2/2, step 3451/7134 completed (loss: 0.13539892435073853, acc: 0.9503546357154846)
[2025-02-13 20:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28][root][INFO] - Training Epoch: 2/2, step 3452/7134 completed (loss: 0.25778865814208984, acc: 0.9268292784690857)
[2025-02-13 20:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29][root][INFO] - Training Epoch: 2/2, step 3453/7134 completed (loss: 0.11108272522687912, acc: 0.9722222089767456)
[2025-02-13 20:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29][root][INFO] - Training Epoch: 2/2, step 3454/7134 completed (loss: 0.10445200651884079, acc: 0.9638554453849792)
[2025-02-13 20:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29][root][INFO] - Training Epoch: 2/2, step 3455/7134 completed (loss: 0.0292369294911623, acc: 1.0)
[2025-02-13 20:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30][root][INFO] - Training Epoch: 2/2, step 3456/7134 completed (loss: 0.04210821911692619, acc: 0.9899497628211975)
[2025-02-13 20:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30][root][INFO] - Training Epoch: 2/2, step 3457/7134 completed (loss: 0.05203285440802574, acc: 0.9929577708244324)
[2025-02-13 20:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31][root][INFO] - Training Epoch: 2/2, step 3458/7134 completed (loss: 0.13164907693862915, acc: 0.9780219793319702)
[2025-02-13 20:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31][root][INFO] - Training Epoch: 2/2, step 3459/7134 completed (loss: 0.08736947178840637, acc: 0.9748427867889404)
[2025-02-13 20:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31][root][INFO] - Training Epoch: 2/2, step 3460/7134 completed (loss: 0.4262830913066864, acc: 0.9090909361839294)
[2025-02-13 20:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32][root][INFO] - Training Epoch: 2/2, step 3461/7134 completed (loss: 0.42828118801116943, acc: 0.8769230842590332)
[2025-02-13 20:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32][root][INFO] - Training Epoch: 2/2, step 3462/7134 completed (loss: 0.21754984557628632, acc: 0.9707602262496948)
[2025-02-13 20:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32][root][INFO] - Training Epoch: 2/2, step 3463/7134 completed (loss: 0.04079420119524002, acc: 0.9876543283462524)
[2025-02-13 20:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:33][root][INFO] - Training Epoch: 2/2, step 3464/7134 completed (loss: 0.05755472183227539, acc: 0.9775280952453613)
[2025-02-13 20:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:33][root][INFO] - Training Epoch: 2/2, step 3465/7134 completed (loss: 0.0779828205704689, acc: 0.9629629850387573)
[2025-02-13 20:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34][root][INFO] - Training Epoch: 2/2, step 3466/7134 completed (loss: 0.036797378212213516, acc: 0.9938271641731262)
[2025-02-13 20:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34][root][INFO] - Training Epoch: 2/2, step 3467/7134 completed (loss: 0.017450077459216118, acc: 1.0)
[2025-02-13 20:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34][root][INFO] - Training Epoch: 2/2, step 3468/7134 completed (loss: 0.02027985267341137, acc: 1.0)
[2025-02-13 20:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35][root][INFO] - Training Epoch: 2/2, step 3469/7134 completed (loss: 0.03053661249577999, acc: 1.0)
[2025-02-13 20:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35][root][INFO] - Training Epoch: 2/2, step 3470/7134 completed (loss: 0.014258726499974728, acc: 1.0)
[2025-02-13 20:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35][root][INFO] - Training Epoch: 2/2, step 3471/7134 completed (loss: 0.0397338904440403, acc: 0.9888888597488403)
[2025-02-13 20:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36][root][INFO] - Training Epoch: 2/2, step 3472/7134 completed (loss: 0.027371812611818314, acc: 0.993630588054657)
[2025-02-13 20:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36][root][INFO] - Training Epoch: 2/2, step 3473/7134 completed (loss: 0.04639481380581856, acc: 0.9868420958518982)
[2025-02-13 20:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37][root][INFO] - Training Epoch: 2/2, step 3474/7134 completed (loss: 0.06702841073274612, acc: 0.9855072498321533)
[2025-02-13 20:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37][root][INFO] - Training Epoch: 2/2, step 3475/7134 completed (loss: 0.0910324826836586, acc: 0.9772727489471436)
[2025-02-13 20:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37][root][INFO] - Training Epoch: 2/2, step 3476/7134 completed (loss: 0.19288696348667145, acc: 0.9589040875434875)
[2025-02-13 20:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38][root][INFO] - Training Epoch: 2/2, step 3477/7134 completed (loss: 0.09744193404912949, acc: 0.9932432174682617)
[2025-02-13 20:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38][root][INFO] - Training Epoch: 2/2, step 3478/7134 completed (loss: 0.016069335862994194, acc: 1.0)
[2025-02-13 20:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38][root][INFO] - Training Epoch: 2/2, step 3479/7134 completed (loss: 0.008584937080740929, acc: 1.0)
[2025-02-13 20:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39][root][INFO] - Training Epoch: 2/2, step 3480/7134 completed (loss: 0.10594842582941055, acc: 0.9670329689979553)
[2025-02-13 20:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39][root][INFO] - Training Epoch: 2/2, step 3481/7134 completed (loss: 0.06873438507318497, acc: 0.9849624037742615)
[2025-02-13 20:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40][root][INFO] - Training Epoch: 2/2, step 3482/7134 completed (loss: 0.006506995763629675, acc: 1.0)
[2025-02-13 20:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40][root][INFO] - Training Epoch: 2/2, step 3483/7134 completed (loss: 0.01526323426514864, acc: 1.0)
[2025-02-13 20:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40][root][INFO] - Training Epoch: 2/2, step 3484/7134 completed (loss: 0.06030849739909172, acc: 0.9821428656578064)
[2025-02-13 20:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41][root][INFO] - Training Epoch: 2/2, step 3485/7134 completed (loss: 0.09650108218193054, acc: 0.9696969985961914)
[2025-02-13 20:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41][root][INFO] - Training Epoch: 2/2, step 3486/7134 completed (loss: 0.15224207937717438, acc: 0.9572649598121643)
[2025-02-13 20:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41][root][INFO] - Training Epoch: 2/2, step 3487/7134 completed (loss: 0.10167323052883148, acc: 0.9698795080184937)
[2025-02-13 20:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:42][root][INFO] - Training Epoch: 2/2, step 3488/7134 completed (loss: 0.05920793116092682, acc: 0.9785714149475098)
[2025-02-13 20:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:42][root][INFO] - Training Epoch: 2/2, step 3489/7134 completed (loss: 0.16863508522510529, acc: 0.9407407641410828)
[2025-02-13 20:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43][root][INFO] - Training Epoch: 2/2, step 3490/7134 completed (loss: 0.10932957381010056, acc: 0.9860140085220337)
[2025-02-13 20:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43][root][INFO] - Training Epoch: 2/2, step 3491/7134 completed (loss: 0.117369145154953, acc: 0.987261176109314)
[2025-02-13 20:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43][root][INFO] - Training Epoch: 2/2, step 3492/7134 completed (loss: 0.07077687233686447, acc: 0.9816513657569885)
[2025-02-13 20:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44][root][INFO] - Training Epoch: 2/2, step 3493/7134 completed (loss: 0.07151797413825989, acc: 0.9814814925193787)
[2025-02-13 20:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44][root][INFO] - Training Epoch: 2/2, step 3494/7134 completed (loss: 0.06699591875076294, acc: 0.9932432174682617)
[2025-02-13 20:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44][root][INFO] - Training Epoch: 2/2, step 3495/7134 completed (loss: 0.03303348273038864, acc: 0.9896907210350037)
[2025-02-13 20:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45][root][INFO] - Training Epoch: 2/2, step 3496/7134 completed (loss: 0.06817174702882767, acc: 0.9901960492134094)
[2025-02-13 20:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45][root][INFO] - Training Epoch: 2/2, step 3497/7134 completed (loss: 0.07309825718402863, acc: 0.9873417615890503)
[2025-02-13 20:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45][root][INFO] - Training Epoch: 2/2, step 3498/7134 completed (loss: 0.13754507899284363, acc: 0.9683544039726257)
[2025-02-13 20:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:46][root][INFO] - Training Epoch: 2/2, step 3499/7134 completed (loss: 0.13044673204421997, acc: 0.9803921580314636)
[2025-02-13 20:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:46][root][INFO] - Training Epoch: 2/2, step 3500/7134 completed (loss: 0.13227567076683044, acc: 0.9707602262496948)
[2025-02-13 20:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47][root][INFO] - Training Epoch: 2/2, step 3501/7134 completed (loss: 0.12807343900203705, acc: 0.9545454382896423)
[2025-02-13 20:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47][root][INFO] - Training Epoch: 2/2, step 3502/7134 completed (loss: 0.16433598101139069, acc: 0.9527027010917664)
[2025-02-13 20:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47][root][INFO] - Training Epoch: 2/2, step 3503/7134 completed (loss: 0.2004404515028, acc: 0.9640287756919861)
[2025-02-13 20:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48][root][INFO] - Training Epoch: 2/2, step 3504/7134 completed (loss: 0.013905787840485573, acc: 1.0)
[2025-02-13 20:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48][root][INFO] - Training Epoch: 2/2, step 3505/7134 completed (loss: 0.0531512126326561, acc: 0.9841269850730896)
[2025-02-13 20:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48][root][INFO] - Training Epoch: 2/2, step 3506/7134 completed (loss: 0.04595193639397621, acc: 0.9922480583190918)
[2025-02-13 20:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:49][root][INFO] - Training Epoch: 2/2, step 3507/7134 completed (loss: 0.04552198946475983, acc: 0.9861111044883728)
[2025-02-13 20:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:49][root][INFO] - Training Epoch: 2/2, step 3508/7134 completed (loss: 0.031549133360385895, acc: 0.9867549538612366)
[2025-02-13 20:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50][root][INFO] - Training Epoch: 2/2, step 3509/7134 completed (loss: 0.09507518261671066, acc: 0.9631901979446411)
[2025-02-13 20:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50][root][INFO] - Training Epoch: 2/2, step 3510/7134 completed (loss: 0.04320725426077843, acc: 0.9849624037742615)
[2025-02-13 20:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50][root][INFO] - Training Epoch: 2/2, step 3511/7134 completed (loss: 0.02086258865892887, acc: 0.9926470518112183)
[2025-02-13 20:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51][root][INFO] - Training Epoch: 2/2, step 3512/7134 completed (loss: 0.19494755566120148, acc: 0.9615384340286255)
[2025-02-13 20:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51][root][INFO] - Training Epoch: 2/2, step 3513/7134 completed (loss: 0.020502768456935883, acc: 0.9927536249160767)
[2025-02-13 20:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51][root][INFO] - Training Epoch: 2/2, step 3514/7134 completed (loss: 0.04749160632491112, acc: 0.9856459498405457)
[2025-02-13 20:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52][root][INFO] - Training Epoch: 2/2, step 3515/7134 completed (loss: 0.017606589943170547, acc: 0.994350254535675)
[2025-02-13 20:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52][root][INFO] - Training Epoch: 2/2, step 3516/7134 completed (loss: 0.014759695157408714, acc: 1.0)
[2025-02-13 20:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53][root][INFO] - Training Epoch: 2/2, step 3517/7134 completed (loss: 0.023892156779766083, acc: 0.9949495196342468)
[2025-02-13 20:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53][root][INFO] - Training Epoch: 2/2, step 3518/7134 completed (loss: 0.0525909960269928, acc: 0.9851852059364319)
[2025-02-13 20:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53][root][INFO] - Training Epoch: 2/2, step 3519/7134 completed (loss: 0.048249755054712296, acc: 0.995555579662323)
[2025-02-13 20:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54][root][INFO] - Training Epoch: 2/2, step 3520/7134 completed (loss: 0.10638203471899033, acc: 0.9848484992980957)
[2025-02-13 20:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54][root][INFO] - Training Epoch: 2/2, step 3521/7134 completed (loss: 0.035896118730306625, acc: 0.9820359349250793)
[2025-02-13 20:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54][root][INFO] - Training Epoch: 2/2, step 3522/7134 completed (loss: 0.03664286807179451, acc: 0.9888888597488403)
[2025-02-13 20:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55][root][INFO] - Training Epoch: 2/2, step 3523/7134 completed (loss: 0.01143612153828144, acc: 1.0)
[2025-02-13 20:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55][root][INFO] - Training Epoch: 2/2, step 3524/7134 completed (loss: 0.03708183020353317, acc: 0.9945054650306702)
[2025-02-13 20:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56][root][INFO] - Training Epoch: 2/2, step 3525/7134 completed (loss: 0.02325516752898693, acc: 1.0)
[2025-02-13 20:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56][root][INFO] - Training Epoch: 2/2, step 3526/7134 completed (loss: 0.07576858252286911, acc: 0.9848484992980957)
[2025-02-13 20:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56][root][INFO] - Training Epoch: 2/2, step 3527/7134 completed (loss: 0.07475162297487259, acc: 0.9734042286872864)
[2025-02-13 20:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57][root][INFO] - Training Epoch: 2/2, step 3528/7134 completed (loss: 0.0165855810046196, acc: 0.995192289352417)
[2025-02-13 20:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57][root][INFO] - Training Epoch: 2/2, step 3529/7134 completed (loss: 0.0180096086114645, acc: 1.0)
[2025-02-13 20:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57][root][INFO] - Training Epoch: 2/2, step 3530/7134 completed (loss: 0.08198043704032898, acc: 0.982300877571106)
[2025-02-13 20:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58][root][INFO] - Training Epoch: 2/2, step 3531/7134 completed (loss: 0.04973817616701126, acc: 0.9937499761581421)
[2025-02-13 20:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58][root][INFO] - Training Epoch: 2/2, step 3532/7134 completed (loss: 0.013382861390709877, acc: 1.0)
[2025-02-13 20:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58][root][INFO] - Training Epoch: 2/2, step 3533/7134 completed (loss: 0.04570231959223747, acc: 0.9911110997200012)
[2025-02-13 20:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:59][root][INFO] - Training Epoch: 2/2, step 3534/7134 completed (loss: 0.043249599635601044, acc: 0.9824561476707458)
[2025-02-13 20:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:59][root][INFO] - Training Epoch: 2/2, step 3535/7134 completed (loss: 0.050910286605358124, acc: 0.9856114983558655)
[2025-02-13 20:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00][root][INFO] - Training Epoch: 2/2, step 3536/7134 completed (loss: 0.0482381135225296, acc: 0.9938271641731262)
[2025-02-13 20:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00][root][INFO] - Training Epoch: 2/2, step 3537/7134 completed (loss: 0.0728818029165268, acc: 0.984375)
[2025-02-13 20:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00][root][INFO] - Training Epoch: 2/2, step 3538/7134 completed (loss: 0.05695553869009018, acc: 0.9831932783126831)
[2025-02-13 20:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01][root][INFO] - Training Epoch: 2/2, step 3539/7134 completed (loss: 0.06023359298706055, acc: 0.977011501789093)
[2025-02-13 20:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01][root][INFO] - Training Epoch: 2/2, step 3540/7134 completed (loss: 0.05833463743329048, acc: 0.9846153855323792)
[2025-02-13 20:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01][root][INFO] - Training Epoch: 2/2, step 3541/7134 completed (loss: 0.04540839046239853, acc: 1.0)
[2025-02-13 20:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02][root][INFO] - Training Epoch: 2/2, step 3542/7134 completed (loss: 0.03736657276749611, acc: 0.9931507110595703)
[2025-02-13 20:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02][root][INFO] - Training Epoch: 2/2, step 3543/7134 completed (loss: 0.07758691906929016, acc: 0.9788732528686523)
[2025-02-13 20:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02][root][INFO] - Training Epoch: 2/2, step 3544/7134 completed (loss: 0.09643573313951492, acc: 0.9801324605941772)
[2025-02-13 20:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03][root][INFO] - Training Epoch: 2/2, step 3545/7134 completed (loss: 0.08135344088077545, acc: 0.9861111044883728)
[2025-02-13 20:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03][root][INFO] - Training Epoch: 2/2, step 3546/7134 completed (loss: 0.0671527311205864, acc: 0.9856114983558655)
[2025-02-13 20:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04][root][INFO] - Training Epoch: 2/2, step 3547/7134 completed (loss: 0.10480806231498718, acc: 0.9717513918876648)
[2025-02-13 20:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04][root][INFO] - Training Epoch: 2/2, step 3548/7134 completed (loss: 0.03254956379532814, acc: 0.9908257126808167)
[2025-02-13 20:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04][root][INFO] - Training Epoch: 2/2, step 3549/7134 completed (loss: 0.036600906401872635, acc: 1.0)
[2025-02-13 20:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05][root][INFO] - Training Epoch: 2/2, step 3550/7134 completed (loss: 0.1073775365948677, acc: 0.9781022071838379)
[2025-02-13 20:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05][root][INFO] - Training Epoch: 2/2, step 3551/7134 completed (loss: 0.12843719124794006, acc: 0.9591836929321289)
[2025-02-13 20:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05][root][INFO] - Training Epoch: 2/2, step 3552/7134 completed (loss: 0.10226450860500336, acc: 0.9842519760131836)
[2025-02-13 20:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:06][root][INFO] - Training Epoch: 2/2, step 3553/7134 completed (loss: 0.09727925062179565, acc: 0.9632353186607361)
[2025-02-13 20:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:06][root][INFO] - Training Epoch: 2/2, step 3554/7134 completed (loss: 0.21052336692810059, acc: 0.9509202241897583)
[2025-02-13 20:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07][root][INFO] - Training Epoch: 2/2, step 3555/7134 completed (loss: 0.22771261632442474, acc: 0.9290322661399841)
[2025-02-13 20:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07][root][INFO] - Training Epoch: 2/2, step 3556/7134 completed (loss: 0.128640815615654, acc: 0.9545454382896423)
[2025-02-13 20:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07][root][INFO] - Training Epoch: 2/2, step 3557/7134 completed (loss: 0.15529528260231018, acc: 0.957446813583374)
[2025-02-13 20:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08][root][INFO] - Training Epoch: 2/2, step 3558/7134 completed (loss: 0.13754691183567047, acc: 0.9520000219345093)
[2025-02-13 20:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08][root][INFO] - Training Epoch: 2/2, step 3559/7134 completed (loss: 0.0803036317229271, acc: 0.976190447807312)
[2025-02-13 20:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08][root][INFO] - Training Epoch: 2/2, step 3560/7134 completed (loss: 0.07420959323644638, acc: 0.9781420826911926)
[2025-02-13 20:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09][root][INFO] - Training Epoch: 2/2, step 3561/7134 completed (loss: 0.07608073204755783, acc: 0.9774011373519897)
[2025-02-13 20:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09][root][INFO] - Training Epoch: 2/2, step 3562/7134 completed (loss: 0.06031608581542969, acc: 0.9852941036224365)
[2025-02-13 20:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09][root][INFO] - Training Epoch: 2/2, step 3563/7134 completed (loss: 0.08882198482751846, acc: 0.9716312289237976)
[2025-02-13 20:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:57][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2338, device='cuda:0') eval_epoch_loss=tensor(0.2101, device='cuda:0') eval_epoch_acc=tensor(0.9490, device='cuda:0')
[2025-02-13 20:49:57][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:49:57][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:49:57][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_3564_loss_0.21009458601474762/model.pt
[2025-02-13 20:49:57][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:49:57][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.21009458601474762
[2025-02-13 20:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58][root][INFO] - Training Epoch: 2/2, step 3564/7134 completed (loss: 0.1137130856513977, acc: 0.9603174328804016)
[2025-02-13 20:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58][root][INFO] - Training Epoch: 2/2, step 3565/7134 completed (loss: 0.16991202533245087, acc: 0.9485294222831726)
[2025-02-13 20:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58][root][INFO] - Training Epoch: 2/2, step 3566/7134 completed (loss: 0.11696797609329224, acc: 0.9831932783126831)
[2025-02-13 20:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:59][root][INFO] - Training Epoch: 2/2, step 3567/7134 completed (loss: 0.12690934538841248, acc: 0.9775280952453613)
[2025-02-13 20:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:59][root][INFO] - Training Epoch: 2/2, step 3568/7134 completed (loss: 0.12066259235143661, acc: 0.9645389914512634)
[2025-02-13 20:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00][root][INFO] - Training Epoch: 2/2, step 3569/7134 completed (loss: 0.11873963475227356, acc: 0.9750000238418579)
[2025-02-13 20:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00][root][INFO] - Training Epoch: 2/2, step 3570/7134 completed (loss: 0.3213009536266327, acc: 0.9264705777168274)
[2025-02-13 20:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00][root][INFO] - Training Epoch: 2/2, step 3571/7134 completed (loss: 0.19958993792533875, acc: 0.966292142868042)
[2025-02-13 20:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01][root][INFO] - Training Epoch: 2/2, step 3572/7134 completed (loss: 0.05198677256703377, acc: 0.976190447807312)
[2025-02-13 20:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01][root][INFO] - Training Epoch: 2/2, step 3573/7134 completed (loss: 0.13300177454948425, acc: 0.9605262875556946)
[2025-02-13 20:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01][root][INFO] - Training Epoch: 2/2, step 3574/7134 completed (loss: 0.10458576679229736, acc: 0.9752066135406494)
[2025-02-13 20:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02][root][INFO] - Training Epoch: 2/2, step 3575/7134 completed (loss: 0.338072270154953, acc: 0.9473684430122375)
[2025-02-13 20:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02][root][INFO] - Training Epoch: 2/2, step 3576/7134 completed (loss: 0.07112863659858704, acc: 0.9925373196601868)
[2025-02-13 20:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02][root][INFO] - Training Epoch: 2/2, step 3577/7134 completed (loss: 0.1128210499882698, acc: 0.9793814420700073)
[2025-02-13 20:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03][root][INFO] - Training Epoch: 2/2, step 3578/7134 completed (loss: 0.0782635435461998, acc: 0.9698795080184937)
[2025-02-13 20:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03][root][INFO] - Training Epoch: 2/2, step 3579/7134 completed (loss: 0.11987590789794922, acc: 0.9709302186965942)
[2025-02-13 20:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04][root][INFO] - Training Epoch: 2/2, step 3580/7134 completed (loss: 0.24457062780857086, acc: 0.9421965479850769)
[2025-02-13 20:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04][root][INFO] - Training Epoch: 2/2, step 3581/7134 completed (loss: 0.14516650140285492, acc: 0.9588235020637512)
[2025-02-13 20:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04][root][INFO] - Training Epoch: 2/2, step 3582/7134 completed (loss: 0.12653353810310364, acc: 0.9658119678497314)
[2025-02-13 20:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05][root][INFO] - Training Epoch: 2/2, step 3583/7134 completed (loss: 0.1648057997226715, acc: 0.9520958065986633)
[2025-02-13 20:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05][root][INFO] - Training Epoch: 2/2, step 3584/7134 completed (loss: 0.18731683492660522, acc: 0.9468085169792175)
[2025-02-13 20:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05][root][INFO] - Training Epoch: 2/2, step 3585/7134 completed (loss: 0.19130849838256836, acc: 0.9509202241897583)
[2025-02-13 20:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06][root][INFO] - Training Epoch: 2/2, step 3586/7134 completed (loss: 0.21061255037784576, acc: 0.9490445852279663)
[2025-02-13 20:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06][root][INFO] - Training Epoch: 2/2, step 3587/7134 completed (loss: 0.16615277528762817, acc: 0.9650349617004395)
[2025-02-13 20:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07][root][INFO] - Training Epoch: 2/2, step 3588/7134 completed (loss: 0.11740275472402573, acc: 0.9595375657081604)
[2025-02-13 20:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07][root][INFO] - Training Epoch: 2/2, step 3589/7134 completed (loss: 0.18864336609840393, acc: 0.9781420826911926)
[2025-02-13 20:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07][root][INFO] - Training Epoch: 2/2, step 3590/7134 completed (loss: 0.049062080681324005, acc: 0.9833333492279053)
[2025-02-13 20:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08][root][INFO] - Training Epoch: 2/2, step 3591/7134 completed (loss: 0.11691876500844955, acc: 0.9707602262496948)
[2025-02-13 20:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08][root][INFO] - Training Epoch: 2/2, step 3592/7134 completed (loss: 0.039522211998701096, acc: 0.9928571581840515)
[2025-02-13 20:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08][root][INFO] - Training Epoch: 2/2, step 3593/7134 completed (loss: 0.28291040658950806, acc: 0.9718309640884399)
[2025-02-13 20:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09][root][INFO] - Training Epoch: 2/2, step 3594/7134 completed (loss: 0.08316615968942642, acc: 0.9801324605941772)
[2025-02-13 20:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09][root][INFO] - Training Epoch: 2/2, step 3595/7134 completed (loss: 0.07930638641119003, acc: 0.9738562107086182)
[2025-02-13 20:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09][root][INFO] - Training Epoch: 2/2, step 3596/7134 completed (loss: 0.1425483524799347, acc: 0.970588207244873)
[2025-02-13 20:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10][root][INFO] - Training Epoch: 2/2, step 3597/7134 completed (loss: 0.06463844329118729, acc: 0.9934210777282715)
[2025-02-13 20:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10][root][INFO] - Training Epoch: 2/2, step 3598/7134 completed (loss: 0.14034415781497955, acc: 0.9573459625244141)
[2025-02-13 20:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10][root][INFO] - Training Epoch: 2/2, step 3599/7134 completed (loss: 0.19402626156806946, acc: 0.9243243336677551)
[2025-02-13 20:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:11][root][INFO] - Training Epoch: 2/2, step 3600/7134 completed (loss: 0.15618662536144257, acc: 0.9585492014884949)
[2025-02-13 20:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:11][root][INFO] - Training Epoch: 2/2, step 3601/7134 completed (loss: 0.34949734807014465, acc: 0.898876428604126)
[2025-02-13 20:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12][root][INFO] - Training Epoch: 2/2, step 3602/7134 completed (loss: 0.2656060457229614, acc: 0.934883713722229)
[2025-02-13 20:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12][root][INFO] - Training Epoch: 2/2, step 3603/7134 completed (loss: 0.10380033403635025, acc: 0.9685534834861755)
[2025-02-13 20:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12][root][INFO] - Training Epoch: 2/2, step 3604/7134 completed (loss: 0.12391634285449982, acc: 0.9685039520263672)
[2025-02-13 20:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13][root][INFO] - Training Epoch: 2/2, step 3605/7134 completed (loss: 0.15600921213626862, acc: 0.9641255736351013)
[2025-02-13 20:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13][root][INFO] - Training Epoch: 2/2, step 3606/7134 completed (loss: 0.25151944160461426, acc: 0.9452054500579834)
[2025-02-13 20:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13][root][INFO] - Training Epoch: 2/2, step 3607/7134 completed (loss: 0.15826290845870972, acc: 0.9589040875434875)
[2025-02-13 20:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:14][root][INFO] - Training Epoch: 2/2, step 3608/7134 completed (loss: 0.22856058180332184, acc: 0.946601927280426)
[2025-02-13 20:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:14][root][INFO] - Training Epoch: 2/2, step 3609/7134 completed (loss: 0.12174047529697418, acc: 0.9578947424888611)
[2025-02-13 20:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15][root][INFO] - Training Epoch: 2/2, step 3610/7134 completed (loss: 0.1586088091135025, acc: 0.9615384340286255)
[2025-02-13 20:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15][root][INFO] - Training Epoch: 2/2, step 3611/7134 completed (loss: 0.1392943412065506, acc: 0.9657142758369446)
[2025-02-13 20:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15][root][INFO] - Training Epoch: 2/2, step 3612/7134 completed (loss: 0.3077831566333771, acc: 0.931034505367279)
[2025-02-13 20:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16][root][INFO] - Training Epoch: 2/2, step 3613/7134 completed (loss: 0.1269957572221756, acc: 0.9562841653823853)
[2025-02-13 20:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16][root][INFO] - Training Epoch: 2/2, step 3614/7134 completed (loss: 0.06958196312189102, acc: 0.9840425252914429)
[2025-02-13 20:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16][root][INFO] - Training Epoch: 2/2, step 3615/7134 completed (loss: 0.0756540521979332, acc: 0.9729729890823364)
[2025-02-13 20:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17][root][INFO] - Training Epoch: 2/2, step 3616/7134 completed (loss: 0.18017800152301788, acc: 0.9503105878829956)
[2025-02-13 20:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17][root][INFO] - Training Epoch: 2/2, step 3617/7134 completed (loss: 0.07192014157772064, acc: 0.9945945739746094)
[2025-02-13 20:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18][root][INFO] - Training Epoch: 2/2, step 3618/7134 completed (loss: 0.1856449991464615, acc: 0.9631901979446411)
[2025-02-13 20:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18][root][INFO] - Training Epoch: 2/2, step 3619/7134 completed (loss: 0.3054027557373047, acc: 0.9301310181617737)
[2025-02-13 20:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18][root][INFO] - Training Epoch: 2/2, step 3620/7134 completed (loss: 0.09964998066425323, acc: 0.976047933101654)
[2025-02-13 20:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19][root][INFO] - Training Epoch: 2/2, step 3621/7134 completed (loss: 0.08791276067495346, acc: 0.9745222926139832)
[2025-02-13 20:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19][root][INFO] - Training Epoch: 2/2, step 3622/7134 completed (loss: 0.06504614651203156, acc: 0.9870129823684692)
[2025-02-13 20:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19][root][INFO] - Training Epoch: 2/2, step 3623/7134 completed (loss: 0.024218309670686722, acc: 1.0)
[2025-02-13 20:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20][root][INFO] - Training Epoch: 2/2, step 3624/7134 completed (loss: 0.07212430983781815, acc: 0.9856114983558655)
[2025-02-13 20:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20][root][INFO] - Training Epoch: 2/2, step 3625/7134 completed (loss: 0.21104712784290314, acc: 0.9495798349380493)
[2025-02-13 20:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21][root][INFO] - Training Epoch: 2/2, step 3626/7134 completed (loss: 0.20289075374603271, acc: 0.966292142868042)
[2025-02-13 20:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21][root][INFO] - Training Epoch: 2/2, step 3627/7134 completed (loss: 0.2645692229270935, acc: 0.9433962106704712)
[2025-02-13 20:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21][root][INFO] - Training Epoch: 2/2, step 3628/7134 completed (loss: 0.1824006289243698, acc: 0.9523809552192688)
[2025-02-13 20:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22][root][INFO] - Training Epoch: 2/2, step 3629/7134 completed (loss: 0.20039008557796478, acc: 0.9596773982048035)
[2025-02-13 20:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22][root][INFO] - Training Epoch: 2/2, step 3630/7134 completed (loss: 0.0664592757821083, acc: 0.9784946441650391)
[2025-02-13 20:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22][root][INFO] - Training Epoch: 2/2, step 3631/7134 completed (loss: 0.09438818693161011, acc: 0.9714285731315613)
[2025-02-13 20:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:23][root][INFO] - Training Epoch: 2/2, step 3632/7134 completed (loss: 0.06346612423658371, acc: 0.9918032884597778)
[2025-02-13 20:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:23][root][INFO] - Training Epoch: 2/2, step 3633/7134 completed (loss: 0.12500141561031342, acc: 0.9745222926139832)
[2025-02-13 20:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24][root][INFO] - Training Epoch: 2/2, step 3634/7134 completed (loss: 0.10943207144737244, acc: 0.9719626307487488)
[2025-02-13 20:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24][root][INFO] - Training Epoch: 2/2, step 3635/7134 completed (loss: 0.11479655653238297, acc: 0.9741379022598267)
[2025-02-13 20:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24][root][INFO] - Training Epoch: 2/2, step 3636/7134 completed (loss: 0.07233429700136185, acc: 0.9811320900917053)
[2025-02-13 20:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25][root][INFO] - Training Epoch: 2/2, step 3637/7134 completed (loss: 0.14349618554115295, acc: 0.9636363387107849)
[2025-02-13 20:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25][root][INFO] - Training Epoch: 2/2, step 3638/7134 completed (loss: 0.05380146950483322, acc: 0.9890109896659851)
[2025-02-13 20:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25][root][INFO] - Training Epoch: 2/2, step 3639/7134 completed (loss: 0.10438402742147446, acc: 0.978723406791687)
[2025-02-13 20:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26][root][INFO] - Training Epoch: 2/2, step 3640/7134 completed (loss: 0.12770630419254303, acc: 0.9586777091026306)
[2025-02-13 20:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26][root][INFO] - Training Epoch: 2/2, step 3641/7134 completed (loss: 0.04816800728440285, acc: 0.9838709831237793)
[2025-02-13 20:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26][root][INFO] - Training Epoch: 2/2, step 3642/7134 completed (loss: 0.018905943259596825, acc: 1.0)
[2025-02-13 20:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27][root][INFO] - Training Epoch: 2/2, step 3643/7134 completed (loss: 0.06790246069431305, acc: 0.984375)
[2025-02-13 20:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27][root][INFO] - Training Epoch: 2/2, step 3644/7134 completed (loss: 0.13302311301231384, acc: 0.9662162065505981)
[2025-02-13 20:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28][root][INFO] - Training Epoch: 2/2, step 3645/7134 completed (loss: 0.26124003529548645, acc: 0.9375)
[2025-02-13 20:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28][root][INFO] - Training Epoch: 2/2, step 3646/7134 completed (loss: 0.31417885422706604, acc: 0.9318181872367859)
[2025-02-13 20:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28][root][INFO] - Training Epoch: 2/2, step 3647/7134 completed (loss: 0.07268056273460388, acc: 0.9887640476226807)
[2025-02-13 20:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29][root][INFO] - Training Epoch: 2/2, step 3648/7134 completed (loss: 0.14863669872283936, acc: 0.976331353187561)
[2025-02-13 20:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29][root][INFO] - Training Epoch: 2/2, step 3649/7134 completed (loss: 0.1110888198018074, acc: 0.9798657894134521)
[2025-02-13 20:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29][root][INFO] - Training Epoch: 2/2, step 3650/7134 completed (loss: 0.09554485976696014, acc: 0.9822485446929932)
[2025-02-13 20:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30][root][INFO] - Training Epoch: 2/2, step 3651/7134 completed (loss: 0.17191733419895172, acc: 0.9698795080184937)
[2025-02-13 20:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30][root][INFO] - Training Epoch: 2/2, step 3652/7134 completed (loss: 0.10048294812440872, acc: 0.9727891087532043)
[2025-02-13 20:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30][root][INFO] - Training Epoch: 2/2, step 3653/7134 completed (loss: 0.2003025859594345, acc: 0.9570552110671997)
[2025-02-13 20:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31][root][INFO] - Training Epoch: 2/2, step 3654/7134 completed (loss: 0.03897940367460251, acc: 0.9919999837875366)
[2025-02-13 20:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31][root][INFO] - Training Epoch: 2/2, step 3655/7134 completed (loss: 0.2720995247364044, acc: 0.940119743347168)
[2025-02-13 20:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32][root][INFO] - Training Epoch: 2/2, step 3656/7134 completed (loss: 0.26995036005973816, acc: 0.9420289993286133)
[2025-02-13 20:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32][root][INFO] - Training Epoch: 2/2, step 3657/7134 completed (loss: 0.04147578403353691, acc: 0.9930070042610168)
[2025-02-13 20:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32][root][INFO] - Training Epoch: 2/2, step 3658/7134 completed (loss: 0.1254940927028656, acc: 0.9652777910232544)
[2025-02-13 20:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33][root][INFO] - Training Epoch: 2/2, step 3659/7134 completed (loss: 0.12549544870853424, acc: 0.9795918464660645)
[2025-02-13 20:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33][root][INFO] - Training Epoch: 2/2, step 3660/7134 completed (loss: 0.17254558205604553, acc: 0.9629629850387573)
[2025-02-13 20:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33][root][INFO] - Training Epoch: 2/2, step 3661/7134 completed (loss: 0.06929472833871841, acc: 0.9857142567634583)
[2025-02-13 20:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34][root][INFO] - Training Epoch: 2/2, step 3662/7134 completed (loss: 0.26750704646110535, acc: 0.9440993666648865)
[2025-02-13 20:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34][root][INFO] - Training Epoch: 2/2, step 3663/7134 completed (loss: 0.18451076745986938, acc: 0.9296875)
[2025-02-13 20:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34][root][INFO] - Training Epoch: 2/2, step 3664/7134 completed (loss: 0.23372353613376617, acc: 0.9610389471054077)
[2025-02-13 20:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35][root][INFO] - Training Epoch: 2/2, step 3665/7134 completed (loss: 0.14062045514583588, acc: 0.9594594836235046)
[2025-02-13 20:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35][root][INFO] - Training Epoch: 2/2, step 3666/7134 completed (loss: 0.17552267014980316, acc: 0.9589040875434875)
[2025-02-13 20:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35][root][INFO] - Training Epoch: 2/2, step 3667/7134 completed (loss: 0.3109314441680908, acc: 0.930232584476471)
[2025-02-13 20:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36][root][INFO] - Training Epoch: 2/2, step 3668/7134 completed (loss: 0.22024478018283844, acc: 0.9724137783050537)
[2025-02-13 20:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36][root][INFO] - Training Epoch: 2/2, step 3669/7134 completed (loss: 0.043369799852371216, acc: 0.9917355179786682)
[2025-02-13 20:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36][root][INFO] - Training Epoch: 2/2, step 3670/7134 completed (loss: 0.03961535543203354, acc: 0.9879518151283264)
[2025-02-13 20:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:37][root][INFO] - Training Epoch: 2/2, step 3671/7134 completed (loss: 0.06694737076759338, acc: 0.9810126423835754)
[2025-02-13 20:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:37][root][INFO] - Training Epoch: 2/2, step 3672/7134 completed (loss: 0.0901314839720726, acc: 0.9834254384040833)
[2025-02-13 20:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38][root][INFO] - Training Epoch: 2/2, step 3673/7134 completed (loss: 0.10651642084121704, acc: 0.9793103337287903)
[2025-02-13 20:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38][root][INFO] - Training Epoch: 2/2, step 3674/7134 completed (loss: 0.03331497684121132, acc: 1.0)
[2025-02-13 20:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38][root][INFO] - Training Epoch: 2/2, step 3675/7134 completed (loss: 0.08720562607049942, acc: 0.988950252532959)
[2025-02-13 20:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39][root][INFO] - Training Epoch: 2/2, step 3676/7134 completed (loss: 0.08855625241994858, acc: 0.9871794581413269)
[2025-02-13 20:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39][root][INFO] - Training Epoch: 2/2, step 3677/7134 completed (loss: 0.13892364501953125, acc: 0.9488636255264282)
[2025-02-13 20:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39][root][INFO] - Training Epoch: 2/2, step 3678/7134 completed (loss: 0.1505671739578247, acc: 0.9620253443717957)
[2025-02-13 20:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40][root][INFO] - Training Epoch: 2/2, step 3679/7134 completed (loss: 0.057450052350759506, acc: 0.9769230484962463)
[2025-02-13 20:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40][root][INFO] - Training Epoch: 2/2, step 3680/7134 completed (loss: 0.11879348754882812, acc: 0.9666666388511658)
[2025-02-13 20:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41][root][INFO] - Training Epoch: 2/2, step 3681/7134 completed (loss: 0.05130695551633835, acc: 0.9937499761581421)
[2025-02-13 20:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41][root][INFO] - Training Epoch: 2/2, step 3682/7134 completed (loss: 0.020509250462055206, acc: 1.0)
[2025-02-13 20:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41][root][INFO] - Training Epoch: 2/2, step 3683/7134 completed (loss: 0.026580501347780228, acc: 0.9926470518112183)
[2025-02-13 20:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42][root][INFO] - Training Epoch: 2/2, step 3684/7134 completed (loss: 0.02242458425462246, acc: 1.0)
[2025-02-13 20:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42][root][INFO] - Training Epoch: 2/2, step 3685/7134 completed (loss: 0.04819798842072487, acc: 0.9918699264526367)
[2025-02-13 20:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42][root][INFO] - Training Epoch: 2/2, step 3686/7134 completed (loss: 0.05879981070756912, acc: 0.98591548204422)
[2025-02-13 20:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43][root][INFO] - Training Epoch: 2/2, step 3687/7134 completed (loss: 0.03280017524957657, acc: 0.9937888383865356)
[2025-02-13 20:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43][root][INFO] - Training Epoch: 2/2, step 3688/7134 completed (loss: 0.061165738850831985, acc: 0.9833333492279053)
[2025-02-13 20:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43][root][INFO] - Training Epoch: 2/2, step 3689/7134 completed (loss: 0.07858351618051529, acc: 0.9858155846595764)
[2025-02-13 20:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44][root][INFO] - Training Epoch: 2/2, step 3690/7134 completed (loss: 0.04184746369719505, acc: 0.9873417615890503)
[2025-02-13 20:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44][root][INFO] - Training Epoch: 2/2, step 3691/7134 completed (loss: 0.05802619084715843, acc: 0.9900990128517151)
[2025-02-13 20:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45][root][INFO] - Training Epoch: 2/2, step 3692/7134 completed (loss: 0.05041062831878662, acc: 0.9938650131225586)
[2025-02-13 20:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45][root][INFO] - Training Epoch: 2/2, step 3693/7134 completed (loss: 0.04512808844447136, acc: 0.9924242496490479)
[2025-02-13 20:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45][root][INFO] - Training Epoch: 2/2, step 3694/7134 completed (loss: 0.03941374272108078, acc: 0.987500011920929)
[2025-02-13 20:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46][root][INFO] - Training Epoch: 2/2, step 3695/7134 completed (loss: 0.03836552053689957, acc: 0.993630588054657)
[2025-02-13 20:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46][root][INFO] - Training Epoch: 2/2, step 3696/7134 completed (loss: 0.052336398512125015, acc: 0.9885057210922241)
[2025-02-13 20:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46][root][INFO] - Training Epoch: 2/2, step 3697/7134 completed (loss: 0.05087963119149208, acc: 0.9878048896789551)
[2025-02-13 20:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47][root][INFO] - Training Epoch: 2/2, step 3698/7134 completed (loss: 0.02852085791528225, acc: 1.0)
[2025-02-13 20:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47][root][INFO] - Training Epoch: 2/2, step 3699/7134 completed (loss: 0.0311928391456604, acc: 0.9921259880065918)
[2025-02-13 20:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48][root][INFO] - Training Epoch: 2/2, step 3700/7134 completed (loss: 0.08389246463775635, acc: 0.9937106966972351)
[2025-02-13 20:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48][root][INFO] - Training Epoch: 2/2, step 3701/7134 completed (loss: 0.07190633565187454, acc: 0.9927007555961609)
[2025-02-13 20:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48][root][INFO] - Training Epoch: 2/2, step 3702/7134 completed (loss: 0.07464119791984558, acc: 0.9939393997192383)
[2025-02-13 20:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49][root][INFO] - Training Epoch: 2/2, step 3703/7134 completed (loss: 0.06210017576813698, acc: 0.9886363744735718)
[2025-02-13 20:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49][root][INFO] - Training Epoch: 2/2, step 3704/7134 completed (loss: 0.031170297414064407, acc: 0.9900990128517151)
[2025-02-13 20:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49][root][INFO] - Training Epoch: 2/2, step 3705/7134 completed (loss: 0.08031328022480011, acc: 0.9815950989723206)
[2025-02-13 20:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50][root][INFO] - Training Epoch: 2/2, step 3706/7134 completed (loss: 0.18434615433216095, acc: 0.9405940771102905)
[2025-02-13 20:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50][root][INFO] - Training Epoch: 2/2, step 3707/7134 completed (loss: 0.05321943014860153, acc: 0.9800000190734863)
[2025-02-13 20:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50][root][INFO] - Training Epoch: 2/2, step 3708/7134 completed (loss: 0.1345449537038803, acc: 0.9692307710647583)
[2025-02-13 20:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:51][root][INFO] - Training Epoch: 2/2, step 3709/7134 completed (loss: 0.04342213645577431, acc: 0.9907407164573669)
[2025-02-13 20:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:51][root][INFO] - Training Epoch: 2/2, step 3710/7134 completed (loss: 0.10179832577705383, acc: 0.9722222089767456)
[2025-02-13 20:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52][root][INFO] - Training Epoch: 2/2, step 3711/7134 completed (loss: 0.1646592915058136, acc: 0.9615384340286255)
[2025-02-13 20:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52][root][INFO] - Training Epoch: 2/2, step 3712/7134 completed (loss: 0.12349124252796173, acc: 0.9619565010070801)
[2025-02-13 20:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52][root][INFO] - Training Epoch: 2/2, step 3713/7134 completed (loss: 0.07757719606161118, acc: 0.971222996711731)
[2025-02-13 20:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53][root][INFO] - Training Epoch: 2/2, step 3714/7134 completed (loss: 0.09712208062410355, acc: 0.9740932583808899)
[2025-02-13 20:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53][root][INFO] - Training Epoch: 2/2, step 3715/7134 completed (loss: 0.08208226412534714, acc: 0.9680851101875305)
[2025-02-13 20:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53][root][INFO] - Training Epoch: 2/2, step 3716/7134 completed (loss: 0.17680928111076355, acc: 0.9543147087097168)
[2025-02-13 20:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:54][root][INFO] - Training Epoch: 2/2, step 3717/7134 completed (loss: 0.055579256266355515, acc: 0.9878048896789551)
[2025-02-13 20:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:54][root][INFO] - Training Epoch: 2/2, step 3718/7134 completed (loss: 0.1461031436920166, acc: 0.984375)
[2025-02-13 20:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55][root][INFO] - Training Epoch: 2/2, step 3719/7134 completed (loss: 0.10560574382543564, acc: 0.9766082167625427)
[2025-02-13 20:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55][root][INFO] - Training Epoch: 2/2, step 3720/7134 completed (loss: 0.039245735853910446, acc: 0.9873417615890503)
[2025-02-13 20:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55][root][INFO] - Training Epoch: 2/2, step 3721/7134 completed (loss: 0.04842785373330116, acc: 0.9884393215179443)
[2025-02-13 20:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56][root][INFO] - Training Epoch: 2/2, step 3722/7134 completed (loss: 0.07445160299539566, acc: 0.984455943107605)
[2025-02-13 20:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56][root][INFO] - Training Epoch: 2/2, step 3723/7134 completed (loss: 0.11065267771482468, acc: 0.9677419066429138)
[2025-02-13 20:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56][root][INFO] - Training Epoch: 2/2, step 3724/7134 completed (loss: 0.049078319221735, acc: 0.9879518151283264)
[2025-02-13 20:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57][root][INFO] - Training Epoch: 2/2, step 3725/7134 completed (loss: 0.18917037546634674, acc: 0.957317054271698)
[2025-02-13 20:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57][root][INFO] - Training Epoch: 2/2, step 3726/7134 completed (loss: 0.08967513591051102, acc: 0.9648241400718689)
[2025-02-13 20:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57][root][INFO] - Training Epoch: 2/2, step 3727/7134 completed (loss: 0.10383040457963943, acc: 0.9595375657081604)
[2025-02-13 20:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58][root][INFO] - Training Epoch: 2/2, step 3728/7134 completed (loss: 0.021869545802474022, acc: 1.0)
[2025-02-13 20:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58][root][INFO] - Training Epoch: 2/2, step 3729/7134 completed (loss: 0.07864634692668915, acc: 0.9764150977134705)
[2025-02-13 20:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59][root][INFO] - Training Epoch: 2/2, step 3730/7134 completed (loss: 0.08449643105268478, acc: 0.9757575988769531)
[2025-02-13 20:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59][root][INFO] - Training Epoch: 2/2, step 3731/7134 completed (loss: 0.029803341254591942, acc: 0.9942528605461121)
[2025-02-13 20:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59][root][INFO] - Training Epoch: 2/2, step 3732/7134 completed (loss: 0.05025891587138176, acc: 0.9897435903549194)
[2025-02-13 20:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00][root][INFO] - Training Epoch: 2/2, step 3733/7134 completed (loss: 0.09329777210950851, acc: 0.9875776171684265)
[2025-02-13 20:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00][root][INFO] - Training Epoch: 2/2, step 3734/7134 completed (loss: 0.21329964697360992, acc: 0.9615384340286255)
[2025-02-13 20:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00][root][INFO] - Training Epoch: 2/2, step 3735/7134 completed (loss: 0.08303741365671158, acc: 0.9675324559211731)
[2025-02-13 20:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01][root][INFO] - Training Epoch: 2/2, step 3736/7134 completed (loss: 0.10806424915790558, acc: 0.9632353186607361)
[2025-02-13 20:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01][root][INFO] - Training Epoch: 2/2, step 3737/7134 completed (loss: 0.1145426407456398, acc: 0.9636363387107849)
[2025-02-13 20:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01][root][INFO] - Training Epoch: 2/2, step 3738/7134 completed (loss: 0.03806115314364433, acc: 0.9930555820465088)
[2025-02-13 20:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02][root][INFO] - Training Epoch: 2/2, step 3739/7134 completed (loss: 0.038102224469184875, acc: 0.9893617033958435)
[2025-02-13 20:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02][root][INFO] - Training Epoch: 2/2, step 3740/7134 completed (loss: 0.046333715319633484, acc: 0.993630588054657)
[2025-02-13 20:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03][root][INFO] - Training Epoch: 2/2, step 3741/7134 completed (loss: 0.10638631880283356, acc: 0.9885057210922241)
[2025-02-13 20:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03][root][INFO] - Training Epoch: 2/2, step 3742/7134 completed (loss: 0.1093566045165062, acc: 0.9727891087532043)
[2025-02-13 20:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03][root][INFO] - Training Epoch: 2/2, step 3743/7134 completed (loss: 0.08952648192644119, acc: 0.9647058844566345)
[2025-02-13 20:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04][root][INFO] - Training Epoch: 2/2, step 3744/7134 completed (loss: 0.12572428584098816, acc: 0.9593023061752319)
[2025-02-13 20:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04][root][INFO] - Training Epoch: 2/2, step 3745/7134 completed (loss: 0.11315153539180756, acc: 0.9809523820877075)
[2025-02-13 20:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04][root][INFO] - Training Epoch: 2/2, step 3746/7134 completed (loss: 0.12572871148586273, acc: 0.9717513918876648)
[2025-02-13 20:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05][root][INFO] - Training Epoch: 2/2, step 3747/7134 completed (loss: 0.1089363545179367, acc: 0.9611111283302307)
[2025-02-13 20:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05][root][INFO] - Training Epoch: 2/2, step 3748/7134 completed (loss: 0.08144979178905487, acc: 0.9733333587646484)
[2025-02-13 20:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05][root][INFO] - Training Epoch: 2/2, step 3749/7134 completed (loss: 0.21994660794734955, acc: 0.9404761791229248)
[2025-02-13 20:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06][root][INFO] - Training Epoch: 2/2, step 3750/7134 completed (loss: 0.028085989877581596, acc: 0.9938650131225586)
[2025-02-13 20:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06][root][INFO] - Training Epoch: 2/2, step 3751/7134 completed (loss: 0.09735395014286041, acc: 0.9759036302566528)
[2025-02-13 20:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07][root][INFO] - Training Epoch: 2/2, step 3752/7134 completed (loss: 0.1560101956129074, acc: 0.9496855139732361)
[2025-02-13 20:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07][root][INFO] - Training Epoch: 2/2, step 3753/7134 completed (loss: 0.21082977950572968, acc: 0.9447852969169617)
[2025-02-13 20:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07][root][INFO] - Training Epoch: 2/2, step 3754/7134 completed (loss: 0.025981200858950615, acc: 1.0)
[2025-02-13 20:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08][root][INFO] - Training Epoch: 2/2, step 3755/7134 completed (loss: 0.10023503750562668, acc: 0.9752066135406494)
[2025-02-13 20:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08][root][INFO] - Training Epoch: 2/2, step 3756/7134 completed (loss: 0.055933628231287, acc: 0.9884393215179443)
[2025-02-13 20:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08][root][INFO] - Training Epoch: 2/2, step 3757/7134 completed (loss: 0.17769020795822144, acc: 0.9675324559211731)
[2025-02-13 20:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09][root][INFO] - Training Epoch: 2/2, step 3758/7134 completed (loss: 0.1258794516324997, acc: 0.9685039520263672)
[2025-02-13 20:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09][root][INFO] - Training Epoch: 2/2, step 3759/7134 completed (loss: 0.10402017831802368, acc: 0.9720279574394226)
[2025-02-13 20:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10][root][INFO] - Training Epoch: 2/2, step 3760/7134 completed (loss: 0.11168606579303741, acc: 0.9756097793579102)
[2025-02-13 20:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10][root][INFO] - Training Epoch: 2/2, step 3761/7134 completed (loss: 0.03787284344434738, acc: 0.9924242496490479)
[2025-02-13 20:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10][root][INFO] - Training Epoch: 2/2, step 3762/7134 completed (loss: 0.03810109198093414, acc: 1.0)
[2025-02-13 20:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11][root][INFO] - Training Epoch: 2/2, step 3763/7134 completed (loss: 0.03633103892207146, acc: 1.0)
[2025-02-13 20:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11][root][INFO] - Training Epoch: 2/2, step 3764/7134 completed (loss: 0.06333820521831512, acc: 0.9820359349250793)
[2025-02-13 20:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11][root][INFO] - Training Epoch: 2/2, step 3765/7134 completed (loss: 0.05975041165947914, acc: 0.9886363744735718)
[2025-02-13 20:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12][root][INFO] - Training Epoch: 2/2, step 3766/7134 completed (loss: 0.1377228945493698, acc: 0.9731543660163879)
[2025-02-13 20:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12][root][INFO] - Training Epoch: 2/2, step 3767/7134 completed (loss: 0.05905209481716156, acc: 0.9832402467727661)
[2025-02-13 20:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13][root][INFO] - Training Epoch: 2/2, step 3768/7134 completed (loss: 0.09014757722616196, acc: 0.9711538553237915)
[2025-02-13 20:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13][root][INFO] - Training Epoch: 2/2, step 3769/7134 completed (loss: 0.11266613006591797, acc: 0.970588207244873)
[2025-02-13 20:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13][root][INFO] - Training Epoch: 2/2, step 3770/7134 completed (loss: 0.05288543179631233, acc: 0.9865771532058716)
[2025-02-13 20:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14][root][INFO] - Training Epoch: 2/2, step 3771/7134 completed (loss: 0.026714617386460304, acc: 0.9934210777282715)
[2025-02-13 20:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14][root][INFO] - Training Epoch: 2/2, step 3772/7134 completed (loss: 0.05684111639857292, acc: 0.9857142567634583)
[2025-02-13 20:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14][root][INFO] - Training Epoch: 2/2, step 3773/7134 completed (loss: 0.07023200392723083, acc: 0.9893617033958435)
[2025-02-13 20:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15][root][INFO] - Training Epoch: 2/2, step 3774/7134 completed (loss: 0.05309893935918808, acc: 0.9890109896659851)
[2025-02-13 20:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15][root][INFO] - Training Epoch: 2/2, step 3775/7134 completed (loss: 0.08532886952161789, acc: 0.9875776171684265)
[2025-02-13 20:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15][root][INFO] - Training Epoch: 2/2, step 3776/7134 completed (loss: 0.07795780152082443, acc: 0.9807692170143127)
[2025-02-13 20:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:16][root][INFO] - Training Epoch: 2/2, step 3777/7134 completed (loss: 0.027269627898931503, acc: 1.0)
[2025-02-13 20:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:16][root][INFO] - Training Epoch: 2/2, step 3778/7134 completed (loss: 0.12400931119918823, acc: 0.9709302186965942)
[2025-02-13 20:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17][root][INFO] - Training Epoch: 2/2, step 3779/7134 completed (loss: 0.08225099742412567, acc: 0.9937499761581421)
[2025-02-13 20:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17][root][INFO] - Training Epoch: 2/2, step 3780/7134 completed (loss: 0.03337141498923302, acc: 0.9939393997192383)
[2025-02-13 20:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17][root][INFO] - Training Epoch: 2/2, step 3781/7134 completed (loss: 0.0854928269982338, acc: 0.9751552939414978)
[2025-02-13 20:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18][root][INFO] - Training Epoch: 2/2, step 3782/7134 completed (loss: 0.06022912636399269, acc: 0.9874213933944702)
[2025-02-13 20:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18][root][INFO] - Training Epoch: 2/2, step 3783/7134 completed (loss: 0.11573127657175064, acc: 0.976331353187561)
[2025-02-13 20:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18][root][INFO] - Training Epoch: 2/2, step 3784/7134 completed (loss: 0.050297584384679794, acc: 0.9932885766029358)
[2025-02-13 20:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19][root][INFO] - Training Epoch: 2/2, step 3785/7134 completed (loss: 0.058760177344083786, acc: 0.9929078221321106)
[2025-02-13 20:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19][root][INFO] - Training Epoch: 2/2, step 3786/7134 completed (loss: 0.02761492319405079, acc: 0.9940476417541504)
[2025-02-13 20:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19][root][INFO] - Training Epoch: 2/2, step 3787/7134 completed (loss: 0.027356980368494987, acc: 1.0)
[2025-02-13 20:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:20][root][INFO] - Training Epoch: 2/2, step 3788/7134 completed (loss: 0.14949007332324982, acc: 0.9693251252174377)
[2025-02-13 20:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:20][root][INFO] - Training Epoch: 2/2, step 3789/7134 completed (loss: 0.10614220052957535, acc: 0.970588207244873)
[2025-02-13 20:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21][root][INFO] - Training Epoch: 2/2, step 3790/7134 completed (loss: 0.15152792632579803, acc: 0.970588207244873)
[2025-02-13 20:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21][root][INFO] - Training Epoch: 2/2, step 3791/7134 completed (loss: 0.0642927810549736, acc: 0.9826589822769165)
[2025-02-13 20:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21][root][INFO] - Training Epoch: 2/2, step 3792/7134 completed (loss: 0.04793410003185272, acc: 0.9924812316894531)
[2025-02-13 20:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22][root][INFO] - Training Epoch: 2/2, step 3793/7134 completed (loss: 0.062307633459568024, acc: 0.9885057210922241)
[2025-02-13 20:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22][root][INFO] - Training Epoch: 2/2, step 3794/7134 completed (loss: 0.16530445218086243, acc: 0.9666666388511658)
[2025-02-13 20:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22][root][INFO] - Training Epoch: 2/2, step 3795/7134 completed (loss: 0.07551752775907516, acc: 0.9833333492279053)
[2025-02-13 20:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:23][root][INFO] - Training Epoch: 2/2, step 3796/7134 completed (loss: 0.06703238189220428, acc: 0.9824561476707458)
[2025-02-13 20:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:23][root][INFO] - Training Epoch: 2/2, step 3797/7134 completed (loss: 0.05687807872891426, acc: 0.9878787994384766)
[2025-02-13 20:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24][root][INFO] - Training Epoch: 2/2, step 3798/7134 completed (loss: 0.2549367845058441, acc: 0.9308176040649414)
[2025-02-13 20:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24][root][INFO] - Training Epoch: 2/2, step 3799/7134 completed (loss: 0.23098881542682648, acc: 0.9491525292396545)
[2025-02-13 20:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24][root][INFO] - Training Epoch: 2/2, step 3800/7134 completed (loss: 0.21773844957351685, acc: 0.9586206674575806)
[2025-02-13 20:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25][root][INFO] - Training Epoch: 2/2, step 3801/7134 completed (loss: 0.13485859334468842, acc: 0.9723756909370422)
[2025-02-13 20:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25][root][INFO] - Training Epoch: 2/2, step 3802/7134 completed (loss: 0.1494356244802475, acc: 0.953125)
[2025-02-13 20:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25][root][INFO] - Training Epoch: 2/2, step 3803/7134 completed (loss: 0.09479685127735138, acc: 0.9745222926139832)
[2025-02-13 20:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:26][root][INFO] - Training Epoch: 2/2, step 3804/7134 completed (loss: 0.0765162780880928, acc: 0.9929577708244324)
[2025-02-13 20:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:26][root][INFO] - Training Epoch: 2/2, step 3805/7134 completed (loss: 0.101378433406353, acc: 0.977142870426178)
[2025-02-13 20:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27][root][INFO] - Training Epoch: 2/2, step 3806/7134 completed (loss: 0.03337731957435608, acc: 1.0)
[2025-02-13 20:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27][root][INFO] - Training Epoch: 2/2, step 3807/7134 completed (loss: 0.0776836946606636, acc: 0.9726027250289917)
[2025-02-13 20:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27][root][INFO] - Training Epoch: 2/2, step 3808/7134 completed (loss: 0.10521131008863449, acc: 0.97826087474823)
[2025-02-13 20:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28][root][INFO] - Training Epoch: 2/2, step 3809/7134 completed (loss: 0.09248433262109756, acc: 0.9883720874786377)
[2025-02-13 20:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28][root][INFO] - Training Epoch: 2/2, step 3810/7134 completed (loss: 0.0670405700802803, acc: 0.9805194735527039)
[2025-02-13 20:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28][root][INFO] - Training Epoch: 2/2, step 3811/7134 completed (loss: 0.164473295211792, acc: 0.9640718698501587)
[2025-02-13 20:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:29][root][INFO] - Training Epoch: 2/2, step 3812/7134 completed (loss: 0.3175436556339264, acc: 0.9319728016853333)
[2025-02-13 20:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:29][root][INFO] - Training Epoch: 2/2, step 3813/7134 completed (loss: 0.29843342304229736, acc: 0.9513513445854187)
[2025-02-13 20:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30][root][INFO] - Training Epoch: 2/2, step 3814/7134 completed (loss: 0.06337663531303406, acc: 0.977142870426178)
[2025-02-13 20:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30][root][INFO] - Training Epoch: 2/2, step 3815/7134 completed (loss: 0.10148043930530548, acc: 0.9671052694320679)
[2025-02-13 20:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30][root][INFO] - Training Epoch: 2/2, step 3816/7134 completed (loss: 0.05790937691926956, acc: 0.9850746393203735)
[2025-02-13 20:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31][root][INFO] - Training Epoch: 2/2, step 3817/7134 completed (loss: 0.12284789234399796, acc: 0.9775280952453613)
[2025-02-13 20:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31][root][INFO] - Training Epoch: 2/2, step 3818/7134 completed (loss: 0.17939110100269318, acc: 0.9561403393745422)
[2025-02-13 20:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31][root][INFO] - Training Epoch: 2/2, step 3819/7134 completed (loss: 0.013567824847996235, acc: 1.0)
[2025-02-13 20:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32][root][INFO] - Training Epoch: 2/2, step 3820/7134 completed (loss: 0.0733659565448761, acc: 0.9814814925193787)
[2025-02-13 20:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32][root][INFO] - Training Epoch: 2/2, step 3821/7134 completed (loss: 0.03474194183945656, acc: 0.9931507110595703)
[2025-02-13 20:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33][root][INFO] - Training Epoch: 2/2, step 3822/7134 completed (loss: 0.037293799221515656, acc: 0.9905660152435303)
[2025-02-13 20:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33][root][INFO] - Training Epoch: 2/2, step 3823/7134 completed (loss: 0.04392741993069649, acc: 0.9865771532058716)
[2025-02-13 20:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33][root][INFO] - Training Epoch: 2/2, step 3824/7134 completed (loss: 0.09406082332134247, acc: 0.984375)
[2025-02-13 20:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34][root][INFO] - Training Epoch: 2/2, step 3825/7134 completed (loss: 0.2849431037902832, acc: 0.9722222089767456)
[2025-02-13 20:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34][root][INFO] - Training Epoch: 2/2, step 3826/7134 completed (loss: 0.05778491869568825, acc: 0.98591548204422)
[2025-02-13 20:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34][root][INFO] - Training Epoch: 2/2, step 3827/7134 completed (loss: 0.04144170507788658, acc: 0.987261176109314)
[2025-02-13 20:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:35][root][INFO] - Training Epoch: 2/2, step 3828/7134 completed (loss: 0.026552189141511917, acc: 1.0)
[2025-02-13 20:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:35][root][INFO] - Training Epoch: 2/2, step 3829/7134 completed (loss: 0.048770174384117126, acc: 0.9921875)
[2025-02-13 20:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36][root][INFO] - Training Epoch: 2/2, step 3830/7134 completed (loss: 0.13222812116146088, acc: 0.9851852059364319)
[2025-02-13 20:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36][root][INFO] - Training Epoch: 2/2, step 3831/7134 completed (loss: 0.12432263791561127, acc: 0.9671052694320679)
[2025-02-13 20:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36][root][INFO] - Training Epoch: 2/2, step 3832/7134 completed (loss: 0.04267324134707451, acc: 0.9910714030265808)
[2025-02-13 20:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37][root][INFO] - Training Epoch: 2/2, step 3833/7134 completed (loss: 0.061799727380275726, acc: 0.9836065769195557)
[2025-02-13 20:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37][root][INFO] - Training Epoch: 2/2, step 3834/7134 completed (loss: 0.008346052840352058, acc: 1.0)
[2025-02-13 20:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37][root][INFO] - Training Epoch: 2/2, step 3835/7134 completed (loss: 0.036161042749881744, acc: 0.9947916865348816)
[2025-02-13 20:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38][root][INFO] - Training Epoch: 2/2, step 3836/7134 completed (loss: 0.06403274089097977, acc: 0.9753086566925049)
[2025-02-13 20:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38][root][INFO] - Training Epoch: 2/2, step 3837/7134 completed (loss: 0.07666140049695969, acc: 0.993630588054657)
[2025-02-13 20:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39][root][INFO] - Training Epoch: 2/2, step 3838/7134 completed (loss: 0.024607352912425995, acc: 1.0)
[2025-02-13 20:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39][root][INFO] - Training Epoch: 2/2, step 3839/7134 completed (loss: 0.029511837288737297, acc: 0.9937888383865356)
[2025-02-13 20:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39][root][INFO] - Training Epoch: 2/2, step 3840/7134 completed (loss: 0.022518018260598183, acc: 0.9939393997192383)
[2025-02-13 20:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40][root][INFO] - Training Epoch: 2/2, step 3841/7134 completed (loss: 0.12742750346660614, acc: 0.9698492288589478)
[2025-02-13 20:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40][root][INFO] - Training Epoch: 2/2, step 3842/7134 completed (loss: 0.01469446811825037, acc: 1.0)
[2025-02-13 20:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40][root][INFO] - Training Epoch: 2/2, step 3843/7134 completed (loss: 0.11467205733060837, acc: 0.977477490901947)
[2025-02-13 20:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41][root][INFO] - Training Epoch: 2/2, step 3844/7134 completed (loss: 0.12013223022222519, acc: 0.9738219976425171)
[2025-02-13 20:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41][root][INFO] - Training Epoch: 2/2, step 3845/7134 completed (loss: 0.08795252442359924, acc: 0.9857142567634583)
[2025-02-13 20:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41][root][INFO] - Training Epoch: 2/2, step 3846/7134 completed (loss: 0.022515200078487396, acc: 1.0)
[2025-02-13 20:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:42][root][INFO] - Training Epoch: 2/2, step 3847/7134 completed (loss: 0.06078428030014038, acc: 0.978723406791687)
[2025-02-13 20:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:42][root][INFO] - Training Epoch: 2/2, step 3848/7134 completed (loss: 0.07276502251625061, acc: 0.9886363744735718)
[2025-02-13 20:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43][root][INFO] - Training Epoch: 2/2, step 3849/7134 completed (loss: 0.09484995901584625, acc: 0.9629629850387573)
[2025-02-13 20:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43][root][INFO] - Training Epoch: 2/2, step 3850/7134 completed (loss: 0.09013353288173676, acc: 0.9836065769195557)
[2025-02-13 20:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43][root][INFO] - Training Epoch: 2/2, step 3851/7134 completed (loss: 0.11462166160345078, acc: 0.9464285969734192)
[2025-02-13 20:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44][root][INFO] - Training Epoch: 2/2, step 3852/7134 completed (loss: 0.02321162447333336, acc: 0.9928057789802551)
[2025-02-13 20:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44][root][INFO] - Training Epoch: 2/2, step 3853/7134 completed (loss: 0.12287578731775284, acc: 0.9664804339408875)
[2025-02-13 20:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44][root][INFO] - Training Epoch: 2/2, step 3854/7134 completed (loss: 0.079501673579216, acc: 0.9745222926139832)
[2025-02-13 20:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45][root][INFO] - Training Epoch: 2/2, step 3855/7134 completed (loss: 0.0733608826994896, acc: 0.9880239367485046)
[2025-02-13 20:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45][root][INFO] - Training Epoch: 2/2, step 3856/7134 completed (loss: 0.08595210313796997, acc: 0.9745222926139832)
[2025-02-13 20:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45][root][INFO] - Training Epoch: 2/2, step 3857/7134 completed (loss: 0.08033037930727005, acc: 0.9729729890823364)
[2025-02-13 20:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46][root][INFO] - Training Epoch: 2/2, step 3858/7134 completed (loss: 0.2073960304260254, acc: 0.949999988079071)
[2025-02-13 20:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46][root][INFO] - Training Epoch: 2/2, step 3859/7134 completed (loss: 0.09932482242584229, acc: 0.9830508232116699)
[2025-02-13 20:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47][root][INFO] - Training Epoch: 2/2, step 3860/7134 completed (loss: 0.035967420786619186, acc: 0.9947643876075745)
[2025-02-13 20:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47][root][INFO] - Training Epoch: 2/2, step 3861/7134 completed (loss: 0.06934574991464615, acc: 0.9934640526771545)
[2025-02-13 20:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47][root][INFO] - Training Epoch: 2/2, step 3862/7134 completed (loss: 0.054663822054862976, acc: 0.9923076629638672)
[2025-02-13 20:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48][root][INFO] - Training Epoch: 2/2, step 3863/7134 completed (loss: 0.029758965596556664, acc: 0.9941860437393188)
[2025-02-13 20:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48][root][INFO] - Training Epoch: 2/2, step 3864/7134 completed (loss: 0.06470394134521484, acc: 0.9805194735527039)
[2025-02-13 20:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48][root][INFO] - Training Epoch: 2/2, step 3865/7134 completed (loss: 0.0961499884724617, acc: 0.9732142686843872)
[2025-02-13 20:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49][root][INFO] - Training Epoch: 2/2, step 3866/7134 completed (loss: 0.13159050047397614, acc: 0.9700000286102295)
[2025-02-13 20:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49][root][INFO] - Training Epoch: 2/2, step 3867/7134 completed (loss: 0.062151823192834854, acc: 0.9818181991577148)
[2025-02-13 20:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50][root][INFO] - Training Epoch: 2/2, step 3868/7134 completed (loss: 0.04161730781197548, acc: 0.9926470518112183)
[2025-02-13 20:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50][root][INFO] - Training Epoch: 2/2, step 3869/7134 completed (loss: 0.07160443067550659, acc: 0.9790209531784058)
[2025-02-13 20:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50][root][INFO] - Training Epoch: 2/2, step 3870/7134 completed (loss: 0.15349608659744263, acc: 0.9738562107086182)
[2025-02-13 20:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51][root][INFO] - Training Epoch: 2/2, step 3871/7134 completed (loss: 0.10201379656791687, acc: 0.977011501789093)
[2025-02-13 20:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51][root][INFO] - Training Epoch: 2/2, step 3872/7134 completed (loss: 0.06357499212026596, acc: 0.9848484992980957)
[2025-02-13 20:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51][root][INFO] - Training Epoch: 2/2, step 3873/7134 completed (loss: 0.06248648092150688, acc: 0.9818181991577148)
[2025-02-13 20:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52][root][INFO] - Training Epoch: 2/2, step 3874/7134 completed (loss: 0.08429640531539917, acc: 0.988304078578949)
[2025-02-13 20:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52][root][INFO] - Training Epoch: 2/2, step 3875/7134 completed (loss: 0.08763661235570908, acc: 0.9810126423835754)
[2025-02-13 20:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52][root][INFO] - Training Epoch: 2/2, step 3876/7134 completed (loss: 0.04136577993631363, acc: 0.9919999837875366)
[2025-02-13 20:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53][root][INFO] - Training Epoch: 2/2, step 3877/7134 completed (loss: 0.0972541868686676, acc: 0.9767441749572754)
[2025-02-13 20:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53][root][INFO] - Training Epoch: 2/2, step 3878/7134 completed (loss: 0.4064379334449768, acc: 0.9244186282157898)
[2025-02-13 20:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53][root][INFO] - Training Epoch: 2/2, step 3879/7134 completed (loss: 0.10606559365987778, acc: 0.9776119589805603)
[2025-02-13 20:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:54][root][INFO] - Training Epoch: 2/2, step 3880/7134 completed (loss: 0.09596948325634003, acc: 0.9551281929016113)
[2025-02-13 20:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:54][root][INFO] - Training Epoch: 2/2, step 3881/7134 completed (loss: 0.27856332063674927, acc: 0.9304812550544739)
[2025-02-13 20:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55][root][INFO] - Training Epoch: 2/2, step 3882/7134 completed (loss: 0.13116848468780518, acc: 0.9587628841400146)
[2025-02-13 20:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55][root][INFO] - Training Epoch: 2/2, step 3883/7134 completed (loss: 0.06656378507614136, acc: 0.9934640526771545)
[2025-02-13 20:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55][root][INFO] - Training Epoch: 2/2, step 3884/7134 completed (loss: 0.05833577364683151, acc: 0.9885714054107666)
[2025-02-13 20:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56][root][INFO] - Training Epoch: 2/2, step 3885/7134 completed (loss: 0.025072496384382248, acc: 1.0)
[2025-02-13 20:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56][root][INFO] - Training Epoch: 2/2, step 3886/7134 completed (loss: 0.11151938140392303, acc: 0.9682539701461792)
[2025-02-13 20:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56][root][INFO] - Training Epoch: 2/2, step 3887/7134 completed (loss: 0.12040114402770996, acc: 0.9811320900917053)
[2025-02-13 20:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57][root][INFO] - Training Epoch: 2/2, step 3888/7134 completed (loss: 0.050064560025930405, acc: 0.9767441749572754)
[2025-02-13 20:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57][root][INFO] - Training Epoch: 2/2, step 3889/7134 completed (loss: 0.04378015547990799, acc: 0.9774011373519897)
[2025-02-13 20:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57][root][INFO] - Training Epoch: 2/2, step 3890/7134 completed (loss: 0.038266945630311966, acc: 0.9896907210350037)
[2025-02-13 20:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58][root][INFO] - Training Epoch: 2/2, step 3891/7134 completed (loss: 0.05119907483458519, acc: 0.9884393215179443)
[2025-02-13 20:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58][root][INFO] - Training Epoch: 2/2, step 3892/7134 completed (loss: 0.1027117446064949, acc: 0.9800000190734863)
[2025-02-13 20:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59][root][INFO] - Training Epoch: 2/2, step 3893/7134 completed (loss: 0.15372370183467865, acc: 0.9587628841400146)
[2025-02-13 20:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59][root][INFO] - Training Epoch: 2/2, step 3894/7134 completed (loss: 0.06740424782037735, acc: 0.9873417615890503)
[2025-02-13 20:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59][root][INFO] - Training Epoch: 2/2, step 3895/7134 completed (loss: 0.09906433522701263, acc: 0.9779005646705627)
[2025-02-13 20:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00][root][INFO] - Training Epoch: 2/2, step 3896/7134 completed (loss: 0.06950775533914566, acc: 0.9763033390045166)
[2025-02-13 20:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00][root][INFO] - Training Epoch: 2/2, step 3897/7134 completed (loss: 0.2280057668685913, acc: 0.9651162624359131)
[2025-02-13 20:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00][root][INFO] - Training Epoch: 2/2, step 3898/7134 completed (loss: 0.07805059105157852, acc: 0.977011501789093)
[2025-02-13 20:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:01][root][INFO] - Training Epoch: 2/2, step 3899/7134 completed (loss: 0.06972629576921463, acc: 0.9884393215179443)
[2025-02-13 20:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:01][root][INFO] - Training Epoch: 2/2, step 3900/7134 completed (loss: 0.07890595495700836, acc: 0.9793814420700073)
[2025-02-13 20:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02][root][INFO] - Training Epoch: 2/2, step 3901/7134 completed (loss: 0.2105884999036789, acc: 0.9578313231468201)
[2025-02-13 20:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02][root][INFO] - Training Epoch: 2/2, step 3902/7134 completed (loss: 0.1397656351327896, acc: 0.9739583134651184)
[2025-02-13 20:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02][root][INFO] - Training Epoch: 2/2, step 3903/7134 completed (loss: 0.26558151841163635, acc: 0.9430052042007446)
[2025-02-13 20:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03][root][INFO] - Training Epoch: 2/2, step 3904/7134 completed (loss: 0.13702571392059326, acc: 0.9649122953414917)
[2025-02-13 20:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03][root][INFO] - Training Epoch: 2/2, step 3905/7134 completed (loss: 0.04248631373047829, acc: 0.9833333492279053)
[2025-02-13 20:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03][root][INFO] - Training Epoch: 2/2, step 3906/7134 completed (loss: 0.04795832559466362, acc: 0.9887640476226807)
[2025-02-13 20:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04][root][INFO] - Training Epoch: 2/2, step 3907/7134 completed (loss: 0.08760049939155579, acc: 0.9742268323898315)
[2025-02-13 20:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04][root][INFO] - Training Epoch: 2/2, step 3908/7134 completed (loss: 0.1835842728614807, acc: 0.9595375657081604)
[2025-02-13 20:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05][root][INFO] - Training Epoch: 2/2, step 3909/7134 completed (loss: 0.04373191297054291, acc: 1.0)
[2025-02-13 20:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05][root][INFO] - Training Epoch: 2/2, step 3910/7134 completed (loss: 0.03465054929256439, acc: 0.9943181872367859)
[2025-02-13 20:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05][root][INFO] - Training Epoch: 2/2, step 3911/7134 completed (loss: 0.08045919239521027, acc: 0.9685534834861755)
[2025-02-13 20:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06][root][INFO] - Training Epoch: 2/2, step 3912/7134 completed (loss: 0.010866700671613216, acc: 1.0)
[2025-02-13 20:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06][root][INFO] - Training Epoch: 2/2, step 3913/7134 completed (loss: 0.02984859235584736, acc: 0.9898989796638489)
[2025-02-13 20:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06][root][INFO] - Training Epoch: 2/2, step 3914/7134 completed (loss: 0.07413327693939209, acc: 0.9748427867889404)
[2025-02-13 20:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07][root][INFO] - Training Epoch: 2/2, step 3915/7134 completed (loss: 0.02539565972983837, acc: 0.9942528605461121)
[2025-02-13 20:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07][root][INFO] - Training Epoch: 2/2, step 3916/7134 completed (loss: 0.05803889036178589, acc: 0.9897959232330322)
[2025-02-13 20:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08][root][INFO] - Training Epoch: 2/2, step 3917/7134 completed (loss: 0.02271842584013939, acc: 0.9953488111495972)
[2025-02-13 20:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08][root][INFO] - Training Epoch: 2/2, step 3918/7134 completed (loss: 0.06921657174825668, acc: 0.9666666388511658)
[2025-02-13 20:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08][root][INFO] - Training Epoch: 2/2, step 3919/7134 completed (loss: 0.14240965247154236, acc: 0.9704142212867737)
[2025-02-13 20:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09][root][INFO] - Training Epoch: 2/2, step 3920/7134 completed (loss: 0.18209759891033173, acc: 0.9520958065986633)
[2025-02-13 20:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09][root][INFO] - Training Epoch: 2/2, step 3921/7134 completed (loss: 0.14649710059165955, acc: 0.9647887349128723)
[2025-02-13 20:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09][root][INFO] - Training Epoch: 2/2, step 3922/7134 completed (loss: 0.18868164718151093, acc: 0.9610389471054077)
[2025-02-13 20:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10][root][INFO] - Training Epoch: 2/2, step 3923/7134 completed (loss: 0.22090616822242737, acc: 0.9642857313156128)
[2025-02-13 20:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10][root][INFO] - Training Epoch: 2/2, step 3924/7134 completed (loss: 0.11997084319591522, acc: 0.9484536051750183)
[2025-02-13 20:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10][root][INFO] - Training Epoch: 2/2, step 3925/7134 completed (loss: 0.10772999376058578, acc: 0.9779005646705627)
[2025-02-13 20:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11][root][INFO] - Training Epoch: 2/2, step 3926/7134 completed (loss: 0.07348920404911041, acc: 0.9810126423835754)
[2025-02-13 20:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11][root][INFO] - Training Epoch: 2/2, step 3927/7134 completed (loss: 0.05272149667143822, acc: 0.9939393997192383)
[2025-02-13 20:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12][root][INFO] - Training Epoch: 2/2, step 3928/7134 completed (loss: 0.14921776950359344, acc: 0.976047933101654)
[2025-02-13 20:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12][root][INFO] - Training Epoch: 2/2, step 3929/7134 completed (loss: 0.09651046246290207, acc: 0.9805194735527039)
[2025-02-13 20:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12][root][INFO] - Training Epoch: 2/2, step 3930/7134 completed (loss: 0.12913909554481506, acc: 0.9576271176338196)
[2025-02-13 20:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13][root][INFO] - Training Epoch: 2/2, step 3931/7134 completed (loss: 0.05423698574304581, acc: 0.9810126423835754)
[2025-02-13 20:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13][root][INFO] - Training Epoch: 2/2, step 3932/7134 completed (loss: 0.043450821191072464, acc: 1.0)
[2025-02-13 20:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13][root][INFO] - Training Epoch: 2/2, step 3933/7134 completed (loss: 0.00756543455645442, acc: 1.0)
[2025-02-13 20:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:14][root][INFO] - Training Epoch: 2/2, step 3934/7134 completed (loss: 0.020560944452881813, acc: 1.0)
[2025-02-13 20:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:14][root][INFO] - Training Epoch: 2/2, step 3935/7134 completed (loss: 0.01582483947277069, acc: 1.0)
[2025-02-13 20:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15][root][INFO] - Training Epoch: 2/2, step 3936/7134 completed (loss: 0.06355993449687958, acc: 0.9919354915618896)
[2025-02-13 20:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15][root][INFO] - Training Epoch: 2/2, step 3937/7134 completed (loss: 0.15996518731117249, acc: 0.9661017060279846)
[2025-02-13 20:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15][root][INFO] - Training Epoch: 2/2, step 3938/7134 completed (loss: 0.0771956741809845, acc: 0.9819819927215576)
[2025-02-13 20:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16][root][INFO] - Training Epoch: 2/2, step 3939/7134 completed (loss: 0.13149836659431458, acc: 0.9629629850387573)
[2025-02-13 20:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16][root][INFO] - Training Epoch: 2/2, step 3940/7134 completed (loss: 0.041118837893009186, acc: 1.0)
[2025-02-13 20:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16][root][INFO] - Training Epoch: 2/2, step 3941/7134 completed (loss: 0.08517041802406311, acc: 0.9878048896789551)
[2025-02-13 20:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17][root][INFO] - Training Epoch: 2/2, step 3942/7134 completed (loss: 0.06545612961053848, acc: 0.9919999837875366)
[2025-02-13 20:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17][root][INFO] - Training Epoch: 2/2, step 3943/7134 completed (loss: 0.02658657170832157, acc: 1.0)
[2025-02-13 20:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18][root][INFO] - Training Epoch: 2/2, step 3944/7134 completed (loss: 0.05534091591835022, acc: 0.9850746393203735)
[2025-02-13 20:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18][root][INFO] - Training Epoch: 2/2, step 3945/7134 completed (loss: 0.023153433576226234, acc: 0.9934210777282715)
[2025-02-13 20:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18][root][INFO] - Training Epoch: 2/2, step 3946/7134 completed (loss: 0.03608027845621109, acc: 0.9821428656578064)
[2025-02-13 20:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19][root][INFO] - Training Epoch: 2/2, step 3947/7134 completed (loss: 0.007411051541566849, acc: 1.0)
[2025-02-13 20:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19][root][INFO] - Training Epoch: 2/2, step 3948/7134 completed (loss: 0.012356895953416824, acc: 1.0)
[2025-02-13 20:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19][root][INFO] - Training Epoch: 2/2, step 3949/7134 completed (loss: 0.07063666731119156, acc: 0.9765625)
[2025-02-13 20:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20][root][INFO] - Training Epoch: 2/2, step 3950/7134 completed (loss: 0.02787269838154316, acc: 1.0)
[2025-02-13 20:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20][root][INFO] - Training Epoch: 2/2, step 3951/7134 completed (loss: 0.028538675978779793, acc: 0.9896907210350037)
[2025-02-13 20:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20][root][INFO] - Training Epoch: 2/2, step 3952/7134 completed (loss: 0.014255461283028126, acc: 1.0)
[2025-02-13 20:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21][root][INFO] - Training Epoch: 2/2, step 3953/7134 completed (loss: 0.022223584353923798, acc: 0.9927536249160767)
[2025-02-13 20:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21][root][INFO] - Training Epoch: 2/2, step 3954/7134 completed (loss: 0.0223956648260355, acc: 1.0)
[2025-02-13 20:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21][root][INFO] - Training Epoch: 2/2, step 3955/7134 completed (loss: 0.028139658272266388, acc: 0.9921259880065918)
[2025-02-13 20:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:22][root][INFO] - Training Epoch: 2/2, step 3956/7134 completed (loss: 0.051781561225652695, acc: 0.9934640526771545)
[2025-02-13 20:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:22][root][INFO] - Training Epoch: 2/2, step 3957/7134 completed (loss: 0.027092190459370613, acc: 1.0)
[2025-02-13 20:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23][root][INFO] - Training Epoch: 2/2, step 3958/7134 completed (loss: 0.07247067242860794, acc: 0.976190447807312)
[2025-02-13 20:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23][root][INFO] - Training Epoch: 2/2, step 3959/7134 completed (loss: 0.020633284002542496, acc: 1.0)
[2025-02-13 20:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23][root][INFO] - Training Epoch: 2/2, step 3960/7134 completed (loss: 0.056029707193374634, acc: 0.994413435459137)
[2025-02-13 20:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24][root][INFO] - Training Epoch: 2/2, step 3961/7134 completed (loss: 0.013532896526157856, acc: 1.0)
[2025-02-13 20:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24][root][INFO] - Training Epoch: 2/2, step 3962/7134 completed (loss: 0.033159106969833374, acc: 0.9932885766029358)
[2025-02-13 20:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24][root][INFO] - Training Epoch: 2/2, step 3963/7134 completed (loss: 0.16022762656211853, acc: 0.9619565010070801)
[2025-02-13 20:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25][root][INFO] - Training Epoch: 2/2, step 3964/7134 completed (loss: 0.16369789838790894, acc: 0.9615384340286255)
[2025-02-13 20:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25][root][INFO] - Training Epoch: 2/2, step 3965/7134 completed (loss: 0.06422578543424606, acc: 0.9818181991577148)
[2025-02-13 20:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25][root][INFO] - Training Epoch: 2/2, step 3966/7134 completed (loss: 0.034451548010110855, acc: 0.9921875)
[2025-02-13 20:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26][root][INFO] - Training Epoch: 2/2, step 3967/7134 completed (loss: 0.16981685161590576, acc: 0.95652174949646)
[2025-02-13 20:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26][root][INFO] - Training Epoch: 2/2, step 3968/7134 completed (loss: 0.01723388582468033, acc: 1.0)
[2025-02-13 20:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27][root][INFO] - Training Epoch: 2/2, step 3969/7134 completed (loss: 0.10288742929697037, acc: 0.9622641801834106)
[2025-02-13 20:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27][root][INFO] - Training Epoch: 2/2, step 3970/7134 completed (loss: 0.15018104016780853, acc: 0.9653179049491882)
[2025-02-13 20:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27][root][INFO] - Training Epoch: 2/2, step 3971/7134 completed (loss: 0.2989489436149597, acc: 0.9384615421295166)
[2025-02-13 20:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28][root][INFO] - Training Epoch: 2/2, step 3972/7134 completed (loss: 0.4319855272769928, acc: 0.9038461446762085)
[2025-02-13 20:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28][root][INFO] - Training Epoch: 2/2, step 3973/7134 completed (loss: 0.14040738344192505, acc: 0.9647887349128723)
[2025-02-13 20:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28][root][INFO] - Training Epoch: 2/2, step 3974/7134 completed (loss: 0.21681582927703857, acc: 0.9589743614196777)
[2025-02-13 20:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:29][root][INFO] - Training Epoch: 2/2, step 3975/7134 completed (loss: 0.11563561856746674, acc: 0.977142870426178)
[2025-02-13 20:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:29][root][INFO] - Training Epoch: 2/2, step 3976/7134 completed (loss: 0.22705549001693726, acc: 0.9518072009086609)
[2025-02-13 20:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30][root][INFO] - Training Epoch: 2/2, step 3977/7134 completed (loss: 0.04386582225561142, acc: 0.9927536249160767)
[2025-02-13 20:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30][root][INFO] - Training Epoch: 2/2, step 3978/7134 completed (loss: 0.04811061546206474, acc: 0.9900497794151306)
[2025-02-13 20:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30][root][INFO] - Training Epoch: 2/2, step 3979/7134 completed (loss: 0.08310938626527786, acc: 0.985401451587677)
[2025-02-13 20:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31][root][INFO] - Training Epoch: 2/2, step 3980/7134 completed (loss: 0.059895362704992294, acc: 0.9806451797485352)
[2025-02-13 20:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31][root][INFO] - Training Epoch: 2/2, step 3981/7134 completed (loss: 0.11535625904798508, acc: 0.9698795080184937)
[2025-02-13 20:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31][root][INFO] - Training Epoch: 2/2, step 3982/7134 completed (loss: 0.25357893109321594, acc: 0.9534883499145508)
[2025-02-13 20:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32][root][INFO] - Training Epoch: 2/2, step 3983/7134 completed (loss: 0.5685885548591614, acc: 0.8604651093482971)
[2025-02-13 20:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32][root][INFO] - Training Epoch: 2/2, step 3984/7134 completed (loss: 0.2569550573825836, acc: 0.9202127456665039)
[2025-02-13 20:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32][root][INFO] - Training Epoch: 2/2, step 3985/7134 completed (loss: 0.07764242589473724, acc: 0.9769230484962463)
[2025-02-13 20:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:33][root][INFO] - Training Epoch: 2/2, step 3986/7134 completed (loss: 0.4821970760822296, acc: 0.8977272510528564)
[2025-02-13 20:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:33][root][INFO] - Training Epoch: 2/2, step 3987/7134 completed (loss: 0.15276598930358887, acc: 0.9682539701461792)
[2025-02-13 20:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34][root][INFO] - Training Epoch: 2/2, step 3988/7134 completed (loss: 0.2184237539768219, acc: 0.9473684430122375)
[2025-02-13 20:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34][root][INFO] - Training Epoch: 2/2, step 3989/7134 completed (loss: 0.21487750113010406, acc: 0.942148745059967)
[2025-02-13 20:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34][root][INFO] - Training Epoch: 2/2, step 3990/7134 completed (loss: 0.1715957522392273, acc: 0.9602649211883545)
[2025-02-13 20:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35][root][INFO] - Training Epoch: 2/2, step 3991/7134 completed (loss: 0.08928276598453522, acc: 0.9866666793823242)
[2025-02-13 20:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35][root][INFO] - Training Epoch: 2/2, step 3992/7134 completed (loss: 0.13533155620098114, acc: 0.9780219793319702)
[2025-02-13 20:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35][root][INFO] - Training Epoch: 2/2, step 3993/7134 completed (loss: 0.12076353281736374, acc: 0.9814814925193787)
[2025-02-13 20:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36][root][INFO] - Training Epoch: 2/2, step 3994/7134 completed (loss: 0.09730220586061478, acc: 0.9901960492134094)
[2025-02-13 20:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36][root][INFO] - Training Epoch: 2/2, step 3995/7134 completed (loss: 0.243587926030159, acc: 0.9069767594337463)
[2025-02-13 20:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36][root][INFO] - Training Epoch: 2/2, step 3996/7134 completed (loss: 0.13074028491973877, acc: 0.9586777091026306)
[2025-02-13 20:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37][root][INFO] - Training Epoch: 2/2, step 3997/7134 completed (loss: 0.17658205330371857, acc: 0.9568965435028076)
[2025-02-13 20:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37][root][INFO] - Training Epoch: 2/2, step 3998/7134 completed (loss: 0.1675983965396881, acc: 0.9519230723381042)
[2025-02-13 20:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37][root][INFO] - Training Epoch: 2/2, step 3999/7134 completed (loss: 0.09255266934633255, acc: 0.9838709831237793)
[2025-02-13 20:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:38][root][INFO] - Training Epoch: 2/2, step 4000/7134 completed (loss: 0.12935365736484528, acc: 0.9674796462059021)
[2025-02-13 20:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:38][root][INFO] - Training Epoch: 2/2, step 4001/7134 completed (loss: 0.1997937560081482, acc: 0.9596773982048035)
[2025-02-13 20:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39][root][INFO] - Training Epoch: 2/2, step 4002/7134 completed (loss: 0.226765438914299, acc: 0.9532710313796997)
[2025-02-13 20:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39][root][INFO] - Training Epoch: 2/2, step 4003/7134 completed (loss: 0.18390293419361115, acc: 0.9548872113227844)
[2025-02-13 20:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39][root][INFO] - Training Epoch: 2/2, step 4004/7134 completed (loss: 0.12911655008792877, acc: 0.9700000286102295)
[2025-02-13 20:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40][root][INFO] - Training Epoch: 2/2, step 4005/7134 completed (loss: 0.03153201565146446, acc: 0.9934640526771545)
[2025-02-13 20:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40][root][INFO] - Training Epoch: 2/2, step 4006/7134 completed (loss: 0.032627977430820465, acc: 0.9954954981803894)
[2025-02-13 20:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40][root][INFO] - Training Epoch: 2/2, step 4007/7134 completed (loss: 0.09715000540018082, acc: 0.976190447807312)
[2025-02-13 20:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:41][root][INFO] - Training Epoch: 2/2, step 4008/7134 completed (loss: 0.03320658579468727, acc: 0.9910314083099365)
[2025-02-13 20:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:41][root][INFO] - Training Epoch: 2/2, step 4009/7134 completed (loss: 0.021646913141012192, acc: 1.0)
[2025-02-13 20:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42][root][INFO] - Training Epoch: 2/2, step 4010/7134 completed (loss: 0.047661639750003815, acc: 0.9856459498405457)
[2025-02-13 20:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42][root][INFO] - Training Epoch: 2/2, step 4011/7134 completed (loss: 0.04922989010810852, acc: 0.9885714054107666)
[2025-02-13 20:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42][root][INFO] - Training Epoch: 2/2, step 4012/7134 completed (loss: 0.0430750772356987, acc: 0.9879518151283264)
[2025-02-13 20:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43][root][INFO] - Training Epoch: 2/2, step 4013/7134 completed (loss: 0.014058955013751984, acc: 1.0)
[2025-02-13 20:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43][root][INFO] - Training Epoch: 2/2, step 4014/7134 completed (loss: 0.024403585121035576, acc: 0.9933333396911621)
[2025-02-13 20:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43][root][INFO] - Training Epoch: 2/2, step 4015/7134 completed (loss: 0.026339532807469368, acc: 0.9862068891525269)
[2025-02-13 20:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44][root][INFO] - Training Epoch: 2/2, step 4016/7134 completed (loss: 0.11741447448730469, acc: 0.9740932583808899)
[2025-02-13 20:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44][root][INFO] - Training Epoch: 2/2, step 4017/7134 completed (loss: 0.054981108754873276, acc: 0.9896907210350037)
[2025-02-13 20:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44][root][INFO] - Training Epoch: 2/2, step 4018/7134 completed (loss: 0.04036074876785278, acc: 0.9951691031455994)
[2025-02-13 20:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:45][root][INFO] - Training Epoch: 2/2, step 4019/7134 completed (loss: 0.054123762995004654, acc: 0.9944444298744202)
[2025-02-13 20:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:45][root][INFO] - Training Epoch: 2/2, step 4020/7134 completed (loss: 0.06826642900705338, acc: 0.9886363744735718)
[2025-02-13 20:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46][root][INFO] - Training Epoch: 2/2, step 4021/7134 completed (loss: 0.03800820931792259, acc: 0.9852941036224365)
[2025-02-13 20:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46][root][INFO] - Training Epoch: 2/2, step 4022/7134 completed (loss: 0.040534332394599915, acc: 0.9903846383094788)
[2025-02-13 20:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46][root][INFO] - Training Epoch: 2/2, step 4023/7134 completed (loss: 0.1168484166264534, acc: 0.9657142758369446)
[2025-02-13 20:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47][root][INFO] - Training Epoch: 2/2, step 4024/7134 completed (loss: 0.04328290745615959, acc: 0.9885057210922241)
[2025-02-13 20:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47][root][INFO] - Training Epoch: 2/2, step 4025/7134 completed (loss: 0.16238471865653992, acc: 0.9581151604652405)
[2025-02-13 20:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47][root][INFO] - Training Epoch: 2/2, step 4026/7134 completed (loss: 0.17390230298042297, acc: 0.9813664555549622)
[2025-02-13 20:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48][root][INFO] - Training Epoch: 2/2, step 4027/7134 completed (loss: 0.1245071217417717, acc: 0.9550561904907227)
[2025-02-13 20:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48][root][INFO] - Training Epoch: 2/2, step 4028/7134 completed (loss: 0.18833598494529724, acc: 0.9685039520263672)
[2025-02-13 20:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48][root][INFO] - Training Epoch: 2/2, step 4029/7134 completed (loss: 0.05644439905881882, acc: 0.9837837815284729)
[2025-02-13 20:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49][root][INFO] - Training Epoch: 2/2, step 4030/7134 completed (loss: 0.037670813500881195, acc: 0.9903846383094788)
[2025-02-13 20:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49][root][INFO] - Training Epoch: 2/2, step 4031/7134 completed (loss: 0.05621260777115822, acc: 0.9875776171684265)
[2025-02-13 20:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50][root][INFO] - Training Epoch: 2/2, step 4032/7134 completed (loss: 0.09989634156227112, acc: 0.9767441749572754)
[2025-02-13 20:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50][root][INFO] - Training Epoch: 2/2, step 4033/7134 completed (loss: 0.01338291447609663, acc: 1.0)
[2025-02-13 20:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50][root][INFO] - Training Epoch: 2/2, step 4034/7134 completed (loss: 0.05587188899517059, acc: 0.9948979616165161)
[2025-02-13 20:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51][root][INFO] - Training Epoch: 2/2, step 4035/7134 completed (loss: 0.2082105129957199, acc: 0.9560975432395935)
[2025-02-13 20:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51][root][INFO] - Training Epoch: 2/2, step 4036/7134 completed (loss: 0.07563915103673935, acc: 0.9846153855323792)
[2025-02-13 20:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51][root][INFO] - Training Epoch: 2/2, step 4037/7134 completed (loss: 0.08960222452878952, acc: 0.9739583134651184)
[2025-02-13 20:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52][root][INFO] - Training Epoch: 2/2, step 4038/7134 completed (loss: 0.07189588993787766, acc: 0.9886363744735718)
[2025-02-13 20:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52][root][INFO] - Training Epoch: 2/2, step 4039/7134 completed (loss: 0.12411244213581085, acc: 0.976331353187561)
[2025-02-13 20:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52][root][INFO] - Training Epoch: 2/2, step 4040/7134 completed (loss: 0.04224754497408867, acc: 0.9890710115432739)
[2025-02-13 20:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53][root][INFO] - Training Epoch: 2/2, step 4041/7134 completed (loss: 0.11323198676109314, acc: 0.9710982441902161)
[2025-02-13 20:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53][root][INFO] - Training Epoch: 2/2, step 4042/7134 completed (loss: 0.07617633789777756, acc: 0.9797297120094299)
[2025-02-13 20:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53][root][INFO] - Training Epoch: 2/2, step 4043/7134 completed (loss: 0.10373128950595856, acc: 0.9683544039726257)
[2025-02-13 20:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:54][root][INFO] - Training Epoch: 2/2, step 4044/7134 completed (loss: 0.070035919547081, acc: 0.9835164546966553)
[2025-02-13 20:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:54][root][INFO] - Training Epoch: 2/2, step 4045/7134 completed (loss: 0.0745958536863327, acc: 0.977142870426178)
[2025-02-13 20:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55][root][INFO] - Training Epoch: 2/2, step 4046/7134 completed (loss: 0.10389112681150436, acc: 0.9824561476707458)
[2025-02-13 20:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55][root][INFO] - Training Epoch: 2/2, step 4047/7134 completed (loss: 0.09457935392856598, acc: 0.9890710115432739)
[2025-02-13 20:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55][root][INFO] - Training Epoch: 2/2, step 4048/7134 completed (loss: 0.032508108764886856, acc: 1.0)
[2025-02-13 20:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56][root][INFO] - Training Epoch: 2/2, step 4049/7134 completed (loss: 0.09391461312770844, acc: 0.9751243591308594)
[2025-02-13 20:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56][root][INFO] - Training Epoch: 2/2, step 4050/7134 completed (loss: 0.12754620611667633, acc: 0.9624999761581421)
[2025-02-13 20:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56][root][INFO] - Training Epoch: 2/2, step 4051/7134 completed (loss: 0.1415017992258072, acc: 0.9608938694000244)
[2025-02-13 20:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57][root][INFO] - Training Epoch: 2/2, step 4052/7134 completed (loss: 0.08224701881408691, acc: 0.9830508232116699)
[2025-02-13 20:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57][root][INFO] - Training Epoch: 2/2, step 4053/7134 completed (loss: 0.07497495412826538, acc: 0.9702380895614624)
[2025-02-13 20:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58][root][INFO] - Training Epoch: 2/2, step 4054/7134 completed (loss: 0.23544958233833313, acc: 0.939130425453186)
[2025-02-13 20:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58][root][INFO] - Training Epoch: 2/2, step 4055/7134 completed (loss: 0.09353573620319366, acc: 0.9680851101875305)
[2025-02-13 20:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58][root][INFO] - Training Epoch: 2/2, step 4056/7134 completed (loss: 0.062149930745363235, acc: 0.9842105507850647)
[2025-02-13 20:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59][root][INFO] - Training Epoch: 2/2, step 4057/7134 completed (loss: 0.06805519014596939, acc: 0.9823529124259949)
[2025-02-13 20:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59][root][INFO] - Training Epoch: 2/2, step 4058/7134 completed (loss: 0.08365039527416229, acc: 0.9772727489471436)
[2025-02-13 20:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59][root][INFO] - Training Epoch: 2/2, step 4059/7134 completed (loss: 0.022632239386439323, acc: 0.9925373196601868)
[2025-02-13 20:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:00][root][INFO] - Training Epoch: 2/2, step 4060/7134 completed (loss: 0.026275696232914925, acc: 0.9932432174682617)
[2025-02-13 20:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:00][root][INFO] - Training Epoch: 2/2, step 4061/7134 completed (loss: 0.018481751903891563, acc: 0.9941860437393188)
[2025-02-13 20:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01][root][INFO] - Training Epoch: 2/2, step 4062/7134 completed (loss: 0.018462086096405983, acc: 0.9934210777282715)
[2025-02-13 20:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01][root][INFO] - Training Epoch: 2/2, step 4063/7134 completed (loss: 0.029360460117459297, acc: 1.0)
[2025-02-13 20:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01][root][INFO] - Training Epoch: 2/2, step 4064/7134 completed (loss: 0.019548753276467323, acc: 1.0)
[2025-02-13 20:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02][root][INFO] - Training Epoch: 2/2, step 4065/7134 completed (loss: 0.013875015079975128, acc: 1.0)
[2025-02-13 20:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02][root][INFO] - Training Epoch: 2/2, step 4066/7134 completed (loss: 0.014906211756169796, acc: 0.9934640526771545)
[2025-02-13 20:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02][root][INFO] - Training Epoch: 2/2, step 4067/7134 completed (loss: 0.06311555206775665, acc: 0.9794520735740662)
[2025-02-13 20:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:03][root][INFO] - Training Epoch: 2/2, step 4068/7134 completed (loss: 0.13553838431835175, acc: 0.9669421315193176)
[2025-02-13 20:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:03][root][INFO] - Training Epoch: 2/2, step 4069/7134 completed (loss: 0.032567959278821945, acc: 0.9929577708244324)
[2025-02-13 20:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04][root][INFO] - Training Epoch: 2/2, step 4070/7134 completed (loss: 0.061735861003398895, acc: 0.9867549538612366)
[2025-02-13 20:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04][root][INFO] - Training Epoch: 2/2, step 4071/7134 completed (loss: 0.05809041112661362, acc: 0.9863013625144958)
[2025-02-13 20:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04][root][INFO] - Training Epoch: 2/2, step 4072/7134 completed (loss: 0.04664670675992966, acc: 0.9878048896789551)
[2025-02-13 20:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05][root][INFO] - Training Epoch: 2/2, step 4073/7134 completed (loss: 0.023780498653650284, acc: 0.9939758777618408)
[2025-02-13 20:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05][root][INFO] - Training Epoch: 2/2, step 4074/7134 completed (loss: 0.0478702075779438, acc: 0.9867549538612366)
[2025-02-13 20:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05][root][INFO] - Training Epoch: 2/2, step 4075/7134 completed (loss: 0.02693689987063408, acc: 0.9866666793823242)
[2025-02-13 20:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06][root][INFO] - Training Epoch: 2/2, step 4076/7134 completed (loss: 0.02650490589439869, acc: 0.9939024448394775)
[2025-02-13 20:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06][root][INFO] - Training Epoch: 2/2, step 4077/7134 completed (loss: 0.07811097055673599, acc: 0.9795918464660645)
[2025-02-13 20:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:07][root][INFO] - Training Epoch: 2/2, step 4078/7134 completed (loss: 0.05930833891034126, acc: 0.9874213933944702)
[2025-02-13 20:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:07][root][INFO] - Training Epoch: 2/2, step 4079/7134 completed (loss: 0.03961590304970741, acc: 0.9847328066825867)
[2025-02-13 20:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:07][root][INFO] - Training Epoch: 2/2, step 4080/7134 completed (loss: 0.02330159582197666, acc: 0.9931034445762634)
[2025-02-13 20:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08][root][INFO] - Training Epoch: 2/2, step 4081/7134 completed (loss: 0.009852860122919083, acc: 1.0)
[2025-02-13 20:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08][root][INFO] - Training Epoch: 2/2, step 4082/7134 completed (loss: 0.1739014834165573, acc: 0.9605262875556946)
[2025-02-13 20:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08][root][INFO] - Training Epoch: 2/2, step 4083/7134 completed (loss: 0.19071368873119354, acc: 0.9356725215911865)
[2025-02-13 20:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09][root][INFO] - Training Epoch: 2/2, step 4084/7134 completed (loss: 0.32155707478523254, acc: 0.9194630980491638)
[2025-02-13 20:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09][root][INFO] - Training Epoch: 2/2, step 4085/7134 completed (loss: 0.04959073290228844, acc: 0.9894737005233765)
[2025-02-13 20:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09][root][INFO] - Training Epoch: 2/2, step 4086/7134 completed (loss: 0.1334225833415985, acc: 0.9679487347602844)
[2025-02-13 20:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10][root][INFO] - Training Epoch: 2/2, step 4087/7134 completed (loss: 0.08675675094127655, acc: 0.9946236610412598)
[2025-02-13 20:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10][root][INFO] - Training Epoch: 2/2, step 4088/7134 completed (loss: 0.22804045677185059, acc: 0.9340101480484009)
[2025-02-13 20:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11][root][INFO] - Training Epoch: 2/2, step 4089/7134 completed (loss: 0.17774154245853424, acc: 0.9469696879386902)
[2025-02-13 20:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11][root][INFO] - Training Epoch: 2/2, step 4090/7134 completed (loss: 0.30050745606422424, acc: 0.9234449863433838)
[2025-02-13 20:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11][root][INFO] - Training Epoch: 2/2, step 4091/7134 completed (loss: 0.43530598282814026, acc: 0.8876404762268066)
[2025-02-13 20:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12][root][INFO] - Training Epoch: 2/2, step 4092/7134 completed (loss: 0.08148013055324554, acc: 0.9895287752151489)
[2025-02-13 20:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12][root][INFO] - Training Epoch: 2/2, step 4093/7134 completed (loss: 0.07702911645174026, acc: 0.984000027179718)
[2025-02-13 20:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12][root][INFO] - Training Epoch: 2/2, step 4094/7134 completed (loss: 0.12977470457553864, acc: 0.9634146094322205)
[2025-02-13 20:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13][root][INFO] - Training Epoch: 2/2, step 4095/7134 completed (loss: 0.049934614449739456, acc: 0.987261176109314)
[2025-02-13 20:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13][root][INFO] - Training Epoch: 2/2, step 4096/7134 completed (loss: 0.11998765170574188, acc: 0.948051929473877)
[2025-02-13 20:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14][root][INFO] - Training Epoch: 2/2, step 4097/7134 completed (loss: 0.10943593084812164, acc: 0.9800000190734863)
[2025-02-13 20:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14][root][INFO] - Training Epoch: 2/2, step 4098/7134 completed (loss: 0.054230015724897385, acc: 0.9942528605461121)
[2025-02-13 20:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14][root][INFO] - Training Epoch: 2/2, step 4099/7134 completed (loss: 0.08271462470293045, acc: 0.9848484992980957)
[2025-02-13 20:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15][root][INFO] - Training Epoch: 2/2, step 4100/7134 completed (loss: 0.01676342450082302, acc: 1.0)
[2025-02-13 20:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15][root][INFO] - Training Epoch: 2/2, step 4101/7134 completed (loss: 0.05686960741877556, acc: 0.984000027179718)
[2025-02-13 20:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15][root][INFO] - Training Epoch: 2/2, step 4102/7134 completed (loss: 0.07422028481960297, acc: 0.9837398529052734)
[2025-02-13 20:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16][root][INFO] - Training Epoch: 2/2, step 4103/7134 completed (loss: 0.01928742788732052, acc: 1.0)
[2025-02-13 20:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16][root][INFO] - Training Epoch: 2/2, step 4104/7134 completed (loss: 0.02720239944756031, acc: 1.0)
[2025-02-13 20:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17][root][INFO] - Training Epoch: 2/2, step 4105/7134 completed (loss: 0.032327499240636826, acc: 0.9933333396911621)
[2025-02-13 20:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17][root][INFO] - Training Epoch: 2/2, step 4106/7134 completed (loss: 0.04495555907487869, acc: 0.9931972622871399)
[2025-02-13 20:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17][root][INFO] - Training Epoch: 2/2, step 4107/7134 completed (loss: 0.07178813964128494, acc: 0.9852941036224365)
[2025-02-13 20:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18][root][INFO] - Training Epoch: 2/2, step 4108/7134 completed (loss: 0.03829486668109894, acc: 0.9863945841789246)
[2025-02-13 20:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18][root][INFO] - Training Epoch: 2/2, step 4109/7134 completed (loss: 0.023528631776571274, acc: 1.0)
[2025-02-13 20:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19][root][INFO] - Training Epoch: 2/2, step 4110/7134 completed (loss: 0.02127763442695141, acc: 0.9922480583190918)
[2025-02-13 20:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19][root][INFO] - Training Epoch: 2/2, step 4111/7134 completed (loss: 0.07576434314250946, acc: 0.9793103337287903)
[2025-02-13 20:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19][root][INFO] - Training Epoch: 2/2, step 4112/7134 completed (loss: 0.04923466220498085, acc: 0.9767441749572754)
[2025-02-13 20:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20][root][INFO] - Training Epoch: 2/2, step 4113/7134 completed (loss: 0.0213109590113163, acc: 0.9930070042610168)
[2025-02-13 20:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20][root][INFO] - Training Epoch: 2/2, step 4114/7134 completed (loss: 0.11202714592218399, acc: 0.9826086759567261)
[2025-02-13 20:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20][root][INFO] - Training Epoch: 2/2, step 4115/7134 completed (loss: 0.022076496854424477, acc: 1.0)
[2025-02-13 20:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21][root][INFO] - Training Epoch: 2/2, step 4116/7134 completed (loss: 0.016432106494903564, acc: 1.0)
[2025-02-13 20:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21][root][INFO] - Training Epoch: 2/2, step 4117/7134 completed (loss: 0.021325981244444847, acc: 1.0)
[2025-02-13 20:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21][root][INFO] - Training Epoch: 2/2, step 4118/7134 completed (loss: 0.04334087669849396, acc: 0.9895833134651184)
[2025-02-13 20:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:22][root][INFO] - Training Epoch: 2/2, step 4119/7134 completed (loss: 0.10900966823101044, acc: 0.9729729890823364)
[2025-02-13 20:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:22][root][INFO] - Training Epoch: 2/2, step 4120/7134 completed (loss: 0.11333830654621124, acc: 0.9924812316894531)
[2025-02-13 20:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23][root][INFO] - Training Epoch: 2/2, step 4121/7134 completed (loss: 0.04796922951936722, acc: 0.9878048896789551)
[2025-02-13 20:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23][root][INFO] - Training Epoch: 2/2, step 4122/7134 completed (loss: 0.00993704330176115, acc: 1.0)
[2025-02-13 20:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23][root][INFO] - Training Epoch: 2/2, step 4123/7134 completed (loss: 0.01598634570837021, acc: 0.9935064911842346)
[2025-02-13 20:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24][root][INFO] - Training Epoch: 2/2, step 4124/7134 completed (loss: 0.03316100314259529, acc: 0.9937499761581421)
[2025-02-13 20:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24][root][INFO] - Training Epoch: 2/2, step 4125/7134 completed (loss: 0.032866738736629486, acc: 0.9937106966972351)
[2025-02-13 20:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24][root][INFO] - Training Epoch: 2/2, step 4126/7134 completed (loss: 0.05939085781574249, acc: 0.9802631735801697)
[2025-02-13 20:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25][root][INFO] - Training Epoch: 2/2, step 4127/7134 completed (loss: 0.04496697336435318, acc: 0.9865771532058716)
[2025-02-13 20:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25][root][INFO] - Training Epoch: 2/2, step 4128/7134 completed (loss: 0.01778932847082615, acc: 1.0)
[2025-02-13 20:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26][root][INFO] - Training Epoch: 2/2, step 4129/7134 completed (loss: 0.04174616560339928, acc: 0.9865771532058716)
[2025-02-13 20:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26][root][INFO] - Training Epoch: 2/2, step 4130/7134 completed (loss: 0.027553457766771317, acc: 0.9913793206214905)
[2025-02-13 20:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26][root][INFO] - Training Epoch: 2/2, step 4131/7134 completed (loss: 0.04713005572557449, acc: 0.9851852059364319)
[2025-02-13 20:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27][root][INFO] - Training Epoch: 2/2, step 4132/7134 completed (loss: 0.020311638712882996, acc: 1.0)
[2025-02-13 20:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27][root][INFO] - Training Epoch: 2/2, step 4133/7134 completed (loss: 0.06642759591341019, acc: 0.9930070042610168)
[2025-02-13 20:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27][root][INFO] - Training Epoch: 2/2, step 4134/7134 completed (loss: 0.08290501683950424, acc: 0.9727272987365723)
[2025-02-13 20:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28][root][INFO] - Training Epoch: 2/2, step 4135/7134 completed (loss: 0.03552925959229469, acc: 0.9928571581840515)
[2025-02-13 20:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28][root][INFO] - Training Epoch: 2/2, step 4136/7134 completed (loss: 0.056792039424180984, acc: 0.9849624037742615)
[2025-02-13 20:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28][root][INFO] - Training Epoch: 2/2, step 4137/7134 completed (loss: 0.03170540928840637, acc: 1.0)
[2025-02-13 20:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29][root][INFO] - Training Epoch: 2/2, step 4138/7134 completed (loss: 0.11797159165143967, acc: 0.9607843160629272)
[2025-02-13 20:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29][root][INFO] - Training Epoch: 2/2, step 4139/7134 completed (loss: 0.053506892174482346, acc: 0.9904761910438538)
[2025-02-13 20:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29][root][INFO] - Training Epoch: 2/2, step 4140/7134 completed (loss: 0.121337890625, acc: 0.9604519605636597)
[2025-02-13 20:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30][root][INFO] - Training Epoch: 2/2, step 4141/7134 completed (loss: 0.1139952689409256, acc: 0.9807692170143127)
[2025-02-13 20:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30][root][INFO] - Training Epoch: 2/2, step 4142/7134 completed (loss: 0.13803011178970337, acc: 0.9549999833106995)
[2025-02-13 20:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31][root][INFO] - Training Epoch: 2/2, step 4143/7134 completed (loss: 0.11611463129520416, acc: 0.9694322943687439)
[2025-02-13 20:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31][root][INFO] - Training Epoch: 2/2, step 4144/7134 completed (loss: 0.14750248193740845, acc: 0.9589743614196777)
[2025-02-13 20:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31][root][INFO] - Training Epoch: 2/2, step 4145/7134 completed (loss: 0.09358056634664536, acc: 0.9661017060279846)
[2025-02-13 20:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32][root][INFO] - Training Epoch: 2/2, step 4146/7134 completed (loss: 0.2140944004058838, acc: 0.9285714030265808)
[2025-02-13 20:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32][root][INFO] - Training Epoch: 2/2, step 4147/7134 completed (loss: 0.16692034900188446, acc: 0.9576271176338196)
[2025-02-13 20:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32][root][INFO] - Training Epoch: 2/2, step 4148/7134 completed (loss: 0.4565076231956482, acc: 0.8791946172714233)
[2025-02-13 20:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33][root][INFO] - Training Epoch: 2/2, step 4149/7134 completed (loss: 0.2171538770198822, acc: 0.9651162624359131)
[2025-02-13 20:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33][root][INFO] - Training Epoch: 2/2, step 4150/7134 completed (loss: 0.4171893894672394, acc: 0.9316239356994629)
[2025-02-13 20:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34][root][INFO] - Training Epoch: 2/2, step 4151/7134 completed (loss: 0.2086656242609024, acc: 0.942148745059967)
[2025-02-13 20:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34][root][INFO] - Training Epoch: 2/2, step 4152/7134 completed (loss: 0.09622855484485626, acc: 0.9863945841789246)
[2025-02-13 20:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34][root][INFO] - Training Epoch: 2/2, step 4153/7134 completed (loss: 0.1398259401321411, acc: 0.9714285731315613)
[2025-02-13 20:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35][root][INFO] - Training Epoch: 2/2, step 4154/7134 completed (loss: 0.1638350486755371, acc: 0.9473684430122375)
[2025-02-13 20:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35][root][INFO] - Training Epoch: 2/2, step 4155/7134 completed (loss: 0.18761013448238373, acc: 0.9731543660163879)
[2025-02-13 20:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36][root][INFO] - Training Epoch: 2/2, step 4156/7134 completed (loss: 0.10146810114383698, acc: 0.9649122953414917)
[2025-02-13 20:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36][root][INFO] - Training Epoch: 2/2, step 4157/7134 completed (loss: 0.0762321799993515, acc: 0.9831932783126831)
[2025-02-13 20:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36][root][INFO] - Training Epoch: 2/2, step 4158/7134 completed (loss: 0.08829454332590103, acc: 0.9780219793319702)
[2025-02-13 20:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37][root][INFO] - Training Epoch: 2/2, step 4159/7134 completed (loss: 0.10005660355091095, acc: 0.9849624037742615)
[2025-02-13 20:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37][root][INFO] - Training Epoch: 2/2, step 4160/7134 completed (loss: 0.11994708329439163, acc: 0.9831932783126831)
[2025-02-13 20:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37][root][INFO] - Training Epoch: 2/2, step 4161/7134 completed (loss: 0.05760477855801582, acc: 0.9913793206214905)
[2025-02-13 20:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38][root][INFO] - Training Epoch: 2/2, step 4162/7134 completed (loss: 0.10209979116916656, acc: 0.970059871673584)
[2025-02-13 20:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38][root][INFO] - Training Epoch: 2/2, step 4163/7134 completed (loss: 0.1057114228606224, acc: 0.9932885766029358)
[2025-02-13 20:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38][root][INFO] - Training Epoch: 2/2, step 4164/7134 completed (loss: 0.05541307479143143, acc: 0.9751552939414978)
[2025-02-13 20:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39][root][INFO] - Training Epoch: 2/2, step 4165/7134 completed (loss: 0.08838512003421783, acc: 0.9784946441650391)
[2025-02-13 20:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39][root][INFO] - Training Epoch: 2/2, step 4166/7134 completed (loss: 0.14409790933132172, acc: 0.9615384340286255)
[2025-02-13 20:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40][root][INFO] - Training Epoch: 2/2, step 4167/7134 completed (loss: 0.08624885231256485, acc: 0.9837398529052734)
[2025-02-13 20:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40][root][INFO] - Training Epoch: 2/2, step 4168/7134 completed (loss: 0.11405649036169052, acc: 0.969924807548523)
[2025-02-13 20:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40][root][INFO] - Training Epoch: 2/2, step 4169/7134 completed (loss: 0.10866936296224594, acc: 0.9769585132598877)
[2025-02-13 20:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41][root][INFO] - Training Epoch: 2/2, step 4170/7134 completed (loss: 0.10657015442848206, acc: 0.961904764175415)
[2025-02-13 20:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41][root][INFO] - Training Epoch: 2/2, step 4171/7134 completed (loss: 0.06602980196475983, acc: 0.9802955389022827)
[2025-02-13 20:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41][root][INFO] - Training Epoch: 2/2, step 4172/7134 completed (loss: 0.06962034106254578, acc: 0.9888888597488403)
[2025-02-13 20:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42][root][INFO] - Training Epoch: 2/2, step 4173/7134 completed (loss: 0.043010592460632324, acc: 1.0)
[2025-02-13 20:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42][root][INFO] - Training Epoch: 2/2, step 4174/7134 completed (loss: 0.012426687404513359, acc: 1.0)
[2025-02-13 20:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43][root][INFO] - Training Epoch: 2/2, step 4175/7134 completed (loss: 0.06892609596252441, acc: 0.9647887349128723)
[2025-02-13 20:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43][root][INFO] - Training Epoch: 2/2, step 4176/7134 completed (loss: 0.03126966208219528, acc: 0.9870967864990234)
[2025-02-13 20:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43][root][INFO] - Training Epoch: 2/2, step 4177/7134 completed (loss: 0.031053200364112854, acc: 0.9941520690917969)
[2025-02-13 20:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44][root][INFO] - Training Epoch: 2/2, step 4178/7134 completed (loss: 0.05731627345085144, acc: 0.9868420958518982)
[2025-02-13 20:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44][root][INFO] - Training Epoch: 2/2, step 4179/7134 completed (loss: 0.018053872510790825, acc: 1.0)
[2025-02-13 20:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44][root][INFO] - Training Epoch: 2/2, step 4180/7134 completed (loss: 0.030976999551057816, acc: 0.9937499761581421)
[2025-02-13 20:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45][root][INFO] - Training Epoch: 2/2, step 4181/7134 completed (loss: 0.01086110807955265, acc: 1.0)
[2025-02-13 20:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45][root][INFO] - Training Epoch: 2/2, step 4182/7134 completed (loss: 0.011362436227500439, acc: 1.0)
[2025-02-13 20:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45][root][INFO] - Training Epoch: 2/2, step 4183/7134 completed (loss: 0.03610403090715408, acc: 1.0)
[2025-02-13 20:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:46][root][INFO] - Training Epoch: 2/2, step 4184/7134 completed (loss: 0.021538151428103447, acc: 0.9941176176071167)
[2025-02-13 20:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:46][root][INFO] - Training Epoch: 2/2, step 4185/7134 completed (loss: 0.012928549200296402, acc: 1.0)
[2025-02-13 20:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47][root][INFO] - Training Epoch: 2/2, step 4186/7134 completed (loss: 0.10149585455656052, acc: 0.9793814420700073)
[2025-02-13 20:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47][root][INFO] - Training Epoch: 2/2, step 4187/7134 completed (loss: 0.013456705957651138, acc: 1.0)
[2025-02-13 20:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47][root][INFO] - Training Epoch: 2/2, step 4188/7134 completed (loss: 0.015307694673538208, acc: 0.9947368502616882)
[2025-02-13 20:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48][root][INFO] - Training Epoch: 2/2, step 4189/7134 completed (loss: 0.04868379980325699, acc: 0.9820359349250793)
[2025-02-13 20:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48][root][INFO] - Training Epoch: 2/2, step 4190/7134 completed (loss: 0.05913655087351799, acc: 0.9766082167625427)
[2025-02-13 20:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48][root][INFO] - Training Epoch: 2/2, step 4191/7134 completed (loss: 0.05298066511750221, acc: 0.9903846383094788)
[2025-02-13 20:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49][root][INFO] - Training Epoch: 2/2, step 4192/7134 completed (loss: 0.015020857565104961, acc: 1.0)
[2025-02-13 20:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49][root][INFO] - Training Epoch: 2/2, step 4193/7134 completed (loss: 0.054646022617816925, acc: 0.994413435459137)
[2025-02-13 20:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50][root][INFO] - Training Epoch: 2/2, step 4194/7134 completed (loss: 0.008903071284294128, acc: 1.0)
[2025-02-13 20:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50][root][INFO] - Training Epoch: 2/2, step 4195/7134 completed (loss: 0.0868532806634903, acc: 0.9885714054107666)
[2025-02-13 20:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50][root][INFO] - Training Epoch: 2/2, step 4196/7134 completed (loss: 0.03283122554421425, acc: 0.9928057789802551)
[2025-02-13 20:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51][root][INFO] - Training Epoch: 2/2, step 4197/7134 completed (loss: 0.04793528467416763, acc: 0.989130437374115)
[2025-02-13 20:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51][root][INFO] - Training Epoch: 2/2, step 4198/7134 completed (loss: 0.07882744073867798, acc: 0.9942857027053833)
[2025-02-13 20:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51][root][INFO] - Training Epoch: 2/2, step 4199/7134 completed (loss: 0.09310448169708252, acc: 0.9647887349128723)
[2025-02-13 20:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52][root][INFO] - Training Epoch: 2/2, step 4200/7134 completed (loss: 0.03299842029809952, acc: 1.0)
[2025-02-13 20:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52][root][INFO] - Training Epoch: 2/2, step 4201/7134 completed (loss: 0.025918077677488327, acc: 0.9929078221321106)
[2025-02-13 20:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52][root][INFO] - Training Epoch: 2/2, step 4202/7134 completed (loss: 0.06989115476608276, acc: 0.9924812316894531)
[2025-02-13 20:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53][root][INFO] - Training Epoch: 2/2, step 4203/7134 completed (loss: 0.054407380521297455, acc: 0.9873417615890503)
[2025-02-13 20:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53][root][INFO] - Training Epoch: 2/2, step 4204/7134 completed (loss: 0.10568936914205551, acc: 0.977142870426178)
[2025-02-13 20:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54][root][INFO] - Training Epoch: 2/2, step 4205/7134 completed (loss: 0.13550330698490143, acc: 0.9647058844566345)
[2025-02-13 20:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54][root][INFO] - Training Epoch: 2/2, step 4206/7134 completed (loss: 0.12863785028457642, acc: 0.9784946441650391)
[2025-02-13 20:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54][root][INFO] - Training Epoch: 2/2, step 4207/7134 completed (loss: 0.06233656778931618, acc: 0.9874213933944702)
[2025-02-13 20:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55][root][INFO] - Training Epoch: 2/2, step 4208/7134 completed (loss: 0.11531274020671844, acc: 0.977011501789093)
[2025-02-13 20:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55][root][INFO] - Training Epoch: 2/2, step 4209/7134 completed (loss: 0.08879557996988297, acc: 0.988950252532959)
[2025-02-13 20:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55][root][INFO] - Training Epoch: 2/2, step 4210/7134 completed (loss: 0.04865596443414688, acc: 0.9912280440330505)
[2025-02-13 20:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56][root][INFO] - Training Epoch: 2/2, step 4211/7134 completed (loss: 0.027953915297985077, acc: 1.0)
[2025-02-13 20:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56][root][INFO] - Training Epoch: 2/2, step 4212/7134 completed (loss: 0.028257615864276886, acc: 1.0)
[2025-02-13 20:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57][root][INFO] - Training Epoch: 2/2, step 4213/7134 completed (loss: 0.012110117822885513, acc: 1.0)
[2025-02-13 20:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57][root][INFO] - Training Epoch: 2/2, step 4214/7134 completed (loss: 0.11537965387105942, acc: 0.9766082167625427)
[2025-02-13 20:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57][root][INFO] - Training Epoch: 2/2, step 4215/7134 completed (loss: 0.07592704892158508, acc: 0.9842519760131836)
[2025-02-13 20:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58][root][INFO] - Training Epoch: 2/2, step 4216/7134 completed (loss: 0.06946088373661041, acc: 0.9923076629638672)
[2025-02-13 20:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58][root][INFO] - Training Epoch: 2/2, step 4217/7134 completed (loss: 0.022664954885840416, acc: 0.9944444298744202)
[2025-02-13 20:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58][root][INFO] - Training Epoch: 2/2, step 4218/7134 completed (loss: 0.10661069303750992, acc: 0.9776119589805603)
[2025-02-13 20:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59][root][INFO] - Training Epoch: 2/2, step 4219/7134 completed (loss: 0.028477299958467484, acc: 1.0)
[2025-02-13 20:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59][root][INFO] - Training Epoch: 2/2, step 4220/7134 completed (loss: 0.18811799585819244, acc: 0.9602272510528564)
[2025-02-13 20:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59][root][INFO] - Training Epoch: 2/2, step 4221/7134 completed (loss: 0.07481749355792999, acc: 0.9797297120094299)
[2025-02-13 20:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00][root][INFO] - Training Epoch: 2/2, step 4222/7134 completed (loss: 0.1723651885986328, acc: 0.9824561476707458)
[2025-02-13 20:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00][root][INFO] - Training Epoch: 2/2, step 4223/7134 completed (loss: 0.19562658667564392, acc: 0.9447236061096191)
[2025-02-13 20:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00][root][INFO] - Training Epoch: 2/2, step 4224/7134 completed (loss: 0.1559785157442093, acc: 0.9596773982048035)
[2025-02-13 20:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01][root][INFO] - Training Epoch: 2/2, step 4225/7134 completed (loss: 0.1078716516494751, acc: 0.9743589758872986)
[2025-02-13 20:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01][root][INFO] - Training Epoch: 2/2, step 4226/7134 completed (loss: 0.17184053361415863, acc: 0.9644669890403748)
[2025-02-13 20:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02][root][INFO] - Training Epoch: 2/2, step 4227/7134 completed (loss: 0.1867346465587616, acc: 0.9528301954269409)
[2025-02-13 20:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02][root][INFO] - Training Epoch: 2/2, step 4228/7134 completed (loss: 0.24172939360141754, acc: 0.9487179517745972)
[2025-02-13 20:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02][root][INFO] - Training Epoch: 2/2, step 4229/7134 completed (loss: 0.2007681429386139, acc: 0.9520000219345093)
[2025-02-13 20:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03][root][INFO] - Training Epoch: 2/2, step 4230/7134 completed (loss: 0.2396266907453537, acc: 0.9418604373931885)
[2025-02-13 20:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03][root][INFO] - Training Epoch: 2/2, step 4231/7134 completed (loss: 0.37097886204719543, acc: 0.9463087320327759)
[2025-02-13 20:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03][root][INFO] - Training Epoch: 2/2, step 4232/7134 completed (loss: 0.19472439587116241, acc: 0.9444444179534912)
[2025-02-13 20:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04][root][INFO] - Training Epoch: 2/2, step 4233/7134 completed (loss: 0.124937042593956, acc: 0.978723406791687)
[2025-02-13 20:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04][root][INFO] - Training Epoch: 2/2, step 4234/7134 completed (loss: 0.08625472337007523, acc: 0.978723406791687)
[2025-02-13 20:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04][root][INFO] - Training Epoch: 2/2, step 4235/7134 completed (loss: 0.058647915720939636, acc: 0.9855072498321533)
[2025-02-13 20:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05][root][INFO] - Training Epoch: 2/2, step 4236/7134 completed (loss: 0.016965875402092934, acc: 1.0)
[2025-02-13 20:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05][root][INFO] - Training Epoch: 2/2, step 4237/7134 completed (loss: 0.057455480098724365, acc: 1.0)
[2025-02-13 20:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06][root][INFO] - Training Epoch: 2/2, step 4238/7134 completed (loss: 0.12812019884586334, acc: 0.9692307710647583)
[2025-02-13 20:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06][root][INFO] - Training Epoch: 2/2, step 4239/7134 completed (loss: 0.035871293395757675, acc: 0.9919999837875366)
[2025-02-13 20:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06][root][INFO] - Training Epoch: 2/2, step 4240/7134 completed (loss: 0.16729333996772766, acc: 0.9647887349128723)
[2025-02-13 20:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07][root][INFO] - Training Epoch: 2/2, step 4241/7134 completed (loss: 0.025154704228043556, acc: 1.0)
[2025-02-13 20:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07][root][INFO] - Training Epoch: 2/2, step 4242/7134 completed (loss: 0.06741601973772049, acc: 0.9897959232330322)
[2025-02-13 20:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07][root][INFO] - Training Epoch: 2/2, step 4243/7134 completed (loss: 0.09462859481573105, acc: 0.9752066135406494)
[2025-02-13 20:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08][root][INFO] - Training Epoch: 2/2, step 4244/7134 completed (loss: 0.02391473948955536, acc: 1.0)
[2025-02-13 20:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08][root][INFO] - Training Epoch: 2/2, step 4245/7134 completed (loss: 0.03447885438799858, acc: 1.0)
[2025-02-13 20:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08][root][INFO] - Training Epoch: 2/2, step 4246/7134 completed (loss: 0.07075731456279755, acc: 0.9931507110595703)
[2025-02-13 20:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09][root][INFO] - Training Epoch: 2/2, step 4247/7134 completed (loss: 0.025212902575731277, acc: 1.0)
[2025-02-13 20:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09][root][INFO] - Training Epoch: 2/2, step 4248/7134 completed (loss: 0.1633480042219162, acc: 0.9726027250289917)
[2025-02-13 20:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10][root][INFO] - Training Epoch: 2/2, step 4249/7134 completed (loss: 0.1372375786304474, acc: 0.9779411554336548)
[2025-02-13 20:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10][root][INFO] - Training Epoch: 2/2, step 4250/7134 completed (loss: 0.2531399726867676, acc: 0.9545454382896423)
[2025-02-13 20:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10][root][INFO] - Training Epoch: 2/2, step 4251/7134 completed (loss: 0.06397388875484467, acc: 0.9849624037742615)
[2025-02-13 20:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11][root][INFO] - Training Epoch: 2/2, step 4252/7134 completed (loss: 0.06393471360206604, acc: 0.9838709831237793)
[2025-02-13 20:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11][root][INFO] - Training Epoch: 2/2, step 4253/7134 completed (loss: 0.08044421672821045, acc: 0.9739130139350891)
[2025-02-13 20:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11][root][INFO] - Training Epoch: 2/2, step 4254/7134 completed (loss: 0.01714889146387577, acc: 1.0)
[2025-02-13 20:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:12][root][INFO] - Training Epoch: 2/2, step 4255/7134 completed (loss: 0.09862953424453735, acc: 0.9692307710647583)
[2025-02-13 20:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:12][root][INFO] - Training Epoch: 2/2, step 4256/7134 completed (loss: 0.06993374228477478, acc: 0.9743589758872986)
[2025-02-13 20:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13][root][INFO] - Training Epoch: 2/2, step 4257/7134 completed (loss: 0.022540580481290817, acc: 1.0)
[2025-02-13 20:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13][root][INFO] - Training Epoch: 2/2, step 4258/7134 completed (loss: 0.015141426585614681, acc: 1.0)
[2025-02-13 20:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13][root][INFO] - Training Epoch: 2/2, step 4259/7134 completed (loss: 0.1485351324081421, acc: 0.9750000238418579)
[2025-02-13 20:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14][root][INFO] - Training Epoch: 2/2, step 4260/7134 completed (loss: 0.051154766231775284, acc: 0.9908257126808167)
[2025-02-13 20:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14][root][INFO] - Training Epoch: 2/2, step 4261/7134 completed (loss: 0.03523235395550728, acc: 0.991304337978363)
[2025-02-13 20:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14][root][INFO] - Training Epoch: 2/2, step 4262/7134 completed (loss: 0.05229966342449188, acc: 0.9930070042610168)
[2025-02-13 20:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15][root][INFO] - Training Epoch: 2/2, step 4263/7134 completed (loss: 0.0968519002199173, acc: 0.9777777791023254)
[2025-02-13 20:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15][root][INFO] - Training Epoch: 2/2, step 4264/7134 completed (loss: 0.12891560792922974, acc: 0.9775280952453613)
[2025-02-13 20:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16][root][INFO] - Training Epoch: 2/2, step 4265/7134 completed (loss: 0.11154269427061081, acc: 0.9709302186965942)
[2025-02-13 20:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16][root][INFO] - Training Epoch: 2/2, step 4266/7134 completed (loss: 0.05252508074045181, acc: 0.9842932224273682)
[2025-02-13 20:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16][root][INFO] - Training Epoch: 2/2, step 4267/7134 completed (loss: 0.05262274667620659, acc: 0.9797979593276978)
[2025-02-13 20:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17][root][INFO] - Training Epoch: 2/2, step 4268/7134 completed (loss: 0.07220765203237534, acc: 0.9918699264526367)
[2025-02-13 20:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17][root][INFO] - Training Epoch: 2/2, step 4269/7134 completed (loss: 0.17071449756622314, acc: 0.9537572264671326)
[2025-02-13 20:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17][root][INFO] - Training Epoch: 2/2, step 4270/7134 completed (loss: 0.056877896189689636, acc: 0.9895833134651184)
[2025-02-13 20:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18][root][INFO] - Training Epoch: 2/2, step 4271/7134 completed (loss: 0.04892995208501816, acc: 0.9945945739746094)
[2025-02-13 20:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18][root][INFO] - Training Epoch: 2/2, step 4272/7134 completed (loss: 0.0934443473815918, acc: 0.9800994992256165)
[2025-02-13 20:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19][root][INFO] - Training Epoch: 2/2, step 4273/7134 completed (loss: 0.035436853766441345, acc: 0.9894179701805115)
[2025-02-13 20:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19][root][INFO] - Training Epoch: 2/2, step 4274/7134 completed (loss: 0.14937710762023926, acc: 0.9754902124404907)
[2025-02-13 20:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19][root][INFO] - Training Epoch: 2/2, step 4275/7134 completed (loss: 0.06550589948892593, acc: 0.9832402467727661)
[2025-02-13 20:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20][root][INFO] - Training Epoch: 2/2, step 4276/7134 completed (loss: 0.08490017056465149, acc: 0.9891892075538635)
[2025-02-13 20:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20][root][INFO] - Training Epoch: 2/2, step 4277/7134 completed (loss: 0.022789742797613144, acc: 1.0)
[2025-02-13 20:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20][root][INFO] - Training Epoch: 2/2, step 4278/7134 completed (loss: 0.09818607568740845, acc: 0.9795918464660645)
[2025-02-13 20:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21][root][INFO] - Training Epoch: 2/2, step 4279/7134 completed (loss: 0.046316854655742645, acc: 0.9937888383865356)
[2025-02-13 20:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21][root][INFO] - Training Epoch: 2/2, step 4280/7134 completed (loss: 0.009406620636582375, acc: 1.0)
[2025-02-13 20:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21][root][INFO] - Training Epoch: 2/2, step 4281/7134 completed (loss: 0.07810370624065399, acc: 0.9892473220825195)
[2025-02-13 20:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22][root][INFO] - Training Epoch: 2/2, step 4282/7134 completed (loss: 0.04403379186987877, acc: 0.9741379022598267)
[2025-02-13 20:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22][root][INFO] - Training Epoch: 2/2, step 4283/7134 completed (loss: 0.03888025879859924, acc: 0.9897959232330322)
[2025-02-13 20:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23][root][INFO] - Training Epoch: 2/2, step 4284/7134 completed (loss: 0.07712511718273163, acc: 0.9844961166381836)
[2025-02-13 20:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23][root][INFO] - Training Epoch: 2/2, step 4285/7134 completed (loss: 0.018807103857398033, acc: 1.0)
[2025-02-13 20:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23][root][INFO] - Training Epoch: 2/2, step 4286/7134 completed (loss: 0.043808676302433014, acc: 0.9831932783126831)
[2025-02-13 20:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24][root][INFO] - Training Epoch: 2/2, step 4287/7134 completed (loss: 0.04664428159594536, acc: 0.989130437374115)
[2025-02-13 20:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24][root][INFO] - Training Epoch: 2/2, step 4288/7134 completed (loss: 0.047079574316740036, acc: 0.9882352948188782)
[2025-02-13 20:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25][root][INFO] - Training Epoch: 2/2, step 4289/7134 completed (loss: 0.10836587101221085, acc: 0.9754601120948792)
[2025-02-13 20:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25][root][INFO] - Training Epoch: 2/2, step 4290/7134 completed (loss: 0.05798499286174774, acc: 0.9826589822769165)
[2025-02-13 20:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25][root][INFO] - Training Epoch: 2/2, step 4291/7134 completed (loss: 0.02611229382455349, acc: 0.9942196607589722)
[2025-02-13 20:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26][root][INFO] - Training Epoch: 2/2, step 4292/7134 completed (loss: 0.06823213398456573, acc: 0.9935483932495117)
[2025-02-13 20:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26][root][INFO] - Training Epoch: 2/2, step 4293/7134 completed (loss: 0.04213116317987442, acc: 0.9916666746139526)
[2025-02-13 20:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26][root][INFO] - Training Epoch: 2/2, step 4294/7134 completed (loss: 0.09110832214355469, acc: 0.977142870426178)
[2025-02-13 20:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27][root][INFO] - Training Epoch: 2/2, step 4295/7134 completed (loss: 0.0842699259519577, acc: 0.9714285731315613)
[2025-02-13 20:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27][root][INFO] - Training Epoch: 2/2, step 4296/7134 completed (loss: 0.02517366223037243, acc: 0.9900990128517151)
[2025-02-13 20:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28][root][INFO] - Training Epoch: 2/2, step 4297/7134 completed (loss: 0.005855708848685026, acc: 1.0)
[2025-02-13 20:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28][root][INFO] - Training Epoch: 2/2, step 4298/7134 completed (loss: 0.11639959365129471, acc: 0.9659090638160706)
[2025-02-13 20:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28][root][INFO] - Training Epoch: 2/2, step 4299/7134 completed (loss: 0.10285748541355133, acc: 0.9731183052062988)
[2025-02-13 20:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29][root][INFO] - Training Epoch: 2/2, step 4300/7134 completed (loss: 0.13636203110218048, acc: 0.9831932783126831)
[2025-02-13 20:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29][root][INFO] - Training Epoch: 2/2, step 4301/7134 completed (loss: 0.035569701343774796, acc: 0.9929577708244324)
[2025-02-13 20:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29][root][INFO] - Training Epoch: 2/2, step 4302/7134 completed (loss: 0.02668563835322857, acc: 1.0)
[2025-02-13 20:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:30][root][INFO] - Training Epoch: 2/2, step 4303/7134 completed (loss: 0.09637027978897095, acc: 0.9653465151786804)
[2025-02-13 20:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:30][root][INFO] - Training Epoch: 2/2, step 4304/7134 completed (loss: 0.028569040820002556, acc: 1.0)
[2025-02-13 20:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31][root][INFO] - Training Epoch: 2/2, step 4305/7134 completed (loss: 0.06536263227462769, acc: 0.9850000143051147)
[2025-02-13 20:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31][root][INFO] - Training Epoch: 2/2, step 4306/7134 completed (loss: 0.1252700835466385, acc: 0.9738219976425171)
[2025-02-13 20:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31][root][INFO] - Training Epoch: 2/2, step 4307/7134 completed (loss: 0.13395434617996216, acc: 0.9673202633857727)
[2025-02-13 20:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32][root][INFO] - Training Epoch: 2/2, step 4308/7134 completed (loss: 0.07034869492053986, acc: 0.9893048405647278)
[2025-02-13 20:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32][root][INFO] - Training Epoch: 2/2, step 4309/7134 completed (loss: 0.1155344694852829, acc: 0.9715909361839294)
[2025-02-13 20:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32][root][INFO] - Training Epoch: 2/2, step 4310/7134 completed (loss: 0.061654482036828995, acc: 0.9847715497016907)
[2025-02-13 20:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33][root][INFO] - Training Epoch: 2/2, step 4311/7134 completed (loss: 0.04906098172068596, acc: 0.9929078221321106)
[2025-02-13 20:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33][root][INFO] - Training Epoch: 2/2, step 4312/7134 completed (loss: 0.07655283808708191, acc: 0.9815950989723206)
[2025-02-13 20:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34][root][INFO] - Training Epoch: 2/2, step 4313/7134 completed (loss: 0.020013706758618355, acc: 1.0)
[2025-02-13 20:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34][root][INFO] - Training Epoch: 2/2, step 4314/7134 completed (loss: 0.05926566570997238, acc: 0.9803921580314636)
[2025-02-13 20:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34][root][INFO] - Training Epoch: 2/2, step 4315/7134 completed (loss: 0.03463629260659218, acc: 0.9940476417541504)
[2025-02-13 20:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:35][root][INFO] - Training Epoch: 2/2, step 4316/7134 completed (loss: 0.02626081369817257, acc: 1.0)
[2025-02-13 20:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:35][root][INFO] - Training Epoch: 2/2, step 4317/7134 completed (loss: 0.048855360597372055, acc: 0.9767441749572754)
[2025-02-13 20:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:35][root][INFO] - Training Epoch: 2/2, step 4318/7134 completed (loss: 0.08606678992509842, acc: 0.9846938848495483)
[2025-02-13 20:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36][root][INFO] - Training Epoch: 2/2, step 4319/7134 completed (loss: 0.04535127803683281, acc: 0.9852216839790344)
[2025-02-13 20:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36][root][INFO] - Training Epoch: 2/2, step 4320/7134 completed (loss: 0.035809412598609924, acc: 0.9935897588729858)
[2025-02-13 20:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37][root][INFO] - Training Epoch: 2/2, step 4321/7134 completed (loss: 0.1300407499074936, acc: 0.9801980257034302)
[2025-02-13 20:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37][root][INFO] - Training Epoch: 2/2, step 4322/7134 completed (loss: 0.10849025100469589, acc: 0.9748427867889404)
[2025-02-13 20:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37][root][INFO] - Training Epoch: 2/2, step 4323/7134 completed (loss: 0.03541824594140053, acc: 0.9924812316894531)
[2025-02-13 20:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38][root][INFO] - Training Epoch: 2/2, step 4324/7134 completed (loss: 0.04439229145646095, acc: 0.991304337978363)
[2025-02-13 20:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38][root][INFO] - Training Epoch: 2/2, step 4325/7134 completed (loss: 0.13218426704406738, acc: 0.9726027250289917)
[2025-02-13 20:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38][root][INFO] - Training Epoch: 2/2, step 4326/7134 completed (loss: 0.13428688049316406, acc: 0.9621211886405945)
[2025-02-13 20:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39][root][INFO] - Training Epoch: 2/2, step 4327/7134 completed (loss: 0.052160389721393585, acc: 0.985401451587677)
[2025-02-13 20:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39][root][INFO] - Training Epoch: 2/2, step 4328/7134 completed (loss: 0.03271172568202019, acc: 0.9921875)
[2025-02-13 20:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40][root][INFO] - Training Epoch: 2/2, step 4329/7134 completed (loss: 0.07429394125938416, acc: 0.988304078578949)
[2025-02-13 20:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40][root][INFO] - Training Epoch: 2/2, step 4330/7134 completed (loss: 0.03989250212907791, acc: 0.9909909963607788)
[2025-02-13 20:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40][root][INFO] - Training Epoch: 2/2, step 4331/7134 completed (loss: 0.07468878477811813, acc: 0.9753086566925049)
[2025-02-13 20:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41][root][INFO] - Training Epoch: 2/2, step 4332/7134 completed (loss: 0.07407713681459427, acc: 0.9841269850730896)
[2025-02-13 20:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41][root][INFO] - Training Epoch: 2/2, step 4333/7134 completed (loss: 0.0555863194167614, acc: 0.987500011920929)
[2025-02-13 20:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41][root][INFO] - Training Epoch: 2/2, step 4334/7134 completed (loss: 0.11051904410123825, acc: 0.9815950989723206)
[2025-02-13 20:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42][root][INFO] - Training Epoch: 2/2, step 4335/7134 completed (loss: 0.08790574222803116, acc: 0.9834710955619812)
[2025-02-13 20:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42][root][INFO] - Training Epoch: 2/2, step 4336/7134 completed (loss: 0.01804148219525814, acc: 0.9928057789802551)
[2025-02-13 20:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43][root][INFO] - Training Epoch: 2/2, step 4337/7134 completed (loss: 0.05681143328547478, acc: 0.9790209531784058)
[2025-02-13 20:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43][root][INFO] - Training Epoch: 2/2, step 4338/7134 completed (loss: 0.045759085565805435, acc: 0.9878787994384766)
[2025-02-13 20:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43][root][INFO] - Training Epoch: 2/2, step 4339/7134 completed (loss: 0.04302426055073738, acc: 0.9882352948188782)
[2025-02-13 20:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44][root][INFO] - Training Epoch: 2/2, step 4340/7134 completed (loss: 0.051816847175359726, acc: 0.9793103337287903)
[2025-02-13 20:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44][root][INFO] - Training Epoch: 2/2, step 4341/7134 completed (loss: 0.07025329023599625, acc: 0.9863945841789246)
[2025-02-13 20:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44][root][INFO] - Training Epoch: 2/2, step 4342/7134 completed (loss: 0.11455221474170685, acc: 0.9668874144554138)
[2025-02-13 20:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:45][root][INFO] - Training Epoch: 2/2, step 4343/7134 completed (loss: 0.05838314816355705, acc: 0.9777777791023254)
[2025-02-13 20:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:45][root][INFO] - Training Epoch: 2/2, step 4344/7134 completed (loss: 0.039201341569423676, acc: 0.991304337978363)
[2025-02-13 20:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46][root][INFO] - Training Epoch: 2/2, step 4345/7134 completed (loss: 0.04714592918753624, acc: 0.9909909963607788)
[2025-02-13 20:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46][root][INFO] - Training Epoch: 2/2, step 4346/7134 completed (loss: 0.0467926561832428, acc: 0.9819819927215576)
[2025-02-13 20:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46][root][INFO] - Training Epoch: 2/2, step 4347/7134 completed (loss: 0.1055721566081047, acc: 0.9704142212867737)
[2025-02-13 20:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47][root][INFO] - Training Epoch: 2/2, step 4348/7134 completed (loss: 0.055082645267248154, acc: 0.9795918464660645)
[2025-02-13 20:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47][root][INFO] - Training Epoch: 2/2, step 4349/7134 completed (loss: 0.10868796706199646, acc: 0.96875)
[2025-02-13 20:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47][root][INFO] - Training Epoch: 2/2, step 4350/7134 completed (loss: 0.10482900589704514, acc: 0.9775280952453613)
[2025-02-13 20:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48][root][INFO] - Training Epoch: 2/2, step 4351/7134 completed (loss: 0.051033101975917816, acc: 1.0)
[2025-02-13 20:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48][root][INFO] - Training Epoch: 2/2, step 4352/7134 completed (loss: 0.09363465756177902, acc: 0.9751552939414978)
[2025-02-13 20:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48][root][INFO] - Training Epoch: 2/2, step 4353/7134 completed (loss: 0.05843764543533325, acc: 0.9820359349250793)
[2025-02-13 20:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:49][root][INFO] - Training Epoch: 2/2, step 4354/7134 completed (loss: 0.041381821036338806, acc: 0.9938650131225586)
[2025-02-13 20:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:49][root][INFO] - Training Epoch: 2/2, step 4355/7134 completed (loss: 0.09974808990955353, acc: 0.9751552939414978)
[2025-02-13 20:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50][root][INFO] - Training Epoch: 2/2, step 4356/7134 completed (loss: 0.030225668102502823, acc: 1.0)
[2025-02-13 20:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50][root][INFO] - Training Epoch: 2/2, step 4357/7134 completed (loss: 0.17590977251529694, acc: 0.9731543660163879)
[2025-02-13 20:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50][root][INFO] - Training Epoch: 2/2, step 4358/7134 completed (loss: 0.10052094608545303, acc: 0.9817073345184326)
[2025-02-13 20:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:51][root][INFO] - Training Epoch: 2/2, step 4359/7134 completed (loss: 0.08738817274570465, acc: 0.9682539701461792)
[2025-02-13 20:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:51][root][INFO] - Training Epoch: 2/2, step 4360/7134 completed (loss: 0.04446558654308319, acc: 0.993630588054657)
[2025-02-13 20:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52][root][INFO] - Training Epoch: 2/2, step 4361/7134 completed (loss: 0.06230118125677109, acc: 0.9735099077224731)
[2025-02-13 20:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52][root][INFO] - Training Epoch: 2/2, step 4362/7134 completed (loss: 0.04351291432976723, acc: 0.9945651888847351)
[2025-02-13 20:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52][root][INFO] - Training Epoch: 2/2, step 4363/7134 completed (loss: 0.07246359437704086, acc: 0.9772727489471436)
[2025-02-13 20:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53][root][INFO] - Training Epoch: 2/2, step 4364/7134 completed (loss: 0.043974559754133224, acc: 0.9922480583190918)
[2025-02-13 20:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53][root][INFO] - Training Epoch: 2/2, step 4365/7134 completed (loss: 0.04762798175215721, acc: 0.9856114983558655)
[2025-02-13 20:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53][root][INFO] - Training Epoch: 2/2, step 4366/7134 completed (loss: 0.07152307033538818, acc: 0.9767441749572754)
[2025-02-13 20:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54][root][INFO] - Training Epoch: 2/2, step 4367/7134 completed (loss: 0.04352908954024315, acc: 1.0)
[2025-02-13 20:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54][root][INFO] - Training Epoch: 2/2, step 4368/7134 completed (loss: 0.06265008449554443, acc: 0.9820359349250793)
[2025-02-13 20:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54][root][INFO] - Training Epoch: 2/2, step 4369/7134 completed (loss: 0.04519375041127205, acc: 0.9918699264526367)
[2025-02-13 20:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55][root][INFO] - Training Epoch: 2/2, step 4370/7134 completed (loss: 0.13768716156482697, acc: 0.9883720874786377)
[2025-02-13 20:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55][root][INFO] - Training Epoch: 2/2, step 4371/7134 completed (loss: 0.06758255511522293, acc: 0.9878048896789551)
[2025-02-13 20:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56][root][INFO] - Training Epoch: 2/2, step 4372/7134 completed (loss: 0.06294232606887817, acc: 0.9836065769195557)
[2025-02-13 20:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56][root][INFO] - Training Epoch: 2/2, step 4373/7134 completed (loss: 0.07258854061365128, acc: 0.9834254384040833)
[2025-02-13 20:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56][root][INFO] - Training Epoch: 2/2, step 4374/7134 completed (loss: 0.08884937316179276, acc: 0.9695122241973877)
[2025-02-13 20:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57][root][INFO] - Training Epoch: 2/2, step 4375/7134 completed (loss: 0.06826051324605942, acc: 0.976190447807312)
[2025-02-13 20:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57][root][INFO] - Training Epoch: 2/2, step 4376/7134 completed (loss: 0.05342048406600952, acc: 0.9895833134651184)
[2025-02-13 20:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57][root][INFO] - Training Epoch: 2/2, step 4377/7134 completed (loss: 0.1833627074956894, acc: 0.970588207244873)
[2025-02-13 20:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58][root][INFO] - Training Epoch: 2/2, step 4378/7134 completed (loss: 0.11126916855573654, acc: 0.9774436354637146)
[2025-02-13 20:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58][root][INFO] - Training Epoch: 2/2, step 4379/7134 completed (loss: 0.2329150289297104, acc: 0.9485294222831726)
[2025-02-13 20:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59][root][INFO] - Training Epoch: 2/2, step 4380/7134 completed (loss: 0.08857458084821701, acc: 0.9650349617004395)
[2025-02-13 20:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59][root][INFO] - Training Epoch: 2/2, step 4381/7134 completed (loss: 0.041363365948200226, acc: 0.9940119981765747)
[2025-02-13 20:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59][root][INFO] - Training Epoch: 2/2, step 4382/7134 completed (loss: 0.0937112346291542, acc: 0.9806451797485352)
[2025-02-13 20:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00][root][INFO] - Training Epoch: 2/2, step 4383/7134 completed (loss: 0.13694944977760315, acc: 0.9593908786773682)
[2025-02-13 20:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00][root][INFO] - Training Epoch: 2/2, step 4384/7134 completed (loss: 0.032911647111177444, acc: 0.9943181872367859)
[2025-02-13 20:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00][root][INFO] - Training Epoch: 2/2, step 4385/7134 completed (loss: 0.11206857115030289, acc: 0.9740932583808899)
[2025-02-13 20:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01][root][INFO] - Training Epoch: 2/2, step 4386/7134 completed (loss: 0.08515891432762146, acc: 0.9805825352668762)
[2025-02-13 20:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01][root][INFO] - Training Epoch: 2/2, step 4387/7134 completed (loss: 0.13862866163253784, acc: 0.9588235020637512)
[2025-02-13 20:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01][root][INFO] - Training Epoch: 2/2, step 4388/7134 completed (loss: 0.16566644608974457, acc: 0.9675324559211731)
[2025-02-13 20:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02][root][INFO] - Training Epoch: 2/2, step 4389/7134 completed (loss: 0.14364896714687347, acc: 0.97826087474823)
[2025-02-13 20:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02][root][INFO] - Training Epoch: 2/2, step 4390/7134 completed (loss: 0.15075035393238068, acc: 0.9754601120948792)
[2025-02-13 20:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03][root][INFO] - Training Epoch: 2/2, step 4391/7134 completed (loss: 0.13705849647521973, acc: 0.9726027250289917)
[2025-02-13 20:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03][root][INFO] - Training Epoch: 2/2, step 4392/7134 completed (loss: 0.03411383926868439, acc: 0.9947090148925781)
[2025-02-13 20:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03][root][INFO] - Training Epoch: 2/2, step 4393/7134 completed (loss: 0.06618525832891464, acc: 0.9866666793823242)
[2025-02-13 20:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04][root][INFO] - Training Epoch: 2/2, step 4394/7134 completed (loss: 0.0507638193666935, acc: 0.9932885766029358)
[2025-02-13 20:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04][root][INFO] - Training Epoch: 2/2, step 4395/7134 completed (loss: 0.12104830890893936, acc: 0.9875776171684265)
[2025-02-13 20:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04][root][INFO] - Training Epoch: 2/2, step 4396/7134 completed (loss: 0.2907572090625763, acc: 0.9153439402580261)
[2025-02-13 20:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:05][root][INFO] - Training Epoch: 2/2, step 4397/7134 completed (loss: 0.42911094427108765, acc: 0.9099099040031433)
[2025-02-13 20:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:05][root][INFO] - Training Epoch: 2/2, step 4398/7134 completed (loss: 0.09765442460775375, acc: 0.9666666388511658)
[2025-02-13 20:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06][root][INFO] - Training Epoch: 2/2, step 4399/7134 completed (loss: 0.15682917833328247, acc: 0.9791666865348816)
[2025-02-13 20:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06][root][INFO] - Training Epoch: 2/2, step 4400/7134 completed (loss: 0.06299247592687607, acc: 0.9894737005233765)
[2025-02-13 20:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06][root][INFO] - Training Epoch: 2/2, step 4401/7134 completed (loss: 0.11433223634958267, acc: 0.9612902998924255)
[2025-02-13 20:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07][root][INFO] - Training Epoch: 2/2, step 4402/7134 completed (loss: 0.2106415331363678, acc: 0.9506173133850098)
[2025-02-13 20:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07][root][INFO] - Training Epoch: 2/2, step 4403/7134 completed (loss: 0.2782942056655884, acc: 0.9318181872367859)
[2025-02-13 20:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07][root][INFO] - Training Epoch: 2/2, step 4404/7134 completed (loss: 0.2481604665517807, acc: 0.9536082744598389)
[2025-02-13 20:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08][root][INFO] - Training Epoch: 2/2, step 4405/7134 completed (loss: 0.1406828910112381, acc: 0.9653465151786804)
[2025-02-13 20:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08][root][INFO] - Training Epoch: 2/2, step 4406/7134 completed (loss: 0.05898720771074295, acc: 0.9853658676147461)
[2025-02-13 20:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09][root][INFO] - Training Epoch: 2/2, step 4407/7134 completed (loss: 0.14754296839237213, acc: 0.9693877696990967)
[2025-02-13 20:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09][root][INFO] - Training Epoch: 2/2, step 4408/7134 completed (loss: 0.1659490466117859, acc: 0.9615384340286255)
[2025-02-13 20:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09][root][INFO] - Training Epoch: 2/2, step 4409/7134 completed (loss: 0.10912901163101196, acc: 0.9635416865348816)
[2025-02-13 20:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10][root][INFO] - Training Epoch: 2/2, step 4410/7134 completed (loss: 0.21795989573001862, acc: 0.9440559148788452)
[2025-02-13 20:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10][root][INFO] - Training Epoch: 2/2, step 4411/7134 completed (loss: 0.22401635348796844, acc: 0.9569892287254333)
[2025-02-13 20:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10][root][INFO] - Training Epoch: 2/2, step 4412/7134 completed (loss: 0.5456967353820801, acc: 0.8592965006828308)
[2025-02-13 20:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11][root][INFO] - Training Epoch: 2/2, step 4413/7134 completed (loss: 0.2919781804084778, acc: 0.9274611473083496)
[2025-02-13 20:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11][root][INFO] - Training Epoch: 2/2, step 4414/7134 completed (loss: 0.0874362364411354, acc: 0.9731183052062988)
[2025-02-13 20:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12][root][INFO] - Training Epoch: 2/2, step 4415/7134 completed (loss: 0.12043449282646179, acc: 0.9606741666793823)
[2025-02-13 20:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12][root][INFO] - Training Epoch: 2/2, step 4416/7134 completed (loss: 0.1332131177186966, acc: 0.9585492014884949)
[2025-02-13 20:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12][root][INFO] - Training Epoch: 2/2, step 4417/7134 completed (loss: 0.2225763350725174, acc: 0.9494949579238892)
[2025-02-13 20:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13][root][INFO] - Training Epoch: 2/2, step 4418/7134 completed (loss: 0.07392291724681854, acc: 0.9945651888847351)
[2025-02-13 20:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13][root][INFO] - Training Epoch: 2/2, step 4419/7134 completed (loss: 0.22066271305084229, acc: 0.9463414549827576)
[2025-02-13 20:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13][root][INFO] - Training Epoch: 2/2, step 4420/7134 completed (loss: 0.2310706377029419, acc: 0.9481865167617798)
[2025-02-13 20:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14][root][INFO] - Training Epoch: 2/2, step 4421/7134 completed (loss: 0.1583152413368225, acc: 0.9541284441947937)
[2025-02-13 20:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14][root][INFO] - Training Epoch: 2/2, step 4422/7134 completed (loss: 0.13911861181259155, acc: 0.946107804775238)
[2025-02-13 20:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15][root][INFO] - Training Epoch: 2/2, step 4423/7134 completed (loss: 0.13477368652820587, acc: 0.9598214030265808)
[2025-02-13 20:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15][root][INFO] - Training Epoch: 2/2, step 4424/7134 completed (loss: 0.09022720158100128, acc: 0.9829545617103577)
[2025-02-13 20:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15][root][INFO] - Training Epoch: 2/2, step 4425/7134 completed (loss: 0.05466796085238457, acc: 0.9801980257034302)
[2025-02-13 20:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16][root][INFO] - Training Epoch: 2/2, step 4426/7134 completed (loss: 0.14673683047294617, acc: 0.9679999947547913)
[2025-02-13 20:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16][root][INFO] - Training Epoch: 2/2, step 4427/7134 completed (loss: 0.24549788236618042, acc: 0.9741379022598267)
[2025-02-13 20:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16][root][INFO] - Training Epoch: 2/2, step 4428/7134 completed (loss: 0.10742906481027603, acc: 0.9768785834312439)
[2025-02-13 20:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17][root][INFO] - Training Epoch: 2/2, step 4429/7134 completed (loss: 0.10759959369897842, acc: 0.9714285731315613)
[2025-02-13 20:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17][root][INFO] - Training Epoch: 2/2, step 4430/7134 completed (loss: 0.10880834609270096, acc: 0.949438214302063)
[2025-02-13 20:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18][root][INFO] - Training Epoch: 2/2, step 4431/7134 completed (loss: 0.14625369012355804, acc: 0.9709302186965942)
[2025-02-13 20:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18][root][INFO] - Training Epoch: 2/2, step 4432/7134 completed (loss: 0.10431249439716339, acc: 0.9808917045593262)
[2025-02-13 20:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18][root][INFO] - Training Epoch: 2/2, step 4433/7134 completed (loss: 0.0794324204325676, acc: 0.976190447807312)
[2025-02-13 20:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19][root][INFO] - Training Epoch: 2/2, step 4434/7134 completed (loss: 0.12954677641391754, acc: 0.9714285731315613)
[2025-02-13 20:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19][root][INFO] - Training Epoch: 2/2, step 4435/7134 completed (loss: 0.0756305605173111, acc: 0.9861111044883728)
[2025-02-13 20:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19][root][INFO] - Training Epoch: 2/2, step 4436/7134 completed (loss: 0.07500969618558884, acc: 0.976331353187561)
[2025-02-13 20:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:20][root][INFO] - Training Epoch: 2/2, step 4437/7134 completed (loss: 0.028859378769993782, acc: 0.994535505771637)
[2025-02-13 20:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:20][root][INFO] - Training Epoch: 2/2, step 4438/7134 completed (loss: 0.17144536972045898, acc: 0.9850746393203735)
[2025-02-13 20:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21][root][INFO] - Training Epoch: 2/2, step 4439/7134 completed (loss: 0.023614535108208656, acc: 0.9929078221321106)
[2025-02-13 20:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21][root][INFO] - Training Epoch: 2/2, step 4440/7134 completed (loss: 0.05585518106818199, acc: 0.9879518151283264)
[2025-02-13 20:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21][root][INFO] - Training Epoch: 2/2, step 4441/7134 completed (loss: 0.06030058115720749, acc: 0.9874213933944702)
[2025-02-13 20:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22][root][INFO] - Training Epoch: 2/2, step 4442/7134 completed (loss: 0.12025514990091324, acc: 0.9719101190567017)
[2025-02-13 20:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22][root][INFO] - Training Epoch: 2/2, step 4443/7134 completed (loss: 0.14249353110790253, acc: 0.9647058844566345)
[2025-02-13 20:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22][root][INFO] - Training Epoch: 2/2, step 4444/7134 completed (loss: 0.061409804970026016, acc: 0.9927007555961609)
[2025-02-13 20:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23][root][INFO] - Training Epoch: 2/2, step 4445/7134 completed (loss: 0.10067874193191528, acc: 0.9704142212867737)
[2025-02-13 20:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23][root][INFO] - Training Epoch: 2/2, step 4446/7134 completed (loss: 0.03653879836201668, acc: 1.0)
[2025-02-13 20:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24][root][INFO] - Training Epoch: 2/2, step 4447/7134 completed (loss: 0.06125589460134506, acc: 0.9885057210922241)
[2025-02-13 20:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24][root][INFO] - Training Epoch: 2/2, step 4448/7134 completed (loss: 0.17059317231178284, acc: 0.9689922332763672)
[2025-02-13 20:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24][root][INFO] - Training Epoch: 2/2, step 4449/7134 completed (loss: 0.1462547481060028, acc: 0.9576271176338196)
[2025-02-13 20:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25][root][INFO] - Training Epoch: 2/2, step 4450/7134 completed (loss: 0.1105273962020874, acc: 0.9680851101875305)
[2025-02-13 20:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25][root][INFO] - Training Epoch: 2/2, step 4451/7134 completed (loss: 0.25872743129730225, acc: 0.9490445852279663)
[2025-02-13 20:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25][root][INFO] - Training Epoch: 2/2, step 4452/7134 completed (loss: 0.08932391554117203, acc: 0.970370352268219)
[2025-02-13 20:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26][root][INFO] - Training Epoch: 2/2, step 4453/7134 completed (loss: 0.045281946659088135, acc: 0.9869281053543091)
[2025-02-13 20:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26][root][INFO] - Training Epoch: 2/2, step 4454/7134 completed (loss: 0.17751552164554596, acc: 0.9751552939414978)
[2025-02-13 20:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26][root][INFO] - Training Epoch: 2/2, step 4455/7134 completed (loss: 0.2277563065290451, acc: 0.9722222089767456)
[2025-02-13 20:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:27][root][INFO] - Training Epoch: 2/2, step 4456/7134 completed (loss: 0.03808106109499931, acc: 0.9818181991577148)
[2025-02-13 20:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:27][root][INFO] - Training Epoch: 2/2, step 4457/7134 completed (loss: 0.02030343748629093, acc: 1.0)
[2025-02-13 20:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28][root][INFO] - Training Epoch: 2/2, step 4458/7134 completed (loss: 0.016262777149677277, acc: 0.9930070042610168)
[2025-02-13 20:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28][root][INFO] - Training Epoch: 2/2, step 4459/7134 completed (loss: 0.020909393206238747, acc: 1.0)
[2025-02-13 20:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28][root][INFO] - Training Epoch: 2/2, step 4460/7134 completed (loss: 0.03633309528231621, acc: 0.9901477694511414)
[2025-02-13 20:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29][root][INFO] - Training Epoch: 2/2, step 4461/7134 completed (loss: 0.03122956119477749, acc: 0.9895833134651184)
[2025-02-13 20:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29][root][INFO] - Training Epoch: 2/2, step 4462/7134 completed (loss: 0.04045115038752556, acc: 0.9885714054107666)
[2025-02-13 20:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29][root][INFO] - Training Epoch: 2/2, step 4463/7134 completed (loss: 0.03679239749908447, acc: 0.9894179701805115)
[2025-02-13 20:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30][root][INFO] - Training Epoch: 2/2, step 4464/7134 completed (loss: 0.07351798564195633, acc: 0.9931507110595703)
[2025-02-13 20:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30][root][INFO] - Training Epoch: 2/2, step 4465/7134 completed (loss: 0.07090926170349121, acc: 0.9898989796638489)
[2025-02-13 20:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31][root][INFO] - Training Epoch: 2/2, step 4466/7134 completed (loss: 0.043481893837451935, acc: 0.9898989796638489)
[2025-02-13 20:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31][root][INFO] - Training Epoch: 2/2, step 4467/7134 completed (loss: 0.0385950431227684, acc: 0.9893048405647278)
[2025-02-13 20:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31][root][INFO] - Training Epoch: 2/2, step 4468/7134 completed (loss: 0.026164166629314423, acc: 0.9893048405647278)
[2025-02-13 20:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32][root][INFO] - Training Epoch: 2/2, step 4469/7134 completed (loss: 0.010558301582932472, acc: 1.0)
[2025-02-13 20:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32][root][INFO] - Training Epoch: 2/2, step 4470/7134 completed (loss: 0.04894425347447395, acc: 0.9855769276618958)
[2025-02-13 20:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32][root][INFO] - Training Epoch: 2/2, step 4471/7134 completed (loss: 0.019555067643523216, acc: 0.9949495196342468)
[2025-02-13 20:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33][root][INFO] - Training Epoch: 2/2, step 4472/7134 completed (loss: 0.024249065667390823, acc: 1.0)
[2025-02-13 20:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33][root][INFO] - Training Epoch: 2/2, step 4473/7134 completed (loss: 0.0927039384841919, acc: 0.976331353187561)
[2025-02-13 20:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33][root][INFO] - Training Epoch: 2/2, step 4474/7134 completed (loss: 0.11844243854284286, acc: 0.9751552939414978)
[2025-02-13 20:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34][root][INFO] - Training Epoch: 2/2, step 4475/7134 completed (loss: 0.20027682185173035, acc: 0.9521276354789734)
[2025-02-13 20:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34][root][INFO] - Training Epoch: 2/2, step 4476/7134 completed (loss: 0.10702545195817947, acc: 0.9725274443626404)
[2025-02-13 20:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35][root][INFO] - Training Epoch: 2/2, step 4477/7134 completed (loss: 0.0290824044495821, acc: 0.9897959232330322)
[2025-02-13 20:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35][root][INFO] - Training Epoch: 2/2, step 4478/7134 completed (loss: 0.06643371284008026, acc: 0.9842932224273682)
[2025-02-13 20:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35][root][INFO] - Training Epoch: 2/2, step 4479/7134 completed (loss: 0.029815662652254105, acc: 0.9944444298744202)
[2025-02-13 20:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36][root][INFO] - Training Epoch: 2/2, step 4480/7134 completed (loss: 0.08142482489347458, acc: 0.9752475023269653)
[2025-02-13 20:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36][root][INFO] - Training Epoch: 2/2, step 4481/7134 completed (loss: 0.05777052789926529, acc: 0.9900000095367432)
[2025-02-13 20:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37][root][INFO] - Training Epoch: 2/2, step 4482/7134 completed (loss: 0.08603180199861526, acc: 0.9851484894752502)
[2025-02-13 20:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37][root][INFO] - Training Epoch: 2/2, step 4483/7134 completed (loss: 0.10337293148040771, acc: 0.95652174949646)
[2025-02-13 20:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37][root][INFO] - Training Epoch: 2/2, step 4484/7134 completed (loss: 0.032802075147628784, acc: 0.9886363744735718)
[2025-02-13 20:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38][root][INFO] - Training Epoch: 2/2, step 4485/7134 completed (loss: 0.02561352215707302, acc: 0.9934640526771545)
[2025-02-13 20:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38][root][INFO] - Training Epoch: 2/2, step 4486/7134 completed (loss: 0.029460405930876732, acc: 0.9875776171684265)
[2025-02-13 20:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38][root][INFO] - Training Epoch: 2/2, step 4487/7134 completed (loss: 0.05318234860897064, acc: 0.987730085849762)
[2025-02-13 20:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39][root][INFO] - Training Epoch: 2/2, step 4488/7134 completed (loss: 0.04616670310497284, acc: 0.9922480583190918)
[2025-02-13 20:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39][root][INFO] - Training Epoch: 2/2, step 4489/7134 completed (loss: 0.04712677747011185, acc: 0.9862068891525269)
[2025-02-13 20:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39][root][INFO] - Training Epoch: 2/2, step 4490/7134 completed (loss: 0.041850000619888306, acc: 0.9919999837875366)
[2025-02-13 20:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40][root][INFO] - Training Epoch: 2/2, step 4491/7134 completed (loss: 0.04143881797790527, acc: 0.9847328066825867)
[2025-02-13 20:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40][root][INFO] - Training Epoch: 2/2, step 4492/7134 completed (loss: 0.04334735497832298, acc: 0.9924812316894531)
[2025-02-13 20:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41][root][INFO] - Training Epoch: 2/2, step 4493/7134 completed (loss: 0.043008267879486084, acc: 0.9941860437393188)
[2025-02-13 20:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41][root][INFO] - Training Epoch: 2/2, step 4494/7134 completed (loss: 0.04208683595061302, acc: 0.9887640476226807)
[2025-02-13 20:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41][root][INFO] - Training Epoch: 2/2, step 4495/7134 completed (loss: 0.02101931907236576, acc: 1.0)
[2025-02-13 20:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42][root][INFO] - Training Epoch: 2/2, step 4496/7134 completed (loss: 0.055256061255931854, acc: 0.988304078578949)
[2025-02-13 20:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42][root][INFO] - Training Epoch: 2/2, step 4497/7134 completed (loss: 0.020448878407478333, acc: 0.9947090148925781)
[2025-02-13 20:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42][root][INFO] - Training Epoch: 2/2, step 4498/7134 completed (loss: 0.012013142928481102, acc: 1.0)
[2025-02-13 20:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43][root][INFO] - Training Epoch: 2/2, step 4499/7134 completed (loss: 0.02700822800397873, acc: 0.9929078221321106)
[2025-02-13 20:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43][root][INFO] - Training Epoch: 2/2, step 4500/7134 completed (loss: 0.029231080785393715, acc: 0.9839572310447693)
[2025-02-13 20:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43][root][INFO] - Training Epoch: 2/2, step 4501/7134 completed (loss: 0.021053096279501915, acc: 0.988950252532959)
[2025-02-13 20:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44][root][INFO] - Training Epoch: 2/2, step 4502/7134 completed (loss: 0.014228826388716698, acc: 1.0)
[2025-02-13 20:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44][root][INFO] - Training Epoch: 2/2, step 4503/7134 completed (loss: 0.04624496027827263, acc: 0.9937106966972351)
[2025-02-13 20:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45][root][INFO] - Training Epoch: 2/2, step 4504/7134 completed (loss: 0.056297942996025085, acc: 1.0)
[2025-02-13 20:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45][root][INFO] - Training Epoch: 2/2, step 4505/7134 completed (loss: 0.08165215700864792, acc: 0.9729729890823364)
[2025-02-13 20:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45][root][INFO] - Training Epoch: 2/2, step 4506/7134 completed (loss: 0.1016717329621315, acc: 0.9570552110671997)
[2025-02-13 20:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46][root][INFO] - Training Epoch: 2/2, step 4507/7134 completed (loss: 0.019806325435638428, acc: 1.0)
[2025-02-13 20:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46][root][INFO] - Training Epoch: 2/2, step 4508/7134 completed (loss: 0.09766155481338501, acc: 0.9627329111099243)
[2025-02-13 20:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46][root][INFO] - Training Epoch: 2/2, step 4509/7134 completed (loss: 0.052981872111558914, acc: 0.9878787994384766)
[2025-02-13 20:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47][root][INFO] - Training Epoch: 2/2, step 4510/7134 completed (loss: 0.11051753908395767, acc: 0.976190447807312)
[2025-02-13 20:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47][root][INFO] - Training Epoch: 2/2, step 4511/7134 completed (loss: 0.06152402237057686, acc: 0.9886363744735718)
[2025-02-13 20:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48][root][INFO] - Training Epoch: 2/2, step 4512/7134 completed (loss: 0.09081219881772995, acc: 0.9777777791023254)
[2025-02-13 20:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48][root][INFO] - Training Epoch: 2/2, step 4513/7134 completed (loss: 0.09003590047359467, acc: 0.9824561476707458)
[2025-02-13 20:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48][root][INFO] - Training Epoch: 2/2, step 4514/7134 completed (loss: 0.310482919216156, acc: 0.9479768872261047)
[2025-02-13 20:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49][root][INFO] - Training Epoch: 2/2, step 4515/7134 completed (loss: 0.062484730035066605, acc: 0.9868420958518982)
[2025-02-13 20:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49][root][INFO] - Training Epoch: 2/2, step 4516/7134 completed (loss: 0.10455048829317093, acc: 0.9679144620895386)
[2025-02-13 20:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49][root][INFO] - Training Epoch: 2/2, step 4517/7134 completed (loss: 0.06404908746480942, acc: 0.9750000238418579)
[2025-02-13 20:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50][root][INFO] - Training Epoch: 2/2, step 4518/7134 completed (loss: 0.08846055716276169, acc: 0.9793814420700073)
[2025-02-13 20:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50][root][INFO] - Training Epoch: 2/2, step 4519/7134 completed (loss: 0.030393190681934357, acc: 1.0)
[2025-02-13 20:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50][root][INFO] - Training Epoch: 2/2, step 4520/7134 completed (loss: 0.04666024446487427, acc: 0.9893048405647278)
[2025-02-13 20:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51][root][INFO] - Training Epoch: 2/2, step 4521/7134 completed (loss: 0.10851075500249863, acc: 0.9850000143051147)
[2025-02-13 20:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51][root][INFO] - Training Epoch: 2/2, step 4522/7134 completed (loss: 0.10662325471639633, acc: 0.971563994884491)
[2025-02-13 20:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52][root][INFO] - Training Epoch: 2/2, step 4523/7134 completed (loss: 0.04356594383716583, acc: 0.9765258431434631)
[2025-02-13 20:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52][root][INFO] - Training Epoch: 2/2, step 4524/7134 completed (loss: 0.0703258365392685, acc: 0.9842932224273682)
[2025-02-13 20:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52][root][INFO] - Training Epoch: 2/2, step 4525/7134 completed (loss: 0.10199929028749466, acc: 0.9704433679580688)
[2025-02-13 20:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53][root][INFO] - Training Epoch: 2/2, step 4526/7134 completed (loss: 0.11405697464942932, acc: 0.9725274443626404)
[2025-02-13 20:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53][root][INFO] - Training Epoch: 2/2, step 4527/7134 completed (loss: 0.09487109631299973, acc: 0.9820627570152283)
[2025-02-13 20:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53][root][INFO] - Training Epoch: 2/2, step 4528/7134 completed (loss: 0.07456564158201218, acc: 0.9901477694511414)
[2025-02-13 20:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54][root][INFO] - Training Epoch: 2/2, step 4529/7134 completed (loss: 0.10972629487514496, acc: 0.9685863852500916)
[2025-02-13 20:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54][root][INFO] - Training Epoch: 2/2, step 4530/7134 completed (loss: 0.032742612063884735, acc: 0.9939024448394775)
[2025-02-13 20:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54][root][INFO] - Training Epoch: 2/2, step 4531/7134 completed (loss: 0.01128895953297615, acc: 1.0)
[2025-02-13 20:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55][root][INFO] - Training Epoch: 2/2, step 4532/7134 completed (loss: 0.08645501732826233, acc: 0.9831223487854004)
[2025-02-13 20:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55][root][INFO] - Training Epoch: 2/2, step 4533/7134 completed (loss: 0.022635813802480698, acc: 0.9953703880310059)
[2025-02-13 20:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56][root][INFO] - Training Epoch: 2/2, step 4534/7134 completed (loss: 0.04984346777200699, acc: 0.9796954393386841)
[2025-02-13 20:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56][root][INFO] - Training Epoch: 2/2, step 4535/7134 completed (loss: 0.04077664762735367, acc: 0.9903846383094788)
[2025-02-13 20:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56][root][INFO] - Training Epoch: 2/2, step 4536/7134 completed (loss: 0.036459267139434814, acc: 0.9879518151283264)
[2025-02-13 20:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57][root][INFO] - Training Epoch: 2/2, step 4537/7134 completed (loss: 0.029355430975556374, acc: 0.9909502267837524)
[2025-02-13 20:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57][root][INFO] - Training Epoch: 2/2, step 4538/7134 completed (loss: 0.05603174492716789, acc: 0.9951691031455994)
[2025-02-13 20:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57][root][INFO] - Training Epoch: 2/2, step 4539/7134 completed (loss: 0.04837484657764435, acc: 0.9906103014945984)
[2025-02-13 20:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58][root][INFO] - Training Epoch: 2/2, step 4540/7134 completed (loss: 0.15419334173202515, acc: 0.9642857313156128)
[2025-02-13 20:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58][root][INFO] - Training Epoch: 2/2, step 4541/7134 completed (loss: 0.117411307990551, acc: 0.9672130942344666)
[2025-02-13 20:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58][root][INFO] - Training Epoch: 2/2, step 4542/7134 completed (loss: 0.12484748661518097, acc: 0.9561403393745422)
[2025-02-13 20:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59][root][INFO] - Training Epoch: 2/2, step 4543/7134 completed (loss: 0.247598797082901, acc: 0.9285714030265808)
[2025-02-13 20:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59][root][INFO] - Training Epoch: 2/2, step 4544/7134 completed (loss: 0.08116801083087921, acc: 0.9797297120094299)
[2025-02-13 20:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00][root][INFO] - Training Epoch: 2/2, step 4545/7134 completed (loss: 0.047833915799856186, acc: 0.9814814925193787)
[2025-02-13 20:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00][root][INFO] - Training Epoch: 2/2, step 4546/7134 completed (loss: 0.09095863252878189, acc: 0.9763779640197754)
[2025-02-13 20:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00][root][INFO] - Training Epoch: 2/2, step 4547/7134 completed (loss: 0.057827915996313095, acc: 0.9830508232116699)
[2025-02-13 20:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01][root][INFO] - Training Epoch: 2/2, step 4548/7134 completed (loss: 0.12009383738040924, acc: 0.9763779640197754)
[2025-02-13 20:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01][root][INFO] - Training Epoch: 2/2, step 4549/7134 completed (loss: 0.12342758476734161, acc: 0.9736841917037964)
[2025-02-13 20:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01][root][INFO] - Training Epoch: 2/2, step 4550/7134 completed (loss: 0.12105884402990341, acc: 0.969924807548523)
[2025-02-13 20:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:02][root][INFO] - Training Epoch: 2/2, step 4551/7134 completed (loss: 0.15860876441001892, acc: 0.9729729890823364)
[2025-02-13 20:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:02][root][INFO] - Training Epoch: 2/2, step 4552/7134 completed (loss: 0.04080259054899216, acc: 0.9824561476707458)
[2025-02-13 20:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03][root][INFO] - Training Epoch: 2/2, step 4553/7134 completed (loss: 0.01902262307703495, acc: 1.0)
[2025-02-13 20:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03][root][INFO] - Training Epoch: 2/2, step 4554/7134 completed (loss: 0.08668438345193863, acc: 0.9781022071838379)
[2025-02-13 20:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03][root][INFO] - Training Epoch: 2/2, step 4555/7134 completed (loss: 0.0781613141298294, acc: 1.0)
[2025-02-13 20:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04][root][INFO] - Training Epoch: 2/2, step 4556/7134 completed (loss: 0.21073180437088013, acc: 0.9354838728904724)
[2025-02-13 20:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04][root][INFO] - Training Epoch: 2/2, step 4557/7134 completed (loss: 0.02818177454173565, acc: 1.0)
[2025-02-13 20:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04][root][INFO] - Training Epoch: 2/2, step 4558/7134 completed (loss: 0.16554158926010132, acc: 0.961240291595459)
[2025-02-13 20:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05][root][INFO] - Training Epoch: 2/2, step 4559/7134 completed (loss: 0.10760747641324997, acc: 0.9621211886405945)
[2025-02-13 20:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05][root][INFO] - Training Epoch: 2/2, step 4560/7134 completed (loss: 0.08041241019964218, acc: 0.975806474685669)
[2025-02-13 20:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05][root][INFO] - Training Epoch: 2/2, step 4561/7134 completed (loss: 0.053025808185338974, acc: 0.9918032884597778)
[2025-02-13 20:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06][root][INFO] - Training Epoch: 2/2, step 4562/7134 completed (loss: 0.08965206891298294, acc: 0.9918699264526367)
[2025-02-13 20:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06][root][INFO] - Training Epoch: 2/2, step 4563/7134 completed (loss: 0.045057814568281174, acc: 0.9929078221321106)
[2025-02-13 20:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07][root][INFO] - Training Epoch: 2/2, step 4564/7134 completed (loss: 0.18224893510341644, acc: 0.948051929473877)
[2025-02-13 20:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07][root][INFO] - Training Epoch: 2/2, step 4565/7134 completed (loss: 0.05692121386528015, acc: 0.991150438785553)
[2025-02-13 20:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07][root][INFO] - Training Epoch: 2/2, step 4566/7134 completed (loss: 0.0751100555062294, acc: 0.9774436354637146)
[2025-02-13 20:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08][root][INFO] - Training Epoch: 2/2, step 4567/7134 completed (loss: 0.041585877537727356, acc: 0.9909909963607788)
[2025-02-13 20:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08][root][INFO] - Training Epoch: 2/2, step 4568/7134 completed (loss: 0.1141209676861763, acc: 0.9578947424888611)
[2025-02-13 20:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08][root][INFO] - Training Epoch: 2/2, step 4569/7134 completed (loss: 0.22239701449871063, acc: 0.982300877571106)
[2025-02-13 20:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09][root][INFO] - Training Epoch: 2/2, step 4570/7134 completed (loss: 0.05455901846289635, acc: 0.9805194735527039)
[2025-02-13 20:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09][root][INFO] - Training Epoch: 2/2, step 4571/7134 completed (loss: 0.07696554064750671, acc: 0.977011501789093)
[2025-02-13 20:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10][root][INFO] - Training Epoch: 2/2, step 4572/7134 completed (loss: 0.07111265510320663, acc: 0.9868420958518982)
[2025-02-13 20:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10][root][INFO] - Training Epoch: 2/2, step 4573/7134 completed (loss: 0.06632055342197418, acc: 0.9807692170143127)
[2025-02-13 20:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10][root][INFO] - Training Epoch: 2/2, step 4574/7134 completed (loss: 0.029693013057112694, acc: 1.0)
[2025-02-13 20:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11][root][INFO] - Training Epoch: 2/2, step 4575/7134 completed (loss: 0.08648640662431717, acc: 0.9723756909370422)
[2025-02-13 20:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11][root][INFO] - Training Epoch: 2/2, step 4576/7134 completed (loss: 0.027818448841571808, acc: 0.9942857027053833)
[2025-02-13 20:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12][root][INFO] - Training Epoch: 2/2, step 4577/7134 completed (loss: 0.10427788645029068, acc: 0.9651162624359131)
[2025-02-13 20:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12][root][INFO] - Training Epoch: 2/2, step 4578/7134 completed (loss: 0.047356605529785156, acc: 0.9878787994384766)
[2025-02-13 20:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12][root][INFO] - Training Epoch: 2/2, step 4579/7134 completed (loss: 0.12289348989725113, acc: 0.9746835231781006)
[2025-02-13 20:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13][root][INFO] - Training Epoch: 2/2, step 4580/7134 completed (loss: 0.15259188413619995, acc: 0.9746835231781006)
[2025-02-13 20:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13][root][INFO] - Training Epoch: 2/2, step 4581/7134 completed (loss: 0.02623048610985279, acc: 0.9930070042610168)
[2025-02-13 20:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13][root][INFO] - Training Epoch: 2/2, step 4582/7134 completed (loss: 0.05345381423830986, acc: 0.9861111044883728)
[2025-02-13 20:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:14][root][INFO] - Training Epoch: 2/2, step 4583/7134 completed (loss: 0.07147568464279175, acc: 0.9800000190734863)
[2025-02-13 20:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:14][root][INFO] - Training Epoch: 2/2, step 4584/7134 completed (loss: 0.054257649928331375, acc: 0.9862068891525269)
[2025-02-13 20:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15][root][INFO] - Training Epoch: 2/2, step 4585/7134 completed (loss: 0.1066286638379097, acc: 0.9743589758872986)
[2025-02-13 20:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15][root][INFO] - Training Epoch: 2/2, step 4586/7134 completed (loss: 0.04386717453598976, acc: 0.9881656765937805)
[2025-02-13 20:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15][root][INFO] - Training Epoch: 2/2, step 4587/7134 completed (loss: 0.21076828241348267, acc: 0.9825581312179565)
[2025-02-13 20:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16][root][INFO] - Training Epoch: 2/2, step 4588/7134 completed (loss: 0.10262817144393921, acc: 0.9750000238418579)
[2025-02-13 20:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16][root][INFO] - Training Epoch: 2/2, step 4589/7134 completed (loss: 0.07790663093328476, acc: 0.9712643623352051)
[2025-02-13 20:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16][root][INFO] - Training Epoch: 2/2, step 4590/7134 completed (loss: 0.11294518411159515, acc: 0.9642857313156128)
[2025-02-13 20:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17][root][INFO] - Training Epoch: 2/2, step 4591/7134 completed (loss: 0.1248156726360321, acc: 0.9545454382896423)
[2025-02-13 20:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17][root][INFO] - Training Epoch: 2/2, step 4592/7134 completed (loss: 0.08660997450351715, acc: 0.9756097793579102)
[2025-02-13 20:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17][root][INFO] - Training Epoch: 2/2, step 4593/7134 completed (loss: 0.12185446918010712, acc: 0.9407894611358643)
[2025-02-13 20:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18][root][INFO] - Training Epoch: 2/2, step 4594/7134 completed (loss: 0.1345585733652115, acc: 0.9576719403266907)
[2025-02-13 20:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18][root][INFO] - Training Epoch: 2/2, step 4595/7134 completed (loss: 0.08724461495876312, acc: 0.9873417615890503)
[2025-02-13 20:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19][root][INFO] - Training Epoch: 2/2, step 4596/7134 completed (loss: 0.05497089773416519, acc: 0.987500011920929)
[2025-02-13 20:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19][root][INFO] - Training Epoch: 2/2, step 4597/7134 completed (loss: 0.07233772426843643, acc: 0.9743589758872986)
[2025-02-13 20:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19][root][INFO] - Training Epoch: 2/2, step 4598/7134 completed (loss: 0.15846963226795197, acc: 0.9567567706108093)
[2025-02-13 20:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20][root][INFO] - Training Epoch: 2/2, step 4599/7134 completed (loss: 0.01779560185968876, acc: 1.0)
[2025-02-13 20:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20][root][INFO] - Training Epoch: 2/2, step 4600/7134 completed (loss: 0.05795516446232796, acc: 0.9874213933944702)
[2025-02-13 20:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20][root][INFO] - Training Epoch: 2/2, step 4601/7134 completed (loss: 0.029770350083708763, acc: 0.994350254535675)
[2025-02-13 20:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21][root][INFO] - Training Epoch: 2/2, step 4602/7134 completed (loss: 0.0881633311510086, acc: 0.9666666388511658)
[2025-02-13 20:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21][root][INFO] - Training Epoch: 2/2, step 4603/7134 completed (loss: 0.05419767647981644, acc: 0.9825581312179565)
[2025-02-13 20:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22][root][INFO] - Training Epoch: 2/2, step 4604/7134 completed (loss: 0.03849771246314049, acc: 1.0)
[2025-02-13 20:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22][root][INFO] - Training Epoch: 2/2, step 4605/7134 completed (loss: 0.09247591346502304, acc: 0.9636363387107849)
[2025-02-13 20:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22][root][INFO] - Training Epoch: 2/2, step 4606/7134 completed (loss: 0.03725326061248779, acc: 0.9874213933944702)
[2025-02-13 20:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23][root][INFO] - Training Epoch: 2/2, step 4607/7134 completed (loss: 0.03786073252558708, acc: 0.9929078221321106)
[2025-02-13 20:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23][root][INFO] - Training Epoch: 2/2, step 4608/7134 completed (loss: 0.07266777753829956, acc: 0.988304078578949)
[2025-02-13 20:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23][root][INFO] - Training Epoch: 2/2, step 4609/7134 completed (loss: 0.027981378138065338, acc: 0.9923664331436157)
[2025-02-13 20:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24][root][INFO] - Training Epoch: 2/2, step 4610/7134 completed (loss: 0.03172151371836662, acc: 0.9888268113136292)
[2025-02-13 20:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24][root][INFO] - Training Epoch: 2/2, step 4611/7134 completed (loss: 0.04006132483482361, acc: 0.9931034445762634)
[2025-02-13 20:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25][root][INFO] - Training Epoch: 2/2, step 4612/7134 completed (loss: 0.07863101363182068, acc: 0.9851852059364319)
[2025-02-13 20:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25][root][INFO] - Training Epoch: 2/2, step 4613/7134 completed (loss: 0.05822065845131874, acc: 0.9938271641731262)
[2025-02-13 20:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25][root][INFO] - Training Epoch: 2/2, step 4614/7134 completed (loss: 0.03540915623307228, acc: 0.9923076629638672)
[2025-02-13 20:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26][root][INFO] - Training Epoch: 2/2, step 4615/7134 completed (loss: 0.08584380894899368, acc: 0.9880239367485046)
[2025-02-13 20:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26][root][INFO] - Training Epoch: 2/2, step 4616/7134 completed (loss: 0.09089135378599167, acc: 0.9767441749572754)
[2025-02-13 20:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26][root][INFO] - Training Epoch: 2/2, step 4617/7134 completed (loss: 0.12144551426172256, acc: 0.9820359349250793)
[2025-02-13 20:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27][root][INFO] - Training Epoch: 2/2, step 4618/7134 completed (loss: 0.08229187875986099, acc: 0.9689922332763672)
[2025-02-13 20:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27][root][INFO] - Training Epoch: 2/2, step 4619/7134 completed (loss: 0.11917964369058609, acc: 0.981249988079071)
[2025-02-13 20:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27][root][INFO] - Training Epoch: 2/2, step 4620/7134 completed (loss: 0.02680938132107258, acc: 0.9941176176071167)
[2025-02-13 20:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28][root][INFO] - Training Epoch: 2/2, step 4621/7134 completed (loss: 0.031413640826940536, acc: 0.9939024448394775)
[2025-02-13 20:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28][root][INFO] - Training Epoch: 2/2, step 4622/7134 completed (loss: 0.04112989827990532, acc: 0.9888888597488403)
[2025-02-13 20:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28][root][INFO] - Training Epoch: 2/2, step 4623/7134 completed (loss: 0.07851184904575348, acc: 0.9882352948188782)
[2025-02-13 20:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29][root][INFO] - Training Epoch: 2/2, step 4624/7134 completed (loss: 0.16201597452163696, acc: 0.970588207244873)
[2025-02-13 20:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29][root][INFO] - Training Epoch: 2/2, step 4625/7134 completed (loss: 0.15232035517692566, acc: 0.9509202241897583)
[2025-02-13 20:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30][root][INFO] - Training Epoch: 2/2, step 4626/7134 completed (loss: 0.1873091608285904, acc: 0.9239130616188049)
[2025-02-13 20:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30][root][INFO] - Training Epoch: 2/2, step 4627/7134 completed (loss: 0.11023634672164917, acc: 0.9677419066429138)
[2025-02-13 20:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30][root][INFO] - Training Epoch: 2/2, step 4628/7134 completed (loss: 0.05579259991645813, acc: 0.9878048896789551)
[2025-02-13 20:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31][root][INFO] - Training Epoch: 2/2, step 4629/7134 completed (loss: 0.0736149474978447, acc: 0.9935064911842346)
[2025-02-13 20:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31][root][INFO] - Training Epoch: 2/2, step 4630/7134 completed (loss: 0.14046157896518707, acc: 0.9777777791023254)
[2025-02-13 20:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31][root][INFO] - Training Epoch: 2/2, step 4631/7134 completed (loss: 0.06684231758117676, acc: 0.9906976819038391)
[2025-02-13 20:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32][root][INFO] - Training Epoch: 2/2, step 4632/7134 completed (loss: 0.1204657182097435, acc: 0.9819276928901672)
[2025-02-13 20:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32][root][INFO] - Training Epoch: 2/2, step 4633/7134 completed (loss: 0.20433716475963593, acc: 0.9452054500579834)
[2025-02-13 20:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33][root][INFO] - Training Epoch: 2/2, step 4634/7134 completed (loss: 0.08822046965360641, acc: 0.9777777791023254)
[2025-02-13 20:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33][root][INFO] - Training Epoch: 2/2, step 4635/7134 completed (loss: 0.1306651085615158, acc: 0.9817073345184326)
[2025-02-13 20:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33][root][INFO] - Training Epoch: 2/2, step 4636/7134 completed (loss: 0.028463102877140045, acc: 0.9893617033958435)
[2025-02-13 20:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34][root][INFO] - Training Epoch: 2/2, step 4637/7134 completed (loss: 0.0867641344666481, acc: 0.9841269850730896)
[2025-02-13 20:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34][root][INFO] - Training Epoch: 2/2, step 4638/7134 completed (loss: 0.10387147217988968, acc: 0.9768785834312439)
[2025-02-13 20:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34][root][INFO] - Training Epoch: 2/2, step 4639/7134 completed (loss: 0.07508581131696701, acc: 0.9729729890823364)
[2025-02-13 20:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35][root][INFO] - Training Epoch: 2/2, step 4640/7134 completed (loss: 0.09207111597061157, acc: 0.9682539701461792)
[2025-02-13 20:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35][root][INFO] - Training Epoch: 2/2, step 4641/7134 completed (loss: 0.05954059585928917, acc: 0.9930555820465088)
[2025-02-13 20:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36][root][INFO] - Training Epoch: 2/2, step 4642/7134 completed (loss: 0.02675078809261322, acc: 0.994350254535675)
[2025-02-13 20:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36][root][INFO] - Training Epoch: 2/2, step 4643/7134 completed (loss: 0.03929085284471512, acc: 0.9853658676147461)
[2025-02-13 20:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36][root][INFO] - Training Epoch: 2/2, step 4644/7134 completed (loss: 0.026110736653208733, acc: 1.0)
[2025-02-13 20:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37][root][INFO] - Training Epoch: 2/2, step 4645/7134 completed (loss: 0.014496461488306522, acc: 1.0)
[2025-02-13 20:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37][root][INFO] - Training Epoch: 2/2, step 4646/7134 completed (loss: 0.05532141029834747, acc: 0.9857142567634583)
[2025-02-13 20:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37][root][INFO] - Training Epoch: 2/2, step 4647/7134 completed (loss: 0.12745973467826843, acc: 0.9672130942344666)
[2025-02-13 20:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38][root][INFO] - Training Epoch: 2/2, step 4648/7134 completed (loss: 0.12725843489170074, acc: 0.9745222926139832)
[2025-02-13 20:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38][root][INFO] - Training Epoch: 2/2, step 4649/7134 completed (loss: 0.0640811175107956, acc: 0.9674796462059021)
[2025-02-13 20:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38][root][INFO] - Training Epoch: 2/2, step 4650/7134 completed (loss: 0.0875338539481163, acc: 0.9729729890823364)
[2025-02-13 20:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:39][root][INFO] - Training Epoch: 2/2, step 4651/7134 completed (loss: 0.1534510850906372, acc: 0.9553072452545166)
[2025-02-13 20:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:39][root][INFO] - Training Epoch: 2/2, step 4652/7134 completed (loss: 0.0793398916721344, acc: 0.9691358208656311)
[2025-02-13 20:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40][root][INFO] - Training Epoch: 2/2, step 4653/7134 completed (loss: 0.047842271625995636, acc: 0.990338146686554)
[2025-02-13 20:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40][root][INFO] - Training Epoch: 2/2, step 4654/7134 completed (loss: 0.20369042456150055, acc: 0.9459459185600281)
[2025-02-13 20:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40][root][INFO] - Training Epoch: 2/2, step 4655/7134 completed (loss: 0.06680949777364731, acc: 0.9828571677207947)
[2025-02-13 20:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41][root][INFO] - Training Epoch: 2/2, step 4656/7134 completed (loss: 0.06325210630893707, acc: 0.9830508232116699)
[2025-02-13 20:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41][root][INFO] - Training Epoch: 2/2, step 4657/7134 completed (loss: 0.13329440355300903, acc: 0.954023003578186)
[2025-02-13 20:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41][root][INFO] - Training Epoch: 2/2, step 4658/7134 completed (loss: 0.060948606580495834, acc: 0.9829545617103577)
[2025-02-13 20:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42][root][INFO] - Training Epoch: 2/2, step 4659/7134 completed (loss: 0.12326222658157349, acc: 0.9576719403266907)
[2025-02-13 20:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42][root][INFO] - Training Epoch: 2/2, step 4660/7134 completed (loss: 0.13724516332149506, acc: 0.9743589758872986)
[2025-02-13 20:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43][root][INFO] - Training Epoch: 2/2, step 4661/7134 completed (loss: 0.10213427245616913, acc: 0.9447852969169617)
[2025-02-13 20:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43][root][INFO] - Training Epoch: 2/2, step 4662/7134 completed (loss: 0.13150295615196228, acc: 0.978723406791687)
[2025-02-13 20:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43][root][INFO] - Training Epoch: 2/2, step 4663/7134 completed (loss: 0.20138660073280334, acc: 0.9398906826972961)
[2025-02-13 20:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44][root][INFO] - Training Epoch: 2/2, step 4664/7134 completed (loss: 0.12634459137916565, acc: 0.9729729890823364)
[2025-02-13 20:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44][root][INFO] - Training Epoch: 2/2, step 4665/7134 completed (loss: 0.05390070378780365, acc: 0.9890109896659851)
[2025-02-13 20:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44][root][INFO] - Training Epoch: 2/2, step 4666/7134 completed (loss: 0.19372448325157166, acc: 0.9585492014884949)
[2025-02-13 20:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45][root][INFO] - Training Epoch: 2/2, step 4667/7134 completed (loss: 0.15830959379673004, acc: 0.9729729890823364)
[2025-02-13 20:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45][root][INFO] - Training Epoch: 2/2, step 4668/7134 completed (loss: 0.07776346057653427, acc: 0.9865771532058716)
[2025-02-13 20:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46][root][INFO] - Training Epoch: 2/2, step 4669/7134 completed (loss: 0.08900958299636841, acc: 0.9774011373519897)
[2025-02-13 20:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46][root][INFO] - Training Epoch: 2/2, step 4670/7134 completed (loss: 0.04264675825834274, acc: 1.0)
[2025-02-13 20:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46][root][INFO] - Training Epoch: 2/2, step 4671/7134 completed (loss: 0.07986462861299515, acc: 0.9781420826911926)
[2025-02-13 20:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47][root][INFO] - Training Epoch: 2/2, step 4672/7134 completed (loss: 0.09595593810081482, acc: 0.9724137783050537)
[2025-02-13 20:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47][root][INFO] - Training Epoch: 2/2, step 4673/7134 completed (loss: 0.10310926288366318, acc: 0.9811320900917053)
[2025-02-13 20:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47][root][INFO] - Training Epoch: 2/2, step 4674/7134 completed (loss: 0.04283317178487778, acc: 0.9935064911842346)
[2025-02-13 20:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48][root][INFO] - Training Epoch: 2/2, step 4675/7134 completed (loss: 0.10916357487440109, acc: 0.9743589758872986)
[2025-02-13 20:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48][root][INFO] - Training Epoch: 2/2, step 4676/7134 completed (loss: 0.03977774828672409, acc: 0.9935064911842346)
[2025-02-13 20:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48][root][INFO] - Training Epoch: 2/2, step 4677/7134 completed (loss: 0.03763652592897415, acc: 0.9915966391563416)
[2025-02-13 20:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49][root][INFO] - Training Epoch: 2/2, step 4678/7134 completed (loss: 0.09727992862462997, acc: 0.976047933101654)
[2025-02-13 20:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49][root][INFO] - Training Epoch: 2/2, step 4679/7134 completed (loss: 0.05002712085843086, acc: 0.9942196607589722)
[2025-02-13 20:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50][root][INFO] - Training Epoch: 2/2, step 4680/7134 completed (loss: 0.11834602057933807, acc: 0.9636363387107849)
[2025-02-13 20:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50][root][INFO] - Training Epoch: 2/2, step 4681/7134 completed (loss: 0.16523465514183044, acc: 0.948051929473877)
[2025-02-13 20:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50][root][INFO] - Training Epoch: 2/2, step 4682/7134 completed (loss: 0.08799109607934952, acc: 0.9668508172035217)
[2025-02-13 20:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51][root][INFO] - Training Epoch: 2/2, step 4683/7134 completed (loss: 0.0768439993262291, acc: 0.9800000190734863)
[2025-02-13 20:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51][root][INFO] - Training Epoch: 2/2, step 4684/7134 completed (loss: 0.05738682672381401, acc: 0.9948453903198242)
[2025-02-13 20:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51][root][INFO] - Training Epoch: 2/2, step 4685/7134 completed (loss: 0.06454779952764511, acc: 0.9852216839790344)
[2025-02-13 20:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52][root][INFO] - Training Epoch: 2/2, step 4686/7134 completed (loss: 0.05771980062127113, acc: 0.9894179701805115)
[2025-02-13 20:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52][root][INFO] - Training Epoch: 2/2, step 4687/7134 completed (loss: 0.11162517219781876, acc: 0.95652174949646)
[2025-02-13 20:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52][root][INFO] - Training Epoch: 2/2, step 4688/7134 completed (loss: 0.4065698981285095, acc: 0.9140625)
[2025-02-13 20:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53][root][INFO] - Training Epoch: 2/2, step 4689/7134 completed (loss: 0.2075340896844864, acc: 0.959770143032074)
[2025-02-13 20:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53][root][INFO] - Training Epoch: 2/2, step 4690/7134 completed (loss: 0.2420956939458847, acc: 0.9473684430122375)
[2025-02-13 20:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54][root][INFO] - Training Epoch: 2/2, step 4691/7134 completed (loss: 0.05823741480708122, acc: 0.9846153855323792)
[2025-02-13 20:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54][root][INFO] - Training Epoch: 2/2, step 4692/7134 completed (loss: 0.33672037720680237, acc: 0.9365079402923584)
[2025-02-13 20:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54][root][INFO] - Training Epoch: 2/2, step 4693/7134 completed (loss: 0.2956511080265045, acc: 0.9314285516738892)
[2025-02-13 20:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55][root][INFO] - Training Epoch: 2/2, step 4694/7134 completed (loss: 0.2652031183242798, acc: 0.9603174328804016)
[2025-02-13 20:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55][root][INFO] - Training Epoch: 2/2, step 4695/7134 completed (loss: 0.23916083574295044, acc: 0.95333331823349)
[2025-02-13 20:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55][root][INFO] - Training Epoch: 2/2, step 4696/7134 completed (loss: 0.026384243741631508, acc: 1.0)
[2025-02-13 20:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:56][root][INFO] - Training Epoch: 2/2, step 4697/7134 completed (loss: 0.05296783521771431, acc: 0.9920634627342224)
[2025-02-13 20:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:56][root][INFO] - Training Epoch: 2/2, step 4698/7134 completed (loss: 0.19703678786754608, acc: 0.9571428298950195)
[2025-02-13 20:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57][root][INFO] - Training Epoch: 2/2, step 4699/7134 completed (loss: 0.14581026136875153, acc: 0.9568345546722412)
[2025-02-13 20:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57][root][INFO] - Training Epoch: 2/2, step 4700/7134 completed (loss: 0.13163022696971893, acc: 0.9467455744743347)
[2025-02-13 20:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57][root][INFO] - Training Epoch: 2/2, step 4701/7134 completed (loss: 0.22659388184547424, acc: 0.9375)
[2025-02-13 20:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58][root][INFO] - Training Epoch: 2/2, step 4702/7134 completed (loss: 0.056682899594306946, acc: 0.9806451797485352)
[2025-02-13 20:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58][root][INFO] - Training Epoch: 2/2, step 4703/7134 completed (loss: 0.05344183370471001, acc: 0.9882352948188782)
[2025-02-13 20:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58][root][INFO] - Training Epoch: 2/2, step 4704/7134 completed (loss: 0.023083413019776344, acc: 1.0)
[2025-02-13 20:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59][root][INFO] - Training Epoch: 2/2, step 4705/7134 completed (loss: 0.11931922286748886, acc: 0.9605911374092102)
[2025-02-13 20:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59][root][INFO] - Training Epoch: 2/2, step 4706/7134 completed (loss: 0.17772343754768372, acc: 0.9487179517745972)
[2025-02-13 20:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59][root][INFO] - Training Epoch: 2/2, step 4707/7134 completed (loss: 0.36184951663017273, acc: 0.9119496941566467)
[2025-02-13 20:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00][root][INFO] - Training Epoch: 2/2, step 4708/7134 completed (loss: 0.0937562957406044, acc: 0.9748743772506714)
[2025-02-13 20:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00][root][INFO] - Training Epoch: 2/2, step 4709/7134 completed (loss: 0.06200655177235603, acc: 0.9837837815284729)
[2025-02-13 20:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00][root][INFO] - Training Epoch: 2/2, step 4710/7134 completed (loss: 0.10260496288537979, acc: 0.9757575988769531)
[2025-02-13 20:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01][root][INFO] - Training Epoch: 2/2, step 4711/7134 completed (loss: 0.14552801847457886, acc: 0.9391891956329346)
[2025-02-13 20:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01][root][INFO] - Training Epoch: 2/2, step 4712/7134 completed (loss: 0.2579677700996399, acc: 0.9411764740943909)
[2025-02-13 20:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02][root][INFO] - Training Epoch: 2/2, step 4713/7134 completed (loss: 0.11339662224054337, acc: 0.9701492786407471)
[2025-02-13 20:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02][root][INFO] - Training Epoch: 2/2, step 4714/7134 completed (loss: 0.10937011986970901, acc: 0.9694656729698181)
[2025-02-13 20:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02][root][INFO] - Training Epoch: 2/2, step 4715/7134 completed (loss: 0.2942352592945099, acc: 0.9583333134651184)
[2025-02-13 20:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03][root][INFO] - Training Epoch: 2/2, step 4716/7134 completed (loss: 0.08197397738695145, acc: 0.9870967864990234)
[2025-02-13 20:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03][root][INFO] - Training Epoch: 2/2, step 4717/7134 completed (loss: 0.0527033656835556, acc: 0.9800000190734863)
[2025-02-13 20:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03][root][INFO] - Training Epoch: 2/2, step 4718/7134 completed (loss: 0.023530060425400734, acc: 0.9928057789802551)
[2025-02-13 20:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04][root][INFO] - Training Epoch: 2/2, step 4719/7134 completed (loss: 0.08953091502189636, acc: 0.976331353187561)
[2025-02-13 20:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04][root][INFO] - Training Epoch: 2/2, step 4720/7134 completed (loss: 0.013704662211239338, acc: 1.0)
[2025-02-13 20:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04][root][INFO] - Training Epoch: 2/2, step 4721/7134 completed (loss: 0.018173569813370705, acc: 0.9938271641731262)
[2025-02-13 20:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05][root][INFO] - Training Epoch: 2/2, step 4722/7134 completed (loss: 0.08100169897079468, acc: 0.9748427867889404)
[2025-02-13 20:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05][root][INFO] - Training Epoch: 2/2, step 4723/7134 completed (loss: 0.08604054152965546, acc: 0.9808917045593262)
[2025-02-13 20:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05][root][INFO] - Training Epoch: 2/2, step 4724/7134 completed (loss: 0.0744514986872673, acc: 0.9695122241973877)
[2025-02-13 20:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06][root][INFO] - Training Epoch: 2/2, step 4725/7134 completed (loss: 0.058650389313697815, acc: 0.9825581312179565)
[2025-02-13 20:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06][root][INFO] - Training Epoch: 2/2, step 4726/7134 completed (loss: 0.13637498021125793, acc: 0.9655172228813171)
[2025-02-13 20:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07][root][INFO] - Training Epoch: 2/2, step 4727/7134 completed (loss: 0.09101172536611557, acc: 0.9933775067329407)
[2025-02-13 20:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07][root][INFO] - Training Epoch: 2/2, step 4728/7134 completed (loss: 0.07685211300849915, acc: 0.9801324605941772)
[2025-02-13 20:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07][root][INFO] - Training Epoch: 2/2, step 4729/7134 completed (loss: 0.04954753816127777, acc: 0.9884393215179443)
[2025-02-13 20:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08][root][INFO] - Training Epoch: 2/2, step 4730/7134 completed (loss: 0.11085145175457001, acc: 0.9707602262496948)
[2025-02-13 20:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08][root][INFO] - Training Epoch: 2/2, step 4731/7134 completed (loss: 0.2166012078523636, acc: 0.9655172228813171)
[2025-02-13 20:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08][root][INFO] - Training Epoch: 2/2, step 4732/7134 completed (loss: 0.09399411827325821, acc: 0.9558823704719543)
[2025-02-13 20:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09][root][INFO] - Training Epoch: 2/2, step 4733/7134 completed (loss: 0.06895885616540909, acc: 0.97826087474823)
[2025-02-13 20:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09][root][INFO] - Training Epoch: 2/2, step 4734/7134 completed (loss: 0.19863304495811462, acc: 0.9496855139732361)
[2025-02-13 20:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09][root][INFO] - Training Epoch: 2/2, step 4735/7134 completed (loss: 0.06790867447853088, acc: 0.9811320900917053)
[2025-02-13 20:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:10][root][INFO] - Training Epoch: 2/2, step 4736/7134 completed (loss: 0.21006140112876892, acc: 0.9666666388511658)
[2025-02-13 20:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:10][root][INFO] - Training Epoch: 2/2, step 4737/7134 completed (loss: 0.08260580152273178, acc: 0.9793103337287903)
[2025-02-13 20:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11][root][INFO] - Training Epoch: 2/2, step 4738/7134 completed (loss: 0.053942423313856125, acc: 0.981249988079071)
[2025-02-13 20:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11][root][INFO] - Training Epoch: 2/2, step 4739/7134 completed (loss: 0.24472388625144958, acc: 0.932584285736084)
[2025-02-13 20:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11][root][INFO] - Training Epoch: 2/2, step 4740/7134 completed (loss: 0.2124156951904297, acc: 0.9444444179534912)
[2025-02-13 20:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12][root][INFO] - Training Epoch: 2/2, step 4741/7134 completed (loss: 0.09717245399951935, acc: 0.9714285731315613)
[2025-02-13 20:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12][root][INFO] - Training Epoch: 2/2, step 4742/7134 completed (loss: 0.3253594636917114, acc: 0.9272727370262146)
[2025-02-13 20:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12][root][INFO] - Training Epoch: 2/2, step 4743/7134 completed (loss: 0.2008446902036667, acc: 0.9342105388641357)
[2025-02-13 20:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13][root][INFO] - Training Epoch: 2/2, step 4744/7134 completed (loss: 0.07992155104875565, acc: 0.9724770784378052)
[2025-02-13 20:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13][root][INFO] - Training Epoch: 2/2, step 4745/7134 completed (loss: 0.026740286499261856, acc: 1.0)
[2025-02-13 20:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13][root][INFO] - Training Epoch: 2/2, step 4746/7134 completed (loss: 0.0492277555167675, acc: 0.9767441749572754)
[2025-02-13 20:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14][root][INFO] - Training Epoch: 2/2, step 4747/7134 completed (loss: 0.08476876467466354, acc: 0.9651162624359131)
[2025-02-13 20:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14][root][INFO] - Training Epoch: 2/2, step 4748/7134 completed (loss: 0.17537342011928558, acc: 0.9585798978805542)
[2025-02-13 20:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14][root][INFO] - Training Epoch: 2/2, step 4749/7134 completed (loss: 0.028568904846906662, acc: 0.9941860437393188)
[2025-02-13 20:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:15][root][INFO] - Training Epoch: 2/2, step 4750/7134 completed (loss: 0.10643857717514038, acc: 0.9756097793579102)
[2025-02-13 20:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:15][root][INFO] - Training Epoch: 2/2, step 4751/7134 completed (loss: 0.049922745674848557, acc: 0.9878048896789551)
[2025-02-13 20:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16][root][INFO] - Training Epoch: 2/2, step 4752/7134 completed (loss: 0.14052215218544006, acc: 0.9751552939414978)
[2025-02-13 20:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16][root][INFO] - Training Epoch: 2/2, step 4753/7134 completed (loss: 0.13207639753818512, acc: 0.976331353187561)
[2025-02-13 20:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16][root][INFO] - Training Epoch: 2/2, step 4754/7134 completed (loss: 0.03276878222823143, acc: 0.9935064911842346)
[2025-02-13 20:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:17][root][INFO] - Training Epoch: 2/2, step 4755/7134 completed (loss: 0.07684043049812317, acc: 0.9780219793319702)
[2025-02-13 20:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:17][root][INFO] - Training Epoch: 2/2, step 4756/7134 completed (loss: 0.03236880525946617, acc: 0.9919354915618896)
[2025-02-13 20:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:17][root][INFO] - Training Epoch: 2/2, step 4757/7134 completed (loss: 0.05554153770208359, acc: 0.9750000238418579)
[2025-02-13 20:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:18][root][INFO] - Training Epoch: 2/2, step 4758/7134 completed (loss: 0.039577145129442215, acc: 0.9944751262664795)
[2025-02-13 20:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:18][root][INFO] - Training Epoch: 2/2, step 4759/7134 completed (loss: 0.03934304416179657, acc: 0.9929078221321106)
[2025-02-13 20:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:18][root][INFO] - Training Epoch: 2/2, step 4760/7134 completed (loss: 0.05512915179133415, acc: 0.9915966391563416)
[2025-02-13 20:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:19][root][INFO] - Training Epoch: 2/2, step 4761/7134 completed (loss: 0.10557085275650024, acc: 0.9817073345184326)
[2025-02-13 20:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:19][root][INFO] - Training Epoch: 2/2, step 4762/7134 completed (loss: 0.20584458112716675, acc: 0.9670329689979553)
[2025-02-13 20:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:20][root][INFO] - Training Epoch: 2/2, step 4763/7134 completed (loss: 0.0667216032743454, acc: 0.9867549538612366)
[2025-02-13 20:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:20][root][INFO] - Training Epoch: 2/2, step 4764/7134 completed (loss: 0.10847733914852142, acc: 0.9742268323898315)
[2025-02-13 20:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:20][root][INFO] - Training Epoch: 2/2, step 4765/7134 completed (loss: 0.15163609385490417, acc: 0.9666666388511658)
[2025-02-13 20:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:21][root][INFO] - Training Epoch: 2/2, step 4766/7134 completed (loss: 0.06856106966733932, acc: 0.9833333492279053)
[2025-02-13 20:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:21][root][INFO] - Training Epoch: 2/2, step 4767/7134 completed (loss: 0.03401133045554161, acc: 0.9897959232330322)
[2025-02-13 20:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:21][root][INFO] - Training Epoch: 2/2, step 4768/7134 completed (loss: 0.09051350504159927, acc: 0.9764705896377563)
[2025-02-13 20:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:22][root][INFO] - Training Epoch: 2/2, step 4769/7134 completed (loss: 0.11027877032756805, acc: 0.9736841917037964)
[2025-02-13 20:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:22][root][INFO] - Training Epoch: 2/2, step 4770/7134 completed (loss: 0.03847045451402664, acc: 1.0)
[2025-02-13 20:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:22][root][INFO] - Training Epoch: 2/2, step 4771/7134 completed (loss: 0.17097115516662598, acc: 0.9754601120948792)
[2025-02-13 20:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:23][root][INFO] - Training Epoch: 2/2, step 4772/7134 completed (loss: 0.13548552989959717, acc: 0.9677419066429138)
[2025-02-13 20:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:23][root][INFO] - Training Epoch: 2/2, step 4773/7134 completed (loss: 0.06493229418992996, acc: 0.9807692170143127)
[2025-02-13 20:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:23][root][INFO] - Training Epoch: 2/2, step 4774/7134 completed (loss: 0.05646388977766037, acc: 0.9881656765937805)
[2025-02-13 20:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:24][root][INFO] - Training Epoch: 2/2, step 4775/7134 completed (loss: 0.1321018487215042, acc: 0.9811320900917053)
[2025-02-13 20:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:24][root][INFO] - Training Epoch: 2/2, step 4776/7134 completed (loss: 0.13547760248184204, acc: 0.9718309640884399)
[2025-02-13 20:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:25][root][INFO] - Training Epoch: 2/2, step 4777/7134 completed (loss: 0.1345939338207245, acc: 0.9726027250289917)
[2025-02-13 20:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:25][root][INFO] - Training Epoch: 2/2, step 4778/7134 completed (loss: 0.0639735609292984, acc: 0.9910714030265808)
[2025-02-13 20:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:25][root][INFO] - Training Epoch: 2/2, step 4779/7134 completed (loss: 0.05598260089755058, acc: 0.9851852059364319)
[2025-02-13 20:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:26][root][INFO] - Training Epoch: 2/2, step 4780/7134 completed (loss: 0.028227027505636215, acc: 0.9807692170143127)
[2025-02-13 20:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:26][root][INFO] - Training Epoch: 2/2, step 4781/7134 completed (loss: 0.03784654289484024, acc: 1.0)
[2025-02-13 20:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:26][root][INFO] - Training Epoch: 2/2, step 4782/7134 completed (loss: 0.040934912860393524, acc: 0.987500011920929)
[2025-02-13 20:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:27][root][INFO] - Training Epoch: 2/2, step 4783/7134 completed (loss: 0.14301657676696777, acc: 0.9492753744125366)
[2025-02-13 20:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:27][root][INFO] - Training Epoch: 2/2, step 4784/7134 completed (loss: 0.06254050135612488, acc: 0.9833333492279053)
[2025-02-13 20:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:28][root][INFO] - Training Epoch: 2/2, step 4785/7134 completed (loss: 0.09497834742069244, acc: 0.9850746393203735)
[2025-02-13 20:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:28][root][INFO] - Training Epoch: 2/2, step 4786/7134 completed (loss: 0.049203552305698395, acc: 0.9876543283462524)
[2025-02-13 20:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:28][root][INFO] - Training Epoch: 2/2, step 4787/7134 completed (loss: 0.04169890284538269, acc: 0.9937106966972351)
[2025-02-13 20:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:29][root][INFO] - Training Epoch: 2/2, step 4788/7134 completed (loss: 0.028677452355623245, acc: 0.9923076629638672)
[2025-02-13 20:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:29][root][INFO] - Training Epoch: 2/2, step 4789/7134 completed (loss: 0.04141857102513313, acc: 0.9935897588729858)
[2025-02-13 20:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:30][root][INFO] - Training Epoch: 2/2, step 4790/7134 completed (loss: 0.06744453310966492, acc: 0.983146071434021)
[2025-02-13 20:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:30][root][INFO] - Training Epoch: 2/2, step 4791/7134 completed (loss: 0.03067816235125065, acc: 0.9925373196601868)
[2025-02-13 20:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:30][root][INFO] - Training Epoch: 2/2, step 4792/7134 completed (loss: 0.031008463352918625, acc: 0.9932432174682617)
[2025-02-13 20:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:31][root][INFO] - Training Epoch: 2/2, step 4793/7134 completed (loss: 0.06176582723855972, acc: 0.9789473414421082)
[2025-02-13 20:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:31][root][INFO] - Training Epoch: 2/2, step 4794/7134 completed (loss: 0.23331905901432037, acc: 0.9432623982429504)
[2025-02-13 20:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:31][root][INFO] - Training Epoch: 2/2, step 4795/7134 completed (loss: 0.04455185681581497, acc: 0.987730085849762)
[2025-02-13 20:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:32][root][INFO] - Training Epoch: 2/2, step 4796/7134 completed (loss: 0.12033722549676895, acc: 0.9736841917037964)
[2025-02-13 20:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:32][root][INFO] - Training Epoch: 2/2, step 4797/7134 completed (loss: 0.03799816966056824, acc: 0.9890710115432739)
[2025-02-13 20:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:33][root][INFO] - Training Epoch: 2/2, step 4798/7134 completed (loss: 0.118134506046772, acc: 0.9880239367485046)
[2025-02-13 20:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:33][root][INFO] - Training Epoch: 2/2, step 4799/7134 completed (loss: 0.11160600930452347, acc: 0.9659090638160706)
[2025-02-13 20:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:33][root][INFO] - Training Epoch: 2/2, step 4800/7134 completed (loss: 0.06135820224881172, acc: 0.9868420958518982)
[2025-02-13 20:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:34][root][INFO] - Training Epoch: 2/2, step 4801/7134 completed (loss: 0.026886142790317535, acc: 1.0)
[2025-02-13 20:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:34][root][INFO] - Training Epoch: 2/2, step 4802/7134 completed (loss: 0.12160106748342514, acc: 0.9639175534248352)
[2025-02-13 20:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:34][root][INFO] - Training Epoch: 2/2, step 4803/7134 completed (loss: 0.051688045263290405, acc: 0.9878787994384766)
[2025-02-13 20:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:35][root][INFO] - Training Epoch: 2/2, step 4804/7134 completed (loss: 0.15777653455734253, acc: 0.9638554453849792)
[2025-02-13 20:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:35][root][INFO] - Training Epoch: 2/2, step 4805/7134 completed (loss: 0.0516185387969017, acc: 0.9933775067329407)
[2025-02-13 20:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:36][root][INFO] - Training Epoch: 2/2, step 4806/7134 completed (loss: 0.11218609660863876, acc: 0.9860140085220337)
[2025-02-13 20:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:36][root][INFO] - Training Epoch: 2/2, step 4807/7134 completed (loss: 0.04844067245721817, acc: 0.9873417615890503)
[2025-02-13 20:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:36][root][INFO] - Training Epoch: 2/2, step 4808/7134 completed (loss: 0.11970486491918564, acc: 0.9848484992980957)
[2025-02-13 20:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:37][root][INFO] - Training Epoch: 2/2, step 4809/7134 completed (loss: 0.08668293058872223, acc: 0.9724137783050537)
[2025-02-13 20:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:37][root][INFO] - Training Epoch: 2/2, step 4810/7134 completed (loss: 0.10041259229183197, acc: 0.970588207244873)
[2025-02-13 20:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:38][root][INFO] - Training Epoch: 2/2, step 4811/7134 completed (loss: 0.06521394103765488, acc: 0.9918032884597778)
[2025-02-13 20:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:38][root][INFO] - Training Epoch: 2/2, step 4812/7134 completed (loss: 0.1419695019721985, acc: 0.9774011373519897)
[2025-02-13 20:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:38][root][INFO] - Training Epoch: 2/2, step 4813/7134 completed (loss: 0.018068699166178703, acc: 1.0)
[2025-02-13 20:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:39][root][INFO] - Training Epoch: 2/2, step 4814/7134 completed (loss: 0.0574287474155426, acc: 0.989130437374115)
[2025-02-13 20:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:39][root][INFO] - Training Epoch: 2/2, step 4815/7134 completed (loss: 0.019796598702669144, acc: 0.9873417615890503)
[2025-02-13 20:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:39][root][INFO] - Training Epoch: 2/2, step 4816/7134 completed (loss: 0.019687842577695847, acc: 0.9949748516082764)
[2025-02-13 20:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:40][root][INFO] - Training Epoch: 2/2, step 4817/7134 completed (loss: 0.0710182934999466, acc: 0.9797979593276978)
[2025-02-13 20:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:40][root][INFO] - Training Epoch: 2/2, step 4818/7134 completed (loss: 0.026311039924621582, acc: 1.0)
[2025-02-13 20:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:41][root][INFO] - Training Epoch: 2/2, step 4819/7134 completed (loss: 0.022753996774554253, acc: 0.9916666746139526)
[2025-02-13 20:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:41][root][INFO] - Training Epoch: 2/2, step 4820/7134 completed (loss: 0.030390005558729172, acc: 0.9946523904800415)
[2025-02-13 20:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:41][root][INFO] - Training Epoch: 2/2, step 4821/7134 completed (loss: 0.027190426364541054, acc: 0.9942196607589722)
[2025-02-13 20:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:42][root][INFO] - Training Epoch: 2/2, step 4822/7134 completed (loss: 0.03668094053864479, acc: 0.9926470518112183)
[2025-02-13 20:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:42][root][INFO] - Training Epoch: 2/2, step 4823/7134 completed (loss: 0.1342054009437561, acc: 0.9622641801834106)
[2025-02-13 20:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:42][root][INFO] - Training Epoch: 2/2, step 4824/7134 completed (loss: 0.12162917852401733, acc: 0.9661017060279846)
[2025-02-13 20:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:43][root][INFO] - Training Epoch: 2/2, step 4825/7134 completed (loss: 0.0273943729698658, acc: 0.9926470518112183)
[2025-02-13 20:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:43][root][INFO] - Training Epoch: 2/2, step 4826/7134 completed (loss: 0.03763510659337044, acc: 1.0)
[2025-02-13 20:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:44][root][INFO] - Training Epoch: 2/2, step 4827/7134 completed (loss: 0.18405881524085999, acc: 0.9612902998924255)
[2025-02-13 20:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:44][root][INFO] - Training Epoch: 2/2, step 4828/7134 completed (loss: 0.020118897780776024, acc: 1.0)
[2025-02-13 20:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:44][root][INFO] - Training Epoch: 2/2, step 4829/7134 completed (loss: 0.01736264117062092, acc: 0.9950494766235352)
[2025-02-13 20:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:45][root][INFO] - Training Epoch: 2/2, step 4830/7134 completed (loss: 0.02754572592675686, acc: 0.9939393997192383)
[2025-02-13 20:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:45][root][INFO] - Training Epoch: 2/2, step 4831/7134 completed (loss: 0.00678935507312417, acc: 1.0)
[2025-02-13 20:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:45][root][INFO] - Training Epoch: 2/2, step 4832/7134 completed (loss: 0.022892063483595848, acc: 0.9869281053543091)
[2025-02-13 20:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:46][root][INFO] - Training Epoch: 2/2, step 4833/7134 completed (loss: 0.025155266746878624, acc: 1.0)
[2025-02-13 20:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:46][root][INFO] - Training Epoch: 2/2, step 4834/7134 completed (loss: 0.016551952809095383, acc: 0.9953488111495972)
[2025-02-13 20:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:47][root][INFO] - Training Epoch: 2/2, step 4835/7134 completed (loss: 0.029457606375217438, acc: 0.9846153855323792)
[2025-02-13 20:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:47][root][INFO] - Training Epoch: 2/2, step 4836/7134 completed (loss: 0.03981952369213104, acc: 0.9938271641731262)
[2025-02-13 20:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:47][root][INFO] - Training Epoch: 2/2, step 4837/7134 completed (loss: 0.19646596908569336, acc: 0.949999988079071)
[2025-02-13 20:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:48][root][INFO] - Training Epoch: 2/2, step 4838/7134 completed (loss: 0.11530455946922302, acc: 0.9784172773361206)
[2025-02-13 20:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:48][root][INFO] - Training Epoch: 2/2, step 4839/7134 completed (loss: 0.12655240297317505, acc: 0.9689922332763672)
[2025-02-13 20:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:48][root][INFO] - Training Epoch: 2/2, step 4840/7134 completed (loss: 0.0837668925523758, acc: 0.9868420958518982)
[2025-02-13 20:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:49][root][INFO] - Training Epoch: 2/2, step 4841/7134 completed (loss: 0.05358891561627388, acc: 0.9930555820465088)
[2025-02-13 20:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:49][root][INFO] - Training Epoch: 2/2, step 4842/7134 completed (loss: 0.09274592995643616, acc: 0.9722222089767456)
[2025-02-13 20:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:49][root][INFO] - Training Epoch: 2/2, step 4843/7134 completed (loss: 0.12276383489370346, acc: 0.9622641801834106)
[2025-02-13 20:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:50][root][INFO] - Training Epoch: 2/2, step 4844/7134 completed (loss: 0.05437784641981125, acc: 0.9846153855323792)
[2025-02-13 20:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:50][root][INFO] - Training Epoch: 2/2, step 4845/7134 completed (loss: 0.055478937923908234, acc: 0.987500011920929)
[2025-02-13 20:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:51][root][INFO] - Training Epoch: 2/2, step 4846/7134 completed (loss: 0.13787995278835297, acc: 0.96875)
[2025-02-13 20:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:51][root][INFO] - Training Epoch: 2/2, step 4847/7134 completed (loss: 0.04938101768493652, acc: 0.9940828680992126)
[2025-02-13 20:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:51][root][INFO] - Training Epoch: 2/2, step 4848/7134 completed (loss: 0.11614568531513214, acc: 0.9753086566925049)
[2025-02-13 20:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:52][root][INFO] - Training Epoch: 2/2, step 4849/7134 completed (loss: 0.041141536086797714, acc: 0.9931507110595703)
[2025-02-13 20:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:52][root][INFO] - Training Epoch: 2/2, step 4850/7134 completed (loss: 0.1148318499326706, acc: 0.9608938694000244)
[2025-02-13 20:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:52][root][INFO] - Training Epoch: 2/2, step 4851/7134 completed (loss: 0.056081876158714294, acc: 0.9891892075538635)
[2025-02-13 20:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:53][root][INFO] - Training Epoch: 2/2, step 4852/7134 completed (loss: 0.10887356847524643, acc: 0.9651162624359131)
[2025-02-13 20:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:53][root][INFO] - Training Epoch: 2/2, step 4853/7134 completed (loss: 0.10718392580747604, acc: 0.9640718698501587)
[2025-02-13 20:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:53][root][INFO] - Training Epoch: 2/2, step 4854/7134 completed (loss: 0.06266159564256668, acc: 0.9932885766029358)
[2025-02-13 20:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:54][root][INFO] - Training Epoch: 2/2, step 4855/7134 completed (loss: 0.13180731236934662, acc: 0.9712643623352051)
[2025-02-13 20:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:54][root][INFO] - Training Epoch: 2/2, step 4856/7134 completed (loss: 0.07103761285543442, acc: 0.9642857313156128)
[2025-02-13 20:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:55][root][INFO] - Training Epoch: 2/2, step 4857/7134 completed (loss: 0.05727379396557808, acc: 0.9876543283462524)
[2025-02-13 20:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:55][root][INFO] - Training Epoch: 2/2, step 4858/7134 completed (loss: 0.14297007024288177, acc: 0.9567901492118835)
[2025-02-13 20:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:55][root][INFO] - Training Epoch: 2/2, step 4859/7134 completed (loss: 0.049974579364061356, acc: 0.976190447807312)
[2025-02-13 20:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:56][root][INFO] - Training Epoch: 2/2, step 4860/7134 completed (loss: 0.06773573905229568, acc: 0.9941520690917969)
[2025-02-13 20:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:56][root][INFO] - Training Epoch: 2/2, step 4861/7134 completed (loss: 0.055560845881700516, acc: 0.9881656765937805)
[2025-02-13 20:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:56][root][INFO] - Training Epoch: 2/2, step 4862/7134 completed (loss: 0.07542000710964203, acc: 0.9736841917037964)
[2025-02-13 20:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:57][root][INFO] - Training Epoch: 2/2, step 4863/7134 completed (loss: 0.040127500891685486, acc: 0.9922480583190918)
[2025-02-13 20:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:57][root][INFO] - Training Epoch: 2/2, step 4864/7134 completed (loss: 0.08465105295181274, acc: 0.987261176109314)
[2025-02-13 20:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:58][root][INFO] - Training Epoch: 2/2, step 4865/7134 completed (loss: 0.0816749706864357, acc: 0.9623655676841736)
[2025-02-13 20:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:58][root][INFO] - Training Epoch: 2/2, step 4866/7134 completed (loss: 0.05949816480278969, acc: 0.9767441749572754)
[2025-02-13 20:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:58][root][INFO] - Training Epoch: 2/2, step 4867/7134 completed (loss: 0.09765060245990753, acc: 0.9832402467727661)
[2025-02-13 20:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:59][root][INFO] - Training Epoch: 2/2, step 4868/7134 completed (loss: 0.07485350966453552, acc: 0.9747899174690247)
[2025-02-13 20:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:59][root][INFO] - Training Epoch: 2/2, step 4869/7134 completed (loss: 0.08151307702064514, acc: 0.9795918464660645)
[2025-02-13 20:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:59][root][INFO] - Training Epoch: 2/2, step 4870/7134 completed (loss: 0.038779471069574356, acc: 0.9891892075538635)
[2025-02-13 20:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:00][root][INFO] - Training Epoch: 2/2, step 4871/7134 completed (loss: 0.06499043852090836, acc: 0.9736841917037964)
[2025-02-13 20:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:00][root][INFO] - Training Epoch: 2/2, step 4872/7134 completed (loss: 0.01580463908612728, acc: 0.9940476417541504)
[2025-02-13 20:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:00][root][INFO] - Training Epoch: 2/2, step 4873/7134 completed (loss: 0.07472603023052216, acc: 0.9781420826911926)
[2025-02-13 20:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:01][root][INFO] - Training Epoch: 2/2, step 4874/7134 completed (loss: 0.0738162249326706, acc: 0.9804878234863281)
[2025-02-13 20:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:01][root][INFO] - Training Epoch: 2/2, step 4875/7134 completed (loss: 0.03701755404472351, acc: 0.9937888383865356)
[2025-02-13 20:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:02][root][INFO] - Training Epoch: 2/2, step 4876/7134 completed (loss: 0.07791072130203247, acc: 0.9814814925193787)
[2025-02-13 20:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:02][root][INFO] - Training Epoch: 2/2, step 4877/7134 completed (loss: 0.051918186247348785, acc: 0.9846153855323792)
[2025-02-13 20:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:02][root][INFO] - Training Epoch: 2/2, step 4878/7134 completed (loss: 0.05448921024799347, acc: 0.9935064911842346)
[2025-02-13 20:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:03][root][INFO] - Training Epoch: 2/2, step 4879/7134 completed (loss: 0.03360460326075554, acc: 0.9935897588729858)
[2025-02-13 20:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:03][root][INFO] - Training Epoch: 2/2, step 4880/7134 completed (loss: 0.023921754211187363, acc: 1.0)
[2025-02-13 20:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:03][root][INFO] - Training Epoch: 2/2, step 4881/7134 completed (loss: 0.07372722029685974, acc: 0.9865771532058716)
[2025-02-13 20:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:04][root][INFO] - Training Epoch: 2/2, step 4882/7134 completed (loss: 0.028341293334960938, acc: 0.9935483932495117)
[2025-02-13 20:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:04][root][INFO] - Training Epoch: 2/2, step 4883/7134 completed (loss: 0.01474853791296482, acc: 1.0)
[2025-02-13 20:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:05][root][INFO] - Training Epoch: 2/2, step 4884/7134 completed (loss: 0.04630913585424423, acc: 0.9869281053543091)
[2025-02-13 20:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:05][root][INFO] - Training Epoch: 2/2, step 4885/7134 completed (loss: 0.01754899136722088, acc: 1.0)
[2025-02-13 20:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:05][root][INFO] - Training Epoch: 2/2, step 4886/7134 completed (loss: 0.03308164328336716, acc: 0.9903846383094788)
[2025-02-13 20:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:06][root][INFO] - Training Epoch: 2/2, step 4887/7134 completed (loss: 0.04541643708944321, acc: 0.9842932224273682)
[2025-02-13 20:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:06][root][INFO] - Training Epoch: 2/2, step 4888/7134 completed (loss: 0.08233443647623062, acc: 0.9835164546966553)
[2025-02-13 20:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:06][root][INFO] - Training Epoch: 2/2, step 4889/7134 completed (loss: 0.02043786086142063, acc: 1.0)
[2025-02-13 20:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:07][root][INFO] - Training Epoch: 2/2, step 4890/7134 completed (loss: 0.053555928170681, acc: 0.9905660152435303)
[2025-02-13 20:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:07][root][INFO] - Training Epoch: 2/2, step 4891/7134 completed (loss: 0.12050437182188034, acc: 0.954081654548645)
[2025-02-13 20:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:07][root][INFO] - Training Epoch: 2/2, step 4892/7134 completed (loss: 0.028209589421749115, acc: 1.0)
[2025-02-13 20:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:08][root][INFO] - Training Epoch: 2/2, step 4893/7134 completed (loss: 0.25657927989959717, acc: 0.9396551847457886)
[2025-02-13 20:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:08][root][INFO] - Training Epoch: 2/2, step 4894/7134 completed (loss: 0.06507450342178345, acc: 0.9764705896377563)
[2025-02-13 20:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:09][root][INFO] - Training Epoch: 2/2, step 4895/7134 completed (loss: 0.02024424448609352, acc: 0.9948979616165161)
[2025-02-13 20:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:09][root][INFO] - Training Epoch: 2/2, step 4896/7134 completed (loss: 0.08561314642429352, acc: 0.994413435459137)
[2025-02-13 20:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:09][root][INFO] - Training Epoch: 2/2, step 4897/7134 completed (loss: 0.045634180307388306, acc: 0.9802955389022827)
[2025-02-13 20:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:10][root][INFO] - Training Epoch: 2/2, step 4898/7134 completed (loss: 0.04456799849867821, acc: 0.9892473220825195)
[2025-02-13 20:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:10][root][INFO] - Training Epoch: 2/2, step 4899/7134 completed (loss: 0.031862933188676834, acc: 0.9939024448394775)
[2025-02-13 20:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:10][root][INFO] - Training Epoch: 2/2, step 4900/7134 completed (loss: 0.052689939737319946, acc: 0.9839572310447693)
[2025-02-13 20:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:11][root][INFO] - Training Epoch: 2/2, step 4901/7134 completed (loss: 0.043150644749403, acc: 0.9817073345184326)
[2025-02-13 20:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:11][root][INFO] - Training Epoch: 2/2, step 4902/7134 completed (loss: 0.021441342309117317, acc: 0.9948717951774597)
[2025-02-13 20:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:12][root][INFO] - Training Epoch: 2/2, step 4903/7134 completed (loss: 0.018193822354078293, acc: 1.0)
[2025-02-13 20:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:12][root][INFO] - Training Epoch: 2/2, step 4904/7134 completed (loss: 0.03794986382126808, acc: 0.9882352948188782)
[2025-02-13 20:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:12][root][INFO] - Training Epoch: 2/2, step 4905/7134 completed (loss: 0.027849826961755753, acc: 0.9869281053543091)
[2025-02-13 20:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:13][root][INFO] - Training Epoch: 2/2, step 4906/7134 completed (loss: 0.015043559484183788, acc: 1.0)
[2025-02-13 20:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:13][root][INFO] - Training Epoch: 2/2, step 4907/7134 completed (loss: 0.037871286273002625, acc: 0.9931972622871399)
[2025-02-13 20:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:13][root][INFO] - Training Epoch: 2/2, step 4908/7134 completed (loss: 0.009309890680015087, acc: 1.0)
[2025-02-13 20:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:14][root][INFO] - Training Epoch: 2/2, step 4909/7134 completed (loss: 0.018054550513625145, acc: 0.9932885766029358)
[2025-02-13 20:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:14][root][INFO] - Training Epoch: 2/2, step 4910/7134 completed (loss: 0.07625948637723923, acc: 0.9870967864990234)
[2025-02-13 20:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:14][root][INFO] - Training Epoch: 2/2, step 4911/7134 completed (loss: 0.025515003129839897, acc: 0.9946523904800415)
[2025-02-13 20:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:15][root][INFO] - Training Epoch: 2/2, step 4912/7134 completed (loss: 0.022419627755880356, acc: 0.9950248599052429)
[2025-02-13 20:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:15][root][INFO] - Training Epoch: 2/2, step 4913/7134 completed (loss: 0.06013540178537369, acc: 0.9860140085220337)
[2025-02-13 20:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:16][root][INFO] - Training Epoch: 2/2, step 4914/7134 completed (loss: 0.026932405307888985, acc: 1.0)
[2025-02-13 20:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:16][root][INFO] - Training Epoch: 2/2, step 4915/7134 completed (loss: 0.019274132326245308, acc: 0.9938271641731262)
[2025-02-13 20:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:16][root][INFO] - Training Epoch: 2/2, step 4916/7134 completed (loss: 0.04906150698661804, acc: 0.989130437374115)
[2025-02-13 20:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:17][root][INFO] - Training Epoch: 2/2, step 4917/7134 completed (loss: 0.020271258428692818, acc: 1.0)
[2025-02-13 20:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:17][root][INFO] - Training Epoch: 2/2, step 4918/7134 completed (loss: 0.012592064216732979, acc: 0.9938271641731262)
[2025-02-13 20:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:18][root][INFO] - Training Epoch: 2/2, step 4919/7134 completed (loss: 0.036681950092315674, acc: 0.9887005686759949)
[2025-02-13 20:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:18][root][INFO] - Training Epoch: 2/2, step 4920/7134 completed (loss: 0.05396214500069618, acc: 0.9813664555549622)
[2025-02-13 20:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:18][root][INFO] - Training Epoch: 2/2, step 4921/7134 completed (loss: 0.0812297835946083, acc: 0.985401451587677)
[2025-02-13 20:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:19][root][INFO] - Training Epoch: 2/2, step 4922/7134 completed (loss: 0.09318585693836212, acc: 0.9875776171684265)
[2025-02-13 20:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:19][root][INFO] - Training Epoch: 2/2, step 4923/7134 completed (loss: 0.013463450595736504, acc: 1.0)
[2025-02-13 20:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:19][root][INFO] - Training Epoch: 2/2, step 4924/7134 completed (loss: 0.015913404524326324, acc: 1.0)
[2025-02-13 20:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:20][root][INFO] - Training Epoch: 2/2, step 4925/7134 completed (loss: 0.04831542819738388, acc: 0.9932885766029358)
[2025-02-13 20:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:20][root][INFO] - Training Epoch: 2/2, step 4926/7134 completed (loss: 0.012955702841281891, acc: 1.0)
[2025-02-13 20:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:20][root][INFO] - Training Epoch: 2/2, step 4927/7134 completed (loss: 0.03496500849723816, acc: 0.9935483932495117)
[2025-02-13 20:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:21][root][INFO] - Training Epoch: 2/2, step 4928/7134 completed (loss: 0.01944185607135296, acc: 0.9947368502616882)
[2025-02-13 20:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:21][root][INFO] - Training Epoch: 2/2, step 4929/7134 completed (loss: 0.024766042828559875, acc: 1.0)
[2025-02-13 20:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:21][root][INFO] - Training Epoch: 2/2, step 4930/7134 completed (loss: 0.037628356367349625, acc: 0.9864864945411682)
[2025-02-13 20:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:22][root][INFO] - Training Epoch: 2/2, step 4931/7134 completed (loss: 0.025227822363376617, acc: 1.0)
[2025-02-13 20:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:22][root][INFO] - Training Epoch: 2/2, step 4932/7134 completed (loss: 0.029843024909496307, acc: 0.9941176176071167)
[2025-02-13 20:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:23][root][INFO] - Training Epoch: 2/2, step 4933/7134 completed (loss: 0.06529463082551956, acc: 0.9738219976425171)
[2025-02-13 20:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:23][root][INFO] - Training Epoch: 2/2, step 4934/7134 completed (loss: 0.08747744560241699, acc: 0.9823529124259949)
[2025-02-13 20:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:23][root][INFO] - Training Epoch: 2/2, step 4935/7134 completed (loss: 0.05941152572631836, acc: 0.9823529124259949)
[2025-02-13 20:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:24][root][INFO] - Training Epoch: 2/2, step 4936/7134 completed (loss: 0.014558127149939537, acc: 1.0)
[2025-02-13 20:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:24][root][INFO] - Training Epoch: 2/2, step 4937/7134 completed (loss: 0.04505638778209686, acc: 0.9930070042610168)
[2025-02-13 20:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:24][root][INFO] - Training Epoch: 2/2, step 4938/7134 completed (loss: 0.013551685959100723, acc: 1.0)
[2025-02-13 20:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:25][root][INFO] - Training Epoch: 2/2, step 4939/7134 completed (loss: 0.05707937851548195, acc: 0.9928571581840515)
[2025-02-13 20:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:25][root][INFO] - Training Epoch: 2/2, step 4940/7134 completed (loss: 0.12295990437269211, acc: 0.9922480583190918)
[2025-02-13 20:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:26][root][INFO] - Training Epoch: 2/2, step 4941/7134 completed (loss: 0.06872721016407013, acc: 0.9820359349250793)
[2025-02-13 20:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:26][root][INFO] - Training Epoch: 2/2, step 4942/7134 completed (loss: 0.048049814999103546, acc: 0.9810126423835754)
[2025-02-13 20:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:26][root][INFO] - Training Epoch: 2/2, step 4943/7134 completed (loss: 0.05580834671854973, acc: 0.987500011920929)
[2025-02-13 20:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:27][root][INFO] - Training Epoch: 2/2, step 4944/7134 completed (loss: 0.11705811321735382, acc: 0.9747899174690247)
[2025-02-13 20:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:27][root][INFO] - Training Epoch: 2/2, step 4945/7134 completed (loss: 0.08129259198904037, acc: 0.9772727489471436)
[2025-02-13 20:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:27][root][INFO] - Training Epoch: 2/2, step 4946/7134 completed (loss: 0.036907028406858444, acc: 0.9921875)
[2025-02-13 20:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:28][root][INFO] - Training Epoch: 2/2, step 4947/7134 completed (loss: 0.05972788855433464, acc: 0.9739130139350891)
[2025-02-13 20:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:28][root][INFO] - Training Epoch: 2/2, step 4948/7134 completed (loss: 0.05611353740096092, acc: 0.9904761910438538)
[2025-02-13 20:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:28][root][INFO] - Training Epoch: 2/2, step 4949/7134 completed (loss: 0.013458861038088799, acc: 1.0)
[2025-02-13 20:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:29][root][INFO] - Training Epoch: 2/2, step 4950/7134 completed (loss: 0.05483802407979965, acc: 0.9931972622871399)
[2025-02-13 20:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:29][root][INFO] - Training Epoch: 2/2, step 4951/7134 completed (loss: 0.046756159514188766, acc: 0.9887640476226807)
[2025-02-13 20:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:30][root][INFO] - Training Epoch: 2/2, step 4952/7134 completed (loss: 0.030076339840888977, acc: 0.9864864945411682)
[2025-02-13 20:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:30][root][INFO] - Training Epoch: 2/2, step 4953/7134 completed (loss: 0.024990133941173553, acc: 1.0)
[2025-02-13 20:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:30][root][INFO] - Training Epoch: 2/2, step 4954/7134 completed (loss: 0.017274441197514534, acc: 1.0)
[2025-02-13 20:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:31][root][INFO] - Training Epoch: 2/2, step 4955/7134 completed (loss: 0.022358665242791176, acc: 1.0)
[2025-02-13 20:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:31][root][INFO] - Training Epoch: 2/2, step 4956/7134 completed (loss: 0.10907787829637527, acc: 0.9647058844566345)
[2025-02-13 20:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:32][root][INFO] - Training Epoch: 2/2, step 4957/7134 completed (loss: 0.09334687143564224, acc: 0.9858155846595764)
[2025-02-13 20:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:32][root][INFO] - Training Epoch: 2/2, step 4958/7134 completed (loss: 0.0594579353928566, acc: 0.9811320900917053)
[2025-02-13 20:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:32][root][INFO] - Training Epoch: 2/2, step 4959/7134 completed (loss: 0.04003993421792984, acc: 0.9879518151283264)
[2025-02-13 20:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:33][root][INFO] - Training Epoch: 2/2, step 4960/7134 completed (loss: 0.05188513547182083, acc: 0.9852941036224365)
[2025-02-13 20:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:33][root][INFO] - Training Epoch: 2/2, step 4961/7134 completed (loss: 0.026057058945298195, acc: 1.0)
[2025-02-13 20:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:33][root][INFO] - Training Epoch: 2/2, step 4962/7134 completed (loss: 0.013997101224958897, acc: 1.0)
[2025-02-13 20:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:34][root][INFO] - Training Epoch: 2/2, step 4963/7134 completed (loss: 0.016464315354824066, acc: 1.0)
[2025-02-13 20:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:34][root][INFO] - Training Epoch: 2/2, step 4964/7134 completed (loss: 0.08524751663208008, acc: 0.9856114983558655)
[2025-02-13 20:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:34][root][INFO] - Training Epoch: 2/2, step 4965/7134 completed (loss: 0.017609987407922745, acc: 1.0)
[2025-02-13 20:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:35][root][INFO] - Training Epoch: 2/2, step 4966/7134 completed (loss: 0.15358372032642365, acc: 0.9670329689979553)
[2025-02-13 20:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:35][root][INFO] - Training Epoch: 2/2, step 4967/7134 completed (loss: 0.08032207190990448, acc: 0.9844961166381836)
[2025-02-13 20:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:35][root][INFO] - Training Epoch: 2/2, step 4968/7134 completed (loss: 0.1705227643251419, acc: 0.9503546357154846)
[2025-02-13 20:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:36][root][INFO] - Training Epoch: 2/2, step 4969/7134 completed (loss: 0.04976153373718262, acc: 0.9793103337287903)
[2025-02-13 20:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:36][root][INFO] - Training Epoch: 2/2, step 4970/7134 completed (loss: 0.039591703563928604, acc: 0.9931034445762634)
[2025-02-13 20:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:37][root][INFO] - Training Epoch: 2/2, step 4971/7134 completed (loss: 0.03815050050616264, acc: 0.9867549538612366)
[2025-02-13 20:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:37][root][INFO] - Training Epoch: 2/2, step 4972/7134 completed (loss: 0.06420847028493881, acc: 0.981249988079071)
[2025-02-13 20:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:37][root][INFO] - Training Epoch: 2/2, step 4973/7134 completed (loss: 0.03771211579442024, acc: 0.9924242496490479)
[2025-02-13 20:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:38][root][INFO] - Training Epoch: 2/2, step 4974/7134 completed (loss: 0.09509024024009705, acc: 0.9545454382896423)
[2025-02-13 20:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:38][root][INFO] - Training Epoch: 2/2, step 4975/7134 completed (loss: 0.1904386729001999, acc: 0.951724112033844)
[2025-02-13 20:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:38][root][INFO] - Training Epoch: 2/2, step 4976/7134 completed (loss: 0.05161738023161888, acc: 0.9789473414421082)
[2025-02-13 20:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:39][root][INFO] - Training Epoch: 2/2, step 4977/7134 completed (loss: 0.15443335473537445, acc: 0.9708737730979919)
[2025-02-13 20:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:39][root][INFO] - Training Epoch: 2/2, step 4978/7134 completed (loss: 0.02371091954410076, acc: 0.9942528605461121)
[2025-02-13 20:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:39][root][INFO] - Training Epoch: 2/2, step 4979/7134 completed (loss: 0.18217341601848602, acc: 0.9370629191398621)
[2025-02-13 20:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:40][root][INFO] - Training Epoch: 2/2, step 4980/7134 completed (loss: 0.1253320425748825, acc: 0.9714285731315613)
[2025-02-13 20:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:40][root][INFO] - Training Epoch: 2/2, step 4981/7134 completed (loss: 0.06013920530676842, acc: 0.9857142567634583)
[2025-02-13 20:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:41][root][INFO] - Training Epoch: 2/2, step 4982/7134 completed (loss: 0.07052022218704224, acc: 0.9819819927215576)
[2025-02-13 20:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:41][root][INFO] - Training Epoch: 2/2, step 4983/7134 completed (loss: 0.09330428391695023, acc: 0.9856114983558655)
[2025-02-13 20:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:41][root][INFO] - Training Epoch: 2/2, step 4984/7134 completed (loss: 0.03463820740580559, acc: 1.0)
[2025-02-13 20:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:42][root][INFO] - Training Epoch: 2/2, step 4985/7134 completed (loss: 0.01978548988699913, acc: 1.0)
[2025-02-13 20:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:42][root][INFO] - Training Epoch: 2/2, step 4986/7134 completed (loss: 0.06686824560165405, acc: 0.9930070042610168)
[2025-02-13 20:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:42][root][INFO] - Training Epoch: 2/2, step 4987/7134 completed (loss: 0.020582783967256546, acc: 1.0)
[2025-02-13 20:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:43][root][INFO] - Training Epoch: 2/2, step 4988/7134 completed (loss: 0.022051140666007996, acc: 1.0)
[2025-02-13 20:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:43][root][INFO] - Training Epoch: 2/2, step 4989/7134 completed (loss: 0.10279848426580429, acc: 0.9882352948188782)
[2025-02-13 20:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:43][root][INFO] - Training Epoch: 2/2, step 4990/7134 completed (loss: 0.020245803520083427, acc: 0.9914529919624329)
[2025-02-13 20:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:44][root][INFO] - Training Epoch: 2/2, step 4991/7134 completed (loss: 0.017435338348150253, acc: 1.0)
[2025-02-13 20:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:44][root][INFO] - Training Epoch: 2/2, step 4992/7134 completed (loss: 0.06047617644071579, acc: 0.982758641242981)
[2025-02-13 20:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:44][root][INFO] - Training Epoch: 2/2, step 4993/7134 completed (loss: 0.01574924774467945, acc: 1.0)
[2025-02-13 20:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:45][root][INFO] - Training Epoch: 2/2, step 4994/7134 completed (loss: 0.031356893479824066, acc: 0.9933775067329407)
[2025-02-13 20:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:45][root][INFO] - Training Epoch: 2/2, step 4995/7134 completed (loss: 0.06296653300523758, acc: 0.9814814925193787)
[2025-02-13 20:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:46][root][INFO] - Training Epoch: 2/2, step 4996/7134 completed (loss: 0.052187442779541016, acc: 0.9916666746139526)
[2025-02-13 20:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:46][root][INFO] - Training Epoch: 2/2, step 4997/7134 completed (loss: 0.030932750552892685, acc: 0.9847328066825867)
[2025-02-13 20:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:46][root][INFO] - Training Epoch: 2/2, step 4998/7134 completed (loss: 0.05899640545248985, acc: 0.9866666793823242)
[2025-02-13 20:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:47][root][INFO] - Training Epoch: 2/2, step 4999/7134 completed (loss: 0.05134206265211105, acc: 0.9842519760131836)
[2025-02-13 20:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:47][root][INFO] - Training Epoch: 2/2, step 5000/7134 completed (loss: 0.017883919179439545, acc: 1.0)
[2025-02-13 20:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:47][root][INFO] - Training Epoch: 2/2, step 5001/7134 completed (loss: 0.12715502083301544, acc: 0.9863945841789246)
[2025-02-13 20:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:48][root][INFO] - Training Epoch: 2/2, step 5002/7134 completed (loss: 0.020098192617297173, acc: 1.0)
[2025-02-13 20:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:48][root][INFO] - Training Epoch: 2/2, step 5003/7134 completed (loss: 0.02625337615609169, acc: 1.0)
[2025-02-13 20:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:49][root][INFO] - Training Epoch: 2/2, step 5004/7134 completed (loss: 0.009156366810202599, acc: 1.0)
[2025-02-13 20:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:49][root][INFO] - Training Epoch: 2/2, step 5005/7134 completed (loss: 0.06874136626720428, acc: 0.9890109896659851)
[2025-02-13 20:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:49][root][INFO] - Training Epoch: 2/2, step 5006/7134 completed (loss: 0.04590015113353729, acc: 0.9868420958518982)
[2025-02-13 20:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:50][root][INFO] - Training Epoch: 2/2, step 5007/7134 completed (loss: 0.019976068288087845, acc: 1.0)
[2025-02-13 20:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:50][root][INFO] - Training Epoch: 2/2, step 5008/7134 completed (loss: 0.1277931183576584, acc: 0.9833333492279053)
[2025-02-13 20:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:50][root][INFO] - Training Epoch: 2/2, step 5009/7134 completed (loss: 0.07860985398292542, acc: 0.9896907210350037)
[2025-02-13 20:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:51][root][INFO] - Training Epoch: 2/2, step 5010/7134 completed (loss: 0.08639129251241684, acc: 0.9914529919624329)
[2025-02-13 20:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:51][root][INFO] - Training Epoch: 2/2, step 5011/7134 completed (loss: 0.07436883449554443, acc: 0.9655172228813171)
[2025-02-13 20:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:51][root][INFO] - Training Epoch: 2/2, step 5012/7134 completed (loss: 0.03391963988542557, acc: 0.987500011920929)
[2025-02-13 20:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:52][root][INFO] - Training Epoch: 2/2, step 5013/7134 completed (loss: 0.045149464160203934, acc: 0.9829059839248657)
[2025-02-13 20:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:52][root][INFO] - Training Epoch: 2/2, step 5014/7134 completed (loss: 0.08549271523952484, acc: 0.9807692170143127)
[2025-02-13 20:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:52][root][INFO] - Training Epoch: 2/2, step 5015/7134 completed (loss: 0.15469448268413544, acc: 0.9693251252174377)
[2025-02-13 20:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:53][root][INFO] - Training Epoch: 2/2, step 5016/7134 completed (loss: 0.15940465033054352, acc: 0.9618320465087891)
[2025-02-13 20:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:53][root][INFO] - Training Epoch: 2/2, step 5017/7134 completed (loss: 0.027575816959142685, acc: 1.0)
[2025-02-13 20:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:54][root][INFO] - Training Epoch: 2/2, step 5018/7134 completed (loss: 0.04297715425491333, acc: 0.9924812316894531)
[2025-02-13 20:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:54][root][INFO] - Training Epoch: 2/2, step 5019/7134 completed (loss: 0.14911270141601562, acc: 0.9717513918876648)
[2025-02-13 20:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:54][root][INFO] - Training Epoch: 2/2, step 5020/7134 completed (loss: 0.12026071548461914, acc: 0.9629629850387573)
[2025-02-13 20:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:55][root][INFO] - Training Epoch: 2/2, step 5021/7134 completed (loss: 0.08914792537689209, acc: 0.9904761910438538)
[2025-02-13 20:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:55][root][INFO] - Training Epoch: 2/2, step 5022/7134 completed (loss: 0.057299233973026276, acc: 0.9927536249160767)
[2025-02-13 20:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:55][root][INFO] - Training Epoch: 2/2, step 5023/7134 completed (loss: 0.04077086225152016, acc: 0.9870967864990234)
[2025-02-13 20:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:56][root][INFO] - Training Epoch: 2/2, step 5024/7134 completed (loss: 0.04127323627471924, acc: 0.9819276928901672)
[2025-02-13 20:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:56][root][INFO] - Training Epoch: 2/2, step 5025/7134 completed (loss: 0.01574479602277279, acc: 1.0)
[2025-02-13 20:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:56][root][INFO] - Training Epoch: 2/2, step 5026/7134 completed (loss: 0.021887335926294327, acc: 1.0)
[2025-02-13 20:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:57][root][INFO] - Training Epoch: 2/2, step 5027/7134 completed (loss: 0.03330658748745918, acc: 0.9922480583190918)
[2025-02-13 20:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:57][root][INFO] - Training Epoch: 2/2, step 5028/7134 completed (loss: 0.061816416680812836, acc: 0.9788732528686523)
[2025-02-13 20:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:58][root][INFO] - Training Epoch: 2/2, step 5029/7134 completed (loss: 0.03373468667268753, acc: 0.9923076629638672)
[2025-02-13 20:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:58][root][INFO] - Training Epoch: 2/2, step 5030/7134 completed (loss: 0.03515658527612686, acc: 1.0)
[2025-02-13 20:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:58][root][INFO] - Training Epoch: 2/2, step 5031/7134 completed (loss: 0.15118199586868286, acc: 0.9710144996643066)
[2025-02-13 20:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:59][root][INFO] - Training Epoch: 2/2, step 5032/7134 completed (loss: 0.04803774133324623, acc: 0.9788732528686523)
[2025-02-13 20:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:59][root][INFO] - Training Epoch: 2/2, step 5033/7134 completed (loss: 0.01790454611182213, acc: 1.0)
[2025-02-13 20:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:59][root][INFO] - Training Epoch: 2/2, step 5034/7134 completed (loss: 0.03821428492665291, acc: 0.9849624037742615)
[2025-02-13 20:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:00][root][INFO] - Training Epoch: 2/2, step 5035/7134 completed (loss: 0.008861462585628033, acc: 1.0)
[2025-02-13 20:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:00][root][INFO] - Training Epoch: 2/2, step 5036/7134 completed (loss: 0.10916455090045929, acc: 0.9807692170143127)
[2025-02-13 20:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:01][root][INFO] - Training Epoch: 2/2, step 5037/7134 completed (loss: 0.12863299250602722, acc: 0.970588207244873)
[2025-02-13 20:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:01][root][INFO] - Training Epoch: 2/2, step 5038/7134 completed (loss: 0.18419209122657776, acc: 0.95652174949646)
[2025-02-13 20:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:01][root][INFO] - Training Epoch: 2/2, step 5039/7134 completed (loss: 0.01655031181871891, acc: 1.0)
[2025-02-13 20:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:02][root][INFO] - Training Epoch: 2/2, step 5040/7134 completed (loss: 0.0255852323025465, acc: 1.0)
[2025-02-13 20:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:02][root][INFO] - Training Epoch: 2/2, step 5041/7134 completed (loss: 0.016960807144641876, acc: 1.0)
[2025-02-13 20:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:02][root][INFO] - Training Epoch: 2/2, step 5042/7134 completed (loss: 0.05674812197685242, acc: 0.9805825352668762)
[2025-02-13 20:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:03][root][INFO] - Training Epoch: 2/2, step 5043/7134 completed (loss: 0.04487113282084465, acc: 0.991150438785553)
[2025-02-13 20:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:03][root][INFO] - Training Epoch: 2/2, step 5044/7134 completed (loss: 0.032520417124032974, acc: 0.993630588054657)
[2025-02-13 20:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:04][root][INFO] - Training Epoch: 2/2, step 5045/7134 completed (loss: 0.05667877569794655, acc: 0.9837837815284729)
[2025-02-13 20:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:04][root][INFO] - Training Epoch: 2/2, step 5046/7134 completed (loss: 0.09742472320795059, acc: 0.9677419066429138)
[2025-02-13 20:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:04][root][INFO] - Training Epoch: 2/2, step 5047/7134 completed (loss: 0.029783746227622032, acc: 1.0)
[2025-02-13 20:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:05][root][INFO] - Training Epoch: 2/2, step 5048/7134 completed (loss: 0.08075535297393799, acc: 0.9857142567634583)
[2025-02-13 20:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:05][root][INFO] - Training Epoch: 2/2, step 5049/7134 completed (loss: 0.07665478438138962, acc: 0.987261176109314)
[2025-02-13 20:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:05][root][INFO] - Training Epoch: 2/2, step 5050/7134 completed (loss: 0.16548818349838257, acc: 0.981249988079071)
[2025-02-13 20:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:06][root][INFO] - Training Epoch: 2/2, step 5051/7134 completed (loss: 0.11838652193546295, acc: 0.9727891087532043)
[2025-02-13 20:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:06][root][INFO] - Training Epoch: 2/2, step 5052/7134 completed (loss: 0.095025435090065, acc: 0.9743589758872986)
[2025-02-13 20:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:07][root][INFO] - Training Epoch: 2/2, step 5053/7134 completed (loss: 0.14352640509605408, acc: 0.949999988079071)
[2025-02-13 20:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:07][root][INFO] - Training Epoch: 2/2, step 5054/7134 completed (loss: 0.12755538523197174, acc: 0.9555555582046509)
[2025-02-13 20:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:07][root][INFO] - Training Epoch: 2/2, step 5055/7134 completed (loss: 0.13093774020671844, acc: 0.9646017551422119)
[2025-02-13 20:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:08][root][INFO] - Training Epoch: 2/2, step 5056/7134 completed (loss: 0.09491710364818573, acc: 0.9798657894134521)
[2025-02-13 20:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:08][root][INFO] - Training Epoch: 2/2, step 5057/7134 completed (loss: 0.21105621755123138, acc: 0.9617834687232971)
[2025-02-13 20:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:08][root][INFO] - Training Epoch: 2/2, step 5058/7134 completed (loss: 0.08714797347784042, acc: 0.9756097793579102)
[2025-02-13 20:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:09][root][INFO] - Training Epoch: 2/2, step 5059/7134 completed (loss: 0.04975564032793045, acc: 0.9820359349250793)
[2025-02-13 20:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:10][root][INFO] - Training Epoch: 2/2, step 5060/7134 completed (loss: 0.11231053620576859, acc: 0.9800000190734863)
[2025-02-13 20:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:10][root][INFO] - Training Epoch: 2/2, step 5061/7134 completed (loss: 0.05224982276558876, acc: 0.9904761910438538)
[2025-02-13 20:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:11][root][INFO] - Training Epoch: 2/2, step 5062/7134 completed (loss: 0.06217260658740997, acc: 0.9851852059364319)
[2025-02-13 20:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:11][root][INFO] - Training Epoch: 2/2, step 5063/7134 completed (loss: 0.06288573890924454, acc: 0.9860140085220337)
[2025-02-13 20:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:11][root][INFO] - Training Epoch: 2/2, step 5064/7134 completed (loss: 0.07964135706424713, acc: 0.9785714149475098)
[2025-02-13 20:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:12][root][INFO] - Training Epoch: 2/2, step 5065/7134 completed (loss: 0.09027169644832611, acc: 0.9748427867889404)
[2025-02-13 20:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:12][root][INFO] - Training Epoch: 2/2, step 5066/7134 completed (loss: 0.07682528346776962, acc: 0.9655172228813171)
[2025-02-13 20:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:12][root][INFO] - Training Epoch: 2/2, step 5067/7134 completed (loss: 0.08921569585800171, acc: 0.9836065769195557)
[2025-02-13 20:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:13][root][INFO] - Training Epoch: 2/2, step 5068/7134 completed (loss: 0.1245851144194603, acc: 0.9577465057373047)
[2025-02-13 20:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:13][root][INFO] - Training Epoch: 2/2, step 5069/7134 completed (loss: 0.03857817128300667, acc: 0.9864864945411682)
[2025-02-13 20:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:14][root][INFO] - Training Epoch: 2/2, step 5070/7134 completed (loss: 0.032618261873722076, acc: 0.9878048896789551)
[2025-02-13 20:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:14][root][INFO] - Training Epoch: 2/2, step 5071/7134 completed (loss: 0.026953041553497314, acc: 1.0)
[2025-02-13 20:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:14][root][INFO] - Training Epoch: 2/2, step 5072/7134 completed (loss: 0.06502576172351837, acc: 0.9890109896659851)
[2025-02-13 20:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:15][root][INFO] - Training Epoch: 2/2, step 5073/7134 completed (loss: 0.080132856965065, acc: 0.9694656729698181)
[2025-02-13 20:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:15][root][INFO] - Training Epoch: 2/2, step 5074/7134 completed (loss: 0.08581533282995224, acc: 0.9767441749572754)
[2025-02-13 20:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:15][root][INFO] - Training Epoch: 2/2, step 5075/7134 completed (loss: 0.15721650421619415, acc: 0.9652174115180969)
[2025-02-13 20:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:16][root][INFO] - Training Epoch: 2/2, step 5076/7134 completed (loss: 0.04710010439157486, acc: 0.9946236610412598)
[2025-02-13 20:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:16][root][INFO] - Training Epoch: 2/2, step 5077/7134 completed (loss: 0.056793078780174255, acc: 0.9852941036224365)
[2025-02-13 20:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:17][root][INFO] - Training Epoch: 2/2, step 5078/7134 completed (loss: 0.2015528380870819, acc: 0.949999988079071)
[2025-02-13 20:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:17][root][INFO] - Training Epoch: 2/2, step 5079/7134 completed (loss: 0.043130386620759964, acc: 0.9888888597488403)
[2025-02-13 20:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:17][root][INFO] - Training Epoch: 2/2, step 5080/7134 completed (loss: 0.021794119849801064, acc: 0.9950980544090271)
[2025-02-13 20:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:18][root][INFO] - Training Epoch: 2/2, step 5081/7134 completed (loss: 0.03637152537703514, acc: 0.9826589822769165)
[2025-02-13 20:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:18][root][INFO] - Training Epoch: 2/2, step 5082/7134 completed (loss: 0.06282058358192444, acc: 0.9931034445762634)
[2025-02-13 20:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:19][root][INFO] - Training Epoch: 2/2, step 5083/7134 completed (loss: 0.07322650402784348, acc: 0.9822485446929932)
[2025-02-13 20:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:19][root][INFO] - Training Epoch: 2/2, step 5084/7134 completed (loss: 0.04830923676490784, acc: 0.9900990128517151)
[2025-02-13 20:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:19][root][INFO] - Training Epoch: 2/2, step 5085/7134 completed (loss: 0.05487808212637901, acc: 0.9793814420700073)
[2025-02-13 20:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:20][root][INFO] - Training Epoch: 2/2, step 5086/7134 completed (loss: 0.043613266199827194, acc: 0.988950252532959)
[2025-02-13 20:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:20][root][INFO] - Training Epoch: 2/2, step 5087/7134 completed (loss: 0.07189803570508957, acc: 0.9837837815284729)
[2025-02-13 20:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:20][root][INFO] - Training Epoch: 2/2, step 5088/7134 completed (loss: 0.019092325121164322, acc: 0.9940828680992126)
[2025-02-13 20:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:21][root][INFO] - Training Epoch: 2/2, step 5089/7134 completed (loss: 0.057451874017715454, acc: 0.9897959232330322)
[2025-02-13 20:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:21][root][INFO] - Training Epoch: 2/2, step 5090/7134 completed (loss: 0.029314767569303513, acc: 0.9902912378311157)
[2025-02-13 20:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:22][root][INFO] - Training Epoch: 2/2, step 5091/7134 completed (loss: 0.037691887468099594, acc: 1.0)
[2025-02-13 20:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:22][root][INFO] - Training Epoch: 2/2, step 5092/7134 completed (loss: 0.06073024868965149, acc: 0.9836956262588501)
[2025-02-13 20:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:22][root][INFO] - Training Epoch: 2/2, step 5093/7134 completed (loss: 0.06034146621823311, acc: 0.9772727489471436)
[2025-02-13 20:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:23][root][INFO] - Training Epoch: 2/2, step 5094/7134 completed (loss: 0.04697391763329506, acc: 0.9923076629638672)
[2025-02-13 20:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:23][root][INFO] - Training Epoch: 2/2, step 5095/7134 completed (loss: 0.1013377457857132, acc: 0.9745222926139832)
[2025-02-13 20:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:23][root][INFO] - Training Epoch: 2/2, step 5096/7134 completed (loss: 0.10217039287090302, acc: 0.9893617033958435)
[2025-02-13 20:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:24][root][INFO] - Training Epoch: 2/2, step 5097/7134 completed (loss: 0.0464961975812912, acc: 0.9893048405647278)
[2025-02-13 20:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:24][root][INFO] - Training Epoch: 2/2, step 5098/7134 completed (loss: 0.08334880322217941, acc: 0.9795918464660645)
[2025-02-13 20:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:25][root][INFO] - Training Epoch: 2/2, step 5099/7134 completed (loss: 0.047889985144138336, acc: 0.9894179701805115)
[2025-02-13 20:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:25][root][INFO] - Training Epoch: 2/2, step 5100/7134 completed (loss: 0.04385324567556381, acc: 0.9803921580314636)
[2025-02-13 20:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:25][root][INFO] - Training Epoch: 2/2, step 5101/7134 completed (loss: 0.04120154678821564, acc: 0.9947643876075745)
[2025-02-13 20:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:26][root][INFO] - Training Epoch: 2/2, step 5102/7134 completed (loss: 0.05927148088812828, acc: 0.9878048896789551)
[2025-02-13 20:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:26][root][INFO] - Training Epoch: 2/2, step 5103/7134 completed (loss: 0.03664623945951462, acc: 1.0)
[2025-02-13 20:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:27][root][INFO] - Training Epoch: 2/2, step 5104/7134 completed (loss: 0.037949077785015106, acc: 0.9842105507850647)
[2025-02-13 20:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:27][root][INFO] - Training Epoch: 2/2, step 5105/7134 completed (loss: 0.10188739746809006, acc: 0.9801324605941772)
[2025-02-13 20:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:27][root][INFO] - Training Epoch: 2/2, step 5106/7134 completed (loss: 0.08524015545845032, acc: 0.9794520735740662)
[2025-02-13 20:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:28][root][INFO] - Training Epoch: 2/2, step 5107/7134 completed (loss: 0.1805143803358078, acc: 0.9523809552192688)
[2025-02-13 20:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:28][root][INFO] - Training Epoch: 2/2, step 5108/7134 completed (loss: 0.14948239922523499, acc: 0.957446813583374)
[2025-02-13 20:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:28][root][INFO] - Training Epoch: 2/2, step 5109/7134 completed (loss: 0.19477999210357666, acc: 0.9648241400718689)
[2025-02-13 20:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:29][root][INFO] - Training Epoch: 2/2, step 5110/7134 completed (loss: 0.13358259201049805, acc: 0.9560439586639404)
[2025-02-13 20:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:29][root][INFO] - Training Epoch: 2/2, step 5111/7134 completed (loss: 0.06395794451236725, acc: 0.977011501789093)
[2025-02-13 20:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:30][root][INFO] - Training Epoch: 2/2, step 5112/7134 completed (loss: 0.13432982563972473, acc: 0.9621621370315552)
[2025-02-13 20:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:30][root][INFO] - Training Epoch: 2/2, step 5113/7134 completed (loss: 0.14215898513793945, acc: 0.9602649211883545)
[2025-02-13 20:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:30][root][INFO] - Training Epoch: 2/2, step 5114/7134 completed (loss: 0.06394960731267929, acc: 0.9735099077224731)
[2025-02-13 20:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:31][root][INFO] - Training Epoch: 2/2, step 5115/7134 completed (loss: 0.06430067121982574, acc: 0.9735449552536011)
[2025-02-13 20:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:31][root][INFO] - Training Epoch: 2/2, step 5116/7134 completed (loss: 0.023284869268536568, acc: 1.0)
[2025-02-13 20:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:31][root][INFO] - Training Epoch: 2/2, step 5117/7134 completed (loss: 0.029065480455756187, acc: 0.9944751262664795)
[2025-02-13 20:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:32][root][INFO] - Training Epoch: 2/2, step 5118/7134 completed (loss: 0.07782591134309769, acc: 0.9759036302566528)
[2025-02-13 20:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:32][root][INFO] - Training Epoch: 2/2, step 5119/7134 completed (loss: 0.052923738956451416, acc: 0.9942528605461121)
[2025-02-13 20:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:33][root][INFO] - Training Epoch: 2/2, step 5120/7134 completed (loss: 0.08437429368495941, acc: 0.9788359999656677)
[2025-02-13 20:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:33][root][INFO] - Training Epoch: 2/2, step 5121/7134 completed (loss: 0.029960673302412033, acc: 0.9878787994384766)
[2025-02-13 20:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:33][root][INFO] - Training Epoch: 2/2, step 5122/7134 completed (loss: 0.16564816236495972, acc: 0.9534883499145508)
[2025-02-13 20:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:34][root][INFO] - Training Epoch: 2/2, step 5123/7134 completed (loss: 0.06532038003206253, acc: 0.9900990128517151)
[2025-02-13 20:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:34][root][INFO] - Training Epoch: 2/2, step 5124/7134 completed (loss: 0.06925726681947708, acc: 0.9852216839790344)
[2025-02-13 20:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:34][root][INFO] - Training Epoch: 2/2, step 5125/7134 completed (loss: 0.026361150667071342, acc: 0.9934640526771545)
[2025-02-13 20:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:35][root][INFO] - Training Epoch: 2/2, step 5126/7134 completed (loss: 0.07121911644935608, acc: 0.9724137783050537)
[2025-02-13 20:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:35][root][INFO] - Training Epoch: 2/2, step 5127/7134 completed (loss: 0.10535925626754761, acc: 0.977011501789093)
[2025-02-13 20:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:36][root][INFO] - Training Epoch: 2/2, step 5128/7134 completed (loss: 0.034847285598516464, acc: 0.9913793206214905)
[2025-02-13 20:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:36][root][INFO] - Training Epoch: 2/2, step 5129/7134 completed (loss: 0.08938995748758316, acc: 0.9844961166381836)
[2025-02-13 20:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:36][root][INFO] - Training Epoch: 2/2, step 5130/7134 completed (loss: 0.025839682668447495, acc: 1.0)
[2025-02-13 20:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:37][root][INFO] - Training Epoch: 2/2, step 5131/7134 completed (loss: 0.17294356226921082, acc: 0.9743589758872986)
[2025-02-13 20:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:37][root][INFO] - Training Epoch: 2/2, step 5132/7134 completed (loss: 0.07690819352865219, acc: 0.9765625)
[2025-02-13 20:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:37][root][INFO] - Training Epoch: 2/2, step 5133/7134 completed (loss: 0.0653805360198021, acc: 0.9817073345184326)
[2025-02-13 20:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:38][root][INFO] - Training Epoch: 2/2, step 5134/7134 completed (loss: 0.0854669064283371, acc: 0.97826087474823)
[2025-02-13 20:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:38][root][INFO] - Training Epoch: 2/2, step 5135/7134 completed (loss: 0.10296127945184708, acc: 0.9768785834312439)
[2025-02-13 20:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:39][root][INFO] - Training Epoch: 2/2, step 5136/7134 completed (loss: 0.059957943856716156, acc: 0.9892473220825195)
[2025-02-13 20:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:39][root][INFO] - Training Epoch: 2/2, step 5137/7134 completed (loss: 0.058142922818660736, acc: 0.97826087474823)
[2025-02-13 20:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:39][root][INFO] - Training Epoch: 2/2, step 5138/7134 completed (loss: 0.13272234797477722, acc: 0.9734042286872864)
[2025-02-13 20:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:40][root][INFO] - Training Epoch: 2/2, step 5139/7134 completed (loss: 0.04400084540247917, acc: 0.9882352948188782)
[2025-02-13 20:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:40][root][INFO] - Training Epoch: 2/2, step 5140/7134 completed (loss: 0.08121874928474426, acc: 0.9666666388511658)
[2025-02-13 20:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:41][root][INFO] - Training Epoch: 2/2, step 5141/7134 completed (loss: 0.01519443653523922, acc: 1.0)
[2025-02-13 20:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:41][root][INFO] - Training Epoch: 2/2, step 5142/7134 completed (loss: 0.024502942338585854, acc: 0.9950248599052429)
[2025-02-13 20:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:41][root][INFO] - Training Epoch: 2/2, step 5143/7134 completed (loss: 0.023526715114712715, acc: 0.9950248599052429)
[2025-02-13 20:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:42][root][INFO] - Training Epoch: 2/2, step 5144/7134 completed (loss: 0.02878657914698124, acc: 0.9894179701805115)
[2025-02-13 20:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:42][root][INFO] - Training Epoch: 2/2, step 5145/7134 completed (loss: 0.06823094934225082, acc: 0.9947916865348816)
[2025-02-13 20:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:42][root][INFO] - Training Epoch: 2/2, step 5146/7134 completed (loss: 0.08887419104576111, acc: 0.9726775884628296)
[2025-02-13 20:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:43][root][INFO] - Training Epoch: 2/2, step 5147/7134 completed (loss: 0.03566870838403702, acc: 0.9871794581413269)
[2025-02-13 20:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:43][root][INFO] - Training Epoch: 2/2, step 5148/7134 completed (loss: 0.1487579047679901, acc: 0.9624999761581421)
[2025-02-13 20:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:44][root][INFO] - Training Epoch: 2/2, step 5149/7134 completed (loss: 0.03474261611700058, acc: 0.9954751133918762)
[2025-02-13 20:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:44][root][INFO] - Training Epoch: 2/2, step 5150/7134 completed (loss: 0.10023774951696396, acc: 0.970059871673584)
[2025-02-13 20:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:44][root][INFO] - Training Epoch: 2/2, step 5151/7134 completed (loss: 0.09525851905345917, acc: 0.977142870426178)
[2025-02-13 20:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:45][root][INFO] - Training Epoch: 2/2, step 5152/7134 completed (loss: 0.03952353075146675, acc: 0.9882352948188782)
[2025-02-13 20:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:45][root][INFO] - Training Epoch: 2/2, step 5153/7134 completed (loss: 0.04932155832648277, acc: 1.0)
[2025-02-13 20:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:46][root][INFO] - Training Epoch: 2/2, step 5154/7134 completed (loss: 0.05134005472064018, acc: 0.9800000190734863)
[2025-02-13 20:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:46][root][INFO] - Training Epoch: 2/2, step 5155/7134 completed (loss: 0.11757612228393555, acc: 0.970802903175354)
[2025-02-13 20:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:46][root][INFO] - Training Epoch: 2/2, step 5156/7134 completed (loss: 0.05261329934000969, acc: 0.9768518805503845)
[2025-02-13 20:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:47][root][INFO] - Training Epoch: 2/2, step 5157/7134 completed (loss: 0.06069956719875336, acc: 0.9829545617103577)
[2025-02-13 20:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:47][root][INFO] - Training Epoch: 2/2, step 5158/7134 completed (loss: 0.12866882979869843, acc: 0.9679487347602844)
[2025-02-13 20:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:48][root][INFO] - Training Epoch: 2/2, step 5159/7134 completed (loss: 0.09571043401956558, acc: 0.9797979593276978)
[2025-02-13 20:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:48][root][INFO] - Training Epoch: 2/2, step 5160/7134 completed (loss: 0.12269481271505356, acc: 0.9666666388511658)
[2025-02-13 20:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:48][root][INFO] - Training Epoch: 2/2, step 5161/7134 completed (loss: 0.11741340905427933, acc: 0.9677419066429138)
[2025-02-13 20:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:49][root][INFO] - Training Epoch: 2/2, step 5162/7134 completed (loss: 0.06169890984892845, acc: 0.9885714054107666)
[2025-02-13 20:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:49][root][INFO] - Training Epoch: 2/2, step 5163/7134 completed (loss: 0.04963647201657295, acc: 0.9838709831237793)
[2025-02-13 20:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:50][root][INFO] - Training Epoch: 2/2, step 5164/7134 completed (loss: 0.02048211172223091, acc: 1.0)
[2025-02-13 20:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:50][root][INFO] - Training Epoch: 2/2, step 5165/7134 completed (loss: 0.07329186052083969, acc: 0.9818181991577148)
[2025-02-13 20:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:50][root][INFO] - Training Epoch: 2/2, step 5166/7134 completed (loss: 0.06580641120672226, acc: 0.9814814925193787)
[2025-02-13 20:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:51][root][INFO] - Training Epoch: 2/2, step 5167/7134 completed (loss: 0.09746016561985016, acc: 0.9668508172035217)
[2025-02-13 20:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:51][root][INFO] - Training Epoch: 2/2, step 5168/7134 completed (loss: 0.1275271773338318, acc: 0.9651162624359131)
[2025-02-13 20:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:51][root][INFO] - Training Epoch: 2/2, step 5169/7134 completed (loss: 0.10866548120975494, acc: 0.9695122241973877)
[2025-02-13 20:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:52][root][INFO] - Training Epoch: 2/2, step 5170/7134 completed (loss: 0.15659528970718384, acc: 0.9637305736541748)
[2025-02-13 20:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:52][root][INFO] - Training Epoch: 2/2, step 5171/7134 completed (loss: 0.05956433713436127, acc: 0.9837398529052734)
[2025-02-13 20:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:53][root][INFO] - Training Epoch: 2/2, step 5172/7134 completed (loss: 0.1833566278219223, acc: 0.957317054271698)
[2025-02-13 20:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:53][root][INFO] - Training Epoch: 2/2, step 5173/7134 completed (loss: 0.06460994482040405, acc: 0.9938650131225586)
[2025-02-13 20:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:53][root][INFO] - Training Epoch: 2/2, step 5174/7134 completed (loss: 0.10420241206884384, acc: 0.9826589822769165)
[2025-02-13 20:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:54][root][INFO] - Training Epoch: 2/2, step 5175/7134 completed (loss: 0.05371440201997757, acc: 0.9820359349250793)
[2025-02-13 20:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:54][root][INFO] - Training Epoch: 2/2, step 5176/7134 completed (loss: 0.05288555845618248, acc: 0.9802631735801697)
[2025-02-13 20:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:54][root][INFO] - Training Epoch: 2/2, step 5177/7134 completed (loss: 0.14555592834949493, acc: 0.9599999785423279)
[2025-02-13 20:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:55][root][INFO] - Training Epoch: 2/2, step 5178/7134 completed (loss: 0.31403255462646484, acc: 0.9466666579246521)
[2025-02-13 20:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:55][root][INFO] - Training Epoch: 2/2, step 5179/7134 completed (loss: 0.07773296535015106, acc: 0.976190447807312)
[2025-02-13 20:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:56][root][INFO] - Training Epoch: 2/2, step 5180/7134 completed (loss: 0.19173754751682281, acc: 0.9432623982429504)
[2025-02-13 20:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:56][root][INFO] - Training Epoch: 2/2, step 5181/7134 completed (loss: 0.1835707277059555, acc: 0.9550561904907227)
[2025-02-13 20:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:56][root][INFO] - Training Epoch: 2/2, step 5182/7134 completed (loss: 0.2554023563861847, acc: 0.9370078444480896)
[2025-02-13 20:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:57][root][INFO] - Training Epoch: 2/2, step 5183/7134 completed (loss: 0.14495038986206055, acc: 0.9558823704719543)
[2025-02-13 20:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:57][root][INFO] - Training Epoch: 2/2, step 5184/7134 completed (loss: 0.04029388725757599, acc: 0.9903846383094788)
[2025-02-13 20:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:57][root][INFO] - Training Epoch: 2/2, step 5185/7134 completed (loss: 0.10881586372852325, acc: 0.9801980257034302)
[2025-02-13 20:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:58][root][INFO] - Training Epoch: 2/2, step 5186/7134 completed (loss: 0.09503927081823349, acc: 0.9685039520263672)
[2025-02-13 20:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:58][root][INFO] - Training Epoch: 2/2, step 5187/7134 completed (loss: 0.03228960558772087, acc: 0.9919999837875366)
[2025-02-13 20:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:58][root][INFO] - Training Epoch: 2/2, step 5188/7134 completed (loss: 0.07226594537496567, acc: 0.9855072498321533)
[2025-02-13 20:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:59][root][INFO] - Training Epoch: 2/2, step 5189/7134 completed (loss: 0.10743429511785507, acc: 0.9900990128517151)
[2025-02-13 20:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:59][root][INFO] - Training Epoch: 2/2, step 5190/7134 completed (loss: 0.021487532183527946, acc: 0.9909090995788574)
[2025-02-13 20:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:00][root][INFO] - Training Epoch: 2/2, step 5191/7134 completed (loss: 0.1367252767086029, acc: 0.9691358208656311)
[2025-02-13 21:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:00][root][INFO] - Training Epoch: 2/2, step 5192/7134 completed (loss: 0.12517105042934418, acc: 0.9797979593276978)
[2025-02-13 21:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:00][root][INFO] - Training Epoch: 2/2, step 5193/7134 completed (loss: 0.02374330535531044, acc: 1.0)
[2025-02-13 21:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:01][root][INFO] - Training Epoch: 2/2, step 5194/7134 completed (loss: 0.049451086670160294, acc: 0.993630588054657)
[2025-02-13 21:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:01][root][INFO] - Training Epoch: 2/2, step 5195/7134 completed (loss: 0.12945659458637238, acc: 0.9814814925193787)
[2025-02-13 21:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:01][root][INFO] - Training Epoch: 2/2, step 5196/7134 completed (loss: 0.07867976278066635, acc: 0.9800000190734863)
[2025-02-13 21:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:02][root][INFO] - Training Epoch: 2/2, step 5197/7134 completed (loss: 0.11383409798145294, acc: 0.9603174328804016)
[2025-02-13 21:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:02][root][INFO] - Training Epoch: 2/2, step 5198/7134 completed (loss: 0.03927858918905258, acc: 1.0)
[2025-02-13 21:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:03][root][INFO] - Training Epoch: 2/2, step 5199/7134 completed (loss: 0.10936670750379562, acc: 0.9825581312179565)
[2025-02-13 21:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:03][root][INFO] - Training Epoch: 2/2, step 5200/7134 completed (loss: 0.0366954542696476, acc: 0.9920634627342224)
[2025-02-13 21:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:03][root][INFO] - Training Epoch: 2/2, step 5201/7134 completed (loss: 0.10876768082380295, acc: 0.9758453965187073)
[2025-02-13 21:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:04][root][INFO] - Training Epoch: 2/2, step 5202/7134 completed (loss: 0.08860573172569275, acc: 0.984375)
[2025-02-13 21:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:04][root][INFO] - Training Epoch: 2/2, step 5203/7134 completed (loss: 0.06171627342700958, acc: 0.9890109896659851)
[2025-02-13 21:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:04][root][INFO] - Training Epoch: 2/2, step 5204/7134 completed (loss: 0.02432277239859104, acc: 1.0)
[2025-02-13 21:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:05][root][INFO] - Training Epoch: 2/2, step 5205/7134 completed (loss: 0.03482140228152275, acc: 1.0)
[2025-02-13 21:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:05][root][INFO] - Training Epoch: 2/2, step 5206/7134 completed (loss: 0.21783587336540222, acc: 0.9590643048286438)
[2025-02-13 21:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:06][root][INFO] - Training Epoch: 2/2, step 5207/7134 completed (loss: 0.2680676281452179, acc: 0.9285714030265808)
[2025-02-13 21:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:06][root][INFO] - Training Epoch: 2/2, step 5208/7134 completed (loss: 0.0729290023446083, acc: 0.9885057210922241)
[2025-02-13 21:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:06][root][INFO] - Training Epoch: 2/2, step 5209/7134 completed (loss: 0.09823349863290787, acc: 0.969924807548523)
[2025-02-13 21:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:07][root][INFO] - Training Epoch: 2/2, step 5210/7134 completed (loss: 0.08300069719552994, acc: 0.9704142212867737)
[2025-02-13 21:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:07][root][INFO] - Training Epoch: 2/2, step 5211/7134 completed (loss: 0.11019832640886307, acc: 0.976190447807312)
[2025-02-13 21:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:07][root][INFO] - Training Epoch: 2/2, step 5212/7134 completed (loss: 0.08040980994701385, acc: 0.9740259647369385)
[2025-02-13 21:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:08][root][INFO] - Training Epoch: 2/2, step 5213/7134 completed (loss: 0.09855302423238754, acc: 0.9814814925193787)
[2025-02-13 21:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:08][root][INFO] - Training Epoch: 2/2, step 5214/7134 completed (loss: 0.08112873136997223, acc: 0.9915966391563416)
[2025-02-13 21:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:09][root][INFO] - Training Epoch: 2/2, step 5215/7134 completed (loss: 0.1067340150475502, acc: 0.9702970385551453)
[2025-02-13 21:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:09][root][INFO] - Training Epoch: 2/2, step 5216/7134 completed (loss: 0.048552557826042175, acc: 0.9861111044883728)
[2025-02-13 21:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:09][root][INFO] - Training Epoch: 2/2, step 5217/7134 completed (loss: 0.05897805094718933, acc: 0.989130437374115)
[2025-02-13 21:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:10][root][INFO] - Training Epoch: 2/2, step 5218/7134 completed (loss: 0.10865949094295502, acc: 0.9603960514068604)
[2025-02-13 21:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:10][root][INFO] - Training Epoch: 2/2, step 5219/7134 completed (loss: 0.1013927161693573, acc: 0.9814814925193787)
[2025-02-13 21:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:10][root][INFO] - Training Epoch: 2/2, step 5220/7134 completed (loss: 0.0574520006775856, acc: 0.9863013625144958)
[2025-02-13 21:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:11][root][INFO] - Training Epoch: 2/2, step 5221/7134 completed (loss: 0.07922198623418808, acc: 0.9876543283462524)
[2025-02-13 21:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:11][root][INFO] - Training Epoch: 2/2, step 5222/7134 completed (loss: 0.06386610865592957, acc: 0.988095223903656)
[2025-02-13 21:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:11][root][INFO] - Training Epoch: 2/2, step 5223/7134 completed (loss: 0.04784639924764633, acc: 0.982300877571106)
[2025-02-13 21:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:12][root][INFO] - Training Epoch: 2/2, step 5224/7134 completed (loss: 0.07873077690601349, acc: 0.9777777791023254)
[2025-02-13 21:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:12][root][INFO] - Training Epoch: 2/2, step 5225/7134 completed (loss: 0.12859968841075897, acc: 0.9802631735801697)
[2025-02-13 21:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:12][root][INFO] - Training Epoch: 2/2, step 5226/7134 completed (loss: 0.10477113723754883, acc: 0.9807692170143127)
[2025-02-13 21:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:13][root][INFO] - Training Epoch: 2/2, step 5227/7134 completed (loss: 0.08912178128957748, acc: 0.9897435903549194)
[2025-02-13 21:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:13][root][INFO] - Training Epoch: 2/2, step 5228/7134 completed (loss: 0.023346614092588425, acc: 1.0)
[2025-02-13 21:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:13][root][INFO] - Training Epoch: 2/2, step 5229/7134 completed (loss: 0.04374966770410538, acc: 0.9870967864990234)
[2025-02-13 21:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:14][root][INFO] - Training Epoch: 2/2, step 5230/7134 completed (loss: 0.03793903812766075, acc: 0.9944751262664795)
[2025-02-13 21:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:14][root][INFO] - Training Epoch: 2/2, step 5231/7134 completed (loss: 0.04441221430897713, acc: 0.9793814420700073)
[2025-02-13 21:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:14][root][INFO] - Training Epoch: 2/2, step 5232/7134 completed (loss: 0.05582109093666077, acc: 0.9887005686759949)
[2025-02-13 21:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:15][root][INFO] - Training Epoch: 2/2, step 5233/7134 completed (loss: 0.11499940603971481, acc: 0.9678899049758911)
[2025-02-13 21:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:15][root][INFO] - Training Epoch: 2/2, step 5234/7134 completed (loss: 0.05068770796060562, acc: 0.9894737005233765)
[2025-02-13 21:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:16][root][INFO] - Training Epoch: 2/2, step 5235/7134 completed (loss: 0.019536655396223068, acc: 0.9932432174682617)
[2025-02-13 21:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:16][root][INFO] - Training Epoch: 2/2, step 5236/7134 completed (loss: 0.023383120074868202, acc: 0.9927007555961609)
[2025-02-13 21:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:16][root][INFO] - Training Epoch: 2/2, step 5237/7134 completed (loss: 0.03003114089369774, acc: 0.9929577708244324)
[2025-02-13 21:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:17][root][INFO] - Training Epoch: 2/2, step 5238/7134 completed (loss: 0.20671990513801575, acc: 0.9776119589805603)
[2025-02-13 21:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:17][root][INFO] - Training Epoch: 2/2, step 5239/7134 completed (loss: 0.07197176665067673, acc: 0.9663865566253662)
[2025-02-13 21:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:17][root][INFO] - Training Epoch: 2/2, step 5240/7134 completed (loss: 0.03779640048742294, acc: 0.9883720874786377)
[2025-02-13 21:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:18][root][INFO] - Training Epoch: 2/2, step 5241/7134 completed (loss: 0.13987195491790771, acc: 0.9677419066429138)
[2025-02-13 21:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:18][root][INFO] - Training Epoch: 2/2, step 5242/7134 completed (loss: 0.031153468415141106, acc: 0.9934210777282715)
[2025-02-13 21:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:18][root][INFO] - Training Epoch: 2/2, step 5243/7134 completed (loss: 0.03223515674471855, acc: 0.9928571581840515)
[2025-02-13 21:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:19][root][INFO] - Training Epoch: 2/2, step 5244/7134 completed (loss: 0.06234170123934746, acc: 0.9825581312179565)
[2025-02-13 21:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:19][root][INFO] - Training Epoch: 2/2, step 5245/7134 completed (loss: 0.04725685343146324, acc: 0.9920634627342224)
[2025-02-13 21:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:20][root][INFO] - Training Epoch: 2/2, step 5246/7134 completed (loss: 0.08551033586263657, acc: 0.9683544039726257)
[2025-02-13 21:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:20][root][INFO] - Training Epoch: 2/2, step 5247/7134 completed (loss: 0.053046245127916336, acc: 0.9851852059364319)
[2025-02-13 21:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:20][root][INFO] - Training Epoch: 2/2, step 5248/7134 completed (loss: 0.06317108124494553, acc: 0.9847328066825867)
[2025-02-13 21:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:21][root][INFO] - Training Epoch: 2/2, step 5249/7134 completed (loss: 0.03642836958169937, acc: 1.0)
[2025-02-13 21:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:21][root][INFO] - Training Epoch: 2/2, step 5250/7134 completed (loss: 0.05446876212954521, acc: 0.9836956262588501)
[2025-02-13 21:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:21][root][INFO] - Training Epoch: 2/2, step 5251/7134 completed (loss: 0.03782102093100548, acc: 0.9947916865348816)
[2025-02-13 21:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:22][root][INFO] - Training Epoch: 2/2, step 5252/7134 completed (loss: 0.06784235686063766, acc: 0.982300877571106)
[2025-02-13 21:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:22][root][INFO] - Training Epoch: 2/2, step 5253/7134 completed (loss: 0.025763381272554398, acc: 0.9930555820465088)
[2025-02-13 21:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:22][root][INFO] - Training Epoch: 2/2, step 5254/7134 completed (loss: 0.16285920143127441, acc: 0.9510489702224731)
[2025-02-13 21:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:23][root][INFO] - Training Epoch: 2/2, step 5255/7134 completed (loss: 0.08757669478654861, acc: 0.977011501789093)
[2025-02-13 21:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:23][root][INFO] - Training Epoch: 2/2, step 5256/7134 completed (loss: 0.06237814575433731, acc: 0.9866666793823242)
[2025-02-13 21:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:24][root][INFO] - Training Epoch: 2/2, step 5257/7134 completed (loss: 0.13595877587795258, acc: 0.9640718698501587)
[2025-02-13 21:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:24][root][INFO] - Training Epoch: 2/2, step 5258/7134 completed (loss: 0.13887161016464233, acc: 0.9629629850387573)
[2025-02-13 21:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:24][root][INFO] - Training Epoch: 2/2, step 5259/7134 completed (loss: 0.16843974590301514, acc: 0.971222996711731)
[2025-02-13 21:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:25][root][INFO] - Training Epoch: 2/2, step 5260/7134 completed (loss: 0.04621896892786026, acc: 0.9947090148925781)
[2025-02-13 21:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:25][root][INFO] - Training Epoch: 2/2, step 5261/7134 completed (loss: 0.11856862157583237, acc: 0.9720279574394226)
[2025-02-13 21:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:25][root][INFO] - Training Epoch: 2/2, step 5262/7134 completed (loss: 0.032630424946546555, acc: 0.9939758777618408)
[2025-02-13 21:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:26][root][INFO] - Training Epoch: 2/2, step 5263/7134 completed (loss: 0.056569360196590424, acc: 0.9788732528686523)
[2025-02-13 21:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:26][root][INFO] - Training Epoch: 2/2, step 5264/7134 completed (loss: 0.06341437995433807, acc: 0.9795918464660645)
[2025-02-13 21:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:27][root][INFO] - Training Epoch: 2/2, step 5265/7134 completed (loss: 0.06145131215453148, acc: 0.9781420826911926)
[2025-02-13 21:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:27][root][INFO] - Training Epoch: 2/2, step 5266/7134 completed (loss: 0.03254104405641556, acc: 0.9938271641731262)
[2025-02-13 21:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:27][root][INFO] - Training Epoch: 2/2, step 5267/7134 completed (loss: 0.14968106150627136, acc: 0.9567901492118835)
[2025-02-13 21:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:28][root][INFO] - Training Epoch: 2/2, step 5268/7134 completed (loss: 0.09387757629156113, acc: 0.9767441749572754)
[2025-02-13 21:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:28][root][INFO] - Training Epoch: 2/2, step 5269/7134 completed (loss: 0.0677664577960968, acc: 0.9857142567634583)
[2025-02-13 21:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:28][root][INFO] - Training Epoch: 2/2, step 5270/7134 completed (loss: 0.07935384660959244, acc: 0.982758641242981)
[2025-02-13 21:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:29][root][INFO] - Training Epoch: 2/2, step 5271/7134 completed (loss: 0.059031251817941666, acc: 0.9856114983558655)
[2025-02-13 21:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:29][root][INFO] - Training Epoch: 2/2, step 5272/7134 completed (loss: 0.03942827135324478, acc: 0.9942196607589722)
[2025-02-13 21:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:29][root][INFO] - Training Epoch: 2/2, step 5273/7134 completed (loss: 0.10202280431985855, acc: 0.9652777910232544)
[2025-02-13 21:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:30][root][INFO] - Training Epoch: 2/2, step 5274/7134 completed (loss: 0.11767326295375824, acc: 0.9743589758872986)
[2025-02-13 21:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:30][root][INFO] - Training Epoch: 2/2, step 5275/7134 completed (loss: 0.06506592780351639, acc: 0.9918699264526367)
[2025-02-13 21:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:31][root][INFO] - Training Epoch: 2/2, step 5276/7134 completed (loss: 0.09544463455677032, acc: 0.9671052694320679)
[2025-02-13 21:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:31][root][INFO] - Training Epoch: 2/2, step 5277/7134 completed (loss: 0.04708531126379967, acc: 0.987261176109314)
[2025-02-13 21:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:31][root][INFO] - Training Epoch: 2/2, step 5278/7134 completed (loss: 0.10422318428754807, acc: 0.9696969985961914)
[2025-02-13 21:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:32][root][INFO] - Training Epoch: 2/2, step 5279/7134 completed (loss: 0.014004169031977654, acc: 1.0)
[2025-02-13 21:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:32][root][INFO] - Training Epoch: 2/2, step 5280/7134 completed (loss: 0.036257293075323105, acc: 0.9857142567634583)
[2025-02-13 21:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:32][root][INFO] - Training Epoch: 2/2, step 5281/7134 completed (loss: 0.06144900992512703, acc: 0.97826087474823)
[2025-02-13 21:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:33][root][INFO] - Training Epoch: 2/2, step 5282/7134 completed (loss: 0.10059860348701477, acc: 0.9825581312179565)
[2025-02-13 21:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:33][root][INFO] - Training Epoch: 2/2, step 5283/7134 completed (loss: 0.06016882508993149, acc: 0.9890710115432739)
[2025-02-13 21:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:33][root][INFO] - Training Epoch: 2/2, step 5284/7134 completed (loss: 0.18255434930324554, acc: 0.987500011920929)
[2025-02-13 21:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:34][root][INFO] - Training Epoch: 2/2, step 5285/7134 completed (loss: 0.1317129135131836, acc: 0.9569892287254333)
[2025-02-13 21:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:34][root][INFO] - Training Epoch: 2/2, step 5286/7134 completed (loss: 0.10278366506099701, acc: 0.9865771532058716)
[2025-02-13 21:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:35][root][INFO] - Training Epoch: 2/2, step 5287/7134 completed (loss: 0.09576880931854248, acc: 0.9747474789619446)
[2025-02-13 21:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:35][root][INFO] - Training Epoch: 2/2, step 5288/7134 completed (loss: 0.0484277680516243, acc: 0.9937499761581421)
[2025-02-13 21:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:35][root][INFO] - Training Epoch: 2/2, step 5289/7134 completed (loss: 0.05251335725188255, acc: 0.9886363744735718)
[2025-02-13 21:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:36][root][INFO] - Training Epoch: 2/2, step 5290/7134 completed (loss: 0.09689532220363617, acc: 0.9735449552536011)
[2025-02-13 21:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:36][root][INFO] - Training Epoch: 2/2, step 5291/7134 completed (loss: 0.022014226764440536, acc: 0.9945945739746094)
[2025-02-13 21:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:36][root][INFO] - Training Epoch: 2/2, step 5292/7134 completed (loss: 0.031152259558439255, acc: 0.994350254535675)
[2025-02-13 21:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:37][root][INFO] - Training Epoch: 2/2, step 5293/7134 completed (loss: 0.026879262179136276, acc: 0.993630588054657)
[2025-02-13 21:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:37][root][INFO] - Training Epoch: 2/2, step 5294/7134 completed (loss: 0.05255326256155968, acc: 0.9881656765937805)
[2025-02-13 21:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:37][root][INFO] - Training Epoch: 2/2, step 5295/7134 completed (loss: 0.09745912253856659, acc: 0.9922480583190918)
[2025-02-13 21:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:38][root][INFO] - Training Epoch: 2/2, step 5296/7134 completed (loss: 0.04878056421875954, acc: 0.9887005686759949)
[2025-02-13 21:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:38][root][INFO] - Training Epoch: 2/2, step 5297/7134 completed (loss: 0.061441414058208466, acc: 0.9879518151283264)
[2025-02-13 21:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:39][root][INFO] - Training Epoch: 2/2, step 5298/7134 completed (loss: 0.04065548628568649, acc: 0.9929078221321106)
[2025-02-13 21:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:39][root][INFO] - Training Epoch: 2/2, step 5299/7134 completed (loss: 0.03448811545968056, acc: 0.9931507110595703)
[2025-02-13 21:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:39][root][INFO] - Training Epoch: 2/2, step 5300/7134 completed (loss: 0.01749458909034729, acc: 0.9939758777618408)
[2025-02-13 21:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:40][root][INFO] - Training Epoch: 2/2, step 5301/7134 completed (loss: 0.036448948085308075, acc: 0.9817073345184326)
[2025-02-13 21:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:40][root][INFO] - Training Epoch: 2/2, step 5302/7134 completed (loss: 0.11185579001903534, acc: 0.9720670580863953)
[2025-02-13 21:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:40][root][INFO] - Training Epoch: 2/2, step 5303/7134 completed (loss: 0.20093829929828644, acc: 0.9469696879386902)
[2025-02-13 21:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:41][root][INFO] - Training Epoch: 2/2, step 5304/7134 completed (loss: 0.2768992483615875, acc: 0.9452054500579834)
[2025-02-13 21:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:41][root][INFO] - Training Epoch: 2/2, step 5305/7134 completed (loss: 0.09471471607685089, acc: 0.96875)
[2025-02-13 21:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:42][root][INFO] - Training Epoch: 2/2, step 5306/7134 completed (loss: 0.06984437257051468, acc: 0.9833333492279053)
[2025-02-13 21:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:42][root][INFO] - Training Epoch: 2/2, step 5307/7134 completed (loss: 0.2672251760959625, acc: 0.9473684430122375)
[2025-02-13 21:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:42][root][INFO] - Training Epoch: 2/2, step 5308/7134 completed (loss: 0.11097173392772675, acc: 0.9691358208656311)
[2025-02-13 21:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:43][root][INFO] - Training Epoch: 2/2, step 5309/7134 completed (loss: 0.124264657497406, acc: 0.9790209531784058)
[2025-02-13 21:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:43][root][INFO] - Training Epoch: 2/2, step 5310/7134 completed (loss: 0.1421372890472412, acc: 0.9659090638160706)
[2025-02-13 21:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:43][root][INFO] - Training Epoch: 2/2, step 5311/7134 completed (loss: 0.1849154382944107, acc: 0.95652174949646)
[2025-02-13 21:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:44][root][INFO] - Training Epoch: 2/2, step 5312/7134 completed (loss: 0.16835978627204895, acc: 0.9682539701461792)
[2025-02-13 21:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:44][root][INFO] - Training Epoch: 2/2, step 5313/7134 completed (loss: 0.07873863726854324, acc: 0.9918032884597778)
[2025-02-13 21:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:45][root][INFO] - Training Epoch: 2/2, step 5314/7134 completed (loss: 0.19599147140979767, acc: 0.9428571462631226)
[2025-02-13 21:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:45][root][INFO] - Training Epoch: 2/2, step 5315/7134 completed (loss: 0.10092665255069733, acc: 0.9784172773361206)
[2025-02-13 21:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:45][root][INFO] - Training Epoch: 2/2, step 5316/7134 completed (loss: 0.06861031800508499, acc: 0.9933333396911621)
[2025-02-13 21:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:46][root][INFO] - Training Epoch: 2/2, step 5317/7134 completed (loss: 0.05025898665189743, acc: 0.9920634627342224)
[2025-02-13 21:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:46][root][INFO] - Training Epoch: 2/2, step 5318/7134 completed (loss: 0.04609840363264084, acc: 0.993630588054657)
[2025-02-13 21:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:46][root][INFO] - Training Epoch: 2/2, step 5319/7134 completed (loss: 0.14594139158725739, acc: 0.9548872113227844)
[2025-02-13 21:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:47][root][INFO] - Training Epoch: 2/2, step 5320/7134 completed (loss: 0.20396026968955994, acc: 0.9496855139732361)
[2025-02-13 21:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:47][root][INFO] - Training Epoch: 2/2, step 5321/7134 completed (loss: 0.08399680256843567, acc: 0.9805194735527039)
[2025-02-13 21:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:48][root][INFO] - Training Epoch: 2/2, step 5322/7134 completed (loss: 0.04795940965414047, acc: 0.9800000190734863)
[2025-02-13 21:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:48][root][INFO] - Training Epoch: 2/2, step 5323/7134 completed (loss: 0.11149090528488159, acc: 0.9731543660163879)
[2025-02-13 21:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:48][root][INFO] - Training Epoch: 2/2, step 5324/7134 completed (loss: 0.07942953705787659, acc: 0.9802631735801697)
[2025-02-13 21:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:49][root][INFO] - Training Epoch: 2/2, step 5325/7134 completed (loss: 0.09094901382923126, acc: 0.9710144996643066)
[2025-02-13 21:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:49][root][INFO] - Training Epoch: 2/2, step 5326/7134 completed (loss: 0.13769039511680603, acc: 0.9655172228813171)
[2025-02-13 21:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:49][root][INFO] - Training Epoch: 2/2, step 5327/7134 completed (loss: 0.08072080463171005, acc: 0.9612902998924255)
[2025-02-13 21:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:50][root][INFO] - Training Epoch: 2/2, step 5328/7134 completed (loss: 0.04191068559885025, acc: 1.0)
[2025-02-13 21:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:50][root][INFO] - Training Epoch: 2/2, step 5329/7134 completed (loss: 0.13908445835113525, acc: 0.9800000190734863)
[2025-02-13 21:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:50][root][INFO] - Training Epoch: 2/2, step 5330/7134 completed (loss: 0.080106221139431, acc: 0.9873417615890503)
[2025-02-13 21:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:51][root][INFO] - Training Epoch: 2/2, step 5331/7134 completed (loss: 0.0846620500087738, acc: 0.9851852059364319)
[2025-02-13 21:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:51][root][INFO] - Training Epoch: 2/2, step 5332/7134 completed (loss: 0.06468106061220169, acc: 0.9863013625144958)
[2025-02-13 21:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:52][root][INFO] - Training Epoch: 2/2, step 5333/7134 completed (loss: 0.05297427996993065, acc: 0.9868420958518982)
[2025-02-13 21:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:52][root][INFO] - Training Epoch: 2/2, step 5334/7134 completed (loss: 0.09933875501155853, acc: 0.9785714149475098)
[2025-02-13 21:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:52][root][INFO] - Training Epoch: 2/2, step 5335/7134 completed (loss: 0.0523466132581234, acc: 0.9932885766029358)
[2025-02-13 21:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:53][root][INFO] - Training Epoch: 2/2, step 5336/7134 completed (loss: 0.0702570304274559, acc: 0.9929577708244324)
[2025-02-13 21:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:53][root][INFO] - Training Epoch: 2/2, step 5337/7134 completed (loss: 0.09484241902828217, acc: 0.9719626307487488)
[2025-02-13 21:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:53][root][INFO] - Training Epoch: 2/2, step 5338/7134 completed (loss: 0.05490460246801376, acc: 0.9857819676399231)
[2025-02-13 21:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:54][root][INFO] - Training Epoch: 2/2, step 5339/7134 completed (loss: 0.060393448919057846, acc: 0.9890109896659851)
[2025-02-13 21:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:54][root][INFO] - Training Epoch: 2/2, step 5340/7134 completed (loss: 0.15198536217212677, acc: 0.9597315192222595)
[2025-02-13 21:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:54][root][INFO] - Training Epoch: 2/2, step 5341/7134 completed (loss: 0.053140949457883835, acc: 0.9900000095367432)
[2025-02-13 21:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:55][root][INFO] - Training Epoch: 2/2, step 5342/7134 completed (loss: 0.15587200224399567, acc: 0.9578947424888611)
[2025-02-13 21:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:55][root][INFO] - Training Epoch: 2/2, step 5343/7134 completed (loss: 0.14107726514339447, acc: 0.9729729890823364)
[2025-02-13 21:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:56][root][INFO] - Training Epoch: 2/2, step 5344/7134 completed (loss: 0.11957159638404846, acc: 0.9605262875556946)
[2025-02-13 21:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:56][root][INFO] - Training Epoch: 2/2, step 5345/7134 completed (loss: 0.1270461231470108, acc: 0.9595959782600403)
[2025-02-13 21:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:56][root][INFO] - Training Epoch: 2/2, step 5346/7134 completed (loss: 0.09066801518201828, acc: 0.9816513657569885)
[2025-02-13 21:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:52][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2299, device='cuda:0') eval_epoch_loss=tensor(0.2069, device='cuda:0') eval_epoch_acc=tensor(0.9509, device='cuda:0')
[2025-02-13 21:04:52][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 21:04:52][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 21:04:52][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_5347_loss_0.20693475008010864/model.pt
[2025-02-13 21:04:52][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 21:04:52][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.20693475008010864
[2025-02-13 21:04:52][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9508558511734009
[2025-02-13 21:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:53][root][INFO] - Training Epoch: 2/2, step 5347/7134 completed (loss: 0.10720323026180267, acc: 0.9731183052062988)
[2025-02-13 21:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:53][root][INFO] - Training Epoch: 2/2, step 5348/7134 completed (loss: 0.17219415307044983, acc: 0.9698795080184937)
[2025-02-13 21:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:53][root][INFO] - Training Epoch: 2/2, step 5349/7134 completed (loss: 0.1334761679172516, acc: 0.9714285731315613)
[2025-02-13 21:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:54][root][INFO] - Training Epoch: 2/2, step 5350/7134 completed (loss: 0.07088460773229599, acc: 0.9764705896377563)
[2025-02-13 21:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:54][root][INFO] - Training Epoch: 2/2, step 5351/7134 completed (loss: 0.10984334349632263, acc: 0.9567567706108093)
[2025-02-13 21:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:54][root][INFO] - Training Epoch: 2/2, step 5352/7134 completed (loss: 0.053149543702602386, acc: 0.9820359349250793)
[2025-02-13 21:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:55][root][INFO] - Training Epoch: 2/2, step 5353/7134 completed (loss: 0.07992479205131531, acc: 0.9747474789619446)
[2025-02-13 21:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:55][root][INFO] - Training Epoch: 2/2, step 5354/7134 completed (loss: 0.10864392668008804, acc: 0.9629629850387573)
[2025-02-13 21:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:56][root][INFO] - Training Epoch: 2/2, step 5355/7134 completed (loss: 0.1371096521615982, acc: 0.9635416865348816)
[2025-02-13 21:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:56][root][INFO] - Training Epoch: 2/2, step 5356/7134 completed (loss: 0.11687419563531876, acc: 0.9627659320831299)
[2025-02-13 21:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:56][root][INFO] - Training Epoch: 2/2, step 5357/7134 completed (loss: 0.06358592957258224, acc: 0.9858155846595764)
[2025-02-13 21:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:57][root][INFO] - Training Epoch: 2/2, step 5358/7134 completed (loss: 0.30891796946525574, acc: 0.9340659379959106)
[2025-02-13 21:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:57][root][INFO] - Training Epoch: 2/2, step 5359/7134 completed (loss: 0.13391701877117157, acc: 0.9704433679580688)
[2025-02-13 21:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:58][root][INFO] - Training Epoch: 2/2, step 5360/7134 completed (loss: 0.05231022462248802, acc: 0.9885057210922241)
[2025-02-13 21:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:58][root][INFO] - Training Epoch: 2/2, step 5361/7134 completed (loss: 0.14002715051174164, acc: 0.9644669890403748)
[2025-02-13 21:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:58][root][INFO] - Training Epoch: 2/2, step 5362/7134 completed (loss: 0.20382818579673767, acc: 0.9729729890823364)
[2025-02-13 21:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:59][root][INFO] - Training Epoch: 2/2, step 5363/7134 completed (loss: 0.08794055879116058, acc: 0.9817073345184326)
[2025-02-13 21:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:59][root][INFO] - Training Epoch: 2/2, step 5364/7134 completed (loss: 0.05547080188989639, acc: 0.9884393215179443)
[2025-02-13 21:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:00][root][INFO] - Training Epoch: 2/2, step 5365/7134 completed (loss: 0.04338425397872925, acc: 0.9924242496490479)
[2025-02-13 21:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:00][root][INFO] - Training Epoch: 2/2, step 5366/7134 completed (loss: 0.19434186816215515, acc: 0.9701492786407471)
[2025-02-13 21:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:00][root][INFO] - Training Epoch: 2/2, step 5367/7134 completed (loss: 0.12889616191387177, acc: 0.9728260636329651)
[2025-02-13 21:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:01][root][INFO] - Training Epoch: 2/2, step 5368/7134 completed (loss: 0.08351083844900131, acc: 0.9689922332763672)
[2025-02-13 21:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:01][root][INFO] - Training Epoch: 2/2, step 5369/7134 completed (loss: 0.10089823603630066, acc: 0.9830508232116699)
[2025-02-13 21:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:02][root][INFO] - Training Epoch: 2/2, step 5370/7134 completed (loss: 0.16562722623348236, acc: 0.9740259647369385)
[2025-02-13 21:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:02][root][INFO] - Training Epoch: 2/2, step 5371/7134 completed (loss: 0.1935551017522812, acc: 0.9599999785423279)
[2025-02-13 21:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:02][root][INFO] - Training Epoch: 2/2, step 5372/7134 completed (loss: 0.03371693566441536, acc: 1.0)
[2025-02-13 21:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:03][root][INFO] - Training Epoch: 2/2, step 5373/7134 completed (loss: 0.04931483417749405, acc: 0.9920634627342224)
[2025-02-13 21:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:03][root][INFO] - Training Epoch: 2/2, step 5374/7134 completed (loss: 0.010063678957521915, acc: 1.0)
[2025-02-13 21:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:04][root][INFO] - Training Epoch: 2/2, step 5375/7134 completed (loss: 0.04834117740392685, acc: 0.9922480583190918)
[2025-02-13 21:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:04][root][INFO] - Training Epoch: 2/2, step 5376/7134 completed (loss: 0.042707763612270355, acc: 0.9797297120094299)
[2025-02-13 21:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:04][root][INFO] - Training Epoch: 2/2, step 5377/7134 completed (loss: 0.055578380823135376, acc: 0.9876543283462524)
[2025-02-13 21:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:05][root][INFO] - Training Epoch: 2/2, step 5378/7134 completed (loss: 0.025264911353588104, acc: 0.9947368502616882)
[2025-02-13 21:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:05][root][INFO] - Training Epoch: 2/2, step 5379/7134 completed (loss: 0.027985133230686188, acc: 0.9895833134651184)
[2025-02-13 21:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:05][root][INFO] - Training Epoch: 2/2, step 5380/7134 completed (loss: 0.06497377157211304, acc: 0.9851852059364319)
[2025-02-13 21:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:06][root][INFO] - Training Epoch: 2/2, step 5381/7134 completed (loss: 0.04797183722257614, acc: 0.9830508232116699)
[2025-02-13 21:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:06][root][INFO] - Training Epoch: 2/2, step 5382/7134 completed (loss: 0.03128645941615105, acc: 0.9855072498321533)
[2025-02-13 21:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:07][root][INFO] - Training Epoch: 2/2, step 5383/7134 completed (loss: 0.049374498426914215, acc: 0.9919354915618896)
[2025-02-13 21:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:07][root][INFO] - Training Epoch: 2/2, step 5384/7134 completed (loss: 0.03398476913571358, acc: 0.9949238300323486)
[2025-02-13 21:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:07][root][INFO] - Training Epoch: 2/2, step 5385/7134 completed (loss: 0.05321839079260826, acc: 0.9883720874786377)
[2025-02-13 21:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:08][root][INFO] - Training Epoch: 2/2, step 5386/7134 completed (loss: 0.022398486733436584, acc: 1.0)
[2025-02-13 21:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:08][root][INFO] - Training Epoch: 2/2, step 5387/7134 completed (loss: 0.01872452348470688, acc: 1.0)
[2025-02-13 21:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:08][root][INFO] - Training Epoch: 2/2, step 5388/7134 completed (loss: 0.01989785209298134, acc: 1.0)
[2025-02-13 21:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:09][root][INFO] - Training Epoch: 2/2, step 5389/7134 completed (loss: 0.015991197898983955, acc: 1.0)
[2025-02-13 21:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:09][root][INFO] - Training Epoch: 2/2, step 5390/7134 completed (loss: 0.07411512732505798, acc: 0.9685534834861755)
[2025-02-13 21:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:10][root][INFO] - Training Epoch: 2/2, step 5391/7134 completed (loss: 0.04935663565993309, acc: 0.9928057789802551)
[2025-02-13 21:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:10][root][INFO] - Training Epoch: 2/2, step 5392/7134 completed (loss: 0.08057239651679993, acc: 0.9886363744735718)
[2025-02-13 21:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:11][root][INFO] - Training Epoch: 2/2, step 5393/7134 completed (loss: 0.0839909017086029, acc: 0.9750000238418579)
[2025-02-13 21:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:11][root][INFO] - Training Epoch: 2/2, step 5394/7134 completed (loss: 0.20325621962547302, acc: 0.96875)
[2025-02-13 21:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:11][root][INFO] - Training Epoch: 2/2, step 5395/7134 completed (loss: 0.04321826994419098, acc: 0.9932432174682617)
[2025-02-13 21:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:12][root][INFO] - Training Epoch: 2/2, step 5396/7134 completed (loss: 0.034352052956819534, acc: 1.0)
[2025-02-13 21:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:12][root][INFO] - Training Epoch: 2/2, step 5397/7134 completed (loss: 0.05915599688887596, acc: 0.9716312289237976)
[2025-02-13 21:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:13][root][INFO] - Training Epoch: 2/2, step 5398/7134 completed (loss: 0.08375836163759232, acc: 0.9759036302566528)
[2025-02-13 21:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:13][root][INFO] - Training Epoch: 2/2, step 5399/7134 completed (loss: 0.07608847320079803, acc: 0.9714285731315613)
[2025-02-13 21:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:13][root][INFO] - Training Epoch: 2/2, step 5400/7134 completed (loss: 0.19207175076007843, acc: 0.9724137783050537)
[2025-02-13 21:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:14][root][INFO] - Training Epoch: 2/2, step 5401/7134 completed (loss: 0.06370201706886292, acc: 0.9861111044883728)
[2025-02-13 21:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:14][root][INFO] - Training Epoch: 2/2, step 5402/7134 completed (loss: 0.021701877936720848, acc: 1.0)
[2025-02-13 21:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:14][root][INFO] - Training Epoch: 2/2, step 5403/7134 completed (loss: 0.10930114984512329, acc: 0.9836065769195557)
[2025-02-13 21:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:15][root][INFO] - Training Epoch: 2/2, step 5404/7134 completed (loss: 0.18151627480983734, acc: 0.9664429426193237)
[2025-02-13 21:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:15][root][INFO] - Training Epoch: 2/2, step 5405/7134 completed (loss: 0.035088542848825455, acc: 0.991304337978363)
[2025-02-13 21:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:15][root][INFO] - Training Epoch: 2/2, step 5406/7134 completed (loss: 0.06850587576627731, acc: 0.9819819927215576)
[2025-02-13 21:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:16][root][INFO] - Training Epoch: 2/2, step 5407/7134 completed (loss: 0.03799992799758911, acc: 1.0)
[2025-02-13 21:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:16][root][INFO] - Training Epoch: 2/2, step 5408/7134 completed (loss: 0.15823493897914886, acc: 0.9716312289237976)
[2025-02-13 21:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:17][root][INFO] - Training Epoch: 2/2, step 5409/7134 completed (loss: 0.21106091141700745, acc: 0.9720279574394226)
[2025-02-13 21:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:17][root][INFO] - Training Epoch: 2/2, step 5410/7134 completed (loss: 0.0520436055958271, acc: 0.9879518151283264)
[2025-02-13 21:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:17][root][INFO] - Training Epoch: 2/2, step 5411/7134 completed (loss: 0.09721200913190842, acc: 0.9788732528686523)
[2025-02-13 21:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:18][root][INFO] - Training Epoch: 2/2, step 5412/7134 completed (loss: 0.16983596980571747, acc: 0.982300877571106)
[2025-02-13 21:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:18][root][INFO] - Training Epoch: 2/2, step 5413/7134 completed (loss: 0.1212148368358612, acc: 0.9583333134651184)
[2025-02-13 21:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:18][root][INFO] - Training Epoch: 2/2, step 5414/7134 completed (loss: 0.20080961287021637, acc: 0.9341317415237427)
[2025-02-13 21:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:19][root][INFO] - Training Epoch: 2/2, step 5415/7134 completed (loss: 0.16321787238121033, acc: 0.9805194735527039)
[2025-02-13 21:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:19][root][INFO] - Training Epoch: 2/2, step 5416/7134 completed (loss: 0.13021725416183472, acc: 0.9679144620895386)
[2025-02-13 21:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:19][root][INFO] - Training Epoch: 2/2, step 5417/7134 completed (loss: 0.03990525007247925, acc: 0.9935897588729858)
[2025-02-13 21:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:20][root][INFO] - Training Epoch: 2/2, step 5418/7134 completed (loss: 0.1241278350353241, acc: 0.9375)
[2025-02-13 21:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:20][root][INFO] - Training Epoch: 2/2, step 5419/7134 completed (loss: 0.21747614443302155, acc: 0.9343065619468689)
[2025-02-13 21:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:21][root][INFO] - Training Epoch: 2/2, step 5420/7134 completed (loss: 0.2122712880373001, acc: 0.9624060392379761)
[2025-02-13 21:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:21][root][INFO] - Training Epoch: 2/2, step 5421/7134 completed (loss: 0.15789344906806946, acc: 0.9437500238418579)
[2025-02-13 21:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:21][root][INFO] - Training Epoch: 2/2, step 5422/7134 completed (loss: 0.055619094520807266, acc: 1.0)
[2025-02-13 21:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:22][root][INFO] - Training Epoch: 2/2, step 5423/7134 completed (loss: 0.049670521169900894, acc: 0.987500011920929)
[2025-02-13 21:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:22][root][INFO] - Training Epoch: 2/2, step 5424/7134 completed (loss: 0.0730455294251442, acc: 0.9769230484962463)
[2025-02-13 21:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:22][root][INFO] - Training Epoch: 2/2, step 5425/7134 completed (loss: 0.0833481177687645, acc: 0.9765625)
[2025-02-13 21:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:23][root][INFO] - Training Epoch: 2/2, step 5426/7134 completed (loss: 0.1346537172794342, acc: 0.9836065769195557)
[2025-02-13 21:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:23][root][INFO] - Training Epoch: 2/2, step 5427/7134 completed (loss: 0.046623919159173965, acc: 1.0)
[2025-02-13 21:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:24][root][INFO] - Training Epoch: 2/2, step 5428/7134 completed (loss: 0.05657980591058731, acc: 0.9862068891525269)
[2025-02-13 21:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:24][root][INFO] - Training Epoch: 2/2, step 5429/7134 completed (loss: 0.06464657187461853, acc: 0.9798657894134521)
[2025-02-13 21:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:24][root][INFO] - Training Epoch: 2/2, step 5430/7134 completed (loss: 0.13523177802562714, acc: 0.9740259647369385)
[2025-02-13 21:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:25][root][INFO] - Training Epoch: 2/2, step 5431/7134 completed (loss: 0.08118898421525955, acc: 0.9624060392379761)
[2025-02-13 21:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:25][root][INFO] - Training Epoch: 2/2, step 5432/7134 completed (loss: 0.04110662266612053, acc: 0.9765625)
[2025-02-13 21:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:26][root][INFO] - Training Epoch: 2/2, step 5433/7134 completed (loss: 0.14521478116512299, acc: 0.9831932783126831)
[2025-02-13 21:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:26][root][INFO] - Training Epoch: 2/2, step 5434/7134 completed (loss: 0.06934770941734314, acc: 0.9677419066429138)
[2025-02-13 21:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:26][root][INFO] - Training Epoch: 2/2, step 5435/7134 completed (loss: 0.06766896694898605, acc: 0.9919999837875366)
[2025-02-13 21:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:27][root][INFO] - Training Epoch: 2/2, step 5436/7134 completed (loss: 0.0683882087469101, acc: 0.9741935729980469)
[2025-02-13 21:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:27][root][INFO] - Training Epoch: 2/2, step 5437/7134 completed (loss: 0.07179182767868042, acc: 0.9849624037742615)
[2025-02-13 21:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:28][root][INFO] - Training Epoch: 2/2, step 5438/7134 completed (loss: 0.10890626162290573, acc: 0.9666666388511658)
[2025-02-13 21:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:28][root][INFO] - Training Epoch: 2/2, step 5439/7134 completed (loss: 0.16664153337478638, acc: 0.9636363387107849)
[2025-02-13 21:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:28][root][INFO] - Training Epoch: 2/2, step 5440/7134 completed (loss: 0.12564700841903687, acc: 0.9666666388511658)
[2025-02-13 21:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:29][root][INFO] - Training Epoch: 2/2, step 5441/7134 completed (loss: 0.09569476544857025, acc: 0.9783783555030823)
[2025-02-13 21:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:29][root][INFO] - Training Epoch: 2/2, step 5442/7134 completed (loss: 0.06517194956541061, acc: 0.988095223903656)
[2025-02-13 21:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:30][root][INFO] - Training Epoch: 2/2, step 5443/7134 completed (loss: 0.059435877948999405, acc: 0.9939024448394775)
[2025-02-13 21:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:30][root][INFO] - Training Epoch: 2/2, step 5444/7134 completed (loss: 0.059795793145895004, acc: 0.9779411554336548)
[2025-02-13 21:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:30][root][INFO] - Training Epoch: 2/2, step 5445/7134 completed (loss: 0.12657897174358368, acc: 0.9779411554336548)
[2025-02-13 21:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:31][root][INFO] - Training Epoch: 2/2, step 5446/7134 completed (loss: 0.06709984689950943, acc: 0.9815950989723206)
[2025-02-13 21:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:31][root][INFO] - Training Epoch: 2/2, step 5447/7134 completed (loss: 0.0772474855184555, acc: 0.9726027250289917)
[2025-02-13 21:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:31][root][INFO] - Training Epoch: 2/2, step 5448/7134 completed (loss: 0.12664157152175903, acc: 0.9670329689979553)
[2025-02-13 21:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:32][root][INFO] - Training Epoch: 2/2, step 5449/7134 completed (loss: 0.08275267481803894, acc: 0.9836065769195557)
[2025-02-13 21:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:32][root][INFO] - Training Epoch: 2/2, step 5450/7134 completed (loss: 0.07856433838605881, acc: 0.9777777791023254)
[2025-02-13 21:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:33][root][INFO] - Training Epoch: 2/2, step 5451/7134 completed (loss: 0.03648250177502632, acc: 0.9940828680992126)
[2025-02-13 21:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:33][root][INFO] - Training Epoch: 2/2, step 5452/7134 completed (loss: 0.0546112060546875, acc: 0.9822485446929932)
[2025-02-13 21:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:34][root][INFO] - Training Epoch: 2/2, step 5453/7134 completed (loss: 0.1413644701242447, acc: 0.9777777791023254)
[2025-02-13 21:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:34][root][INFO] - Training Epoch: 2/2, step 5454/7134 completed (loss: 0.08074302971363068, acc: 0.9945651888847351)
[2025-02-13 21:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:34][root][INFO] - Training Epoch: 2/2, step 5455/7134 completed (loss: 0.10986892879009247, acc: 0.9839572310447693)
[2025-02-13 21:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:35][root][INFO] - Training Epoch: 2/2, step 5456/7134 completed (loss: 0.07400036603212357, acc: 0.9784946441650391)
[2025-02-13 21:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:35][root][INFO] - Training Epoch: 2/2, step 5457/7134 completed (loss: 0.08854172378778458, acc: 0.9885057210922241)
[2025-02-13 21:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:36][root][INFO] - Training Epoch: 2/2, step 5458/7134 completed (loss: 0.08449438214302063, acc: 0.9944751262664795)
[2025-02-13 21:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:36][root][INFO] - Training Epoch: 2/2, step 5459/7134 completed (loss: 0.03602439910173416, acc: 0.9815950989723206)
[2025-02-13 21:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:37][root][INFO] - Training Epoch: 2/2, step 5460/7134 completed (loss: 0.06339874863624573, acc: 0.9822485446929932)
[2025-02-13 21:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:37][root][INFO] - Training Epoch: 2/2, step 5461/7134 completed (loss: 0.11599394679069519, acc: 0.963302731513977)
[2025-02-13 21:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:37][root][INFO] - Training Epoch: 2/2, step 5462/7134 completed (loss: 0.04127953201532364, acc: 0.9938650131225586)
[2025-02-13 21:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:38][root][INFO] - Training Epoch: 2/2, step 5463/7134 completed (loss: 0.07532592117786407, acc: 0.9691358208656311)
[2025-02-13 21:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:38][root][INFO] - Training Epoch: 2/2, step 5464/7134 completed (loss: 0.08365673571825027, acc: 0.9781420826911926)
[2025-02-13 21:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:38][root][INFO] - Training Epoch: 2/2, step 5465/7134 completed (loss: 0.07563517242670059, acc: 0.981249988079071)
[2025-02-13 21:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:39][root][INFO] - Training Epoch: 2/2, step 5466/7134 completed (loss: 0.0826835185289383, acc: 0.9751243591308594)
[2025-02-13 21:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:39][root][INFO] - Training Epoch: 2/2, step 5467/7134 completed (loss: 0.03835131973028183, acc: 0.9950248599052429)
[2025-02-13 21:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:40][root][INFO] - Training Epoch: 2/2, step 5468/7134 completed (loss: 0.026159420609474182, acc: 0.9947643876075745)
[2025-02-13 21:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:40][root][INFO] - Training Epoch: 2/2, step 5469/7134 completed (loss: 0.08898889273405075, acc: 0.9682539701461792)
[2025-02-13 21:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:40][root][INFO] - Training Epoch: 2/2, step 5470/7134 completed (loss: 0.017028409987688065, acc: 1.0)
[2025-02-13 21:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:41][root][INFO] - Training Epoch: 2/2, step 5471/7134 completed (loss: 0.02955791726708412, acc: 0.9941176176071167)
[2025-02-13 21:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:41][root][INFO] - Training Epoch: 2/2, step 5472/7134 completed (loss: 0.03112187422811985, acc: 0.994413435459137)
[2025-02-13 21:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:42][root][INFO] - Training Epoch: 2/2, step 5473/7134 completed (loss: 0.019085047766566277, acc: 0.9949495196342468)
[2025-02-13 21:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:42][root][INFO] - Training Epoch: 2/2, step 5474/7134 completed (loss: 0.03957033157348633, acc: 0.9878787994384766)
[2025-02-13 21:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:42][root][INFO] - Training Epoch: 2/2, step 5475/7134 completed (loss: 0.04804026708006859, acc: 0.9814814925193787)
[2025-02-13 21:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:43][root][INFO] - Training Epoch: 2/2, step 5476/7134 completed (loss: 0.004063095431774855, acc: 1.0)
[2025-02-13 21:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:43][root][INFO] - Training Epoch: 2/2, step 5477/7134 completed (loss: 0.019663115963339806, acc: 1.0)
[2025-02-13 21:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:43][root][INFO] - Training Epoch: 2/2, step 5478/7134 completed (loss: 0.0546257421374321, acc: 0.9851852059364319)
[2025-02-13 21:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:44][root][INFO] - Training Epoch: 2/2, step 5479/7134 completed (loss: 0.09441160410642624, acc: 0.9797297120094299)
[2025-02-13 21:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:44][root][INFO] - Training Epoch: 2/2, step 5480/7134 completed (loss: 0.10821940749883652, acc: 0.9702380895614624)
[2025-02-13 21:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:45][root][INFO] - Training Epoch: 2/2, step 5481/7134 completed (loss: 0.053986772894859314, acc: 0.9836065769195557)
[2025-02-13 21:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:45][root][INFO] - Training Epoch: 2/2, step 5482/7134 completed (loss: 0.20785510540008545, acc: 0.9587628841400146)
[2025-02-13 21:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:46][root][INFO] - Training Epoch: 2/2, step 5483/7134 completed (loss: 0.08951041847467422, acc: 0.9757575988769531)
[2025-02-13 21:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:46][root][INFO] - Training Epoch: 2/2, step 5484/7134 completed (loss: 0.06723392754793167, acc: 0.9876543283462524)
[2025-02-13 21:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:46][root][INFO] - Training Epoch: 2/2, step 5485/7134 completed (loss: 0.09835844486951828, acc: 0.975806474685669)
[2025-02-13 21:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:47][root][INFO] - Training Epoch: 2/2, step 5486/7134 completed (loss: 0.0753689631819725, acc: 0.9772727489471436)
[2025-02-13 21:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:47][root][INFO] - Training Epoch: 2/2, step 5487/7134 completed (loss: 0.060971565544605255, acc: 0.9795918464660645)
[2025-02-13 21:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:48][root][INFO] - Training Epoch: 2/2, step 5488/7134 completed (loss: 0.09722261130809784, acc: 0.976047933101654)
[2025-02-13 21:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:48][root][INFO] - Training Epoch: 2/2, step 5489/7134 completed (loss: 0.1820470541715622, acc: 0.9512194991111755)
[2025-02-13 21:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:48][root][INFO] - Training Epoch: 2/2, step 5490/7134 completed (loss: 0.045623332262039185, acc: 0.9898989796638489)
[2025-02-13 21:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:49][root][INFO] - Training Epoch: 2/2, step 5491/7134 completed (loss: 0.06649086624383926, acc: 0.9824561476707458)
[2025-02-13 21:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:49][root][INFO] - Training Epoch: 2/2, step 5492/7134 completed (loss: 0.06986847519874573, acc: 0.9850746393203735)
[2025-02-13 21:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:49][root][INFO] - Training Epoch: 2/2, step 5493/7134 completed (loss: 0.15790602564811707, acc: 0.9635416865348816)
[2025-02-13 21:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:50][root][INFO] - Training Epoch: 2/2, step 5494/7134 completed (loss: 0.06202097237110138, acc: 0.9874213933944702)
[2025-02-13 21:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:50][root][INFO] - Training Epoch: 2/2, step 5495/7134 completed (loss: 0.03346879780292511, acc: 1.0)
[2025-02-13 21:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:51][root][INFO] - Training Epoch: 2/2, step 5496/7134 completed (loss: 0.020380329340696335, acc: 1.0)
[2025-02-13 21:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:51][root][INFO] - Training Epoch: 2/2, step 5497/7134 completed (loss: 0.10080224275588989, acc: 0.970588207244873)
[2025-02-13 21:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:51][root][INFO] - Training Epoch: 2/2, step 5498/7134 completed (loss: 0.12758032977581024, acc: 0.9627659320831299)
[2025-02-13 21:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:52][root][INFO] - Training Epoch: 2/2, step 5499/7134 completed (loss: 0.07488100975751877, acc: 0.9819819927215576)
[2025-02-13 21:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:52][root][INFO] - Training Epoch: 2/2, step 5500/7134 completed (loss: 0.061898160725831985, acc: 0.9792746305465698)
[2025-02-13 21:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:52][root][INFO] - Training Epoch: 2/2, step 5501/7134 completed (loss: 0.10355213284492493, acc: 0.9696969985961914)
[2025-02-13 21:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:53][root][INFO] - Training Epoch: 2/2, step 5502/7134 completed (loss: 0.07982398569583893, acc: 0.9774011373519897)
[2025-02-13 21:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:53][root][INFO] - Training Epoch: 2/2, step 5503/7134 completed (loss: 0.1173446998000145, acc: 0.9846153855323792)
[2025-02-13 21:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:54][root][INFO] - Training Epoch: 2/2, step 5504/7134 completed (loss: 0.02944079041481018, acc: 0.9879518151283264)
[2025-02-13 21:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:54][root][INFO] - Training Epoch: 2/2, step 5505/7134 completed (loss: 0.1694398671388626, acc: 0.9659090638160706)
[2025-02-13 21:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:54][root][INFO] - Training Epoch: 2/2, step 5506/7134 completed (loss: 0.09089537709951401, acc: 0.9797979593276978)
[2025-02-13 21:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:55][root][INFO] - Training Epoch: 2/2, step 5507/7134 completed (loss: 0.06987123936414719, acc: 0.9824561476707458)
[2025-02-13 21:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:55][root][INFO] - Training Epoch: 2/2, step 5508/7134 completed (loss: 0.05660906806588173, acc: 0.9952152967453003)
[2025-02-13 21:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:55][root][INFO] - Training Epoch: 2/2, step 5509/7134 completed (loss: 0.13735699653625488, acc: 0.971563994884491)
[2025-02-13 21:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:56][root][INFO] - Training Epoch: 2/2, step 5510/7134 completed (loss: 0.11679064482450485, acc: 0.9677419066429138)
[2025-02-13 21:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:56][root][INFO] - Training Epoch: 2/2, step 5511/7134 completed (loss: 0.16614744067192078, acc: 0.9660193920135498)
[2025-02-13 21:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:57][root][INFO] - Training Epoch: 2/2, step 5512/7134 completed (loss: 0.13567017018795013, acc: 0.9560439586639404)
[2025-02-13 21:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:57][root][INFO] - Training Epoch: 2/2, step 5513/7134 completed (loss: 0.10751239210367203, acc: 0.9704142212867737)
[2025-02-13 21:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:57][root][INFO] - Training Epoch: 2/2, step 5514/7134 completed (loss: 0.17592506110668182, acc: 0.9583333134651184)
[2025-02-13 21:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:58][root][INFO] - Training Epoch: 2/2, step 5515/7134 completed (loss: 0.2783355116844177, acc: 0.9642857313156128)
[2025-02-13 21:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:58][root][INFO] - Training Epoch: 2/2, step 5516/7134 completed (loss: 0.10458210855722427, acc: 0.9885057210922241)
[2025-02-13 21:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:58][root][INFO] - Training Epoch: 2/2, step 5517/7134 completed (loss: 0.07986214011907578, acc: 0.9888268113136292)
[2025-02-13 21:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:59][root][INFO] - Training Epoch: 2/2, step 5518/7134 completed (loss: 0.06706629693508148, acc: 0.9777777791023254)
[2025-02-13 21:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:59][root][INFO] - Training Epoch: 2/2, step 5519/7134 completed (loss: 0.08075851202011108, acc: 0.9753086566925049)
[2025-02-13 21:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:00][root][INFO] - Training Epoch: 2/2, step 5520/7134 completed (loss: 0.06749606132507324, acc: 0.9905213117599487)
[2025-02-13 21:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:00][root][INFO] - Training Epoch: 2/2, step 5521/7134 completed (loss: 0.1079171895980835, acc: 0.9738219976425171)
[2025-02-13 21:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:00][root][INFO] - Training Epoch: 2/2, step 5522/7134 completed (loss: 0.12534672021865845, acc: 0.963350772857666)
[2025-02-13 21:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:01][root][INFO] - Training Epoch: 2/2, step 5523/7134 completed (loss: 0.0968976691365242, acc: 0.9670329689979553)
[2025-02-13 21:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:01][root][INFO] - Training Epoch: 2/2, step 5524/7134 completed (loss: 0.1566838175058365, acc: 0.9620253443717957)
[2025-02-13 21:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:01][root][INFO] - Training Epoch: 2/2, step 5525/7134 completed (loss: 0.04477696865797043, acc: 0.9878048896789551)
[2025-02-13 21:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:02][root][INFO] - Training Epoch: 2/2, step 5526/7134 completed (loss: 0.05151665583252907, acc: 0.9813664555549622)
[2025-02-13 21:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:02][root][INFO] - Training Epoch: 2/2, step 5527/7134 completed (loss: 0.03859192878007889, acc: 0.9930070042610168)
[2025-02-13 21:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:02][root][INFO] - Training Epoch: 2/2, step 5528/7134 completed (loss: 0.07379579544067383, acc: 0.9848484992980957)
[2025-02-13 21:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:03][root][INFO] - Training Epoch: 2/2, step 5529/7134 completed (loss: 0.10725800693035126, acc: 0.9636363387107849)
[2025-02-13 21:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:03][root][INFO] - Training Epoch: 2/2, step 5530/7134 completed (loss: 0.06603170186281204, acc: 0.9863945841789246)
[2025-02-13 21:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:04][root][INFO] - Training Epoch: 2/2, step 5531/7134 completed (loss: 0.06912785768508911, acc: 0.9939758777618408)
[2025-02-13 21:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:04][root][INFO] - Training Epoch: 2/2, step 5532/7134 completed (loss: 0.08804137259721756, acc: 0.9774011373519897)
[2025-02-13 21:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:05][root][INFO] - Training Epoch: 2/2, step 5533/7134 completed (loss: 0.02916240692138672, acc: 0.9888888597488403)
[2025-02-13 21:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:05][root][INFO] - Training Epoch: 2/2, step 5534/7134 completed (loss: 0.12861108779907227, acc: 0.966292142868042)
[2025-02-13 21:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:05][root][INFO] - Training Epoch: 2/2, step 5535/7134 completed (loss: 0.041963301599025726, acc: 0.9896373152732849)
[2025-02-13 21:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:06][root][INFO] - Training Epoch: 2/2, step 5536/7134 completed (loss: 0.0682193711400032, acc: 0.9890710115432739)
[2025-02-13 21:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:06][root][INFO] - Training Epoch: 2/2, step 5537/7134 completed (loss: 0.040483761578798294, acc: 0.9885057210922241)
[2025-02-13 21:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:07][root][INFO] - Training Epoch: 2/2, step 5538/7134 completed (loss: 0.07496453821659088, acc: 0.9842932224273682)
[2025-02-13 21:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:07][root][INFO] - Training Epoch: 2/2, step 5539/7134 completed (loss: 0.07738010585308075, acc: 0.9781420826911926)
[2025-02-13 21:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:07][root][INFO] - Training Epoch: 2/2, step 5540/7134 completed (loss: 0.1096397340297699, acc: 0.9759036302566528)
[2025-02-13 21:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:08][root][INFO] - Training Epoch: 2/2, step 5541/7134 completed (loss: 0.03181162476539612, acc: 1.0)
[2025-02-13 21:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:08][root][INFO] - Training Epoch: 2/2, step 5542/7134 completed (loss: 0.014480415731668472, acc: 1.0)
[2025-02-13 21:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:08][root][INFO] - Training Epoch: 2/2, step 5543/7134 completed (loss: 0.06029466167092323, acc: 0.9893048405647278)
[2025-02-13 21:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:09][root][INFO] - Training Epoch: 2/2, step 5544/7134 completed (loss: 0.024236438795924187, acc: 0.994535505771637)
[2025-02-13 21:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:09][root][INFO] - Training Epoch: 2/2, step 5545/7134 completed (loss: 0.06212134659290314, acc: 0.988095223903656)
[2025-02-13 21:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:10][root][INFO] - Training Epoch: 2/2, step 5546/7134 completed (loss: 0.03180467709898949, acc: 1.0)
[2025-02-13 21:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:10][root][INFO] - Training Epoch: 2/2, step 5547/7134 completed (loss: 0.028021393343806267, acc: 1.0)
[2025-02-13 21:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:11][root][INFO] - Training Epoch: 2/2, step 5548/7134 completed (loss: 0.034653980284929276, acc: 0.9941176176071167)
[2025-02-13 21:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:11][root][INFO] - Training Epoch: 2/2, step 5549/7134 completed (loss: 0.08496973663568497, acc: 0.9798657894134521)
[2025-02-13 21:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:11][root][INFO] - Training Epoch: 2/2, step 5550/7134 completed (loss: 0.0436449870467186, acc: 0.9887640476226807)
[2025-02-13 21:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:12][root][INFO] - Training Epoch: 2/2, step 5551/7134 completed (loss: 0.16162221133708954, acc: 0.9677419066429138)
[2025-02-13 21:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:12][root][INFO] - Training Epoch: 2/2, step 5552/7134 completed (loss: 0.039379678666591644, acc: 0.9938650131225586)
[2025-02-13 21:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:12][root][INFO] - Training Epoch: 2/2, step 5553/7134 completed (loss: 0.09823314845561981, acc: 0.976331353187561)
[2025-02-13 21:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:13][root][INFO] - Training Epoch: 2/2, step 5554/7134 completed (loss: 0.041077543050050735, acc: 0.993630588054657)
[2025-02-13 21:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:13][root][INFO] - Training Epoch: 2/2, step 5555/7134 completed (loss: 0.03286783769726753, acc: 1.0)
[2025-02-13 21:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:13][root][INFO] - Training Epoch: 2/2, step 5556/7134 completed (loss: 0.07128968834877014, acc: 0.9720670580863953)
[2025-02-13 21:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:14][root][INFO] - Training Epoch: 2/2, step 5557/7134 completed (loss: 0.06087564304471016, acc: 0.9875776171684265)
[2025-02-13 21:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:14][root][INFO] - Training Epoch: 2/2, step 5558/7134 completed (loss: 0.0575091615319252, acc: 0.9777777791023254)
[2025-02-13 21:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:15][root][INFO] - Training Epoch: 2/2, step 5559/7134 completed (loss: 0.11198994517326355, acc: 0.9776536226272583)
[2025-02-13 21:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:15][root][INFO] - Training Epoch: 2/2, step 5560/7134 completed (loss: 0.100589320063591, acc: 0.9777777791023254)
[2025-02-13 21:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:15][root][INFO] - Training Epoch: 2/2, step 5561/7134 completed (loss: 0.07969190925359726, acc: 0.9754098653793335)
[2025-02-13 21:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:16][root][INFO] - Training Epoch: 2/2, step 5562/7134 completed (loss: 0.15086477994918823, acc: 0.9640287756919861)
[2025-02-13 21:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:16][root][INFO] - Training Epoch: 2/2, step 5563/7134 completed (loss: 0.045623525977134705, acc: 0.9861111044883728)
[2025-02-13 21:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:17][root][INFO] - Training Epoch: 2/2, step 5564/7134 completed (loss: 0.02618136629462242, acc: 0.987500011920929)
[2025-02-13 21:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:17][root][INFO] - Training Epoch: 2/2, step 5565/7134 completed (loss: 0.008857965469360352, acc: 1.0)
[2025-02-13 21:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:17][root][INFO] - Training Epoch: 2/2, step 5566/7134 completed (loss: 0.025980239734053612, acc: 0.9949238300323486)
[2025-02-13 21:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:18][root][INFO] - Training Epoch: 2/2, step 5567/7134 completed (loss: 0.030252834782004356, acc: 0.9900497794151306)
[2025-02-13 21:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:18][root][INFO] - Training Epoch: 2/2, step 5568/7134 completed (loss: 0.02630951814353466, acc: 0.9945054650306702)
[2025-02-13 21:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:18][root][INFO] - Training Epoch: 2/2, step 5569/7134 completed (loss: 0.030417615547776222, acc: 0.9895833134651184)
[2025-02-13 21:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:19][root][INFO] - Training Epoch: 2/2, step 5570/7134 completed (loss: 0.04475267976522446, acc: 0.989130437374115)
[2025-02-13 21:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:19][root][INFO] - Training Epoch: 2/2, step 5571/7134 completed (loss: 0.04114307463169098, acc: 0.9936708807945251)
[2025-02-13 21:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:19][root][INFO] - Training Epoch: 2/2, step 5572/7134 completed (loss: 0.06467913091182709, acc: 0.9938650131225586)
[2025-02-13 21:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:20][root][INFO] - Training Epoch: 2/2, step 5573/7134 completed (loss: 0.05792834609746933, acc: 0.9869281053543091)
[2025-02-13 21:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:20][root][INFO] - Training Epoch: 2/2, step 5574/7134 completed (loss: 0.020201507955789566, acc: 1.0)
[2025-02-13 21:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:21][root][INFO] - Training Epoch: 2/2, step 5575/7134 completed (loss: 0.06483588367700577, acc: 0.9933333396911621)
[2025-02-13 21:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:21][root][INFO] - Training Epoch: 2/2, step 5576/7134 completed (loss: 0.03460663929581642, acc: 0.9940476417541504)
[2025-02-13 21:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:21][root][INFO] - Training Epoch: 2/2, step 5577/7134 completed (loss: 0.08777347207069397, acc: 0.9851852059364319)
[2025-02-13 21:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:22][root][INFO] - Training Epoch: 2/2, step 5578/7134 completed (loss: 0.037616681307554245, acc: 0.9936708807945251)
[2025-02-13 21:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:22][root][INFO] - Training Epoch: 2/2, step 5579/7134 completed (loss: 0.05578257143497467, acc: 0.9815950989723206)
[2025-02-13 21:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:23][root][INFO] - Training Epoch: 2/2, step 5580/7134 completed (loss: 0.042677901685237885, acc: 0.9833333492279053)
[2025-02-13 21:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:23][root][INFO] - Training Epoch: 2/2, step 5581/7134 completed (loss: 0.04587921127676964, acc: 0.9878787994384766)
[2025-02-13 21:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:23][root][INFO] - Training Epoch: 2/2, step 5582/7134 completed (loss: 0.04319440573453903, acc: 0.988950252532959)
[2025-02-13 21:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:24][root][INFO] - Training Epoch: 2/2, step 5583/7134 completed (loss: 0.03422706574201584, acc: 0.9949748516082764)
[2025-02-13 21:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:24][root][INFO] - Training Epoch: 2/2, step 5584/7134 completed (loss: 0.063797727227211, acc: 0.9810126423835754)
[2025-02-13 21:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:24][root][INFO] - Training Epoch: 2/2, step 5585/7134 completed (loss: 0.07983275502920151, acc: 0.9756097793579102)
[2025-02-13 21:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:25][root][INFO] - Training Epoch: 2/2, step 5586/7134 completed (loss: 0.11027194559574127, acc: 0.9931972622871399)
[2025-02-13 21:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:25][root][INFO] - Training Epoch: 2/2, step 5587/7134 completed (loss: 0.045038655400276184, acc: 0.9828571677207947)
[2025-02-13 21:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:26][root][INFO] - Training Epoch: 2/2, step 5588/7134 completed (loss: 0.033468395471572876, acc: 0.9876543283462524)
[2025-02-13 21:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:26][root][INFO] - Training Epoch: 2/2, step 5589/7134 completed (loss: 0.084182009100914, acc: 0.9875776171684265)
[2025-02-13 21:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:26][root][INFO] - Training Epoch: 2/2, step 5590/7134 completed (loss: 0.0924733355641365, acc: 0.9710982441902161)
[2025-02-13 21:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:27][root][INFO] - Training Epoch: 2/2, step 5591/7134 completed (loss: 0.04555648937821388, acc: 0.9942196607589722)
[2025-02-13 21:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:27][root][INFO] - Training Epoch: 2/2, step 5592/7134 completed (loss: 0.046997908502817154, acc: 0.9941860437393188)
[2025-02-13 21:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:27][root][INFO] - Training Epoch: 2/2, step 5593/7134 completed (loss: 0.048059239983558655, acc: 0.9836956262588501)
[2025-02-13 21:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:28][root][INFO] - Training Epoch: 2/2, step 5594/7134 completed (loss: 0.03034631535410881, acc: 0.9886363744735718)
[2025-02-13 21:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:28][root][INFO] - Training Epoch: 2/2, step 5595/7134 completed (loss: 0.04079189896583557, acc: 0.9829545617103577)
[2025-02-13 21:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:28][root][INFO] - Training Epoch: 2/2, step 5596/7134 completed (loss: 0.03618267923593521, acc: 1.0)
[2025-02-13 21:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:29][root][INFO] - Training Epoch: 2/2, step 5597/7134 completed (loss: 0.07704408466815948, acc: 0.9819276928901672)
[2025-02-13 21:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:29][root][INFO] - Training Epoch: 2/2, step 5598/7134 completed (loss: 0.07177801430225372, acc: 0.9884393215179443)
[2025-02-13 21:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:30][root][INFO] - Training Epoch: 2/2, step 5599/7134 completed (loss: 0.11426973342895508, acc: 0.9727891087532043)
[2025-02-13 21:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:30][root][INFO] - Training Epoch: 2/2, step 5600/7134 completed (loss: 0.0322536826133728, acc: 1.0)
[2025-02-13 21:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:30][root][INFO] - Training Epoch: 2/2, step 5601/7134 completed (loss: 0.052985627204179764, acc: 0.9847328066825867)
[2025-02-13 21:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:31][root][INFO] - Training Epoch: 2/2, step 5602/7134 completed (loss: 0.04282493144273758, acc: 0.9929577708244324)
[2025-02-13 21:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:31][root][INFO] - Training Epoch: 2/2, step 5603/7134 completed (loss: 0.07499698549509048, acc: 0.9754098653793335)
[2025-02-13 21:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:31][root][INFO] - Training Epoch: 2/2, step 5604/7134 completed (loss: 0.08246730268001556, acc: 0.9918699264526367)
[2025-02-13 21:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:32][root][INFO] - Training Epoch: 2/2, step 5605/7134 completed (loss: 0.06512419879436493, acc: 0.9763779640197754)
[2025-02-13 21:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:32][root][INFO] - Training Epoch: 2/2, step 5606/7134 completed (loss: 0.07073013484477997, acc: 0.9813084006309509)
[2025-02-13 21:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:32][root][INFO] - Training Epoch: 2/2, step 5607/7134 completed (loss: 0.044791996479034424, acc: 0.9856114983558655)
[2025-02-13 21:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:33][root][INFO] - Training Epoch: 2/2, step 5608/7134 completed (loss: 0.04622362181544304, acc: 0.9922480583190918)
[2025-02-13 21:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:33][root][INFO] - Training Epoch: 2/2, step 5609/7134 completed (loss: 0.04072771593928337, acc: 0.9833333492279053)
[2025-02-13 21:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:33][root][INFO] - Training Epoch: 2/2, step 5610/7134 completed (loss: 0.046487465500831604, acc: 0.9909090995788574)
[2025-02-13 21:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:34][root][INFO] - Training Epoch: 2/2, step 5611/7134 completed (loss: 0.04162660241127014, acc: 0.9925373196601868)
[2025-02-13 21:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:34][root][INFO] - Training Epoch: 2/2, step 5612/7134 completed (loss: 0.0932290330529213, acc: 0.9788732528686523)
[2025-02-13 21:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:35][root][INFO] - Training Epoch: 2/2, step 5613/7134 completed (loss: 0.05082259327173233, acc: 0.9821428656578064)
[2025-02-13 21:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:35][root][INFO] - Training Epoch: 2/2, step 5614/7134 completed (loss: 0.12108764052391052, acc: 0.9865771532058716)
[2025-02-13 21:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:35][root][INFO] - Training Epoch: 2/2, step 5615/7134 completed (loss: 0.043109484016895294, acc: 1.0)
[2025-02-13 21:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:36][root][INFO] - Training Epoch: 2/2, step 5616/7134 completed (loss: 0.09798778593540192, acc: 0.9777777791023254)
[2025-02-13 21:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:36][root][INFO] - Training Epoch: 2/2, step 5617/7134 completed (loss: 0.03965598717331886, acc: 0.9922480583190918)
[2025-02-13 21:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:36][root][INFO] - Training Epoch: 2/2, step 5618/7134 completed (loss: 0.03696290776133537, acc: 1.0)
[2025-02-13 21:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:37][root][INFO] - Training Epoch: 2/2, step 5619/7134 completed (loss: 0.0759611502289772, acc: 0.9747899174690247)
[2025-02-13 21:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:37][root][INFO] - Training Epoch: 2/2, step 5620/7134 completed (loss: 0.04009140282869339, acc: 1.0)
[2025-02-13 21:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:38][root][INFO] - Training Epoch: 2/2, step 5621/7134 completed (loss: 0.04388072341680527, acc: 0.9935483932495117)
[2025-02-13 21:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:38][root][INFO] - Training Epoch: 2/2, step 5622/7134 completed (loss: 0.026659958064556122, acc: 0.9946523904800415)
[2025-02-13 21:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:38][root][INFO] - Training Epoch: 2/2, step 5623/7134 completed (loss: 0.06312202662229538, acc: 0.9861111044883728)
[2025-02-13 21:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:39][root][INFO] - Training Epoch: 2/2, step 5624/7134 completed (loss: 0.03709251433610916, acc: 0.9934640526771545)
[2025-02-13 21:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:39][root][INFO] - Training Epoch: 2/2, step 5625/7134 completed (loss: 0.025472993031144142, acc: 1.0)
[2025-02-13 21:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:40][root][INFO] - Training Epoch: 2/2, step 5626/7134 completed (loss: 0.02416745387017727, acc: 0.9931034445762634)
[2025-02-13 21:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:40][root][INFO] - Training Epoch: 2/2, step 5627/7134 completed (loss: 0.10143980383872986, acc: 0.9750000238418579)
[2025-02-13 21:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:40][root][INFO] - Training Epoch: 2/2, step 5628/7134 completed (loss: 0.0507231168448925, acc: 0.9910714030265808)
[2025-02-13 21:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:41][root][INFO] - Training Epoch: 2/2, step 5629/7134 completed (loss: 0.06301302462816238, acc: 0.9836065769195557)
[2025-02-13 21:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:41][root][INFO] - Training Epoch: 2/2, step 5630/7134 completed (loss: 0.07633097469806671, acc: 0.9803921580314636)
[2025-02-13 21:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:41][root][INFO] - Training Epoch: 2/2, step 5631/7134 completed (loss: 0.051859937608242035, acc: 0.9837398529052734)
[2025-02-13 21:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:42][root][INFO] - Training Epoch: 2/2, step 5632/7134 completed (loss: 0.07373479008674622, acc: 0.9807692170143127)
[2025-02-13 21:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:42][root][INFO] - Training Epoch: 2/2, step 5633/7134 completed (loss: 0.02597932144999504, acc: 0.9910714030265808)
[2025-02-13 21:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:43][root][INFO] - Training Epoch: 2/2, step 5634/7134 completed (loss: 0.04248255491256714, acc: 0.982300877571106)
[2025-02-13 21:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:43][root][INFO] - Training Epoch: 2/2, step 5635/7134 completed (loss: 0.04303508996963501, acc: 0.984375)
[2025-02-13 21:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:43][root][INFO] - Training Epoch: 2/2, step 5636/7134 completed (loss: 0.042297348380088806, acc: 0.9923076629638672)
[2025-02-13 21:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:44][root][INFO] - Training Epoch: 2/2, step 5637/7134 completed (loss: 0.025834165513515472, acc: 1.0)
[2025-02-13 21:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:44][root][INFO] - Training Epoch: 2/2, step 5638/7134 completed (loss: 0.05082735791802406, acc: 0.9824561476707458)
[2025-02-13 21:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:45][root][INFO] - Training Epoch: 2/2, step 5639/7134 completed (loss: 0.06198953837156296, acc: 0.9805194735527039)
[2025-02-13 21:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:45][root][INFO] - Training Epoch: 2/2, step 5640/7134 completed (loss: 0.11757435649633408, acc: 0.9645389914512634)
[2025-02-13 21:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:45][root][INFO] - Training Epoch: 2/2, step 5641/7134 completed (loss: 0.06596875190734863, acc: 0.9887640476226807)
[2025-02-13 21:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:46][root][INFO] - Training Epoch: 2/2, step 5642/7134 completed (loss: 0.02385784313082695, acc: 1.0)
[2025-02-13 21:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:46][root][INFO] - Training Epoch: 2/2, step 5643/7134 completed (loss: 0.09378015249967575, acc: 0.9789473414421082)
[2025-02-13 21:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:46][root][INFO] - Training Epoch: 2/2, step 5644/7134 completed (loss: 0.04905347526073456, acc: 0.9904761910438538)
[2025-02-13 21:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:47][root][INFO] - Training Epoch: 2/2, step 5645/7134 completed (loss: 0.029264777898788452, acc: 1.0)
[2025-02-13 21:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:47][root][INFO] - Training Epoch: 2/2, step 5646/7134 completed (loss: 0.02097753994166851, acc: 0.9928571581840515)
[2025-02-13 21:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:48][root][INFO] - Training Epoch: 2/2, step 5647/7134 completed (loss: 0.03552291914820671, acc: 1.0)
[2025-02-13 21:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:48][root][INFO] - Training Epoch: 2/2, step 5648/7134 completed (loss: 0.18750359117984772, acc: 0.9666666388511658)
[2025-02-13 21:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:48][root][INFO] - Training Epoch: 2/2, step 5649/7134 completed (loss: 0.06422444432973862, acc: 0.9797297120094299)
[2025-02-13 21:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:49][root][INFO] - Training Epoch: 2/2, step 5650/7134 completed (loss: 0.043902285397052765, acc: 0.9838709831237793)
[2025-02-13 21:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:49][root][INFO] - Training Epoch: 2/2, step 5651/7134 completed (loss: 0.04673362150788307, acc: 0.9931972622871399)
[2025-02-13 21:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:50][root][INFO] - Training Epoch: 2/2, step 5652/7134 completed (loss: 0.04810774698853493, acc: 0.9928057789802551)
[2025-02-13 21:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:50][root][INFO] - Training Epoch: 2/2, step 5653/7134 completed (loss: 0.04454195871949196, acc: 0.9870129823684692)
[2025-02-13 21:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:50][root][INFO] - Training Epoch: 2/2, step 5654/7134 completed (loss: 0.016122832894325256, acc: 1.0)
[2025-02-13 21:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:51][root][INFO] - Training Epoch: 2/2, step 5655/7134 completed (loss: 0.02324388548731804, acc: 1.0)
[2025-02-13 21:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:51][root][INFO] - Training Epoch: 2/2, step 5656/7134 completed (loss: 0.07576534152030945, acc: 0.9754098653793335)
[2025-02-13 21:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:51][root][INFO] - Training Epoch: 2/2, step 5657/7134 completed (loss: 0.15274573862552643, acc: 0.9668874144554138)
[2025-02-13 21:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:52][root][INFO] - Training Epoch: 2/2, step 5658/7134 completed (loss: 0.06775736063718796, acc: 0.9869281053543091)
[2025-02-13 21:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:52][root][INFO] - Training Epoch: 2/2, step 5659/7134 completed (loss: 0.04086223989725113, acc: 0.9928571581840515)
[2025-02-13 21:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:53][root][INFO] - Training Epoch: 2/2, step 5660/7134 completed (loss: 0.05717376247048378, acc: 0.9921875)
[2025-02-13 21:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:53][root][INFO] - Training Epoch: 2/2, step 5661/7134 completed (loss: 0.0657348558306694, acc: 0.9915966391563416)
[2025-02-13 21:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:53][root][INFO] - Training Epoch: 2/2, step 5662/7134 completed (loss: 0.06899706274271011, acc: 0.9856114983558655)
[2025-02-13 21:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:54][root][INFO] - Training Epoch: 2/2, step 5663/7134 completed (loss: 0.026413004845380783, acc: 1.0)
[2025-02-13 21:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:54][root][INFO] - Training Epoch: 2/2, step 5664/7134 completed (loss: 0.11525070667266846, acc: 0.9794520735740662)
[2025-02-13 21:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:54][root][INFO] - Training Epoch: 2/2, step 5665/7134 completed (loss: 0.02406136877834797, acc: 1.0)
[2025-02-13 21:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:55][root][INFO] - Training Epoch: 2/2, step 5666/7134 completed (loss: 0.05860525742173195, acc: 0.9869281053543091)
[2025-02-13 21:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:55][root][INFO] - Training Epoch: 2/2, step 5667/7134 completed (loss: 0.06570049375295639, acc: 0.9756097793579102)
[2025-02-13 21:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:56][root][INFO] - Training Epoch: 2/2, step 5668/7134 completed (loss: 0.11794311553239822, acc: 0.9777777791023254)
[2025-02-13 21:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:56][root][INFO] - Training Epoch: 2/2, step 5669/7134 completed (loss: 0.09619265049695969, acc: 0.987261176109314)
[2025-02-13 21:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:56][root][INFO] - Training Epoch: 2/2, step 5670/7134 completed (loss: 0.034754130989313126, acc: 0.9920634627342224)
[2025-02-13 21:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:57][root][INFO] - Training Epoch: 2/2, step 5671/7134 completed (loss: 0.041127707809209824, acc: 0.9943820238113403)
[2025-02-13 21:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:57][root][INFO] - Training Epoch: 2/2, step 5672/7134 completed (loss: 0.0669025182723999, acc: 0.9851852059364319)
[2025-02-13 21:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:57][root][INFO] - Training Epoch: 2/2, step 5673/7134 completed (loss: 0.08065395057201385, acc: 0.9794520735740662)
[2025-02-13 21:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:58][root][INFO] - Training Epoch: 2/2, step 5674/7134 completed (loss: 0.08762680739164352, acc: 0.9727891087532043)
[2025-02-13 21:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:58][root][INFO] - Training Epoch: 2/2, step 5675/7134 completed (loss: 0.13710784912109375, acc: 0.9666666388511658)
[2025-02-13 21:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:59][root][INFO] - Training Epoch: 2/2, step 5676/7134 completed (loss: 0.1729050576686859, acc: 0.9655172228813171)
[2025-02-13 21:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:59][root][INFO] - Training Epoch: 2/2, step 5677/7134 completed (loss: 0.19089238345623016, acc: 0.9505494236946106)
[2025-02-13 21:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:59][root][INFO] - Training Epoch: 2/2, step 5678/7134 completed (loss: 0.062418509274721146, acc: 0.9876543283462524)
[2025-02-13 21:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:00][root][INFO] - Training Epoch: 2/2, step 5679/7134 completed (loss: 0.21797628700733185, acc: 0.9620253443717957)
[2025-02-13 21:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:00][root][INFO] - Training Epoch: 2/2, step 5680/7134 completed (loss: 0.19595648348331451, acc: 0.9619565010070801)
[2025-02-13 21:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:01][root][INFO] - Training Epoch: 2/2, step 5681/7134 completed (loss: 0.12358658760786057, acc: 0.9636363387107849)
[2025-02-13 21:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:01][root][INFO] - Training Epoch: 2/2, step 5682/7134 completed (loss: 0.0758746787905693, acc: 0.9879518151283264)
[2025-02-13 21:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:01][root][INFO] - Training Epoch: 2/2, step 5683/7134 completed (loss: 0.10408773273229599, acc: 0.9777777791023254)
[2025-02-13 21:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:02][root][INFO] - Training Epoch: 2/2, step 5684/7134 completed (loss: 0.21356917917728424, acc: 0.9634146094322205)
[2025-02-13 21:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:02][root][INFO] - Training Epoch: 2/2, step 5685/7134 completed (loss: 0.11497267335653305, acc: 0.9685039520263672)
[2025-02-13 21:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:03][root][INFO] - Training Epoch: 2/2, step 5686/7134 completed (loss: 0.10094170272350311, acc: 0.9784172773361206)
[2025-02-13 21:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:03][root][INFO] - Training Epoch: 2/2, step 5687/7134 completed (loss: 0.16998666524887085, acc: 0.9696969985961914)
[2025-02-13 21:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:03][root][INFO] - Training Epoch: 2/2, step 5688/7134 completed (loss: 0.20053021609783173, acc: 0.9611111283302307)
[2025-02-13 21:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:04][root][INFO] - Training Epoch: 2/2, step 5689/7134 completed (loss: 0.1227436438202858, acc: 0.9756097793579102)
[2025-02-13 21:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:04][root][INFO] - Training Epoch: 2/2, step 5690/7134 completed (loss: 0.09468543529510498, acc: 0.969072163105011)
[2025-02-13 21:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:04][root][INFO] - Training Epoch: 2/2, step 5691/7134 completed (loss: 0.11639704555273056, acc: 0.9781420826911926)
[2025-02-13 21:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:05][root][INFO] - Training Epoch: 2/2, step 5692/7134 completed (loss: 0.1223112940788269, acc: 0.977142870426178)
[2025-02-13 21:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:05][root][INFO] - Training Epoch: 2/2, step 5693/7134 completed (loss: 0.031600043177604675, acc: 0.9874213933944702)
[2025-02-13 21:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:06][root][INFO] - Training Epoch: 2/2, step 5694/7134 completed (loss: 0.10161663591861725, acc: 0.9825581312179565)
[2025-02-13 21:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:06][root][INFO] - Training Epoch: 2/2, step 5695/7134 completed (loss: 0.041943829506635666, acc: 0.987730085849762)
[2025-02-13 21:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:06][root][INFO] - Training Epoch: 2/2, step 5696/7134 completed (loss: 0.06694311648607254, acc: 0.9851484894752502)
[2025-02-13 21:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:07][root][INFO] - Training Epoch: 2/2, step 5697/7134 completed (loss: 0.08733481913805008, acc: 0.9757575988769531)
[2025-02-13 21:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:07][root][INFO] - Training Epoch: 2/2, step 5698/7134 completed (loss: 0.07339399307966232, acc: 0.9829545617103577)
[2025-02-13 21:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:08][root][INFO] - Training Epoch: 2/2, step 5699/7134 completed (loss: 0.10396384447813034, acc: 0.982758641242981)
[2025-02-13 21:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:08][root][INFO] - Training Epoch: 2/2, step 5700/7134 completed (loss: 0.07376693189144135, acc: 0.9808917045593262)
[2025-02-13 21:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:08][root][INFO] - Training Epoch: 2/2, step 5701/7134 completed (loss: 0.07618311792612076, acc: 0.9891892075538635)
[2025-02-13 21:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:09][root][INFO] - Training Epoch: 2/2, step 5702/7134 completed (loss: 0.11043437570333481, acc: 0.9825581312179565)
[2025-02-13 21:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:09][root][INFO] - Training Epoch: 2/2, step 5703/7134 completed (loss: 0.019108489155769348, acc: 1.0)
[2025-02-13 21:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:09][root][INFO] - Training Epoch: 2/2, step 5704/7134 completed (loss: 0.0794939175248146, acc: 0.9647058844566345)
[2025-02-13 21:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:10][root][INFO] - Training Epoch: 2/2, step 5705/7134 completed (loss: 0.01056191697716713, acc: 1.0)
[2025-02-13 21:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:10][root][INFO] - Training Epoch: 2/2, step 5706/7134 completed (loss: 0.03951377794146538, acc: 0.9950494766235352)
[2025-02-13 21:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:11][root][INFO] - Training Epoch: 2/2, step 5707/7134 completed (loss: 0.0791425108909607, acc: 0.9760000109672546)
[2025-02-13 21:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:11][root][INFO] - Training Epoch: 2/2, step 5708/7134 completed (loss: 0.02167133428156376, acc: 0.9935897588729858)
[2025-02-13 21:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:11][root][INFO] - Training Epoch: 2/2, step 5709/7134 completed (loss: 0.12933093309402466, acc: 0.9813664555549622)
[2025-02-13 21:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:12][root][INFO] - Training Epoch: 2/2, step 5710/7134 completed (loss: 0.08261124044656754, acc: 0.977142870426178)
[2025-02-13 21:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:12][root][INFO] - Training Epoch: 2/2, step 5711/7134 completed (loss: 0.1246819943189621, acc: 0.9784946441650391)
[2025-02-13 21:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:13][root][INFO] - Training Epoch: 2/2, step 5712/7134 completed (loss: 0.02720216102898121, acc: 1.0)
[2025-02-13 21:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:13][root][INFO] - Training Epoch: 2/2, step 5713/7134 completed (loss: 0.035584598779678345, acc: 1.0)
[2025-02-13 21:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:13][root][INFO] - Training Epoch: 2/2, step 5714/7134 completed (loss: 0.027696475386619568, acc: 0.9945651888847351)
[2025-02-13 21:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:14][root][INFO] - Training Epoch: 2/2, step 5715/7134 completed (loss: 0.1710079461336136, acc: 0.9632353186607361)
[2025-02-13 21:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:14][root][INFO] - Training Epoch: 2/2, step 5716/7134 completed (loss: 0.05274733528494835, acc: 0.9930555820465088)
[2025-02-13 21:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:15][root][INFO] - Training Epoch: 2/2, step 5717/7134 completed (loss: 0.15015852451324463, acc: 0.9532163739204407)
[2025-02-13 21:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:15][root][INFO] - Training Epoch: 2/2, step 5718/7134 completed (loss: 0.04047440364956856, acc: 0.9868420958518982)
[2025-02-13 21:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:15][root][INFO] - Training Epoch: 2/2, step 5719/7134 completed (loss: 0.10178601741790771, acc: 0.9764705896377563)
[2025-02-13 21:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:16][root][INFO] - Training Epoch: 2/2, step 5720/7134 completed (loss: 0.08456901460886002, acc: 0.9743589758872986)
[2025-02-13 21:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:16][root][INFO] - Training Epoch: 2/2, step 5721/7134 completed (loss: 0.1089775413274765, acc: 0.9689119458198547)
[2025-02-13 21:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:16][root][INFO] - Training Epoch: 2/2, step 5722/7134 completed (loss: 0.02167925424873829, acc: 1.0)
[2025-02-13 21:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:17][root][INFO] - Training Epoch: 2/2, step 5723/7134 completed (loss: 0.07515129446983337, acc: 0.9894179701805115)
[2025-02-13 21:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:17][root][INFO] - Training Epoch: 2/2, step 5724/7134 completed (loss: 0.04375177621841431, acc: 0.9939393997192383)
[2025-02-13 21:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:18][root][INFO] - Training Epoch: 2/2, step 5725/7134 completed (loss: 0.10500326752662659, acc: 0.9657142758369446)
[2025-02-13 21:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:18][root][INFO] - Training Epoch: 2/2, step 5726/7134 completed (loss: 0.10108969360589981, acc: 0.970059871673584)
[2025-02-13 21:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:18][root][INFO] - Training Epoch: 2/2, step 5727/7134 completed (loss: 0.03002445213496685, acc: 1.0)
[2025-02-13 21:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:19][root][INFO] - Training Epoch: 2/2, step 5728/7134 completed (loss: 0.00727870175614953, acc: 1.0)
[2025-02-13 21:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:19][root][INFO] - Training Epoch: 2/2, step 5729/7134 completed (loss: 0.0249780286103487, acc: 1.0)
[2025-02-13 21:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:19][root][INFO] - Training Epoch: 2/2, step 5730/7134 completed (loss: 0.10003232955932617, acc: 0.9777777791023254)
[2025-02-13 21:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:20][root][INFO] - Training Epoch: 2/2, step 5731/7134 completed (loss: 0.08671845495700836, acc: 0.981249988079071)
[2025-02-13 21:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:20][root][INFO] - Training Epoch: 2/2, step 5732/7134 completed (loss: 0.46571025252342224, acc: 0.8674699068069458)
[2025-02-13 21:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:21][root][INFO] - Training Epoch: 2/2, step 5733/7134 completed (loss: 0.1115812137722969, acc: 0.9746835231781006)
[2025-02-13 21:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:21][root][INFO] - Training Epoch: 2/2, step 5734/7134 completed (loss: 0.04124417528510094, acc: 1.0)
[2025-02-13 21:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:21][root][INFO] - Training Epoch: 2/2, step 5735/7134 completed (loss: 0.08637787401676178, acc: 0.9867549538612366)
[2025-02-13 21:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:22][root][INFO] - Training Epoch: 2/2, step 5736/7134 completed (loss: 0.041476842015981674, acc: 0.9834254384040833)
[2025-02-13 21:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:22][root][INFO] - Training Epoch: 2/2, step 5737/7134 completed (loss: 0.03377638757228851, acc: 0.9941520690917969)
[2025-02-13 21:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:23][root][INFO] - Training Epoch: 2/2, step 5738/7134 completed (loss: 0.06107867509126663, acc: 0.9848484992980957)
[2025-02-13 21:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:23][root][INFO] - Training Epoch: 2/2, step 5739/7134 completed (loss: 0.05819788947701454, acc: 0.9916666746139526)
[2025-02-13 21:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:23][root][INFO] - Training Epoch: 2/2, step 5740/7134 completed (loss: 0.07834786921739578, acc: 0.9791666865348816)
[2025-02-13 21:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:24][root][INFO] - Training Epoch: 2/2, step 5741/7134 completed (loss: 0.05497466027736664, acc: 0.9829545617103577)
[2025-02-13 21:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:24][root][INFO] - Training Epoch: 2/2, step 5742/7134 completed (loss: 0.0413341261446476, acc: 0.9887640476226807)
[2025-02-13 21:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:24][root][INFO] - Training Epoch: 2/2, step 5743/7134 completed (loss: 0.03279425576329231, acc: 0.9947916865348816)
[2025-02-13 21:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:25][root][INFO] - Training Epoch: 2/2, step 5744/7134 completed (loss: 0.01256327424198389, acc: 1.0)
[2025-02-13 21:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:25][root][INFO] - Training Epoch: 2/2, step 5745/7134 completed (loss: 0.11949141323566437, acc: 0.9550561904907227)
[2025-02-13 21:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:26][root][INFO] - Training Epoch: 2/2, step 5746/7134 completed (loss: 0.06171906366944313, acc: 0.9876543283462524)
[2025-02-13 21:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:26][root][INFO] - Training Epoch: 2/2, step 5747/7134 completed (loss: 0.05997088924050331, acc: 0.9818181991577148)
[2025-02-13 21:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:26][root][INFO] - Training Epoch: 2/2, step 5748/7134 completed (loss: 0.023204347118735313, acc: 0.9921259880065918)
[2025-02-13 21:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:27][root][INFO] - Training Epoch: 2/2, step 5749/7134 completed (loss: 0.12892980873584747, acc: 0.9822485446929932)
[2025-02-13 21:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:27][root][INFO] - Training Epoch: 2/2, step 5750/7134 completed (loss: 0.033684756606817245, acc: 0.994413435459137)
[2025-02-13 21:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:27][root][INFO] - Training Epoch: 2/2, step 5751/7134 completed (loss: 0.07218366861343384, acc: 0.9867549538612366)
[2025-02-13 21:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:28][root][INFO] - Training Epoch: 2/2, step 5752/7134 completed (loss: 0.2492883950471878, acc: 0.9465649127960205)
[2025-02-13 21:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:28][root][INFO] - Training Epoch: 2/2, step 5753/7134 completed (loss: 0.08276082575321198, acc: 0.9597315192222595)
[2025-02-13 21:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:28][root][INFO] - Training Epoch: 2/2, step 5754/7134 completed (loss: 0.16310515999794006, acc: 0.954023003578186)
[2025-02-13 21:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:29][root][INFO] - Training Epoch: 2/2, step 5755/7134 completed (loss: 0.10718641430139542, acc: 0.9805194735527039)
[2025-02-13 21:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:29][root][INFO] - Training Epoch: 2/2, step 5756/7134 completed (loss: 0.14401672780513763, acc: 0.9476743936538696)
[2025-02-13 21:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:30][root][INFO] - Training Epoch: 2/2, step 5757/7134 completed (loss: 0.1159968450665474, acc: 0.9775280952453613)
[2025-02-13 21:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:30][root][INFO] - Training Epoch: 2/2, step 5758/7134 completed (loss: 0.20634329319000244, acc: 0.9320987462997437)
[2025-02-13 21:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:30][root][INFO] - Training Epoch: 2/2, step 5759/7134 completed (loss: 0.2024255394935608, acc: 0.9611650705337524)
[2025-02-13 21:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:31][root][INFO] - Training Epoch: 2/2, step 5760/7134 completed (loss: 0.14756081998348236, acc: 0.948051929473877)
[2025-02-13 21:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:31][root][INFO] - Training Epoch: 2/2, step 5761/7134 completed (loss: 0.057924240827560425, acc: 0.984000027179718)
[2025-02-13 21:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:31][root][INFO] - Training Epoch: 2/2, step 5762/7134 completed (loss: 0.05202918499708176, acc: 0.9922480583190918)
[2025-02-13 21:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:32][root][INFO] - Training Epoch: 2/2, step 5763/7134 completed (loss: 0.06442887336015701, acc: 0.9781420826911926)
[2025-02-13 21:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:32][root][INFO] - Training Epoch: 2/2, step 5764/7134 completed (loss: 0.014556041918694973, acc: 1.0)
[2025-02-13 21:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:33][root][INFO] - Training Epoch: 2/2, step 5765/7134 completed (loss: 0.1289360225200653, acc: 0.9622641801834106)
[2025-02-13 21:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:33][root][INFO] - Training Epoch: 2/2, step 5766/7134 completed (loss: 0.0654032751917839, acc: 0.9870967864990234)
[2025-02-13 21:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:33][root][INFO] - Training Epoch: 2/2, step 5767/7134 completed (loss: 0.01814805343747139, acc: 0.9937888383865356)
[2025-02-13 21:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:34][root][INFO] - Training Epoch: 2/2, step 5768/7134 completed (loss: 0.02265053801238537, acc: 0.9937499761581421)
[2025-02-13 21:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:34][root][INFO] - Training Epoch: 2/2, step 5769/7134 completed (loss: 0.014468305744230747, acc: 1.0)
[2025-02-13 21:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:34][root][INFO] - Training Epoch: 2/2, step 5770/7134 completed (loss: 0.03967657312750816, acc: 0.9891892075538635)
[2025-02-13 21:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:35][root][INFO] - Training Epoch: 2/2, step 5771/7134 completed (loss: 0.14699135720729828, acc: 0.9693251252174377)
[2025-02-13 21:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:35][root][INFO] - Training Epoch: 2/2, step 5772/7134 completed (loss: 0.0282421987503767, acc: 0.9916666746139526)
[2025-02-13 21:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:36][root][INFO] - Training Epoch: 2/2, step 5773/7134 completed (loss: 0.07189465314149857, acc: 0.9774436354637146)
[2025-02-13 21:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:36][root][INFO] - Training Epoch: 2/2, step 5774/7134 completed (loss: 0.06478358805179596, acc: 0.9753086566925049)
[2025-02-13 21:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:36][root][INFO] - Training Epoch: 2/2, step 5775/7134 completed (loss: 0.021516287699341774, acc: 0.9933333396911621)
[2025-02-13 21:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:37][root][INFO] - Training Epoch: 2/2, step 5776/7134 completed (loss: 0.06296131014823914, acc: 0.9887640476226807)
[2025-02-13 21:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:37][root][INFO] - Training Epoch: 2/2, step 5777/7134 completed (loss: 0.11598680913448334, acc: 0.9813084006309509)
[2025-02-13 21:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:37][root][INFO] - Training Epoch: 2/2, step 5778/7134 completed (loss: 0.18265536427497864, acc: 0.9634146094322205)
[2025-02-13 21:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:38][root][INFO] - Training Epoch: 2/2, step 5779/7134 completed (loss: 0.12849678099155426, acc: 0.9764705896377563)
[2025-02-13 21:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:38][root][INFO] - Training Epoch: 2/2, step 5780/7134 completed (loss: 0.08184201270341873, acc: 0.9601989984512329)
[2025-02-13 21:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:39][root][INFO] - Training Epoch: 2/2, step 5781/7134 completed (loss: 0.11117848008871078, acc: 0.9826589822769165)
[2025-02-13 21:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:39][root][INFO] - Training Epoch: 2/2, step 5782/7134 completed (loss: 0.039294589310884476, acc: 0.9944751262664795)
[2025-02-13 21:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:39][root][INFO] - Training Epoch: 2/2, step 5783/7134 completed (loss: 0.04187319427728653, acc: 0.9942196607589722)
[2025-02-13 21:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:40][root][INFO] - Training Epoch: 2/2, step 5784/7134 completed (loss: 0.04411858320236206, acc: 0.9717513918876648)
[2025-02-13 21:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:40][root][INFO] - Training Epoch: 2/2, step 5785/7134 completed (loss: 0.06713306158781052, acc: 0.9837837815284729)
[2025-02-13 21:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:40][root][INFO] - Training Epoch: 2/2, step 5786/7134 completed (loss: 0.037848617881536484, acc: 1.0)
[2025-02-13 21:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:41][root][INFO] - Training Epoch: 2/2, step 5787/7134 completed (loss: 0.062327418476343155, acc: 0.9790576100349426)
[2025-02-13 21:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:41][root][INFO] - Training Epoch: 2/2, step 5788/7134 completed (loss: 0.03170410916209221, acc: 0.9934210777282715)
[2025-02-13 21:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:42][root][INFO] - Training Epoch: 2/2, step 5789/7134 completed (loss: 0.05644378438591957, acc: 0.9898989796638489)
[2025-02-13 21:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:42][root][INFO] - Training Epoch: 2/2, step 5790/7134 completed (loss: 0.07842622697353363, acc: 0.976190447807312)
[2025-02-13 21:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:42][root][INFO] - Training Epoch: 2/2, step 5791/7134 completed (loss: 0.06359228491783142, acc: 0.9701492786407471)
[2025-02-13 21:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:43][root][INFO] - Training Epoch: 2/2, step 5792/7134 completed (loss: 0.15020790696144104, acc: 0.9530201554298401)
[2025-02-13 21:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:43][root][INFO] - Training Epoch: 2/2, step 5793/7134 completed (loss: 0.10814572870731354, acc: 0.9570552110671997)
[2025-02-13 21:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:43][root][INFO] - Training Epoch: 2/2, step 5794/7134 completed (loss: 0.2191026508808136, acc: 0.9490445852279663)
[2025-02-13 21:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:44][root][INFO] - Training Epoch: 2/2, step 5795/7134 completed (loss: 0.14290094375610352, acc: 0.9624060392379761)
[2025-02-13 21:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:44][root][INFO] - Training Epoch: 2/2, step 5796/7134 completed (loss: 0.07786988466978073, acc: 0.9770992398262024)
[2025-02-13 21:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:44][root][INFO] - Training Epoch: 2/2, step 5797/7134 completed (loss: 0.1058531105518341, acc: 0.9664429426193237)
[2025-02-13 21:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:45][root][INFO] - Training Epoch: 2/2, step 5798/7134 completed (loss: 0.6090513467788696, acc: 0.8779069781303406)
[2025-02-13 21:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:45][root][INFO] - Training Epoch: 2/2, step 5799/7134 completed (loss: 0.06957139074802399, acc: 0.9842519760131836)
[2025-02-13 21:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:46][root][INFO] - Training Epoch: 2/2, step 5800/7134 completed (loss: 0.21054303646087646, acc: 0.9640287756919861)
[2025-02-13 21:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:46][root][INFO] - Training Epoch: 2/2, step 5801/7134 completed (loss: 0.13871467113494873, acc: 0.9814814925193787)
[2025-02-13 21:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:46][root][INFO] - Training Epoch: 2/2, step 5802/7134 completed (loss: 0.064720019698143, acc: 0.9779411554336548)
[2025-02-13 21:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:47][root][INFO] - Training Epoch: 2/2, step 5803/7134 completed (loss: 0.03446822986006737, acc: 0.9942528605461121)
[2025-02-13 21:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:47][root][INFO] - Training Epoch: 2/2, step 5804/7134 completed (loss: 0.15256349742412567, acc: 0.9426751732826233)
[2025-02-13 21:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:47][root][INFO] - Training Epoch: 2/2, step 5805/7134 completed (loss: 0.09582111984491348, acc: 0.9735099077224731)
[2025-02-13 21:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:48][root][INFO] - Training Epoch: 2/2, step 5806/7134 completed (loss: 0.04444100335240364, acc: 0.9878787994384766)
[2025-02-13 21:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:48][root][INFO] - Training Epoch: 2/2, step 5807/7134 completed (loss: 0.12909847497940063, acc: 0.9718309640884399)
[2025-02-13 21:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:49][root][INFO] - Training Epoch: 2/2, step 5808/7134 completed (loss: 0.06256969273090363, acc: 0.9810126423835754)
[2025-02-13 21:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:49][root][INFO] - Training Epoch: 2/2, step 5809/7134 completed (loss: 0.04003021866083145, acc: 0.9901960492134094)
[2025-02-13 21:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:49][root][INFO] - Training Epoch: 2/2, step 5810/7134 completed (loss: 0.03285256400704384, acc: 0.9917355179786682)
[2025-02-13 21:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:50][root][INFO] - Training Epoch: 2/2, step 5811/7134 completed (loss: 0.037637170404195786, acc: 1.0)
[2025-02-13 21:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:50][root][INFO] - Training Epoch: 2/2, step 5812/7134 completed (loss: 0.034396469593048096, acc: 1.0)
[2025-02-13 21:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:50][root][INFO] - Training Epoch: 2/2, step 5813/7134 completed (loss: 0.09631913155317307, acc: 0.9672130942344666)
[2025-02-13 21:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:51][root][INFO] - Training Epoch: 2/2, step 5814/7134 completed (loss: 0.05944185331463814, acc: 0.984000027179718)
[2025-02-13 21:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:51][root][INFO] - Training Epoch: 2/2, step 5815/7134 completed (loss: 0.058537524193525314, acc: 0.982758641242981)
[2025-02-13 21:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:52][root][INFO] - Training Epoch: 2/2, step 5816/7134 completed (loss: 0.012707676738500595, acc: 1.0)
[2025-02-13 21:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:52][root][INFO] - Training Epoch: 2/2, step 5817/7134 completed (loss: 0.11961298435926437, acc: 0.9809523820877075)
[2025-02-13 21:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:53][root][INFO] - Training Epoch: 2/2, step 5818/7134 completed (loss: 0.02118483930826187, acc: 1.0)
[2025-02-13 21:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:53][root][INFO] - Training Epoch: 2/2, step 5819/7134 completed (loss: 0.02066853828728199, acc: 1.0)
[2025-02-13 21:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:53][root][INFO] - Training Epoch: 2/2, step 5820/7134 completed (loss: 0.06917881220579147, acc: 0.9917355179786682)
[2025-02-13 21:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:54][root][INFO] - Training Epoch: 2/2, step 5821/7134 completed (loss: 0.022583480924367905, acc: 0.9931034445762634)
[2025-02-13 21:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:54][root][INFO] - Training Epoch: 2/2, step 5822/7134 completed (loss: 0.05053211748600006, acc: 0.9865771532058716)
[2025-02-13 21:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:54][root][INFO] - Training Epoch: 2/2, step 5823/7134 completed (loss: 0.03729046881198883, acc: 0.9863945841789246)
[2025-02-13 21:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:55][root][INFO] - Training Epoch: 2/2, step 5824/7134 completed (loss: 0.044651128351688385, acc: 0.9818181991577148)
[2025-02-13 21:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:55][root][INFO] - Training Epoch: 2/2, step 5825/7134 completed (loss: 0.09941417723894119, acc: 0.9694656729698181)
[2025-02-13 21:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:55][root][INFO] - Training Epoch: 2/2, step 5826/7134 completed (loss: 0.014683091081678867, acc: 1.0)
[2025-02-13 21:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:56][root][INFO] - Training Epoch: 2/2, step 5827/7134 completed (loss: 0.027251822873950005, acc: 0.9838709831237793)
[2025-02-13 21:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:56][root][INFO] - Training Epoch: 2/2, step 5828/7134 completed (loss: 0.053239211440086365, acc: 0.9803921580314636)
[2025-02-13 21:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:56][root][INFO] - Training Epoch: 2/2, step 5829/7134 completed (loss: 0.029802147299051285, acc: 0.9939024448394775)
[2025-02-13 21:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:57][root][INFO] - Training Epoch: 2/2, step 5830/7134 completed (loss: 0.04158707335591316, acc: 0.9919999837875366)
[2025-02-13 21:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:57][root][INFO] - Training Epoch: 2/2, step 5831/7134 completed (loss: 0.0284523107111454, acc: 0.9897959232330322)
[2025-02-13 21:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:58][root][INFO] - Training Epoch: 2/2, step 5832/7134 completed (loss: 0.09089897572994232, acc: 0.9847328066825867)
[2025-02-13 21:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:58][root][INFO] - Training Epoch: 2/2, step 5833/7134 completed (loss: 0.03757837414741516, acc: 0.9849624037742615)
[2025-02-13 21:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:58][root][INFO] - Training Epoch: 2/2, step 5834/7134 completed (loss: 0.08419874310493469, acc: 0.9766082167625427)
[2025-02-13 21:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:59][root][INFO] - Training Epoch: 2/2, step 5835/7134 completed (loss: 0.04028083384037018, acc: 1.0)
[2025-02-13 21:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:59][root][INFO] - Training Epoch: 2/2, step 5836/7134 completed (loss: 0.11010952293872833, acc: 0.9808917045593262)
[2025-02-13 21:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:59][root][INFO] - Training Epoch: 2/2, step 5837/7134 completed (loss: 0.2715575695037842, acc: 0.9328858852386475)
[2025-02-13 21:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:00][root][INFO] - Training Epoch: 2/2, step 5838/7134 completed (loss: 0.09595093131065369, acc: 0.9928057789802551)
[2025-02-13 21:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:00][root][INFO] - Training Epoch: 2/2, step 5839/7134 completed (loss: 0.12152270972728729, acc: 0.95652174949646)
[2025-02-13 21:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:01][root][INFO] - Training Epoch: 2/2, step 5840/7134 completed (loss: 0.022503668442368507, acc: 0.9930070042610168)
[2025-02-13 21:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:01][root][INFO] - Training Epoch: 2/2, step 5841/7134 completed (loss: 0.1006801575422287, acc: 0.987500011920929)
[2025-02-13 21:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:01][root][INFO] - Training Epoch: 2/2, step 5842/7134 completed (loss: 0.02893471159040928, acc: 0.9917355179786682)
[2025-02-13 21:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:02][root][INFO] - Training Epoch: 2/2, step 5843/7134 completed (loss: 0.1289539337158203, acc: 0.9702380895614624)
[2025-02-13 21:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:02][root][INFO] - Training Epoch: 2/2, step 5844/7134 completed (loss: 0.04630979895591736, acc: 0.9879518151283264)
[2025-02-13 21:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:03][root][INFO] - Training Epoch: 2/2, step 5845/7134 completed (loss: 0.06960190832614899, acc: 0.9805194735527039)
[2025-02-13 21:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:03][root][INFO] - Training Epoch: 2/2, step 5846/7134 completed (loss: 0.026664847508072853, acc: 1.0)
[2025-02-13 21:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:03][root][INFO] - Training Epoch: 2/2, step 5847/7134 completed (loss: 0.03441794589161873, acc: 0.9939758777618408)
[2025-02-13 21:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:04][root][INFO] - Training Epoch: 2/2, step 5848/7134 completed (loss: 0.07225387543439865, acc: 0.9864864945411682)
[2025-02-13 21:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:04][root][INFO] - Training Epoch: 2/2, step 5849/7134 completed (loss: 0.09348368644714355, acc: 0.9740259647369385)
[2025-02-13 21:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:04][root][INFO] - Training Epoch: 2/2, step 5850/7134 completed (loss: 0.05684112384915352, acc: 0.9788732528686523)
[2025-02-13 21:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:05][root][INFO] - Training Epoch: 2/2, step 5851/7134 completed (loss: 0.11654677242040634, acc: 0.9520547986030579)
[2025-02-13 21:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:05][root][INFO] - Training Epoch: 2/2, step 5852/7134 completed (loss: 0.0831281989812851, acc: 0.9860140085220337)
[2025-02-13 21:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:05][root][INFO] - Training Epoch: 2/2, step 5853/7134 completed (loss: 0.12318123877048492, acc: 0.966292142868042)
[2025-02-13 21:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:06][root][INFO] - Training Epoch: 2/2, step 5854/7134 completed (loss: 0.06988121569156647, acc: 0.9923076629638672)
[2025-02-13 21:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:06][root][INFO] - Training Epoch: 2/2, step 5855/7134 completed (loss: 0.09606605023145676, acc: 0.991525411605835)
[2025-02-13 21:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:07][root][INFO] - Training Epoch: 2/2, step 5856/7134 completed (loss: 0.05043606832623482, acc: 0.9929577708244324)
[2025-02-13 21:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:07][root][INFO] - Training Epoch: 2/2, step 5857/7134 completed (loss: 0.08158861845731735, acc: 0.9774436354637146)
[2025-02-13 21:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:07][root][INFO] - Training Epoch: 2/2, step 5858/7134 completed (loss: 0.03316875919699669, acc: 0.9908257126808167)
[2025-02-13 21:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:08][root][INFO] - Training Epoch: 2/2, step 5859/7134 completed (loss: 0.02010810375213623, acc: 1.0)
[2025-02-13 21:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:08][root][INFO] - Training Epoch: 2/2, step 5860/7134 completed (loss: 0.035717885941267014, acc: 1.0)
[2025-02-13 21:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:08][root][INFO] - Training Epoch: 2/2, step 5861/7134 completed (loss: 0.014992787502706051, acc: 1.0)
[2025-02-13 21:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:09][root][INFO] - Training Epoch: 2/2, step 5862/7134 completed (loss: 0.03502508997917175, acc: 1.0)
[2025-02-13 21:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:09][root][INFO] - Training Epoch: 2/2, step 5863/7134 completed (loss: 0.17102155089378357, acc: 0.9677419066429138)
[2025-02-13 21:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:10][root][INFO] - Training Epoch: 2/2, step 5864/7134 completed (loss: 0.19564563035964966, acc: 0.9473684430122375)
[2025-02-13 21:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:10][root][INFO] - Training Epoch: 2/2, step 5865/7134 completed (loss: 0.05855025351047516, acc: 0.9791666865348816)
[2025-02-13 21:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:10][root][INFO] - Training Epoch: 2/2, step 5866/7134 completed (loss: 0.022524774074554443, acc: 0.9945054650306702)
[2025-02-13 21:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:11][root][INFO] - Training Epoch: 2/2, step 5867/7134 completed (loss: 0.04020693525671959, acc: 0.9864864945411682)
[2025-02-13 21:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:11][root][INFO] - Training Epoch: 2/2, step 5868/7134 completed (loss: 0.058484144508838654, acc: 0.9803921580314636)
[2025-02-13 21:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:12][root][INFO] - Training Epoch: 2/2, step 5869/7134 completed (loss: 0.06319401413202286, acc: 0.976047933101654)
[2025-02-13 21:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:12][root][INFO] - Training Epoch: 2/2, step 5870/7134 completed (loss: 0.04085099697113037, acc: 0.9907407164573669)
[2025-02-13 21:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:12][root][INFO] - Training Epoch: 2/2, step 5871/7134 completed (loss: 0.04910034313797951, acc: 0.9908257126808167)
[2025-02-13 21:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:13][root][INFO] - Training Epoch: 2/2, step 5872/7134 completed (loss: 0.04650077596306801, acc: 0.9751552939414978)
[2025-02-13 21:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:13][root][INFO] - Training Epoch: 2/2, step 5873/7134 completed (loss: 0.0749405100941658, acc: 0.9893048405647278)
[2025-02-13 21:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:13][root][INFO] - Training Epoch: 2/2, step 5874/7134 completed (loss: 0.03042147122323513, acc: 0.9920634627342224)
[2025-02-13 21:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:14][root][INFO] - Training Epoch: 2/2, step 5875/7134 completed (loss: 0.0345308855175972, acc: 0.9924242496490479)
[2025-02-13 21:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:14][root][INFO] - Training Epoch: 2/2, step 5876/7134 completed (loss: 0.03873025253415108, acc: 0.9775280952453613)
[2025-02-13 21:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:15][root][INFO] - Training Epoch: 2/2, step 5877/7134 completed (loss: 0.0679316520690918, acc: 0.9776536226272583)
[2025-02-13 21:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:15][root][INFO] - Training Epoch: 2/2, step 5878/7134 completed (loss: 0.07270161807537079, acc: 0.9818181991577148)
[2025-02-13 21:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:16][root][INFO] - Training Epoch: 2/2, step 5879/7134 completed (loss: 0.02163940854370594, acc: 1.0)
[2025-02-13 21:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:16][root][INFO] - Training Epoch: 2/2, step 5880/7134 completed (loss: 0.03901711851358414, acc: 0.9821428656578064)
[2025-02-13 21:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:16][root][INFO] - Training Epoch: 2/2, step 5881/7134 completed (loss: 0.0319741889834404, acc: 0.9914529919624329)
[2025-02-13 21:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:17][root][INFO] - Training Epoch: 2/2, step 5882/7134 completed (loss: 0.03158344700932503, acc: 0.9901960492134094)
[2025-02-13 21:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:17][root][INFO] - Training Epoch: 2/2, step 5883/7134 completed (loss: 0.04876362532377243, acc: 0.9769230484962463)
[2025-02-13 21:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:17][root][INFO] - Training Epoch: 2/2, step 5884/7134 completed (loss: 0.07326444983482361, acc: 0.9920634627342224)
[2025-02-13 21:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:18][root][INFO] - Training Epoch: 2/2, step 5885/7134 completed (loss: 0.17945009469985962, acc: 0.9509803652763367)
[2025-02-13 21:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:18][root][INFO] - Training Epoch: 2/2, step 5886/7134 completed (loss: 0.011154040694236755, acc: 1.0)
[2025-02-13 21:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:19][root][INFO] - Training Epoch: 2/2, step 5887/7134 completed (loss: 0.012396157719194889, acc: 1.0)
[2025-02-13 21:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:19][root][INFO] - Training Epoch: 2/2, step 5888/7134 completed (loss: 0.09599527716636658, acc: 0.9716312289237976)
[2025-02-13 21:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:19][root][INFO] - Training Epoch: 2/2, step 5889/7134 completed (loss: 0.06772373616695404, acc: 0.9615384340286255)
[2025-02-13 21:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:20][root][INFO] - Training Epoch: 2/2, step 5890/7134 completed (loss: 0.0391170009970665, acc: 0.9932432174682617)
[2025-02-13 21:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:20][root][INFO] - Training Epoch: 2/2, step 5891/7134 completed (loss: 0.14923416078090668, acc: 0.9642857313156128)
[2025-02-13 21:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:21][root][INFO] - Training Epoch: 2/2, step 5892/7134 completed (loss: 0.0659024566411972, acc: 0.9883720874786377)
[2025-02-13 21:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:21][root][INFO] - Training Epoch: 2/2, step 5893/7134 completed (loss: 0.04601743444800377, acc: 0.987730085849762)
[2025-02-13 21:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:21][root][INFO] - Training Epoch: 2/2, step 5894/7134 completed (loss: 0.019894108176231384, acc: 1.0)
[2025-02-13 21:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:22][root][INFO] - Training Epoch: 2/2, step 5895/7134 completed (loss: 0.02917657233774662, acc: 1.0)
[2025-02-13 21:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:22][root][INFO] - Training Epoch: 2/2, step 5896/7134 completed (loss: 0.05336800590157509, acc: 0.9937106966972351)
[2025-02-13 21:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:23][root][INFO] - Training Epoch: 2/2, step 5897/7134 completed (loss: 0.03208339959383011, acc: 0.9943181872367859)
[2025-02-13 21:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:23][root][INFO] - Training Epoch: 2/2, step 5898/7134 completed (loss: 0.05206040292978287, acc: 0.9821428656578064)
[2025-02-13 21:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:23][root][INFO] - Training Epoch: 2/2, step 5899/7134 completed (loss: 0.018391534686088562, acc: 1.0)
[2025-02-13 21:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:24][root][INFO] - Training Epoch: 2/2, step 5900/7134 completed (loss: 0.011041874065995216, acc: 1.0)
[2025-02-13 21:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:24][root][INFO] - Training Epoch: 2/2, step 5901/7134 completed (loss: 0.09444818645715714, acc: 0.9793814420700073)
[2025-02-13 21:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:25][root][INFO] - Training Epoch: 2/2, step 5902/7134 completed (loss: 0.04429924115538597, acc: 0.9897959232330322)
[2025-02-13 21:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:25][root][INFO] - Training Epoch: 2/2, step 5903/7134 completed (loss: 0.019334930926561356, acc: 1.0)
[2025-02-13 21:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:25][root][INFO] - Training Epoch: 2/2, step 5904/7134 completed (loss: 0.08615150302648544, acc: 0.98591548204422)
[2025-02-13 21:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:26][root][INFO] - Training Epoch: 2/2, step 5905/7134 completed (loss: 0.06339375674724579, acc: 0.9729729890823364)
[2025-02-13 21:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:26][root][INFO] - Training Epoch: 2/2, step 5906/7134 completed (loss: 0.05006221681833267, acc: 0.9869281053543091)
[2025-02-13 21:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:27][root][INFO] - Training Epoch: 2/2, step 5907/7134 completed (loss: 0.042820949107408524, acc: 0.9930555820465088)
[2025-02-13 21:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:27][root][INFO] - Training Epoch: 2/2, step 5908/7134 completed (loss: 0.03253648057579994, acc: 0.9935064911842346)
[2025-02-13 21:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:28][root][INFO] - Training Epoch: 2/2, step 5909/7134 completed (loss: 0.06761478632688522, acc: 0.970802903175354)
[2025-02-13 21:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:28][root][INFO] - Training Epoch: 2/2, step 5910/7134 completed (loss: 0.09102357923984528, acc: 0.9668874144554138)
[2025-02-13 21:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:28][root][INFO] - Training Epoch: 2/2, step 5911/7134 completed (loss: 0.09248232841491699, acc: 0.9739130139350891)
[2025-02-13 21:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:29][root][INFO] - Training Epoch: 2/2, step 5912/7134 completed (loss: 0.04106946289539337, acc: 0.9921875)
[2025-02-13 21:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:29][root][INFO] - Training Epoch: 2/2, step 5913/7134 completed (loss: 0.09700606763362885, acc: 0.9644970297813416)
[2025-02-13 21:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:30][root][INFO] - Training Epoch: 2/2, step 5914/7134 completed (loss: 0.08104565739631653, acc: 0.9857142567634583)
[2025-02-13 21:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:30][root][INFO] - Training Epoch: 2/2, step 5915/7134 completed (loss: 0.06785168498754501, acc: 0.9847328066825867)
[2025-02-13 21:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:30][root][INFO] - Training Epoch: 2/2, step 5916/7134 completed (loss: 0.09011183679103851, acc: 0.9801324605941772)
[2025-02-13 21:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:31][root][INFO] - Training Epoch: 2/2, step 5917/7134 completed (loss: 0.02611691690981388, acc: 0.9919999837875366)
[2025-02-13 21:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:31][root][INFO] - Training Epoch: 2/2, step 5918/7134 completed (loss: 0.11143394559621811, acc: 0.9720279574394226)
[2025-02-13 21:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:31][root][INFO] - Training Epoch: 2/2, step 5919/7134 completed (loss: 0.017128102481365204, acc: 1.0)
[2025-02-13 21:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:32][root][INFO] - Training Epoch: 2/2, step 5920/7134 completed (loss: 0.011484385468065739, acc: 1.0)
[2025-02-13 21:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:32][root][INFO] - Training Epoch: 2/2, step 5921/7134 completed (loss: 0.08670657128095627, acc: 0.9741379022598267)
[2025-02-13 21:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:33][root][INFO] - Training Epoch: 2/2, step 5922/7134 completed (loss: 0.2723236680030823, acc: 0.9487179517745972)
[2025-02-13 21:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:33][root][INFO] - Training Epoch: 2/2, step 5923/7134 completed (loss: 0.11398278176784515, acc: 0.9873417615890503)
[2025-02-13 21:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:33][root][INFO] - Training Epoch: 2/2, step 5924/7134 completed (loss: 0.04036223515868187, acc: 1.0)
[2025-02-13 21:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:34][root][INFO] - Training Epoch: 2/2, step 5925/7134 completed (loss: 0.2684876620769501, acc: 0.9333333373069763)
[2025-02-13 21:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:34][root][INFO] - Training Epoch: 2/2, step 5926/7134 completed (loss: 0.16584119200706482, acc: 0.9407894611358643)
[2025-02-13 21:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:35][root][INFO] - Training Epoch: 2/2, step 5927/7134 completed (loss: 0.09121301025152206, acc: 0.9821428656578064)
[2025-02-13 21:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:35][root][INFO] - Training Epoch: 2/2, step 5928/7134 completed (loss: 0.4022669792175293, acc: 0.9459459185600281)
[2025-02-13 21:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:35][root][INFO] - Training Epoch: 2/2, step 5929/7134 completed (loss: 0.15342488884925842, acc: 0.9677419066429138)
[2025-02-13 21:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:36][root][INFO] - Training Epoch: 2/2, step 5930/7134 completed (loss: 0.3217070400714874, acc: 0.942148745059967)
[2025-02-13 21:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:36][root][INFO] - Training Epoch: 2/2, step 5931/7134 completed (loss: 0.07949763536453247, acc: 0.9914529919624329)
[2025-02-13 21:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:37][root][INFO] - Training Epoch: 2/2, step 5932/7134 completed (loss: 0.29904380440711975, acc: 0.9136690497398376)
[2025-02-13 21:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:37][root][INFO] - Training Epoch: 2/2, step 5933/7134 completed (loss: 0.21734289824962616, acc: 0.9485294222831726)
[2025-02-13 21:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:37][root][INFO] - Training Epoch: 2/2, step 5934/7134 completed (loss: 0.10140302777290344, acc: 0.9857142567634583)
[2025-02-13 21:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:38][root][INFO] - Training Epoch: 2/2, step 5935/7134 completed (loss: 0.04915976524353027, acc: 0.9836065769195557)
[2025-02-13 21:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:38][root][INFO] - Training Epoch: 2/2, step 5936/7134 completed (loss: 0.22305592894554138, acc: 0.9693251252174377)
[2025-02-13 21:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:38][root][INFO] - Training Epoch: 2/2, step 5937/7134 completed (loss: 0.055510085076093674, acc: 0.9929577708244324)
[2025-02-13 21:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:39][root][INFO] - Training Epoch: 2/2, step 5938/7134 completed (loss: 0.05587727949023247, acc: 0.9849624037742615)
[2025-02-13 21:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:39][root][INFO] - Training Epoch: 2/2, step 5939/7134 completed (loss: 0.3342447578907013, acc: 0.931034505367279)
[2025-02-13 21:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:39][root][INFO] - Training Epoch: 2/2, step 5940/7134 completed (loss: 0.13292232155799866, acc: 0.9751552939414978)
[2025-02-13 21:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:40][root][INFO] - Training Epoch: 2/2, step 5941/7134 completed (loss: 0.06437759101390839, acc: 0.976190447807312)
[2025-02-13 21:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:40][root][INFO] - Training Epoch: 2/2, step 5942/7134 completed (loss: 0.02599405311048031, acc: 0.9919999837875366)
[2025-02-13 21:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:41][root][INFO] - Training Epoch: 2/2, step 5943/7134 completed (loss: 0.04785260185599327, acc: 0.9815950989723206)
[2025-02-13 21:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:41][root][INFO] - Training Epoch: 2/2, step 5944/7134 completed (loss: 0.08206123858690262, acc: 0.9846938848495483)
[2025-02-13 21:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:41][root][INFO] - Training Epoch: 2/2, step 5945/7134 completed (loss: 0.022976001724600792, acc: 1.0)
[2025-02-13 21:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:42][root][INFO] - Training Epoch: 2/2, step 5946/7134 completed (loss: 0.07843451201915741, acc: 0.9583333134651184)
[2025-02-13 21:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:42][root][INFO] - Training Epoch: 2/2, step 5947/7134 completed (loss: 0.16435201466083527, acc: 0.9663461446762085)
[2025-02-13 21:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:42][root][INFO] - Training Epoch: 2/2, step 5948/7134 completed (loss: 0.14111772179603577, acc: 0.9674796462059021)
[2025-02-13 21:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:43][root][INFO] - Training Epoch: 2/2, step 5949/7134 completed (loss: 0.023350100964307785, acc: 1.0)
[2025-02-13 21:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:43][root][INFO] - Training Epoch: 2/2, step 5950/7134 completed (loss: 0.09375887364149094, acc: 0.9801980257034302)
[2025-02-13 21:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:44][root][INFO] - Training Epoch: 2/2, step 5951/7134 completed (loss: 0.1339186578989029, acc: 0.9642857313156128)
[2025-02-13 21:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:44][root][INFO] - Training Epoch: 2/2, step 5952/7134 completed (loss: 0.40644413232803345, acc: 0.9276315569877625)
[2025-02-13 21:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:44][root][INFO] - Training Epoch: 2/2, step 5953/7134 completed (loss: 0.07144644856452942, acc: 0.9905660152435303)
[2025-02-13 21:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:45][root][INFO] - Training Epoch: 2/2, step 5954/7134 completed (loss: 0.025557979941368103, acc: 0.9900990128517151)
[2025-02-13 21:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:45][root][INFO] - Training Epoch: 2/2, step 5955/7134 completed (loss: 0.15178579092025757, acc: 0.9814814925193787)
[2025-02-13 21:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:45][root][INFO] - Training Epoch: 2/2, step 5956/7134 completed (loss: 0.10400620102882385, acc: 0.9833333492279053)
[2025-02-13 21:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:46][root][INFO] - Training Epoch: 2/2, step 5957/7134 completed (loss: 0.10049381852149963, acc: 0.9878048896789551)
[2025-02-13 21:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:46][root][INFO] - Training Epoch: 2/2, step 5958/7134 completed (loss: 0.09612448513507843, acc: 0.977142870426178)
[2025-02-13 21:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:47][root][INFO] - Training Epoch: 2/2, step 5959/7134 completed (loss: 0.12830357253551483, acc: 0.9734042286872864)
[2025-02-13 21:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:47][root][INFO] - Training Epoch: 2/2, step 5960/7134 completed (loss: 0.015564566478133202, acc: 0.9940476417541504)
[2025-02-13 21:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:47][root][INFO] - Training Epoch: 2/2, step 5961/7134 completed (loss: 0.07987764477729797, acc: 0.9722222089767456)
[2025-02-13 21:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:48][root][INFO] - Training Epoch: 2/2, step 5962/7134 completed (loss: 0.03992808237671852, acc: 0.9862068891525269)
[2025-02-13 21:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:48][root][INFO] - Training Epoch: 2/2, step 5963/7134 completed (loss: 0.03159507364034653, acc: 0.9942857027053833)
[2025-02-13 21:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:48][root][INFO] - Training Epoch: 2/2, step 5964/7134 completed (loss: 0.028201261535286903, acc: 0.9921259880065918)
[2025-02-13 21:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:49][root][INFO] - Training Epoch: 2/2, step 5965/7134 completed (loss: 0.051850490272045135, acc: 0.9871794581413269)
[2025-02-13 21:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:49][root][INFO] - Training Epoch: 2/2, step 5966/7134 completed (loss: 0.08249395340681076, acc: 0.97826087474823)
[2025-02-13 21:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:50][root][INFO] - Training Epoch: 2/2, step 5967/7134 completed (loss: 0.06897756457328796, acc: 0.9791666865348816)
[2025-02-13 21:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:50][root][INFO] - Training Epoch: 2/2, step 5968/7134 completed (loss: 0.051153793931007385, acc: 0.9900000095367432)
[2025-02-13 21:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:50][root][INFO] - Training Epoch: 2/2, step 5969/7134 completed (loss: 0.09780712425708771, acc: 0.9806451797485352)
[2025-02-13 21:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:51][root][INFO] - Training Epoch: 2/2, step 5970/7134 completed (loss: 0.13593456149101257, acc: 0.9765625)
[2025-02-13 21:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:51][root][INFO] - Training Epoch: 2/2, step 5971/7134 completed (loss: 0.035149771720170975, acc: 0.9894737005233765)
[2025-02-13 21:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:52][root][INFO] - Training Epoch: 2/2, step 5972/7134 completed (loss: 0.17651285231113434, acc: 0.9693251252174377)
[2025-02-13 21:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:52][root][INFO] - Training Epoch: 2/2, step 5973/7134 completed (loss: 0.14449815452098846, acc: 0.9599999785423279)
[2025-02-13 21:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:52][root][INFO] - Training Epoch: 2/2, step 5974/7134 completed (loss: 0.011390340514481068, acc: 1.0)
[2025-02-13 21:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:53][root][INFO] - Training Epoch: 2/2, step 5975/7134 completed (loss: 0.032491620630025864, acc: 0.9939024448394775)
[2025-02-13 21:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:53][root][INFO] - Training Epoch: 2/2, step 5976/7134 completed (loss: 0.02205776795744896, acc: 1.0)
[2025-02-13 21:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:53][root][INFO] - Training Epoch: 2/2, step 5977/7134 completed (loss: 0.07207372039556503, acc: 0.9850000143051147)
[2025-02-13 21:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:54][root][INFO] - Training Epoch: 2/2, step 5978/7134 completed (loss: 0.09037831425666809, acc: 0.9875776171684265)
[2025-02-13 21:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:54][root][INFO] - Training Epoch: 2/2, step 5979/7134 completed (loss: 0.10241662710905075, acc: 0.9894737005233765)
[2025-02-13 21:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:55][root][INFO] - Training Epoch: 2/2, step 5980/7134 completed (loss: 0.08530449122190475, acc: 0.97826087474823)
[2025-02-13 21:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:55][root][INFO] - Training Epoch: 2/2, step 5981/7134 completed (loss: 0.06981275975704193, acc: 0.989847719669342)
[2025-02-13 21:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:55][root][INFO] - Training Epoch: 2/2, step 5982/7134 completed (loss: 0.06592334061861038, acc: 0.9919354915618896)
[2025-02-13 21:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:56][root][INFO] - Training Epoch: 2/2, step 5983/7134 completed (loss: 0.06920144706964493, acc: 0.982758641242981)
[2025-02-13 21:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:56][root][INFO] - Training Epoch: 2/2, step 5984/7134 completed (loss: 0.050553251057863235, acc: 0.9878787994384766)
[2025-02-13 21:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:56][root][INFO] - Training Epoch: 2/2, step 5985/7134 completed (loss: 0.0878637284040451, acc: 0.983146071434021)
[2025-02-13 21:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:57][root][INFO] - Training Epoch: 2/2, step 5986/7134 completed (loss: 0.08788006752729416, acc: 0.9841269850730896)
[2025-02-13 21:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:57][root][INFO] - Training Epoch: 2/2, step 5987/7134 completed (loss: 0.0689791738986969, acc: 0.9756097793579102)
[2025-02-13 21:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:58][root][INFO] - Training Epoch: 2/2, step 5988/7134 completed (loss: 0.035197917371988297, acc: 0.9888888597488403)
[2025-02-13 21:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:58][root][INFO] - Training Epoch: 2/2, step 5989/7134 completed (loss: 0.0829189270734787, acc: 0.9846153855323792)
[2025-02-13 21:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:58][root][INFO] - Training Epoch: 2/2, step 5990/7134 completed (loss: 0.07278592139482498, acc: 0.9834254384040833)
[2025-02-13 21:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:59][root][INFO] - Training Epoch: 2/2, step 5991/7134 completed (loss: 0.1231849193572998, acc: 0.9659090638160706)
[2025-02-13 21:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:59][root][INFO] - Training Epoch: 2/2, step 5992/7134 completed (loss: 0.033760156482458115, acc: 0.9948979616165161)
[2025-02-13 21:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:59][root][INFO] - Training Epoch: 2/2, step 5993/7134 completed (loss: 0.043300796300172806, acc: 0.9887640476226807)
[2025-02-13 21:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:00][root][INFO] - Training Epoch: 2/2, step 5994/7134 completed (loss: 0.022837720811367035, acc: 0.9917355179786682)
[2025-02-13 21:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:00][root][INFO] - Training Epoch: 2/2, step 5995/7134 completed (loss: 0.03207242488861084, acc: 0.9884393215179443)
[2025-02-13 21:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:01][root][INFO] - Training Epoch: 2/2, step 5996/7134 completed (loss: 0.07838956266641617, acc: 0.9800000190734863)
[2025-02-13 21:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:01][root][INFO] - Training Epoch: 2/2, step 5997/7134 completed (loss: 0.10063386708498001, acc: 0.9772727489471436)
[2025-02-13 21:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:01][root][INFO] - Training Epoch: 2/2, step 5998/7134 completed (loss: 0.038895025849342346, acc: 0.9947090148925781)
[2025-02-13 21:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:02][root][INFO] - Training Epoch: 2/2, step 5999/7134 completed (loss: 0.045618828386068344, acc: 0.9855072498321533)
[2025-02-13 21:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:02][root][INFO] - Training Epoch: 2/2, step 6000/7134 completed (loss: 0.027175571769475937, acc: 1.0)
[2025-02-13 21:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:02][root][INFO] - Training Epoch: 2/2, step 6001/7134 completed (loss: 0.01968286745250225, acc: 1.0)
[2025-02-13 21:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:03][root][INFO] - Training Epoch: 2/2, step 6002/7134 completed (loss: 0.09621956944465637, acc: 0.9917355179786682)
[2025-02-13 21:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:03][root][INFO] - Training Epoch: 2/2, step 6003/7134 completed (loss: 0.05226154997944832, acc: 0.9870129823684692)
[2025-02-13 21:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:04][root][INFO] - Training Epoch: 2/2, step 6004/7134 completed (loss: 0.17843258380889893, acc: 0.9595375657081604)
[2025-02-13 21:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:04][root][INFO] - Training Epoch: 2/2, step 6005/7134 completed (loss: 0.06721854209899902, acc: 0.9801324605941772)
[2025-02-13 21:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:04][root][INFO] - Training Epoch: 2/2, step 6006/7134 completed (loss: 0.08081051707267761, acc: 0.984000027179718)
[2025-02-13 21:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:05][root][INFO] - Training Epoch: 2/2, step 6007/7134 completed (loss: 0.043093156069517136, acc: 1.0)
[2025-02-13 21:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:05][root][INFO] - Training Epoch: 2/2, step 6008/7134 completed (loss: 0.07825479656457901, acc: 0.9790209531784058)
[2025-02-13 21:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:06][root][INFO] - Training Epoch: 2/2, step 6009/7134 completed (loss: 0.07096626609563828, acc: 0.9924812316894531)
[2025-02-13 21:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:06][root][INFO] - Training Epoch: 2/2, step 6010/7134 completed (loss: 0.11159278452396393, acc: 0.9620253443717957)
[2025-02-13 21:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:06][root][INFO] - Training Epoch: 2/2, step 6011/7134 completed (loss: 0.17218869924545288, acc: 0.9702380895614624)
[2025-02-13 21:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:07][root][INFO] - Training Epoch: 2/2, step 6012/7134 completed (loss: 0.06219181418418884, acc: 0.9818181991577148)
[2025-02-13 21:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:07][root][INFO] - Training Epoch: 2/2, step 6013/7134 completed (loss: 0.09329129010438919, acc: 0.9719101190567017)
[2025-02-13 21:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:08][root][INFO] - Training Epoch: 2/2, step 6014/7134 completed (loss: 0.07233838737010956, acc: 0.9763779640197754)
[2025-02-13 21:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:08][root][INFO] - Training Epoch: 2/2, step 6015/7134 completed (loss: 0.0802590548992157, acc: 0.9819819927215576)
[2025-02-13 21:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:08][root][INFO] - Training Epoch: 2/2, step 6016/7134 completed (loss: 0.23755447566509247, acc: 0.9729729890823364)
[2025-02-13 21:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:09][root][INFO] - Training Epoch: 2/2, step 6017/7134 completed (loss: 0.02524017170071602, acc: 1.0)
[2025-02-13 21:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:09][root][INFO] - Training Epoch: 2/2, step 6018/7134 completed (loss: 0.13338603079319, acc: 0.9615384340286255)
[2025-02-13 21:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:09][root][INFO] - Training Epoch: 2/2, step 6019/7134 completed (loss: 0.022617071866989136, acc: 1.0)
[2025-02-13 21:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:10][root][INFO] - Training Epoch: 2/2, step 6020/7134 completed (loss: 0.009652047418057919, acc: 1.0)
[2025-02-13 21:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:10][root][INFO] - Training Epoch: 2/2, step 6021/7134 completed (loss: 0.019962327554821968, acc: 1.0)
[2025-02-13 21:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:11][root][INFO] - Training Epoch: 2/2, step 6022/7134 completed (loss: 0.024686608463525772, acc: 1.0)
[2025-02-13 21:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:11][root][INFO] - Training Epoch: 2/2, step 6023/7134 completed (loss: 0.0179153885692358, acc: 1.0)
[2025-02-13 21:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:11][root][INFO] - Training Epoch: 2/2, step 6024/7134 completed (loss: 0.016646672040224075, acc: 1.0)
[2025-02-13 21:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:12][root][INFO] - Training Epoch: 2/2, step 6025/7134 completed (loss: 0.06758486479520798, acc: 0.9836065769195557)
[2025-02-13 21:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:12][root][INFO] - Training Epoch: 2/2, step 6026/7134 completed (loss: 0.29278799891471863, acc: 0.9661017060279846)
[2025-02-13 21:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:12][root][INFO] - Training Epoch: 2/2, step 6027/7134 completed (loss: 0.04091596230864525, acc: 0.9887640476226807)
[2025-02-13 21:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:13][root][INFO] - Training Epoch: 2/2, step 6028/7134 completed (loss: 0.017835645005106926, acc: 1.0)
[2025-02-13 21:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:13][root][INFO] - Training Epoch: 2/2, step 6029/7134 completed (loss: 0.05965658649802208, acc: 0.98591548204422)
[2025-02-13 21:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:14][root][INFO] - Training Epoch: 2/2, step 6030/7134 completed (loss: 0.04462234303355217, acc: 0.9848484992980957)
[2025-02-13 21:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:14][root][INFO] - Training Epoch: 2/2, step 6031/7134 completed (loss: 0.020739590749144554, acc: 1.0)
[2025-02-13 21:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:14][root][INFO] - Training Epoch: 2/2, step 6032/7134 completed (loss: 0.025909962132573128, acc: 0.9900990128517151)
[2025-02-13 21:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:15][root][INFO] - Training Epoch: 2/2, step 6033/7134 completed (loss: 0.10436305403709412, acc: 0.9696969985961914)
[2025-02-13 21:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:15][root][INFO] - Training Epoch: 2/2, step 6034/7134 completed (loss: 0.12864483892917633, acc: 0.9552238583564758)
[2025-02-13 21:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:15][root][INFO] - Training Epoch: 2/2, step 6035/7134 completed (loss: 0.20612679421901703, acc: 0.9483568072319031)
[2025-02-13 21:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:16][root][INFO] - Training Epoch: 2/2, step 6036/7134 completed (loss: 0.21319925785064697, acc: 0.9438202381134033)
[2025-02-13 21:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:16][root][INFO] - Training Epoch: 2/2, step 6037/7134 completed (loss: 0.11151999235153198, acc: 0.9503546357154846)
[2025-02-13 21:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:17][root][INFO] - Training Epoch: 2/2, step 6038/7134 completed (loss: 0.09203984588384628, acc: 0.9754902124404907)
[2025-02-13 21:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:17][root][INFO] - Training Epoch: 2/2, step 6039/7134 completed (loss: 0.15314410626888275, acc: 0.9609755873680115)
[2025-02-13 21:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:17][root][INFO] - Training Epoch: 2/2, step 6040/7134 completed (loss: 0.08459113538265228, acc: 0.9779735803604126)
[2025-02-13 21:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:18][root][INFO] - Training Epoch: 2/2, step 6041/7134 completed (loss: 0.20766253769397736, acc: 0.9516128897666931)
[2025-02-13 21:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:18][root][INFO] - Training Epoch: 2/2, step 6042/7134 completed (loss: 0.08548236638307571, acc: 0.9724770784378052)
[2025-02-13 21:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:18][root][INFO] - Training Epoch: 2/2, step 6043/7134 completed (loss: 0.09591300785541534, acc: 0.9731183052062988)
[2025-02-13 21:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:19][root][INFO] - Training Epoch: 2/2, step 6044/7134 completed (loss: 0.06466669589281082, acc: 0.9857142567634583)
[2025-02-13 21:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:19][root][INFO] - Training Epoch: 2/2, step 6045/7134 completed (loss: 0.19349201023578644, acc: 0.9712918400764465)
[2025-02-13 21:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:20][root][INFO] - Training Epoch: 2/2, step 6046/7134 completed (loss: 0.13072533905506134, acc: 0.9520547986030579)
[2025-02-13 21:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:20][root][INFO] - Training Epoch: 2/2, step 6047/7134 completed (loss: 0.2175399214029312, acc: 0.918367326259613)
[2025-02-13 21:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:20][root][INFO] - Training Epoch: 2/2, step 6048/7134 completed (loss: 0.10248667746782303, acc: 0.9593495726585388)
[2025-02-13 21:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:21][root][INFO] - Training Epoch: 2/2, step 6049/7134 completed (loss: 0.10399782657623291, acc: 0.9790209531784058)
[2025-02-13 21:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:21][root][INFO] - Training Epoch: 2/2, step 6050/7134 completed (loss: 0.16173475980758667, acc: 0.9653179049491882)
[2025-02-13 21:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:21][root][INFO] - Training Epoch: 2/2, step 6051/7134 completed (loss: 0.17771290242671967, acc: 0.9488636255264282)
[2025-02-13 21:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:22][root][INFO] - Training Epoch: 2/2, step 6052/7134 completed (loss: 0.16001398861408234, acc: 0.9466666579246521)
[2025-02-13 21:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:22][root][INFO] - Training Epoch: 2/2, step 6053/7134 completed (loss: 0.14131204783916473, acc: 0.9646017551422119)
[2025-02-13 21:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:23][root][INFO] - Training Epoch: 2/2, step 6054/7134 completed (loss: 0.07520636171102524, acc: 0.984455943107605)
[2025-02-13 21:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:23][root][INFO] - Training Epoch: 2/2, step 6055/7134 completed (loss: 0.16596673429012299, acc: 0.9415584206581116)
[2025-02-13 21:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:23][root][INFO] - Training Epoch: 2/2, step 6056/7134 completed (loss: 0.08781613409519196, acc: 0.9909909963607788)
[2025-02-13 21:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:24][root][INFO] - Training Epoch: 2/2, step 6057/7134 completed (loss: 0.03885326907038689, acc: 0.9870129823684692)
[2025-02-13 21:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:24][root][INFO] - Training Epoch: 2/2, step 6058/7134 completed (loss: 0.017195457592606544, acc: 1.0)
[2025-02-13 21:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:24][root][INFO] - Training Epoch: 2/2, step 6059/7134 completed (loss: 0.1745898276567459, acc: 0.931506872177124)
[2025-02-13 21:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:25][root][INFO] - Training Epoch: 2/2, step 6060/7134 completed (loss: 0.10137160867452621, acc: 0.9938271641731262)
[2025-02-13 21:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:25][root][INFO] - Training Epoch: 2/2, step 6061/7134 completed (loss: 0.07512212544679642, acc: 0.9788732528686523)
[2025-02-13 21:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:26][root][INFO] - Training Epoch: 2/2, step 6062/7134 completed (loss: 0.1417054831981659, acc: 0.9576719403266907)
[2025-02-13 21:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:26][root][INFO] - Training Epoch: 2/2, step 6063/7134 completed (loss: 0.05338134244084358, acc: 0.991525411605835)
[2025-02-13 21:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:26][root][INFO] - Training Epoch: 2/2, step 6064/7134 completed (loss: 0.12307503819465637, acc: 0.9627329111099243)
[2025-02-13 21:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:27][root][INFO] - Training Epoch: 2/2, step 6065/7134 completed (loss: 0.07633736729621887, acc: 0.989847719669342)
[2025-02-13 21:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:27][root][INFO] - Training Epoch: 2/2, step 6066/7134 completed (loss: 0.11690380424261093, acc: 0.970370352268219)
[2025-02-13 21:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:27][root][INFO] - Training Epoch: 2/2, step 6067/7134 completed (loss: 0.16439442336559296, acc: 0.9407894611358643)
[2025-02-13 21:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:28][root][INFO] - Training Epoch: 2/2, step 6068/7134 completed (loss: 0.24698032438755035, acc: 0.9243243336677551)
[2025-02-13 21:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:28][root][INFO] - Training Epoch: 2/2, step 6069/7134 completed (loss: 0.169247567653656, acc: 0.9647058844566345)
[2025-02-13 21:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:29][root][INFO] - Training Epoch: 2/2, step 6070/7134 completed (loss: 0.11180049180984497, acc: 0.9657142758369446)
[2025-02-13 21:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:29][root][INFO] - Training Epoch: 2/2, step 6071/7134 completed (loss: 0.14336161315441132, acc: 0.9467455744743347)
[2025-02-13 21:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:29][root][INFO] - Training Epoch: 2/2, step 6072/7134 completed (loss: 0.18987877666950226, acc: 0.9416666626930237)
[2025-02-13 21:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:30][root][INFO] - Training Epoch: 2/2, step 6073/7134 completed (loss: 0.14400137960910797, acc: 0.9426751732826233)
[2025-02-13 21:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:30][root][INFO] - Training Epoch: 2/2, step 6074/7134 completed (loss: 0.22348324954509735, acc: 0.9677419066429138)
[2025-02-13 21:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:30][root][INFO] - Training Epoch: 2/2, step 6075/7134 completed (loss: 0.12417609244585037, acc: 0.9789473414421082)
[2025-02-13 21:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:31][root][INFO] - Training Epoch: 2/2, step 6076/7134 completed (loss: 0.21309970319271088, acc: 0.9358288645744324)
[2025-02-13 21:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:31][root][INFO] - Training Epoch: 2/2, step 6077/7134 completed (loss: 0.04107013717293739, acc: 0.9940828680992126)
[2025-02-13 21:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:32][root][INFO] - Training Epoch: 2/2, step 6078/7134 completed (loss: 0.1726510226726532, acc: 0.9774011373519897)
[2025-02-13 21:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:32][root][INFO] - Training Epoch: 2/2, step 6079/7134 completed (loss: 0.09538298845291138, acc: 0.9696969985961914)
[2025-02-13 21:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:33][root][INFO] - Training Epoch: 2/2, step 6080/7134 completed (loss: 0.07164811342954636, acc: 0.9885057210922241)
[2025-02-13 21:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:33][root][INFO] - Training Epoch: 2/2, step 6081/7134 completed (loss: 0.10774009674787521, acc: 0.9662162065505981)
[2025-02-13 21:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:33][root][INFO] - Training Epoch: 2/2, step 6082/7134 completed (loss: 0.05007326602935791, acc: 0.9798657894134521)
[2025-02-13 21:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:34][root][INFO] - Training Epoch: 2/2, step 6083/7134 completed (loss: 0.04720861837267876, acc: 0.9792746305465698)
[2025-02-13 21:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:34][root][INFO] - Training Epoch: 2/2, step 6084/7134 completed (loss: 0.05168294906616211, acc: 0.9866666793823242)
[2025-02-13 21:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:35][root][INFO] - Training Epoch: 2/2, step 6085/7134 completed (loss: 0.09184800088405609, acc: 0.9652777910232544)
[2025-02-13 21:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:35][root][INFO] - Training Epoch: 2/2, step 6086/7134 completed (loss: 0.04435023292899132, acc: 0.9890710115432739)
[2025-02-13 21:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:35][root][INFO] - Training Epoch: 2/2, step 6087/7134 completed (loss: 0.07778865844011307, acc: 0.9852941036224365)
[2025-02-13 21:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:36][root][INFO] - Training Epoch: 2/2, step 6088/7134 completed (loss: 0.10529854148626328, acc: 0.9675675630569458)
[2025-02-13 21:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:36][root][INFO] - Training Epoch: 2/2, step 6089/7134 completed (loss: 0.06321435421705246, acc: 0.9828571677207947)
[2025-02-13 21:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:37][root][INFO] - Training Epoch: 2/2, step 6090/7134 completed (loss: 0.04942407086491585, acc: 0.9893048405647278)
[2025-02-13 21:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:37][root][INFO] - Training Epoch: 2/2, step 6091/7134 completed (loss: 0.026730407029390335, acc: 1.0)
[2025-02-13 21:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:38][root][INFO] - Training Epoch: 2/2, step 6092/7134 completed (loss: 0.051606185734272, acc: 0.9878787994384766)
[2025-02-13 21:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:38][root][INFO] - Training Epoch: 2/2, step 6093/7134 completed (loss: 0.08801283687353134, acc: 0.9701492786407471)
[2025-02-13 21:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:38][root][INFO] - Training Epoch: 2/2, step 6094/7134 completed (loss: 0.21007056534290314, acc: 0.9375)
[2025-02-13 21:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:39][root][INFO] - Training Epoch: 2/2, step 6095/7134 completed (loss: 0.07713795453310013, acc: 0.987730085849762)
[2025-02-13 21:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:39][root][INFO] - Training Epoch: 2/2, step 6096/7134 completed (loss: 0.16537755727767944, acc: 0.9492385983467102)
[2025-02-13 21:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:40][root][INFO] - Training Epoch: 2/2, step 6097/7134 completed (loss: 0.07910168915987015, acc: 0.9689440727233887)
[2025-02-13 21:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:40][root][INFO] - Training Epoch: 2/2, step 6098/7134 completed (loss: 0.08217970281839371, acc: 0.9649122953414917)
[2025-02-13 21:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:40][root][INFO] - Training Epoch: 2/2, step 6099/7134 completed (loss: 0.06552086025476456, acc: 0.9863945841789246)
[2025-02-13 21:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:41][root][INFO] - Training Epoch: 2/2, step 6100/7134 completed (loss: 0.16769230365753174, acc: 0.9743589758872986)
[2025-02-13 21:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:41][root][INFO] - Training Epoch: 2/2, step 6101/7134 completed (loss: 0.016750456765294075, acc: 1.0)
[2025-02-13 21:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:41][root][INFO] - Training Epoch: 2/2, step 6102/7134 completed (loss: 0.22317394614219666, acc: 0.9453125)
[2025-02-13 21:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:42][root][INFO] - Training Epoch: 2/2, step 6103/7134 completed (loss: 0.059175439178943634, acc: 0.9818181991577148)
[2025-02-13 21:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:42][root][INFO] - Training Epoch: 2/2, step 6104/7134 completed (loss: 0.2505333125591278, acc: 0.9324324131011963)
[2025-02-13 21:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:43][root][INFO] - Training Epoch: 2/2, step 6105/7134 completed (loss: 0.39802610874176025, acc: 0.9212121367454529)
[2025-02-13 21:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:43][root][INFO] - Training Epoch: 2/2, step 6106/7134 completed (loss: 0.06107285991311073, acc: 0.9842519760131836)
[2025-02-13 21:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:43][root][INFO] - Training Epoch: 2/2, step 6107/7134 completed (loss: 0.022146454080939293, acc: 1.0)
[2025-02-13 21:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:44][root][INFO] - Training Epoch: 2/2, step 6108/7134 completed (loss: 0.12957721948623657, acc: 0.9586777091026306)
[2025-02-13 21:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:44][root][INFO] - Training Epoch: 2/2, step 6109/7134 completed (loss: 0.04205223172903061, acc: 0.9826086759567261)
[2025-02-13 21:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:44][root][INFO] - Training Epoch: 2/2, step 6110/7134 completed (loss: 0.1237611323595047, acc: 0.96875)
[2025-02-13 21:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:45][root][INFO] - Training Epoch: 2/2, step 6111/7134 completed (loss: 0.07140377163887024, acc: 0.989130437374115)
[2025-02-13 21:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:45][root][INFO] - Training Epoch: 2/2, step 6112/7134 completed (loss: 0.0527469664812088, acc: 0.9934640526771545)
[2025-02-13 21:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:46][root][INFO] - Training Epoch: 2/2, step 6113/7134 completed (loss: 0.08820437639951706, acc: 0.9756097793579102)
[2025-02-13 21:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:46][root][INFO] - Training Epoch: 2/2, step 6114/7134 completed (loss: 0.10454954206943512, acc: 0.9860140085220337)
[2025-02-13 21:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:46][root][INFO] - Training Epoch: 2/2, step 6115/7134 completed (loss: 0.04168287292122841, acc: 0.9919999837875366)
[2025-02-13 21:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:47][root][INFO] - Training Epoch: 2/2, step 6116/7134 completed (loss: 0.13936318457126617, acc: 0.9596773982048035)
[2025-02-13 21:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:47][root][INFO] - Training Epoch: 2/2, step 6117/7134 completed (loss: 0.12927542626857758, acc: 0.9520958065986633)
[2025-02-13 21:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:47][root][INFO] - Training Epoch: 2/2, step 6118/7134 completed (loss: 0.03894100338220596, acc: 0.9920634627342224)
[2025-02-13 21:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:48][root][INFO] - Training Epoch: 2/2, step 6119/7134 completed (loss: 0.0733267068862915, acc: 0.9729729890823364)
[2025-02-13 21:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:48][root][INFO] - Training Epoch: 2/2, step 6120/7134 completed (loss: 0.037166569381952286, acc: 0.9924242496490479)
[2025-02-13 21:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:49][root][INFO] - Training Epoch: 2/2, step 6121/7134 completed (loss: 0.125362828373909, acc: 0.9685039520263672)
[2025-02-13 21:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:49][root][INFO] - Training Epoch: 2/2, step 6122/7134 completed (loss: 0.04295387491583824, acc: 0.9919354915618896)
[2025-02-13 21:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:49][root][INFO] - Training Epoch: 2/2, step 6123/7134 completed (loss: 0.0667821541428566, acc: 0.9914529919624329)
[2025-02-13 21:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:50][root][INFO] - Training Epoch: 2/2, step 6124/7134 completed (loss: 0.024613581597805023, acc: 1.0)
[2025-02-13 21:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:50][root][INFO] - Training Epoch: 2/2, step 6125/7134 completed (loss: 0.2391129583120346, acc: 0.9626168012619019)
[2025-02-13 21:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:51][root][INFO] - Training Epoch: 2/2, step 6126/7134 completed (loss: 0.1342959702014923, acc: 0.9801324605941772)
[2025-02-13 21:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:51][root][INFO] - Training Epoch: 2/2, step 6127/7134 completed (loss: 0.033236805349588394, acc: 1.0)
[2025-02-13 21:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:51][root][INFO] - Training Epoch: 2/2, step 6128/7134 completed (loss: 0.04797522723674774, acc: 1.0)
[2025-02-13 21:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:52][root][INFO] - Training Epoch: 2/2, step 6129/7134 completed (loss: 0.14199066162109375, acc: 0.9704142212867737)
[2025-02-13 21:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:52][root][INFO] - Training Epoch: 2/2, step 6130/7134 completed (loss: 0.1185825988650322, acc: 0.9626865386962891)
[2025-02-13 21:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:53][root][INFO] - Training Epoch: 2/2, step 6131/7134 completed (loss: 0.1541147530078888, acc: 0.9611650705337524)
[2025-02-13 21:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:53][root][INFO] - Training Epoch: 2/2, step 6132/7134 completed (loss: 0.07486542314291, acc: 0.9807692170143127)
[2025-02-13 21:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:53][root][INFO] - Training Epoch: 2/2, step 6133/7134 completed (loss: 0.07386664301156998, acc: 0.9903846383094788)
[2025-02-13 21:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:54][root][INFO] - Training Epoch: 2/2, step 6134/7134 completed (loss: 0.18306154012680054, acc: 0.9479768872261047)
[2025-02-13 21:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:54][root][INFO] - Training Epoch: 2/2, step 6135/7134 completed (loss: 0.03786291182041168, acc: 0.9922480583190918)
[2025-02-13 21:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:55][root][INFO] - Training Epoch: 2/2, step 6136/7134 completed (loss: 0.1582203060388565, acc: 0.976047933101654)
[2025-02-13 21:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:55][root][INFO] - Training Epoch: 2/2, step 6137/7134 completed (loss: 0.11947001516819, acc: 0.9805194735527039)
[2025-02-13 21:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:55][root][INFO] - Training Epoch: 2/2, step 6138/7134 completed (loss: 0.13239459693431854, acc: 0.9693251252174377)
[2025-02-13 21:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:56][root][INFO] - Training Epoch: 2/2, step 6139/7134 completed (loss: 0.11761514842510223, acc: 0.9795918464660645)
[2025-02-13 21:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:56][root][INFO] - Training Epoch: 2/2, step 6140/7134 completed (loss: 0.05581061914563179, acc: 0.9923076629638672)
[2025-02-13 21:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:56][root][INFO] - Training Epoch: 2/2, step 6141/7134 completed (loss: 0.14806871116161346, acc: 0.9855072498321533)
[2025-02-13 21:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:57][root][INFO] - Training Epoch: 2/2, step 6142/7134 completed (loss: 0.0738498717546463, acc: 0.9828571677207947)
[2025-02-13 21:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:57][root][INFO] - Training Epoch: 2/2, step 6143/7134 completed (loss: 0.09445856511592865, acc: 0.9939393997192383)
[2025-02-13 21:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:58][root][INFO] - Training Epoch: 2/2, step 6144/7134 completed (loss: 0.05356566607952118, acc: 0.9931507110595703)
[2025-02-13 21:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:58][root][INFO] - Training Epoch: 2/2, step 6145/7134 completed (loss: 0.14611169695854187, acc: 0.9726775884628296)
[2025-02-13 21:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:58][root][INFO] - Training Epoch: 2/2, step 6146/7134 completed (loss: 0.12061988562345505, acc: 0.9759036302566528)
[2025-02-13 21:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:59][root][INFO] - Training Epoch: 2/2, step 6147/7134 completed (loss: 0.04082128033041954, acc: 0.9921259880065918)
[2025-02-13 21:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:59][root][INFO] - Training Epoch: 2/2, step 6148/7134 completed (loss: 0.04945052042603493, acc: 1.0)
[2025-02-13 21:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:59][root][INFO] - Training Epoch: 2/2, step 6149/7134 completed (loss: 0.14065539836883545, acc: 0.9735099077224731)
[2025-02-13 21:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:00][root][INFO] - Training Epoch: 2/2, step 6150/7134 completed (loss: 0.043723322451114655, acc: 0.9860140085220337)
[2025-02-13 21:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:00][root][INFO] - Training Epoch: 2/2, step 6151/7134 completed (loss: 0.03141094371676445, acc: 1.0)
[2025-02-13 21:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:01][root][INFO] - Training Epoch: 2/2, step 6152/7134 completed (loss: 0.2003161609172821, acc: 0.977142870426178)
[2025-02-13 21:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:01][root][INFO] - Training Epoch: 2/2, step 6153/7134 completed (loss: 0.05396876484155655, acc: 0.9876543283462524)
[2025-02-13 21:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:01][root][INFO] - Training Epoch: 2/2, step 6154/7134 completed (loss: 0.10584678500890732, acc: 0.988950252532959)
[2025-02-13 21:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:02][root][INFO] - Training Epoch: 2/2, step 6155/7134 completed (loss: 0.0892004445195198, acc: 0.976331353187561)
[2025-02-13 21:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:02][root][INFO] - Training Epoch: 2/2, step 6156/7134 completed (loss: 0.0578717477619648, acc: 0.9810126423835754)
[2025-02-13 21:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:02][root][INFO] - Training Epoch: 2/2, step 6157/7134 completed (loss: 0.0594661682844162, acc: 0.9873417615890503)
[2025-02-13 21:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:03][root][INFO] - Training Epoch: 2/2, step 6158/7134 completed (loss: 0.0820169746875763, acc: 0.9736841917037964)
[2025-02-13 21:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:03][root][INFO] - Training Epoch: 2/2, step 6159/7134 completed (loss: 0.1488664448261261, acc: 0.9642857313156128)
[2025-02-13 21:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:03][root][INFO] - Training Epoch: 2/2, step 6160/7134 completed (loss: 0.18482153117656708, acc: 0.9426751732826233)
[2025-02-13 21:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:04][root][INFO] - Training Epoch: 2/2, step 6161/7134 completed (loss: 0.1229911595582962, acc: 0.9685039520263672)
[2025-02-13 21:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:04][root][INFO] - Training Epoch: 2/2, step 6162/7134 completed (loss: 0.22954146564006805, acc: 0.9615384340286255)
[2025-02-13 21:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:05][root][INFO] - Training Epoch: 2/2, step 6163/7134 completed (loss: 0.1984943002462387, acc: 0.961240291595459)
[2025-02-13 21:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:05][root][INFO] - Training Epoch: 2/2, step 6164/7134 completed (loss: 0.15379923582077026, acc: 0.957317054271698)
[2025-02-13 21:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:05][root][INFO] - Training Epoch: 2/2, step 6165/7134 completed (loss: 0.05979615077376366, acc: 0.9923664331436157)
[2025-02-13 21:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:06][root][INFO] - Training Epoch: 2/2, step 6166/7134 completed (loss: 0.059110675007104874, acc: 0.9927007555961609)
[2025-02-13 21:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:06][root][INFO] - Training Epoch: 2/2, step 6167/7134 completed (loss: 0.03399232029914856, acc: 0.9825581312179565)
[2025-02-13 21:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:06][root][INFO] - Training Epoch: 2/2, step 6168/7134 completed (loss: 0.07342837750911713, acc: 0.9892473220825195)
[2025-02-13 21:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:07][root][INFO] - Training Epoch: 2/2, step 6169/7134 completed (loss: 0.10549475997686386, acc: 0.9759036302566528)
[2025-02-13 21:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:07][root][INFO] - Training Epoch: 2/2, step 6170/7134 completed (loss: 0.04945998266339302, acc: 0.9928057789802551)
[2025-02-13 21:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:08][root][INFO] - Training Epoch: 2/2, step 6171/7134 completed (loss: 0.04600612819194794, acc: 0.9835164546966553)
[2025-02-13 21:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:08][root][INFO] - Training Epoch: 2/2, step 6172/7134 completed (loss: 0.044716253876686096, acc: 0.984375)
[2025-02-13 21:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:08][root][INFO] - Training Epoch: 2/2, step 6173/7134 completed (loss: 0.025292333215475082, acc: 1.0)
[2025-02-13 21:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:09][root][INFO] - Training Epoch: 2/2, step 6174/7134 completed (loss: 0.017420509830117226, acc: 0.9939393997192383)
[2025-02-13 21:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:09][root][INFO] - Training Epoch: 2/2, step 6175/7134 completed (loss: 0.08268838375806808, acc: 0.9814814925193787)
[2025-02-13 21:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:10][root][INFO] - Training Epoch: 2/2, step 6176/7134 completed (loss: 0.04374246299266815, acc: 0.9939024448394775)
[2025-02-13 21:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:10][root][INFO] - Training Epoch: 2/2, step 6177/7134 completed (loss: 0.026832690462470055, acc: 1.0)
[2025-02-13 21:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:10][root][INFO] - Training Epoch: 2/2, step 6178/7134 completed (loss: 0.1552751362323761, acc: 0.9673202633857727)
[2025-02-13 21:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:11][root][INFO] - Training Epoch: 2/2, step 6179/7134 completed (loss: 0.13614541292190552, acc: 0.9599999785423279)
[2025-02-13 21:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:11][root][INFO] - Training Epoch: 2/2, step 6180/7134 completed (loss: 0.29494422674179077, acc: 0.9680851101875305)
[2025-02-13 21:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:11][root][INFO] - Training Epoch: 2/2, step 6181/7134 completed (loss: 0.2161923348903656, acc: 0.9583333134651184)
[2025-02-13 21:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:12][root][INFO] - Training Epoch: 2/2, step 6182/7134 completed (loss: 0.13692910969257355, acc: 0.9534883499145508)
[2025-02-13 21:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:12][root][INFO] - Training Epoch: 2/2, step 6183/7134 completed (loss: 0.3707996904850006, acc: 0.895061731338501)
[2025-02-13 21:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:13][root][INFO] - Training Epoch: 2/2, step 6184/7134 completed (loss: 0.2607274651527405, acc: 0.9304347634315491)
[2025-02-13 21:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:13][root][INFO] - Training Epoch: 2/2, step 6185/7134 completed (loss: 0.104415662586689, acc: 0.9677419066429138)
[2025-02-13 21:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:13][root][INFO] - Training Epoch: 2/2, step 6186/7134 completed (loss: 0.049254171550273895, acc: 0.9923664331436157)
[2025-02-13 21:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:14][root][INFO] - Training Epoch: 2/2, step 6187/7134 completed (loss: 0.10045096278190613, acc: 0.988304078578949)
[2025-02-13 21:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:14][root][INFO] - Training Epoch: 2/2, step 6188/7134 completed (loss: 0.1541491001844406, acc: 0.9607843160629272)
[2025-02-13 21:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:15][root][INFO] - Training Epoch: 2/2, step 6189/7134 completed (loss: 0.15518903732299805, acc: 0.9720279574394226)
[2025-02-13 21:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:15][root][INFO] - Training Epoch: 2/2, step 6190/7134 completed (loss: 0.06800075620412827, acc: 0.9928571581840515)
[2025-02-13 21:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:15][root][INFO] - Training Epoch: 2/2, step 6191/7134 completed (loss: 0.08281605690717697, acc: 0.9837398529052734)
[2025-02-13 21:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:16][root][INFO] - Training Epoch: 2/2, step 6192/7134 completed (loss: 0.042287420481443405, acc: 0.9902439117431641)
[2025-02-13 21:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:16][root][INFO] - Training Epoch: 2/2, step 6193/7134 completed (loss: 0.06764713674783707, acc: 0.9848484992980957)
[2025-02-13 21:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:17][root][INFO] - Training Epoch: 2/2, step 6194/7134 completed (loss: 0.03243030235171318, acc: 0.9918032884597778)
[2025-02-13 21:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:17][root][INFO] - Training Epoch: 2/2, step 6195/7134 completed (loss: 0.0409817099571228, acc: 0.9905660152435303)
[2025-02-13 21:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:17][root][INFO] - Training Epoch: 2/2, step 6196/7134 completed (loss: 0.050645217299461365, acc: 0.9876543283462524)
[2025-02-13 21:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:18][root][INFO] - Training Epoch: 2/2, step 6197/7134 completed (loss: 0.11974525451660156, acc: 0.9772727489471436)
[2025-02-13 21:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:18][root][INFO] - Training Epoch: 2/2, step 6198/7134 completed (loss: 0.08539502322673798, acc: 0.9734513163566589)
[2025-02-13 21:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:18][root][INFO] - Training Epoch: 2/2, step 6199/7134 completed (loss: 0.07273846864700317, acc: 0.9841269850730896)
[2025-02-13 21:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:19][root][INFO] - Training Epoch: 2/2, step 6200/7134 completed (loss: 0.0349830761551857, acc: 0.9948717951774597)
[2025-02-13 21:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:19][root][INFO] - Training Epoch: 2/2, step 6201/7134 completed (loss: 0.06377119570970535, acc: 0.9712918400764465)
[2025-02-13 21:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:20][root][INFO] - Training Epoch: 2/2, step 6202/7134 completed (loss: 0.10538066923618317, acc: 0.9714285731315613)
[2025-02-13 21:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:20][root][INFO] - Training Epoch: 2/2, step 6203/7134 completed (loss: 0.041934896260499954, acc: 0.9883720874786377)
[2025-02-13 21:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:21][root][INFO] - Training Epoch: 2/2, step 6204/7134 completed (loss: 0.01592697575688362, acc: 1.0)
[2025-02-13 21:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:21][root][INFO] - Training Epoch: 2/2, step 6205/7134 completed (loss: 0.07507485151290894, acc: 0.9784172773361206)
[2025-02-13 21:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:21][root][INFO] - Training Epoch: 2/2, step 6206/7134 completed (loss: 0.045076120644807816, acc: 0.9875776171684265)
[2025-02-13 21:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:22][root][INFO] - Training Epoch: 2/2, step 6207/7134 completed (loss: 0.2604514956474304, acc: 0.9333333373069763)
[2025-02-13 21:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:22][root][INFO] - Training Epoch: 2/2, step 6208/7134 completed (loss: 0.05575154349207878, acc: 0.984375)
[2025-02-13 21:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:23][root][INFO] - Training Epoch: 2/2, step 6209/7134 completed (loss: 0.06280747056007385, acc: 0.9821428656578064)
[2025-02-13 21:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:23][root][INFO] - Training Epoch: 2/2, step 6210/7134 completed (loss: 0.08808111399412155, acc: 0.9679144620895386)
[2025-02-13 21:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:23][root][INFO] - Training Epoch: 2/2, step 6211/7134 completed (loss: 0.052176348865032196, acc: 0.9939393997192383)
[2025-02-13 21:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:24][root][INFO] - Training Epoch: 2/2, step 6212/7134 completed (loss: 0.019520919770002365, acc: 1.0)
[2025-02-13 21:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:24][root][INFO] - Training Epoch: 2/2, step 6213/7134 completed (loss: 0.01991778239607811, acc: 1.0)
[2025-02-13 21:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:25][root][INFO] - Training Epoch: 2/2, step 6214/7134 completed (loss: 0.03215290978550911, acc: 0.9887005686759949)
[2025-02-13 21:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:25][root][INFO] - Training Epoch: 2/2, step 6215/7134 completed (loss: 0.12169937789440155, acc: 0.9758453965187073)
[2025-02-13 21:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:25][root][INFO] - Training Epoch: 2/2, step 6216/7134 completed (loss: 0.017638666555285454, acc: 1.0)
[2025-02-13 21:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:26][root][INFO] - Training Epoch: 2/2, step 6217/7134 completed (loss: 0.04043332487344742, acc: 0.9873417615890503)
[2025-02-13 21:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:26][root][INFO] - Training Epoch: 2/2, step 6218/7134 completed (loss: 0.04977943003177643, acc: 0.9870967864990234)
[2025-02-13 21:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:27][root][INFO] - Training Epoch: 2/2, step 6219/7134 completed (loss: 0.06124649941921234, acc: 0.9738562107086182)
[2025-02-13 21:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:27][root][INFO] - Training Epoch: 2/2, step 6220/7134 completed (loss: 0.06149746850132942, acc: 0.977011501789093)
[2025-02-13 21:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:28][root][INFO] - Training Epoch: 2/2, step 6221/7134 completed (loss: 0.011963386088609695, acc: 1.0)
[2025-02-13 21:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:28][root][INFO] - Training Epoch: 2/2, step 6222/7134 completed (loss: 0.08128773421049118, acc: 0.9819276928901672)
[2025-02-13 21:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:29][root][INFO] - Training Epoch: 2/2, step 6223/7134 completed (loss: 0.20011264085769653, acc: 0.9583333134651184)
[2025-02-13 21:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:29][root][INFO] - Training Epoch: 2/2, step 6224/7134 completed (loss: 0.026508353650569916, acc: 1.0)
[2025-02-13 21:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:29][root][INFO] - Training Epoch: 2/2, step 6225/7134 completed (loss: 0.08190213143825531, acc: 0.9720670580863953)
[2025-02-13 21:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:30][root][INFO] - Training Epoch: 2/2, step 6226/7134 completed (loss: 0.0889989510178566, acc: 0.97826087474823)
[2025-02-13 21:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:30][root][INFO] - Training Epoch: 2/2, step 6227/7134 completed (loss: 0.03702337667346001, acc: 0.9937499761581421)
[2025-02-13 21:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:31][root][INFO] - Training Epoch: 2/2, step 6228/7134 completed (loss: 0.0830058753490448, acc: 0.9840425252914429)
[2025-02-13 21:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:31][root][INFO] - Training Epoch: 2/2, step 6229/7134 completed (loss: 0.15912914276123047, acc: 0.9560975432395935)
[2025-02-13 21:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:31][root][INFO] - Training Epoch: 2/2, step 6230/7134 completed (loss: 0.11197229474782944, acc: 0.957446813583374)
[2025-02-13 21:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:32][root][INFO] - Training Epoch: 2/2, step 6231/7134 completed (loss: 0.07041066884994507, acc: 0.9707602262496948)
[2025-02-13 21:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:32][root][INFO] - Training Epoch: 2/2, step 6232/7134 completed (loss: 0.1106603741645813, acc: 0.9780219793319702)
[2025-02-13 21:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:33][root][INFO] - Training Epoch: 2/2, step 6233/7134 completed (loss: 0.04192536696791649, acc: 0.9942528605461121)
[2025-02-13 21:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:33][root][INFO] - Training Epoch: 2/2, step 6234/7134 completed (loss: 0.12294546514749527, acc: 0.9777777791023254)
[2025-02-13 21:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:33][root][INFO] - Training Epoch: 2/2, step 6235/7134 completed (loss: 0.04561196640133858, acc: 0.9821428656578064)
[2025-02-13 21:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:34][root][INFO] - Training Epoch: 2/2, step 6236/7134 completed (loss: 0.08528959006071091, acc: 0.9776536226272583)
[2025-02-13 21:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:34][root][INFO] - Training Epoch: 2/2, step 6237/7134 completed (loss: 0.14240901172161102, acc: 0.9661017060279846)
[2025-02-13 21:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:34][root][INFO] - Training Epoch: 2/2, step 6238/7134 completed (loss: 0.05899326503276825, acc: 0.9801324605941772)
[2025-02-13 21:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:35][root][INFO] - Training Epoch: 2/2, step 6239/7134 completed (loss: 0.10599815100431442, acc: 0.9689119458198547)
[2025-02-13 21:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:35][root][INFO] - Training Epoch: 2/2, step 6240/7134 completed (loss: 0.01791774481534958, acc: 0.9939758777618408)
[2025-02-13 21:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:36][root][INFO] - Training Epoch: 2/2, step 6241/7134 completed (loss: 0.038618508726358414, acc: 1.0)
[2025-02-13 21:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:36][root][INFO] - Training Epoch: 2/2, step 6242/7134 completed (loss: 0.03587495535612106, acc: 0.9863945841789246)
[2025-02-13 21:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:36][root][INFO] - Training Epoch: 2/2, step 6243/7134 completed (loss: 0.12210353463888168, acc: 0.9664429426193237)
[2025-02-13 21:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:37][root][INFO] - Training Epoch: 2/2, step 6244/7134 completed (loss: 0.06981948763132095, acc: 0.9820359349250793)
[2025-02-13 21:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:37][root][INFO] - Training Epoch: 2/2, step 6245/7134 completed (loss: 0.11065378040075302, acc: 0.9881656765937805)
[2025-02-13 21:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:37][root][INFO] - Training Epoch: 2/2, step 6246/7134 completed (loss: 0.017819644883275032, acc: 0.9946523904800415)
[2025-02-13 21:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:38][root][INFO] - Training Epoch: 2/2, step 6247/7134 completed (loss: 0.09694144129753113, acc: 0.97826087474823)
[2025-02-13 21:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:38][root][INFO] - Training Epoch: 2/2, step 6248/7134 completed (loss: 0.19876377284526825, acc: 0.946107804775238)
[2025-02-13 21:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:39][root][INFO] - Training Epoch: 2/2, step 6249/7134 completed (loss: 0.014947561547160149, acc: 1.0)
[2025-02-13 21:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:39][root][INFO] - Training Epoch: 2/2, step 6250/7134 completed (loss: 0.012586903758347034, acc: 1.0)
[2025-02-13 21:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:39][root][INFO] - Training Epoch: 2/2, step 6251/7134 completed (loss: 0.031957536935806274, acc: 0.9933775067329407)
[2025-02-13 21:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:40][root][INFO] - Training Epoch: 2/2, step 6252/7134 completed (loss: 0.006899239495396614, acc: 1.0)
[2025-02-13 21:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:40][root][INFO] - Training Epoch: 2/2, step 6253/7134 completed (loss: 0.042551249265670776, acc: 0.9930070042610168)
[2025-02-13 21:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:40][root][INFO] - Training Epoch: 2/2, step 6254/7134 completed (loss: 0.04255320131778717, acc: 0.9925373196601868)
[2025-02-13 21:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:41][root][INFO] - Training Epoch: 2/2, step 6255/7134 completed (loss: 0.023783816024661064, acc: 1.0)
[2025-02-13 21:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:41][root][INFO] - Training Epoch: 2/2, step 6256/7134 completed (loss: 0.009529412724077702, acc: 1.0)
[2025-02-13 21:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:42][root][INFO] - Training Epoch: 2/2, step 6257/7134 completed (loss: 0.024702690541744232, acc: 0.9940828680992126)
[2025-02-13 21:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:42][root][INFO] - Training Epoch: 2/2, step 6258/7134 completed (loss: 0.05808432400226593, acc: 0.9800000190734863)
[2025-02-13 21:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:42][root][INFO] - Training Epoch: 2/2, step 6259/7134 completed (loss: 0.018748179078102112, acc: 1.0)
[2025-02-13 21:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:43][root][INFO] - Training Epoch: 2/2, step 6260/7134 completed (loss: 0.04417591169476509, acc: 0.993630588054657)
[2025-02-13 21:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:43][root][INFO] - Training Epoch: 2/2, step 6261/7134 completed (loss: 0.04507286474108696, acc: 0.9813664555549622)
[2025-02-13 21:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:44][root][INFO] - Training Epoch: 2/2, step 6262/7134 completed (loss: 0.018899310380220413, acc: 0.9938271641731262)
[2025-02-13 21:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:44][root][INFO] - Training Epoch: 2/2, step 6263/7134 completed (loss: 0.015037615783512592, acc: 0.9935897588729858)
[2025-02-13 21:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:44][root][INFO] - Training Epoch: 2/2, step 6264/7134 completed (loss: 0.026913266628980637, acc: 0.9931507110595703)
[2025-02-13 21:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:45][root][INFO] - Training Epoch: 2/2, step 6265/7134 completed (loss: 0.01736585795879364, acc: 1.0)
[2025-02-13 21:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:45][root][INFO] - Training Epoch: 2/2, step 6266/7134 completed (loss: 0.010186930187046528, acc: 1.0)
[2025-02-13 21:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:45][root][INFO] - Training Epoch: 2/2, step 6267/7134 completed (loss: 0.02282700687646866, acc: 0.9934210777282715)
[2025-02-13 21:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:46][root][INFO] - Training Epoch: 2/2, step 6268/7134 completed (loss: 0.03277663514018059, acc: 0.9875776171684265)
[2025-02-13 21:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:46][root][INFO] - Training Epoch: 2/2, step 6269/7134 completed (loss: 0.005572419613599777, acc: 1.0)
[2025-02-13 21:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:47][root][INFO] - Training Epoch: 2/2, step 6270/7134 completed (loss: 0.01677638106048107, acc: 0.9934210777282715)
[2025-02-13 21:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:47][root][INFO] - Training Epoch: 2/2, step 6271/7134 completed (loss: 0.05070130527019501, acc: 0.9943181872367859)
[2025-02-13 21:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:47][root][INFO] - Training Epoch: 2/2, step 6272/7134 completed (loss: 0.004313287790864706, acc: 1.0)
[2025-02-13 21:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:48][root][INFO] - Training Epoch: 2/2, step 6273/7134 completed (loss: 0.016014864668250084, acc: 0.9940828680992126)
[2025-02-13 21:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:48][root][INFO] - Training Epoch: 2/2, step 6274/7134 completed (loss: 0.010472063906490803, acc: 1.0)
[2025-02-13 21:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:49][root][INFO] - Training Epoch: 2/2, step 6275/7134 completed (loss: 0.029912849888205528, acc: 0.9932885766029358)
[2025-02-13 21:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:49][root][INFO] - Training Epoch: 2/2, step 6276/7134 completed (loss: 0.03793365880846977, acc: 0.9918699264526367)
[2025-02-13 21:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:49][root][INFO] - Training Epoch: 2/2, step 6277/7134 completed (loss: 0.03689107671380043, acc: 1.0)
[2025-02-13 21:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:50][root][INFO] - Training Epoch: 2/2, step 6278/7134 completed (loss: 0.02182151935994625, acc: 0.9926470518112183)
[2025-02-13 21:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:50][root][INFO] - Training Epoch: 2/2, step 6279/7134 completed (loss: 0.03786010295152664, acc: 0.9846153855323792)
[2025-02-13 21:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:51][root][INFO] - Training Epoch: 2/2, step 6280/7134 completed (loss: 0.04841356351971626, acc: 0.9776119589805603)
[2025-02-13 21:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:51][root][INFO] - Training Epoch: 2/2, step 6281/7134 completed (loss: 0.01245847437530756, acc: 1.0)
[2025-02-13 21:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:51][root][INFO] - Training Epoch: 2/2, step 6282/7134 completed (loss: 0.06302135437726974, acc: 0.9861111044883728)
[2025-02-13 21:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:52][root][INFO] - Training Epoch: 2/2, step 6283/7134 completed (loss: 0.034350257366895676, acc: 1.0)
[2025-02-13 21:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:52][root][INFO] - Training Epoch: 2/2, step 6284/7134 completed (loss: 0.06769108772277832, acc: 0.9818181991577148)
[2025-02-13 21:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:53][root][INFO] - Training Epoch: 2/2, step 6285/7134 completed (loss: 0.1020764708518982, acc: 0.9578947424888611)
[2025-02-13 21:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:53][root][INFO] - Training Epoch: 2/2, step 6286/7134 completed (loss: 0.028144415467977524, acc: 0.9861111044883728)
[2025-02-13 21:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:53][root][INFO] - Training Epoch: 2/2, step 6287/7134 completed (loss: 0.03662959858775139, acc: 0.9848484992980957)
[2025-02-13 21:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:54][root][INFO] - Training Epoch: 2/2, step 6288/7134 completed (loss: 0.01571452058851719, acc: 0.9929078221321106)
[2025-02-13 21:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:54][root][INFO] - Training Epoch: 2/2, step 6289/7134 completed (loss: 0.01959039643406868, acc: 1.0)
[2025-02-13 21:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:54][root][INFO] - Training Epoch: 2/2, step 6290/7134 completed (loss: 0.019842077046632767, acc: 1.0)
[2025-02-13 21:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:55][root][INFO] - Training Epoch: 2/2, step 6291/7134 completed (loss: 0.018866894766688347, acc: 0.9924242496490479)
[2025-02-13 21:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:55][root][INFO] - Training Epoch: 2/2, step 6292/7134 completed (loss: 0.05008644983172417, acc: 0.9910714030265808)
[2025-02-13 21:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:56][root][INFO] - Training Epoch: 2/2, step 6293/7134 completed (loss: 0.09885472059249878, acc: 0.9696969985961914)
[2025-02-13 21:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:56][root][INFO] - Training Epoch: 2/2, step 6294/7134 completed (loss: 0.0907459482550621, acc: 0.9736841917037964)
[2025-02-13 21:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:56][root][INFO] - Training Epoch: 2/2, step 6295/7134 completed (loss: 0.014554096385836601, acc: 1.0)
[2025-02-13 21:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:57][root][INFO] - Training Epoch: 2/2, step 6296/7134 completed (loss: 0.16255685687065125, acc: 0.9640287756919861)
[2025-02-13 21:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:57][root][INFO] - Training Epoch: 2/2, step 6297/7134 completed (loss: 0.08390958607196808, acc: 0.9849624037742615)
[2025-02-13 21:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:57][root][INFO] - Training Epoch: 2/2, step 6298/7134 completed (loss: 0.07721119374036789, acc: 0.9722222089767456)
[2025-02-13 21:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:58][root][INFO] - Training Epoch: 2/2, step 6299/7134 completed (loss: 0.14491945505142212, acc: 0.9671052694320679)
[2025-02-13 21:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:58][root][INFO] - Training Epoch: 2/2, step 6300/7134 completed (loss: 0.0640549585223198, acc: 0.9802631735801697)
[2025-02-13 21:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:59][root][INFO] - Training Epoch: 2/2, step 6301/7134 completed (loss: 0.0310678631067276, acc: 0.9953703880310059)
[2025-02-13 21:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:59][root][INFO] - Training Epoch: 2/2, step 6302/7134 completed (loss: 0.01563597470521927, acc: 0.994413435459137)
[2025-02-13 21:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:59][root][INFO] - Training Epoch: 2/2, step 6303/7134 completed (loss: 0.020911892876029015, acc: 0.9910314083099365)
[2025-02-13 21:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:00][root][INFO] - Training Epoch: 2/2, step 6304/7134 completed (loss: 0.016828063875436783, acc: 0.9943181872367859)
[2025-02-13 21:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:00][root][INFO] - Training Epoch: 2/2, step 6305/7134 completed (loss: 0.057707831263542175, acc: 0.9794520735740662)
[2025-02-13 21:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:00][root][INFO] - Training Epoch: 2/2, step 6306/7134 completed (loss: 0.025865307077765465, acc: 0.9905213117599487)
[2025-02-13 21:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:01][root][INFO] - Training Epoch: 2/2, step 6307/7134 completed (loss: 0.057628463953733444, acc: 0.9923664331436157)
[2025-02-13 21:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:01][root][INFO] - Training Epoch: 2/2, step 6308/7134 completed (loss: 0.08972350507974625, acc: 0.984455943107605)
[2025-02-13 21:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:01][root][INFO] - Training Epoch: 2/2, step 6309/7134 completed (loss: 0.06633227318525314, acc: 0.9881656765937805)
[2025-02-13 21:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:02][root][INFO] - Training Epoch: 2/2, step 6310/7134 completed (loss: 0.060837749391794205, acc: 0.9824561476707458)
[2025-02-13 21:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:02][root][INFO] - Training Epoch: 2/2, step 6311/7134 completed (loss: 0.028045080602169037, acc: 0.9947916865348816)
[2025-02-13 21:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:03][root][INFO] - Training Epoch: 2/2, step 6312/7134 completed (loss: 0.02700691483914852, acc: 0.993630588054657)
[2025-02-13 21:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:03][root][INFO] - Training Epoch: 2/2, step 6313/7134 completed (loss: 0.01929851435124874, acc: 0.9945945739746094)
[2025-02-13 21:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:03][root][INFO] - Training Epoch: 2/2, step 6314/7134 completed (loss: 0.08277378976345062, acc: 0.9838709831237793)
[2025-02-13 21:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:04][root][INFO] - Training Epoch: 2/2, step 6315/7134 completed (loss: 0.14795972406864166, acc: 0.9798657894134521)
[2025-02-13 21:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:04][root][INFO] - Training Epoch: 2/2, step 6316/7134 completed (loss: 0.03443489223718643, acc: 0.9862068891525269)
[2025-02-13 21:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:04][root][INFO] - Training Epoch: 2/2, step 6317/7134 completed (loss: 0.021089274436235428, acc: 1.0)
[2025-02-13 21:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:05][root][INFO] - Training Epoch: 2/2, step 6318/7134 completed (loss: 0.030425598844885826, acc: 0.9868420958518982)
[2025-02-13 21:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:05][root][INFO] - Training Epoch: 2/2, step 6319/7134 completed (loss: 0.02684883028268814, acc: 0.9911110997200012)
[2025-02-13 21:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:06][root][INFO] - Training Epoch: 2/2, step 6320/7134 completed (loss: 0.022869333624839783, acc: 1.0)
[2025-02-13 21:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:06][root][INFO] - Training Epoch: 2/2, step 6321/7134 completed (loss: 0.025952190160751343, acc: 0.9952380657196045)
[2025-02-13 21:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:06][root][INFO] - Training Epoch: 2/2, step 6322/7134 completed (loss: 0.037687283009290695, acc: 0.993630588054657)
[2025-02-13 21:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:07][root][INFO] - Training Epoch: 2/2, step 6323/7134 completed (loss: 0.04817092418670654, acc: 0.987500011920929)
[2025-02-13 21:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:07][root][INFO] - Training Epoch: 2/2, step 6324/7134 completed (loss: 0.14325672388076782, acc: 0.9617486596107483)
[2025-02-13 21:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:07][root][INFO] - Training Epoch: 2/2, step 6325/7134 completed (loss: 0.04644383117556572, acc: 0.9919354915618896)
[2025-02-13 21:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:08][root][INFO] - Training Epoch: 2/2, step 6326/7134 completed (loss: 0.057382870465517044, acc: 0.9748427867889404)
[2025-02-13 21:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:08][root][INFO] - Training Epoch: 2/2, step 6327/7134 completed (loss: 0.01146220788359642, acc: 1.0)
[2025-02-13 21:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:09][root][INFO] - Training Epoch: 2/2, step 6328/7134 completed (loss: 0.0719122663140297, acc: 0.9813664555549622)
[2025-02-13 21:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:09][root][INFO] - Training Epoch: 2/2, step 6329/7134 completed (loss: 0.03734457865357399, acc: 0.9935897588729858)
[2025-02-13 21:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:09][root][INFO] - Training Epoch: 2/2, step 6330/7134 completed (loss: 0.031310781836509705, acc: 0.9905660152435303)
[2025-02-13 21:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:10][root][INFO] - Training Epoch: 2/2, step 6331/7134 completed (loss: 0.011496759951114655, acc: 1.0)
[2025-02-13 21:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:10][root][INFO] - Training Epoch: 2/2, step 6332/7134 completed (loss: 0.09932932257652283, acc: 0.9716312289237976)
[2025-02-13 21:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:10][root][INFO] - Training Epoch: 2/2, step 6333/7134 completed (loss: 0.10043839365243912, acc: 0.959770143032074)
[2025-02-13 21:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:11][root][INFO] - Training Epoch: 2/2, step 6334/7134 completed (loss: 0.025410978123545647, acc: 0.9926470518112183)
[2025-02-13 21:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:11][root][INFO] - Training Epoch: 2/2, step 6335/7134 completed (loss: 0.011511644348502159, acc: 1.0)
[2025-02-13 21:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:12][root][INFO] - Training Epoch: 2/2, step 6336/7134 completed (loss: 0.03802909702062607, acc: 0.9912280440330505)
[2025-02-13 21:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:12][root][INFO] - Training Epoch: 2/2, step 6337/7134 completed (loss: 0.021970652043819427, acc: 1.0)
[2025-02-13 21:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:12][root][INFO] - Training Epoch: 2/2, step 6338/7134 completed (loss: 0.11899318546056747, acc: 0.9874213933944702)
[2025-02-13 21:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:13][root][INFO] - Training Epoch: 2/2, step 6339/7134 completed (loss: 0.03960002213716507, acc: 0.9803921580314636)
[2025-02-13 21:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:13][root][INFO] - Training Epoch: 2/2, step 6340/7134 completed (loss: 0.02195262536406517, acc: 0.9938650131225586)
[2025-02-13 21:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:13][root][INFO] - Training Epoch: 2/2, step 6341/7134 completed (loss: 0.030773958191275597, acc: 0.9842519760131836)
[2025-02-13 21:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:14][root][INFO] - Training Epoch: 2/2, step 6342/7134 completed (loss: 0.04940091446042061, acc: 0.9768785834312439)
[2025-02-13 21:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:14][root][INFO] - Training Epoch: 2/2, step 6343/7134 completed (loss: 0.041348427534103394, acc: 0.9931507110595703)
[2025-02-13 21:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:15][root][INFO] - Training Epoch: 2/2, step 6344/7134 completed (loss: 0.04734399542212486, acc: 0.988950252532959)
[2025-02-13 21:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:15][root][INFO] - Training Epoch: 2/2, step 6345/7134 completed (loss: 0.044254206120967865, acc: 0.989130437374115)
[2025-02-13 21:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:15][root][INFO] - Training Epoch: 2/2, step 6346/7134 completed (loss: 0.1410875916481018, acc: 0.9651162624359131)
[2025-02-13 21:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:16][root][INFO] - Training Epoch: 2/2, step 6347/7134 completed (loss: 0.10153990238904953, acc: 0.9831932783126831)
[2025-02-13 21:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:16][root][INFO] - Training Epoch: 2/2, step 6348/7134 completed (loss: 0.10169103741645813, acc: 0.9930555820465088)
[2025-02-13 21:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:16][root][INFO] - Training Epoch: 2/2, step 6349/7134 completed (loss: 0.027666142210364342, acc: 0.9924242496490479)
[2025-02-13 21:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:17][root][INFO] - Training Epoch: 2/2, step 6350/7134 completed (loss: 0.07437137514352798, acc: 0.9735449552536011)
[2025-02-13 21:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:17][root][INFO] - Training Epoch: 2/2, step 6351/7134 completed (loss: 0.10537595301866531, acc: 0.976047933101654)
[2025-02-13 21:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:17][root][INFO] - Training Epoch: 2/2, step 6352/7134 completed (loss: 0.10973241180181503, acc: 0.9768785834312439)
[2025-02-13 21:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:18][root][INFO] - Training Epoch: 2/2, step 6353/7134 completed (loss: 0.1037602573633194, acc: 0.9693251252174377)
[2025-02-13 21:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:18][root][INFO] - Training Epoch: 2/2, step 6354/7134 completed (loss: 0.03149528428912163, acc: 0.9942528605461121)
[2025-02-13 21:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:19][root][INFO] - Training Epoch: 2/2, step 6355/7134 completed (loss: 0.029244881123304367, acc: 0.9933333396911621)
[2025-02-13 21:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:19][root][INFO] - Training Epoch: 2/2, step 6356/7134 completed (loss: 0.039335187524557114, acc: 0.987261176109314)
[2025-02-13 21:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:19][root][INFO] - Training Epoch: 2/2, step 6357/7134 completed (loss: 0.042912013828754425, acc: 0.9878787994384766)
[2025-02-13 21:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:20][root][INFO] - Training Epoch: 2/2, step 6358/7134 completed (loss: 0.08095569163560867, acc: 0.9824561476707458)
[2025-02-13 21:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:20][root][INFO] - Training Epoch: 2/2, step 6359/7134 completed (loss: 0.1297609955072403, acc: 0.9776536226272583)
[2025-02-13 21:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:20][root][INFO] - Training Epoch: 2/2, step 6360/7134 completed (loss: 0.05036177113652229, acc: 0.9863945841789246)
[2025-02-13 21:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:21][root][INFO] - Training Epoch: 2/2, step 6361/7134 completed (loss: 0.10085329413414001, acc: 0.971222996711731)
[2025-02-13 21:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:21][root][INFO] - Training Epoch: 2/2, step 6362/7134 completed (loss: 0.029735291376709938, acc: 1.0)
[2025-02-13 21:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:22][root][INFO] - Training Epoch: 2/2, step 6363/7134 completed (loss: 0.10051122307777405, acc: 0.9822485446929932)
[2025-02-13 21:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:22][root][INFO] - Training Epoch: 2/2, step 6364/7134 completed (loss: 0.03773504123091698, acc: 0.993630588054657)
[2025-02-13 21:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:22][root][INFO] - Training Epoch: 2/2, step 6365/7134 completed (loss: 0.03258505463600159, acc: 1.0)
[2025-02-13 21:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:23][root][INFO] - Training Epoch: 2/2, step 6366/7134 completed (loss: 0.06620360165834427, acc: 0.9940828680992126)
[2025-02-13 21:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:23][root][INFO] - Training Epoch: 2/2, step 6367/7134 completed (loss: 0.03105933591723442, acc: 1.0)
[2025-02-13 21:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:23][root][INFO] - Training Epoch: 2/2, step 6368/7134 completed (loss: 0.08238636702299118, acc: 0.9819276928901672)
[2025-02-13 21:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:24][root][INFO] - Training Epoch: 2/2, step 6369/7134 completed (loss: 0.07851332426071167, acc: 0.9821428656578064)
[2025-02-13 21:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:24][root][INFO] - Training Epoch: 2/2, step 6370/7134 completed (loss: 0.14500002562999725, acc: 0.9476743936538696)
[2025-02-13 21:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:25][root][INFO] - Training Epoch: 2/2, step 6371/7134 completed (loss: 0.10151012986898422, acc: 0.9803921580314636)
[2025-02-13 21:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:25][root][INFO] - Training Epoch: 2/2, step 6372/7134 completed (loss: 0.09331084787845612, acc: 0.982758641242981)
[2025-02-13 21:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:25][root][INFO] - Training Epoch: 2/2, step 6373/7134 completed (loss: 0.04977403208613396, acc: 0.9919354915618896)
[2025-02-13 21:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:26][root][INFO] - Training Epoch: 2/2, step 6374/7134 completed (loss: 0.029373135417699814, acc: 1.0)
[2025-02-13 21:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:26][root][INFO] - Training Epoch: 2/2, step 6375/7134 completed (loss: 0.11862411350011826, acc: 0.9723756909370422)
[2025-02-13 21:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:26][root][INFO] - Training Epoch: 2/2, step 6376/7134 completed (loss: 0.029864443466067314, acc: 1.0)
[2025-02-13 21:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:27][root][INFO] - Training Epoch: 2/2, step 6377/7134 completed (loss: 0.07564689218997955, acc: 0.984000027179718)
[2025-02-13 21:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:27][root][INFO] - Training Epoch: 2/2, step 6378/7134 completed (loss: 0.07228326052427292, acc: 0.9720930457115173)
[2025-02-13 21:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:28][root][INFO] - Training Epoch: 2/2, step 6379/7134 completed (loss: 0.061301182955503464, acc: 0.9842105507850647)
[2025-02-13 21:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:28][root][INFO] - Training Epoch: 2/2, step 6380/7134 completed (loss: 0.12340960651636124, acc: 0.9750000238418579)
[2025-02-13 21:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:28][root][INFO] - Training Epoch: 2/2, step 6381/7134 completed (loss: 0.019419850781559944, acc: 1.0)
[2025-02-13 21:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:29][root][INFO] - Training Epoch: 2/2, step 6382/7134 completed (loss: 0.019948411732912064, acc: 1.0)
[2025-02-13 21:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:29][root][INFO] - Training Epoch: 2/2, step 6383/7134 completed (loss: 0.18280938267707825, acc: 0.9710982441902161)
[2025-02-13 21:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:30][root][INFO] - Training Epoch: 2/2, step 6384/7134 completed (loss: 0.06011900678277016, acc: 0.9873417615890503)
[2025-02-13 21:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:30][root][INFO] - Training Epoch: 2/2, step 6385/7134 completed (loss: 0.16954928636550903, acc: 0.9658536314964294)
[2025-02-13 21:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:30][root][INFO] - Training Epoch: 2/2, step 6386/7134 completed (loss: 0.0561317577958107, acc: 0.9731183052062988)
[2025-02-13 21:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:31][root][INFO] - Training Epoch: 2/2, step 6387/7134 completed (loss: 0.07148484885692596, acc: 0.9939393997192383)
[2025-02-13 21:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:31][root][INFO] - Training Epoch: 2/2, step 6388/7134 completed (loss: 0.050982970744371414, acc: 0.9873417615890503)
[2025-02-13 21:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:31][root][INFO] - Training Epoch: 2/2, step 6389/7134 completed (loss: 0.07174647599458694, acc: 0.9807692170143127)
[2025-02-13 21:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:32][root][INFO] - Training Epoch: 2/2, step 6390/7134 completed (loss: 0.043220628052949905, acc: 0.991304337978363)
[2025-02-13 21:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:32][root][INFO] - Training Epoch: 2/2, step 6391/7134 completed (loss: 0.1890569031238556, acc: 0.9593023061752319)
[2025-02-13 21:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:33][root][INFO] - Training Epoch: 2/2, step 6392/7134 completed (loss: 0.14663304388523102, acc: 0.960869550704956)
[2025-02-13 21:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:33][root][INFO] - Training Epoch: 2/2, step 6393/7134 completed (loss: 0.04090718924999237, acc: 0.9907407164573669)
[2025-02-13 21:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:33][root][INFO] - Training Epoch: 2/2, step 6394/7134 completed (loss: 0.14225512742996216, acc: 0.9833333492279053)
[2025-02-13 21:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:34][root][INFO] - Training Epoch: 2/2, step 6395/7134 completed (loss: 0.06336227804422379, acc: 0.9851484894752502)
[2025-02-13 21:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:34][root][INFO] - Training Epoch: 2/2, step 6396/7134 completed (loss: 0.07090441137552261, acc: 0.9760765433311462)
[2025-02-13 21:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:34][root][INFO] - Training Epoch: 2/2, step 6397/7134 completed (loss: 0.024474378675222397, acc: 1.0)
[2025-02-13 21:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:35][root][INFO] - Training Epoch: 2/2, step 6398/7134 completed (loss: 0.05339938774704933, acc: 0.9862068891525269)
[2025-02-13 21:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:35][root][INFO] - Training Epoch: 2/2, step 6399/7134 completed (loss: 0.0775039866566658, acc: 0.9887640476226807)
[2025-02-13 21:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:36][root][INFO] - Training Epoch: 2/2, step 6400/7134 completed (loss: 0.015381836332380772, acc: 1.0)
[2025-02-13 21:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:36][root][INFO] - Training Epoch: 2/2, step 6401/7134 completed (loss: 0.045401446521282196, acc: 0.9904761910438538)
[2025-02-13 21:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:36][root][INFO] - Training Epoch: 2/2, step 6402/7134 completed (loss: 0.1022334024310112, acc: 0.9747474789619446)
[2025-02-13 21:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:37][root][INFO] - Training Epoch: 2/2, step 6403/7134 completed (loss: 0.02256815694272518, acc: 0.9948186278343201)
[2025-02-13 21:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:37][root][INFO] - Training Epoch: 2/2, step 6404/7134 completed (loss: 0.05806179717183113, acc: 0.9822485446929932)
[2025-02-13 21:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:38][root][INFO] - Training Epoch: 2/2, step 6405/7134 completed (loss: 0.05848397687077522, acc: 0.9903846383094788)
[2025-02-13 21:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:38][root][INFO] - Training Epoch: 2/2, step 6406/7134 completed (loss: 0.09032920002937317, acc: 0.9700000286102295)
[2025-02-13 21:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:38][root][INFO] - Training Epoch: 2/2, step 6407/7134 completed (loss: 0.05710679665207863, acc: 0.9888888597488403)
[2025-02-13 21:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:39][root][INFO] - Training Epoch: 2/2, step 6408/7134 completed (loss: 0.11177211254835129, acc: 0.9615384340286255)
[2025-02-13 21:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:39][root][INFO] - Training Epoch: 2/2, step 6409/7134 completed (loss: 0.130374938249588, acc: 0.9878048896789551)
[2025-02-13 21:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:39][root][INFO] - Training Epoch: 2/2, step 6410/7134 completed (loss: 0.07527093589305878, acc: 0.9916666746139526)
[2025-02-13 21:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:40][root][INFO] - Training Epoch: 2/2, step 6411/7134 completed (loss: 0.09820384532213211, acc: 0.9767441749572754)
[2025-02-13 21:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:40][root][INFO] - Training Epoch: 2/2, step 6412/7134 completed (loss: 0.023453451693058014, acc: 0.9935483932495117)
[2025-02-13 21:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:40][root][INFO] - Training Epoch: 2/2, step 6413/7134 completed (loss: 0.13934165239334106, acc: 0.9731543660163879)
[2025-02-13 21:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:41][root][INFO] - Training Epoch: 2/2, step 6414/7134 completed (loss: 0.036069709807634354, acc: 0.9802631735801697)
[2025-02-13 21:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:41][root][INFO] - Training Epoch: 2/2, step 6415/7134 completed (loss: 0.02673538774251938, acc: 0.9943181872367859)
[2025-02-13 21:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:42][root][INFO] - Training Epoch: 2/2, step 6416/7134 completed (loss: 0.08417234569787979, acc: 0.97826087474823)
[2025-02-13 21:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:42][root][INFO] - Training Epoch: 2/2, step 6417/7134 completed (loss: 0.11527658253908157, acc: 0.961240291595459)
[2025-02-13 21:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:42][root][INFO] - Training Epoch: 2/2, step 6418/7134 completed (loss: 0.1605854034423828, acc: 0.9530201554298401)
[2025-02-13 21:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:43][root][INFO] - Training Epoch: 2/2, step 6419/7134 completed (loss: 0.18180769681930542, acc: 0.9610389471054077)
[2025-02-13 21:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:43][root][INFO] - Training Epoch: 2/2, step 6420/7134 completed (loss: 0.11347775161266327, acc: 0.9746835231781006)
[2025-02-13 21:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:43][root][INFO] - Training Epoch: 2/2, step 6421/7134 completed (loss: 0.16757631301879883, acc: 0.9638554453849792)
[2025-02-13 21:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:44][root][INFO] - Training Epoch: 2/2, step 6422/7134 completed (loss: 0.11863839626312256, acc: 0.9608938694000244)
[2025-02-13 21:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:44][root][INFO] - Training Epoch: 2/2, step 6423/7134 completed (loss: 0.1143658310174942, acc: 0.9807692170143127)
[2025-02-13 21:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:45][root][INFO] - Training Epoch: 2/2, step 6424/7134 completed (loss: 0.05100390687584877, acc: 0.9893617033958435)
[2025-02-13 21:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:45][root][INFO] - Training Epoch: 2/2, step 6425/7134 completed (loss: 0.04841788485646248, acc: 0.9888268113136292)
[2025-02-13 21:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:45][root][INFO] - Training Epoch: 2/2, step 6426/7134 completed (loss: 0.0849672332406044, acc: 0.9776536226272583)
[2025-02-13 21:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:46][root][INFO] - Training Epoch: 2/2, step 6427/7134 completed (loss: 0.07534481585025787, acc: 0.9808917045593262)
[2025-02-13 21:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:46][root][INFO] - Training Epoch: 2/2, step 6428/7134 completed (loss: 0.08817531913518906, acc: 0.9808917045593262)
[2025-02-13 21:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:47][root][INFO] - Training Epoch: 2/2, step 6429/7134 completed (loss: 0.061270665377378464, acc: 0.9806451797485352)
[2025-02-13 21:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:47][root][INFO] - Training Epoch: 2/2, step 6430/7134 completed (loss: 0.17631593346595764, acc: 0.9624999761581421)
[2025-02-13 21:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:47][root][INFO] - Training Epoch: 2/2, step 6431/7134 completed (loss: 0.17225214838981628, acc: 0.9513513445854187)
[2025-02-13 21:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:48][root][INFO] - Training Epoch: 2/2, step 6432/7134 completed (loss: 0.48325756192207336, acc: 0.8774193525314331)
[2025-02-13 21:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:48][root][INFO] - Training Epoch: 2/2, step 6433/7134 completed (loss: 0.1387847512960434, acc: 0.9715909361839294)
[2025-02-13 21:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:48][root][INFO] - Training Epoch: 2/2, step 6434/7134 completed (loss: 0.050345148891210556, acc: 0.9826589822769165)
[2025-02-13 21:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:49][root][INFO] - Training Epoch: 2/2, step 6435/7134 completed (loss: 0.06950924545526505, acc: 0.97826087474823)
[2025-02-13 21:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:49][root][INFO] - Training Epoch: 2/2, step 6436/7134 completed (loss: 0.04154571145772934, acc: 0.9798657894134521)
[2025-02-13 21:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:50][root][INFO] - Training Epoch: 2/2, step 6437/7134 completed (loss: 0.14575618505477905, acc: 0.9671052694320679)
[2025-02-13 21:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:50][root][INFO] - Training Epoch: 2/2, step 6438/7134 completed (loss: 0.1527668535709381, acc: 0.9611650705337524)
[2025-02-13 21:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:50][root][INFO] - Training Epoch: 2/2, step 6439/7134 completed (loss: 0.06891797482967377, acc: 0.9813664555549622)
[2025-02-13 21:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:51][root][INFO] - Training Epoch: 2/2, step 6440/7134 completed (loss: 0.06704078614711761, acc: 0.9807692170143127)
[2025-02-13 21:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:51][root][INFO] - Training Epoch: 2/2, step 6441/7134 completed (loss: 0.036244072020053864, acc: 1.0)
[2025-02-13 21:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:52][root][INFO] - Training Epoch: 2/2, step 6442/7134 completed (loss: 0.03848673403263092, acc: 0.9951456189155579)
[2025-02-13 21:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:52][root][INFO] - Training Epoch: 2/2, step 6443/7134 completed (loss: 0.04045732691884041, acc: 0.9852941036224365)
[2025-02-13 21:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:52][root][INFO] - Training Epoch: 2/2, step 6444/7134 completed (loss: 0.05232089012861252, acc: 0.9833333492279053)
[2025-02-13 21:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:53][root][INFO] - Training Epoch: 2/2, step 6445/7134 completed (loss: 0.1413678228855133, acc: 0.9753694534301758)
[2025-02-13 21:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:53][root][INFO] - Training Epoch: 2/2, step 6446/7134 completed (loss: 0.04382322356104851, acc: 0.9950980544090271)
[2025-02-13 21:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:54][root][INFO] - Training Epoch: 2/2, step 6447/7134 completed (loss: 0.07304135710000992, acc: 0.9791666865348816)
[2025-02-13 21:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:54][root][INFO] - Training Epoch: 2/2, step 6448/7134 completed (loss: 0.23972339928150177, acc: 0.9419354796409607)
[2025-02-13 21:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:54][root][INFO] - Training Epoch: 2/2, step 6449/7134 completed (loss: 0.06775756925344467, acc: 0.984455943107605)
[2025-02-13 21:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:55][root][INFO] - Training Epoch: 2/2, step 6450/7134 completed (loss: 0.04074028506875038, acc: 0.9942196607589722)
[2025-02-13 21:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:55][root][INFO] - Training Epoch: 2/2, step 6451/7134 completed (loss: 0.1205761581659317, acc: 0.9747899174690247)
[2025-02-13 21:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:56][root][INFO] - Training Epoch: 2/2, step 6452/7134 completed (loss: 0.023250862956047058, acc: 0.9948979616165161)
[2025-02-13 21:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:56][root][INFO] - Training Epoch: 2/2, step 6453/7134 completed (loss: 0.03987906128168106, acc: 1.0)
[2025-02-13 21:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:56][root][INFO] - Training Epoch: 2/2, step 6454/7134 completed (loss: 0.09550482034683228, acc: 0.9836065769195557)
[2025-02-13 21:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:57][root][INFO] - Training Epoch: 2/2, step 6455/7134 completed (loss: 0.1191415935754776, acc: 0.9733333587646484)
[2025-02-13 21:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:57][root][INFO] - Training Epoch: 2/2, step 6456/7134 completed (loss: 0.11999940872192383, acc: 0.970059871673584)
[2025-02-13 21:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:58][root][INFO] - Training Epoch: 2/2, step 6457/7134 completed (loss: 0.02745089866220951, acc: 0.9908257126808167)
[2025-02-13 21:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:58][root][INFO] - Training Epoch: 2/2, step 6458/7134 completed (loss: 0.04159059375524521, acc: 1.0)
[2025-02-13 21:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:59][root][INFO] - Training Epoch: 2/2, step 6459/7134 completed (loss: 0.01105943787842989, acc: 1.0)
[2025-02-13 21:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:59][root][INFO] - Training Epoch: 2/2, step 6460/7134 completed (loss: 0.04599229618906975, acc: 0.9846938848495483)
[2025-02-13 21:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:59][root][INFO] - Training Epoch: 2/2, step 6461/7134 completed (loss: 0.025877952575683594, acc: 0.9942196607589722)
[2025-02-13 21:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:00][root][INFO] - Training Epoch: 2/2, step 6462/7134 completed (loss: 0.03389815241098404, acc: 0.9893617033958435)
[2025-02-13 21:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:00][root][INFO] - Training Epoch: 2/2, step 6463/7134 completed (loss: 0.09408124536275864, acc: 0.9757281541824341)
[2025-02-13 21:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:00][root][INFO] - Training Epoch: 2/2, step 6464/7134 completed (loss: 0.06879029422998428, acc: 0.9832402467727661)
[2025-02-13 21:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:01][root][INFO] - Training Epoch: 2/2, step 6465/7134 completed (loss: 0.015754610300064087, acc: 1.0)
[2025-02-13 21:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:01][root][INFO] - Training Epoch: 2/2, step 6466/7134 completed (loss: 0.0480811670422554, acc: 0.9841269850730896)
[2025-02-13 21:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:02][root][INFO] - Training Epoch: 2/2, step 6467/7134 completed (loss: 0.15229713916778564, acc: 0.9698795080184937)
[2025-02-13 21:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:02][root][INFO] - Training Epoch: 2/2, step 6468/7134 completed (loss: 0.047119978815317154, acc: 0.9852941036224365)
[2025-02-13 21:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:02][root][INFO] - Training Epoch: 2/2, step 6469/7134 completed (loss: 0.15106113255023956, acc: 0.9719626307487488)
[2025-02-13 21:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:03][root][INFO] - Training Epoch: 2/2, step 6470/7134 completed (loss: 0.06764047592878342, acc: 0.9803921580314636)
[2025-02-13 21:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:03][root][INFO] - Training Epoch: 2/2, step 6471/7134 completed (loss: 0.03298691287636757, acc: 1.0)
[2025-02-13 21:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:04][root][INFO] - Training Epoch: 2/2, step 6472/7134 completed (loss: 0.11428801715373993, acc: 0.9800000190734863)
[2025-02-13 21:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:04][root][INFO] - Training Epoch: 2/2, step 6473/7134 completed (loss: 0.055415842682123184, acc: 0.9722222089767456)
[2025-02-13 21:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:04][root][INFO] - Training Epoch: 2/2, step 6474/7134 completed (loss: 0.0625186339020729, acc: 0.9784946441650391)
[2025-02-13 21:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:05][root][INFO] - Training Epoch: 2/2, step 6475/7134 completed (loss: 0.03888363391160965, acc: 0.9928571581840515)
[2025-02-13 21:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:05][root][INFO] - Training Epoch: 2/2, step 6476/7134 completed (loss: 0.04006192833185196, acc: 0.9866666793823242)
[2025-02-13 21:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:06][root][INFO] - Training Epoch: 2/2, step 6477/7134 completed (loss: 0.030524790287017822, acc: 0.991304337978363)
[2025-02-13 21:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:06][root][INFO] - Training Epoch: 2/2, step 6478/7134 completed (loss: 0.06504891812801361, acc: 0.9710144996643066)
[2025-02-13 21:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:06][root][INFO] - Training Epoch: 2/2, step 6479/7134 completed (loss: 0.03525310754776001, acc: 0.9932885766029358)
[2025-02-13 21:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:07][root][INFO] - Training Epoch: 2/2, step 6480/7134 completed (loss: 0.03056158311665058, acc: 0.9928571581840515)
[2025-02-13 21:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:07][root][INFO] - Training Epoch: 2/2, step 6481/7134 completed (loss: 0.02717108279466629, acc: 0.9857142567634583)
[2025-02-13 21:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:08][root][INFO] - Training Epoch: 2/2, step 6482/7134 completed (loss: 0.011572586372494698, acc: 1.0)
[2025-02-13 21:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:08][root][INFO] - Training Epoch: 2/2, step 6483/7134 completed (loss: 0.09059645235538483, acc: 0.9930555820465088)
[2025-02-13 21:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:09][root][INFO] - Training Epoch: 2/2, step 6484/7134 completed (loss: 0.05049917846918106, acc: 0.988304078578949)
[2025-02-13 21:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:09][root][INFO] - Training Epoch: 2/2, step 6485/7134 completed (loss: 0.014990709722042084, acc: 1.0)
[2025-02-13 21:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:09][root][INFO] - Training Epoch: 2/2, step 6486/7134 completed (loss: 0.17315517365932465, acc: 0.95652174949646)
[2025-02-13 21:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:10][root][INFO] - Training Epoch: 2/2, step 6487/7134 completed (loss: 0.10974179953336716, acc: 0.970588207244873)
[2025-02-13 21:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:10][root][INFO] - Training Epoch: 2/2, step 6488/7134 completed (loss: 0.05402756109833717, acc: 0.9848484992980957)
[2025-02-13 21:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:11][root][INFO] - Training Epoch: 2/2, step 6489/7134 completed (loss: 0.13149307668209076, acc: 0.9743589758872986)
[2025-02-13 21:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:11][root][INFO] - Training Epoch: 2/2, step 6490/7134 completed (loss: 0.22493892908096313, acc: 0.9552238583564758)
[2025-02-13 21:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:11][root][INFO] - Training Epoch: 2/2, step 6491/7134 completed (loss: 0.12730520963668823, acc: 0.9740259647369385)
[2025-02-13 21:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:12][root][INFO] - Training Epoch: 2/2, step 6492/7134 completed (loss: 0.07105512917041779, acc: 0.9806451797485352)
[2025-02-13 21:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:12][root][INFO] - Training Epoch: 2/2, step 6493/7134 completed (loss: 0.060130421072244644, acc: 0.9821428656578064)
[2025-02-13 21:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:13][root][INFO] - Training Epoch: 2/2, step 6494/7134 completed (loss: 0.15746790170669556, acc: 0.9646017551422119)
[2025-02-13 21:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:13][root][INFO] - Training Epoch: 2/2, step 6495/7134 completed (loss: 0.06993332505226135, acc: 0.9876543283462524)
[2025-02-13 21:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:13][root][INFO] - Training Epoch: 2/2, step 6496/7134 completed (loss: 0.05821209400892258, acc: 0.9768785834312439)
[2025-02-13 21:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:14][root][INFO] - Training Epoch: 2/2, step 6497/7134 completed (loss: 0.16433773934841156, acc: 0.9583333134651184)
[2025-02-13 21:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:14][root][INFO] - Training Epoch: 2/2, step 6498/7134 completed (loss: 0.07703879475593567, acc: 0.9764705896377563)
[2025-02-13 21:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:15][root][INFO] - Training Epoch: 2/2, step 6499/7134 completed (loss: 0.09416840225458145, acc: 0.978723406791687)
[2025-02-13 21:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:15][root][INFO] - Training Epoch: 2/2, step 6500/7134 completed (loss: 0.1941361427307129, acc: 0.959770143032074)
[2025-02-13 21:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:15][root][INFO] - Training Epoch: 2/2, step 6501/7134 completed (loss: 0.3524898290634155, acc: 0.9226804375648499)
[2025-02-13 21:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:16][root][INFO] - Training Epoch: 2/2, step 6502/7134 completed (loss: 0.08873675763607025, acc: 0.9646464586257935)
[2025-02-13 21:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:16][root][INFO] - Training Epoch: 2/2, step 6503/7134 completed (loss: 0.11756674200296402, acc: 0.9689440727233887)
[2025-02-13 21:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:17][root][INFO] - Training Epoch: 2/2, step 6504/7134 completed (loss: 0.09416653215885162, acc: 0.9863013625144958)
[2025-02-13 21:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:17][root][INFO] - Training Epoch: 2/2, step 6505/7134 completed (loss: 0.1427968144416809, acc: 0.9666666388511658)
[2025-02-13 21:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:17][root][INFO] - Training Epoch: 2/2, step 6506/7134 completed (loss: 0.1510344296693802, acc: 0.969072163105011)
[2025-02-13 21:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:18][root][INFO] - Training Epoch: 2/2, step 6507/7134 completed (loss: 0.17011678218841553, acc: 0.9509202241897583)
[2025-02-13 21:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:18][root][INFO] - Training Epoch: 2/2, step 6508/7134 completed (loss: 0.05888281762599945, acc: 0.9888888597488403)
[2025-02-13 21:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:19][root][INFO] - Training Epoch: 2/2, step 6509/7134 completed (loss: 0.04587353393435478, acc: 0.9948453903198242)
[2025-02-13 21:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:19][root][INFO] - Training Epoch: 2/2, step 6510/7134 completed (loss: 0.09628192335367203, acc: 0.9805825352668762)
[2025-02-13 21:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:19][root][INFO] - Training Epoch: 2/2, step 6511/7134 completed (loss: 0.08935273438692093, acc: 0.9776119589805603)
[2025-02-13 21:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:20][root][INFO] - Training Epoch: 2/2, step 6512/7134 completed (loss: 0.018746227025985718, acc: 1.0)
[2025-02-13 21:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:20][root][INFO] - Training Epoch: 2/2, step 6513/7134 completed (loss: 0.06444443017244339, acc: 0.9851852059364319)
[2025-02-13 21:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:21][root][INFO] - Training Epoch: 2/2, step 6514/7134 completed (loss: 0.20997241139411926, acc: 0.9629629850387573)
[2025-02-13 21:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:21][root][INFO] - Training Epoch: 2/2, step 6515/7134 completed (loss: 0.10114181786775589, acc: 0.9670329689979553)
[2025-02-13 21:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:21][root][INFO] - Training Epoch: 2/2, step 6516/7134 completed (loss: 0.1091991737484932, acc: 0.9764705896377563)
[2025-02-13 21:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:22][root][INFO] - Training Epoch: 2/2, step 6517/7134 completed (loss: 0.03578566759824753, acc: 0.9937888383865356)
[2025-02-13 21:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:22][root][INFO] - Training Epoch: 2/2, step 6518/7134 completed (loss: 0.02083464153110981, acc: 1.0)
[2025-02-13 21:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:22][root][INFO] - Training Epoch: 2/2, step 6519/7134 completed (loss: 0.04457174614071846, acc: 0.9921259880065918)
[2025-02-13 21:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:23][root][INFO] - Training Epoch: 2/2, step 6520/7134 completed (loss: 0.03653884306550026, acc: 0.9870129823684692)
[2025-02-13 21:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:23][root][INFO] - Training Epoch: 2/2, step 6521/7134 completed (loss: 0.038332149386405945, acc: 0.9837837815284729)
[2025-02-13 21:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:24][root][INFO] - Training Epoch: 2/2, step 6522/7134 completed (loss: 0.046871818602085114, acc: 0.9934640526771545)
[2025-02-13 21:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:24][root][INFO] - Training Epoch: 2/2, step 6523/7134 completed (loss: 0.21368533372879028, acc: 0.9722222089767456)
[2025-02-13 21:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:24][root][INFO] - Training Epoch: 2/2, step 6524/7134 completed (loss: 0.12791579961776733, acc: 0.9668508172035217)
[2025-02-13 21:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:25][root][INFO] - Training Epoch: 2/2, step 6525/7134 completed (loss: 0.02494361624121666, acc: 0.9917355179786682)
[2025-02-13 21:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:25][root][INFO] - Training Epoch: 2/2, step 6526/7134 completed (loss: 0.06865440309047699, acc: 0.9847715497016907)
[2025-02-13 21:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:26][root][INFO] - Training Epoch: 2/2, step 6527/7134 completed (loss: 0.07549276202917099, acc: 0.9873417615890503)
[2025-02-13 21:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:26][root][INFO] - Training Epoch: 2/2, step 6528/7134 completed (loss: 0.16055577993392944, acc: 0.9637681245803833)
[2025-02-13 21:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:26][root][INFO] - Training Epoch: 2/2, step 6529/7134 completed (loss: 0.18505671620368958, acc: 0.9694656729698181)
[2025-02-13 21:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:27][root][INFO] - Training Epoch: 2/2, step 6530/7134 completed (loss: 0.07553940266370773, acc: 0.9779005646705627)
[2025-02-13 21:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:27][root][INFO] - Training Epoch: 2/2, step 6531/7134 completed (loss: 0.046035923063755035, acc: 0.9878048896789551)
[2025-02-13 21:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:27][root][INFO] - Training Epoch: 2/2, step 6532/7134 completed (loss: 0.161629781126976, acc: 0.9670329689979553)
[2025-02-13 21:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:28][root][INFO] - Training Epoch: 2/2, step 6533/7134 completed (loss: 0.01444210670888424, acc: 1.0)
[2025-02-13 21:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:28][root][INFO] - Training Epoch: 2/2, step 6534/7134 completed (loss: 0.18550555408000946, acc: 0.9653179049491882)
[2025-02-13 21:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:29][root][INFO] - Training Epoch: 2/2, step 6535/7134 completed (loss: 0.028990088030695915, acc: 0.9905660152435303)
[2025-02-13 21:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:29][root][INFO] - Training Epoch: 2/2, step 6536/7134 completed (loss: 0.03385185822844505, acc: 0.991304337978363)
[2025-02-13 21:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:29][root][INFO] - Training Epoch: 2/2, step 6537/7134 completed (loss: 0.020417379215359688, acc: 1.0)
[2025-02-13 21:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:30][root][INFO] - Training Epoch: 2/2, step 6538/7134 completed (loss: 0.1415434330701828, acc: 0.965753436088562)
[2025-02-13 21:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:30][root][INFO] - Training Epoch: 2/2, step 6539/7134 completed (loss: 0.014510799199342728, acc: 1.0)
[2025-02-13 21:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:30][root][INFO] - Training Epoch: 2/2, step 6540/7134 completed (loss: 0.33622679114341736, acc: 0.9204545617103577)
[2025-02-13 21:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:31][root][INFO] - Training Epoch: 2/2, step 6541/7134 completed (loss: 0.043848391622304916, acc: 0.988950252532959)
[2025-02-13 21:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:31][root][INFO] - Training Epoch: 2/2, step 6542/7134 completed (loss: 0.08340495824813843, acc: 0.9857142567634583)
[2025-02-13 21:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:31][root][INFO] - Training Epoch: 2/2, step 6543/7134 completed (loss: 0.05145951360464096, acc: 0.9912280440330505)
[2025-02-13 21:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:32][root][INFO] - Training Epoch: 2/2, step 6544/7134 completed (loss: 0.13402990996837616, acc: 0.9639639854431152)
[2025-02-13 21:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:32][root][INFO] - Training Epoch: 2/2, step 6545/7134 completed (loss: 0.021976349875330925, acc: 0.9834710955619812)
[2025-02-13 21:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:32][root][INFO] - Training Epoch: 2/2, step 6546/7134 completed (loss: 0.10422635078430176, acc: 0.9710144996643066)
[2025-02-13 21:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:33][root][INFO] - Training Epoch: 2/2, step 6547/7134 completed (loss: 0.17887702584266663, acc: 0.9710144996643066)
[2025-02-13 21:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:33][root][INFO] - Training Epoch: 2/2, step 6548/7134 completed (loss: 0.01696949452161789, acc: 1.0)
[2025-02-13 21:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:34][root][INFO] - Training Epoch: 2/2, step 6549/7134 completed (loss: 0.19532977044582367, acc: 0.9541984796524048)
[2025-02-13 21:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:34][root][INFO] - Training Epoch: 2/2, step 6550/7134 completed (loss: 0.18280506134033203, acc: 0.9617834687232971)
[2025-02-13 21:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:34][root][INFO] - Training Epoch: 2/2, step 6551/7134 completed (loss: 0.059205736964941025, acc: 0.9847715497016907)
[2025-02-13 21:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:35][root][INFO] - Training Epoch: 2/2, step 6552/7134 completed (loss: 0.14940954744815826, acc: 0.9559471607208252)
[2025-02-13 21:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:35][root][INFO] - Training Epoch: 2/2, step 6553/7134 completed (loss: 0.10234523564577103, acc: 0.9797979593276978)
[2025-02-13 21:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:35][root][INFO] - Training Epoch: 2/2, step 6554/7134 completed (loss: 0.09286627173423767, acc: 0.9906542301177979)
[2025-02-13 21:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:36][root][INFO] - Training Epoch: 2/2, step 6555/7134 completed (loss: 0.10006267577409744, acc: 0.9784482717514038)
[2025-02-13 21:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:36][root][INFO] - Training Epoch: 2/2, step 6556/7134 completed (loss: 0.13044637441635132, acc: 0.9819004535675049)
[2025-02-13 21:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:37][root][INFO] - Training Epoch: 2/2, step 6557/7134 completed (loss: 0.13201278448104858, acc: 0.9684684872627258)
[2025-02-13 21:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:37][root][INFO] - Training Epoch: 2/2, step 6558/7134 completed (loss: 0.1277717500925064, acc: 0.9813084006309509)
[2025-02-13 21:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:37][root][INFO] - Training Epoch: 2/2, step 6559/7134 completed (loss: 0.07605747133493423, acc: 0.9819276928901672)
[2025-02-13 21:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:38][root][INFO] - Training Epoch: 2/2, step 6560/7134 completed (loss: 0.044878605753183365, acc: 0.9895287752151489)
[2025-02-13 21:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:38][root][INFO] - Training Epoch: 2/2, step 6561/7134 completed (loss: 0.047195740044116974, acc: 0.9947643876075745)
[2025-02-13 21:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:39][root][INFO] - Training Epoch: 2/2, step 6562/7134 completed (loss: 0.04777104780077934, acc: 0.9937106966972351)
[2025-02-13 21:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:39][root][INFO] - Training Epoch: 2/2, step 6563/7134 completed (loss: 0.08405949175357819, acc: 0.9747474789619446)
[2025-02-13 21:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:39][root][INFO] - Training Epoch: 2/2, step 6564/7134 completed (loss: 0.05978243052959442, acc: 0.9939758777618408)
[2025-02-13 21:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:40][root][INFO] - Training Epoch: 2/2, step 6565/7134 completed (loss: 0.18054941296577454, acc: 0.9729729890823364)
[2025-02-13 21:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:40][root][INFO] - Training Epoch: 2/2, step 6566/7134 completed (loss: 0.06437930464744568, acc: 0.975806474685669)
[2025-02-13 21:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:40][root][INFO] - Training Epoch: 2/2, step 6567/7134 completed (loss: 0.07993310689926147, acc: 0.9822221994400024)
[2025-02-13 21:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:41][root][INFO] - Training Epoch: 2/2, step 6568/7134 completed (loss: 0.059036143124103546, acc: 0.9890109896659851)
[2025-02-13 21:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:41][root][INFO] - Training Epoch: 2/2, step 6569/7134 completed (loss: 0.058862004429101944, acc: 0.989847719669342)
[2025-02-13 21:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:42][root][INFO] - Training Epoch: 2/2, step 6570/7134 completed (loss: 0.05508538335561752, acc: 0.9950739145278931)
[2025-02-13 21:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:42][root][INFO] - Training Epoch: 2/2, step 6571/7134 completed (loss: 0.02566724829375744, acc: 1.0)
[2025-02-13 21:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:42][root][INFO] - Training Epoch: 2/2, step 6572/7134 completed (loss: 0.07014632970094681, acc: 0.9804878234863281)
[2025-02-13 21:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:43][root][INFO] - Training Epoch: 2/2, step 6573/7134 completed (loss: 0.028865348547697067, acc: 0.9902912378311157)
[2025-02-13 21:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:43][root][INFO] - Training Epoch: 2/2, step 6574/7134 completed (loss: 0.03302699327468872, acc: 0.9956896305084229)
[2025-02-13 21:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:43][root][INFO] - Training Epoch: 2/2, step 6575/7134 completed (loss: 0.11188793927431107, acc: 0.9624413251876831)
[2025-02-13 21:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:44][root][INFO] - Training Epoch: 2/2, step 6576/7134 completed (loss: 0.03926509991288185, acc: 0.9862385392189026)
[2025-02-13 21:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:44][root][INFO] - Training Epoch: 2/2, step 6577/7134 completed (loss: 0.03372858092188835, acc: 0.9939393997192383)
[2025-02-13 21:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:45][root][INFO] - Training Epoch: 2/2, step 6578/7134 completed (loss: 0.040484409779310226, acc: 0.9918032884597778)
[2025-02-13 21:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:45][root][INFO] - Training Epoch: 2/2, step 6579/7134 completed (loss: 0.1552753448486328, acc: 0.9774436354637146)
[2025-02-13 21:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:46][root][INFO] - Training Epoch: 2/2, step 6580/7134 completed (loss: 0.07575292885303497, acc: 0.9789473414421082)
[2025-02-13 21:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:46][root][INFO] - Training Epoch: 2/2, step 6581/7134 completed (loss: 0.06545373797416687, acc: 0.9783783555030823)
[2025-02-13 21:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:46][root][INFO] - Training Epoch: 2/2, step 6582/7134 completed (loss: 0.09813560545444489, acc: 0.9736841917037964)
[2025-02-13 21:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:47][root][INFO] - Training Epoch: 2/2, step 6583/7134 completed (loss: 0.05459606274962425, acc: 0.9899497628211975)
[2025-02-13 21:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:47][root][INFO] - Training Epoch: 2/2, step 6584/7134 completed (loss: 0.06079044193029404, acc: 0.9838709831237793)
[2025-02-13 21:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:47][root][INFO] - Training Epoch: 2/2, step 6585/7134 completed (loss: 0.04992518946528435, acc: 0.9932432174682617)
[2025-02-13 21:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:48][root][INFO] - Training Epoch: 2/2, step 6586/7134 completed (loss: 0.09349656850099564, acc: 0.9693251252174377)
[2025-02-13 21:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:48][root][INFO] - Training Epoch: 2/2, step 6587/7134 completed (loss: 0.1355535238981247, acc: 0.9845361113548279)
[2025-02-13 21:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:49][root][INFO] - Training Epoch: 2/2, step 6588/7134 completed (loss: 0.08027677983045578, acc: 0.9767441749572754)
[2025-02-13 21:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:49][root][INFO] - Training Epoch: 2/2, step 6589/7134 completed (loss: 0.08537819981575012, acc: 0.9813664555549622)
[2025-02-13 21:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:49][root][INFO] - Training Epoch: 2/2, step 6590/7134 completed (loss: 0.07843275368213654, acc: 0.9840425252914429)
[2025-02-13 21:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:50][root][INFO] - Training Epoch: 2/2, step 6591/7134 completed (loss: 0.06935426592826843, acc: 0.9766082167625427)
[2025-02-13 21:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:50][root][INFO] - Training Epoch: 2/2, step 6592/7134 completed (loss: 0.06625977903604507, acc: 0.9852216839790344)
[2025-02-13 21:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:51][root][INFO] - Training Epoch: 2/2, step 6593/7134 completed (loss: 0.08670639991760254, acc: 0.9800000190734863)
[2025-02-13 21:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:51][root][INFO] - Training Epoch: 2/2, step 6594/7134 completed (loss: 0.05173797160387039, acc: 0.9901477694511414)
[2025-02-13 21:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:51][root][INFO] - Training Epoch: 2/2, step 6595/7134 completed (loss: 0.06850778311491013, acc: 0.9938271641731262)
[2025-02-13 21:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:52][root][INFO] - Training Epoch: 2/2, step 6596/7134 completed (loss: 0.08461717516183853, acc: 0.9759036302566528)
[2025-02-13 21:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:52][root][INFO] - Training Epoch: 2/2, step 6597/7134 completed (loss: 0.06348218768835068, acc: 0.9895287752151489)
[2025-02-13 21:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:53][root][INFO] - Training Epoch: 2/2, step 6598/7134 completed (loss: 0.08921515196561813, acc: 0.9890710115432739)
[2025-02-13 21:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:53][root][INFO] - Training Epoch: 2/2, step 6599/7134 completed (loss: 0.07013155519962311, acc: 0.9756097793579102)
[2025-02-13 21:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:53][root][INFO] - Training Epoch: 2/2, step 6600/7134 completed (loss: 0.10103467851877213, acc: 0.9775280952453613)
[2025-02-13 21:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:54][root][INFO] - Training Epoch: 2/2, step 6601/7134 completed (loss: 0.18528231978416443, acc: 0.9460784196853638)
[2025-02-13 21:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:54][root][INFO] - Training Epoch: 2/2, step 6602/7134 completed (loss: 0.08029002696275711, acc: 0.9851484894752502)
[2025-02-13 21:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:54][root][INFO] - Training Epoch: 2/2, step 6603/7134 completed (loss: 0.10583995282649994, acc: 0.9811320900917053)
[2025-02-13 21:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:55][root][INFO] - Training Epoch: 2/2, step 6604/7134 completed (loss: 0.05706625059247017, acc: 0.9892473220825195)
[2025-02-13 21:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:55][root][INFO] - Training Epoch: 2/2, step 6605/7134 completed (loss: 0.14532791078090668, acc: 0.9677419066429138)
[2025-02-13 21:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:55][root][INFO] - Training Epoch: 2/2, step 6606/7134 completed (loss: 0.05551963299512863, acc: 0.9832402467727661)
[2025-02-13 21:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:56][root][INFO] - Training Epoch: 2/2, step 6607/7134 completed (loss: 0.10546423494815826, acc: 0.9612902998924255)
[2025-02-13 21:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:56][root][INFO] - Training Epoch: 2/2, step 6608/7134 completed (loss: 0.04167554900050163, acc: 0.9863013625144958)
[2025-02-13 21:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:57][root][INFO] - Training Epoch: 2/2, step 6609/7134 completed (loss: 0.07107443362474442, acc: 0.9685534834861755)
[2025-02-13 21:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:57][root][INFO] - Training Epoch: 2/2, step 6610/7134 completed (loss: 0.1136699765920639, acc: 0.9696969985961914)
[2025-02-13 21:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:57][root][INFO] - Training Epoch: 2/2, step 6611/7134 completed (loss: 0.0795719102025032, acc: 0.9726027250289917)
[2025-02-13 21:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:58][root][INFO] - Training Epoch: 2/2, step 6612/7134 completed (loss: 0.1745794266462326, acc: 0.9487179517745972)
[2025-02-13 21:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:58][root][INFO] - Training Epoch: 2/2, step 6613/7134 completed (loss: 0.09558337926864624, acc: 0.9731543660163879)
[2025-02-13 21:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:58][root][INFO] - Training Epoch: 2/2, step 6614/7134 completed (loss: 0.08345767110586166, acc: 0.9945054650306702)
[2025-02-13 21:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:59][root][INFO] - Training Epoch: 2/2, step 6615/7134 completed (loss: 0.06410913914442062, acc: 0.9864864945411682)
[2025-02-13 21:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:59][root][INFO] - Training Epoch: 2/2, step 6616/7134 completed (loss: 0.0659705400466919, acc: 0.9878787994384766)
[2025-02-13 21:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:59][root][INFO] - Training Epoch: 2/2, step 6617/7134 completed (loss: 0.13220135867595673, acc: 0.970370352268219)
[2025-02-13 21:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:00][root][INFO] - Training Epoch: 2/2, step 6618/7134 completed (loss: 0.023104125633835793, acc: 1.0)
[2025-02-13 21:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:00][root][INFO] - Training Epoch: 2/2, step 6619/7134 completed (loss: 0.01929098181426525, acc: 1.0)
[2025-02-13 21:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:01][root][INFO] - Training Epoch: 2/2, step 6620/7134 completed (loss: 0.11073150485754013, acc: 0.9652777910232544)
[2025-02-13 21:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:01][root][INFO] - Training Epoch: 2/2, step 6621/7134 completed (loss: 0.12595520913600922, acc: 0.977142870426178)
[2025-02-13 21:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:01][root][INFO] - Training Epoch: 2/2, step 6622/7134 completed (loss: 0.06021937355399132, acc: 0.9871794581413269)
[2025-02-13 21:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:02][root][INFO] - Training Epoch: 2/2, step 6623/7134 completed (loss: 0.05956704542040825, acc: 0.9820359349250793)
[2025-02-13 21:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:02][root][INFO] - Training Epoch: 2/2, step 6624/7134 completed (loss: 0.050621047616004944, acc: 0.993630588054657)
[2025-02-13 21:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:02][root][INFO] - Training Epoch: 2/2, step 6625/7134 completed (loss: 0.03562420979142189, acc: 0.9933775067329407)
[2025-02-13 21:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:03][root][INFO] - Training Epoch: 2/2, step 6626/7134 completed (loss: 0.046907827258110046, acc: 0.977142870426178)
[2025-02-13 21:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:03][root][INFO] - Training Epoch: 2/2, step 6627/7134 completed (loss: 0.04569779708981514, acc: 0.9892473220825195)
[2025-02-13 21:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:03][root][INFO] - Training Epoch: 2/2, step 6628/7134 completed (loss: 0.15858322381973267, acc: 0.9702380895614624)
[2025-02-13 21:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:04][root][INFO] - Training Epoch: 2/2, step 6629/7134 completed (loss: 0.15572892129421234, acc: 0.9624999761581421)
[2025-02-13 21:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:04][root][INFO] - Training Epoch: 2/2, step 6630/7134 completed (loss: 0.05020490288734436, acc: 0.9947916865348816)
[2025-02-13 21:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:05][root][INFO] - Training Epoch: 2/2, step 6631/7134 completed (loss: 0.045420341193675995, acc: 0.9937888383865356)
[2025-02-13 21:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:05][root][INFO] - Training Epoch: 2/2, step 6632/7134 completed (loss: 0.055470481514930725, acc: 0.989847719669342)
[2025-02-13 21:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:05][root][INFO] - Training Epoch: 2/2, step 6633/7134 completed (loss: 0.09303547441959381, acc: 0.9835164546966553)
[2025-02-13 21:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:06][root][INFO] - Training Epoch: 2/2, step 6634/7134 completed (loss: 0.04681158810853958, acc: 0.9849624037742615)
[2025-02-13 21:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:06][root][INFO] - Training Epoch: 2/2, step 6635/7134 completed (loss: 0.22250671684741974, acc: 0.9449999928474426)
[2025-02-13 21:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:06][root][INFO] - Training Epoch: 2/2, step 6636/7134 completed (loss: 0.09519127011299133, acc: 0.9658536314964294)
[2025-02-13 21:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:07][root][INFO] - Training Epoch: 2/2, step 6637/7134 completed (loss: 0.08203160017728806, acc: 0.981249988079071)
[2025-02-13 21:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:07][root][INFO] - Training Epoch: 2/2, step 6638/7134 completed (loss: 0.08628731966018677, acc: 0.9615384340286255)
[2025-02-13 21:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:07][root][INFO] - Training Epoch: 2/2, step 6639/7134 completed (loss: 0.0912221223115921, acc: 0.976047933101654)
[2025-02-13 21:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:08][root][INFO] - Training Epoch: 2/2, step 6640/7134 completed (loss: 0.0793110579252243, acc: 0.987261176109314)
[2025-02-13 21:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:08][root][INFO] - Training Epoch: 2/2, step 6641/7134 completed (loss: 0.19824405014514923, acc: 0.9557521939277649)
[2025-02-13 21:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:08][root][INFO] - Training Epoch: 2/2, step 6642/7134 completed (loss: 0.06831348687410355, acc: 0.9879518151283264)
[2025-02-13 21:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:09][root][INFO] - Training Epoch: 2/2, step 6643/7134 completed (loss: 0.06809309124946594, acc: 0.9838709831237793)
[2025-02-13 21:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:09][root][INFO] - Training Epoch: 2/2, step 6644/7134 completed (loss: 0.08106118440628052, acc: 0.9709543585777283)
[2025-02-13 21:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:10][root][INFO] - Training Epoch: 2/2, step 6645/7134 completed (loss: 0.04092045873403549, acc: 0.9947090148925781)
[2025-02-13 21:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:10][root][INFO] - Training Epoch: 2/2, step 6646/7134 completed (loss: 0.1330767720937729, acc: 0.9650654792785645)
[2025-02-13 21:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:10][root][INFO] - Training Epoch: 2/2, step 6647/7134 completed (loss: 0.039071694016456604, acc: 0.9941176176071167)
[2025-02-13 21:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:11][root][INFO] - Training Epoch: 2/2, step 6648/7134 completed (loss: 0.06714096665382385, acc: 0.9677419066429138)
[2025-02-13 21:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:11][root][INFO] - Training Epoch: 2/2, step 6649/7134 completed (loss: 0.08758348971605301, acc: 0.9710144996643066)
[2025-02-13 21:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:11][root][INFO] - Training Epoch: 2/2, step 6650/7134 completed (loss: 0.06283313781023026, acc: 0.9818181991577148)
[2025-02-13 21:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:12][root][INFO] - Training Epoch: 2/2, step 6651/7134 completed (loss: 0.135781928896904, acc: 0.9715909361839294)
[2025-02-13 21:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:12][root][INFO] - Training Epoch: 2/2, step 6652/7134 completed (loss: 0.22776740789413452, acc: 0.9411764740943909)
[2025-02-13 21:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:13][root][INFO] - Training Epoch: 2/2, step 6653/7134 completed (loss: 0.2125413864850998, acc: 0.9318181872367859)
[2025-02-13 21:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:13][root][INFO] - Training Epoch: 2/2, step 6654/7134 completed (loss: 0.11452177166938782, acc: 0.9620253443717957)
[2025-02-13 21:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:13][root][INFO] - Training Epoch: 2/2, step 6655/7134 completed (loss: 0.0932324007153511, acc: 0.9904761910438538)
[2025-02-13 21:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:14][root][INFO] - Training Epoch: 2/2, step 6656/7134 completed (loss: 0.08612029254436493, acc: 0.9736841917037964)
[2025-02-13 21:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:14][root][INFO] - Training Epoch: 2/2, step 6657/7134 completed (loss: 0.09934796392917633, acc: 0.978723406791687)
[2025-02-13 21:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:14][root][INFO] - Training Epoch: 2/2, step 6658/7134 completed (loss: 0.0648614689707756, acc: 0.9796954393386841)
[2025-02-13 21:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:15][root][INFO] - Training Epoch: 2/2, step 6659/7134 completed (loss: 0.059643056243658066, acc: 0.9781420826911926)
[2025-02-13 21:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:15][root][INFO] - Training Epoch: 2/2, step 6660/7134 completed (loss: 0.0765228196978569, acc: 0.9836065769195557)
[2025-02-13 21:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:15][root][INFO] - Training Epoch: 2/2, step 6661/7134 completed (loss: 0.07581938058137894, acc: 0.9850746393203735)
[2025-02-13 21:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:16][root][INFO] - Training Epoch: 2/2, step 6662/7134 completed (loss: 0.19874699413776398, acc: 0.9568345546722412)
[2025-02-13 21:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:16][root][INFO] - Training Epoch: 2/2, step 6663/7134 completed (loss: 0.09780466556549072, acc: 0.9696969985961914)
[2025-02-13 21:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:17][root][INFO] - Training Epoch: 2/2, step 6664/7134 completed (loss: 0.10972677916288376, acc: 0.9928571581840515)
[2025-02-13 21:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:17][root][INFO] - Training Epoch: 2/2, step 6665/7134 completed (loss: 0.06772252172231674, acc: 0.9937106966972351)
[2025-02-13 21:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:17][root][INFO] - Training Epoch: 2/2, step 6666/7134 completed (loss: 0.05632933974266052, acc: 0.9939393997192383)
[2025-02-13 21:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:18][root][INFO] - Training Epoch: 2/2, step 6667/7134 completed (loss: 0.1021391823887825, acc: 0.9795918464660645)
[2025-02-13 21:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:18][root][INFO] - Training Epoch: 2/2, step 6668/7134 completed (loss: 0.01423747930675745, acc: 1.0)
[2025-02-13 21:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:18][root][INFO] - Training Epoch: 2/2, step 6669/7134 completed (loss: 0.043157629668712616, acc: 0.9861111044883728)
[2025-02-13 21:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:19][root][INFO] - Training Epoch: 2/2, step 6670/7134 completed (loss: 0.03329622372984886, acc: 0.9928057789802551)
[2025-02-13 21:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:19][root][INFO] - Training Epoch: 2/2, step 6671/7134 completed (loss: 0.023394912481307983, acc: 0.9939024448394775)
[2025-02-13 21:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:19][root][INFO] - Training Epoch: 2/2, step 6672/7134 completed (loss: 0.02322804555296898, acc: 1.0)
[2025-02-13 21:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:20][root][INFO] - Training Epoch: 2/2, step 6673/7134 completed (loss: 0.03964252769947052, acc: 0.9874213933944702)
[2025-02-13 21:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:20][root][INFO] - Training Epoch: 2/2, step 6674/7134 completed (loss: 0.019707556813955307, acc: 0.991525411605835)
[2025-02-13 21:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:21][root][INFO] - Training Epoch: 2/2, step 6675/7134 completed (loss: 0.04233140870928764, acc: 0.9901960492134094)
[2025-02-13 21:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:21][root][INFO] - Training Epoch: 2/2, step 6676/7134 completed (loss: 0.03208877891302109, acc: 0.9900000095367432)
[2025-02-13 21:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:21][root][INFO] - Training Epoch: 2/2, step 6677/7134 completed (loss: 0.04873356595635414, acc: 0.9923664331436157)
[2025-02-13 21:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:22][root][INFO] - Training Epoch: 2/2, step 6678/7134 completed (loss: 0.032249823212623596, acc: 0.9925925731658936)
[2025-02-13 21:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:22][root][INFO] - Training Epoch: 2/2, step 6679/7134 completed (loss: 0.013288171961903572, acc: 1.0)
[2025-02-13 21:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:22][root][INFO] - Training Epoch: 2/2, step 6680/7134 completed (loss: 0.05109420418739319, acc: 0.9885057210922241)
[2025-02-13 21:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:23][root][INFO] - Training Epoch: 2/2, step 6681/7134 completed (loss: 0.0032470067963004112, acc: 1.0)
[2025-02-13 21:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:23][root][INFO] - Training Epoch: 2/2, step 6682/7134 completed (loss: 0.1253894567489624, acc: 0.9611650705337524)
[2025-02-13 21:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:23][root][INFO] - Training Epoch: 2/2, step 6683/7134 completed (loss: 0.10600385069847107, acc: 0.9655172228813171)
[2025-02-13 21:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:24][root][INFO] - Training Epoch: 2/2, step 6684/7134 completed (loss: 0.07344397902488708, acc: 0.9763779640197754)
[2025-02-13 21:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:24][root][INFO] - Training Epoch: 2/2, step 6685/7134 completed (loss: 0.04518142715096474, acc: 0.9919999837875366)
[2025-02-13 21:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:25][root][INFO] - Training Epoch: 2/2, step 6686/7134 completed (loss: 0.12687113881111145, acc: 0.982758641242981)
[2025-02-13 21:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:25][root][INFO] - Training Epoch: 2/2, step 6687/7134 completed (loss: 0.07584462314844131, acc: 0.9883720874786377)
[2025-02-13 21:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:25][root][INFO] - Training Epoch: 2/2, step 6688/7134 completed (loss: 0.10188114643096924, acc: 0.9784172773361206)
[2025-02-13 21:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:26][root][INFO] - Training Epoch: 2/2, step 6689/7134 completed (loss: 0.03581208363175392, acc: 1.0)
[2025-02-13 21:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:26][root][INFO] - Training Epoch: 2/2, step 6690/7134 completed (loss: 0.10307810455560684, acc: 0.9607843160629272)
[2025-02-13 21:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:26][root][INFO] - Training Epoch: 2/2, step 6691/7134 completed (loss: 0.04107867181301117, acc: 0.9930555820465088)
[2025-02-13 21:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:27][root][INFO] - Training Epoch: 2/2, step 6692/7134 completed (loss: 0.17514236271381378, acc: 0.9626168012619019)
[2025-02-13 21:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:27][root][INFO] - Training Epoch: 2/2, step 6693/7134 completed (loss: 0.0945998802781105, acc: 0.9851852059364319)
[2025-02-13 21:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:27][root][INFO] - Training Epoch: 2/2, step 6694/7134 completed (loss: 0.03564375266432762, acc: 0.9933775067329407)
[2025-02-13 21:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:28][root][INFO] - Training Epoch: 2/2, step 6695/7134 completed (loss: 0.05571315437555313, acc: 0.9772727489471436)
[2025-02-13 21:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:28][root][INFO] - Training Epoch: 2/2, step 6696/7134 completed (loss: 0.11736015230417252, acc: 0.9722222089767456)
[2025-02-13 21:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:29][root][INFO] - Training Epoch: 2/2, step 6697/7134 completed (loss: 0.13699349761009216, acc: 0.9642857313156128)
[2025-02-13 21:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:29][root][INFO] - Training Epoch: 2/2, step 6698/7134 completed (loss: 0.05039680376648903, acc: 0.9932885766029358)
[2025-02-13 21:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:29][root][INFO] - Training Epoch: 2/2, step 6699/7134 completed (loss: 0.07556262612342834, acc: 0.9809523820877075)
[2025-02-13 21:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:30][root][INFO] - Training Epoch: 2/2, step 6700/7134 completed (loss: 0.14289136230945587, acc: 0.970370352268219)
[2025-02-13 21:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:30][root][INFO] - Training Epoch: 2/2, step 6701/7134 completed (loss: 0.2469804733991623, acc: 0.9402984976768494)
[2025-02-13 21:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:30][root][INFO] - Training Epoch: 2/2, step 6702/7134 completed (loss: 0.08845782279968262, acc: 0.9739130139350891)
[2025-02-13 21:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:31][root][INFO] - Training Epoch: 2/2, step 6703/7134 completed (loss: 0.11640767008066177, acc: 0.9710144996643066)
[2025-02-13 21:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:31][root][INFO] - Training Epoch: 2/2, step 6704/7134 completed (loss: 0.18783354759216309, acc: 0.9482758641242981)
[2025-02-13 21:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:31][root][INFO] - Training Epoch: 2/2, step 6705/7134 completed (loss: 0.04811448976397514, acc: 0.9884393215179443)
[2025-02-13 21:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:32][root][INFO] - Training Epoch: 2/2, step 6706/7134 completed (loss: 0.023734189569950104, acc: 1.0)
[2025-02-13 21:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:32][root][INFO] - Training Epoch: 2/2, step 6707/7134 completed (loss: 0.1085112988948822, acc: 0.9658119678497314)
[2025-02-13 21:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:33][root][INFO] - Training Epoch: 2/2, step 6708/7134 completed (loss: 0.06154326722025871, acc: 0.9770992398262024)
[2025-02-13 21:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:33][root][INFO] - Training Epoch: 2/2, step 6709/7134 completed (loss: 0.14884215593338013, acc: 0.9878048896789551)
[2025-02-13 21:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:33][root][INFO] - Training Epoch: 2/2, step 6710/7134 completed (loss: 0.09878379106521606, acc: 0.9624060392379761)
[2025-02-13 21:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:34][root][INFO] - Training Epoch: 2/2, step 6711/7134 completed (loss: 0.045527372509241104, acc: 0.9936708807945251)
[2025-02-13 21:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:34][root][INFO] - Training Epoch: 2/2, step 6712/7134 completed (loss: 0.09083351492881775, acc: 0.9793103337287903)
[2025-02-13 21:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:35][root][INFO] - Training Epoch: 2/2, step 6713/7134 completed (loss: 0.13005997240543365, acc: 0.9693251252174377)
[2025-02-13 21:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:35][root][INFO] - Training Epoch: 2/2, step 6714/7134 completed (loss: 0.13693347573280334, acc: 0.9595375657081604)
[2025-02-13 21:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:35][root][INFO] - Training Epoch: 2/2, step 6715/7134 completed (loss: 0.18643797934055328, acc: 0.9594594836235046)
[2025-02-13 21:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:36][root][INFO] - Training Epoch: 2/2, step 6716/7134 completed (loss: 0.06436822563409805, acc: 0.9794520735740662)
[2025-02-13 21:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:36][root][INFO] - Training Epoch: 2/2, step 6717/7134 completed (loss: 0.05875563621520996, acc: 0.9864864945411682)
[2025-02-13 21:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:36][root][INFO] - Training Epoch: 2/2, step 6718/7134 completed (loss: 0.08056411892175674, acc: 0.9862068891525269)
[2025-02-13 21:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:37][root][INFO] - Training Epoch: 2/2, step 6719/7134 completed (loss: 0.04720992594957352, acc: 0.9932885766029358)
[2025-02-13 21:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:37][root][INFO] - Training Epoch: 2/2, step 6720/7134 completed (loss: 0.17086876928806305, acc: 0.959770143032074)
[2025-02-13 21:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:37][root][INFO] - Training Epoch: 2/2, step 6721/7134 completed (loss: 0.15477578341960907, acc: 0.9617834687232971)
[2025-02-13 21:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:38][root][INFO] - Training Epoch: 2/2, step 6722/7134 completed (loss: 0.18426570296287537, acc: 0.9510489702224731)
[2025-02-13 21:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:38][root][INFO] - Training Epoch: 2/2, step 6723/7134 completed (loss: 0.09981732815504074, acc: 0.9781420826911926)
[2025-02-13 21:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:38][root][INFO] - Training Epoch: 2/2, step 6724/7134 completed (loss: 0.1205035001039505, acc: 0.9685534834861755)
[2025-02-13 21:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:39][root][INFO] - Training Epoch: 2/2, step 6725/7134 completed (loss: 0.08624278008937836, acc: 0.9736841917037964)
[2025-02-13 21:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:39][root][INFO] - Training Epoch: 2/2, step 6726/7134 completed (loss: 0.19946953654289246, acc: 0.9669421315193176)
[2025-02-13 21:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:39][root][INFO] - Training Epoch: 2/2, step 6727/7134 completed (loss: 0.07774405181407928, acc: 0.9784172773361206)
[2025-02-13 21:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:40][root][INFO] - Training Epoch: 2/2, step 6728/7134 completed (loss: 0.16820712387561798, acc: 0.9548386931419373)
[2025-02-13 21:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:40][root][INFO] - Training Epoch: 2/2, step 6729/7134 completed (loss: 0.04917611554265022, acc: 0.9936708807945251)
[2025-02-13 21:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:40][root][INFO] - Training Epoch: 2/2, step 6730/7134 completed (loss: 0.16442649066448212, acc: 0.954023003578186)
[2025-02-13 21:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:41][root][INFO] - Training Epoch: 2/2, step 6731/7134 completed (loss: 0.06914057582616806, acc: 0.9759036302566528)
[2025-02-13 21:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:41][root][INFO] - Training Epoch: 2/2, step 6732/7134 completed (loss: 0.06505333632230759, acc: 0.9871794581413269)
[2025-02-13 21:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:42][root][INFO] - Training Epoch: 2/2, step 6733/7134 completed (loss: 0.10265649855136871, acc: 0.9856114983558655)
[2025-02-13 21:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:42][root][INFO] - Training Epoch: 2/2, step 6734/7134 completed (loss: 0.1573888510465622, acc: 0.9650349617004395)
[2025-02-13 21:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:42][root][INFO] - Training Epoch: 2/2, step 6735/7134 completed (loss: 0.10462694615125656, acc: 0.9807692170143127)
[2025-02-13 21:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:43][root][INFO] - Training Epoch: 2/2, step 6736/7134 completed (loss: 0.12746940553188324, acc: 0.9555555582046509)
[2025-02-13 21:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:43][root][INFO] - Training Epoch: 2/2, step 6737/7134 completed (loss: 0.1494528204202652, acc: 0.9470198750495911)
[2025-02-13 21:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:43][root][INFO] - Training Epoch: 2/2, step 6738/7134 completed (loss: 0.09372852742671967, acc: 0.9632353186607361)
[2025-02-13 21:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:44][root][INFO] - Training Epoch: 2/2, step 6739/7134 completed (loss: 0.09861721843481064, acc: 0.9785714149475098)
[2025-02-13 21:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:44][root][INFO] - Training Epoch: 2/2, step 6740/7134 completed (loss: 0.10864529758691788, acc: 0.9729729890823364)
[2025-02-13 21:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:44][root][INFO] - Training Epoch: 2/2, step 6741/7134 completed (loss: 0.05084019526839256, acc: 0.9819276928901672)
[2025-02-13 21:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:45][root][INFO] - Training Epoch: 2/2, step 6742/7134 completed (loss: 0.027923693880438805, acc: 0.9941176176071167)
[2025-02-13 21:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:45][root][INFO] - Training Epoch: 2/2, step 6743/7134 completed (loss: 0.03193679079413414, acc: 0.994350254535675)
[2025-02-13 21:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:45][root][INFO] - Training Epoch: 2/2, step 6744/7134 completed (loss: 0.027286821976304054, acc: 0.9932432174682617)
[2025-02-13 21:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:46][root][INFO] - Training Epoch: 2/2, step 6745/7134 completed (loss: 0.01755766198039055, acc: 1.0)
[2025-02-13 21:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:46][root][INFO] - Training Epoch: 2/2, step 6746/7134 completed (loss: 0.016394035890698433, acc: 1.0)
[2025-02-13 21:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:47][root][INFO] - Training Epoch: 2/2, step 6747/7134 completed (loss: 0.06642402708530426, acc: 0.9738219976425171)
[2025-02-13 21:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:47][root][INFO] - Training Epoch: 2/2, step 6748/7134 completed (loss: 0.06070047616958618, acc: 0.9901960492134094)
[2025-02-13 21:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:47][root][INFO] - Training Epoch: 2/2, step 6749/7134 completed (loss: 0.10578799992799759, acc: 0.9733333587646484)
[2025-02-13 21:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:48][root][INFO] - Training Epoch: 2/2, step 6750/7134 completed (loss: 0.07362905144691467, acc: 0.9851484894752502)
[2025-02-13 21:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:48][root][INFO] - Training Epoch: 2/2, step 6751/7134 completed (loss: 0.03365489840507507, acc: 0.9900497794151306)
[2025-02-13 21:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:48][root][INFO] - Training Epoch: 2/2, step 6752/7134 completed (loss: 0.03658413887023926, acc: 0.987730085849762)
[2025-02-13 21:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:49][root][INFO] - Training Epoch: 2/2, step 6753/7134 completed (loss: 0.021506767719984055, acc: 0.9898989796638489)
[2025-02-13 21:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:49][root][INFO] - Training Epoch: 2/2, step 6754/7134 completed (loss: 0.048995550721883774, acc: 0.9947643876075745)
[2025-02-13 21:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:49][root][INFO] - Training Epoch: 2/2, step 6755/7134 completed (loss: 0.03643243759870529, acc: 0.9947643876075745)
[2025-02-13 21:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:50][root][INFO] - Training Epoch: 2/2, step 6756/7134 completed (loss: 0.01970808207988739, acc: 0.9943820238113403)
[2025-02-13 21:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:50][root][INFO] - Training Epoch: 2/2, step 6757/7134 completed (loss: 0.020458528771996498, acc: 1.0)
[2025-02-13 21:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:51][root][INFO] - Training Epoch: 2/2, step 6758/7134 completed (loss: 0.024070192128419876, acc: 1.0)
[2025-02-13 21:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:51][root][INFO] - Training Epoch: 2/2, step 6759/7134 completed (loss: 0.04352870583534241, acc: 0.9866071343421936)
[2025-02-13 21:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:51][root][INFO] - Training Epoch: 2/2, step 6760/7134 completed (loss: 0.06252481788396835, acc: 0.9893617033958435)
[2025-02-13 21:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:52][root][INFO] - Training Epoch: 2/2, step 6761/7134 completed (loss: 0.05348564311861992, acc: 0.9857142567634583)
[2025-02-13 21:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:52][root][INFO] - Training Epoch: 2/2, step 6762/7134 completed (loss: 0.04155120998620987, acc: 0.9950000047683716)
[2025-02-13 21:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:52][root][INFO] - Training Epoch: 2/2, step 6763/7134 completed (loss: 0.02120349556207657, acc: 0.9946523904800415)
[2025-02-13 21:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:53][root][INFO] - Training Epoch: 2/2, step 6764/7134 completed (loss: 0.024946557357907295, acc: 0.9941176176071167)
[2025-02-13 21:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:53][root][INFO] - Training Epoch: 2/2, step 6765/7134 completed (loss: 0.05731029808521271, acc: 0.9835164546966553)
[2025-02-13 21:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:54][root][INFO] - Training Epoch: 2/2, step 6766/7134 completed (loss: 0.054000310599803925, acc: 0.9885714054107666)
[2025-02-13 21:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:54][root][INFO] - Training Epoch: 2/2, step 6767/7134 completed (loss: 0.05655766278505325, acc: 0.9821428656578064)
[2025-02-13 21:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:54][root][INFO] - Training Epoch: 2/2, step 6768/7134 completed (loss: 0.012598276138305664, acc: 1.0)
[2025-02-13 21:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:55][root][INFO] - Training Epoch: 2/2, step 6769/7134 completed (loss: 0.06950530409812927, acc: 0.9807692170143127)
[2025-02-13 21:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:55][root][INFO] - Training Epoch: 2/2, step 6770/7134 completed (loss: 0.033444248139858246, acc: 0.9947368502616882)
[2025-02-13 21:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:55][root][INFO] - Training Epoch: 2/2, step 6771/7134 completed (loss: 0.016027381643652916, acc: 1.0)
[2025-02-13 21:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:56][root][INFO] - Training Epoch: 2/2, step 6772/7134 completed (loss: 0.16888542473316193, acc: 0.9650349617004395)
[2025-02-13 21:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:56][root][INFO] - Training Epoch: 2/2, step 6773/7134 completed (loss: 0.07268986850976944, acc: 0.9876543283462524)
[2025-02-13 21:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:56][root][INFO] - Training Epoch: 2/2, step 6774/7134 completed (loss: 0.0730326697230339, acc: 0.976331353187561)
[2025-02-13 21:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:57][root][INFO] - Training Epoch: 2/2, step 6775/7134 completed (loss: 0.027229415252804756, acc: 1.0)
[2025-02-13 21:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:57][root][INFO] - Training Epoch: 2/2, step 6776/7134 completed (loss: 0.06424835324287415, acc: 0.984000027179718)
[2025-02-13 21:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:58][root][INFO] - Training Epoch: 2/2, step 6777/7134 completed (loss: 0.14073456823825836, acc: 0.9885714054107666)
[2025-02-13 21:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:58][root][INFO] - Training Epoch: 2/2, step 6778/7134 completed (loss: 0.026883484795689583, acc: 0.9943181872367859)
[2025-02-13 21:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:58][root][INFO] - Training Epoch: 2/2, step 6779/7134 completed (loss: 0.03627064451575279, acc: 0.988095223903656)
[2025-02-13 21:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:59][root][INFO] - Training Epoch: 2/2, step 6780/7134 completed (loss: 0.041470564901828766, acc: 0.9940119981765747)
[2025-02-13 21:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:59][root][INFO] - Training Epoch: 2/2, step 6781/7134 completed (loss: 0.015310118906199932, acc: 1.0)
[2025-02-13 21:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:59][root][INFO] - Training Epoch: 2/2, step 6782/7134 completed (loss: 0.01069041807204485, acc: 1.0)
[2025-02-13 21:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:00][root][INFO] - Training Epoch: 2/2, step 6783/7134 completed (loss: 0.05482299625873566, acc: 0.9724137783050537)
[2025-02-13 21:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:00][root][INFO] - Training Epoch: 2/2, step 6784/7134 completed (loss: 0.017307166010141373, acc: 0.9936708807945251)
[2025-02-13 21:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:00][root][INFO] - Training Epoch: 2/2, step 6785/7134 completed (loss: 0.045132119208574295, acc: 0.9803921580314636)
[2025-02-13 21:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:01][root][INFO] - Training Epoch: 2/2, step 6786/7134 completed (loss: 0.06231197714805603, acc: 0.9774011373519897)
[2025-02-13 21:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:01][root][INFO] - Training Epoch: 2/2, step 6787/7134 completed (loss: 0.02241678722202778, acc: 0.994535505771637)
[2025-02-13 21:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:02][root][INFO] - Training Epoch: 2/2, step 6788/7134 completed (loss: 0.02839111164212227, acc: 0.9928057789802551)
[2025-02-13 21:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:02][root][INFO] - Training Epoch: 2/2, step 6789/7134 completed (loss: 0.028692055493593216, acc: 0.9858155846595764)
[2025-02-13 21:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:02][root][INFO] - Training Epoch: 2/2, step 6790/7134 completed (loss: 0.11685796082019806, acc: 0.9715909361839294)
[2025-02-13 21:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:03][root][INFO] - Training Epoch: 2/2, step 6791/7134 completed (loss: 0.03265582025051117, acc: 0.9935897588729858)
[2025-02-13 21:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:03][root][INFO] - Training Epoch: 2/2, step 6792/7134 completed (loss: 0.014242070727050304, acc: 0.9925373196601868)
[2025-02-13 21:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:03][root][INFO] - Training Epoch: 2/2, step 6793/7134 completed (loss: 0.0186551995575428, acc: 1.0)
[2025-02-13 21:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:04][root][INFO] - Training Epoch: 2/2, step 6794/7134 completed (loss: 0.059038031846284866, acc: 0.9931972622871399)
[2025-02-13 21:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:04][root][INFO] - Training Epoch: 2/2, step 6795/7134 completed (loss: 0.028735367581248283, acc: 0.9942857027053833)
[2025-02-13 21:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:04][root][INFO] - Training Epoch: 2/2, step 6796/7134 completed (loss: 0.01675587147474289, acc: 0.9941860437393188)
[2025-02-13 21:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:05][root][INFO] - Training Epoch: 2/2, step 6797/7134 completed (loss: 0.06371785700321198, acc: 0.9878048896789551)
[2025-02-13 21:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:05][root][INFO] - Training Epoch: 2/2, step 6798/7134 completed (loss: 0.027114752680063248, acc: 0.9879518151283264)
[2025-02-13 21:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:06][root][INFO] - Training Epoch: 2/2, step 6799/7134 completed (loss: 0.03342224285006523, acc: 0.9880239367485046)
[2025-02-13 21:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:06][root][INFO] - Training Epoch: 2/2, step 6800/7134 completed (loss: 0.0443134680390358, acc: 0.9873417615890503)
[2025-02-13 21:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:06][root][INFO] - Training Epoch: 2/2, step 6801/7134 completed (loss: 0.037255048751831055, acc: 0.9928057789802551)
[2025-02-13 21:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:07][root][INFO] - Training Epoch: 2/2, step 6802/7134 completed (loss: 0.03193928301334381, acc: 0.9930070042610168)
[2025-02-13 21:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:07][root][INFO] - Training Epoch: 2/2, step 6803/7134 completed (loss: 0.03586756810545921, acc: 0.9833333492279053)
[2025-02-13 21:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:07][root][INFO] - Training Epoch: 2/2, step 6804/7134 completed (loss: 0.09136738628149033, acc: 0.9798657894134521)
[2025-02-13 21:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:08][root][INFO] - Training Epoch: 2/2, step 6805/7134 completed (loss: 0.034165918827056885, acc: 0.9851852059364319)
[2025-02-13 21:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:08][root][INFO] - Training Epoch: 2/2, step 6806/7134 completed (loss: 0.033759068697690964, acc: 0.9943820238113403)
[2025-02-13 21:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:09][root][INFO] - Training Epoch: 2/2, step 6807/7134 completed (loss: 0.03759419173002243, acc: 0.9937499761581421)
[2025-02-13 21:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:09][root][INFO] - Training Epoch: 2/2, step 6808/7134 completed (loss: 0.049196675419807434, acc: 0.9865771532058716)
[2025-02-13 21:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:09][root][INFO] - Training Epoch: 2/2, step 6809/7134 completed (loss: 0.06866883486509323, acc: 0.9871794581413269)
[2025-02-13 21:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:10][root][INFO] - Training Epoch: 2/2, step 6810/7134 completed (loss: 0.025684667751193047, acc: 0.9886363744735718)
[2025-02-13 21:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:10][root][INFO] - Training Epoch: 2/2, step 6811/7134 completed (loss: 0.03513441979885101, acc: 1.0)
[2025-02-13 21:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:10][root][INFO] - Training Epoch: 2/2, step 6812/7134 completed (loss: 0.02185993455350399, acc: 1.0)
[2025-02-13 21:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:11][root][INFO] - Training Epoch: 2/2, step 6813/7134 completed (loss: 0.04020851105451584, acc: 0.987500011920929)
[2025-02-13 21:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:11][root][INFO] - Training Epoch: 2/2, step 6814/7134 completed (loss: 0.027249449864029884, acc: 0.9942857027053833)
[2025-02-13 21:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:11][root][INFO] - Training Epoch: 2/2, step 6815/7134 completed (loss: 0.04880053550004959, acc: 0.9882352948188782)
[2025-02-13 21:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:12][root][INFO] - Training Epoch: 2/2, step 6816/7134 completed (loss: 0.038726530969142914, acc: 0.9861111044883728)
[2025-02-13 21:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:12][root][INFO] - Training Epoch: 2/2, step 6817/7134 completed (loss: 0.13129962980747223, acc: 0.9718309640884399)
[2025-02-13 21:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:13][root][INFO] - Training Epoch: 2/2, step 6818/7134 completed (loss: 0.028877153992652893, acc: 0.9931972622871399)
[2025-02-13 21:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:13][root][INFO] - Training Epoch: 2/2, step 6819/7134 completed (loss: 0.04113652929663658, acc: 0.985401451587677)
[2025-02-13 21:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:13][root][INFO] - Training Epoch: 2/2, step 6820/7134 completed (loss: 0.04971977695822716, acc: 0.985401451587677)
[2025-02-13 21:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:14][root][INFO] - Training Epoch: 2/2, step 6821/7134 completed (loss: 0.013531087897717953, acc: 1.0)
[2025-02-13 21:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:14][root][INFO] - Training Epoch: 2/2, step 6822/7134 completed (loss: 0.1535991132259369, acc: 0.9534883499145508)
[2025-02-13 21:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:14][root][INFO] - Training Epoch: 2/2, step 6823/7134 completed (loss: 0.05130666866898537, acc: 0.9879518151283264)
[2025-02-13 21:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:15][root][INFO] - Training Epoch: 2/2, step 6824/7134 completed (loss: 0.09216893464326859, acc: 0.9918699264526367)
[2025-02-13 21:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:15][root][INFO] - Training Epoch: 2/2, step 6825/7134 completed (loss: 0.040881115943193436, acc: 0.9892473220825195)
[2025-02-13 21:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:15][root][INFO] - Training Epoch: 2/2, step 6826/7134 completed (loss: 0.10030900686979294, acc: 0.9492753744125366)
[2025-02-13 21:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:16][root][INFO] - Training Epoch: 2/2, step 6827/7134 completed (loss: 0.17441876232624054, acc: 0.9635416865348816)
[2025-02-13 21:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:16][root][INFO] - Training Epoch: 2/2, step 6828/7134 completed (loss: 0.12176681309938431, acc: 0.9788732528686523)
[2025-02-13 21:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:17][root][INFO] - Training Epoch: 2/2, step 6829/7134 completed (loss: 0.10494493693113327, acc: 0.9704142212867737)
[2025-02-13 21:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:17][root][INFO] - Training Epoch: 2/2, step 6830/7134 completed (loss: 0.08889301866292953, acc: 0.9797297120094299)
[2025-02-13 21:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:17][root][INFO] - Training Epoch: 2/2, step 6831/7134 completed (loss: 0.06992408633232117, acc: 0.9720670580863953)
[2025-02-13 21:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:18][root][INFO] - Training Epoch: 2/2, step 6832/7134 completed (loss: 0.09351459890604019, acc: 0.9791666865348816)
[2025-02-13 21:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:18][root][INFO] - Training Epoch: 2/2, step 6833/7134 completed (loss: 0.07451508194208145, acc: 0.9808917045593262)
[2025-02-13 21:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:18][root][INFO] - Training Epoch: 2/2, step 6834/7134 completed (loss: 0.04998765513300896, acc: 0.9865771532058716)
[2025-02-13 21:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:19][root][INFO] - Training Epoch: 2/2, step 6835/7134 completed (loss: 0.03831614926457405, acc: 0.9869281053543091)
[2025-02-13 21:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:19][root][INFO] - Training Epoch: 2/2, step 6836/7134 completed (loss: 0.07854793220758438, acc: 0.9810426831245422)
[2025-02-13 21:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:19][root][INFO] - Training Epoch: 2/2, step 6837/7134 completed (loss: 0.08458197861909866, acc: 0.9848484992980957)
[2025-02-13 21:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:20][root][INFO] - Training Epoch: 2/2, step 6838/7134 completed (loss: 0.19456084072589874, acc: 0.9638554453849792)
[2025-02-13 21:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:20][root][INFO] - Training Epoch: 2/2, step 6839/7134 completed (loss: 0.09939078986644745, acc: 0.9599999785423279)
[2025-02-13 21:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:20][root][INFO] - Training Epoch: 2/2, step 6840/7134 completed (loss: 0.09112977236509323, acc: 0.9745222926139832)
[2025-02-13 21:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:21][root][INFO] - Training Epoch: 2/2, step 6841/7134 completed (loss: 0.030150242149829865, acc: 1.0)
[2025-02-13 21:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:21][root][INFO] - Training Epoch: 2/2, step 6842/7134 completed (loss: 0.07532370090484619, acc: 0.9780219793319702)
[2025-02-13 21:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:22][root][INFO] - Training Epoch: 2/2, step 6843/7134 completed (loss: 0.05676962807774544, acc: 0.9896373152732849)
[2025-02-13 21:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:22][root][INFO] - Training Epoch: 2/2, step 6844/7134 completed (loss: 0.0758080706000328, acc: 0.9798657894134521)
[2025-02-13 21:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:22][root][INFO] - Training Epoch: 2/2, step 6845/7134 completed (loss: 0.053061842918395996, acc: 0.9846153855323792)
[2025-02-13 21:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:23][root][INFO] - Training Epoch: 2/2, step 6846/7134 completed (loss: 0.06590370088815689, acc: 0.9753086566925049)
[2025-02-13 21:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:23][root][INFO] - Training Epoch: 2/2, step 6847/7134 completed (loss: 0.08040504157543182, acc: 0.9750000238418579)
[2025-02-13 21:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:23][root][INFO] - Training Epoch: 2/2, step 6848/7134 completed (loss: 0.031084392219781876, acc: 0.9887640476226807)
[2025-02-13 21:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:24][root][INFO] - Training Epoch: 2/2, step 6849/7134 completed (loss: 0.05544824153184891, acc: 0.9857142567634583)
[2025-02-13 21:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:24][root][INFO] - Training Epoch: 2/2, step 6850/7134 completed (loss: 0.05080915614962578, acc: 0.9904761910438538)
[2025-02-13 21:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:24][root][INFO] - Training Epoch: 2/2, step 6851/7134 completed (loss: 0.08195553719997406, acc: 0.9723756909370422)
[2025-02-13 21:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:25][root][INFO] - Training Epoch: 2/2, step 6852/7134 completed (loss: 0.09198691695928574, acc: 0.9714285731315613)
[2025-02-13 21:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:25][root][INFO] - Training Epoch: 2/2, step 6853/7134 completed (loss: 0.1336475908756256, acc: 0.9710982441902161)
[2025-02-13 21:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:26][root][INFO] - Training Epoch: 2/2, step 6854/7134 completed (loss: 0.015378124080598354, acc: 1.0)
[2025-02-13 21:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:26][root][INFO] - Training Epoch: 2/2, step 6855/7134 completed (loss: 0.02651241049170494, acc: 1.0)
[2025-02-13 21:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:26][root][INFO] - Training Epoch: 2/2, step 6856/7134 completed (loss: 0.02086505852639675, acc: 0.9939758777618408)
[2025-02-13 21:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:27][root][INFO] - Training Epoch: 2/2, step 6857/7134 completed (loss: 0.01593278907239437, acc: 0.9940476417541504)
[2025-02-13 21:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:27][root][INFO] - Training Epoch: 2/2, step 6858/7134 completed (loss: 0.010832560248672962, acc: 1.0)
[2025-02-13 21:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:27][root][INFO] - Training Epoch: 2/2, step 6859/7134 completed (loss: 0.02783789299428463, acc: 0.9887640476226807)
[2025-02-13 21:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:28][root][INFO] - Training Epoch: 2/2, step 6860/7134 completed (loss: 0.02424270287156105, acc: 1.0)
[2025-02-13 21:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:28][root][INFO] - Training Epoch: 2/2, step 6861/7134 completed (loss: 0.014580857940018177, acc: 1.0)
[2025-02-13 21:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:28][root][INFO] - Training Epoch: 2/2, step 6862/7134 completed (loss: 0.0888812467455864, acc: 0.9848484992980957)
[2025-02-13 21:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:29][root][INFO] - Training Epoch: 2/2, step 6863/7134 completed (loss: 0.018671274185180664, acc: 1.0)
[2025-02-13 21:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:29][root][INFO] - Training Epoch: 2/2, step 6864/7134 completed (loss: 0.02140698954463005, acc: 1.0)
[2025-02-13 21:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:30][root][INFO] - Training Epoch: 2/2, step 6865/7134 completed (loss: 0.055794671177864075, acc: 0.9756097793579102)
[2025-02-13 21:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:30][root][INFO] - Training Epoch: 2/2, step 6866/7134 completed (loss: 0.009741666726768017, acc: 0.9934640526771545)
[2025-02-13 21:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:30][root][INFO] - Training Epoch: 2/2, step 6867/7134 completed (loss: 0.005990577396005392, acc: 1.0)
[2025-02-13 21:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:31][root][INFO] - Training Epoch: 2/2, step 6868/7134 completed (loss: 0.0069640628062188625, acc: 1.0)
[2025-02-13 21:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:31][root][INFO] - Training Epoch: 2/2, step 6869/7134 completed (loss: 0.010981442406773567, acc: 1.0)
[2025-02-13 21:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:31][root][INFO] - Training Epoch: 2/2, step 6870/7134 completed (loss: 0.035388316959142685, acc: 0.9873417615890503)
[2025-02-13 21:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:32][root][INFO] - Training Epoch: 2/2, step 6871/7134 completed (loss: 0.007891800254583359, acc: 1.0)
[2025-02-13 21:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:32][root][INFO] - Training Epoch: 2/2, step 6872/7134 completed (loss: 0.04857894033193588, acc: 0.9943181872367859)
[2025-02-13 21:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:33][root][INFO] - Training Epoch: 2/2, step 6873/7134 completed (loss: 0.07001996785402298, acc: 0.9882352948188782)
[2025-02-13 21:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:33][root][INFO] - Training Epoch: 2/2, step 6874/7134 completed (loss: 0.008619822561740875, acc: 1.0)
[2025-02-13 21:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:33][root][INFO] - Training Epoch: 2/2, step 6875/7134 completed (loss: 0.06572073698043823, acc: 0.9798657894134521)
[2025-02-13 21:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:34][root][INFO] - Training Epoch: 2/2, step 6876/7134 completed (loss: 0.019600406289100647, acc: 1.0)
[2025-02-13 21:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:34][root][INFO] - Training Epoch: 2/2, step 6877/7134 completed (loss: 0.020119039341807365, acc: 1.0)
[2025-02-13 21:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:34][root][INFO] - Training Epoch: 2/2, step 6878/7134 completed (loss: 0.03782400116324425, acc: 0.9934640526771545)
[2025-02-13 21:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:35][root][INFO] - Training Epoch: 2/2, step 6879/7134 completed (loss: 0.022948523983359337, acc: 1.0)
[2025-02-13 21:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:35][root][INFO] - Training Epoch: 2/2, step 6880/7134 completed (loss: 0.03694157302379608, acc: 0.989847719669342)
[2025-02-13 21:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:35][root][INFO] - Training Epoch: 2/2, step 6881/7134 completed (loss: 0.12041793018579483, acc: 0.978723406791687)
[2025-02-13 21:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:36][root][INFO] - Training Epoch: 2/2, step 6882/7134 completed (loss: 0.061162687838077545, acc: 0.9929577708244324)
[2025-02-13 21:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:36][root][INFO] - Training Epoch: 2/2, step 6883/7134 completed (loss: 0.04595322534441948, acc: 0.9852941036224365)
[2025-02-13 21:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:37][root][INFO] - Training Epoch: 2/2, step 6884/7134 completed (loss: 0.08315060287714005, acc: 0.9829545617103577)
[2025-02-13 21:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:37][root][INFO] - Training Epoch: 2/2, step 6885/7134 completed (loss: 0.02524399384856224, acc: 1.0)
[2025-02-13 21:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:37][root][INFO] - Training Epoch: 2/2, step 6886/7134 completed (loss: 0.055729035288095474, acc: 0.9864864945411682)
[2025-02-13 21:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:38][root][INFO] - Training Epoch: 2/2, step 6887/7134 completed (loss: 0.0374356284737587, acc: 0.991304337978363)
[2025-02-13 21:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:38][root][INFO] - Training Epoch: 2/2, step 6888/7134 completed (loss: 0.07627368718385696, acc: 0.9866666793823242)
[2025-02-13 21:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:38][root][INFO] - Training Epoch: 2/2, step 6889/7134 completed (loss: 0.035190023481845856, acc: 1.0)
[2025-02-13 21:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:39][root][INFO] - Training Epoch: 2/2, step 6890/7134 completed (loss: 0.07896629720926285, acc: 0.9784946441650391)
[2025-02-13 21:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:39][root][INFO] - Training Epoch: 2/2, step 6891/7134 completed (loss: 0.025358697399497032, acc: 0.9938271641731262)
[2025-02-13 21:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:39][root][INFO] - Training Epoch: 2/2, step 6892/7134 completed (loss: 0.0517847016453743, acc: 0.9838709831237793)
[2025-02-13 21:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:40][root][INFO] - Training Epoch: 2/2, step 6893/7134 completed (loss: 0.07027480751276016, acc: 0.995192289352417)
[2025-02-13 21:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:40][root][INFO] - Training Epoch: 2/2, step 6894/7134 completed (loss: 0.0881582498550415, acc: 0.984375)
[2025-02-13 21:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:41][root][INFO] - Training Epoch: 2/2, step 6895/7134 completed (loss: 0.05170392245054245, acc: 0.9901477694511414)
[2025-02-13 21:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:41][root][INFO] - Training Epoch: 2/2, step 6896/7134 completed (loss: 0.11418601870536804, acc: 0.9653179049491882)
[2025-02-13 21:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:41][root][INFO] - Training Epoch: 2/2, step 6897/7134 completed (loss: 0.05812968313694, acc: 0.9900990128517151)
[2025-02-13 21:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:42][root][INFO] - Training Epoch: 2/2, step 6898/7134 completed (loss: 0.04308897256851196, acc: 0.9884393215179443)
[2025-02-13 21:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:42][root][INFO] - Training Epoch: 2/2, step 6899/7134 completed (loss: 0.048584893345832825, acc: 0.978723406791687)
[2025-02-13 21:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:42][root][INFO] - Training Epoch: 2/2, step 6900/7134 completed (loss: 0.2488984763622284, acc: 0.9595375657081604)
[2025-02-13 21:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:43][root][INFO] - Training Epoch: 2/2, step 6901/7134 completed (loss: 0.08620283007621765, acc: 0.9729729890823364)
[2025-02-13 21:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:43][root][INFO] - Training Epoch: 2/2, step 6902/7134 completed (loss: 0.011052882298827171, acc: 1.0)
[2025-02-13 21:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:43][root][INFO] - Training Epoch: 2/2, step 6903/7134 completed (loss: 0.025529131293296814, acc: 1.0)
[2025-02-13 21:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:44][root][INFO] - Training Epoch: 2/2, step 6904/7134 completed (loss: 0.046842772513628006, acc: 0.9896373152732849)
[2025-02-13 21:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:44][root][INFO] - Training Epoch: 2/2, step 6905/7134 completed (loss: 0.051629919558763504, acc: 0.9850746393203735)
[2025-02-13 21:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:45][root][INFO] - Training Epoch: 2/2, step 6906/7134 completed (loss: 0.10669233649969101, acc: 0.9852941036224365)
[2025-02-13 21:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:45][root][INFO] - Training Epoch: 2/2, step 6907/7134 completed (loss: 0.04400501400232315, acc: 0.9896373152732849)
[2025-02-13 21:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:45][root][INFO] - Training Epoch: 2/2, step 6908/7134 completed (loss: 0.07463157176971436, acc: 0.9809523820877075)
[2025-02-13 21:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:46][root][INFO] - Training Epoch: 2/2, step 6909/7134 completed (loss: 0.10249385982751846, acc: 0.9781420826911926)
[2025-02-13 21:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:46][root][INFO] - Training Epoch: 2/2, step 6910/7134 completed (loss: 0.04749583080410957, acc: 0.9895833134651184)
[2025-02-13 21:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:46][root][INFO] - Training Epoch: 2/2, step 6911/7134 completed (loss: 0.24510273337364197, acc: 0.950276255607605)
[2025-02-13 21:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:47][root][INFO] - Training Epoch: 2/2, step 6912/7134 completed (loss: 0.10871893912553787, acc: 0.9804878234863281)
[2025-02-13 21:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:47][root][INFO] - Training Epoch: 2/2, step 6913/7134 completed (loss: 0.06920036673545837, acc: 0.9855072498321533)
[2025-02-13 21:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:48][root][INFO] - Training Epoch: 2/2, step 6914/7134 completed (loss: 0.17991459369659424, acc: 0.9607843160629272)
[2025-02-13 21:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:48][root][INFO] - Training Epoch: 2/2, step 6915/7134 completed (loss: 0.08038020133972168, acc: 0.9891892075538635)
[2025-02-13 21:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:48][root][INFO] - Training Epoch: 2/2, step 6916/7134 completed (loss: 0.0946473479270935, acc: 0.9804878234863281)
[2025-02-13 21:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:49][root][INFO] - Training Epoch: 2/2, step 6917/7134 completed (loss: 0.12977083027362823, acc: 0.9793814420700073)
[2025-02-13 21:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:49][root][INFO] - Training Epoch: 2/2, step 6918/7134 completed (loss: 0.05873672291636467, acc: 0.9845361113548279)
[2025-02-13 21:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:49][root][INFO] - Training Epoch: 2/2, step 6919/7134 completed (loss: 0.1178356185555458, acc: 0.9796954393386841)
[2025-02-13 21:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:50][root][INFO] - Training Epoch: 2/2, step 6920/7134 completed (loss: 0.02630867063999176, acc: 0.9940119981765747)
[2025-02-13 21:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:50][root][INFO] - Training Epoch: 2/2, step 6921/7134 completed (loss: 0.08645332604646683, acc: 0.9754902124404907)
[2025-02-13 21:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:51][root][INFO] - Training Epoch: 2/2, step 6922/7134 completed (loss: 0.07788792997598648, acc: 0.9743589758872986)
[2025-02-13 21:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:51][root][INFO] - Training Epoch: 2/2, step 6923/7134 completed (loss: 0.07120242714881897, acc: 0.9912280440330505)
[2025-02-13 21:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:51][root][INFO] - Training Epoch: 2/2, step 6924/7134 completed (loss: 0.0495418906211853, acc: 0.9818181991577148)
[2025-02-13 21:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:52][root][INFO] - Training Epoch: 2/2, step 6925/7134 completed (loss: 0.013127071782946587, acc: 1.0)
[2025-02-13 21:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:52][root][INFO] - Training Epoch: 2/2, step 6926/7134 completed (loss: 0.10335047543048859, acc: 0.9603174328804016)
[2025-02-13 21:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:52][root][INFO] - Training Epoch: 2/2, step 6927/7134 completed (loss: 0.1229635551571846, acc: 0.9752475023269653)
[2025-02-13 21:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:53][root][INFO] - Training Epoch: 2/2, step 6928/7134 completed (loss: 0.1371564269065857, acc: 0.9765625)
[2025-02-13 21:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:53][root][INFO] - Training Epoch: 2/2, step 6929/7134 completed (loss: 0.041411951184272766, acc: 0.9886363744735718)
[2025-02-13 21:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:53][root][INFO] - Training Epoch: 2/2, step 6930/7134 completed (loss: 0.03375132009387016, acc: 0.9895833134651184)
[2025-02-13 21:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:54][root][INFO] - Training Epoch: 2/2, step 6931/7134 completed (loss: 0.0907835066318512, acc: 0.9788359999656677)
[2025-02-13 21:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:54][root][INFO] - Training Epoch: 2/2, step 6932/7134 completed (loss: 0.1355166882276535, acc: 0.976047933101654)
[2025-02-13 21:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:55][root][INFO] - Training Epoch: 2/2, step 6933/7134 completed (loss: 0.13014428317546844, acc: 0.976047933101654)
[2025-02-13 21:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:55][root][INFO] - Training Epoch: 2/2, step 6934/7134 completed (loss: 0.13684329390525818, acc: 0.9615384340286255)
[2025-02-13 21:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:55][root][INFO] - Training Epoch: 2/2, step 6935/7134 completed (loss: 0.0827137902379036, acc: 0.978723406791687)
[2025-02-13 21:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:56][root][INFO] - Training Epoch: 2/2, step 6936/7134 completed (loss: 0.03906533867120743, acc: 0.9946236610412598)
[2025-02-13 21:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:56][root][INFO] - Training Epoch: 2/2, step 6937/7134 completed (loss: 0.14203190803527832, acc: 0.9726027250289917)
[2025-02-13 21:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:56][root][INFO] - Training Epoch: 2/2, step 6938/7134 completed (loss: 0.21748720109462738, acc: 0.9425287246704102)
[2025-02-13 21:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:57][root][INFO] - Training Epoch: 2/2, step 6939/7134 completed (loss: 0.12023739516735077, acc: 0.9642857313156128)
[2025-02-13 21:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:57][root][INFO] - Training Epoch: 2/2, step 6940/7134 completed (loss: 0.17321977019309998, acc: 0.9448275566101074)
[2025-02-13 21:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:57][root][INFO] - Training Epoch: 2/2, step 6941/7134 completed (loss: 0.035981230437755585, acc: 0.9852941036224365)
[2025-02-13 21:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:58][root][INFO] - Training Epoch: 2/2, step 6942/7134 completed (loss: 0.13072435557842255, acc: 0.9745222926139832)
[2025-02-13 21:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:58][root][INFO] - Training Epoch: 2/2, step 6943/7134 completed (loss: 0.11164340376853943, acc: 0.9557521939277649)
[2025-02-13 21:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:58][root][INFO] - Training Epoch: 2/2, step 6944/7134 completed (loss: 0.08487861603498459, acc: 0.9862068891525269)
[2025-02-13 21:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:59][root][INFO] - Training Epoch: 2/2, step 6945/7134 completed (loss: 0.16953931748867035, acc: 0.9609375)
[2025-02-13 21:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:59][root][INFO] - Training Epoch: 2/2, step 6946/7134 completed (loss: 0.06039509177207947, acc: 0.9784946441650391)
[2025-02-13 21:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:59][root][INFO] - Training Epoch: 2/2, step 6947/7134 completed (loss: 0.09417951107025146, acc: 0.982300877571106)
[2025-02-13 21:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:00][root][INFO] - Training Epoch: 2/2, step 6948/7134 completed (loss: 0.14603674411773682, acc: 0.9571428298950195)
[2025-02-13 21:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:00][root][INFO] - Training Epoch: 2/2, step 6949/7134 completed (loss: 0.08378595858812332, acc: 0.9652174115180969)
[2025-02-13 21:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:01][root][INFO] - Training Epoch: 2/2, step 6950/7134 completed (loss: 0.06237264722585678, acc: 0.9873417615890503)
[2025-02-13 21:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:01][root][INFO] - Training Epoch: 2/2, step 6951/7134 completed (loss: 0.07626268267631531, acc: 0.9791666865348816)
[2025-02-13 21:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:01][root][INFO] - Training Epoch: 2/2, step 6952/7134 completed (loss: 0.07214553654193878, acc: 0.9714285731315613)
[2025-02-13 21:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:02][root][INFO] - Training Epoch: 2/2, step 6953/7134 completed (loss: 0.06997555494308472, acc: 0.9924812316894531)
[2025-02-13 21:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:02][root][INFO] - Training Epoch: 2/2, step 6954/7134 completed (loss: 0.17436033487319946, acc: 0.9655172228813171)
[2025-02-13 21:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:02][root][INFO] - Training Epoch: 2/2, step 6955/7134 completed (loss: 0.08420295268297195, acc: 0.9750000238418579)
[2025-02-13 21:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:03][root][INFO] - Training Epoch: 2/2, step 6956/7134 completed (loss: 0.05988062545657158, acc: 0.985401451587677)
[2025-02-13 21:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:03][root][INFO] - Training Epoch: 2/2, step 6957/7134 completed (loss: 0.050772976130247116, acc: 0.984000027179718)
[2025-02-13 21:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:04][root][INFO] - Training Epoch: 2/2, step 6958/7134 completed (loss: 0.10058416426181793, acc: 0.9658119678497314)
[2025-02-13 21:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:04][root][INFO] - Training Epoch: 2/2, step 6959/7134 completed (loss: 0.21286709606647491, acc: 0.9541284441947937)
[2025-02-13 21:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:04][root][INFO] - Training Epoch: 2/2, step 6960/7134 completed (loss: 0.11126963049173355, acc: 0.9791666865348816)
[2025-02-13 21:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:05][root][INFO] - Training Epoch: 2/2, step 6961/7134 completed (loss: 0.04838859662413597, acc: 0.9829059839248657)
[2025-02-13 21:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:05][root][INFO] - Training Epoch: 2/2, step 6962/7134 completed (loss: 0.06806369870901108, acc: 0.9855072498321533)
[2025-02-13 21:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:05][root][INFO] - Training Epoch: 2/2, step 6963/7134 completed (loss: 0.053299430757761, acc: 0.9801324605941772)
[2025-02-13 21:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:06][root][INFO] - Training Epoch: 2/2, step 6964/7134 completed (loss: 0.012598874047398567, acc: 1.0)
[2025-02-13 21:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:06][root][INFO] - Training Epoch: 2/2, step 6965/7134 completed (loss: 0.017389964312314987, acc: 1.0)
[2025-02-13 21:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:07][root][INFO] - Training Epoch: 2/2, step 6966/7134 completed (loss: 0.0442051887512207, acc: 0.9904761910438538)
[2025-02-13 21:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:07][root][INFO] - Training Epoch: 2/2, step 6967/7134 completed (loss: 0.02465415559709072, acc: 1.0)
[2025-02-13 21:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:07][root][INFO] - Training Epoch: 2/2, step 6968/7134 completed (loss: 0.05877798795700073, acc: 0.9865771532058716)
[2025-02-13 21:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:08][root][INFO] - Training Epoch: 2/2, step 6969/7134 completed (loss: 0.14700265228748322, acc: 0.9596773982048035)
[2025-02-13 21:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:08][root][INFO] - Training Epoch: 2/2, step 6970/7134 completed (loss: 0.17120341956615448, acc: 0.9741379022598267)
[2025-02-13 21:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:08][root][INFO] - Training Epoch: 2/2, step 6971/7134 completed (loss: 0.07815209776163101, acc: 0.9930555820465088)
[2025-02-13 21:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:09][root][INFO] - Training Epoch: 2/2, step 6972/7134 completed (loss: 0.08302785456180573, acc: 0.9767441749572754)
[2025-02-13 21:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:09][root][INFO] - Training Epoch: 2/2, step 6973/7134 completed (loss: 0.167398139834404, acc: 0.982758641242981)
[2025-02-13 21:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:10][root][INFO] - Training Epoch: 2/2, step 6974/7134 completed (loss: 0.11212743073701859, acc: 0.9607843160629272)
[2025-02-13 21:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:10][root][INFO] - Training Epoch: 2/2, step 6975/7134 completed (loss: 0.11465933173894882, acc: 0.9914529919624329)
[2025-02-13 21:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:10][root][INFO] - Training Epoch: 2/2, step 6976/7134 completed (loss: 0.05748452618718147, acc: 0.9785714149475098)
[2025-02-13 21:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:11][root][INFO] - Training Epoch: 2/2, step 6977/7134 completed (loss: 0.0838070660829544, acc: 0.9770992398262024)
[2025-02-13 21:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:11][root][INFO] - Training Epoch: 2/2, step 6978/7134 completed (loss: 0.05984731391072273, acc: 0.9873417615890503)
[2025-02-13 21:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:11][root][INFO] - Training Epoch: 2/2, step 6979/7134 completed (loss: 0.05548698082566261, acc: 0.9933775067329407)
[2025-02-13 21:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:12][root][INFO] - Training Epoch: 2/2, step 6980/7134 completed (loss: 0.17382371425628662, acc: 0.9717513918876648)
[2025-02-13 21:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:12][root][INFO] - Training Epoch: 2/2, step 6981/7134 completed (loss: 0.11319334805011749, acc: 0.9536082744598389)
[2025-02-13 21:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:13][root][INFO] - Training Epoch: 2/2, step 6982/7134 completed (loss: 0.19502967596054077, acc: 0.9581151604652405)
[2025-02-13 21:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:13][root][INFO] - Training Epoch: 2/2, step 6983/7134 completed (loss: 0.2208070009946823, acc: 0.9679487347602844)
[2025-02-13 21:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:13][root][INFO] - Training Epoch: 2/2, step 6984/7134 completed (loss: 0.12798328697681427, acc: 0.9674418568611145)
[2025-02-13 21:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:14][root][INFO] - Training Epoch: 2/2, step 6985/7134 completed (loss: 0.21210049092769623, acc: 0.9527897238731384)
[2025-02-13 21:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:14][root][INFO] - Training Epoch: 2/2, step 6986/7134 completed (loss: 0.08449415862560272, acc: 0.9753086566925049)
[2025-02-13 21:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:15][root][INFO] - Training Epoch: 2/2, step 6987/7134 completed (loss: 0.13659042119979858, acc: 0.9710744023323059)
[2025-02-13 21:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:15][root][INFO] - Training Epoch: 2/2, step 6988/7134 completed (loss: 0.05989481881260872, acc: 0.9921259880065918)
[2025-02-13 21:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:15][root][INFO] - Training Epoch: 2/2, step 6989/7134 completed (loss: 0.250728040933609, acc: 0.9459459185600281)
[2025-02-13 21:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:16][root][INFO] - Training Epoch: 2/2, step 6990/7134 completed (loss: 0.03888745233416557, acc: 0.9814814925193787)
[2025-02-13 21:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:16][root][INFO] - Training Epoch: 2/2, step 6991/7134 completed (loss: 0.22600097954273224, acc: 0.9504950642585754)
[2025-02-13 21:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:16][root][INFO] - Training Epoch: 2/2, step 6992/7134 completed (loss: 0.0709349513053894, acc: 0.9729729890823364)
[2025-02-13 21:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:17][root][INFO] - Training Epoch: 2/2, step 6993/7134 completed (loss: 0.11883313953876495, acc: 0.9554139971733093)
[2025-02-13 21:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:17][root][INFO] - Training Epoch: 2/2, step 6994/7134 completed (loss: 0.17423243820667267, acc: 0.9599999785423279)
[2025-02-13 21:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:17][root][INFO] - Training Epoch: 2/2, step 6995/7134 completed (loss: 0.08063182979822159, acc: 0.9709302186965942)
[2025-02-13 21:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:18][root][INFO] - Training Epoch: 2/2, step 6996/7134 completed (loss: 0.026676317676901817, acc: 0.9931507110595703)
[2025-02-13 21:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:18][root][INFO] - Training Epoch: 2/2, step 6997/7134 completed (loss: 0.05153194069862366, acc: 0.9870967864990234)
[2025-02-13 21:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:19][root][INFO] - Training Epoch: 2/2, step 6998/7134 completed (loss: 0.08607243746519089, acc: 0.9789473414421082)
[2025-02-13 21:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:19][root][INFO] - Training Epoch: 2/2, step 6999/7134 completed (loss: 0.04472656548023224, acc: 0.9830508232116699)
[2025-02-13 21:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:19][root][INFO] - Training Epoch: 2/2, step 7000/7134 completed (loss: 0.0481266975402832, acc: 0.9727272987365723)
[2025-02-13 21:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:20][root][INFO] - Training Epoch: 2/2, step 7001/7134 completed (loss: 0.22449594736099243, acc: 0.9740259647369385)
[2025-02-13 21:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:20][root][INFO] - Training Epoch: 2/2, step 7002/7134 completed (loss: 0.07942573726177216, acc: 0.9915966391563416)
[2025-02-13 21:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:20][root][INFO] - Training Epoch: 2/2, step 7003/7134 completed (loss: 0.04524245858192444, acc: 0.984455943107605)
[2025-02-13 21:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:21][root][INFO] - Training Epoch: 2/2, step 7004/7134 completed (loss: 0.08791816234588623, acc: 0.9756097793579102)
[2025-02-13 21:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:21][root][INFO] - Training Epoch: 2/2, step 7005/7134 completed (loss: 0.05271712318062782, acc: 0.9879518151283264)
[2025-02-13 21:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:22][root][INFO] - Training Epoch: 2/2, step 7006/7134 completed (loss: 0.06354865431785583, acc: 0.9751552939414978)
[2025-02-13 21:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:22][root][INFO] - Training Epoch: 2/2, step 7007/7134 completed (loss: 0.08196190744638443, acc: 0.9870967864990234)
[2025-02-13 21:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:22][root][INFO] - Training Epoch: 2/2, step 7008/7134 completed (loss: 0.037882354110479355, acc: 0.9944751262664795)
[2025-02-13 21:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:23][root][INFO] - Training Epoch: 2/2, step 7009/7134 completed (loss: 0.08598563075065613, acc: 0.96875)
[2025-02-13 21:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:23][root][INFO] - Training Epoch: 2/2, step 7010/7134 completed (loss: 0.05254516005516052, acc: 0.9870967864990234)
[2025-02-13 21:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:23][root][INFO] - Training Epoch: 2/2, step 7011/7134 completed (loss: 0.13392947614192963, acc: 0.9738219976425171)
[2025-02-13 21:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:24][root][INFO] - Training Epoch: 2/2, step 7012/7134 completed (loss: 0.02112959884107113, acc: 1.0)
[2025-02-13 21:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:24][root][INFO] - Training Epoch: 2/2, step 7013/7134 completed (loss: 0.02777133323252201, acc: 0.993630588054657)
[2025-02-13 21:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:24][root][INFO] - Training Epoch: 2/2, step 7014/7134 completed (loss: 0.10631220042705536, acc: 0.9662162065505981)
[2025-02-13 21:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:25][root][INFO] - Training Epoch: 2/2, step 7015/7134 completed (loss: 0.04089216887950897, acc: 0.9850746393203735)
[2025-02-13 21:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:25][root][INFO] - Training Epoch: 2/2, step 7016/7134 completed (loss: 0.0990302562713623, acc: 0.9807692170143127)
[2025-02-13 21:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:26][root][INFO] - Training Epoch: 2/2, step 7017/7134 completed (loss: 0.05659202113747597, acc: 0.982300877571106)
[2025-02-13 21:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:26][root][INFO] - Training Epoch: 2/2, step 7018/7134 completed (loss: 0.17624258995056152, acc: 0.9691358208656311)
[2025-02-13 21:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:26][root][INFO] - Training Epoch: 2/2, step 7019/7134 completed (loss: 0.30382832884788513, acc: 0.949367105960846)
[2025-02-13 21:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:27][root][INFO] - Training Epoch: 2/2, step 7020/7134 completed (loss: 0.03372238948941231, acc: 0.9939758777618408)
[2025-02-13 21:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:27][root][INFO] - Training Epoch: 2/2, step 7021/7134 completed (loss: 0.03963317349553108, acc: 0.9925925731658936)
[2025-02-13 21:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:27][root][INFO] - Training Epoch: 2/2, step 7022/7134 completed (loss: 0.12411054223775864, acc: 0.9583333134651184)
[2025-02-13 21:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:28][root][INFO] - Training Epoch: 2/2, step 7023/7134 completed (loss: 0.04514972120523453, acc: 1.0)
[2025-02-13 21:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:28][root][INFO] - Training Epoch: 2/2, step 7024/7134 completed (loss: 0.03613940253853798, acc: 1.0)
[2025-02-13 21:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:29][root][INFO] - Training Epoch: 2/2, step 7025/7134 completed (loss: 0.0935671329498291, acc: 0.9904761910438538)
[2025-02-13 21:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:29][root][INFO] - Training Epoch: 2/2, step 7026/7134 completed (loss: 0.06958907842636108, acc: 0.984375)
[2025-02-13 21:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:29][root][INFO] - Training Epoch: 2/2, step 7027/7134 completed (loss: 0.06425564736127853, acc: 1.0)
[2025-02-13 21:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:30][root][INFO] - Training Epoch: 2/2, step 7028/7134 completed (loss: 0.02516009286046028, acc: 0.9890109896659851)
[2025-02-13 21:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:30][root][INFO] - Training Epoch: 2/2, step 7029/7134 completed (loss: 0.06387721747159958, acc: 0.9775280952453613)
[2025-02-13 21:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:30][root][INFO] - Training Epoch: 2/2, step 7030/7134 completed (loss: 0.04032235965132713, acc: 1.0)
[2025-02-13 21:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:31][root][INFO] - Training Epoch: 2/2, step 7031/7134 completed (loss: 0.07619016617536545, acc: 1.0)
[2025-02-13 21:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:31][root][INFO] - Training Epoch: 2/2, step 7032/7134 completed (loss: 0.06404085457324982, acc: 1.0)
[2025-02-13 21:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:32][root][INFO] - Training Epoch: 2/2, step 7033/7134 completed (loss: 0.15173618495464325, acc: 0.9506173133850098)
[2025-02-13 21:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:32][root][INFO] - Training Epoch: 2/2, step 7034/7134 completed (loss: 0.035792551934719086, acc: 1.0)
[2025-02-13 21:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:32][root][INFO] - Training Epoch: 2/2, step 7035/7134 completed (loss: 0.11761008203029633, acc: 0.9841269850730896)
[2025-02-13 21:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:33][root][INFO] - Training Epoch: 2/2, step 7036/7134 completed (loss: 0.08392001688480377, acc: 0.9682539701461792)
[2025-02-13 21:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:33][root][INFO] - Training Epoch: 2/2, step 7037/7134 completed (loss: 0.0819505825638771, acc: 0.9701492786407471)
[2025-02-13 21:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:33][root][INFO] - Training Epoch: 2/2, step 7038/7134 completed (loss: 0.0821823924779892, acc: 0.9746835231781006)
[2025-02-13 21:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:34][root][INFO] - Training Epoch: 2/2, step 7039/7134 completed (loss: 0.032055530697107315, acc: 1.0)
[2025-02-13 21:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:34][root][INFO] - Training Epoch: 2/2, step 7040/7134 completed (loss: 0.1556614637374878, acc: 0.9599999785423279)
[2025-02-13 21:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:34][root][INFO] - Training Epoch: 2/2, step 7041/7134 completed (loss: 0.09024170786142349, acc: 0.9750000238418579)
[2025-02-13 21:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:35][root][INFO] - Training Epoch: 2/2, step 7042/7134 completed (loss: 0.07085233926773071, acc: 1.0)
[2025-02-13 21:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:35][root][INFO] - Training Epoch: 2/2, step 7043/7134 completed (loss: 0.11109516024589539, acc: 0.9733333587646484)
[2025-02-13 21:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:36][root][INFO] - Training Epoch: 2/2, step 7044/7134 completed (loss: 0.051478683948516846, acc: 1.0)
[2025-02-13 21:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:36][root][INFO] - Training Epoch: 2/2, step 7045/7134 completed (loss: 0.06438720226287842, acc: 0.9862068891525269)
[2025-02-13 21:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:36][root][INFO] - Training Epoch: 2/2, step 7046/7134 completed (loss: 0.03106890805065632, acc: 1.0)
[2025-02-13 21:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:37][root][INFO] - Training Epoch: 2/2, step 7047/7134 completed (loss: 0.049029164016246796, acc: 0.9953051805496216)
[2025-02-13 21:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:37][root][INFO] - Training Epoch: 2/2, step 7048/7134 completed (loss: 0.01905553974211216, acc: 0.9948453903198242)
[2025-02-13 21:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:37][root][INFO] - Training Epoch: 2/2, step 7049/7134 completed (loss: 0.03772762790322304, acc: 1.0)
[2025-02-13 21:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:38][root][INFO] - Training Epoch: 2/2, step 7050/7134 completed (loss: 0.02210211008787155, acc: 1.0)
[2025-02-13 21:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:38][root][INFO] - Training Epoch: 2/2, step 7051/7134 completed (loss: 0.0305694080889225, acc: 0.9945651888847351)
[2025-02-13 21:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:39][root][INFO] - Training Epoch: 2/2, step 7052/7134 completed (loss: 0.05676824599504471, acc: 0.9826589822769165)
[2025-02-13 21:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:39][root][INFO] - Training Epoch: 2/2, step 7053/7134 completed (loss: 0.019415924325585365, acc: 1.0)
[2025-02-13 21:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:39][root][INFO] - Training Epoch: 2/2, step 7054/7134 completed (loss: 0.12346769869327545, acc: 0.9677419066429138)
[2025-02-13 21:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:40][root][INFO] - Training Epoch: 2/2, step 7055/7134 completed (loss: 0.13447806239128113, acc: 0.9615384340286255)
[2025-02-13 21:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:40][root][INFO] - Training Epoch: 2/2, step 7056/7134 completed (loss: 0.10818714648485184, acc: 0.9722222089767456)
[2025-02-13 21:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:40][root][INFO] - Training Epoch: 2/2, step 7057/7134 completed (loss: 0.3452889919281006, acc: 0.9337016344070435)
[2025-02-13 21:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:41][root][INFO] - Training Epoch: 2/2, step 7058/7134 completed (loss: 0.11375529319047928, acc: 0.9689922332763672)
[2025-02-13 21:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:41][root][INFO] - Training Epoch: 2/2, step 7059/7134 completed (loss: 0.05223143473267555, acc: 0.9935897588729858)
[2025-02-13 21:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:42][root][INFO] - Training Epoch: 2/2, step 7060/7134 completed (loss: 0.12168042361736298, acc: 0.9647887349128723)
[2025-02-13 21:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:42][root][INFO] - Training Epoch: 2/2, step 7061/7134 completed (loss: 0.13158439099788666, acc: 0.9623655676841736)
[2025-02-13 21:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:42][root][INFO] - Training Epoch: 2/2, step 7062/7134 completed (loss: 0.11449187994003296, acc: 0.9745762944221497)
[2025-02-13 21:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:43][root][INFO] - Training Epoch: 2/2, step 7063/7134 completed (loss: 0.23498781025409698, acc: 0.9567567706108093)
[2025-02-13 21:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:43][root][INFO] - Training Epoch: 2/2, step 7064/7134 completed (loss: 0.029603291302919388, acc: 0.9939758777618408)
[2025-02-13 21:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:43][root][INFO] - Training Epoch: 2/2, step 7065/7134 completed (loss: 0.011485770344734192, acc: 1.0)
[2025-02-13 21:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:44][root][INFO] - Training Epoch: 2/2, step 7066/7134 completed (loss: 0.040845345705747604, acc: 0.9842932224273682)
[2025-02-13 21:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:44][root][INFO] - Training Epoch: 2/2, step 7067/7134 completed (loss: 0.03789876401424408, acc: 0.9947090148925781)
[2025-02-13 21:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:45][root][INFO] - Training Epoch: 2/2, step 7068/7134 completed (loss: 0.03456580638885498, acc: 0.995121955871582)
[2025-02-13 21:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:45][root][INFO] - Training Epoch: 2/2, step 7069/7134 completed (loss: 0.050750020891427994, acc: 0.9913420081138611)
[2025-02-13 21:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:45][root][INFO] - Training Epoch: 2/2, step 7070/7134 completed (loss: 0.037329450249671936, acc: 0.9884393215179443)
[2025-02-13 21:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:46][root][INFO] - Training Epoch: 2/2, step 7071/7134 completed (loss: 0.03687971085309982, acc: 0.9831932783126831)
[2025-02-13 21:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:46][root][INFO] - Training Epoch: 2/2, step 7072/7134 completed (loss: 0.059759899973869324, acc: 0.9848484992980957)
[2025-02-13 21:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:47][root][INFO] - Training Epoch: 2/2, step 7073/7134 completed (loss: 0.07870330661535263, acc: 0.9899497628211975)
[2025-02-13 21:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:47][root][INFO] - Training Epoch: 2/2, step 7074/7134 completed (loss: 0.17791296541690826, acc: 0.9593495726585388)
[2025-02-13 21:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:47][root][INFO] - Training Epoch: 2/2, step 7075/7134 completed (loss: 0.09803974628448486, acc: 0.9808917045593262)
[2025-02-13 21:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:48][root][INFO] - Training Epoch: 2/2, step 7076/7134 completed (loss: 0.1505695879459381, acc: 0.9550561904907227)
[2025-02-13 21:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:48][root][INFO] - Training Epoch: 2/2, step 7077/7134 completed (loss: 0.1286313235759735, acc: 0.965753436088562)
[2025-02-13 21:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:48][root][INFO] - Training Epoch: 2/2, step 7078/7134 completed (loss: 0.09804406762123108, acc: 0.9724137783050537)
[2025-02-13 21:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:49][root][INFO] - Training Epoch: 2/2, step 7079/7134 completed (loss: 0.04194926843047142, acc: 0.9868420958518982)
[2025-02-13 21:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:49][root][INFO] - Training Epoch: 2/2, step 7080/7134 completed (loss: 0.09455547481775284, acc: 0.9659863710403442)
[2025-02-13 21:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:50][root][INFO] - Training Epoch: 2/2, step 7081/7134 completed (loss: 0.032620564103126526, acc: 0.9884393215179443)
[2025-02-13 21:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:50][root][INFO] - Training Epoch: 2/2, step 7082/7134 completed (loss: 0.11057019233703613, acc: 0.9868420958518982)
[2025-02-13 21:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:50][root][INFO] - Training Epoch: 2/2, step 7083/7134 completed (loss: 0.03519316762685776, acc: 1.0)
[2025-02-13 21:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:51][root][INFO] - Training Epoch: 2/2, step 7084/7134 completed (loss: 0.06313607096672058, acc: 0.977011501789093)
[2025-02-13 21:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:51][root][INFO] - Training Epoch: 2/2, step 7085/7134 completed (loss: 0.0920691266655922, acc: 0.9800000190734863)
[2025-02-13 21:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:51][root][INFO] - Training Epoch: 2/2, step 7086/7134 completed (loss: 0.054613083600997925, acc: 0.9870967864990234)
[2025-02-13 21:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:52][root][INFO] - Training Epoch: 2/2, step 7087/7134 completed (loss: 0.09321212023496628, acc: 0.9844961166381836)
[2025-02-13 21:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:52][root][INFO] - Training Epoch: 2/2, step 7088/7134 completed (loss: 0.10967584699392319, acc: 0.957446813583374)
[2025-02-13 21:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:53][root][INFO] - Training Epoch: 2/2, step 7089/7134 completed (loss: 0.1450912058353424, acc: 0.9741935729980469)
[2025-02-13 21:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:53][root][INFO] - Training Epoch: 2/2, step 7090/7134 completed (loss: 0.0830167606472969, acc: 0.988095223903656)
[2025-02-13 21:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:53][root][INFO] - Training Epoch: 2/2, step 7091/7134 completed (loss: 0.05277280509471893, acc: 0.9757575988769531)
[2025-02-13 21:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:54][root][INFO] - Training Epoch: 2/2, step 7092/7134 completed (loss: 0.1378667801618576, acc: 0.969924807548523)
[2025-02-13 21:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:54][root][INFO] - Training Epoch: 2/2, step 7093/7134 completed (loss: 0.07034406810998917, acc: 0.9743589758872986)
[2025-02-13 21:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:54][root][INFO] - Training Epoch: 2/2, step 7094/7134 completed (loss: 0.021495329216122627, acc: 1.0)
[2025-02-13 21:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:55][root][INFO] - Training Epoch: 2/2, step 7095/7134 completed (loss: 0.0859474390745163, acc: 0.9830508232116699)
[2025-02-13 21:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:55][root][INFO] - Training Epoch: 2/2, step 7096/7134 completed (loss: 0.0466538742184639, acc: 0.9929577708244324)
[2025-02-13 21:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:56][root][INFO] - Training Epoch: 2/2, step 7097/7134 completed (loss: 0.11015372723340988, acc: 0.9487179517745972)
[2025-02-13 21:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:56][root][INFO] - Training Epoch: 2/2, step 7098/7134 completed (loss: 0.054752252995967865, acc: 0.9930070042610168)
[2025-02-13 21:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:56][root][INFO] - Training Epoch: 2/2, step 7099/7134 completed (loss: 0.12133176624774933, acc: 0.9710144996643066)
[2025-02-13 21:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:57][root][INFO] - Training Epoch: 2/2, step 7100/7134 completed (loss: 0.08858097344636917, acc: 0.976047933101654)
[2025-02-13 21:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:57][root][INFO] - Training Epoch: 2/2, step 7101/7134 completed (loss: 0.05878031998872757, acc: 0.9856114983558655)
[2025-02-13 21:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:57][root][INFO] - Training Epoch: 2/2, step 7102/7134 completed (loss: 0.06280556321144104, acc: 0.9661017060279846)
[2025-02-13 21:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:58][root][INFO] - Training Epoch: 2/2, step 7103/7134 completed (loss: 0.02898130752146244, acc: 1.0)
[2025-02-13 21:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:58][root][INFO] - Training Epoch: 2/2, step 7104/7134 completed (loss: 0.06868013739585876, acc: 0.9824561476707458)
[2025-02-13 21:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:59][root][INFO] - Training Epoch: 2/2, step 7105/7134 completed (loss: 0.04928312823176384, acc: 0.9930555820465088)
[2025-02-13 21:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:59][root][INFO] - Training Epoch: 2/2, step 7106/7134 completed (loss: 0.013897469267249107, acc: 1.0)
[2025-02-13 21:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:59][root][INFO] - Training Epoch: 2/2, step 7107/7134 completed (loss: 0.024877438321709633, acc: 0.9937888383865356)
[2025-02-13 21:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:00][root][INFO] - Training Epoch: 2/2, step 7108/7134 completed (loss: 0.03961528465151787, acc: 0.9927007555961609)
[2025-02-13 21:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:00][root][INFO] - Training Epoch: 2/2, step 7109/7134 completed (loss: 0.0916546881198883, acc: 0.9555555582046509)
[2025-02-13 21:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:00][root][INFO] - Training Epoch: 2/2, step 7110/7134 completed (loss: 0.01674998551607132, acc: 1.0)
[2025-02-13 21:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:01][root][INFO] - Training Epoch: 2/2, step 7111/7134 completed (loss: 0.051721666008234024, acc: 0.9907407164573669)
[2025-02-13 21:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:01][root][INFO] - Training Epoch: 2/2, step 7112/7134 completed (loss: 0.02337542176246643, acc: 1.0)
[2025-02-13 21:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:01][root][INFO] - Training Epoch: 2/2, step 7113/7134 completed (loss: 0.1483963429927826, acc: 0.9617834687232971)
[2025-02-13 21:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:02][root][INFO] - Training Epoch: 2/2, step 7114/7134 completed (loss: 0.04420780390501022, acc: 0.9908257126808167)
[2025-02-13 21:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:02][root][INFO] - Training Epoch: 2/2, step 7115/7134 completed (loss: 0.11611338704824448, acc: 0.9541984796524048)
[2025-02-13 21:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:03][root][INFO] - Training Epoch: 2/2, step 7116/7134 completed (loss: 0.10147347301244736, acc: 0.9852941036224365)
[2025-02-13 21:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:03][root][INFO] - Training Epoch: 2/2, step 7117/7134 completed (loss: 0.07884910702705383, acc: 0.9921259880065918)
[2025-02-13 21:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:03][root][INFO] - Training Epoch: 2/2, step 7118/7134 completed (loss: 0.08025725930929184, acc: 0.9857142567634583)
[2025-02-13 21:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:04][root][INFO] - Training Epoch: 2/2, step 7119/7134 completed (loss: 0.14745554327964783, acc: 0.9862068891525269)
[2025-02-13 21:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:04][root][INFO] - Training Epoch: 2/2, step 7120/7134 completed (loss: 0.05619114637374878, acc: 0.9805825352668762)
[2025-02-13 21:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:04][root][INFO] - Training Epoch: 2/2, step 7121/7134 completed (loss: 0.01448030211031437, acc: 1.0)
[2025-02-13 21:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:05][root][INFO] - Training Epoch: 2/2, step 7122/7134 completed (loss: 0.08317936956882477, acc: 0.9677419066429138)
[2025-02-13 21:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:05][root][INFO] - Training Epoch: 2/2, step 7123/7134 completed (loss: 0.016543004661798477, acc: 1.0)
[2025-02-13 21:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:06][root][INFO] - Training Epoch: 2/2, step 7124/7134 completed (loss: 0.08911009877920151, acc: 0.9754098653793335)
[2025-02-13 21:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:06][root][INFO] - Training Epoch: 2/2, step 7125/7134 completed (loss: 0.08661731332540512, acc: 0.9770992398262024)
[2025-02-13 21:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:06][root][INFO] - Training Epoch: 2/2, step 7126/7134 completed (loss: 0.1515541970729828, acc: 0.9635036587715149)
[2025-02-13 21:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:07][root][INFO] - Training Epoch: 2/2, step 7127/7134 completed (loss: 0.03582124784588814, acc: 0.9876543283462524)
[2025-02-13 21:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:07][root][INFO] - Training Epoch: 2/2, step 7128/7134 completed (loss: 0.07771466672420502, acc: 0.9726027250289917)
[2025-02-13 21:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:07][root][INFO] - Training Epoch: 2/2, step 7129/7134 completed (loss: 0.3068464994430542, acc: 0.9379844665527344)
[2025-02-13 21:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:00][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2268, device='cuda:0') eval_epoch_loss=tensor(0.2044, device='cuda:0') eval_epoch_acc=tensor(0.9521, device='cuda:0')
[2025-02-13 21:20:00][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 21:20:00][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 21:20:00][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_7130_loss_0.20444858074188232/model.pt
[2025-02-13 21:20:00][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 21:20:00][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.20444858074188232
[2025-02-13 21:20:00][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9520611763000488
[2025-02-13 21:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:01][root][INFO] - Training Epoch: 2/2, step 7130/7134 completed (loss: 0.26827287673950195, acc: 0.9200000166893005)
[2025-02-13 21:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:01][root][INFO] - Training Epoch: 2/2, step 7131/7134 completed (loss: 0.04423867538571358, acc: 0.9905660152435303)
[2025-02-13 21:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:01][root][INFO] - Training Epoch: 2/2, step 7132/7134 completed (loss: 0.017424480989575386, acc: 1.0)
[2025-02-13 21:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:02][root][INFO] - Training Epoch: 2/2, step 7133/7134 completed (loss: 0.1254364401102066, acc: 0.9849624037742615)
[2025-02-13 21:20:02][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.1078, train_epoch_loss=0.1024, epoch time 3612.5791520690545s
[2025-02-13 21:20:02][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-13 21:20:02][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 21 GB
[2025-02-13 21:20:02][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-13 21:20:02][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-13 21:20:02][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 8 GB
[2025-02-13 21:20:02][root][INFO] - Key: avg_train_prep, Value: 1.2068006992340088
[2025-02-13 21:20:02][root][INFO] - Key: avg_train_loss, Value: 0.1845998615026474
[2025-02-13 21:20:02][root][INFO] - Key: avg_train_acc, Value: 0.956378698348999
[2025-02-13 21:20:02][root][INFO] - Key: avg_eval_prep, Value: 1.2677905559539795
[2025-02-13 21:20:02][root][INFO] - Key: avg_eval_loss, Value: 0.23653744161128998
[2025-02-13 21:20:02][root][INFO] - Key: avg_eval_acc, Value: 0.9437452554702759
[2025-02-13 21:20:02][root][INFO] - Key: avg_epoch_time, Value: 3634.2287681815214
[2025-02-13 21:20:02][root][INFO] - Key: avg_checkpoint_time, Value: 0.2781551438383758
Selected lowest loss checkpoint: asr_epoch_2_step_7130_loss_0.20444858074188232
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_7130_loss_0.20444858074188232/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_7130_loss_0.20444858074188232
[2025-02-13 21:20:28][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-13 21:20:28][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 21:20:28][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100'}
[2025-02-13 21:20:29][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 21:20:34][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 21:20:34][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 21:20:34][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 21:20:34][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 21:20:37][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 21:20:37][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 21:20:37][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 21:20:37][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 21:20:37][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 21:20:37][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 21:20:37][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 21:20:37][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_7130_loss_0.20444858074188232/model.pt
[2025-02-13 21:20:37][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 21:20:37][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 21:20:39][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 21:20:39][root][INFO] - --> Training Set Length = 2620
[2025-02-13 21:20:39][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/repetition_penalty.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-02-13 21:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:38:25][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/decode_test_beam4_pred_20250213_212039
[2025-02-13 21:38:25][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/decode_test_beam4_gt_20250213_212039
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/decode_test_beam4_gt_20250213_212039
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/decode_test_beam4_pred_20250213_212039
Combined WER: 0.05764987827145466

Filtering repeated words...

Found 0 repeated lines in total.
Filtered Combined WER: 0.05764987827145466
