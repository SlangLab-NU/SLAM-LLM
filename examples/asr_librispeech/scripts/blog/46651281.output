/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
train_data_folder: aphasia_phoneme
test_data_folder: aphasia_phoneme
use_peft: true
seed: 
debug: 
Is test_run? 
freeze_encoder: true
Is save_embedding? false
projector_transfer_learning: true
transfer_data_folder: psst_phoneme
llm_inference_config: repetition_penalty
eval_ckpt: best
----------
----------
Final identifier: aphasia_phoneme_wavlm_llama32_1b_linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_23834_loss_0.4641912281513214



----- Transfer Learning Information -----
Resume Epoch: 1
Resume Step: 0
Train Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl
Validation Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl
Test Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl
Identifier: aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
Output Directory: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
----------------------------------------
----------------------------------------
Resume epoch: 1
Resume step: 0
[2025-02-12 21:56:43][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-12 21:56:43][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-12 21:56:43][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-02-12 21:56:43][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-12_21-56-42.txt', 'log_interval': 5}
[2025-02-12 21:57:11][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-12 21:57:16][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-12 21:57:16][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-12 21:57:16][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-12 21:57:16][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-12 21:57:26][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-12 21:57:26][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-12 21:57:26][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-12 21:57:27][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-12 21:57:27][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-12 21:57:27][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-12 21:57:27][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-12 21:57:27][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_23834_loss_0.4641912281513214/model.pt
[2025-02-12 21:57:27][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-12 21:57:27][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-12 21:57:28][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-12 21:57:34][root][INFO] - --> Training Set Length = 2298
[2025-02-12 21:57:34][root][INFO] - --> Validation Set Length = 341
[2025-02-12 21:57:34][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-12 21:57:34][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-12 21:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:37][root][INFO] - Training Epoch: 1/2, step 0/574 completed (loss: 1.2123773097991943, acc: 0.5925925970077515)
[2025-02-12 21:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:38][root][INFO] - Training Epoch: 1/2, step 1/574 completed (loss: 1.0402058362960815, acc: 0.7200000286102295)
[2025-02-12 21:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:38][root][INFO] - Training Epoch: 1/2, step 2/574 completed (loss: 1.9819635152816772, acc: 0.5945945978164673)
[2025-02-12 21:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:39][root][INFO] - Training Epoch: 1/2, step 3/574 completed (loss: 2.007331132888794, acc: 0.6315789222717285)
[2025-02-12 21:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:39][root][INFO] - Training Epoch: 1/2, step 4/574 completed (loss: 2.044408082962036, acc: 0.5135135054588318)
[2025-02-12 21:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:39][root][INFO] - Training Epoch: 1/2, step 5/574 completed (loss: 0.9733689427375793, acc: 0.6428571343421936)
[2025-02-12 21:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:40][root][INFO] - Training Epoch: 1/2, step 6/574 completed (loss: 2.235187530517578, acc: 0.5918367505073547)
[2025-02-12 21:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:40][root][INFO] - Training Epoch: 1/2, step 7/574 completed (loss: 1.080947756767273, acc: 0.7666666507720947)
[2025-02-12 21:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:40][root][INFO] - Training Epoch: 1/2, step 8/574 completed (loss: 0.5479683876037598, acc: 0.8636363744735718)
[2025-02-12 21:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:41][root][INFO] - Training Epoch: 1/2, step 9/574 completed (loss: 0.3224884569644928, acc: 0.8461538553237915)
[2025-02-12 21:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:41][root][INFO] - Training Epoch: 1/2, step 10/574 completed (loss: 0.5700385570526123, acc: 0.8518518805503845)
[2025-02-12 21:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:42][root][INFO] - Training Epoch: 1/2, step 11/574 completed (loss: 2.7608680725097656, acc: 0.5641025900840759)
[2025-02-12 21:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:42][root][INFO] - Training Epoch: 1/2, step 12/574 completed (loss: 1.9869766235351562, acc: 0.7272727489471436)
[2025-02-12 21:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:42][root][INFO] - Training Epoch: 1/2, step 13/574 completed (loss: 1.67548406124115, acc: 0.6304348111152649)
[2025-02-12 21:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:43][root][INFO] - Training Epoch: 1/2, step 14/574 completed (loss: 1.803454875946045, acc: 0.7647058963775635)
[2025-02-12 21:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:43][root][INFO] - Training Epoch: 1/2, step 15/574 completed (loss: 1.240387201309204, acc: 0.7142857313156128)
[2025-02-12 21:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:44][root][INFO] - Training Epoch: 1/2, step 16/574 completed (loss: 0.8095865845680237, acc: 0.7894737124443054)
[2025-02-12 21:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:44][root][INFO] - Training Epoch: 1/2, step 17/574 completed (loss: 0.9106447100639343, acc: 0.8333333134651184)
[2025-02-12 21:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:44][root][INFO] - Training Epoch: 1/2, step 18/574 completed (loss: 1.6903462409973145, acc: 0.6944444179534912)
[2025-02-12 21:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:45][root][INFO] - Training Epoch: 1/2, step 19/574 completed (loss: 0.5465536117553711, acc: 0.8421052694320679)
[2025-02-12 21:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:45][root][INFO] - Training Epoch: 1/2, step 20/574 completed (loss: 0.8957732319831848, acc: 0.8846153616905212)
[2025-02-12 21:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:45][root][INFO] - Training Epoch: 1/2, step 21/574 completed (loss: 1.5926676988601685, acc: 0.7241379022598267)
[2025-02-12 21:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:46][root][INFO] - Training Epoch: 1/2, step 22/574 completed (loss: 1.7029953002929688, acc: 0.5600000023841858)
[2025-02-12 21:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:46][root][INFO] - Training Epoch: 1/2, step 23/574 completed (loss: 1.1780054569244385, acc: 0.8571428656578064)
[2025-02-12 21:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:46][root][INFO] - Training Epoch: 1/2, step 24/574 completed (loss: 0.841812014579773, acc: 0.75)
[2025-02-12 21:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:47][root][INFO] - Training Epoch: 1/2, step 25/574 completed (loss: 1.5828889608383179, acc: 0.6792452931404114)
[2025-02-12 21:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:47][root][INFO] - Training Epoch: 1/2, step 26/574 completed (loss: 2.1746575832366943, acc: 0.5068492889404297)
[2025-02-12 21:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:48][root][INFO] - Training Epoch: 1/2, step 27/574 completed (loss: 1.8525415658950806, acc: 0.5770751237869263)
[2025-02-12 21:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:49][root][INFO] - Training Epoch: 1/2, step 28/574 completed (loss: 1.5487024784088135, acc: 0.7441860437393188)
[2025-02-12 21:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:49][root][INFO] - Training Epoch: 1/2, step 29/574 completed (loss: 1.6990346908569336, acc: 0.7228915691375732)
[2025-02-12 21:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:49][root][INFO] - Training Epoch: 1/2, step 30/574 completed (loss: 1.3771754503250122, acc: 0.7654321193695068)
[2025-02-12 21:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:50][root][INFO] - Training Epoch: 1/2, step 31/574 completed (loss: 1.5923372507095337, acc: 0.6428571343421936)
[2025-02-12 21:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:50][root][INFO] - Training Epoch: 1/2, step 32/574 completed (loss: 1.13718843460083, acc: 0.7407407164573669)
[2025-02-12 21:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:50][root][INFO] - Training Epoch: 1/2, step 33/574 completed (loss: 0.9883363842964172, acc: 0.9130434989929199)
[2025-02-12 21:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:51][root][INFO] - Training Epoch: 1/2, step 34/574 completed (loss: 1.4844530820846558, acc: 0.680672287940979)
[2025-02-12 21:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:51][root][INFO] - Training Epoch: 1/2, step 35/574 completed (loss: 1.1843876838684082, acc: 0.8196721076965332)
[2025-02-12 21:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:51][root][INFO] - Training Epoch: 1/2, step 36/574 completed (loss: 1.9367684125900269, acc: 0.6349206566810608)
[2025-02-12 21:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:52][root][INFO] - Training Epoch: 1/2, step 37/574 completed (loss: 1.758981704711914, acc: 0.7796609997749329)
[2025-02-12 21:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:52][root][INFO] - Training Epoch: 1/2, step 38/574 completed (loss: 1.778971552848816, acc: 0.7241379022598267)
[2025-02-12 21:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:52][root][INFO] - Training Epoch: 1/2, step 39/574 completed (loss: 0.9752068519592285, acc: 0.7142857313156128)
[2025-02-12 21:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:53][root][INFO] - Training Epoch: 1/2, step 40/574 completed (loss: 0.5208830237388611, acc: 0.8461538553237915)
[2025-02-12 21:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:53][root][INFO] - Training Epoch: 1/2, step 41/574 completed (loss: 1.0186275243759155, acc: 0.7567567825317383)
[2025-02-12 21:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:54][root][INFO] - Training Epoch: 1/2, step 42/574 completed (loss: 2.7743992805480957, acc: 0.5538461804389954)
[2025-02-12 21:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:54][root][INFO] - Training Epoch: 1/2, step 43/574 completed (loss: 2.4932823181152344, acc: 0.5252525210380554)
[2025-02-12 21:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:54][root][INFO] - Training Epoch: 1/2, step 44/574 completed (loss: 2.2864832878112793, acc: 0.6494845151901245)
[2025-02-12 21:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:55][root][INFO] - Training Epoch: 1/2, step 45/574 completed (loss: 2.981733798980713, acc: 0.5661764740943909)
[2025-02-12 21:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:55][root][INFO] - Training Epoch: 1/2, step 46/574 completed (loss: 1.1867396831512451, acc: 0.692307710647583)
[2025-02-12 21:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:55][root][INFO] - Training Epoch: 1/2, step 47/574 completed (loss: 0.7638466954231262, acc: 0.8148148059844971)
[2025-02-12 21:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:56][root][INFO] - Training Epoch: 1/2, step 48/574 completed (loss: 0.9092983603477478, acc: 0.75)
[2025-02-12 21:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:56][root][INFO] - Training Epoch: 1/2, step 49/574 completed (loss: 0.7460682392120361, acc: 0.9444444179534912)
[2025-02-12 21:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:56][root][INFO] - Training Epoch: 1/2, step 50/574 completed (loss: 2.7767059803009033, acc: 0.5964912176132202)
[2025-02-12 21:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:57][root][INFO] - Training Epoch: 1/2, step 51/574 completed (loss: 2.2970659732818604, acc: 0.6507936716079712)
[2025-02-12 21:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:57][root][INFO] - Training Epoch: 1/2, step 52/574 completed (loss: 3.69342303276062, acc: 0.5492957830429077)
[2025-02-12 21:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:58][root][INFO] - Training Epoch: 1/2, step 53/574 completed (loss: 3.6603238582611084, acc: 0.46000000834465027)
[2025-02-12 21:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:58][root][INFO] - Training Epoch: 1/2, step 54/574 completed (loss: 3.4149038791656494, acc: 0.5135135054588318)
[2025-02-12 21:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:57:58][root][INFO] - Training Epoch: 1/2, step 55/574 completed (loss: 0.7382039427757263, acc: 0.7692307829856873)
[2025-02-12 21:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:01][root][INFO] - Training Epoch: 1/2, step 56/574 completed (loss: 2.2497355937957764, acc: 0.49829351902008057)
[2025-02-12 21:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:02][root][INFO] - Training Epoch: 1/2, step 57/574 completed (loss: 2.7494893074035645, acc: 0.4575163424015045)
[2025-02-12 21:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:03][root][INFO] - Training Epoch: 1/2, step 58/574 completed (loss: 3.457415819168091, acc: 0.3920454680919647)
[2025-02-12 21:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:04][root][INFO] - Training Epoch: 1/2, step 59/574 completed (loss: 2.2614872455596924, acc: 0.5441176295280457)
[2025-02-12 21:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:04][root][INFO] - Training Epoch: 1/2, step 60/574 completed (loss: 3.223626136779785, acc: 0.41304346919059753)
[2025-02-12 21:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:05][root][INFO] - Training Epoch: 1/2, step 61/574 completed (loss: 2.5292458534240723, acc: 0.574999988079071)
[2025-02-12 21:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:05][root][INFO] - Training Epoch: 1/2, step 62/574 completed (loss: 1.5675443410873413, acc: 0.6470588445663452)
[2025-02-12 21:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:05][root][INFO] - Training Epoch: 1/2, step 63/574 completed (loss: 3.2193446159362793, acc: 0.4444444477558136)
[2025-02-12 21:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:06][root][INFO] - Training Epoch: 1/2, step 64/574 completed (loss: 1.4208825826644897, acc: 0.796875)
[2025-02-12 21:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:06][root][INFO] - Training Epoch: 1/2, step 65/574 completed (loss: 0.3042449951171875, acc: 0.8965517282485962)
[2025-02-12 21:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:06][root][INFO] - Training Epoch: 1/2, step 66/574 completed (loss: 3.2106730937957764, acc: 0.5178571343421936)
[2025-02-12 21:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:07][root][INFO] - Training Epoch: 1/2, step 67/574 completed (loss: 3.1955277919769287, acc: 0.4000000059604645)
[2025-02-12 21:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:07][root][INFO] - Training Epoch: 1/2, step 68/574 completed (loss: 0.8033586740493774, acc: 0.800000011920929)
[2025-02-12 21:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:07][root][INFO] - Training Epoch: 1/2, step 69/574 completed (loss: 1.654498815536499, acc: 0.6944444179534912)
[2025-02-12 21:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:08][root][INFO] - Training Epoch: 1/2, step 70/574 completed (loss: 3.0442283153533936, acc: 0.5454545617103577)
[2025-02-12 21:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:08][root][INFO] - Training Epoch: 1/2, step 71/574 completed (loss: 2.3495121002197266, acc: 0.5735294222831726)
[2025-02-12 21:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:08][root][INFO] - Training Epoch: 1/2, step 72/574 completed (loss: 1.4581241607666016, acc: 0.6666666865348816)
[2025-02-12 21:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:09][root][INFO] - Training Epoch: 1/2, step 73/574 completed (loss: 2.0577187538146973, acc: 0.5435897707939148)
[2025-02-12 21:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:09][root][INFO] - Training Epoch: 1/2, step 74/574 completed (loss: 3.233293294906616, acc: 0.5)
[2025-02-12 21:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:10][root][INFO] - Training Epoch: 1/2, step 75/574 completed (loss: 2.3597354888916016, acc: 0.5)
[2025-02-12 21:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:10][root][INFO] - Training Epoch: 1/2, step 76/574 completed (loss: 2.5842175483703613, acc: 0.485401451587677)
[2025-02-12 21:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:10][root][INFO] - Training Epoch: 1/2, step 77/574 completed (loss: 0.37037181854248047, acc: 0.9523809552192688)
[2025-02-12 21:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:11][root][INFO] - Training Epoch: 1/2, step 78/574 completed (loss: 0.7823255658149719, acc: 0.7916666865348816)
[2025-02-12 21:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:11][root][INFO] - Training Epoch: 1/2, step 79/574 completed (loss: 0.46254023909568787, acc: 0.8484848737716675)
[2025-02-12 21:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:12][root][INFO] - Training Epoch: 1/2, step 80/574 completed (loss: 1.080047845840454, acc: 0.8461538553237915)
[2025-02-12 21:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:12][root][INFO] - Training Epoch: 1/2, step 81/574 completed (loss: 2.012784242630005, acc: 0.6153846383094788)
[2025-02-12 21:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:12][root][INFO] - Training Epoch: 1/2, step 82/574 completed (loss: 1.8766556978225708, acc: 0.6730769276618958)
[2025-02-12 21:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:13][root][INFO] - Training Epoch: 1/2, step 83/574 completed (loss: 1.2910434007644653, acc: 0.6875)
[2025-02-12 21:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:13][root][INFO] - Training Epoch: 1/2, step 84/574 completed (loss: 2.0254597663879395, acc: 0.6521739363670349)
[2025-02-12 21:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:13][root][INFO] - Training Epoch: 1/2, step 85/574 completed (loss: 2.6548616886138916, acc: 0.5400000214576721)
[2025-02-12 21:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:14][root][INFO] - Training Epoch: 1/2, step 86/574 completed (loss: 1.1134964227676392, acc: 0.695652186870575)
[2025-02-12 21:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:14][root][INFO] - Training Epoch: 1/2, step 87/574 completed (loss: 3.0203843116760254, acc: 0.46000000834465027)
[2025-02-12 21:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:15][root][INFO] - Training Epoch: 1/2, step 88/574 completed (loss: 2.5188121795654297, acc: 0.6019417643547058)
[2025-02-12 21:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:16][root][INFO] - Training Epoch: 1/2, step 89/574 completed (loss: 2.5474514961242676, acc: 0.5728155374526978)
[2025-02-12 21:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:17][root][INFO] - Training Epoch: 1/2, step 90/574 completed (loss: 2.5929300785064697, acc: 0.5645161271095276)
[2025-02-12 21:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:17][root][INFO] - Training Epoch: 1/2, step 91/574 completed (loss: 2.1956162452697754, acc: 0.5517241358757019)
[2025-02-12 21:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:18][root][INFO] - Training Epoch: 1/2, step 92/574 completed (loss: 2.1674816608428955, acc: 0.6421052813529968)
[2025-02-12 21:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:19][root][INFO] - Training Epoch: 1/2, step 93/574 completed (loss: 2.803779125213623, acc: 0.3465346395969391)
[2025-02-12 21:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:20][root][INFO] - Training Epoch: 1/2, step 94/574 completed (loss: 1.9846004247665405, acc: 0.5967742204666138)
[2025-02-12 21:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:20][root][INFO] - Training Epoch: 1/2, step 95/574 completed (loss: 1.9313743114471436, acc: 0.6811594367027283)
[2025-02-12 21:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:20][root][INFO] - Training Epoch: 1/2, step 96/574 completed (loss: 2.903655529022217, acc: 0.5042017102241516)
[2025-02-12 21:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:21][root][INFO] - Training Epoch: 1/2, step 97/574 completed (loss: 2.4056878089904785, acc: 0.5288461446762085)
[2025-02-12 21:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:21][root][INFO] - Training Epoch: 1/2, step 98/574 completed (loss: 2.4805190563201904, acc: 0.540145993232727)
[2025-02-12 21:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:22][root][INFO] - Training Epoch: 1/2, step 99/574 completed (loss: 3.296006202697754, acc: 0.3731343150138855)
[2025-02-12 21:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:22][root][INFO] - Training Epoch: 1/2, step 100/574 completed (loss: 1.5298702716827393, acc: 0.6000000238418579)
[2025-02-12 21:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:22][root][INFO] - Training Epoch: 1/2, step 101/574 completed (loss: 0.7606174349784851, acc: 0.9545454382896423)
[2025-02-12 21:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:23][root][INFO] - Training Epoch: 1/2, step 102/574 completed (loss: 0.24039141833782196, acc: 0.95652174949646)
[2025-02-12 21:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:23][root][INFO] - Training Epoch: 1/2, step 103/574 completed (loss: 0.6113458871841431, acc: 0.7954545617103577)
[2025-02-12 21:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:23][root][INFO] - Training Epoch: 1/2, step 104/574 completed (loss: 1.434637427330017, acc: 0.6896551847457886)
[2025-02-12 21:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:24][root][INFO] - Training Epoch: 1/2, step 105/574 completed (loss: 0.9927895069122314, acc: 0.7906976938247681)
[2025-02-12 21:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:24][root][INFO] - Training Epoch: 1/2, step 106/574 completed (loss: 1.4568448066711426, acc: 0.7200000286102295)
[2025-02-12 21:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:24][root][INFO] - Training Epoch: 1/2, step 107/574 completed (loss: 0.23853784799575806, acc: 0.9411764740943909)
[2025-02-12 21:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:25][root][INFO] - Training Epoch: 1/2, step 108/574 completed (loss: 0.19032065570354462, acc: 0.9615384340286255)
[2025-02-12 21:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:25][root][INFO] - Training Epoch: 1/2, step 109/574 completed (loss: 0.6855543255805969, acc: 0.9285714030265808)
[2025-02-12 21:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:25][root][INFO] - Training Epoch: 1/2, step 110/574 completed (loss: 1.584069848060608, acc: 0.7384615540504456)
[2025-02-12 21:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:26][root][INFO] - Training Epoch: 1/2, step 111/574 completed (loss: 1.8123921155929565, acc: 0.6315789222717285)
[2025-02-12 21:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:26][root][INFO] - Training Epoch: 1/2, step 112/574 completed (loss: 2.127500534057617, acc: 0.6140350699424744)
[2025-02-12 21:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:27][root][INFO] - Training Epoch: 1/2, step 113/574 completed (loss: 1.9607309103012085, acc: 0.6410256624221802)
[2025-02-12 21:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:27][root][INFO] - Training Epoch: 1/2, step 114/574 completed (loss: 1.5019817352294922, acc: 0.6734693646430969)
[2025-02-12 21:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:28][root][INFO] - Training Epoch: 1/2, step 115/574 completed (loss: 0.3831067383289337, acc: 0.9090909361839294)
[2025-02-12 21:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:28][root][INFO] - Training Epoch: 1/2, step 116/574 completed (loss: 1.154505729675293, acc: 0.6984127163887024)
[2025-02-12 21:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:28][root][INFO] - Training Epoch: 1/2, step 117/574 completed (loss: 1.2515751123428345, acc: 0.7479674816131592)
[2025-02-12 21:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:29][root][INFO] - Training Epoch: 1/2, step 118/574 completed (loss: 1.5560117959976196, acc: 0.7580645084381104)
[2025-02-12 21:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:30][root][INFO] - Training Epoch: 1/2, step 119/574 completed (loss: 1.457958698272705, acc: 0.661596953868866)
[2025-02-12 21:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:30][root][INFO] - Training Epoch: 1/2, step 120/574 completed (loss: 1.4706735610961914, acc: 0.746666669845581)
[2025-02-12 21:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:30][root][INFO] - Training Epoch: 1/2, step 121/574 completed (loss: 2.0692861080169678, acc: 0.6730769276618958)
[2025-02-12 21:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:31][root][INFO] - Training Epoch: 1/2, step 122/574 completed (loss: 0.6971147060394287, acc: 0.8333333134651184)
[2025-02-12 21:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:31][root][INFO] - Training Epoch: 1/2, step 123/574 completed (loss: 0.5011228919029236, acc: 0.7894737124443054)
[2025-02-12 21:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:32][root][INFO] - Training Epoch: 1/2, step 124/574 completed (loss: 1.6680556535720825, acc: 0.6257668733596802)
[2025-02-12 21:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:32][root][INFO] - Training Epoch: 1/2, step 125/574 completed (loss: 1.637912631034851, acc: 0.6388888955116272)
[2025-02-12 21:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:32][root][INFO] - Training Epoch: 1/2, step 126/574 completed (loss: 1.6645395755767822, acc: 0.6416666507720947)
[2025-02-12 21:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:33][root][INFO] - Training Epoch: 1/2, step 127/574 completed (loss: 1.1655194759368896, acc: 0.7202380895614624)
[2025-02-12 21:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:33][root][INFO] - Training Epoch: 1/2, step 128/574 completed (loss: 1.2881964445114136, acc: 0.7025641202926636)
[2025-02-12 21:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:34][root][INFO] - Training Epoch: 1/2, step 129/574 completed (loss: 1.5169670581817627, acc: 0.6617646813392639)
[2025-02-12 21:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:34][root][INFO] - Training Epoch: 1/2, step 130/574 completed (loss: 1.96230947971344, acc: 0.5384615659713745)
[2025-02-12 21:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:34][root][INFO] - Training Epoch: 1/2, step 131/574 completed (loss: 1.1008390188217163, acc: 0.8260869383811951)
[2025-02-12 21:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:35][root][INFO] - Training Epoch: 1/2, step 132/574 completed (loss: 1.467386245727539, acc: 0.59375)
[2025-02-12 21:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:35][root][INFO] - Training Epoch: 1/2, step 133/574 completed (loss: 1.883764386177063, acc: 0.47826087474823)
[2025-02-12 21:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:35][root][INFO] - Training Epoch: 1/2, step 134/574 completed (loss: 1.2442923784255981, acc: 0.6571428775787354)
[2025-02-12 21:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:36][root][INFO] - Training Epoch: 1/2, step 135/574 completed (loss: 1.603270411491394, acc: 0.5)
[2025-02-12 21:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:36][root][INFO] - Training Epoch: 1/2, step 136/574 completed (loss: 1.636455774307251, acc: 0.6904761791229248)
[2025-02-12 21:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:36][root][INFO] - Training Epoch: 1/2, step 137/574 completed (loss: 1.6343705654144287, acc: 0.5)
[2025-02-12 21:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:37][root][INFO] - Training Epoch: 1/2, step 138/574 completed (loss: 1.481778860092163, acc: 0.6521739363670349)
[2025-02-12 21:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:37][root][INFO] - Training Epoch: 1/2, step 139/574 completed (loss: 0.8218656182289124, acc: 0.761904776096344)
[2025-02-12 21:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:38][root][INFO] - Training Epoch: 1/2, step 140/574 completed (loss: 1.9753196239471436, acc: 0.6153846383094788)
[2025-02-12 21:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:38][root][INFO] - Training Epoch: 1/2, step 141/574 completed (loss: 1.9750863313674927, acc: 0.4516128897666931)
[2025-02-12 21:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:38][root][INFO] - Training Epoch: 1/2, step 142/574 completed (loss: 1.3458967208862305, acc: 0.6216216087341309)
[2025-02-12 21:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:12][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.9451, device='cuda:0') eval_epoch_loss=tensor(1.0802, device='cuda:0') eval_epoch_acc=tensor(0.7609, device='cuda:0')
[2025-02-12 21:59:12][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 21:59:12][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 21:59:12][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_143_loss_1.0801525115966797/model.pt
[2025-02-12 21:59:12][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 21:59:12][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.0801525115966797
[2025-02-12 21:59:12][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7609144449234009
[2025-02-12 21:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:13][root][INFO] - Training Epoch: 1/2, step 143/574 completed (loss: 2.08769154548645, acc: 0.5350877046585083)
[2025-02-12 21:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:13][root][INFO] - Training Epoch: 1/2, step 144/574 completed (loss: 1.4927887916564941, acc: 0.6641790866851807)
[2025-02-12 21:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:14][root][INFO] - Training Epoch: 1/2, step 145/574 completed (loss: 1.5016640424728394, acc: 0.6224489808082581)
[2025-02-12 21:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:14][root][INFO] - Training Epoch: 1/2, step 146/574 completed (loss: 1.9498937129974365, acc: 0.542553186416626)
[2025-02-12 21:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:15][root][INFO] - Training Epoch: 1/2, step 147/574 completed (loss: 2.396799087524414, acc: 0.5285714268684387)
[2025-02-12 21:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:15][root][INFO] - Training Epoch: 1/2, step 148/574 completed (loss: 2.1481878757476807, acc: 0.5)
[2025-02-12 21:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:15][root][INFO] - Training Epoch: 1/2, step 149/574 completed (loss: 1.4212249517440796, acc: 0.6521739363670349)
[2025-02-12 21:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:16][root][INFO] - Training Epoch: 1/2, step 150/574 completed (loss: 1.0913982391357422, acc: 0.8275862336158752)
[2025-02-12 21:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:16][root][INFO] - Training Epoch: 1/2, step 151/574 completed (loss: 1.9369066953659058, acc: 0.6086956262588501)
[2025-02-12 21:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:16][root][INFO] - Training Epoch: 1/2, step 152/574 completed (loss: 1.57001531124115, acc: 0.6101694703102112)
[2025-02-12 21:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:17][root][INFO] - Training Epoch: 1/2, step 153/574 completed (loss: 1.957608938217163, acc: 0.5789473652839661)
[2025-02-12 21:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:17][root][INFO] - Training Epoch: 1/2, step 154/574 completed (loss: 2.0750279426574707, acc: 0.5405405163764954)
[2025-02-12 21:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:18][root][INFO] - Training Epoch: 1/2, step 155/574 completed (loss: 1.3979756832122803, acc: 0.7142857313156128)
[2025-02-12 21:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:18][root][INFO] - Training Epoch: 1/2, step 156/574 completed (loss: 0.5811362862586975, acc: 0.8260869383811951)
[2025-02-12 21:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:18][root][INFO] - Training Epoch: 1/2, step 157/574 completed (loss: 2.0574448108673096, acc: 0.4736842215061188)
[2025-02-12 21:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:20][root][INFO] - Training Epoch: 1/2, step 158/574 completed (loss: 2.7132954597473145, acc: 0.5675675868988037)
[2025-02-12 21:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:20][root][INFO] - Training Epoch: 1/2, step 159/574 completed (loss: 1.8372586965560913, acc: 0.5740740895271301)
[2025-02-12 21:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:21][root][INFO] - Training Epoch: 1/2, step 160/574 completed (loss: 2.2550735473632812, acc: 0.5348837375640869)
[2025-02-12 21:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:21][root][INFO] - Training Epoch: 1/2, step 161/574 completed (loss: 2.3187878131866455, acc: 0.5647059082984924)
[2025-02-12 21:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:22][root][INFO] - Training Epoch: 1/2, step 162/574 completed (loss: 2.1832025051116943, acc: 0.516853928565979)
[2025-02-12 21:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:22][root][INFO] - Training Epoch: 1/2, step 163/574 completed (loss: 1.3932379484176636, acc: 0.75)
[2025-02-12 21:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:23][root][INFO] - Training Epoch: 1/2, step 164/574 completed (loss: 0.6373917460441589, acc: 0.8095238208770752)
[2025-02-12 21:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:23][root][INFO] - Training Epoch: 1/2, step 165/574 completed (loss: 0.7883686423301697, acc: 0.7241379022598267)
[2025-02-12 21:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:23][root][INFO] - Training Epoch: 1/2, step 166/574 completed (loss: 0.5328935384750366, acc: 0.8979591727256775)
[2025-02-12 21:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:24][root][INFO] - Training Epoch: 1/2, step 167/574 completed (loss: 0.5688377618789673, acc: 0.8399999737739563)
[2025-02-12 21:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:24][root][INFO] - Training Epoch: 1/2, step 168/574 completed (loss: 1.570313572883606, acc: 0.7222222089767456)
[2025-02-12 21:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:24][root][INFO] - Training Epoch: 1/2, step 169/574 completed (loss: 1.160468339920044, acc: 0.7156862616539001)
[2025-02-12 21:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:26][root][INFO] - Training Epoch: 1/2, step 170/574 completed (loss: 1.6547750234603882, acc: 0.6506849527359009)
[2025-02-12 21:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:26][root][INFO] - Training Epoch: 1/2, step 171/574 completed (loss: 0.8481748104095459, acc: 0.875)
[2025-02-12 21:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:26][root][INFO] - Training Epoch: 1/2, step 172/574 completed (loss: 0.8869986534118652, acc: 0.8148148059844971)
[2025-02-12 21:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:27][root][INFO] - Training Epoch: 1/2, step 173/574 completed (loss: 1.3841685056686401, acc: 0.5)
[2025-02-12 21:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:27][root][INFO] - Training Epoch: 1/2, step 174/574 completed (loss: 1.419053554534912, acc: 0.7079645991325378)
[2025-02-12 21:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:28][root][INFO] - Training Epoch: 1/2, step 175/574 completed (loss: 1.5245933532714844, acc: 0.6811594367027283)
[2025-02-12 21:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:28][root][INFO] - Training Epoch: 1/2, step 176/574 completed (loss: 0.8815701007843018, acc: 0.8181818127632141)
[2025-02-12 21:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:29][root][INFO] - Training Epoch: 1/2, step 177/574 completed (loss: 1.7131644487380981, acc: 0.6106870174407959)
[2025-02-12 21:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:30][root][INFO] - Training Epoch: 1/2, step 178/574 completed (loss: 1.9916985034942627, acc: 0.5259259343147278)
[2025-02-12 21:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:30][root][INFO] - Training Epoch: 1/2, step 179/574 completed (loss: 1.0946223735809326, acc: 0.7377049326896667)
[2025-02-12 21:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:30][root][INFO] - Training Epoch: 1/2, step 180/574 completed (loss: 0.5659821629524231, acc: 0.7916666865348816)
[2025-02-12 21:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:31][root][INFO] - Training Epoch: 1/2, step 181/574 completed (loss: 0.28665387630462646, acc: 0.8799999952316284)
[2025-02-12 21:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:31][root][INFO] - Training Epoch: 1/2, step 182/574 completed (loss: 0.5430733561515808, acc: 0.8571428656578064)
[2025-02-12 21:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:31][root][INFO] - Training Epoch: 1/2, step 183/574 completed (loss: 0.7806044816970825, acc: 0.8170731663703918)
[2025-02-12 21:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:32][root][INFO] - Training Epoch: 1/2, step 184/574 completed (loss: 1.3013031482696533, acc: 0.7643504738807678)
[2025-02-12 21:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:32][root][INFO] - Training Epoch: 1/2, step 185/574 completed (loss: 0.9869517683982849, acc: 0.8040345907211304)
[2025-02-12 21:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:33][root][INFO] - Training Epoch: 1/2, step 186/574 completed (loss: 0.9212331771850586, acc: 0.8125)
[2025-02-12 21:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:33][root][INFO] - Training Epoch: 1/2, step 187/574 completed (loss: 0.9023360013961792, acc: 0.7842401266098022)
[2025-02-12 21:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:34][root][INFO] - Training Epoch: 1/2, step 188/574 completed (loss: 1.161608099937439, acc: 0.725978672504425)
[2025-02-12 21:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:34][root][INFO] - Training Epoch: 1/2, step 189/574 completed (loss: 1.1666743755340576, acc: 0.7200000286102295)
[2025-02-12 21:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:35][root][INFO] - Training Epoch: 1/2, step 190/574 completed (loss: 1.853481411933899, acc: 0.5465116500854492)
[2025-02-12 21:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:35][root][INFO] - Training Epoch: 1/2, step 191/574 completed (loss: 2.290001392364502, acc: 0.4920634925365448)
[2025-02-12 21:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:36][root][INFO] - Training Epoch: 1/2, step 192/574 completed (loss: 1.937469244003296, acc: 0.5530303120613098)
[2025-02-12 21:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:37][root][INFO] - Training Epoch: 1/2, step 193/574 completed (loss: 1.8420616388320923, acc: 0.529411792755127)
[2025-02-12 21:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:38][root][INFO] - Training Epoch: 1/2, step 194/574 completed (loss: 1.9001545906066895, acc: 0.5432098507881165)
[2025-02-12 21:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:39][root][INFO] - Training Epoch: 1/2, step 195/574 completed (loss: 1.6814155578613281, acc: 0.5806451439857483)
[2025-02-12 21:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:40][root][INFO] - Training Epoch: 1/2, step 196/574 completed (loss: 0.7297446131706238, acc: 0.8571428656578064)
[2025-02-12 21:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:40][root][INFO] - Training Epoch: 1/2, step 197/574 completed (loss: 2.0972254276275635, acc: 0.574999988079071)
[2025-02-12 21:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:40][root][INFO] - Training Epoch: 1/2, step 198/574 completed (loss: 1.6928048133850098, acc: 0.6323529481887817)
[2025-02-12 21:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:41][root][INFO] - Training Epoch: 1/2, step 199/574 completed (loss: 1.5379893779754639, acc: 0.7132353186607361)
[2025-02-12 21:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:41][root][INFO] - Training Epoch: 1/2, step 200/574 completed (loss: 1.1041122674942017, acc: 0.6525423526763916)
[2025-02-12 21:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:42][root][INFO] - Training Epoch: 1/2, step 201/574 completed (loss: 1.6795494556427002, acc: 0.611940324306488)
[2025-02-12 21:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:42][root][INFO] - Training Epoch: 1/2, step 202/574 completed (loss: 1.818236231803894, acc: 0.6116504669189453)
[2025-02-12 21:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:42][root][INFO] - Training Epoch: 1/2, step 203/574 completed (loss: 1.4484997987747192, acc: 0.60317462682724)
[2025-02-12 21:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:43][root][INFO] - Training Epoch: 1/2, step 204/574 completed (loss: 0.5479060411453247, acc: 0.8791208863258362)
[2025-02-12 21:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:43][root][INFO] - Training Epoch: 1/2, step 205/574 completed (loss: 0.9679054021835327, acc: 0.8161435127258301)
[2025-02-12 21:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:44][root][INFO] - Training Epoch: 1/2, step 206/574 completed (loss: 1.0436527729034424, acc: 0.7440944910049438)
[2025-02-12 21:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:44][root][INFO] - Training Epoch: 1/2, step 207/574 completed (loss: 1.0793498754501343, acc: 0.7758620977401733)
[2025-02-12 21:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:44][root][INFO] - Training Epoch: 1/2, step 208/574 completed (loss: 0.8825123310089111, acc: 0.804347813129425)
[2025-02-12 21:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:45][root][INFO] - Training Epoch: 1/2, step 209/574 completed (loss: 1.1458039283752441, acc: 0.7665369510650635)
[2025-02-12 21:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:45][root][INFO] - Training Epoch: 1/2, step 210/574 completed (loss: 0.6506490707397461, acc: 0.8152173757553101)
[2025-02-12 21:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:45][root][INFO] - Training Epoch: 1/2, step 211/574 completed (loss: 0.6138526797294617, acc: 0.739130437374115)
[2025-02-12 21:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:46][root][INFO] - Training Epoch: 1/2, step 212/574 completed (loss: 0.2716505527496338, acc: 0.9642857313156128)
[2025-02-12 21:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:46][root][INFO] - Training Epoch: 1/2, step 213/574 completed (loss: 0.820440411567688, acc: 0.8936170339584351)
[2025-02-12 21:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:47][root][INFO] - Training Epoch: 1/2, step 214/574 completed (loss: 1.072165608406067, acc: 0.7769230604171753)
[2025-02-12 21:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:47][root][INFO] - Training Epoch: 1/2, step 215/574 completed (loss: 0.48250827193260193, acc: 0.8513513803482056)
[2025-02-12 21:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:48][root][INFO] - Training Epoch: 1/2, step 216/574 completed (loss: 1.0370128154754639, acc: 0.8023256063461304)
[2025-02-12 21:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:48][root][INFO] - Training Epoch: 1/2, step 217/574 completed (loss: 1.1111165285110474, acc: 0.792792797088623)
[2025-02-12 21:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:49][root][INFO] - Training Epoch: 1/2, step 218/574 completed (loss: 0.8534373641014099, acc: 0.855555534362793)
[2025-02-12 21:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:49][root][INFO] - Training Epoch: 1/2, step 219/574 completed (loss: 0.7510775327682495, acc: 0.8484848737716675)
[2025-02-12 21:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:49][root][INFO] - Training Epoch: 1/2, step 220/574 completed (loss: 0.511722207069397, acc: 0.8888888955116272)
[2025-02-12 21:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:50][root][INFO] - Training Epoch: 1/2, step 221/574 completed (loss: 0.2784675657749176, acc: 0.9200000166893005)
[2025-02-12 21:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:50][root][INFO] - Training Epoch: 1/2, step 222/574 completed (loss: 1.3358190059661865, acc: 0.6153846383094788)
[2025-02-12 21:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:51][root][INFO] - Training Epoch: 1/2, step 223/574 completed (loss: 0.8816747069358826, acc: 0.8097826242446899)
[2025-02-12 21:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:51][root][INFO] - Training Epoch: 1/2, step 224/574 completed (loss: 1.1601189374923706, acc: 0.6988636255264282)
[2025-02-12 21:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:52][root][INFO] - Training Epoch: 1/2, step 225/574 completed (loss: 1.0729546546936035, acc: 0.6595744490623474)
[2025-02-12 21:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:52][root][INFO] - Training Epoch: 1/2, step 226/574 completed (loss: 1.4511430263519287, acc: 0.6415094137191772)
[2025-02-12 21:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:53][root][INFO] - Training Epoch: 1/2, step 227/574 completed (loss: 1.1318639516830444, acc: 0.6833333373069763)
[2025-02-12 21:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:53][root][INFO] - Training Epoch: 1/2, step 228/574 completed (loss: 0.828577995300293, acc: 0.7674418687820435)
[2025-02-12 21:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:53][root][INFO] - Training Epoch: 1/2, step 229/574 completed (loss: 1.7356823682785034, acc: 0.6000000238418579)
[2025-02-12 21:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:54][root][INFO] - Training Epoch: 1/2, step 230/574 completed (loss: 1.9508459568023682, acc: 0.5368421077728271)
[2025-02-12 21:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:54][root][INFO] - Training Epoch: 1/2, step 231/574 completed (loss: 1.8662577867507935, acc: 0.5777778029441833)
[2025-02-12 21:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:55][root][INFO] - Training Epoch: 1/2, step 232/574 completed (loss: 1.6468968391418457, acc: 0.5611110925674438)
[2025-02-12 21:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:55][root][INFO] - Training Epoch: 1/2, step 233/574 completed (loss: 2.036818265914917, acc: 0.5137614607810974)
[2025-02-12 21:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:56][root][INFO] - Training Epoch: 1/2, step 234/574 completed (loss: 1.7127125263214111, acc: 0.5692307949066162)
[2025-02-12 21:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:56][root][INFO] - Training Epoch: 1/2, step 235/574 completed (loss: 0.8178277015686035, acc: 0.7368420958518982)
[2025-02-12 21:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:57][root][INFO] - Training Epoch: 1/2, step 236/574 completed (loss: 0.7368059754371643, acc: 0.75)
[2025-02-12 21:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:57][root][INFO] - Training Epoch: 1/2, step 237/574 completed (loss: 1.5920604467391968, acc: 0.6363636255264282)
[2025-02-12 21:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:57][root][INFO] - Training Epoch: 1/2, step 238/574 completed (loss: 0.9472301602363586, acc: 0.7407407164573669)
[2025-02-12 21:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:58][root][INFO] - Training Epoch: 1/2, step 239/574 completed (loss: 1.079761266708374, acc: 0.6285714507102966)
[2025-02-12 21:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:58][root][INFO] - Training Epoch: 1/2, step 240/574 completed (loss: 1.562287449836731, acc: 0.6590909361839294)
[2025-02-12 21:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:58][root][INFO] - Training Epoch: 1/2, step 241/574 completed (loss: 1.2027331590652466, acc: 0.7045454382896423)
[2025-02-12 21:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:59][root][INFO] - Training Epoch: 1/2, step 242/574 completed (loss: 1.7108664512634277, acc: 0.5322580933570862)
[2025-02-12 21:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 21:59:59][root][INFO] - Training Epoch: 1/2, step 243/574 completed (loss: 1.7148677110671997, acc: 0.5454545617103577)
[2025-02-12 22:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:00][root][INFO] - Training Epoch: 1/2, step 244/574 completed (loss: 0.1694619357585907, acc: 0.9523809552192688)
[2025-02-12 22:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:00][root][INFO] - Training Epoch: 1/2, step 245/574 completed (loss: 0.3190336227416992, acc: 0.9230769276618958)
[2025-02-12 22:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:00][root][INFO] - Training Epoch: 1/2, step 246/574 completed (loss: 0.3336920142173767, acc: 0.9032257795333862)
[2025-02-12 22:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:01][root][INFO] - Training Epoch: 1/2, step 247/574 completed (loss: 0.5549851655960083, acc: 0.8500000238418579)
[2025-02-12 22:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:01][root][INFO] - Training Epoch: 1/2, step 248/574 completed (loss: 1.2472898960113525, acc: 0.7297297120094299)
[2025-02-12 22:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:01][root][INFO] - Training Epoch: 1/2, step 249/574 completed (loss: 0.7335430383682251, acc: 0.8108108043670654)
[2025-02-12 22:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:02][root][INFO] - Training Epoch: 1/2, step 250/574 completed (loss: 1.071156620979309, acc: 0.837837815284729)
[2025-02-12 22:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:02][root][INFO] - Training Epoch: 1/2, step 251/574 completed (loss: 0.8526139259338379, acc: 0.8235294222831726)
[2025-02-12 22:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:03][root][INFO] - Training Epoch: 1/2, step 252/574 completed (loss: 1.012326717376709, acc: 0.7317073345184326)
[2025-02-12 22:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:03][root][INFO] - Training Epoch: 1/2, step 253/574 completed (loss: 0.46642962098121643, acc: 0.8399999737739563)
[2025-02-12 22:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:03][root][INFO] - Training Epoch: 1/2, step 254/574 completed (loss: 0.07313131541013718, acc: 1.0)
[2025-02-12 22:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:04][root][INFO] - Training Epoch: 1/2, step 255/574 completed (loss: 0.44773146510124207, acc: 0.8387096524238586)
[2025-02-12 22:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:04][root][INFO] - Training Epoch: 1/2, step 256/574 completed (loss: 0.6544640064239502, acc: 0.8947368264198303)
[2025-02-12 22:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:04][root][INFO] - Training Epoch: 1/2, step 257/574 completed (loss: 0.434998482465744, acc: 0.9142857193946838)
[2025-02-12 22:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:05][root][INFO] - Training Epoch: 1/2, step 258/574 completed (loss: 0.3735605776309967, acc: 0.9210526347160339)
[2025-02-12 22:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:05][root][INFO] - Training Epoch: 1/2, step 259/574 completed (loss: 0.7126741409301758, acc: 0.7830188870429993)
[2025-02-12 22:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:06][root][INFO] - Training Epoch: 1/2, step 260/574 completed (loss: 0.7603464722633362, acc: 0.8083333373069763)
[2025-02-12 22:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:06][root][INFO] - Training Epoch: 1/2, step 261/574 completed (loss: 0.5604020357131958, acc: 0.8611111044883728)
[2025-02-12 22:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:07][root][INFO] - Training Epoch: 1/2, step 262/574 completed (loss: 1.2012379169464111, acc: 0.7096773982048035)
[2025-02-12 22:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:07][root][INFO] - Training Epoch: 1/2, step 263/574 completed (loss: 1.8371034860610962, acc: 0.5866666436195374)
[2025-02-12 22:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:07][root][INFO] - Training Epoch: 1/2, step 264/574 completed (loss: 1.2045860290527344, acc: 0.7291666865348816)
[2025-02-12 22:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:08][root][INFO] - Training Epoch: 1/2, step 265/574 completed (loss: 1.9490433931350708, acc: 0.5600000023841858)
[2025-02-12 22:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:08][root][INFO] - Training Epoch: 1/2, step 266/574 completed (loss: 1.9769465923309326, acc: 0.516853928565979)
[2025-02-12 22:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:09][root][INFO] - Training Epoch: 1/2, step 267/574 completed (loss: 1.9049484729766846, acc: 0.44594594836235046)
[2025-02-12 22:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:09][root][INFO] - Training Epoch: 1/2, step 268/574 completed (loss: 1.2393410205841064, acc: 0.6896551847457886)
[2025-02-12 22:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:10][root][INFO] - Training Epoch: 1/2, step 269/574 completed (loss: 0.7818021178245544, acc: 0.8181818127632141)
[2025-02-12 22:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:10][root][INFO] - Training Epoch: 1/2, step 270/574 completed (loss: 0.4831980764865875, acc: 0.8181818127632141)
[2025-02-12 22:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:10][root][INFO] - Training Epoch: 1/2, step 271/574 completed (loss: 0.3251945972442627, acc: 0.90625)
[2025-02-12 22:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:11][root][INFO] - Training Epoch: 1/2, step 272/574 completed (loss: 0.21700340509414673, acc: 0.9333333373069763)
[2025-02-12 22:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:11][root][INFO] - Training Epoch: 1/2, step 273/574 completed (loss: 0.6478086709976196, acc: 0.8333333134651184)
[2025-02-12 22:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:12][root][INFO] - Training Epoch: 1/2, step 274/574 completed (loss: 0.32198503613471985, acc: 0.90625)
[2025-02-12 22:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:12][root][INFO] - Training Epoch: 1/2, step 275/574 completed (loss: 0.5962250232696533, acc: 0.8666666746139526)
[2025-02-12 22:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:12][root][INFO] - Training Epoch: 1/2, step 276/574 completed (loss: 0.40108799934387207, acc: 0.8965517282485962)
[2025-02-12 22:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:13][root][INFO] - Training Epoch: 1/2, step 277/574 completed (loss: 0.5949490070343018, acc: 0.8399999737739563)
[2025-02-12 22:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:13][root][INFO] - Training Epoch: 1/2, step 278/574 completed (loss: 0.7463226914405823, acc: 0.8936170339584351)
[2025-02-12 22:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:13][root][INFO] - Training Epoch: 1/2, step 279/574 completed (loss: 0.8352725505828857, acc: 0.8333333134651184)
[2025-02-12 22:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:14][root][INFO] - Training Epoch: 1/2, step 280/574 completed (loss: 0.3814314901828766, acc: 0.8863636255264282)
[2025-02-12 22:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:14][root][INFO] - Training Epoch: 1/2, step 281/574 completed (loss: 1.2035377025604248, acc: 0.6746987700462341)
[2025-02-12 22:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:15][root][INFO] - Training Epoch: 1/2, step 282/574 completed (loss: 1.2901239395141602, acc: 0.6851851940155029)
[2025-02-12 22:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:15][root][INFO] - Training Epoch: 1/2, step 283/574 completed (loss: 0.6307272911071777, acc: 0.8421052694320679)
[2025-02-12 22:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:15][root][INFO] - Training Epoch: 1/2, step 284/574 completed (loss: 1.1516668796539307, acc: 0.6764705777168274)
[2025-02-12 22:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:16][root][INFO] - Training Epoch: 1/2, step 285/574 completed (loss: 0.40807995200157166, acc: 0.875)
[2025-02-12 22:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:47][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1102, device='cuda:0') eval_epoch_loss=tensor(0.7468, device='cuda:0') eval_epoch_acc=tensor(0.8076, device='cuda:0')
[2025-02-12 22:00:47][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:00:47][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:00:47][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_286_loss_0.7467750906944275/model.pt
[2025-02-12 22:00:47][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:00:47][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.7467750906944275
[2025-02-12 22:00:47][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8076428771018982
[2025-02-12 22:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:48][root][INFO] - Training Epoch: 1/2, step 286/574 completed (loss: 0.9449479579925537, acc: 0.7421875)
[2025-02-12 22:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:48][root][INFO] - Training Epoch: 1/2, step 287/574 completed (loss: 0.9878478050231934, acc: 0.7440000176429749)
[2025-02-12 22:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:48][root][INFO] - Training Epoch: 1/2, step 288/574 completed (loss: 0.6887773871421814, acc: 0.8351648449897766)
[2025-02-12 22:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:49][root][INFO] - Training Epoch: 1/2, step 289/574 completed (loss: 1.2217451333999634, acc: 0.7142857313156128)
[2025-02-12 22:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:49][root][INFO] - Training Epoch: 1/2, step 290/574 completed (loss: 0.8620022535324097, acc: 0.7989690899848938)
[2025-02-12 22:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:49][root][INFO] - Training Epoch: 1/2, step 291/574 completed (loss: 0.2380959838628769, acc: 0.9090909361839294)
[2025-02-12 22:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:50][root][INFO] - Training Epoch: 1/2, step 292/574 completed (loss: 0.750339925289154, acc: 0.761904776096344)
[2025-02-12 22:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:50][root][INFO] - Training Epoch: 1/2, step 293/574 completed (loss: 0.8763107657432556, acc: 0.8275862336158752)
[2025-02-12 22:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:51][root][INFO] - Training Epoch: 1/2, step 294/574 completed (loss: 0.8441298007965088, acc: 0.7454545497894287)
[2025-02-12 22:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:51][root][INFO] - Training Epoch: 1/2, step 295/574 completed (loss: 0.8440207839012146, acc: 0.8041236996650696)
[2025-02-12 22:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:52][root][INFO] - Training Epoch: 1/2, step 296/574 completed (loss: 0.7602466940879822, acc: 0.8103448152542114)
[2025-02-12 22:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:52][root][INFO] - Training Epoch: 1/2, step 297/574 completed (loss: 0.7117629051208496, acc: 0.7777777910232544)
[2025-02-12 22:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:52][root][INFO] - Training Epoch: 1/2, step 298/574 completed (loss: 1.039329171180725, acc: 0.7105262875556946)
[2025-02-12 22:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:53][root][INFO] - Training Epoch: 1/2, step 299/574 completed (loss: 0.14752556383609772, acc: 0.9821428656578064)
[2025-02-12 22:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:53][root][INFO] - Training Epoch: 1/2, step 300/574 completed (loss: 0.2419891357421875, acc: 0.875)
[2025-02-12 22:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:53][root][INFO] - Training Epoch: 1/2, step 301/574 completed (loss: 0.6242915391921997, acc: 0.8113207817077637)
[2025-02-12 22:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:54][root][INFO] - Training Epoch: 1/2, step 302/574 completed (loss: 0.42530202865600586, acc: 0.9056603908538818)
[2025-02-12 22:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:54][root][INFO] - Training Epoch: 1/2, step 303/574 completed (loss: 0.14365245401859283, acc: 0.970588207244873)
[2025-02-12 22:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:54][root][INFO] - Training Epoch: 1/2, step 304/574 completed (loss: 0.20995578169822693, acc: 0.9375)
[2025-02-12 22:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:55][root][INFO] - Training Epoch: 1/2, step 305/574 completed (loss: 0.888203501701355, acc: 0.7540983557701111)
[2025-02-12 22:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:55][root][INFO] - Training Epoch: 1/2, step 306/574 completed (loss: 0.5124996304512024, acc: 0.9333333373069763)
[2025-02-12 22:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:55][root][INFO] - Training Epoch: 1/2, step 307/574 completed (loss: 0.3449234068393707, acc: 0.8947368264198303)
[2025-02-12 22:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:56][root][INFO] - Training Epoch: 1/2, step 308/574 completed (loss: 0.7191181182861328, acc: 0.7971014380455017)
[2025-02-12 22:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:56][root][INFO] - Training Epoch: 1/2, step 309/574 completed (loss: 0.5229915380477905, acc: 0.8611111044883728)
[2025-02-12 22:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:57][root][INFO] - Training Epoch: 1/2, step 310/574 completed (loss: 0.6473991274833679, acc: 0.8433734774589539)
[2025-02-12 22:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:57][root][INFO] - Training Epoch: 1/2, step 311/574 completed (loss: 0.7557769417762756, acc: 0.7820512652397156)
[2025-02-12 22:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:57][root][INFO] - Training Epoch: 1/2, step 312/574 completed (loss: 0.31616732478141785, acc: 0.9285714030265808)
[2025-02-12 22:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:58][root][INFO] - Training Epoch: 1/2, step 313/574 completed (loss: 0.09525372833013535, acc: 1.0)
[2025-02-12 22:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:58][root][INFO] - Training Epoch: 1/2, step 314/574 completed (loss: 0.12647919356822968, acc: 1.0)
[2025-02-12 22:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:58][root][INFO] - Training Epoch: 1/2, step 315/574 completed (loss: 0.5358619689941406, acc: 0.8387096524238586)
[2025-02-12 22:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:59][root][INFO] - Training Epoch: 1/2, step 316/574 completed (loss: 0.7168335318565369, acc: 0.774193525314331)
[2025-02-12 22:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:59][root][INFO] - Training Epoch: 1/2, step 317/574 completed (loss: 0.6757089495658875, acc: 0.8059701323509216)
[2025-02-12 22:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:00:59][root][INFO] - Training Epoch: 1/2, step 318/574 completed (loss: 0.43351465463638306, acc: 0.8942307829856873)
[2025-02-12 22:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:00][root][INFO] - Training Epoch: 1/2, step 319/574 completed (loss: 0.4769134819507599, acc: 0.8888888955116272)
[2025-02-12 22:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:00][root][INFO] - Training Epoch: 1/2, step 320/574 completed (loss: 0.2151661068201065, acc: 0.9193548560142517)
[2025-02-12 22:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:00][root][INFO] - Training Epoch: 1/2, step 321/574 completed (loss: 0.3638240396976471, acc: 0.9399999976158142)
[2025-02-12 22:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:01][root][INFO] - Training Epoch: 1/2, step 322/574 completed (loss: 1.1290398836135864, acc: 0.7037037014961243)
[2025-02-12 22:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:01][root][INFO] - Training Epoch: 1/2, step 323/574 completed (loss: 1.3304983377456665, acc: 0.5714285969734192)
[2025-02-12 22:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:01][root][INFO] - Training Epoch: 1/2, step 324/574 completed (loss: 1.992315411567688, acc: 0.5128205418586731)
[2025-02-12 22:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:02][root][INFO] - Training Epoch: 1/2, step 325/574 completed (loss: 2.0087759494781494, acc: 0.5365853905677795)
[2025-02-12 22:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:02][root][INFO] - Training Epoch: 1/2, step 326/574 completed (loss: 1.243739366531372, acc: 0.5789473652839661)
[2025-02-12 22:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:02][root][INFO] - Training Epoch: 1/2, step 327/574 completed (loss: 0.721504271030426, acc: 0.8947368264198303)
[2025-02-12 22:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:03][root][INFO] - Training Epoch: 1/2, step 328/574 completed (loss: 0.17686648666858673, acc: 0.9642857313156128)
[2025-02-12 22:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:03][root][INFO] - Training Epoch: 1/2, step 329/574 completed (loss: 0.38242554664611816, acc: 0.8888888955116272)
[2025-02-12 22:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:03][root][INFO] - Training Epoch: 1/2, step 330/574 completed (loss: 0.18382851779460907, acc: 0.96875)
[2025-02-12 22:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:04][root][INFO] - Training Epoch: 1/2, step 331/574 completed (loss: 0.5057264566421509, acc: 0.8548387289047241)
[2025-02-12 22:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:04][root][INFO] - Training Epoch: 1/2, step 332/574 completed (loss: 0.3747192621231079, acc: 0.8947368264198303)
[2025-02-12 22:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:04][root][INFO] - Training Epoch: 1/2, step 333/574 completed (loss: 0.35606709122657776, acc: 0.875)
[2025-02-12 22:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:05][root][INFO] - Training Epoch: 1/2, step 334/574 completed (loss: 0.409087598323822, acc: 0.8666666746139526)
[2025-02-12 22:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:05][root][INFO] - Training Epoch: 1/2, step 335/574 completed (loss: 0.9987577199935913, acc: 0.6315789222717285)
[2025-02-12 22:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:05][root][INFO] - Training Epoch: 1/2, step 336/574 completed (loss: 1.126467227935791, acc: 0.6399999856948853)
[2025-02-12 22:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:06][root][INFO] - Training Epoch: 1/2, step 337/574 completed (loss: 1.5852762460708618, acc: 0.6206896305084229)
[2025-02-12 22:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:06][root][INFO] - Training Epoch: 1/2, step 338/574 completed (loss: 1.6444710493087769, acc: 0.5106382966041565)
[2025-02-12 22:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:06][root][INFO] - Training Epoch: 1/2, step 339/574 completed (loss: 1.598536491394043, acc: 0.5783132314682007)
[2025-02-12 22:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:07][root][INFO] - Training Epoch: 1/2, step 340/574 completed (loss: 0.6313288807868958, acc: 0.8695651888847351)
[2025-02-12 22:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:07][root][INFO] - Training Epoch: 1/2, step 341/574 completed (loss: 0.8895851373672485, acc: 0.7692307829856873)
[2025-02-12 22:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:07][root][INFO] - Training Epoch: 1/2, step 342/574 completed (loss: 0.940765380859375, acc: 0.7831325531005859)
[2025-02-12 22:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:08][root][INFO] - Training Epoch: 1/2, step 343/574 completed (loss: 0.8096299767494202, acc: 0.7924528121948242)
[2025-02-12 22:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:08][root][INFO] - Training Epoch: 1/2, step 344/574 completed (loss: 0.38342007994651794, acc: 0.8987341523170471)
[2025-02-12 22:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:08][root][INFO] - Training Epoch: 1/2, step 345/574 completed (loss: 0.26852482557296753, acc: 0.9215686321258545)
[2025-02-12 22:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:09][root][INFO] - Training Epoch: 1/2, step 346/574 completed (loss: 0.641817033290863, acc: 0.8656716346740723)
[2025-02-12 22:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:09][root][INFO] - Training Epoch: 1/2, step 347/574 completed (loss: 0.08767368644475937, acc: 0.949999988079071)
[2025-02-12 22:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:09][root][INFO] - Training Epoch: 1/2, step 348/574 completed (loss: 1.1131420135498047, acc: 0.7599999904632568)
[2025-02-12 22:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:10][root][INFO] - Training Epoch: 1/2, step 349/574 completed (loss: 0.9989109039306641, acc: 0.6666666865348816)
[2025-02-12 22:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:10][root][INFO] - Training Epoch: 1/2, step 350/574 completed (loss: 1.2323874235153198, acc: 0.6279069781303406)
[2025-02-12 22:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:11][root][INFO] - Training Epoch: 1/2, step 351/574 completed (loss: 0.6488567590713501, acc: 0.8205128312110901)
[2025-02-12 22:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:11][root][INFO] - Training Epoch: 1/2, step 352/574 completed (loss: 1.1771858930587769, acc: 0.7333333492279053)
[2025-02-12 22:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:11][root][INFO] - Training Epoch: 1/2, step 353/574 completed (loss: 0.13328494131565094, acc: 0.95652174949646)
[2025-02-12 22:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:12][root][INFO] - Training Epoch: 1/2, step 354/574 completed (loss: 1.4159101247787476, acc: 0.7307692170143127)
[2025-02-12 22:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:12][root][INFO] - Training Epoch: 1/2, step 355/574 completed (loss: 1.2430665493011475, acc: 0.6593406796455383)
[2025-02-12 22:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:13][root][INFO] - Training Epoch: 1/2, step 356/574 completed (loss: 1.014370322227478, acc: 0.695652186870575)
[2025-02-12 22:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:13][root][INFO] - Training Epoch: 1/2, step 357/574 completed (loss: 1.1030431985855103, acc: 0.6847826242446899)
[2025-02-12 22:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:13][root][INFO] - Training Epoch: 1/2, step 358/574 completed (loss: 0.9928907155990601, acc: 0.7551020383834839)
[2025-02-12 22:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:14][root][INFO] - Training Epoch: 1/2, step 359/574 completed (loss: 0.04363548383116722, acc: 1.0)
[2025-02-12 22:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:14][root][INFO] - Training Epoch: 1/2, step 360/574 completed (loss: 0.46537846326828003, acc: 0.9615384340286255)
[2025-02-12 22:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:14][root][INFO] - Training Epoch: 1/2, step 361/574 completed (loss: 0.8291962742805481, acc: 0.7560975551605225)
[2025-02-12 22:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:15][root][INFO] - Training Epoch: 1/2, step 362/574 completed (loss: 0.5664269924163818, acc: 0.8666666746139526)
[2025-02-12 22:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:15][root][INFO] - Training Epoch: 1/2, step 363/574 completed (loss: 0.6012520790100098, acc: 0.8684210777282715)
[2025-02-12 22:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:16][root][INFO] - Training Epoch: 1/2, step 364/574 completed (loss: 0.39705896377563477, acc: 0.8536585569381714)
[2025-02-12 22:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:16][root][INFO] - Training Epoch: 1/2, step 365/574 completed (loss: 0.3324816823005676, acc: 0.9090909361839294)
[2025-02-12 22:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:16][root][INFO] - Training Epoch: 1/2, step 366/574 completed (loss: 0.14668315649032593, acc: 0.9583333134651184)
[2025-02-12 22:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:17][root][INFO] - Training Epoch: 1/2, step 367/574 completed (loss: 0.40847164392471313, acc: 0.8695651888847351)
[2025-02-12 22:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:17][root][INFO] - Training Epoch: 1/2, step 368/574 completed (loss: 0.2961435317993164, acc: 0.9285714030265808)
[2025-02-12 22:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:17][root][INFO] - Training Epoch: 1/2, step 369/574 completed (loss: 0.5092595219612122, acc: 0.78125)
[2025-02-12 22:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:18][root][INFO] - Training Epoch: 1/2, step 370/574 completed (loss: 0.7358112335205078, acc: 0.7757575511932373)
[2025-02-12 22:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:19][root][INFO] - Training Epoch: 1/2, step 371/574 completed (loss: 0.5949068069458008, acc: 0.849056601524353)
[2025-02-12 22:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:19][root][INFO] - Training Epoch: 1/2, step 372/574 completed (loss: 0.3865479826927185, acc: 0.8999999761581421)
[2025-02-12 22:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:20][root][INFO] - Training Epoch: 1/2, step 373/574 completed (loss: 0.5351743698120117, acc: 0.9107142686843872)
[2025-02-12 22:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:20][root][INFO] - Training Epoch: 1/2, step 374/574 completed (loss: 0.33163005113601685, acc: 0.9142857193946838)
[2025-02-12 22:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:20][root][INFO] - Training Epoch: 1/2, step 375/574 completed (loss: 0.0149117112159729, acc: 1.0)
[2025-02-12 22:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:21][root][INFO] - Training Epoch: 1/2, step 376/574 completed (loss: 0.21681445837020874, acc: 0.8695651888847351)
[2025-02-12 22:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:21][root][INFO] - Training Epoch: 1/2, step 377/574 completed (loss: 0.3319031894207001, acc: 0.9375)
[2025-02-12 22:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:21][root][INFO] - Training Epoch: 1/2, step 378/574 completed (loss: 0.42814308404922485, acc: 0.9157894849777222)
[2025-02-12 22:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:22][root][INFO] - Training Epoch: 1/2, step 379/574 completed (loss: 0.5299251079559326, acc: 0.8502994179725647)
[2025-02-12 22:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:23][root][INFO] - Training Epoch: 1/2, step 380/574 completed (loss: 0.5957359075546265, acc: 0.8270676732063293)
[2025-02-12 22:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:24][root][INFO] - Training Epoch: 1/2, step 381/574 completed (loss: 0.8460173606872559, acc: 0.7486631274223328)
[2025-02-12 22:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:24][root][INFO] - Training Epoch: 1/2, step 382/574 completed (loss: 0.39624452590942383, acc: 0.8918918967247009)
[2025-02-12 22:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:25][root][INFO] - Training Epoch: 1/2, step 383/574 completed (loss: 0.5803336501121521, acc: 0.8928571343421936)
[2025-02-12 22:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:25][root][INFO] - Training Epoch: 1/2, step 384/574 completed (loss: 0.13756978511810303, acc: 0.9285714030265808)
[2025-02-12 22:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:25][root][INFO] - Training Epoch: 1/2, step 385/574 completed (loss: 0.3500874638557434, acc: 0.90625)
[2025-02-12 22:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:26][root][INFO] - Training Epoch: 1/2, step 386/574 completed (loss: 0.09885946661233902, acc: 0.9444444179534912)
[2025-02-12 22:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:26][root][INFO] - Training Epoch: 1/2, step 387/574 completed (loss: 0.11075720191001892, acc: 0.9736841917037964)
[2025-02-12 22:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:26][root][INFO] - Training Epoch: 1/2, step 388/574 completed (loss: 0.05630826577544212, acc: 1.0)
[2025-02-12 22:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:27][root][INFO] - Training Epoch: 1/2, step 389/574 completed (loss: 0.019695723429322243, acc: 1.0)
[2025-02-12 22:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:27][root][INFO] - Training Epoch: 1/2, step 390/574 completed (loss: 0.7137486934661865, acc: 0.8095238208770752)
[2025-02-12 22:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:27][root][INFO] - Training Epoch: 1/2, step 391/574 completed (loss: 1.3920313119888306, acc: 0.6481481194496155)
[2025-02-12 22:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:28][root][INFO] - Training Epoch: 1/2, step 392/574 completed (loss: 1.224128246307373, acc: 0.6990291476249695)
[2025-02-12 22:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:28][root][INFO] - Training Epoch: 1/2, step 393/574 completed (loss: 1.2895818948745728, acc: 0.6911764740943909)
[2025-02-12 22:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:29][root][INFO] - Training Epoch: 1/2, step 394/574 completed (loss: 1.1536788940429688, acc: 0.7066666483879089)
[2025-02-12 22:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:29][root][INFO] - Training Epoch: 1/2, step 395/574 completed (loss: 1.0866389274597168, acc: 0.7083333134651184)
[2025-02-12 22:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:29][root][INFO] - Training Epoch: 1/2, step 396/574 completed (loss: 1.2088638544082642, acc: 0.6744186282157898)
[2025-02-12 22:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:30][root][INFO] - Training Epoch: 1/2, step 397/574 completed (loss: 0.6142895221710205, acc: 0.8333333134651184)
[2025-02-12 22:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:30][root][INFO] - Training Epoch: 1/2, step 398/574 completed (loss: 0.6535062789916992, acc: 0.8372092843055725)
[2025-02-12 22:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:30][root][INFO] - Training Epoch: 1/2, step 399/574 completed (loss: 0.35584911704063416, acc: 0.8799999952316284)
[2025-02-12 22:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:31][root][INFO] - Training Epoch: 1/2, step 400/574 completed (loss: 0.7198086977005005, acc: 0.8088235259056091)
[2025-02-12 22:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:31][root][INFO] - Training Epoch: 1/2, step 401/574 completed (loss: 0.8326534032821655, acc: 0.7733333110809326)
[2025-02-12 22:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:32][root][INFO] - Training Epoch: 1/2, step 402/574 completed (loss: 0.6663604378700256, acc: 0.8787878751754761)
[2025-02-12 22:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:32][root][INFO] - Training Epoch: 1/2, step 403/574 completed (loss: 0.899806022644043, acc: 0.7575757503509521)
[2025-02-12 22:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:32][root][INFO] - Training Epoch: 1/2, step 404/574 completed (loss: 0.35270053148269653, acc: 0.8709677457809448)
[2025-02-12 22:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:33][root][INFO] - Training Epoch: 1/2, step 405/574 completed (loss: 0.31052806973457336, acc: 0.8888888955116272)
[2025-02-12 22:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:33][root][INFO] - Training Epoch: 1/2, step 406/574 completed (loss: 0.27266842126846313, acc: 0.8799999952316284)
[2025-02-12 22:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:33][root][INFO] - Training Epoch: 1/2, step 407/574 completed (loss: 0.22492755949497223, acc: 0.9166666865348816)
[2025-02-12 22:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:34][root][INFO] - Training Epoch: 1/2, step 408/574 completed (loss: 0.2566187381744385, acc: 0.9629629850387573)
[2025-02-12 22:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:34][root][INFO] - Training Epoch: 1/2, step 409/574 completed (loss: 0.20181706547737122, acc: 0.9615384340286255)
[2025-02-12 22:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:35][root][INFO] - Training Epoch: 1/2, step 410/574 completed (loss: 0.41948896646499634, acc: 0.8965517282485962)
[2025-02-12 22:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:35][root][INFO] - Training Epoch: 1/2, step 411/574 completed (loss: 0.18074524402618408, acc: 0.9285714030265808)
[2025-02-12 22:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:35][root][INFO] - Training Epoch: 1/2, step 412/574 completed (loss: 0.3296070098876953, acc: 0.9333333373069763)
[2025-02-12 22:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:36][root][INFO] - Training Epoch: 1/2, step 413/574 completed (loss: 0.5829571485519409, acc: 0.8484848737716675)
[2025-02-12 22:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:36][root][INFO] - Training Epoch: 1/2, step 414/574 completed (loss: 0.17943765223026276, acc: 0.9545454382896423)
[2025-02-12 22:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:36][root][INFO] - Training Epoch: 1/2, step 415/574 completed (loss: 0.4384784996509552, acc: 0.8823529481887817)
[2025-02-12 22:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:37][root][INFO] - Training Epoch: 1/2, step 416/574 completed (loss: 0.4412342309951782, acc: 0.8461538553237915)
[2025-02-12 22:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:37][root][INFO] - Training Epoch: 1/2, step 417/574 completed (loss: 0.5084366202354431, acc: 0.7777777910232544)
[2025-02-12 22:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:37][root][INFO] - Training Epoch: 1/2, step 418/574 completed (loss: 0.40386277437210083, acc: 0.875)
[2025-02-12 22:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:38][root][INFO] - Training Epoch: 1/2, step 419/574 completed (loss: 0.7084393501281738, acc: 0.8999999761581421)
[2025-02-12 22:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:38][root][INFO] - Training Epoch: 1/2, step 420/574 completed (loss: 0.31121471524238586, acc: 0.8571428656578064)
[2025-02-12 22:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:38][root][INFO] - Training Epoch: 1/2, step 421/574 completed (loss: 0.5559085607528687, acc: 0.7666666507720947)
[2025-02-12 22:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:39][root][INFO] - Training Epoch: 1/2, step 422/574 completed (loss: 0.5192359685897827, acc: 0.875)
[2025-02-12 22:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:39][root][INFO] - Training Epoch: 1/2, step 423/574 completed (loss: 0.8973360061645508, acc: 0.6944444179534912)
[2025-02-12 22:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:39][root][INFO] - Training Epoch: 1/2, step 424/574 completed (loss: 0.7189182043075562, acc: 0.8518518805503845)
[2025-02-12 22:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:40][root][INFO] - Training Epoch: 1/2, step 425/574 completed (loss: 0.32069671154022217, acc: 0.9090909361839294)
[2025-02-12 22:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:40][root][INFO] - Training Epoch: 1/2, step 426/574 completed (loss: 0.14609383046627045, acc: 0.95652174949646)
[2025-02-12 22:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:40][root][INFO] - Training Epoch: 1/2, step 427/574 completed (loss: 0.24390754103660583, acc: 0.9729729890823364)
[2025-02-12 22:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:41][root][INFO] - Training Epoch: 1/2, step 428/574 completed (loss: 0.3657982349395752, acc: 0.9629629850387573)
[2025-02-12 22:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:12][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8494, device='cuda:0') eval_epoch_loss=tensor(0.6148, device='cuda:0') eval_epoch_acc=tensor(0.8306, device='cuda:0')
[2025-02-12 22:02:12][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:02:12][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:02:13][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_429_loss_0.6148401498794556/model.pt
[2025-02-12 22:02:13][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:02:13][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6148401498794556
[2025-02-12 22:02:13][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8306153416633606
[2025-02-12 22:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:13][root][INFO] - Training Epoch: 1/2, step 429/574 completed (loss: 0.23397420346736908, acc: 0.8695651888847351)
[2025-02-12 22:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:13][root][INFO] - Training Epoch: 1/2, step 430/574 completed (loss: 0.03239666670560837, acc: 1.0)
[2025-02-12 22:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:14][root][INFO] - Training Epoch: 1/2, step 431/574 completed (loss: 0.0246809720993042, acc: 1.0)
[2025-02-12 22:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:14][root][INFO] - Training Epoch: 1/2, step 432/574 completed (loss: 0.6937286257743835, acc: 0.782608687877655)
[2025-02-12 22:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:14][root][INFO] - Training Epoch: 1/2, step 433/574 completed (loss: 0.2967345416545868, acc: 0.8888888955116272)
[2025-02-12 22:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:15][root][INFO] - Training Epoch: 1/2, step 434/574 completed (loss: 0.019309919327497482, acc: 1.0)
[2025-02-12 22:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:15][root][INFO] - Training Epoch: 1/2, step 435/574 completed (loss: 0.0923340767621994, acc: 0.939393937587738)
[2025-02-12 22:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:15][root][INFO] - Training Epoch: 1/2, step 436/574 completed (loss: 0.3907986283302307, acc: 0.8888888955116272)
[2025-02-12 22:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:16][root][INFO] - Training Epoch: 1/2, step 437/574 completed (loss: 0.15792495012283325, acc: 0.9545454382896423)
[2025-02-12 22:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:16][root][INFO] - Training Epoch: 1/2, step 438/574 completed (loss: 0.10650257766246796, acc: 0.9523809552192688)
[2025-02-12 22:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:16][root][INFO] - Training Epoch: 1/2, step 439/574 completed (loss: 0.8275112509727478, acc: 0.8461538553237915)
[2025-02-12 22:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:17][root][INFO] - Training Epoch: 1/2, step 440/574 completed (loss: 0.8130712509155273, acc: 0.8181818127632141)
[2025-02-12 22:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:18][root][INFO] - Training Epoch: 1/2, step 441/574 completed (loss: 1.0663775205612183, acc: 0.6959999799728394)
[2025-02-12 22:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:18][root][INFO] - Training Epoch: 1/2, step 442/574 completed (loss: 0.9360929727554321, acc: 0.7903226017951965)
[2025-02-12 22:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:19][root][INFO] - Training Epoch: 1/2, step 443/574 completed (loss: 0.7041724920272827, acc: 0.8159204125404358)
[2025-02-12 22:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:19][root][INFO] - Training Epoch: 1/2, step 444/574 completed (loss: 0.21374763548374176, acc: 0.9245283007621765)
[2025-02-12 22:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:20][root][INFO] - Training Epoch: 1/2, step 445/574 completed (loss: 0.23635055124759674, acc: 0.9318181872367859)
[2025-02-12 22:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:20][root][INFO] - Training Epoch: 1/2, step 446/574 completed (loss: 0.39919450879096985, acc: 0.9130434989929199)
[2025-02-12 22:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:20][root][INFO] - Training Epoch: 1/2, step 447/574 completed (loss: 0.7246567010879517, acc: 0.807692289352417)
[2025-02-12 22:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:21][root][INFO] - Training Epoch: 1/2, step 448/574 completed (loss: 0.47080299258232117, acc: 0.8571428656578064)
[2025-02-12 22:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:21][root][INFO] - Training Epoch: 1/2, step 449/574 completed (loss: 0.3923572599887848, acc: 0.9104477763175964)
[2025-02-12 22:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:21][root][INFO] - Training Epoch: 1/2, step 450/574 completed (loss: 0.25193819403648376, acc: 0.9305555820465088)
[2025-02-12 22:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:22][root][INFO] - Training Epoch: 1/2, step 451/574 completed (loss: 0.17494754493236542, acc: 0.95652174949646)
[2025-02-12 22:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:22][root][INFO] - Training Epoch: 1/2, step 452/574 completed (loss: 0.3491184115409851, acc: 0.9102563858032227)
[2025-02-12 22:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:22][root][INFO] - Training Epoch: 1/2, step 453/574 completed (loss: 0.5054208636283875, acc: 0.8684210777282715)
[2025-02-12 22:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:23][root][INFO] - Training Epoch: 1/2, step 454/574 completed (loss: 0.48476994037628174, acc: 0.918367326259613)
[2025-02-12 22:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:23][root][INFO] - Training Epoch: 1/2, step 455/574 completed (loss: 0.32768237590789795, acc: 0.9090909361839294)
[2025-02-12 22:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:23][root][INFO] - Training Epoch: 1/2, step 456/574 completed (loss: 0.8203343152999878, acc: 0.7938144207000732)
[2025-02-12 22:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:24][root][INFO] - Training Epoch: 1/2, step 457/574 completed (loss: 0.11099427193403244, acc: 0.9714285731315613)
[2025-02-12 22:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:24][root][INFO] - Training Epoch: 1/2, step 458/574 completed (loss: 0.5836085677146912, acc: 0.8604651093482971)
[2025-02-12 22:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:25][root][INFO] - Training Epoch: 1/2, step 459/574 completed (loss: 0.09067480266094208, acc: 0.9821428656578064)
[2025-02-12 22:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:25][root][INFO] - Training Epoch: 1/2, step 460/574 completed (loss: 0.2928900420665741, acc: 0.9135802388191223)
[2025-02-12 22:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:25][root][INFO] - Training Epoch: 1/2, step 461/574 completed (loss: 0.7250862121582031, acc: 0.8333333134651184)
[2025-02-12 22:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:26][root][INFO] - Training Epoch: 1/2, step 462/574 completed (loss: 0.20162276923656464, acc: 0.90625)
[2025-02-12 22:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:26][root][INFO] - Training Epoch: 1/2, step 463/574 completed (loss: 0.6230663061141968, acc: 0.8846153616905212)
[2025-02-12 22:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:26][root][INFO] - Training Epoch: 1/2, step 464/574 completed (loss: 0.34417465329170227, acc: 0.8913043737411499)
[2025-02-12 22:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:27][root][INFO] - Training Epoch: 1/2, step 465/574 completed (loss: 0.6045010685920715, acc: 0.7857142686843872)
[2025-02-12 22:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:27][root][INFO] - Training Epoch: 1/2, step 466/574 completed (loss: 0.7032986879348755, acc: 0.8554216623306274)
[2025-02-12 22:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:27][root][INFO] - Training Epoch: 1/2, step 467/574 completed (loss: 0.5460251569747925, acc: 0.837837815284729)
[2025-02-12 22:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:28][root][INFO] - Training Epoch: 1/2, step 468/574 completed (loss: 0.7755334377288818, acc: 0.8252426981925964)
[2025-02-12 22:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:28][root][INFO] - Training Epoch: 1/2, step 469/574 completed (loss: 0.6231670379638672, acc: 0.8536585569381714)
[2025-02-12 22:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:28][root][INFO] - Training Epoch: 1/2, step 470/574 completed (loss: 0.6324350237846375, acc: 0.875)
[2025-02-12 22:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:29][root][INFO] - Training Epoch: 1/2, step 471/574 completed (loss: 0.8765748739242554, acc: 0.75)
[2025-02-12 22:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:29][root][INFO] - Training Epoch: 1/2, step 472/574 completed (loss: 1.1416577100753784, acc: 0.6960784196853638)
[2025-02-12 22:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:29][root][INFO] - Training Epoch: 1/2, step 473/574 completed (loss: 0.9121872782707214, acc: 0.7467249035835266)
[2025-02-12 22:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:30][root][INFO] - Training Epoch: 1/2, step 474/574 completed (loss: 0.9763216972351074, acc: 0.71875)
[2025-02-12 22:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:30][root][INFO] - Training Epoch: 1/2, step 475/574 completed (loss: 0.544738233089447, acc: 0.8343558311462402)
[2025-02-12 22:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:30][root][INFO] - Training Epoch: 1/2, step 476/574 completed (loss: 0.6898936033248901, acc: 0.7913669347763062)
[2025-02-12 22:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:31][root][INFO] - Training Epoch: 1/2, step 477/574 completed (loss: 1.0856657028198242, acc: 0.7085427045822144)
[2025-02-12 22:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:31][root][INFO] - Training Epoch: 1/2, step 478/574 completed (loss: 0.9581884741783142, acc: 0.75)
[2025-02-12 22:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:31][root][INFO] - Training Epoch: 1/2, step 479/574 completed (loss: 1.0624696016311646, acc: 0.7272727489471436)
[2025-02-12 22:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:32][root][INFO] - Training Epoch: 1/2, step 480/574 completed (loss: 1.0824291706085205, acc: 0.7777777910232544)
[2025-02-12 22:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:32][root][INFO] - Training Epoch: 1/2, step 481/574 completed (loss: 0.9570596814155579, acc: 0.75)
[2025-02-12 22:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:32][root][INFO] - Training Epoch: 1/2, step 482/574 completed (loss: 1.1919349431991577, acc: 0.6499999761581421)
[2025-02-12 22:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:33][root][INFO] - Training Epoch: 1/2, step 483/574 completed (loss: 1.132810354232788, acc: 0.7241379022598267)
[2025-02-12 22:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:33][root][INFO] - Training Epoch: 1/2, step 484/574 completed (loss: 0.3342338800430298, acc: 0.9354838728904724)
[2025-02-12 22:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:34][root][INFO] - Training Epoch: 1/2, step 485/574 completed (loss: 0.7845851182937622, acc: 0.7368420958518982)
[2025-02-12 22:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:34][root][INFO] - Training Epoch: 1/2, step 486/574 completed (loss: 1.3882087469100952, acc: 0.5925925970077515)
[2025-02-12 22:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:34][root][INFO] - Training Epoch: 1/2, step 487/574 completed (loss: 0.7367419004440308, acc: 0.8095238208770752)
[2025-02-12 22:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:35][root][INFO] - Training Epoch: 1/2, step 488/574 completed (loss: 0.8589931726455688, acc: 0.8636363744735718)
[2025-02-12 22:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:35][root][INFO] - Training Epoch: 1/2, step 489/574 completed (loss: 1.134720802307129, acc: 0.6769230961799622)
[2025-02-12 22:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:35][root][INFO] - Training Epoch: 1/2, step 490/574 completed (loss: 0.5097893476486206, acc: 0.8666666746139526)
[2025-02-12 22:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:36][root][INFO] - Training Epoch: 1/2, step 491/574 completed (loss: 0.8545566201210022, acc: 0.7586206793785095)
[2025-02-12 22:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:36][root][INFO] - Training Epoch: 1/2, step 492/574 completed (loss: 0.6422857642173767, acc: 0.7843137383460999)
[2025-02-12 22:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:36][root][INFO] - Training Epoch: 1/2, step 493/574 completed (loss: 0.5139840841293335, acc: 0.8620689511299133)
[2025-02-12 22:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:37][root][INFO] - Training Epoch: 1/2, step 494/574 completed (loss: 0.4817551076412201, acc: 0.8421052694320679)
[2025-02-12 22:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:37][root][INFO] - Training Epoch: 1/2, step 495/574 completed (loss: 1.1419587135314941, acc: 0.7894737124443054)
[2025-02-12 22:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:37][root][INFO] - Training Epoch: 1/2, step 496/574 completed (loss: 0.7847212553024292, acc: 0.7410714030265808)
[2025-02-12 22:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:38][root][INFO] - Training Epoch: 1/2, step 497/574 completed (loss: 0.6229108572006226, acc: 0.8202247023582458)
[2025-02-12 22:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:38][root][INFO] - Training Epoch: 1/2, step 498/574 completed (loss: 0.9209917783737183, acc: 0.7191011309623718)
[2025-02-12 22:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:39][root][INFO] - Training Epoch: 1/2, step 499/574 completed (loss: 1.3222332000732422, acc: 0.588652491569519)
[2025-02-12 22:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:39][root][INFO] - Training Epoch: 1/2, step 500/574 completed (loss: 0.8978514671325684, acc: 0.782608687877655)
[2025-02-12 22:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:39][root][INFO] - Training Epoch: 1/2, step 501/574 completed (loss: 0.08962259441614151, acc: 1.0)
[2025-02-12 22:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:40][root][INFO] - Training Epoch: 1/2, step 502/574 completed (loss: 0.24090997874736786, acc: 0.9615384340286255)
[2025-02-12 22:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:40][root][INFO] - Training Epoch: 1/2, step 503/574 completed (loss: 0.23289383947849274, acc: 0.9629629850387573)
[2025-02-12 22:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:41][root][INFO] - Training Epoch: 1/2, step 504/574 completed (loss: 0.4153297245502472, acc: 0.8518518805503845)
[2025-02-12 22:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:41][root][INFO] - Training Epoch: 1/2, step 505/574 completed (loss: 0.6354835629463196, acc: 0.8679245114326477)
[2025-02-12 22:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:41][root][INFO] - Training Epoch: 1/2, step 506/574 completed (loss: 0.8836973905563354, acc: 0.7241379022598267)
[2025-02-12 22:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:42][root][INFO] - Training Epoch: 1/2, step 507/574 completed (loss: 1.2223364114761353, acc: 0.630630612373352)
[2025-02-12 22:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:42][root][INFO] - Training Epoch: 1/2, step 508/574 completed (loss: 0.7583886981010437, acc: 0.8028169274330139)
[2025-02-12 22:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:43][root][INFO] - Training Epoch: 1/2, step 509/574 completed (loss: 0.15341563522815704, acc: 0.949999988079071)
[2025-02-12 22:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:43][root][INFO] - Training Epoch: 1/2, step 510/574 completed (loss: 0.4311927258968353, acc: 0.9333333373069763)
[2025-02-12 22:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:43][root][INFO] - Training Epoch: 1/2, step 511/574 completed (loss: 0.9533984661102295, acc: 0.7307692170143127)
[2025-02-12 22:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:46][root][INFO] - Training Epoch: 1/2, step 512/574 completed (loss: 1.2077388763427734, acc: 0.6571428775787354)
[2025-02-12 22:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:47][root][INFO] - Training Epoch: 1/2, step 513/574 completed (loss: 0.42071035504341125, acc: 0.8730158805847168)
[2025-02-12 22:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:47][root][INFO] - Training Epoch: 1/2, step 514/574 completed (loss: 0.6845826506614685, acc: 0.8214285969734192)
[2025-02-12 22:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:47][root][INFO] - Training Epoch: 1/2, step 515/574 completed (loss: 0.2544923424720764, acc: 0.8999999761581421)
[2025-02-12 22:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:48][root][INFO] - Training Epoch: 1/2, step 516/574 completed (loss: 0.7815446853637695, acc: 0.7777777910232544)
[2025-02-12 22:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:48][root][INFO] - Training Epoch: 1/2, step 517/574 completed (loss: 0.060841117054224014, acc: 1.0)
[2025-02-12 22:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:49][root][INFO] - Training Epoch: 1/2, step 518/574 completed (loss: 0.20649948716163635, acc: 0.9354838728904724)
[2025-02-12 22:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:49][root][INFO] - Training Epoch: 1/2, step 519/574 completed (loss: 0.5166887640953064, acc: 0.8500000238418579)
[2025-02-12 22:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:49][root][INFO] - Training Epoch: 1/2, step 520/574 completed (loss: 0.652916431427002, acc: 0.8518518805503845)
[2025-02-12 22:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:50][root][INFO] - Training Epoch: 1/2, step 521/574 completed (loss: 0.8395906090736389, acc: 0.7372881174087524)
[2025-02-12 22:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:51][root][INFO] - Training Epoch: 1/2, step 522/574 completed (loss: 0.5060369372367859, acc: 0.8656716346740723)
[2025-02-12 22:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:51][root][INFO] - Training Epoch: 1/2, step 523/574 completed (loss: 0.5630235075950623, acc: 0.8248175382614136)
[2025-02-12 22:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:52][root][INFO] - Training Epoch: 1/2, step 524/574 completed (loss: 0.7908744812011719, acc: 0.7699999809265137)
[2025-02-12 22:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:52][root][INFO] - Training Epoch: 1/2, step 525/574 completed (loss: 0.15848636627197266, acc: 0.9259259104728699)
[2025-02-12 22:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:53][root][INFO] - Training Epoch: 1/2, step 526/574 completed (loss: 0.41438788175582886, acc: 0.8846153616905212)
[2025-02-12 22:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:53][root][INFO] - Training Epoch: 1/2, step 527/574 completed (loss: 0.7318476438522339, acc: 0.761904776096344)
[2025-02-12 22:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:53][root][INFO] - Training Epoch: 1/2, step 528/574 completed (loss: 1.8274356126785278, acc: 0.5245901346206665)
[2025-02-12 22:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:54][root][INFO] - Training Epoch: 1/2, step 529/574 completed (loss: 0.47376397252082825, acc: 0.8813559412956238)
[2025-02-12 22:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:54][root][INFO] - Training Epoch: 1/2, step 530/574 completed (loss: 1.2122288942337036, acc: 0.6279069781303406)
[2025-02-12 22:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:54][root][INFO] - Training Epoch: 1/2, step 531/574 completed (loss: 1.109649896621704, acc: 0.7727272510528564)
[2025-02-12 22:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:55][root][INFO] - Training Epoch: 1/2, step 532/574 completed (loss: 1.1964287757873535, acc: 0.698113203048706)
[2025-02-12 22:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:55][root][INFO] - Training Epoch: 1/2, step 533/574 completed (loss: 0.868507444858551, acc: 0.7954545617103577)
[2025-02-12 22:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:56][root][INFO] - Training Epoch: 1/2, step 534/574 completed (loss: 0.9475024342536926, acc: 0.7200000286102295)
[2025-02-12 22:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:56][root][INFO] - Training Epoch: 1/2, step 535/574 completed (loss: 0.6982576251029968, acc: 0.8500000238418579)
[2025-02-12 22:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:56][root][INFO] - Training Epoch: 1/2, step 536/574 completed (loss: 0.3875473737716675, acc: 0.8636363744735718)
[2025-02-12 22:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:57][root][INFO] - Training Epoch: 1/2, step 537/574 completed (loss: 0.7763177156448364, acc: 0.8153846263885498)
[2025-02-12 22:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:57][root][INFO] - Training Epoch: 1/2, step 538/574 completed (loss: 0.4783547818660736, acc: 0.84375)
[2025-02-12 22:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:57][root][INFO] - Training Epoch: 1/2, step 539/574 completed (loss: 0.8550350666046143, acc: 0.71875)
[2025-02-12 22:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:58][root][INFO] - Training Epoch: 1/2, step 540/574 completed (loss: 0.5368919968605042, acc: 0.8484848737716675)
[2025-02-12 22:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:58][root][INFO] - Training Epoch: 1/2, step 541/574 completed (loss: 0.4483359754085541, acc: 0.8125)
[2025-02-12 22:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:58][root][INFO] - Training Epoch: 1/2, step 542/574 completed (loss: 0.09377630054950714, acc: 0.9677419066429138)
[2025-02-12 22:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:59][root][INFO] - Training Epoch: 1/2, step 543/574 completed (loss: 0.14539583027362823, acc: 0.95652174949646)
[2025-02-12 22:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:59][root][INFO] - Training Epoch: 1/2, step 544/574 completed (loss: 0.2565625011920929, acc: 0.9666666388511658)
[2025-02-12 22:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:02:59][root][INFO] - Training Epoch: 1/2, step 545/574 completed (loss: 0.22169922292232513, acc: 0.9756097793579102)
[2025-02-12 22:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:00][root][INFO] - Training Epoch: 1/2, step 546/574 completed (loss: 0.06070699542760849, acc: 0.9714285731315613)
[2025-02-12 22:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:00][root][INFO] - Training Epoch: 1/2, step 547/574 completed (loss: 0.16215476393699646, acc: 0.9210526347160339)
[2025-02-12 22:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:00][root][INFO] - Training Epoch: 1/2, step 548/574 completed (loss: 0.2567678391933441, acc: 0.8709677457809448)
[2025-02-12 22:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:01][root][INFO] - Training Epoch: 1/2, step 549/574 completed (loss: 0.046686865389347076, acc: 0.9599999785423279)
[2025-02-12 22:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:01][root][INFO] - Training Epoch: 1/2, step 550/574 completed (loss: 0.5690134763717651, acc: 0.8484848737716675)
[2025-02-12 22:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:01][root][INFO] - Training Epoch: 1/2, step 551/574 completed (loss: 0.3060899078845978, acc: 0.875)
[2025-02-12 22:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:02][root][INFO] - Training Epoch: 1/2, step 552/574 completed (loss: 0.41961702704429626, acc: 0.9142857193946838)
[2025-02-12 22:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:02][root][INFO] - Training Epoch: 1/2, step 553/574 completed (loss: 0.5561577081680298, acc: 0.8394160866737366)
[2025-02-12 22:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:02][root][INFO] - Training Epoch: 1/2, step 554/574 completed (loss: 0.36470845341682434, acc: 0.8965517282485962)
[2025-02-12 22:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:03][root][INFO] - Training Epoch: 1/2, step 555/574 completed (loss: 0.6284639239311218, acc: 0.8428571224212646)
[2025-02-12 22:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:03][root][INFO] - Training Epoch: 1/2, step 556/574 completed (loss: 0.6314440369606018, acc: 0.8476821184158325)
[2025-02-12 22:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:04][root][INFO] - Training Epoch: 1/2, step 557/574 completed (loss: 0.4968586266040802, acc: 0.8803418874740601)
[2025-02-12 22:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:04][root][INFO] - Training Epoch: 1/2, step 558/574 completed (loss: 0.2714514136314392, acc: 0.8399999737739563)
[2025-02-12 22:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:04][root][INFO] - Training Epoch: 1/2, step 559/574 completed (loss: 0.5512834787368774, acc: 0.807692289352417)
[2025-02-12 22:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:05][root][INFO] - Training Epoch: 1/2, step 560/574 completed (loss: 0.09431929141283035, acc: 0.9615384340286255)
[2025-02-12 22:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:05][root][INFO] - Training Epoch: 1/2, step 561/574 completed (loss: 0.11231961846351624, acc: 0.9743589758872986)
[2025-02-12 22:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:05][root][INFO] - Training Epoch: 1/2, step 562/574 completed (loss: 0.6521114706993103, acc: 0.8444444537162781)
[2025-02-12 22:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:06][root][INFO] - Training Epoch: 1/2, step 563/574 completed (loss: 0.430522084236145, acc: 0.8961039185523987)
[2025-02-12 22:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:06][root][INFO] - Training Epoch: 1/2, step 564/574 completed (loss: 0.4909926950931549, acc: 0.8333333134651184)
[2025-02-12 22:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:06][root][INFO] - Training Epoch: 1/2, step 565/574 completed (loss: 0.3591606020927429, acc: 0.8965517282485962)
[2025-02-12 22:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:07][root][INFO] - Training Epoch: 1/2, step 566/574 completed (loss: 0.5108937621116638, acc: 0.8571428656578064)
[2025-02-12 22:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:07][root][INFO] - Training Epoch: 1/2, step 567/574 completed (loss: 0.040711410343647, acc: 1.0)
[2025-02-12 22:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:07][root][INFO] - Training Epoch: 1/2, step 568/574 completed (loss: 0.18283461034297943, acc: 0.9259259104728699)
[2025-02-12 22:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:08][root][INFO] - Training Epoch: 1/2, step 569/574 completed (loss: 0.25892868638038635, acc: 0.9465240836143494)
[2025-02-12 22:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:08][root][INFO] - Training Epoch: 1/2, step 570/574 completed (loss: 0.10436107963323593, acc: 0.9838709831237793)
[2025-02-12 22:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:08][root][INFO] - Training Epoch: 1/2, step 571/574 completed (loss: 0.2992790937423706, acc: 0.94017094373703)
[2025-02-12 22:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:39][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7633, device='cuda:0') eval_epoch_loss=tensor(0.5672, device='cuda:0') eval_epoch_acc=tensor(0.8423, device='cuda:0')
[2025-02-12 22:03:39][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:03:39][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:03:39][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_572_loss_0.5671751499176025/model.pt
[2025-02-12 22:03:39][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:03:39][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.5671751499176025
[2025-02-12 22:03:39][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8423181176185608
[2025-02-12 22:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:39][root][INFO] - Training Epoch: 1/2, step 572/574 completed (loss: 0.3863694667816162, acc: 0.9234693646430969)
[2025-02-12 22:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:40][root][INFO] - Training Epoch: 1/2, step 573/574 completed (loss: 0.5074118971824646, acc: 0.8867924809455872)
[2025-02-12 22:03:40][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=2.7424, train_epoch_loss=1.0088, epoch time 366.4349821768701s
[2025-02-12 22:03:40][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-12 22:03:40][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-02-12 22:03:40][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-12 22:03:40][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2025-02-12 22:03:40][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-12 22:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:41][root][INFO] - Training Epoch: 2/2, step 0/574 completed (loss: 0.5465433597564697, acc: 0.7407407164573669)
[2025-02-12 22:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:41][root][INFO] - Training Epoch: 2/2, step 1/574 completed (loss: 0.6792596578598022, acc: 0.800000011920929)
[2025-02-12 22:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:42][root][INFO] - Training Epoch: 2/2, step 2/574 completed (loss: 1.1549640893936157, acc: 0.7567567825317383)
[2025-02-12 22:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:42][root][INFO] - Training Epoch: 2/2, step 3/574 completed (loss: 0.6035422682762146, acc: 0.8421052694320679)
[2025-02-12 22:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:42][root][INFO] - Training Epoch: 2/2, step 4/574 completed (loss: 0.8000547885894775, acc: 0.7837837934494019)
[2025-02-12 22:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:43][root][INFO] - Training Epoch: 2/2, step 5/574 completed (loss: 0.47732824087142944, acc: 0.8214285969734192)
[2025-02-12 22:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:43][root][INFO] - Training Epoch: 2/2, step 6/574 completed (loss: 0.9467757940292358, acc: 0.6734693646430969)
[2025-02-12 22:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:43][root][INFO] - Training Epoch: 2/2, step 7/574 completed (loss: 0.5691773891448975, acc: 0.8999999761581421)
[2025-02-12 22:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:44][root][INFO] - Training Epoch: 2/2, step 8/574 completed (loss: 0.20425182580947876, acc: 0.9090909361839294)
[2025-02-12 22:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:44][root][INFO] - Training Epoch: 2/2, step 9/574 completed (loss: 0.16357849538326263, acc: 0.9615384340286255)
[2025-02-12 22:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:45][root][INFO] - Training Epoch: 2/2, step 10/574 completed (loss: 0.3443399667739868, acc: 0.9259259104728699)
[2025-02-12 22:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:45][root][INFO] - Training Epoch: 2/2, step 11/574 completed (loss: 0.5344452261924744, acc: 0.8974359035491943)
[2025-02-12 22:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:45][root][INFO] - Training Epoch: 2/2, step 12/574 completed (loss: 0.170867457985878, acc: 0.939393937587738)
[2025-02-12 22:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:46][root][INFO] - Training Epoch: 2/2, step 13/574 completed (loss: 0.46315476298332214, acc: 0.8260869383811951)
[2025-02-12 22:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:46][root][INFO] - Training Epoch: 2/2, step 14/574 completed (loss: 0.1831027716398239, acc: 0.9803921580314636)
[2025-02-12 22:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:46][root][INFO] - Training Epoch: 2/2, step 15/574 completed (loss: 0.4647703766822815, acc: 0.8979591727256775)
[2025-02-12 22:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:47][root][INFO] - Training Epoch: 2/2, step 16/574 completed (loss: 0.193660706281662, acc: 0.9473684430122375)
[2025-02-12 22:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:47][root][INFO] - Training Epoch: 2/2, step 17/574 completed (loss: 0.45656442642211914, acc: 0.8333333134651184)
[2025-02-12 22:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:47][root][INFO] - Training Epoch: 2/2, step 18/574 completed (loss: 0.615546464920044, acc: 0.8333333134651184)
[2025-02-12 22:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:48][root][INFO] - Training Epoch: 2/2, step 19/574 completed (loss: 0.2737436294555664, acc: 0.8421052694320679)
[2025-02-12 22:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:48][root][INFO] - Training Epoch: 2/2, step 20/574 completed (loss: 0.40601032972335815, acc: 0.9230769276618958)
[2025-02-12 22:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:48][root][INFO] - Training Epoch: 2/2, step 21/574 completed (loss: 0.9861095547676086, acc: 0.8275862336158752)
[2025-02-12 22:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:49][root][INFO] - Training Epoch: 2/2, step 22/574 completed (loss: 0.9260133504867554, acc: 0.7599999904632568)
[2025-02-12 22:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:49][root][INFO] - Training Epoch: 2/2, step 23/574 completed (loss: 1.0607324838638306, acc: 0.8095238208770752)
[2025-02-12 22:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:50][root][INFO] - Training Epoch: 2/2, step 24/574 completed (loss: 0.4976210296154022, acc: 0.8125)
[2025-02-12 22:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:50][root][INFO] - Training Epoch: 2/2, step 25/574 completed (loss: 0.5943254232406616, acc: 0.849056601524353)
[2025-02-12 22:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:50][root][INFO] - Training Epoch: 2/2, step 26/574 completed (loss: 0.90370112657547, acc: 0.698630154132843)
[2025-02-12 22:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:51][root][INFO] - Training Epoch: 2/2, step 27/574 completed (loss: 0.80866938829422, acc: 0.7865612506866455)
[2025-02-12 22:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:52][root][INFO] - Training Epoch: 2/2, step 28/574 completed (loss: 0.5089709162712097, acc: 0.8372092843055725)
[2025-02-12 22:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:52][root][INFO] - Training Epoch: 2/2, step 29/574 completed (loss: 0.7662896513938904, acc: 0.8433734774589539)
[2025-02-12 22:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:52][root][INFO] - Training Epoch: 2/2, step 30/574 completed (loss: 0.6531871557235718, acc: 0.8148148059844971)
[2025-02-12 22:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:52][root][INFO] - Training Epoch: 2/2, step 31/574 completed (loss: 0.7676650881767273, acc: 0.7857142686843872)
[2025-02-12 22:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:53][root][INFO] - Training Epoch: 2/2, step 32/574 completed (loss: 0.6048592925071716, acc: 0.8148148059844971)
[2025-02-12 22:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:53][root][INFO] - Training Epoch: 2/2, step 33/574 completed (loss: 0.18491126596927643, acc: 0.95652174949646)
[2025-02-12 22:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:54][root][INFO] - Training Epoch: 2/2, step 34/574 completed (loss: 0.6591989398002625, acc: 0.7815126180648804)
[2025-02-12 22:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:54][root][INFO] - Training Epoch: 2/2, step 35/574 completed (loss: 0.5056885480880737, acc: 0.8524590134620667)
[2025-02-12 22:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:54][root][INFO] - Training Epoch: 2/2, step 36/574 completed (loss: 0.5710830688476562, acc: 0.8253968358039856)
[2025-02-12 22:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:55][root][INFO] - Training Epoch: 2/2, step 37/574 completed (loss: 0.5887320637702942, acc: 0.8813559412956238)
[2025-02-12 22:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:55][root][INFO] - Training Epoch: 2/2, step 38/574 completed (loss: 0.4081082046031952, acc: 0.8850574493408203)
[2025-02-12 22:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:55][root][INFO] - Training Epoch: 2/2, step 39/574 completed (loss: 0.18911926448345184, acc: 1.0)
[2025-02-12 22:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:56][root][INFO] - Training Epoch: 2/2, step 40/574 completed (loss: 0.48609086871147156, acc: 0.807692289352417)
[2025-02-12 22:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:56][root][INFO] - Training Epoch: 2/2, step 41/574 completed (loss: 0.41219228506088257, acc: 0.8783783912658691)
[2025-02-12 22:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:57][root][INFO] - Training Epoch: 2/2, step 42/574 completed (loss: 0.5080272555351257, acc: 0.8153846263885498)
[2025-02-12 22:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:57][root][INFO] - Training Epoch: 2/2, step 43/574 completed (loss: 0.7690302729606628, acc: 0.808080792427063)
[2025-02-12 22:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:57][root][INFO] - Training Epoch: 2/2, step 44/574 completed (loss: 0.3815346658229828, acc: 0.876288652420044)
[2025-02-12 22:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:58][root][INFO] - Training Epoch: 2/2, step 45/574 completed (loss: 0.5209115147590637, acc: 0.875)
[2025-02-12 22:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:58][root][INFO] - Training Epoch: 2/2, step 46/574 completed (loss: 0.3543085753917694, acc: 0.9230769276618958)
[2025-02-12 22:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:59][root][INFO] - Training Epoch: 2/2, step 47/574 completed (loss: 0.26983025670051575, acc: 0.9629629850387573)
[2025-02-12 22:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:59][root][INFO] - Training Epoch: 2/2, step 48/574 completed (loss: 0.43704816699028015, acc: 0.9285714030265808)
[2025-02-12 22:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:03:59][root][INFO] - Training Epoch: 2/2, step 49/574 completed (loss: 0.07498225569725037, acc: 1.0)
[2025-02-12 22:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:00][root][INFO] - Training Epoch: 2/2, step 50/574 completed (loss: 0.7444141507148743, acc: 0.7719298005104065)
[2025-02-12 22:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:00][root][INFO] - Training Epoch: 2/2, step 51/574 completed (loss: 0.640895426273346, acc: 0.7777777910232544)
[2025-02-12 22:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:00][root][INFO] - Training Epoch: 2/2, step 52/574 completed (loss: 1.2396814823150635, acc: 0.6901408433914185)
[2025-02-12 22:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:01][root][INFO] - Training Epoch: 2/2, step 53/574 completed (loss: 1.5503400564193726, acc: 0.5666666626930237)
[2025-02-12 22:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:01][root][INFO] - Training Epoch: 2/2, step 54/574 completed (loss: 1.2215980291366577, acc: 0.7027027010917664)
[2025-02-12 22:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:01][root][INFO] - Training Epoch: 2/2, step 55/574 completed (loss: 0.18316584825515747, acc: 0.9615384340286255)
[2025-02-12 22:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:04][root][INFO] - Training Epoch: 2/2, step 56/574 completed (loss: 1.034524917602539, acc: 0.6928327679634094)
[2025-02-12 22:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:05][root][INFO] - Training Epoch: 2/2, step 57/574 completed (loss: 1.2235708236694336, acc: 0.6470588445663452)
[2025-02-12 22:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:06][root][INFO] - Training Epoch: 2/2, step 58/574 completed (loss: 0.9962884783744812, acc: 0.6818181872367859)
[2025-02-12 22:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:06][root][INFO] - Training Epoch: 2/2, step 59/574 completed (loss: 0.48373374342918396, acc: 0.875)
[2025-02-12 22:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:07][root][INFO] - Training Epoch: 2/2, step 60/574 completed (loss: 0.9289348721504211, acc: 0.7101449370384216)
[2025-02-12 22:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:07][root][INFO] - Training Epoch: 2/2, step 61/574 completed (loss: 0.8954492807388306, acc: 0.75)
[2025-02-12 22:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:08][root][INFO] - Training Epoch: 2/2, step 62/574 completed (loss: 0.7481579780578613, acc: 0.7647058963775635)
[2025-02-12 22:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:08][root][INFO] - Training Epoch: 2/2, step 63/574 completed (loss: 0.3961782157421112, acc: 0.8888888955116272)
[2025-02-12 22:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:08][root][INFO] - Training Epoch: 2/2, step 64/574 completed (loss: 0.23061954975128174, acc: 0.90625)
[2025-02-12 22:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:09][root][INFO] - Training Epoch: 2/2, step 65/574 completed (loss: 0.14268016815185547, acc: 0.9655172228813171)
[2025-02-12 22:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:09][root][INFO] - Training Epoch: 2/2, step 66/574 completed (loss: 0.8303095698356628, acc: 0.7678571343421936)
[2025-02-12 22:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:09][root][INFO] - Training Epoch: 2/2, step 67/574 completed (loss: 0.6338756680488586, acc: 0.8333333134651184)
[2025-02-12 22:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:10][root][INFO] - Training Epoch: 2/2, step 68/574 completed (loss: 0.14997981488704681, acc: 0.9599999785423279)
[2025-02-12 22:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:10][root][INFO] - Training Epoch: 2/2, step 69/574 completed (loss: 0.8108113408088684, acc: 0.7222222089767456)
[2025-02-12 22:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:10][root][INFO] - Training Epoch: 2/2, step 70/574 completed (loss: 1.178101897239685, acc: 0.6969696879386902)
[2025-02-12 22:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:10][root][INFO] - Training Epoch: 2/2, step 71/574 completed (loss: 1.0656843185424805, acc: 0.7132353186607361)
[2025-02-12 22:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:11][root][INFO] - Training Epoch: 2/2, step 72/574 completed (loss: 0.8566171526908875, acc: 0.7936508059501648)
[2025-02-12 22:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:11][root][INFO] - Training Epoch: 2/2, step 73/574 completed (loss: 1.357043981552124, acc: 0.6307692527770996)
[2025-02-12 22:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:11][root][INFO] - Training Epoch: 2/2, step 74/574 completed (loss: 1.287909746170044, acc: 0.6938775777816772)
[2025-02-12 22:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:12][root][INFO] - Training Epoch: 2/2, step 75/574 completed (loss: 1.2824839353561401, acc: 0.6194030046463013)
[2025-02-12 22:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:12][root][INFO] - Training Epoch: 2/2, step 76/574 completed (loss: 1.4569767713546753, acc: 0.6058394312858582)
[2025-02-12 22:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:12][root][INFO] - Training Epoch: 2/2, step 77/574 completed (loss: 0.10892627388238907, acc: 0.9523809552192688)
[2025-02-12 22:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:13][root][INFO] - Training Epoch: 2/2, step 78/574 completed (loss: 0.300266295671463, acc: 0.9166666865348816)
[2025-02-12 22:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:13][root][INFO] - Training Epoch: 2/2, step 79/574 completed (loss: 0.14265425503253937, acc: 0.9696969985961914)
[2025-02-12 22:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:13][root][INFO] - Training Epoch: 2/2, step 80/574 completed (loss: 0.32256177067756653, acc: 0.8846153616905212)
[2025-02-12 22:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:14][root][INFO] - Training Epoch: 2/2, step 81/574 completed (loss: 0.7566695809364319, acc: 0.7115384340286255)
[2025-02-12 22:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:14][root][INFO] - Training Epoch: 2/2, step 82/574 completed (loss: 0.6709110736846924, acc: 0.8461538553237915)
[2025-02-12 22:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:14][root][INFO] - Training Epoch: 2/2, step 83/574 completed (loss: 0.5262057185173035, acc: 0.875)
[2025-02-12 22:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:15][root][INFO] - Training Epoch: 2/2, step 84/574 completed (loss: 0.659031867980957, acc: 0.8115941882133484)
[2025-02-12 22:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:15][root][INFO] - Training Epoch: 2/2, step 85/574 completed (loss: 0.6015159487724304, acc: 0.8199999928474426)
[2025-02-12 22:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:15][root][INFO] - Training Epoch: 2/2, step 86/574 completed (loss: 0.7272269129753113, acc: 0.782608687877655)
[2025-02-12 22:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:16][root][INFO] - Training Epoch: 2/2, step 87/574 completed (loss: 1.3959580659866333, acc: 0.6600000262260437)
[2025-02-12 22:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:16][root][INFO] - Training Epoch: 2/2, step 88/574 completed (loss: 0.8142185211181641, acc: 0.7961165308952332)
[2025-02-12 22:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:17][root][INFO] - Training Epoch: 2/2, step 89/574 completed (loss: 1.0600045919418335, acc: 0.6990291476249695)
[2025-02-12 22:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:18][root][INFO] - Training Epoch: 2/2, step 90/574 completed (loss: 1.0457199811935425, acc: 0.7150537371635437)
[2025-02-12 22:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:19][root][INFO] - Training Epoch: 2/2, step 91/574 completed (loss: 0.9140870571136475, acc: 0.7413793206214905)
[2025-02-12 22:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:20][root][INFO] - Training Epoch: 2/2, step 92/574 completed (loss: 0.8032099604606628, acc: 0.7789473533630371)
[2025-02-12 22:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:21][root][INFO] - Training Epoch: 2/2, step 93/574 completed (loss: 1.3492084741592407, acc: 0.6336633563041687)
[2025-02-12 22:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:21][root][INFO] - Training Epoch: 2/2, step 94/574 completed (loss: 1.2738988399505615, acc: 0.6935483813285828)
[2025-02-12 22:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:21][root][INFO] - Training Epoch: 2/2, step 95/574 completed (loss: 0.9676163196563721, acc: 0.7681159377098083)
[2025-02-12 22:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:22][root][INFO] - Training Epoch: 2/2, step 96/574 completed (loss: 1.2591972351074219, acc: 0.6134454011917114)
[2025-02-12 22:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:22][root][INFO] - Training Epoch: 2/2, step 97/574 completed (loss: 1.2819459438323975, acc: 0.682692289352417)
[2025-02-12 22:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:22][root][INFO] - Training Epoch: 2/2, step 98/574 completed (loss: 1.266645073890686, acc: 0.6204379796981812)
[2025-02-12 22:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:23][root][INFO] - Training Epoch: 2/2, step 99/574 completed (loss: 1.6733850240707397, acc: 0.5223880410194397)
[2025-02-12 22:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:23][root][INFO] - Training Epoch: 2/2, step 100/574 completed (loss: 0.8509815335273743, acc: 0.699999988079071)
[2025-02-12 22:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:23][root][INFO] - Training Epoch: 2/2, step 101/574 completed (loss: 0.030067892745137215, acc: 1.0)
[2025-02-12 22:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:24][root][INFO] - Training Epoch: 2/2, step 102/574 completed (loss: 0.12918400764465332, acc: 0.95652174949646)
[2025-02-12 22:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:24][root][INFO] - Training Epoch: 2/2, step 103/574 completed (loss: 0.0786270946264267, acc: 0.9545454382896423)
[2025-02-12 22:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:24][root][INFO] - Training Epoch: 2/2, step 104/574 completed (loss: 0.5498583316802979, acc: 0.8448275923728943)
[2025-02-12 22:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:25][root][INFO] - Training Epoch: 2/2, step 105/574 completed (loss: 0.3467012345790863, acc: 0.8604651093482971)
[2025-02-12 22:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:25][root][INFO] - Training Epoch: 2/2, step 106/574 completed (loss: 0.6236713528633118, acc: 0.8399999737739563)
[2025-02-12 22:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:25][root][INFO] - Training Epoch: 2/2, step 107/574 completed (loss: 0.041788533329963684, acc: 1.0)
[2025-02-12 22:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:26][root][INFO] - Training Epoch: 2/2, step 108/574 completed (loss: 0.08369293808937073, acc: 0.9615384340286255)
[2025-02-12 22:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:26][root][INFO] - Training Epoch: 2/2, step 109/574 completed (loss: 0.16650402545928955, acc: 0.9285714030265808)
[2025-02-12 22:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:27][root][INFO] - Training Epoch: 2/2, step 110/574 completed (loss: 0.18964265286922455, acc: 0.9384615421295166)
[2025-02-12 22:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:27][root][INFO] - Training Epoch: 2/2, step 111/574 completed (loss: 0.48918670415878296, acc: 0.8245614171028137)
[2025-02-12 22:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:27][root][INFO] - Training Epoch: 2/2, step 112/574 completed (loss: 0.7392044067382812, acc: 0.8070175647735596)
[2025-02-12 22:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:28][root][INFO] - Training Epoch: 2/2, step 113/574 completed (loss: 0.4216980040073395, acc: 0.8717948794364929)
[2025-02-12 22:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:28][root][INFO] - Training Epoch: 2/2, step 114/574 completed (loss: 0.3017769157886505, acc: 0.8979591727256775)
[2025-02-12 22:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:28][root][INFO] - Training Epoch: 2/2, step 115/574 completed (loss: 0.13237664103507996, acc: 0.9545454382896423)
[2025-02-12 22:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:29][root][INFO] - Training Epoch: 2/2, step 116/574 completed (loss: 0.6063478589057922, acc: 0.841269850730896)
[2025-02-12 22:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:29][root][INFO] - Training Epoch: 2/2, step 117/574 completed (loss: 0.5013893842697144, acc: 0.869918704032898)
[2025-02-12 22:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:29][root][INFO] - Training Epoch: 2/2, step 118/574 completed (loss: 0.3841700851917267, acc: 0.8870967626571655)
[2025-02-12 22:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:30][root][INFO] - Training Epoch: 2/2, step 119/574 completed (loss: 0.5891773700714111, acc: 0.8365018963813782)
[2025-02-12 22:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:31][root][INFO] - Training Epoch: 2/2, step 120/574 completed (loss: 0.4034448564052582, acc: 0.8666666746139526)
[2025-02-12 22:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:31][root][INFO] - Training Epoch: 2/2, step 121/574 completed (loss: 0.5368309020996094, acc: 0.8846153616905212)
[2025-02-12 22:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:31][root][INFO] - Training Epoch: 2/2, step 122/574 completed (loss: 0.14683596789836884, acc: 0.9583333134651184)
[2025-02-12 22:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:32][root][INFO] - Training Epoch: 2/2, step 123/574 completed (loss: 0.3455490171909332, acc: 0.8947368264198303)
[2025-02-12 22:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:32][root][INFO] - Training Epoch: 2/2, step 124/574 completed (loss: 0.9349294304847717, acc: 0.7607361674308777)
[2025-02-12 22:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:33][root][INFO] - Training Epoch: 2/2, step 125/574 completed (loss: 0.9344513416290283, acc: 0.7083333134651184)
[2025-02-12 22:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:33][root][INFO] - Training Epoch: 2/2, step 126/574 completed (loss: 1.2132247686386108, acc: 0.675000011920929)
[2025-02-12 22:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:33][root][INFO] - Training Epoch: 2/2, step 127/574 completed (loss: 0.6058194041252136, acc: 0.8035714030265808)
[2025-02-12 22:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:33][root][INFO] - Training Epoch: 2/2, step 128/574 completed (loss: 0.8246767520904541, acc: 0.800000011920929)
[2025-02-12 22:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:34][root][INFO] - Training Epoch: 2/2, step 129/574 completed (loss: 0.9482977390289307, acc: 0.7352941036224365)
[2025-02-12 22:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:34][root][INFO] - Training Epoch: 2/2, step 130/574 completed (loss: 0.6986697316169739, acc: 0.692307710647583)
[2025-02-12 22:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:35][root][INFO] - Training Epoch: 2/2, step 131/574 completed (loss: 0.40675461292266846, acc: 0.8260869383811951)
[2025-02-12 22:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:35][root][INFO] - Training Epoch: 2/2, step 132/574 completed (loss: 0.6923059225082397, acc: 0.8125)
[2025-02-12 22:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:35][root][INFO] - Training Epoch: 2/2, step 133/574 completed (loss: 0.9296131730079651, acc: 0.739130437374115)
[2025-02-12 22:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:36][root][INFO] - Training Epoch: 2/2, step 134/574 completed (loss: 0.9181509017944336, acc: 0.800000011920929)
[2025-02-12 22:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:36][root][INFO] - Training Epoch: 2/2, step 135/574 completed (loss: 0.9244336485862732, acc: 0.7692307829856873)
[2025-02-12 22:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:37][root][INFO] - Training Epoch: 2/2, step 136/574 completed (loss: 0.6458325386047363, acc: 0.8095238208770752)
[2025-02-12 22:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:37][root][INFO] - Training Epoch: 2/2, step 137/574 completed (loss: 1.1063488721847534, acc: 0.5666666626930237)
[2025-02-12 22:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:37][root][INFO] - Training Epoch: 2/2, step 138/574 completed (loss: 0.9654567837715149, acc: 0.739130437374115)
[2025-02-12 22:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:38][root][INFO] - Training Epoch: 2/2, step 139/574 completed (loss: 0.40841224789619446, acc: 0.9047619104385376)
[2025-02-12 22:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:38][root][INFO] - Training Epoch: 2/2, step 140/574 completed (loss: 0.6273311376571655, acc: 0.7307692170143127)
[2025-02-12 22:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:09][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7016, device='cuda:0') eval_epoch_loss=tensor(0.5316, device='cuda:0') eval_epoch_acc=tensor(0.8524, device='cuda:0')
[2025-02-12 22:05:09][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:05:09][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:05:09][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_141_loss_0.5315613150596619/model.pt
[2025-02-12 22:05:09][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:05:09][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.5315613150596619
[2025-02-12 22:05:09][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8524357080459595
[2025-02-12 22:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:10][root][INFO] - Training Epoch: 2/2, step 141/574 completed (loss: 0.9617251753807068, acc: 0.8064516186714172)
[2025-02-12 22:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:10][root][INFO] - Training Epoch: 2/2, step 142/574 completed (loss: 0.936109721660614, acc: 0.6756756901741028)
[2025-02-12 22:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:11][root][INFO] - Training Epoch: 2/2, step 143/574 completed (loss: 0.9952110648155212, acc: 0.7017543911933899)
[2025-02-12 22:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:11][root][INFO] - Training Epoch: 2/2, step 144/574 completed (loss: 0.8649126887321472, acc: 0.7910447716712952)
[2025-02-12 22:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:11][root][INFO] - Training Epoch: 2/2, step 145/574 completed (loss: 0.7012535333633423, acc: 0.8265306353569031)
[2025-02-12 22:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:12][root][INFO] - Training Epoch: 2/2, step 146/574 completed (loss: 1.2406758069992065, acc: 0.6489361524581909)
[2025-02-12 22:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:12][root][INFO] - Training Epoch: 2/2, step 147/574 completed (loss: 1.0663899183273315, acc: 0.7285714149475098)
[2025-02-12 22:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:12][root][INFO] - Training Epoch: 2/2, step 148/574 completed (loss: 1.2900065183639526, acc: 0.6071428656578064)
[2025-02-12 22:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:13][root][INFO] - Training Epoch: 2/2, step 149/574 completed (loss: 0.9667994379997253, acc: 0.782608687877655)
[2025-02-12 22:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:13][root][INFO] - Training Epoch: 2/2, step 150/574 completed (loss: 0.6146264672279358, acc: 0.8620689511299133)
[2025-02-12 22:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:13][root][INFO] - Training Epoch: 2/2, step 151/574 completed (loss: 0.8516055941581726, acc: 0.739130437374115)
[2025-02-12 22:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:14][root][INFO] - Training Epoch: 2/2, step 152/574 completed (loss: 0.9738832712173462, acc: 0.694915235042572)
[2025-02-12 22:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:14][root][INFO] - Training Epoch: 2/2, step 153/574 completed (loss: 1.0723614692687988, acc: 0.719298243522644)
[2025-02-12 22:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:15][root][INFO] - Training Epoch: 2/2, step 154/574 completed (loss: 0.9079880714416504, acc: 0.7567567825317383)
[2025-02-12 22:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:15][root][INFO] - Training Epoch: 2/2, step 155/574 completed (loss: 0.3714846670627594, acc: 0.8928571343421936)
[2025-02-12 22:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:15][root][INFO] - Training Epoch: 2/2, step 156/574 completed (loss: 0.32524409890174866, acc: 0.8695651888847351)
[2025-02-12 22:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:16][root][INFO] - Training Epoch: 2/2, step 157/574 completed (loss: 1.421199083328247, acc: 0.5789473652839661)
[2025-02-12 22:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:17][root][INFO] - Training Epoch: 2/2, step 158/574 completed (loss: 0.9048901200294495, acc: 0.7702702879905701)
[2025-02-12 22:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:18][root][INFO] - Training Epoch: 2/2, step 159/574 completed (loss: 1.3000531196594238, acc: 0.5925925970077515)
[2025-02-12 22:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:18][root][INFO] - Training Epoch: 2/2, step 160/574 completed (loss: 1.059824824333191, acc: 0.7093023061752319)
[2025-02-12 22:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:19][root][INFO] - Training Epoch: 2/2, step 161/574 completed (loss: 1.1231776475906372, acc: 0.6470588445663452)
[2025-02-12 22:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:19][root][INFO] - Training Epoch: 2/2, step 162/574 completed (loss: 1.293740153312683, acc: 0.6516854166984558)
[2025-02-12 22:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:20][root][INFO] - Training Epoch: 2/2, step 163/574 completed (loss: 0.5020838975906372, acc: 0.8409090638160706)
[2025-02-12 22:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:20][root][INFO] - Training Epoch: 2/2, step 164/574 completed (loss: 0.33666127920150757, acc: 0.9523809552192688)
[2025-02-12 22:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:20][root][INFO] - Training Epoch: 2/2, step 165/574 completed (loss: 0.636423647403717, acc: 0.7931034564971924)
[2025-02-12 22:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:21][root][INFO] - Training Epoch: 2/2, step 166/574 completed (loss: 0.20636940002441406, acc: 0.9591836929321289)
[2025-02-12 22:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:21][root][INFO] - Training Epoch: 2/2, step 167/574 completed (loss: 0.2610202431678772, acc: 0.9200000166893005)
[2025-02-12 22:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:21][root][INFO] - Training Epoch: 2/2, step 168/574 completed (loss: 0.6429989337921143, acc: 0.8055555820465088)
[2025-02-12 22:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:22][root][INFO] - Training Epoch: 2/2, step 169/574 completed (loss: 1.0117472410202026, acc: 0.7745097875595093)
[2025-02-12 22:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:23][root][INFO] - Training Epoch: 2/2, step 170/574 completed (loss: 0.7575929760932922, acc: 0.8082191944122314)
[2025-02-12 22:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:23][root][INFO] - Training Epoch: 2/2, step 171/574 completed (loss: 0.31064411997795105, acc: 0.9166666865348816)
[2025-02-12 22:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:24][root][INFO] - Training Epoch: 2/2, step 172/574 completed (loss: 0.5452229380607605, acc: 0.8148148059844971)
[2025-02-12 22:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:24][root][INFO] - Training Epoch: 2/2, step 173/574 completed (loss: 0.835109293460846, acc: 0.7857142686843872)
[2025-02-12 22:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:24][root][INFO] - Training Epoch: 2/2, step 174/574 completed (loss: 1.0994631052017212, acc: 0.7079645991325378)
[2025-02-12 22:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:25][root][INFO] - Training Epoch: 2/2, step 175/574 completed (loss: 0.751186728477478, acc: 0.8260869383811951)
[2025-02-12 22:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:25][root][INFO] - Training Epoch: 2/2, step 176/574 completed (loss: 0.5341917276382446, acc: 0.8295454382896423)
[2025-02-12 22:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:26][root][INFO] - Training Epoch: 2/2, step 177/574 completed (loss: 1.0366398096084595, acc: 0.694656491279602)
[2025-02-12 22:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:27][root][INFO] - Training Epoch: 2/2, step 178/574 completed (loss: 1.096312165260315, acc: 0.6592592597007751)
[2025-02-12 22:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:27][root][INFO] - Training Epoch: 2/2, step 179/574 completed (loss: 0.5439565777778625, acc: 0.8360655903816223)
[2025-02-12 22:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:27][root][INFO] - Training Epoch: 2/2, step 180/574 completed (loss: 0.09670265763998032, acc: 1.0)
[2025-02-12 22:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:28][root][INFO] - Training Epoch: 2/2, step 181/574 completed (loss: 0.31319332122802734, acc: 0.8799999952316284)
[2025-02-12 22:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:28][root][INFO] - Training Epoch: 2/2, step 182/574 completed (loss: 0.1612313836812973, acc: 0.9642857313156128)
[2025-02-12 22:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:28][root][INFO] - Training Epoch: 2/2, step 183/574 completed (loss: 0.4545342028141022, acc: 0.8292682766914368)
[2025-02-12 22:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:29][root][INFO] - Training Epoch: 2/2, step 184/574 completed (loss: 0.50578773021698, acc: 0.8821752071380615)
[2025-02-12 22:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:29][root][INFO] - Training Epoch: 2/2, step 185/574 completed (loss: 0.5092018246650696, acc: 0.8645533323287964)
[2025-02-12 22:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:29][root][INFO] - Training Epoch: 2/2, step 186/574 completed (loss: 0.45948559045791626, acc: 0.8656250238418579)
[2025-02-12 22:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:30][root][INFO] - Training Epoch: 2/2, step 187/574 completed (loss: 0.5115060806274414, acc: 0.8574109077453613)
[2025-02-12 22:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:30][root][INFO] - Training Epoch: 2/2, step 188/574 completed (loss: 0.5444188714027405, acc: 0.8398576378822327)
[2025-02-12 22:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:31][root][INFO] - Training Epoch: 2/2, step 189/574 completed (loss: 0.5124368071556091, acc: 0.8399999737739563)
[2025-02-12 22:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:31][root][INFO] - Training Epoch: 2/2, step 190/574 completed (loss: 0.8502680659294128, acc: 0.6976743936538696)
[2025-02-12 22:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:32][root][INFO] - Training Epoch: 2/2, step 191/574 completed (loss: 1.1023818254470825, acc: 0.7142857313156128)
[2025-02-12 22:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:33][root][INFO] - Training Epoch: 2/2, step 192/574 completed (loss: 1.0424994230270386, acc: 0.6742424368858337)
[2025-02-12 22:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:34][root][INFO] - Training Epoch: 2/2, step 193/574 completed (loss: 0.8481307029724121, acc: 0.7764706015586853)
[2025-02-12 22:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:35][root][INFO] - Training Epoch: 2/2, step 194/574 completed (loss: 0.988685131072998, acc: 0.7222222089767456)
[2025-02-12 22:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:36][root][INFO] - Training Epoch: 2/2, step 195/574 completed (loss: 0.5498021841049194, acc: 0.7903226017951965)
[2025-02-12 22:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:36][root][INFO] - Training Epoch: 2/2, step 196/574 completed (loss: 0.3235122859477997, acc: 0.8928571343421936)
[2025-02-12 22:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:36][root][INFO] - Training Epoch: 2/2, step 197/574 completed (loss: 1.235466718673706, acc: 0.625)
[2025-02-12 22:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:37][root][INFO] - Training Epoch: 2/2, step 198/574 completed (loss: 0.9794105291366577, acc: 0.7352941036224365)
[2025-02-12 22:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:37][root][INFO] - Training Epoch: 2/2, step 199/574 completed (loss: 0.8936046361923218, acc: 0.7647058963775635)
[2025-02-12 22:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:37][root][INFO] - Training Epoch: 2/2, step 200/574 completed (loss: 0.7046128511428833, acc: 0.7627118825912476)
[2025-02-12 22:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:38][root][INFO] - Training Epoch: 2/2, step 201/574 completed (loss: 0.9997929930686951, acc: 0.7313432693481445)
[2025-02-12 22:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:38][root][INFO] - Training Epoch: 2/2, step 202/574 completed (loss: 1.0043940544128418, acc: 0.7184466123580933)
[2025-02-12 22:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:39][root][INFO] - Training Epoch: 2/2, step 203/574 completed (loss: 1.0302443504333496, acc: 0.6984127163887024)
[2025-02-12 22:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:39][root][INFO] - Training Epoch: 2/2, step 204/574 completed (loss: 0.18167974054813385, acc: 0.9450549483299255)
[2025-02-12 22:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:39][root][INFO] - Training Epoch: 2/2, step 205/574 completed (loss: 0.3517152667045593, acc: 0.9058296084403992)
[2025-02-12 22:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:40][root][INFO] - Training Epoch: 2/2, step 206/574 completed (loss: 0.52056884765625, acc: 0.8267716765403748)
[2025-02-12 22:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:40][root][INFO] - Training Epoch: 2/2, step 207/574 completed (loss: 0.4169369637966156, acc: 0.8879310488700867)
[2025-02-12 22:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:40][root][INFO] - Training Epoch: 2/2, step 208/574 completed (loss: 0.4730278253555298, acc: 0.8804348111152649)
[2025-02-12 22:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:41][root][INFO] - Training Epoch: 2/2, step 209/574 completed (loss: 0.502404510974884, acc: 0.8521400690078735)
[2025-02-12 22:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:41][root][INFO] - Training Epoch: 2/2, step 210/574 completed (loss: 0.258802205324173, acc: 0.9021739363670349)
[2025-02-12 22:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:41][root][INFO] - Training Epoch: 2/2, step 211/574 completed (loss: 0.12673063576221466, acc: 1.0)
[2025-02-12 22:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:41][root][INFO] - Training Epoch: 2/2, step 212/574 completed (loss: 0.12508808076381683, acc: 0.9642857313156128)
[2025-02-12 22:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:42][root][INFO] - Training Epoch: 2/2, step 213/574 completed (loss: 0.13254643976688385, acc: 0.957446813583374)
[2025-02-12 22:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:42][root][INFO] - Training Epoch: 2/2, step 214/574 completed (loss: 0.17449326813220978, acc: 0.9230769276618958)
[2025-02-12 22:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:43][root][INFO] - Training Epoch: 2/2, step 215/574 completed (loss: 0.17702510952949524, acc: 0.9459459185600281)
[2025-02-12 22:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:43][root][INFO] - Training Epoch: 2/2, step 216/574 completed (loss: 0.22882360219955444, acc: 0.9534883499145508)
[2025-02-12 22:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:44][root][INFO] - Training Epoch: 2/2, step 217/574 completed (loss: 0.24255774915218353, acc: 0.9279279112815857)
[2025-02-12 22:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:44][root][INFO] - Training Epoch: 2/2, step 218/574 completed (loss: 0.11985397338867188, acc: 0.9777777791023254)
[2025-02-12 22:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:44][root][INFO] - Training Epoch: 2/2, step 219/574 completed (loss: 0.4593941867351532, acc: 0.9090909361839294)
[2025-02-12 22:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:45][root][INFO] - Training Epoch: 2/2, step 220/574 completed (loss: 0.11733677983283997, acc: 0.9629629850387573)
[2025-02-12 22:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:45][root][INFO] - Training Epoch: 2/2, step 221/574 completed (loss: 0.07766817510128021, acc: 1.0)
[2025-02-12 22:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:45][root][INFO] - Training Epoch: 2/2, step 222/574 completed (loss: 0.8512443900108337, acc: 0.7307692170143127)
[2025-02-12 22:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:46][root][INFO] - Training Epoch: 2/2, step 223/574 completed (loss: 0.32606303691864014, acc: 0.9021739363670349)
[2025-02-12 22:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:47][root][INFO] - Training Epoch: 2/2, step 224/574 completed (loss: 0.5384947657585144, acc: 0.8579545617103577)
[2025-02-12 22:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:47][root][INFO] - Training Epoch: 2/2, step 225/574 completed (loss: 0.7474306225776672, acc: 0.7872340679168701)
[2025-02-12 22:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:48][root][INFO] - Training Epoch: 2/2, step 226/574 completed (loss: 0.5923975706100464, acc: 0.7924528121948242)
[2025-02-12 22:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:48][root][INFO] - Training Epoch: 2/2, step 227/574 completed (loss: 0.3881518542766571, acc: 0.8666666746139526)
[2025-02-12 22:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:48][root][INFO] - Training Epoch: 2/2, step 228/574 completed (loss: 0.42603108286857605, acc: 0.8604651093482971)
[2025-02-12 22:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:49][root][INFO] - Training Epoch: 2/2, step 229/574 completed (loss: 1.0458165407180786, acc: 0.699999988079071)
[2025-02-12 22:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:49][root][INFO] - Training Epoch: 2/2, step 230/574 completed (loss: 1.706234097480774, acc: 0.5052631497383118)
[2025-02-12 22:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:49][root][INFO] - Training Epoch: 2/2, step 231/574 completed (loss: 1.4204437732696533, acc: 0.644444465637207)
[2025-02-12 22:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:50][root][INFO] - Training Epoch: 2/2, step 232/574 completed (loss: 1.16969895362854, acc: 0.6888889074325562)
[2025-02-12 22:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:50][root][INFO] - Training Epoch: 2/2, step 233/574 completed (loss: 1.6308679580688477, acc: 0.5733944773674011)
[2025-02-12 22:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:51][root][INFO] - Training Epoch: 2/2, step 234/574 completed (loss: 1.0934332609176636, acc: 0.6692307591438293)
[2025-02-12 22:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:51][root][INFO] - Training Epoch: 2/2, step 235/574 completed (loss: 0.6165827512741089, acc: 0.8421052694320679)
[2025-02-12 22:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:51][root][INFO] - Training Epoch: 2/2, step 236/574 completed (loss: 0.48497965931892395, acc: 0.7916666865348816)
[2025-02-12 22:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:52][root][INFO] - Training Epoch: 2/2, step 237/574 completed (loss: 1.4969992637634277, acc: 0.6363636255264282)
[2025-02-12 22:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:52][root][INFO] - Training Epoch: 2/2, step 238/574 completed (loss: 0.42650753259658813, acc: 0.8518518805503845)
[2025-02-12 22:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:52][root][INFO] - Training Epoch: 2/2, step 239/574 completed (loss: 0.8905603289604187, acc: 0.6857143044471741)
[2025-02-12 22:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:53][root][INFO] - Training Epoch: 2/2, step 240/574 completed (loss: 1.0061957836151123, acc: 0.7272727489471436)
[2025-02-12 22:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:53][root][INFO] - Training Epoch: 2/2, step 241/574 completed (loss: 0.6298729777336121, acc: 0.75)
[2025-02-12 22:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:54][root][INFO] - Training Epoch: 2/2, step 242/574 completed (loss: 1.1426116228103638, acc: 0.5967742204666138)
[2025-02-12 22:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:54][root][INFO] - Training Epoch: 2/2, step 243/574 completed (loss: 1.0506218671798706, acc: 0.7045454382896423)
[2025-02-12 22:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:55][root][INFO] - Training Epoch: 2/2, step 244/574 completed (loss: 0.029400072991847992, acc: 1.0)
[2025-02-12 22:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:55][root][INFO] - Training Epoch: 2/2, step 245/574 completed (loss: 0.14849600195884705, acc: 0.9230769276618958)
[2025-02-12 22:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:55][root][INFO] - Training Epoch: 2/2, step 246/574 completed (loss: 0.3116108477115631, acc: 0.9032257795333862)
[2025-02-12 22:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:56][root][INFO] - Training Epoch: 2/2, step 247/574 completed (loss: 0.2665458917617798, acc: 0.949999988079071)
[2025-02-12 22:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:56][root][INFO] - Training Epoch: 2/2, step 248/574 completed (loss: 0.5624580383300781, acc: 0.8648648858070374)
[2025-02-12 22:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:56][root][INFO] - Training Epoch: 2/2, step 249/574 completed (loss: 0.38700780272483826, acc: 0.8918918967247009)
[2025-02-12 22:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:57][root][INFO] - Training Epoch: 2/2, step 250/574 completed (loss: 0.15941251814365387, acc: 0.9459459185600281)
[2025-02-12 22:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:57][root][INFO] - Training Epoch: 2/2, step 251/574 completed (loss: 0.26020294427871704, acc: 0.8970588445663452)
[2025-02-12 22:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:57][root][INFO] - Training Epoch: 2/2, step 252/574 completed (loss: 0.20366279780864716, acc: 0.9024389982223511)
[2025-02-12 22:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:58][root][INFO] - Training Epoch: 2/2, step 253/574 completed (loss: 0.08048105984926224, acc: 1.0)
[2025-02-12 22:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:58][root][INFO] - Training Epoch: 2/2, step 254/574 completed (loss: 0.03683312609791756, acc: 0.9599999785423279)
[2025-02-12 22:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:58][root][INFO] - Training Epoch: 2/2, step 255/574 completed (loss: 0.11865834146738052, acc: 0.9677419066429138)
[2025-02-12 22:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:59][root][INFO] - Training Epoch: 2/2, step 256/574 completed (loss: 0.26938825845718384, acc: 0.9122806787490845)
[2025-02-12 22:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:59][root][INFO] - Training Epoch: 2/2, step 257/574 completed (loss: 0.1456795036792755, acc: 0.9714285731315613)
[2025-02-12 22:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:05:59][root][INFO] - Training Epoch: 2/2, step 258/574 completed (loss: 0.1694391965866089, acc: 0.9605262875556946)
[2025-02-12 22:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:00][root][INFO] - Training Epoch: 2/2, step 259/574 completed (loss: 0.47656407952308655, acc: 0.9056603908538818)
[2025-02-12 22:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:00][root][INFO] - Training Epoch: 2/2, step 260/574 completed (loss: 0.4919716715812683, acc: 0.8833333253860474)
[2025-02-12 22:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:01][root][INFO] - Training Epoch: 2/2, step 261/574 completed (loss: 0.19504910707473755, acc: 0.8888888955116272)
[2025-02-12 22:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:01][root][INFO] - Training Epoch: 2/2, step 262/574 completed (loss: 0.7513439059257507, acc: 0.8709677457809448)
[2025-02-12 22:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:01][root][INFO] - Training Epoch: 2/2, step 263/574 completed (loss: 1.2729735374450684, acc: 0.746666669845581)
[2025-02-12 22:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:02][root][INFO] - Training Epoch: 2/2, step 264/574 completed (loss: 0.6317715644836426, acc: 0.7708333134651184)
[2025-02-12 22:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:02][root][INFO] - Training Epoch: 2/2, step 265/574 completed (loss: 1.33262038230896, acc: 0.6320000290870667)
[2025-02-12 22:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:03][root][INFO] - Training Epoch: 2/2, step 266/574 completed (loss: 1.432184100151062, acc: 0.6067415475845337)
[2025-02-12 22:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:03][root][INFO] - Training Epoch: 2/2, step 267/574 completed (loss: 1.1361662149429321, acc: 0.6216216087341309)
[2025-02-12 22:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:04][root][INFO] - Training Epoch: 2/2, step 268/574 completed (loss: 0.8180915117263794, acc: 0.7413793206214905)
[2025-02-12 22:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:04][root][INFO] - Training Epoch: 2/2, step 269/574 completed (loss: 0.23277105391025543, acc: 0.9090909361839294)
[2025-02-12 22:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:04][root][INFO] - Training Epoch: 2/2, step 270/574 completed (loss: 0.14011545479297638, acc: 0.9090909361839294)
[2025-02-12 22:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:04][root][INFO] - Training Epoch: 2/2, step 271/574 completed (loss: 0.10540248453617096, acc: 0.96875)
[2025-02-12 22:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:05][root][INFO] - Training Epoch: 2/2, step 272/574 completed (loss: 0.13621269166469574, acc: 0.9666666388511658)
[2025-02-12 22:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:05][root][INFO] - Training Epoch: 2/2, step 273/574 completed (loss: 0.2726197838783264, acc: 0.949999988079071)
[2025-02-12 22:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:05][root][INFO] - Training Epoch: 2/2, step 274/574 completed (loss: 0.2514801323413849, acc: 0.9375)
[2025-02-12 22:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:06][root][INFO] - Training Epoch: 2/2, step 275/574 completed (loss: 0.21527309715747833, acc: 0.9333333373069763)
[2025-02-12 22:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:06][root][INFO] - Training Epoch: 2/2, step 276/574 completed (loss: 0.3731170892715454, acc: 0.931034505367279)
[2025-02-12 22:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:06][root][INFO] - Training Epoch: 2/2, step 277/574 completed (loss: 0.3181939721107483, acc: 0.9599999785423279)
[2025-02-12 22:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:07][root][INFO] - Training Epoch: 2/2, step 278/574 completed (loss: 0.3859301507472992, acc: 0.8936170339584351)
[2025-02-12 22:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:07][root][INFO] - Training Epoch: 2/2, step 279/574 completed (loss: 0.6061995625495911, acc: 0.875)
[2025-02-12 22:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:07][root][INFO] - Training Epoch: 2/2, step 280/574 completed (loss: 0.13517098128795624, acc: 0.9772727489471436)
[2025-02-12 22:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:08][root][INFO] - Training Epoch: 2/2, step 281/574 completed (loss: 0.8257825970649719, acc: 0.7951807379722595)
[2025-02-12 22:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:08][root][INFO] - Training Epoch: 2/2, step 282/574 completed (loss: 1.0226471424102783, acc: 0.7592592835426331)
[2025-02-12 22:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:09][root][INFO] - Training Epoch: 2/2, step 283/574 completed (loss: 0.1328066736459732, acc: 0.9473684430122375)
[2025-02-12 22:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:40][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.6684, device='cuda:0') eval_epoch_loss=tensor(0.5119, device='cuda:0') eval_epoch_acc=tensor(0.8582, device='cuda:0')
[2025-02-12 22:06:40][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:06:40][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:06:40][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5118914246559143/model.pt
[2025-02-12 22:06:40][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:06:40][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.5118914246559143
[2025-02-12 22:06:40][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8582029342651367
[2025-02-12 22:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:41][root][INFO] - Training Epoch: 2/2, step 284/574 completed (loss: 0.6641221046447754, acc: 0.8235294222831726)
[2025-02-12 22:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:41][root][INFO] - Training Epoch: 2/2, step 285/574 completed (loss: 0.20304033160209656, acc: 0.925000011920929)
[2025-02-12 22:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:41][root][INFO] - Training Epoch: 2/2, step 286/574 completed (loss: 0.6015849113464355, acc: 0.796875)
[2025-02-12 22:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:42][root][INFO] - Training Epoch: 2/2, step 287/574 completed (loss: 0.5611127614974976, acc: 0.8080000281333923)
[2025-02-12 22:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:42][root][INFO] - Training Epoch: 2/2, step 288/574 completed (loss: 0.3295384645462036, acc: 0.9230769276618958)
[2025-02-12 22:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:43][root][INFO] - Training Epoch: 2/2, step 289/574 completed (loss: 0.5553620457649231, acc: 0.8447204828262329)
[2025-02-12 22:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:43][root][INFO] - Training Epoch: 2/2, step 290/574 completed (loss: 0.49334925413131714, acc: 0.876288652420044)
[2025-02-12 22:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:43][root][INFO] - Training Epoch: 2/2, step 291/574 completed (loss: 0.08546169102191925, acc: 1.0)
[2025-02-12 22:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:43][root][INFO] - Training Epoch: 2/2, step 292/574 completed (loss: 0.3515479564666748, acc: 0.8809523582458496)
[2025-02-12 22:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:44][root][INFO] - Training Epoch: 2/2, step 293/574 completed (loss: 0.17178167402744293, acc: 0.9655172228813171)
[2025-02-12 22:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:44][root][INFO] - Training Epoch: 2/2, step 294/574 completed (loss: 0.4478470981121063, acc: 0.8727272748947144)
[2025-02-12 22:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:45][root][INFO] - Training Epoch: 2/2, step 295/574 completed (loss: 0.5731666684150696, acc: 0.8608247637748718)
[2025-02-12 22:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:45][root][INFO] - Training Epoch: 2/2, step 296/574 completed (loss: 0.46430453658103943, acc: 0.8965517282485962)
[2025-02-12 22:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:45][root][INFO] - Training Epoch: 2/2, step 297/574 completed (loss: 0.16166070103645325, acc: 0.9629629850387573)
[2025-02-12 22:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:46][root][INFO] - Training Epoch: 2/2, step 298/574 completed (loss: 0.5888537764549255, acc: 0.7894737124443054)
[2025-02-12 22:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:46][root][INFO] - Training Epoch: 2/2, step 299/574 completed (loss: 0.07121435552835464, acc: 0.9821428656578064)
[2025-02-12 22:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:46][root][INFO] - Training Epoch: 2/2, step 300/574 completed (loss: 0.09735777974128723, acc: 0.96875)
[2025-02-12 22:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:47][root][INFO] - Training Epoch: 2/2, step 301/574 completed (loss: 0.32218286395072937, acc: 0.9245283007621765)
[2025-02-12 22:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:47][root][INFO] - Training Epoch: 2/2, step 302/574 completed (loss: 0.04279404133558273, acc: 1.0)
[2025-02-12 22:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:47][root][INFO] - Training Epoch: 2/2, step 303/574 completed (loss: 0.023933768272399902, acc: 1.0)
[2025-02-12 22:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:48][root][INFO] - Training Epoch: 2/2, step 304/574 completed (loss: 0.09976750612258911, acc: 0.96875)
[2025-02-12 22:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:48][root][INFO] - Training Epoch: 2/2, step 305/574 completed (loss: 0.5351212024688721, acc: 0.8524590134620667)
[2025-02-12 22:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:48][root][INFO] - Training Epoch: 2/2, step 306/574 completed (loss: 0.10068067908287048, acc: 0.9666666388511658)
[2025-02-12 22:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:49][root][INFO] - Training Epoch: 2/2, step 307/574 completed (loss: 0.09207158535718918, acc: 0.9473684430122375)
[2025-02-12 22:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:49][root][INFO] - Training Epoch: 2/2, step 308/574 completed (loss: 0.33395615220069885, acc: 0.8985507488250732)
[2025-02-12 22:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:50][root][INFO] - Training Epoch: 2/2, step 309/574 completed (loss: 0.22040066123008728, acc: 0.9166666865348816)
[2025-02-12 22:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:50][root][INFO] - Training Epoch: 2/2, step 310/574 completed (loss: 0.3198714852333069, acc: 0.891566276550293)
[2025-02-12 22:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:50][root][INFO] - Training Epoch: 2/2, step 311/574 completed (loss: 0.43608924746513367, acc: 0.8461538553237915)
[2025-02-12 22:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:51][root][INFO] - Training Epoch: 2/2, step 312/574 completed (loss: 0.07503421604633331, acc: 0.9693877696990967)
[2025-02-12 22:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:51][root][INFO] - Training Epoch: 2/2, step 313/574 completed (loss: 0.009477376937866211, acc: 1.0)
[2025-02-12 22:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:51][root][INFO] - Training Epoch: 2/2, step 314/574 completed (loss: 0.055905427783727646, acc: 1.0)
[2025-02-12 22:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:52][root][INFO] - Training Epoch: 2/2, step 315/574 completed (loss: 0.31988751888275146, acc: 0.9354838728904724)
[2025-02-12 22:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:52][root][INFO] - Training Epoch: 2/2, step 316/574 completed (loss: 0.1619853526353836, acc: 0.9354838728904724)
[2025-02-12 22:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:52][root][INFO] - Training Epoch: 2/2, step 317/574 completed (loss: 0.2805965840816498, acc: 0.89552241563797)
[2025-02-12 22:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:53][root][INFO] - Training Epoch: 2/2, step 318/574 completed (loss: 0.18096759915351868, acc: 0.9615384340286255)
[2025-02-12 22:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:53][root][INFO] - Training Epoch: 2/2, step 319/574 completed (loss: 0.2952633798122406, acc: 0.9333333373069763)
[2025-02-12 22:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:53][root][INFO] - Training Epoch: 2/2, step 320/574 completed (loss: 0.13583879172801971, acc: 0.9677419066429138)
[2025-02-12 22:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:54][root][INFO] - Training Epoch: 2/2, step 321/574 completed (loss: 0.04466475546360016, acc: 0.9800000190734863)
[2025-02-12 22:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:54][root][INFO] - Training Epoch: 2/2, step 322/574 completed (loss: 0.7091774940490723, acc: 0.7777777910232544)
[2025-02-12 22:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:54][root][INFO] - Training Epoch: 2/2, step 323/574 completed (loss: 0.9248959422111511, acc: 0.7428571581840515)
[2025-02-12 22:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:55][root][INFO] - Training Epoch: 2/2, step 324/574 completed (loss: 1.4262375831604004, acc: 0.6410256624221802)
[2025-02-12 22:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:55][root][INFO] - Training Epoch: 2/2, step 325/574 completed (loss: 1.5646073818206787, acc: 0.6097561120986938)
[2025-02-12 22:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:55][root][INFO] - Training Epoch: 2/2, step 326/574 completed (loss: 0.8338473439216614, acc: 0.7631579041481018)
[2025-02-12 22:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:56][root][INFO] - Training Epoch: 2/2, step 327/574 completed (loss: 0.5034257173538208, acc: 0.9473684430122375)
[2025-02-12 22:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:56][root][INFO] - Training Epoch: 2/2, step 328/574 completed (loss: 0.1017838642001152, acc: 0.9642857313156128)
[2025-02-12 22:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:56][root][INFO] - Training Epoch: 2/2, step 329/574 completed (loss: 0.2540091276168823, acc: 0.9259259104728699)
[2025-02-12 22:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:57][root][INFO] - Training Epoch: 2/2, step 330/574 completed (loss: 0.08657282590866089, acc: 0.96875)
[2025-02-12 22:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:57][root][INFO] - Training Epoch: 2/2, step 331/574 completed (loss: 0.25591105222702026, acc: 0.9354838728904724)
[2025-02-12 22:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:58][root][INFO] - Training Epoch: 2/2, step 332/574 completed (loss: 0.1451268047094345, acc: 0.9649122953414917)
[2025-02-12 22:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:58][root][INFO] - Training Epoch: 2/2, step 333/574 completed (loss: 0.3311879634857178, acc: 0.9375)
[2025-02-12 22:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:58][root][INFO] - Training Epoch: 2/2, step 334/574 completed (loss: 0.1508435755968094, acc: 0.9666666388511658)
[2025-02-12 22:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:59][root][INFO] - Training Epoch: 2/2, step 335/574 completed (loss: 0.24319571256637573, acc: 0.9473684430122375)
[2025-02-12 22:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:59][root][INFO] - Training Epoch: 2/2, step 336/574 completed (loss: 0.8071019649505615, acc: 0.7400000095367432)
[2025-02-12 22:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:06:59][root][INFO] - Training Epoch: 2/2, step 337/574 completed (loss: 1.3152168989181519, acc: 0.6436781883239746)
[2025-02-12 22:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:00][root][INFO] - Training Epoch: 2/2, step 338/574 completed (loss: 1.2229924201965332, acc: 0.6276595592498779)
[2025-02-12 22:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:00][root][INFO] - Training Epoch: 2/2, step 339/574 completed (loss: 1.2467466592788696, acc: 0.650602400302887)
[2025-02-12 22:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:00][root][INFO] - Training Epoch: 2/2, step 340/574 completed (loss: 0.03962517902255058, acc: 1.0)
[2025-02-12 22:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:01][root][INFO] - Training Epoch: 2/2, step 341/574 completed (loss: 0.7177165150642395, acc: 0.8717948794364929)
[2025-02-12 22:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:01][root][INFO] - Training Epoch: 2/2, step 342/574 completed (loss: 0.574920117855072, acc: 0.8795180916786194)
[2025-02-12 22:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:01][root][INFO] - Training Epoch: 2/2, step 343/574 completed (loss: 0.4471702575683594, acc: 0.8679245114326477)
[2025-02-12 22:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:02][root][INFO] - Training Epoch: 2/2, step 344/574 completed (loss: 0.211822509765625, acc: 0.9367088675498962)
[2025-02-12 22:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:02][root][INFO] - Training Epoch: 2/2, step 345/574 completed (loss: 0.11835005134344101, acc: 0.9411764740943909)
[2025-02-12 22:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:02][root][INFO] - Training Epoch: 2/2, step 346/574 completed (loss: 0.3594485819339752, acc: 0.8805969953536987)
[2025-02-12 22:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:03][root][INFO] - Training Epoch: 2/2, step 347/574 completed (loss: 0.011881815269589424, acc: 1.0)
[2025-02-12 22:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:03][root][INFO] - Training Epoch: 2/2, step 348/574 completed (loss: 0.17829570174217224, acc: 0.9599999785423279)
[2025-02-12 22:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:03][root][INFO] - Training Epoch: 2/2, step 349/574 completed (loss: 0.6856319308280945, acc: 0.8055555820465088)
[2025-02-12 22:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:04][root][INFO] - Training Epoch: 2/2, step 350/574 completed (loss: 0.6999639272689819, acc: 0.7209302186965942)
[2025-02-12 22:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:04][root][INFO] - Training Epoch: 2/2, step 351/574 completed (loss: 0.19377239048480988, acc: 0.9487179517745972)
[2025-02-12 22:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:04][root][INFO] - Training Epoch: 2/2, step 352/574 completed (loss: 0.7606956362724304, acc: 0.7777777910232544)
[2025-02-12 22:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:05][root][INFO] - Training Epoch: 2/2, step 353/574 completed (loss: 0.02366681769490242, acc: 1.0)
[2025-02-12 22:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:05][root][INFO] - Training Epoch: 2/2, step 354/574 completed (loss: 0.7723348140716553, acc: 0.807692289352417)
[2025-02-12 22:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:05][root][INFO] - Training Epoch: 2/2, step 355/574 completed (loss: 0.9502097368240356, acc: 0.7142857313156128)
[2025-02-12 22:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:06][root][INFO] - Training Epoch: 2/2, step 356/574 completed (loss: 0.6371392011642456, acc: 0.8086956739425659)
[2025-02-12 22:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:06][root][INFO] - Training Epoch: 2/2, step 357/574 completed (loss: 0.604884147644043, acc: 0.8152173757553101)
[2025-02-12 22:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:07][root][INFO] - Training Epoch: 2/2, step 358/574 completed (loss: 0.7795040607452393, acc: 0.7755101919174194)
[2025-02-12 22:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:07][root][INFO] - Training Epoch: 2/2, step 359/574 completed (loss: 0.007214905694127083, acc: 1.0)
[2025-02-12 22:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:07][root][INFO] - Training Epoch: 2/2, step 360/574 completed (loss: 0.3611215054988861, acc: 0.9230769276618958)
[2025-02-12 22:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:08][root][INFO] - Training Epoch: 2/2, step 361/574 completed (loss: 0.3069142699241638, acc: 0.9268292784690857)
[2025-02-12 22:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:08][root][INFO] - Training Epoch: 2/2, step 362/574 completed (loss: 0.3105834126472473, acc: 0.9333333373069763)
[2025-02-12 22:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:08][root][INFO] - Training Epoch: 2/2, step 363/574 completed (loss: 0.18656405806541443, acc: 0.9078947305679321)
[2025-02-12 22:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:09][root][INFO] - Training Epoch: 2/2, step 364/574 completed (loss: 0.2535824775695801, acc: 0.8780487775802612)
[2025-02-12 22:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:09][root][INFO] - Training Epoch: 2/2, step 365/574 completed (loss: 0.12354746460914612, acc: 0.9696969985961914)
[2025-02-12 22:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:09][root][INFO] - Training Epoch: 2/2, step 366/574 completed (loss: 0.03692694753408432, acc: 1.0)
[2025-02-12 22:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:10][root][INFO] - Training Epoch: 2/2, step 367/574 completed (loss: 0.023051997646689415, acc: 1.0)
[2025-02-12 22:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:10][root][INFO] - Training Epoch: 2/2, step 368/574 completed (loss: 0.042134907096624374, acc: 1.0)
[2025-02-12 22:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:10][root][INFO] - Training Epoch: 2/2, step 369/574 completed (loss: 0.13840292394161224, acc: 1.0)
[2025-02-12 22:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:11][root][INFO] - Training Epoch: 2/2, step 370/574 completed (loss: 0.4535314738750458, acc: 0.8545454740524292)
[2025-02-12 22:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:12][root][INFO] - Training Epoch: 2/2, step 371/574 completed (loss: 0.38356855511665344, acc: 0.8867924809455872)
[2025-02-12 22:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:12][root][INFO] - Training Epoch: 2/2, step 372/574 completed (loss: 0.19416266679763794, acc: 0.9555555582046509)
[2025-02-12 22:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:12][root][INFO] - Training Epoch: 2/2, step 373/574 completed (loss: 0.2780603766441345, acc: 0.9642857313156128)
[2025-02-12 22:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:13][root][INFO] - Training Epoch: 2/2, step 374/574 completed (loss: 0.1718641221523285, acc: 0.9428571462631226)
[2025-02-12 22:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:13][root][INFO] - Training Epoch: 2/2, step 375/574 completed (loss: 0.004215299617499113, acc: 1.0)
[2025-02-12 22:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:14][root][INFO] - Training Epoch: 2/2, step 376/574 completed (loss: 0.018727807328104973, acc: 1.0)
[2025-02-12 22:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:14][root][INFO] - Training Epoch: 2/2, step 377/574 completed (loss: 0.06777632981538773, acc: 0.9791666865348816)
[2025-02-12 22:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:14][root][INFO] - Training Epoch: 2/2, step 378/574 completed (loss: 0.05204371362924576, acc: 0.9894737005233765)
[2025-02-12 22:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:15][root][INFO] - Training Epoch: 2/2, step 379/574 completed (loss: 0.30612584948539734, acc: 0.9041916131973267)
[2025-02-12 22:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:15][root][INFO] - Training Epoch: 2/2, step 380/574 completed (loss: 0.40126511454582214, acc: 0.8796992301940918)
[2025-02-12 22:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:17][root][INFO] - Training Epoch: 2/2, step 381/574 completed (loss: 0.5343453884124756, acc: 0.855614960193634)
[2025-02-12 22:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:17][root][INFO] - Training Epoch: 2/2, step 382/574 completed (loss: 0.1255699098110199, acc: 0.9639639854431152)
[2025-02-12 22:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:17][root][INFO] - Training Epoch: 2/2, step 383/574 completed (loss: 0.38009366393089294, acc: 0.8928571343421936)
[2025-02-12 22:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:18][root][INFO] - Training Epoch: 2/2, step 384/574 completed (loss: 0.04151174798607826, acc: 1.0)
[2025-02-12 22:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:18][root][INFO] - Training Epoch: 2/2, step 385/574 completed (loss: 0.10150866955518723, acc: 0.96875)
[2025-02-12 22:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:18][root][INFO] - Training Epoch: 2/2, step 386/574 completed (loss: 0.02198476530611515, acc: 1.0)
[2025-02-12 22:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:19][root][INFO] - Training Epoch: 2/2, step 387/574 completed (loss: 0.02035115472972393, acc: 1.0)
[2025-02-12 22:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:19][root][INFO] - Training Epoch: 2/2, step 388/574 completed (loss: 0.017614252865314484, acc: 1.0)
[2025-02-12 22:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:20][root][INFO] - Training Epoch: 2/2, step 389/574 completed (loss: 0.004161695018410683, acc: 1.0)
[2025-02-12 22:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:20][root][INFO] - Training Epoch: 2/2, step 390/574 completed (loss: 0.2662311792373657, acc: 0.9047619104385376)
[2025-02-12 22:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:20][root][INFO] - Training Epoch: 2/2, step 391/574 completed (loss: 1.1490591764450073, acc: 0.6666666865348816)
[2025-02-12 22:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:20][root][INFO] - Training Epoch: 2/2, step 392/574 completed (loss: 0.9112418293952942, acc: 0.7572815418243408)
[2025-02-12 22:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:21][root][INFO] - Training Epoch: 2/2, step 393/574 completed (loss: 1.0052626132965088, acc: 0.7867646813392639)
[2025-02-12 22:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:21][root][INFO] - Training Epoch: 2/2, step 394/574 completed (loss: 0.7354065775871277, acc: 0.7933333516120911)
[2025-02-12 22:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:22][root][INFO] - Training Epoch: 2/2, step 395/574 completed (loss: 0.7708996534347534, acc: 0.7777777910232544)
[2025-02-12 22:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:22][root][INFO] - Training Epoch: 2/2, step 396/574 completed (loss: 0.654792308807373, acc: 0.7906976938247681)
[2025-02-12 22:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:22][root][INFO] - Training Epoch: 2/2, step 397/574 completed (loss: 0.23174989223480225, acc: 0.9166666865348816)
[2025-02-12 22:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:23][root][INFO] - Training Epoch: 2/2, step 398/574 completed (loss: 0.3617004454135895, acc: 0.8837209343910217)
[2025-02-12 22:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:23][root][INFO] - Training Epoch: 2/2, step 399/574 completed (loss: 0.17894242703914642, acc: 0.9599999785423279)
[2025-02-12 22:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:23][root][INFO] - Training Epoch: 2/2, step 400/574 completed (loss: 0.3703860342502594, acc: 0.8970588445663452)
[2025-02-12 22:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:24][root][INFO] - Training Epoch: 2/2, step 401/574 completed (loss: 0.5292660593986511, acc: 0.8666666746139526)
[2025-02-12 22:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:24][root][INFO] - Training Epoch: 2/2, step 402/574 completed (loss: 0.41565656661987305, acc: 0.8484848737716675)
[2025-02-12 22:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:24][root][INFO] - Training Epoch: 2/2, step 403/574 completed (loss: 0.21232549846172333, acc: 0.9090909361839294)
[2025-02-12 22:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:25][root][INFO] - Training Epoch: 2/2, step 404/574 completed (loss: 0.12917354702949524, acc: 0.9677419066429138)
[2025-02-12 22:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:25][root][INFO] - Training Epoch: 2/2, step 405/574 completed (loss: 0.16132748126983643, acc: 0.9629629850387573)
[2025-02-12 22:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:25][root][INFO] - Training Epoch: 2/2, step 406/574 completed (loss: 0.022777937352657318, acc: 1.0)
[2025-02-12 22:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:26][root][INFO] - Training Epoch: 2/2, step 407/574 completed (loss: 0.0408414863049984, acc: 1.0)
[2025-02-12 22:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:26][root][INFO] - Training Epoch: 2/2, step 408/574 completed (loss: 0.07246748358011246, acc: 1.0)
[2025-02-12 22:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:26][root][INFO] - Training Epoch: 2/2, step 409/574 completed (loss: 0.05415854975581169, acc: 1.0)
[2025-02-12 22:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:27][root][INFO] - Training Epoch: 2/2, step 410/574 completed (loss: 0.15427479147911072, acc: 0.9482758641242981)
[2025-02-12 22:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:27][root][INFO] - Training Epoch: 2/2, step 411/574 completed (loss: 0.035646598786115646, acc: 1.0)
[2025-02-12 22:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:27][root][INFO] - Training Epoch: 2/2, step 412/574 completed (loss: 0.06741496920585632, acc: 1.0)
[2025-02-12 22:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:28][root][INFO] - Training Epoch: 2/2, step 413/574 completed (loss: 0.18892718851566315, acc: 0.9696969985961914)
[2025-02-12 22:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:28][root][INFO] - Training Epoch: 2/2, step 414/574 completed (loss: 0.03927699103951454, acc: 1.0)
[2025-02-12 22:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:28][root][INFO] - Training Epoch: 2/2, step 415/574 completed (loss: 0.28960996866226196, acc: 0.9215686321258545)
[2025-02-12 22:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:29][root][INFO] - Training Epoch: 2/2, step 416/574 completed (loss: 0.20860902965068817, acc: 0.9230769276618958)
[2025-02-12 22:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:29][root][INFO] - Training Epoch: 2/2, step 417/574 completed (loss: 0.23879525065422058, acc: 0.8888888955116272)
[2025-02-12 22:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:29][root][INFO] - Training Epoch: 2/2, step 418/574 completed (loss: 0.2504664659500122, acc: 0.8999999761581421)
[2025-02-12 22:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:30][root][INFO] - Training Epoch: 2/2, step 419/574 completed (loss: 0.22464290261268616, acc: 0.949999988079071)
[2025-02-12 22:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:30][root][INFO] - Training Epoch: 2/2, step 420/574 completed (loss: 0.09762685745954514, acc: 1.0)
[2025-02-12 22:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:30][root][INFO] - Training Epoch: 2/2, step 421/574 completed (loss: 0.22703711688518524, acc: 0.9666666388511658)
[2025-02-12 22:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:31][root][INFO] - Training Epoch: 2/2, step 422/574 completed (loss: 0.17616984248161316, acc: 0.9375)
[2025-02-12 22:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:31][root][INFO] - Training Epoch: 2/2, step 423/574 completed (loss: 0.3393907845020294, acc: 0.9166666865348816)
[2025-02-12 22:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:31][root][INFO] - Training Epoch: 2/2, step 424/574 completed (loss: 0.38032910227775574, acc: 0.9259259104728699)
[2025-02-12 22:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:32][root][INFO] - Training Epoch: 2/2, step 425/574 completed (loss: 0.10420060902833939, acc: 0.9696969985961914)
[2025-02-12 22:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:32][root][INFO] - Training Epoch: 2/2, step 426/574 completed (loss: 0.010190128348767757, acc: 1.0)
[2025-02-12 22:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:03][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7077, device='cuda:0') eval_epoch_loss=tensor(0.5351, device='cuda:0') eval_epoch_acc=tensor(0.8587, device='cuda:0')
[2025-02-12 22:08:03][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:08:03][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:08:03][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.5351276993751526/model.pt
[2025-02-12 22:08:03][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:08:03][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8586670756340027
[2025-02-12 22:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:04][root][INFO] - Training Epoch: 2/2, step 427/574 completed (loss: 0.05656805634498596, acc: 1.0)
[2025-02-12 22:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:04][root][INFO] - Training Epoch: 2/2, step 428/574 completed (loss: 0.03479184955358505, acc: 1.0)
[2025-02-12 22:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:04][root][INFO] - Training Epoch: 2/2, step 429/574 completed (loss: 0.038526203483343124, acc: 1.0)
[2025-02-12 22:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:05][root][INFO] - Training Epoch: 2/2, step 430/574 completed (loss: 0.002313123783096671, acc: 1.0)
[2025-02-12 22:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:05][root][INFO] - Training Epoch: 2/2, step 431/574 completed (loss: 0.0066916681826114655, acc: 1.0)
[2025-02-12 22:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:05][root][INFO] - Training Epoch: 2/2, step 432/574 completed (loss: 0.05832420662045479, acc: 0.95652174949646)
[2025-02-12 22:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:06][root][INFO] - Training Epoch: 2/2, step 433/574 completed (loss: 0.1337057203054428, acc: 0.9444444179534912)
[2025-02-12 22:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:06][root][INFO] - Training Epoch: 2/2, step 434/574 completed (loss: 0.0017706049839034677, acc: 1.0)
[2025-02-12 22:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:06][root][INFO] - Training Epoch: 2/2, step 435/574 completed (loss: 0.07274425774812698, acc: 0.9696969985961914)
[2025-02-12 22:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:07][root][INFO] - Training Epoch: 2/2, step 436/574 completed (loss: 0.19466520845890045, acc: 0.9444444179534912)
[2025-02-12 22:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:07][root][INFO] - Training Epoch: 2/2, step 437/574 completed (loss: 0.01695832796394825, acc: 1.0)
[2025-02-12 22:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:07][root][INFO] - Training Epoch: 2/2, step 438/574 completed (loss: 0.04643113166093826, acc: 0.9523809552192688)
[2025-02-12 22:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:08][root][INFO] - Training Epoch: 2/2, step 439/574 completed (loss: 0.6004610061645508, acc: 0.8717948794364929)
[2025-02-12 22:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:08][root][INFO] - Training Epoch: 2/2, step 440/574 completed (loss: 0.4819847047328949, acc: 0.9090909361839294)
[2025-02-12 22:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:09][root][INFO] - Training Epoch: 2/2, step 441/574 completed (loss: 0.8016440272331238, acc: 0.7519999742507935)
[2025-02-12 22:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:09][root][INFO] - Training Epoch: 2/2, step 442/574 completed (loss: 0.7703751921653748, acc: 0.7822580933570862)
[2025-02-12 22:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:10][root][INFO] - Training Epoch: 2/2, step 443/574 completed (loss: 0.4884049594402313, acc: 0.8606964945793152)
[2025-02-12 22:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:10][root][INFO] - Training Epoch: 2/2, step 444/574 completed (loss: 0.09783420711755753, acc: 0.9811320900917053)
[2025-02-12 22:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:11][root][INFO] - Training Epoch: 2/2, step 445/574 completed (loss: 0.12046322226524353, acc: 0.9545454382896423)
[2025-02-12 22:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:11][root][INFO] - Training Epoch: 2/2, step 446/574 completed (loss: 0.2081121802330017, acc: 0.95652174949646)
[2025-02-12 22:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:11][root][INFO] - Training Epoch: 2/2, step 447/574 completed (loss: 0.24258239567279816, acc: 0.9615384340286255)
[2025-02-12 22:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:12][root][INFO] - Training Epoch: 2/2, step 448/574 completed (loss: 0.04636240378022194, acc: 0.9642857313156128)
[2025-02-12 22:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:12][root][INFO] - Training Epoch: 2/2, step 449/574 completed (loss: 0.1596890240907669, acc: 0.9402984976768494)
[2025-02-12 22:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:12][root][INFO] - Training Epoch: 2/2, step 450/574 completed (loss: 0.05674805864691734, acc: 0.9861111044883728)
[2025-02-12 22:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:13][root][INFO] - Training Epoch: 2/2, step 451/574 completed (loss: 0.055911142379045486, acc: 0.967391312122345)
[2025-02-12 22:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:13][root][INFO] - Training Epoch: 2/2, step 452/574 completed (loss: 0.2545263171195984, acc: 0.9358974099159241)
[2025-02-12 22:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:13][root][INFO] - Training Epoch: 2/2, step 453/574 completed (loss: 0.29190585017204285, acc: 0.8947368264198303)
[2025-02-12 22:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:13][root][INFO] - Training Epoch: 2/2, step 454/574 completed (loss: 0.18054409325122833, acc: 0.9387755393981934)
[2025-02-12 22:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:14][root][INFO] - Training Epoch: 2/2, step 455/574 completed (loss: 0.21464812755584717, acc: 0.939393937587738)
[2025-02-12 22:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:14][root][INFO] - Training Epoch: 2/2, step 456/574 completed (loss: 0.5940864682197571, acc: 0.8659793734550476)
[2025-02-12 22:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:14][root][INFO] - Training Epoch: 2/2, step 457/574 completed (loss: 0.0697525143623352, acc: 0.9714285731315613)
[2025-02-12 22:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:15][root][INFO] - Training Epoch: 2/2, step 458/574 completed (loss: 0.3054189383983612, acc: 0.9011628031730652)
[2025-02-12 22:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:15][root][INFO] - Training Epoch: 2/2, step 459/574 completed (loss: 0.06920095533132553, acc: 0.9821428656578064)
[2025-02-12 22:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:15][root][INFO] - Training Epoch: 2/2, step 460/574 completed (loss: 0.1675574779510498, acc: 0.9506173133850098)
[2025-02-12 22:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:16][root][INFO] - Training Epoch: 2/2, step 461/574 completed (loss: 0.25090491771698, acc: 0.9166666865348816)
[2025-02-12 22:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:16][root][INFO] - Training Epoch: 2/2, step 462/574 completed (loss: 0.045590590685606, acc: 0.96875)
[2025-02-12 22:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:16][root][INFO] - Training Epoch: 2/2, step 463/574 completed (loss: 0.3326251208782196, acc: 0.9230769276618958)
[2025-02-12 22:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:17][root][INFO] - Training Epoch: 2/2, step 464/574 completed (loss: 0.13515432178974152, acc: 0.97826087474823)
[2025-02-12 22:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:17][root][INFO] - Training Epoch: 2/2, step 465/574 completed (loss: 0.4068399667739868, acc: 0.8690476417541504)
[2025-02-12 22:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:17][root][INFO] - Training Epoch: 2/2, step 466/574 completed (loss: 0.5269102454185486, acc: 0.8674699068069458)
[2025-02-12 22:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:18][root][INFO] - Training Epoch: 2/2, step 467/574 completed (loss: 0.22186443209648132, acc: 0.9189189076423645)
[2025-02-12 22:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:18][root][INFO] - Training Epoch: 2/2, step 468/574 completed (loss: 0.5379582643508911, acc: 0.893203854560852)
[2025-02-12 22:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:18][root][INFO] - Training Epoch: 2/2, step 469/574 completed (loss: 0.398954838514328, acc: 0.8943089246749878)
[2025-02-12 22:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:19][root][INFO] - Training Epoch: 2/2, step 470/574 completed (loss: 0.0767725482583046, acc: 1.0)
[2025-02-12 22:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:19][root][INFO] - Training Epoch: 2/2, step 471/574 completed (loss: 0.29687970876693726, acc: 0.8928571343421936)
[2025-02-12 22:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:19][root][INFO] - Training Epoch: 2/2, step 472/574 completed (loss: 0.6345800757408142, acc: 0.7941176295280457)
[2025-02-12 22:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:20][root][INFO] - Training Epoch: 2/2, step 473/574 completed (loss: 0.7481784820556641, acc: 0.8165938854217529)
[2025-02-12 22:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:20][root][INFO] - Training Epoch: 2/2, step 474/574 completed (loss: 0.6550170183181763, acc: 0.8229166865348816)
[2025-02-12 22:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:20][root][INFO] - Training Epoch: 2/2, step 475/574 completed (loss: 0.40656688809394836, acc: 0.8650306463241577)
[2025-02-12 22:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:21][root][INFO] - Training Epoch: 2/2, step 476/574 completed (loss: 0.5497060418128967, acc: 0.8201438784599304)
[2025-02-12 22:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:21][root][INFO] - Training Epoch: 2/2, step 477/574 completed (loss: 0.7775352001190186, acc: 0.7638190984725952)
[2025-02-12 22:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:21][root][INFO] - Training Epoch: 2/2, step 478/574 completed (loss: 0.6100679039955139, acc: 0.8611111044883728)
[2025-02-12 22:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:22][root][INFO] - Training Epoch: 2/2, step 479/574 completed (loss: 0.6603885889053345, acc: 0.7878788113594055)
[2025-02-12 22:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:22][root][INFO] - Training Epoch: 2/2, step 480/574 completed (loss: 0.4433930814266205, acc: 0.8518518805503845)
[2025-02-12 22:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:22][root][INFO] - Training Epoch: 2/2, step 481/574 completed (loss: 0.2158162146806717, acc: 0.949999988079071)
[2025-02-12 22:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:23][root][INFO] - Training Epoch: 2/2, step 482/574 completed (loss: 0.45661744475364685, acc: 0.8999999761581421)
[2025-02-12 22:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:23][root][INFO] - Training Epoch: 2/2, step 483/574 completed (loss: 0.5877096056938171, acc: 0.8103448152542114)
[2025-02-12 22:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:23][root][INFO] - Training Epoch: 2/2, step 484/574 completed (loss: 0.1274872124195099, acc: 0.9677419066429138)
[2025-02-12 22:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:24][root][INFO] - Training Epoch: 2/2, step 485/574 completed (loss: 0.1355217695236206, acc: 0.9473684430122375)
[2025-02-12 22:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:24][root][INFO] - Training Epoch: 2/2, step 486/574 completed (loss: 0.6321579217910767, acc: 0.7777777910232544)
[2025-02-12 22:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:24][root][INFO] - Training Epoch: 2/2, step 487/574 completed (loss: 0.4399246573448181, acc: 0.9047619104385376)
[2025-02-12 22:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:24][root][INFO] - Training Epoch: 2/2, step 488/574 completed (loss: 0.30871617794036865, acc: 0.9090909361839294)
[2025-02-12 22:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:25][root][INFO] - Training Epoch: 2/2, step 489/574 completed (loss: 0.8162145018577576, acc: 0.800000011920929)
[2025-02-12 22:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:25][root][INFO] - Training Epoch: 2/2, step 490/574 completed (loss: 0.1370909959077835, acc: 0.9666666388511658)
[2025-02-12 22:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:25][root][INFO] - Training Epoch: 2/2, step 491/574 completed (loss: 0.39796286821365356, acc: 0.8965517282485962)
[2025-02-12 22:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:26][root][INFO] - Training Epoch: 2/2, step 492/574 completed (loss: 0.43103304505348206, acc: 0.8627451062202454)
[2025-02-12 22:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:26][root][INFO] - Training Epoch: 2/2, step 493/574 completed (loss: 0.22089961171150208, acc: 0.9655172228813171)
[2025-02-12 22:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:26][root][INFO] - Training Epoch: 2/2, step 494/574 completed (loss: 0.13425035774707794, acc: 1.0)
[2025-02-12 22:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:27][root][INFO] - Training Epoch: 2/2, step 495/574 completed (loss: 0.8619383573532104, acc: 0.7894737124443054)
[2025-02-12 22:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:27][root][INFO] - Training Epoch: 2/2, step 496/574 completed (loss: 0.6562634110450745, acc: 0.8303571343421936)
[2025-02-12 22:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:27][root][INFO] - Training Epoch: 2/2, step 497/574 completed (loss: 0.43168288469314575, acc: 0.898876428604126)
[2025-02-12 22:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:28][root][INFO] - Training Epoch: 2/2, step 498/574 completed (loss: 0.734905481338501, acc: 0.7752808928489685)
[2025-02-12 22:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:28][root][INFO] - Training Epoch: 2/2, step 499/574 completed (loss: 1.1319085359573364, acc: 0.6453900933265686)
[2025-02-12 22:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:28][root][INFO] - Training Epoch: 2/2, step 500/574 completed (loss: 0.6508980393409729, acc: 0.8152173757553101)
[2025-02-12 22:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:29][root][INFO] - Training Epoch: 2/2, step 501/574 completed (loss: 0.008028170093894005, acc: 1.0)
[2025-02-12 22:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:29][root][INFO] - Training Epoch: 2/2, step 502/574 completed (loss: 0.019062228500843048, acc: 1.0)
[2025-02-12 22:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:29][root][INFO] - Training Epoch: 2/2, step 503/574 completed (loss: 0.12278705835342407, acc: 0.9629629850387573)
[2025-02-12 22:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:30][root][INFO] - Training Epoch: 2/2, step 504/574 completed (loss: 0.029645977541804314, acc: 1.0)
[2025-02-12 22:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:30][root][INFO] - Training Epoch: 2/2, step 505/574 completed (loss: 0.5117011666297913, acc: 0.9056603908538818)
[2025-02-12 22:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:30][root][INFO] - Training Epoch: 2/2, step 506/574 completed (loss: 0.45775580406188965, acc: 0.8620689511299133)
[2025-02-12 22:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:31][root][INFO] - Training Epoch: 2/2, step 507/574 completed (loss: 0.8476539850234985, acc: 0.7477477192878723)
[2025-02-12 22:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:31][root][INFO] - Training Epoch: 2/2, step 508/574 completed (loss: 0.43245336413383484, acc: 0.8732394576072693)
[2025-02-12 22:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:32][root][INFO] - Training Epoch: 2/2, step 509/574 completed (loss: 0.08505485951900482, acc: 0.949999988079071)
[2025-02-12 22:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:32][root][INFO] - Training Epoch: 2/2, step 510/574 completed (loss: 0.06983506679534912, acc: 0.9666666388511658)
[2025-02-12 22:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:32][root][INFO] - Training Epoch: 2/2, step 511/574 completed (loss: 0.3776351809501648, acc: 0.8846153616905212)
[2025-02-12 22:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:35][root][INFO] - Training Epoch: 2/2, step 512/574 completed (loss: 0.8713735938072205, acc: 0.7428571581840515)
[2025-02-12 22:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:35][root][INFO] - Training Epoch: 2/2, step 513/574 completed (loss: 0.20979098975658417, acc: 0.9126983880996704)
[2025-02-12 22:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:36][root][INFO] - Training Epoch: 2/2, step 514/574 completed (loss: 0.6381068229675293, acc: 0.8571428656578064)
[2025-02-12 22:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:36][root][INFO] - Training Epoch: 2/2, step 515/574 completed (loss: 0.12118766456842422, acc: 0.9666666388511658)
[2025-02-12 22:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:37][root][INFO] - Training Epoch: 2/2, step 516/574 completed (loss: 0.45740053057670593, acc: 0.8333333134651184)
[2025-02-12 22:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:37][root][INFO] - Training Epoch: 2/2, step 517/574 completed (loss: 0.008763555437326431, acc: 1.0)
[2025-02-12 22:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:38][root][INFO] - Training Epoch: 2/2, step 518/574 completed (loss: 0.04563615098595619, acc: 1.0)
[2025-02-12 22:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:38][root][INFO] - Training Epoch: 2/2, step 519/574 completed (loss: 0.2660158574581146, acc: 0.8999999761581421)
[2025-02-12 22:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:38][root][INFO] - Training Epoch: 2/2, step 520/574 completed (loss: 0.30441415309906006, acc: 0.9259259104728699)
[2025-02-12 22:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:39][root][INFO] - Training Epoch: 2/2, step 521/574 completed (loss: 0.6384173631668091, acc: 0.8008474707603455)
[2025-02-12 22:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:40][root][INFO] - Training Epoch: 2/2, step 522/574 completed (loss: 0.3834778368473053, acc: 0.89552241563797)
[2025-02-12 22:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:40][root][INFO] - Training Epoch: 2/2, step 523/574 completed (loss: 0.3460122048854828, acc: 0.8759124279022217)
[2025-02-12 22:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:40][root][INFO] - Training Epoch: 2/2, step 524/574 completed (loss: 0.5789595246315002, acc: 0.8399999737739563)
[2025-02-12 22:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:41][root][INFO] - Training Epoch: 2/2, step 525/574 completed (loss: 0.04352260380983353, acc: 1.0)
[2025-02-12 22:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:41][root][INFO] - Training Epoch: 2/2, step 526/574 completed (loss: 0.22602325677871704, acc: 0.9038461446762085)
[2025-02-12 22:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:41][root][INFO] - Training Epoch: 2/2, step 527/574 completed (loss: 0.4249337911605835, acc: 0.8571428656578064)
[2025-02-12 22:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:42][root][INFO] - Training Epoch: 2/2, step 528/574 completed (loss: 1.361728549003601, acc: 0.5737704634666443)
[2025-02-12 22:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:42][root][INFO] - Training Epoch: 2/2, step 529/574 completed (loss: 0.41196170449256897, acc: 0.8813559412956238)
[2025-02-12 22:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:42][root][INFO] - Training Epoch: 2/2, step 530/574 completed (loss: 0.9996559619903564, acc: 0.7441860437393188)
[2025-02-12 22:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:43][root][INFO] - Training Epoch: 2/2, step 531/574 completed (loss: 0.4931214153766632, acc: 0.8636363744735718)
[2025-02-12 22:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:43][root][INFO] - Training Epoch: 2/2, step 532/574 completed (loss: 0.7571500539779663, acc: 0.7735849022865295)
[2025-02-12 22:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:43][root][INFO] - Training Epoch: 2/2, step 533/574 completed (loss: 0.5478236675262451, acc: 0.8409090638160706)
[2025-02-12 22:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:44][root][INFO] - Training Epoch: 2/2, step 534/574 completed (loss: 0.4793720543384552, acc: 0.8399999737739563)
[2025-02-12 22:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:44][root][INFO] - Training Epoch: 2/2, step 535/574 completed (loss: 0.41976016759872437, acc: 0.8500000238418579)
[2025-02-12 22:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:44][root][INFO] - Training Epoch: 2/2, step 536/574 completed (loss: 0.07515162974596024, acc: 1.0)
[2025-02-12 22:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:45][root][INFO] - Training Epoch: 2/2, step 537/574 completed (loss: 0.5776805877685547, acc: 0.8615384697914124)
[2025-02-12 22:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:45][root][INFO] - Training Epoch: 2/2, step 538/574 completed (loss: 0.34354594349861145, acc: 0.921875)
[2025-02-12 22:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:46][root][INFO] - Training Epoch: 2/2, step 539/574 completed (loss: 0.340869277715683, acc: 0.90625)
[2025-02-12 22:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:46][root][INFO] - Training Epoch: 2/2, step 540/574 completed (loss: 0.2923389673233032, acc: 0.9090909361839294)
[2025-02-12 22:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:46][root][INFO] - Training Epoch: 2/2, step 541/574 completed (loss: 0.16302786767482758, acc: 0.875)
[2025-02-12 22:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:47][root][INFO] - Training Epoch: 2/2, step 542/574 completed (loss: 0.11688351631164551, acc: 0.9354838728904724)
[2025-02-12 22:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:47][root][INFO] - Training Epoch: 2/2, step 543/574 completed (loss: 0.03467293828725815, acc: 1.0)
[2025-02-12 22:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:47][root][INFO] - Training Epoch: 2/2, step 544/574 completed (loss: 0.06590234488248825, acc: 0.9666666388511658)
[2025-02-12 22:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:48][root][INFO] - Training Epoch: 2/2, step 545/574 completed (loss: 0.11888839304447174, acc: 0.9756097793579102)
[2025-02-12 22:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:48][root][INFO] - Training Epoch: 2/2, step 546/574 completed (loss: 0.023654455319046974, acc: 1.0)
[2025-02-12 22:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:48][root][INFO] - Training Epoch: 2/2, step 547/574 completed (loss: 0.029359964653849602, acc: 0.9736841917037964)
[2025-02-12 22:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:49][root][INFO] - Training Epoch: 2/2, step 548/574 completed (loss: 0.05656478554010391, acc: 1.0)
[2025-02-12 22:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:49][root][INFO] - Training Epoch: 2/2, step 549/574 completed (loss: 0.009645842015743256, acc: 1.0)
[2025-02-12 22:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:49][root][INFO] - Training Epoch: 2/2, step 550/574 completed (loss: 0.3065587282180786, acc: 0.8787878751754761)
[2025-02-12 22:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:50][root][INFO] - Training Epoch: 2/2, step 551/574 completed (loss: 0.12815715372562408, acc: 0.9750000238418579)
[2025-02-12 22:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:50][root][INFO] - Training Epoch: 2/2, step 552/574 completed (loss: 0.1493968814611435, acc: 0.9428571462631226)
[2025-02-12 22:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:50][root][INFO] - Training Epoch: 2/2, step 553/574 completed (loss: 0.4175921082496643, acc: 0.8759124279022217)
[2025-02-12 22:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:51][root][INFO] - Training Epoch: 2/2, step 554/574 completed (loss: 0.17920657992362976, acc: 0.931034505367279)
[2025-02-12 22:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:51][root][INFO] - Training Epoch: 2/2, step 555/574 completed (loss: 0.2794310450553894, acc: 0.9142857193946838)
[2025-02-12 22:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:51][root][INFO] - Training Epoch: 2/2, step 556/574 completed (loss: 0.48230940103530884, acc: 0.8940397500991821)
[2025-02-12 22:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:52][root][INFO] - Training Epoch: 2/2, step 557/574 completed (loss: 0.2661866843700409, acc: 0.9145299196243286)
[2025-02-12 22:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:52][root][INFO] - Training Epoch: 2/2, step 558/574 completed (loss: 0.07797405868768692, acc: 1.0)
[2025-02-12 22:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:52][root][INFO] - Training Epoch: 2/2, step 559/574 completed (loss: 0.1394701898097992, acc: 0.9230769276618958)
[2025-02-12 22:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:53][root][INFO] - Training Epoch: 2/2, step 560/574 completed (loss: 0.06202905997633934, acc: 1.0)
[2025-02-12 22:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:53][root][INFO] - Training Epoch: 2/2, step 561/574 completed (loss: 0.042897846549749374, acc: 1.0)
[2025-02-12 22:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:53][root][INFO] - Training Epoch: 2/2, step 562/574 completed (loss: 0.4327075481414795, acc: 0.8444444537162781)
[2025-02-12 22:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:54][root][INFO] - Training Epoch: 2/2, step 563/574 completed (loss: 0.32117733359336853, acc: 0.8961039185523987)
[2025-02-12 22:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:54][root][INFO] - Training Epoch: 2/2, step 564/574 completed (loss: 0.23630493879318237, acc: 0.9166666865348816)
[2025-02-12 22:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:54][root][INFO] - Training Epoch: 2/2, step 565/574 completed (loss: 0.2583003342151642, acc: 0.9137930870056152)
[2025-02-12 22:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:55][root][INFO] - Training Epoch: 2/2, step 566/574 completed (loss: 0.29658398032188416, acc: 0.9166666865348816)
[2025-02-12 22:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:55][root][INFO] - Training Epoch: 2/2, step 567/574 completed (loss: 0.025827959179878235, acc: 1.0)
[2025-02-12 22:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:55][root][INFO] - Training Epoch: 2/2, step 568/574 completed (loss: 0.014790809713304043, acc: 1.0)
[2025-02-12 22:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:56][root][INFO] - Training Epoch: 2/2, step 569/574 completed (loss: 0.1838274896144867, acc: 0.9786096215248108)
[2025-02-12 22:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:26][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7474, device='cuda:0') eval_epoch_loss=tensor(0.5581, device='cuda:0') eval_epoch_acc=tensor(0.8477, device='cuda:0')
[2025-02-12 22:09:26][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-12 22:09:26][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-12 22:09:26][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_570_loss_0.5581279993057251/model.pt
[2025-02-12 22:09:26][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-02-12 22:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:27][root][INFO] - Training Epoch: 2/2, step 570/574 completed (loss: 0.043785177171230316, acc: 0.9838709831237793)
[2025-02-12 22:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:27][root][INFO] - Training Epoch: 2/2, step 571/574 completed (loss: 0.20440471172332764, acc: 0.94017094373703)
[2025-02-12 22:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:27][root][INFO] - Training Epoch: 2/2, step 572/574 completed (loss: 0.3082229197025299, acc: 0.9336734414100647)
[2025-02-12 22:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:09:28][root][INFO] - Training Epoch: 2/2, step 573/574 completed (loss: 0.3126852512359619, acc: 0.9245283007621765)
[2025-02-12 22:09:28][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.6215, train_epoch_loss=0.4834, epoch time 348.11213466897607s
[2025-02-12 22:09:28][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-12 22:09:28][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-02-12 22:09:28][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-12 22:09:28][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-02-12 22:09:28][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-12 22:09:28][root][INFO] - Key: avg_train_prep, Value: 2.1819686889648438
[2025-02-12 22:09:28][root][INFO] - Key: avg_train_loss, Value: 0.7461016178131104
[2025-02-12 22:09:28][root][INFO] - Key: avg_train_acc, Value: 0.816173791885376
[2025-02-12 22:09:28][root][INFO] - Key: avg_eval_prep, Value: 1.9366310834884644
[2025-02-12 22:09:28][root][INFO] - Key: avg_eval_loss, Value: 0.6432064175605774
[2025-02-12 22:09:28][root][INFO] - Key: avg_eval_acc, Value: 0.8323109149932861
[2025-02-12 22:09:28][root][INFO] - Key: avg_epoch_time, Value: 357.2735584229231
[2025-02-12 22:09:28][root][INFO] - Key: avg_checkpoint_time, Value: 0.33076154021546245
Selected lowest loss checkpoint: asr_epoch_2_step_284_loss_0.5118914246559143
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5118914246559143/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5118914246559143
[2025-02-12 22:10:02][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-12 22:10:02][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-12 22:10:02][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-02-12 22:10:04][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-12 22:10:08][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-12 22:10:08][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-12 22:10:08][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-12 22:10:08][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-12 22:10:12][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-12 22:10:12][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-12 22:10:12][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-12 22:10:12][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-12 22:10:12][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-12 22:10:13][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-12 22:10:13][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-12 22:10:13][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5118914246559143/model.pt
[2025-02-12 22:10:13][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-12 22:10:13][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-12 22:10:14][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-12 22:10:16][root][INFO] - --> Training Set Length = 652
[2025-02-12 22:10:16][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/repetition_penalty.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-02-12 22:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-12 22:12:54][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/decode_test_beam4_pred_20250212_221016
[2025-02-12 22:12:54][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/decode_test_beam4_gt_20250212_221016
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/decode_test_beam4_gt_20250212_221016
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/decode_test_beam4_pred_20250212_221016
Combined WER: 0.32717557251908397

Filtering repeated words...

Found 1 repeated lines in total.
Repeated lines are:
- AH N D IH T S IH T S IH T S AH HH AO R IY K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA
Filtered Combined WER: 0.30056826908309014
