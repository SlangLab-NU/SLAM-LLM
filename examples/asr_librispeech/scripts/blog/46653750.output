/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
train_data_folder: ami_phoneme
test_data_folder: ami_phoneme
use_peft: true
seed: 
debug: 
Is test_run? 
freeze_encoder: true
Is save_embedding? false
projector_transfer_learning: true
transfer_data_folder: librispeech-100_phoneme
llm_inference_config: repetition_penalty
eval_ckpt: best
----------
----------
Final identifier: ami_phoneme_wavlm_llama32_1b_linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.22486534714698792



----- Transfer Learning Information -----
Resume Epoch: 1
Resume Step: 0
Train Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/train.jsonl
Validation Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/validation.jsonl
Test Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test.jsonl
Identifier: ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme
Output Directory: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme
----------------------------------------
----------------------------------------
Resume epoch: 1
Resume step: 0
[2025-02-13 02:24:01][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-13 02:24:01][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 02:24:01][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme'}
[2025-02-13 02:24:01][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-13_02-24-01.txt', 'log_interval': 5}
[2025-02-13 02:24:25][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 02:24:31][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 02:24:31][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 02:24:31][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 02:24:31][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 02:24:39][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 02:24:39][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 02:24:39][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 02:24:40][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 02:24:40][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 02:24:40][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 02:24:40][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 02:24:40][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.22486534714698792/model.pt
[2025-02-13 02:24:40][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 02:24:40][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 02:24:44][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 02:24:47][root][INFO] - --> Training Set Length = 28539
[2025-02-13 02:24:47][root][INFO] - --> Validation Set Length = 2703
[2025-02-13 02:24:47][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 02:24:47][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 02:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:51][root][INFO] - Training Epoch: 1/2, step 0/7134 completed (loss: 2.3707823753356934, acc: 0.7132243514060974)
[2025-02-13 02:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:52][root][INFO] - Training Epoch: 1/2, step 1/7134 completed (loss: 2.198115110397339, acc: 0.7545327544212341)
[2025-02-13 02:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:52][root][INFO] - Training Epoch: 1/2, step 2/7134 completed (loss: 2.26556396484375, acc: 0.721552848815918)
[2025-02-13 02:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:53][root][INFO] - Training Epoch: 1/2, step 3/7134 completed (loss: 2.1062090396881104, acc: 0.7362356185913086)
[2025-02-13 02:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:53][root][INFO] - Training Epoch: 1/2, step 4/7134 completed (loss: 2.143186569213867, acc: 0.7435197830200195)
[2025-02-13 02:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:54][root][INFO] - Training Epoch: 1/2, step 5/7134 completed (loss: 2.2224347591400146, acc: 0.7309644818305969)
[2025-02-13 02:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:54][root][INFO] - Training Epoch: 1/2, step 6/7134 completed (loss: 2.2299492359161377, acc: 0.7191011309623718)
[2025-02-13 02:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:55][root][INFO] - Training Epoch: 1/2, step 7/7134 completed (loss: 2.1155922412872314, acc: 0.7334993481636047)
[2025-02-13 02:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:55][root][INFO] - Training Epoch: 1/2, step 8/7134 completed (loss: 2.105586290359497, acc: 0.7317743897438049)
[2025-02-13 02:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:56][root][INFO] - Training Epoch: 1/2, step 9/7134 completed (loss: 2.382833242416382, acc: 0.7286821603775024)
[2025-02-13 02:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:56][root][INFO] - Training Epoch: 1/2, step 10/7134 completed (loss: 2.325103521347046, acc: 0.7283072471618652)
[2025-02-13 02:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:57][root][INFO] - Training Epoch: 1/2, step 11/7134 completed (loss: 2.2768635749816895, acc: 0.7354596853256226)
[2025-02-13 02:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:57][root][INFO] - Training Epoch: 1/2, step 12/7134 completed (loss: 2.2448694705963135, acc: 0.7335127592086792)
[2025-02-13 02:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:58][root][INFO] - Training Epoch: 1/2, step 13/7134 completed (loss: 2.1307172775268555, acc: 0.7317743897438049)
[2025-02-13 02:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:58][root][INFO] - Training Epoch: 1/2, step 14/7134 completed (loss: 2.401660203933716, acc: 0.7243697643280029)
[2025-02-13 02:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:59][root][INFO] - Training Epoch: 1/2, step 15/7134 completed (loss: 2.3555376529693604, acc: 0.7280550599098206)
[2025-02-13 02:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:24:59][root][INFO] - Training Epoch: 1/2, step 16/7134 completed (loss: 2.163170099258423, acc: 0.7479674816131592)
[2025-02-13 02:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:00][root][INFO] - Training Epoch: 1/2, step 17/7134 completed (loss: 2.5924534797668457, acc: 0.7162162065505981)
[2025-02-13 02:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:00][root][INFO] - Training Epoch: 1/2, step 18/7134 completed (loss: 2.22113299369812, acc: 0.7329545617103577)
[2025-02-13 02:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:01][root][INFO] - Training Epoch: 1/2, step 19/7134 completed (loss: 2.1757140159606934, acc: 0.7310344576835632)
[2025-02-13 02:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:01][root][INFO] - Training Epoch: 1/2, step 20/7134 completed (loss: 2.102932929992676, acc: 0.7447368502616882)
[2025-02-13 02:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:02][root][INFO] - Training Epoch: 1/2, step 21/7134 completed (loss: 1.8738521337509155, acc: 0.7358247637748718)
[2025-02-13 02:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:02][root][INFO] - Training Epoch: 1/2, step 22/7134 completed (loss: 2.0682907104492188, acc: 0.7435158491134644)
[2025-02-13 02:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:02][root][INFO] - Training Epoch: 1/2, step 23/7134 completed (loss: 2.0401830673217773, acc: 0.7241848111152649)
[2025-02-13 02:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:03][root][INFO] - Training Epoch: 1/2, step 24/7134 completed (loss: 2.0442850589752197, acc: 0.7158774137496948)
[2025-02-13 02:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:03][root][INFO] - Training Epoch: 1/2, step 25/7134 completed (loss: 1.9817862510681152, acc: 0.7301136255264282)
[2025-02-13 02:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:04][root][INFO] - Training Epoch: 1/2, step 26/7134 completed (loss: 2.0136561393737793, acc: 0.721238911151886)
[2025-02-13 02:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:04][root][INFO] - Training Epoch: 1/2, step 27/7134 completed (loss: 1.8341517448425293, acc: 0.7488921880722046)
[2025-02-13 02:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:05][root][INFO] - Training Epoch: 1/2, step 28/7134 completed (loss: 1.8129159212112427, acc: 0.740963876247406)
[2025-02-13 02:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:05][root][INFO] - Training Epoch: 1/2, step 29/7134 completed (loss: 2.008225202560425, acc: 0.723796010017395)
[2025-02-13 02:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:06][root][INFO] - Training Epoch: 1/2, step 30/7134 completed (loss: 1.771795392036438, acc: 0.7435027956962585)
[2025-02-13 02:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:07][root][INFO] - Training Epoch: 1/2, step 31/7134 completed (loss: 1.7380661964416504, acc: 0.7394067645072937)
[2025-02-13 02:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:07][root][INFO] - Training Epoch: 1/2, step 32/7134 completed (loss: 1.7286055088043213, acc: 0.7295188307762146)
[2025-02-13 02:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:07][root][INFO] - Training Epoch: 1/2, step 33/7134 completed (loss: 1.954289436340332, acc: 0.7191234827041626)
[2025-02-13 02:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:08][root][INFO] - Training Epoch: 1/2, step 34/7134 completed (loss: 1.8346270322799683, acc: 0.7303634285926819)
[2025-02-13 02:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:09][root][INFO] - Training Epoch: 1/2, step 35/7134 completed (loss: 1.6819027662277222, acc: 0.7295004725456238)
[2025-02-13 02:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:09][root][INFO] - Training Epoch: 1/2, step 36/7134 completed (loss: 1.7244510650634766, acc: 0.7372421026229858)
[2025-02-13 02:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:10][root][INFO] - Training Epoch: 1/2, step 37/7134 completed (loss: 1.6068981885910034, acc: 0.745932400226593)
[2025-02-13 02:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:10][root][INFO] - Training Epoch: 1/2, step 38/7134 completed (loss: 1.6703838109970093, acc: 0.7514880895614624)
[2025-02-13 02:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:10][root][INFO] - Training Epoch: 1/2, step 39/7134 completed (loss: 1.5800087451934814, acc: 0.7523680925369263)
[2025-02-13 02:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:11][root][INFO] - Training Epoch: 1/2, step 40/7134 completed (loss: 1.906369686126709, acc: 0.7354260087013245)
[2025-02-13 02:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:11][root][INFO] - Training Epoch: 1/2, step 41/7134 completed (loss: 1.7015480995178223, acc: 0.7128987312316895)
[2025-02-13 02:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:12][root][INFO] - Training Epoch: 1/2, step 42/7134 completed (loss: 1.424639344215393, acc: 0.7545582056045532)
[2025-02-13 02:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:12][root][INFO] - Training Epoch: 1/2, step 43/7134 completed (loss: 1.3396832942962646, acc: 0.7638709545135498)
[2025-02-13 02:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:13][root][INFO] - Training Epoch: 1/2, step 44/7134 completed (loss: 1.4353766441345215, acc: 0.7464967966079712)
[2025-02-13 02:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:13][root][INFO] - Training Epoch: 1/2, step 45/7134 completed (loss: 1.3567719459533691, acc: 0.7378115057945251)
[2025-02-13 02:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:14][root][INFO] - Training Epoch: 1/2, step 46/7134 completed (loss: 1.3694543838500977, acc: 0.7559999823570251)
[2025-02-13 02:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:14][root][INFO] - Training Epoch: 1/2, step 47/7134 completed (loss: 1.4410713911056519, acc: 0.7128427028656006)
[2025-02-13 02:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:15][root][INFO] - Training Epoch: 1/2, step 48/7134 completed (loss: 1.284812092781067, acc: 0.733707845211029)
[2025-02-13 02:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:15][root][INFO] - Training Epoch: 1/2, step 49/7134 completed (loss: 1.281129240989685, acc: 0.748971164226532)
[2025-02-13 02:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:16][root][INFO] - Training Epoch: 1/2, step 50/7134 completed (loss: 1.2664730548858643, acc: 0.7462514638900757)
[2025-02-13 02:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:16][root][INFO] - Training Epoch: 1/2, step 51/7134 completed (loss: 1.3199174404144287, acc: 0.7246192693710327)
[2025-02-13 02:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:17][root][INFO] - Training Epoch: 1/2, step 52/7134 completed (loss: 1.3012586832046509, acc: 0.7365853786468506)
[2025-02-13 02:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:17][root][INFO] - Training Epoch: 1/2, step 53/7134 completed (loss: 1.1888355016708374, acc: 0.7637795209884644)
[2025-02-13 02:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:18][root][INFO] - Training Epoch: 1/2, step 54/7134 completed (loss: 1.0635308027267456, acc: 0.741081714630127)
[2025-02-13 02:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:18][root][INFO] - Training Epoch: 1/2, step 55/7134 completed (loss: 1.062022089958191, acc: 0.7512690424919128)
[2025-02-13 02:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:19][root][INFO] - Training Epoch: 1/2, step 56/7134 completed (loss: 1.0007539987564087, acc: 0.7678571343421936)
[2025-02-13 02:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:19][root][INFO] - Training Epoch: 1/2, step 57/7134 completed (loss: 0.9242062568664551, acc: 0.7652173638343811)
[2025-02-13 02:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:20][root][INFO] - Training Epoch: 1/2, step 58/7134 completed (loss: 0.9364315271377563, acc: 0.7426376342773438)
[2025-02-13 02:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:20][root][INFO] - Training Epoch: 1/2, step 59/7134 completed (loss: 0.8554496765136719, acc: 0.7565011978149414)
[2025-02-13 02:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:21][root][INFO] - Training Epoch: 1/2, step 60/7134 completed (loss: 0.8706470727920532, acc: 0.7599431872367859)
[2025-02-13 02:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:21][root][INFO] - Training Epoch: 1/2, step 61/7134 completed (loss: 0.8683323264122009, acc: 0.7600979208946228)
[2025-02-13 02:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:22][root][INFO] - Training Epoch: 1/2, step 62/7134 completed (loss: 0.8216831088066101, acc: 0.7712833285331726)
[2025-02-13 02:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:22][root][INFO] - Training Epoch: 1/2, step 63/7134 completed (loss: 0.8952701091766357, acc: 0.7621419429779053)
[2025-02-13 02:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:23][root][INFO] - Training Epoch: 1/2, step 64/7134 completed (loss: 0.8179721832275391, acc: 0.7634408473968506)
[2025-02-13 02:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:23][root][INFO] - Training Epoch: 1/2, step 65/7134 completed (loss: 0.7878502607345581, acc: 0.7751842737197876)
[2025-02-13 02:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:24][root][INFO] - Training Epoch: 1/2, step 66/7134 completed (loss: 0.8649452328681946, acc: 0.7777777910232544)
[2025-02-13 02:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:24][root][INFO] - Training Epoch: 1/2, step 67/7134 completed (loss: 0.7802969217300415, acc: 0.772549033164978)
[2025-02-13 02:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:25][root][INFO] - Training Epoch: 1/2, step 68/7134 completed (loss: 0.7517800331115723, acc: 0.7858064770698547)
[2025-02-13 02:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:25][root][INFO] - Training Epoch: 1/2, step 69/7134 completed (loss: 0.7401193976402283, acc: 0.780548632144928)
[2025-02-13 02:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:26][root][INFO] - Training Epoch: 1/2, step 70/7134 completed (loss: 0.7504868507385254, acc: 0.7842105031013489)
[2025-02-13 02:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:26][root][INFO] - Training Epoch: 1/2, step 71/7134 completed (loss: 0.6528967022895813, acc: 0.7913165092468262)
[2025-02-13 02:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:27][root][INFO] - Training Epoch: 1/2, step 72/7134 completed (loss: 0.6539871692657471, acc: 0.8017902970314026)
[2025-02-13 02:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:27][root][INFO] - Training Epoch: 1/2, step 73/7134 completed (loss: 0.5800972580909729, acc: 0.8301630616188049)
[2025-02-13 02:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:28][root][INFO] - Training Epoch: 1/2, step 74/7134 completed (loss: 0.6536480784416199, acc: 0.8328690528869629)
[2025-02-13 02:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:28][root][INFO] - Training Epoch: 1/2, step 75/7134 completed (loss: 0.7232373356819153, acc: 0.8146487474441528)
[2025-02-13 02:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:29][root][INFO] - Training Epoch: 1/2, step 76/7134 completed (loss: 0.4829716086387634, acc: 0.8450881838798523)
[2025-02-13 02:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:29][root][INFO] - Training Epoch: 1/2, step 77/7134 completed (loss: 0.6146020889282227, acc: 0.8383458852767944)
[2025-02-13 02:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:30][root][INFO] - Training Epoch: 1/2, step 78/7134 completed (loss: 0.5171680450439453, acc: 0.8427672982215881)
[2025-02-13 02:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:30][root][INFO] - Training Epoch: 1/2, step 79/7134 completed (loss: 0.5462315678596497, acc: 0.8487654328346252)
[2025-02-13 02:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:31][root][INFO] - Training Epoch: 1/2, step 80/7134 completed (loss: 0.412765771150589, acc: 0.87890625)
[2025-02-13 02:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:31][root][INFO] - Training Epoch: 1/2, step 81/7134 completed (loss: 0.45693761110305786, acc: 0.8782722353935242)
[2025-02-13 02:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:32][root][INFO] - Training Epoch: 1/2, step 82/7134 completed (loss: 0.3817611336708069, acc: 0.9024045467376709)
[2025-02-13 02:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:32][root][INFO] - Training Epoch: 1/2, step 83/7134 completed (loss: 0.42746904492378235, acc: 0.8791773915290833)
[2025-02-13 02:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:33][root][INFO] - Training Epoch: 1/2, step 84/7134 completed (loss: 0.457409530878067, acc: 0.8860103487968445)
[2025-02-13 02:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:33][root][INFO] - Training Epoch: 1/2, step 85/7134 completed (loss: 0.42974478006362915, acc: 0.8854415416717529)
[2025-02-13 02:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:34][root][INFO] - Training Epoch: 1/2, step 86/7134 completed (loss: 0.4988953173160553, acc: 0.8807339668273926)
[2025-02-13 02:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:34][root][INFO] - Training Epoch: 1/2, step 87/7134 completed (loss: 0.4513094127178192, acc: 0.8795888423919678)
[2025-02-13 02:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:35][root][INFO] - Training Epoch: 1/2, step 88/7134 completed (loss: 0.4956425428390503, acc: 0.8893499374389648)
[2025-02-13 02:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:35][root][INFO] - Training Epoch: 1/2, step 89/7134 completed (loss: 0.40519362688064575, acc: 0.912708580493927)
[2025-02-13 02:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36][root][INFO] - Training Epoch: 1/2, step 90/7134 completed (loss: 0.3874214291572571, acc: 0.8968531489372253)
[2025-02-13 02:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36][root][INFO] - Training Epoch: 1/2, step 91/7134 completed (loss: 0.5099225640296936, acc: 0.8775510191917419)
[2025-02-13 02:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:37][root][INFO] - Training Epoch: 1/2, step 92/7134 completed (loss: 0.3782365024089813, acc: 0.9088358879089355)
[2025-02-13 02:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:37][root][INFO] - Training Epoch: 1/2, step 93/7134 completed (loss: 0.3589092493057251, acc: 0.9118457436561584)
[2025-02-13 02:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:38][root][INFO] - Training Epoch: 1/2, step 94/7134 completed (loss: 0.4914272129535675, acc: 0.8770301342010498)
[2025-02-13 02:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:38][root][INFO] - Training Epoch: 1/2, step 95/7134 completed (loss: 0.3437328338623047, acc: 0.9045225977897644)
[2025-02-13 02:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:38][root][INFO] - Training Epoch: 1/2, step 96/7134 completed (loss: 0.2754290699958801, acc: 0.9404096603393555)
[2025-02-13 02:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:39][root][INFO] - Training Epoch: 1/2, step 97/7134 completed (loss: 0.8135784268379211, acc: 0.8132529854774475)
[2025-02-13 02:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40][root][INFO] - Training Epoch: 1/2, step 98/7134 completed (loss: 0.4813610315322876, acc: 0.8834951519966125)
[2025-02-13 02:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40][root][INFO] - Training Epoch: 1/2, step 99/7134 completed (loss: 0.454684853553772, acc: 0.8917431235313416)
[2025-02-13 02:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:41][root][INFO] - Training Epoch: 1/2, step 100/7134 completed (loss: 0.3768633008003235, acc: 0.9166666865348816)
[2025-02-13 02:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:41][root][INFO] - Training Epoch: 1/2, step 101/7134 completed (loss: 0.3656601905822754, acc: 0.9128289222717285)
[2025-02-13 02:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:41][root][INFO] - Training Epoch: 1/2, step 102/7134 completed (loss: 0.32161447405815125, acc: 0.9218559265136719)
[2025-02-13 02:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:42][root][INFO] - Training Epoch: 1/2, step 103/7134 completed (loss: 0.29027867317199707, acc: 0.925000011920929)
[2025-02-13 02:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:42][root][INFO] - Training Epoch: 1/2, step 104/7134 completed (loss: 0.3175213038921356, acc: 0.9199395775794983)
[2025-02-13 02:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:43][root][INFO] - Training Epoch: 1/2, step 105/7134 completed (loss: 0.31843268871307373, acc: 0.9135399460792542)
[2025-02-13 02:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44][root][INFO] - Training Epoch: 1/2, step 106/7134 completed (loss: 0.26441171765327454, acc: 0.9360100626945496)
[2025-02-13 02:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44][root][INFO] - Training Epoch: 1/2, step 107/7134 completed (loss: 0.44694265723228455, acc: 0.8853868246078491)
[2025-02-13 02:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:45][root][INFO] - Training Epoch: 1/2, step 108/7134 completed (loss: 0.29348117113113403, acc: 0.9196642637252808)
[2025-02-13 02:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:45][root][INFO] - Training Epoch: 1/2, step 109/7134 completed (loss: 0.28688645362854004, acc: 0.935661792755127)
[2025-02-13 02:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46][root][INFO] - Training Epoch: 1/2, step 110/7134 completed (loss: 0.3063713312149048, acc: 0.9165687561035156)
[2025-02-13 02:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46][root][INFO] - Training Epoch: 1/2, step 111/7134 completed (loss: 0.2707786560058594, acc: 0.9337978959083557)
[2025-02-13 02:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:47][root][INFO] - Training Epoch: 1/2, step 112/7134 completed (loss: 0.20791780948638916, acc: 0.9429892301559448)
[2025-02-13 02:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:47][root][INFO] - Training Epoch: 1/2, step 113/7134 completed (loss: 0.24380630254745483, acc: 0.9354430437088013)
[2025-02-13 02:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:47][root][INFO] - Training Epoch: 1/2, step 114/7134 completed (loss: 0.3621883690357208, acc: 0.9058355689048767)
[2025-02-13 02:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:48][root][INFO] - Training Epoch: 1/2, step 115/7134 completed (loss: 0.47310787439346313, acc: 0.887139081954956)
[2025-02-13 02:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:48][root][INFO] - Training Epoch: 1/2, step 116/7134 completed (loss: 0.24488870799541473, acc: 0.9316843152046204)
[2025-02-13 02:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:49][root][INFO] - Training Epoch: 1/2, step 117/7134 completed (loss: 0.21556957066059113, acc: 0.945983350276947)
[2025-02-13 02:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:49][root][INFO] - Training Epoch: 1/2, step 118/7134 completed (loss: 0.24683064222335815, acc: 0.9362244606018066)
[2025-02-13 02:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50][root][INFO] - Training Epoch: 1/2, step 119/7134 completed (loss: 0.28611457347869873, acc: 0.9261501431465149)
[2025-02-13 02:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50][root][INFO] - Training Epoch: 1/2, step 120/7134 completed (loss: 0.2188670039176941, acc: 0.9462647438049316)
[2025-02-13 02:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:51][root][INFO] - Training Epoch: 1/2, step 121/7134 completed (loss: 0.1943473070859909, acc: 0.9550858736038208)
[2025-02-13 02:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:51][root][INFO] - Training Epoch: 1/2, step 122/7134 completed (loss: 0.23392252624034882, acc: 0.939072847366333)
[2025-02-13 02:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:52][root][INFO] - Training Epoch: 1/2, step 123/7134 completed (loss: 0.2284087836742401, acc: 0.9414414167404175)
[2025-02-13 02:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:52][root][INFO] - Training Epoch: 1/2, step 124/7134 completed (loss: 0.25337207317352295, acc: 0.9431034326553345)
[2025-02-13 02:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:53][root][INFO] - Training Epoch: 1/2, step 125/7134 completed (loss: 0.31219589710235596, acc: 0.9181416034698486)
[2025-02-13 02:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:53][root][INFO] - Training Epoch: 1/2, step 126/7134 completed (loss: 0.25543054938316345, acc: 0.9399999976158142)
[2025-02-13 02:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:54][root][INFO] - Training Epoch: 1/2, step 127/7134 completed (loss: 0.22626537084579468, acc: 0.9444444179534912)
[2025-02-13 02:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:54][root][INFO] - Training Epoch: 1/2, step 128/7134 completed (loss: 0.32764941453933716, acc: 0.9365628361701965)
[2025-02-13 02:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55][root][INFO] - Training Epoch: 1/2, step 129/7134 completed (loss: 0.22430554032325745, acc: 0.936274528503418)
[2025-02-13 02:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55][root][INFO] - Training Epoch: 1/2, step 130/7134 completed (loss: 0.23988769948482513, acc: 0.9434210658073425)
[2025-02-13 02:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:56][root][INFO] - Training Epoch: 1/2, step 131/7134 completed (loss: 0.20654086768627167, acc: 0.9565789699554443)
[2025-02-13 02:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:56][root][INFO] - Training Epoch: 1/2, step 132/7134 completed (loss: 0.21473118662834167, acc: 0.9462989568710327)
[2025-02-13 02:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57][root][INFO] - Training Epoch: 1/2, step 133/7134 completed (loss: 0.184831902384758, acc: 0.9450914859771729)
[2025-02-13 02:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57][root][INFO] - Training Epoch: 1/2, step 134/7134 completed (loss: 0.15995624661445618, acc: 0.9592920541763306)
[2025-02-13 02:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:58][root][INFO] - Training Epoch: 1/2, step 135/7134 completed (loss: 0.23626400530338287, acc: 0.948888897895813)
[2025-02-13 02:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:58][root][INFO] - Training Epoch: 1/2, step 136/7134 completed (loss: 0.16018088161945343, acc: 0.9545997381210327)
[2025-02-13 02:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:59][root][INFO] - Training Epoch: 1/2, step 137/7134 completed (loss: 0.2888163924217224, acc: 0.9397293925285339)
[2025-02-13 02:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:59][root][INFO] - Training Epoch: 1/2, step 138/7134 completed (loss: 0.3280421793460846, acc: 0.9292035102844238)
[2025-02-13 02:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00][root][INFO] - Training Epoch: 1/2, step 139/7134 completed (loss: 0.2593950927257538, acc: 0.9411764740943909)
[2025-02-13 02:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00][root][INFO] - Training Epoch: 1/2, step 140/7134 completed (loss: 0.27649423480033875, acc: 0.9358208775520325)
[2025-02-13 02:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:01][root][INFO] - Training Epoch: 1/2, step 141/7134 completed (loss: 0.2543621361255646, acc: 0.9457547068595886)
[2025-02-13 02:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:01][root][INFO] - Training Epoch: 1/2, step 142/7134 completed (loss: 0.29648420214653015, acc: 0.9374130964279175)
[2025-02-13 02:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02][root][INFO] - Training Epoch: 1/2, step 143/7134 completed (loss: 0.142910897731781, acc: 0.9608865976333618)
[2025-02-13 02:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02][root][INFO] - Training Epoch: 1/2, step 144/7134 completed (loss: 0.3562508821487427, acc: 0.9198664426803589)
[2025-02-13 02:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:03][root][INFO] - Training Epoch: 1/2, step 145/7134 completed (loss: 0.39631447196006775, acc: 0.9070422649383545)
[2025-02-13 02:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:03][root][INFO] - Training Epoch: 1/2, step 146/7134 completed (loss: 0.3186822533607483, acc: 0.9245014190673828)
[2025-02-13 02:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:04][root][INFO] - Training Epoch: 1/2, step 147/7134 completed (loss: 0.3500870168209076, acc: 0.9202772974967957)
[2025-02-13 02:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:04][root][INFO] - Training Epoch: 1/2, step 148/7134 completed (loss: 0.4509311616420746, acc: 0.8770301342010498)
[2025-02-13 02:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05][root][INFO] - Training Epoch: 1/2, step 149/7134 completed (loss: 0.3338945806026459, acc: 0.9264516234397888)
[2025-02-13 02:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05][root][INFO] - Training Epoch: 1/2, step 150/7134 completed (loss: 0.33561378717422485, acc: 0.9224599003791809)
[2025-02-13 02:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:06][root][INFO] - Training Epoch: 1/2, step 151/7134 completed (loss: 0.27187225222587585, acc: 0.9355608820915222)
[2025-02-13 02:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:06][root][INFO] - Training Epoch: 1/2, step 152/7134 completed (loss: 0.2614482343196869, acc: 0.9430524110794067)
[2025-02-13 02:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:07][root][INFO] - Training Epoch: 1/2, step 153/7134 completed (loss: 0.31097397208213806, acc: 0.916201114654541)
[2025-02-13 02:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:07][root][INFO] - Training Epoch: 1/2, step 154/7134 completed (loss: 0.3347378075122833, acc: 0.9164133667945862)
[2025-02-13 02:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:07][root][INFO] - Training Epoch: 1/2, step 155/7134 completed (loss: 0.34220969676971436, acc: 0.9246753454208374)
[2025-02-13 02:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08][root][INFO] - Training Epoch: 1/2, step 156/7134 completed (loss: 0.2802957594394684, acc: 0.9264877438545227)
[2025-02-13 02:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08][root][INFO] - Training Epoch: 1/2, step 157/7134 completed (loss: 0.3244028389453888, acc: 0.9325301051139832)
[2025-02-13 02:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:09][root][INFO] - Training Epoch: 1/2, step 158/7134 completed (loss: 0.21330243349075317, acc: 0.9458388090133667)
[2025-02-13 02:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:09][root][INFO] - Training Epoch: 1/2, step 159/7134 completed (loss: 0.28224119544029236, acc: 0.9379509091377258)
[2025-02-13 02:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:10][root][INFO] - Training Epoch: 1/2, step 160/7134 completed (loss: 0.32112517952919006, acc: 0.9230769276618958)
[2025-02-13 02:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:10][root][INFO] - Training Epoch: 1/2, step 161/7134 completed (loss: 0.19561530649662018, acc: 0.9505813717842102)
[2025-02-13 02:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:11][root][INFO] - Training Epoch: 1/2, step 162/7134 completed (loss: 0.16973252594470978, acc: 0.9539641737937927)
[2025-02-13 02:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:11][root][INFO] - Training Epoch: 1/2, step 163/7134 completed (loss: 0.2644219994544983, acc: 0.9394387006759644)
[2025-02-13 02:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:12][root][INFO] - Training Epoch: 1/2, step 164/7134 completed (loss: 0.16946984827518463, acc: 0.957446813583374)
[2025-02-13 02:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:12][root][INFO] - Training Epoch: 1/2, step 165/7134 completed (loss: 0.26252713799476624, acc: 0.940191388130188)
[2025-02-13 02:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13][root][INFO] - Training Epoch: 1/2, step 166/7134 completed (loss: 0.19782550632953644, acc: 0.9492537379264832)
[2025-02-13 02:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13][root][INFO] - Training Epoch: 1/2, step 167/7134 completed (loss: 0.7467918992042542, acc: 0.8467023372650146)
[2025-02-13 02:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:14][root][INFO] - Training Epoch: 1/2, step 168/7134 completed (loss: 2.099738121032715, acc: 0.6117647290229797)
[2025-02-13 02:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:14][root][INFO] - Training Epoch: 1/2, step 169/7134 completed (loss: 1.602811574935913, acc: 0.6694678068161011)
[2025-02-13 02:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:15][root][INFO] - Training Epoch: 1/2, step 170/7134 completed (loss: 0.8426439762115479, acc: 0.8178368210792542)
[2025-02-13 02:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:15][root][INFO] - Training Epoch: 1/2, step 171/7134 completed (loss: 1.0485330820083618, acc: 0.7889610528945923)
[2025-02-13 02:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:15][root][INFO] - Training Epoch: 1/2, step 172/7134 completed (loss: 0.4569569230079651, acc: 0.8997554779052734)
[2025-02-13 02:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16][root][INFO] - Training Epoch: 1/2, step 173/7134 completed (loss: 0.3104936182498932, acc: 0.9269911646842957)
[2025-02-13 02:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16][root][INFO] - Training Epoch: 1/2, step 174/7134 completed (loss: 0.4503284990787506, acc: 0.8767605423927307)
[2025-02-13 02:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:17][root][INFO] - Training Epoch: 1/2, step 175/7134 completed (loss: 0.3154938519001007, acc: 0.9272727370262146)
[2025-02-13 02:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:17][root][INFO] - Training Epoch: 1/2, step 176/7134 completed (loss: 0.3381345570087433, acc: 0.9202898740768433)
[2025-02-13 02:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:18][root][INFO] - Training Epoch: 1/2, step 177/7134 completed (loss: 0.394794225692749, acc: 0.9111111164093018)
[2025-02-13 02:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:18][root][INFO] - Training Epoch: 1/2, step 178/7134 completed (loss: 0.41603168845176697, acc: 0.8914728760719299)
[2025-02-13 02:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19][root][INFO] - Training Epoch: 1/2, step 179/7134 completed (loss: 0.30671191215515137, acc: 0.9284332394599915)
[2025-02-13 02:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19][root][INFO] - Training Epoch: 1/2, step 180/7134 completed (loss: 0.19548755884170532, acc: 0.9460154175758362)
[2025-02-13 02:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:20][root][INFO] - Training Epoch: 1/2, step 181/7134 completed (loss: 0.14727796614170074, acc: 0.9499341249465942)
[2025-02-13 02:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:20][root][INFO] - Training Epoch: 1/2, step 182/7134 completed (loss: 0.13870176672935486, acc: 0.9633699655532837)
[2025-02-13 02:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:21][root][INFO] - Training Epoch: 1/2, step 183/7134 completed (loss: 0.21608954668045044, acc: 0.9482288956642151)
[2025-02-13 02:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:21][root][INFO] - Training Epoch: 1/2, step 184/7134 completed (loss: 0.16509084403514862, acc: 0.9585253596305847)
[2025-02-13 02:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:22][root][INFO] - Training Epoch: 1/2, step 185/7134 completed (loss: 0.12308772653341293, acc: 0.9699570536613464)
[2025-02-13 02:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:22][root][INFO] - Training Epoch: 1/2, step 186/7134 completed (loss: 0.18338842689990997, acc: 0.9455128312110901)
[2025-02-13 02:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23][root][INFO] - Training Epoch: 1/2, step 187/7134 completed (loss: 0.17286212742328644, acc: 0.9441340565681458)
[2025-02-13 02:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23][root][INFO] - Training Epoch: 1/2, step 188/7134 completed (loss: 0.2612573504447937, acc: 0.9305555820465088)
[2025-02-13 02:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:24][root][INFO] - Training Epoch: 1/2, step 189/7134 completed (loss: 0.20017728209495544, acc: 0.9461538195610046)
[2025-02-13 02:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:24][root][INFO] - Training Epoch: 1/2, step 190/7134 completed (loss: 0.2725430727005005, acc: 0.9297971725463867)
[2025-02-13 02:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25][root][INFO] - Training Epoch: 1/2, step 191/7134 completed (loss: 0.20557419955730438, acc: 0.938034176826477)
[2025-02-13 02:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25][root][INFO] - Training Epoch: 1/2, step 192/7134 completed (loss: 0.09790506958961487, acc: 0.9725086092948914)
[2025-02-13 02:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:26][root][INFO] - Training Epoch: 1/2, step 193/7134 completed (loss: 0.2127455472946167, acc: 0.9503355622291565)
[2025-02-13 02:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:26][root][INFO] - Training Epoch: 1/2, step 194/7134 completed (loss: 0.17639867961406708, acc: 0.946915328502655)
[2025-02-13 02:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:27][root][INFO] - Training Epoch: 1/2, step 195/7134 completed (loss: 0.205534428358078, acc: 0.9462184906005859)
[2025-02-13 02:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:27][root][INFO] - Training Epoch: 1/2, step 196/7134 completed (loss: 0.19956405460834503, acc: 0.9545454382896423)
[2025-02-13 02:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28][root][INFO] - Training Epoch: 1/2, step 197/7134 completed (loss: 0.12481636554002762, acc: 0.9700854420661926)
[2025-02-13 02:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28][root][INFO] - Training Epoch: 1/2, step 198/7134 completed (loss: 0.11761236935853958, acc: 0.9603073000907898)
[2025-02-13 02:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:29][root][INFO] - Training Epoch: 1/2, step 199/7134 completed (loss: 0.16627176105976105, acc: 0.9556650519371033)
[2025-02-13 02:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:29][root][INFO] - Training Epoch: 1/2, step 200/7134 completed (loss: 0.1331242024898529, acc: 0.9516778588294983)
[2025-02-13 02:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:30][root][INFO] - Training Epoch: 1/2, step 201/7134 completed (loss: 0.13135862350463867, acc: 0.9606741666793823)
[2025-02-13 02:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:30][root][INFO] - Training Epoch: 1/2, step 202/7134 completed (loss: 0.1283469945192337, acc: 0.9627586007118225)
[2025-02-13 02:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31][root][INFO] - Training Epoch: 1/2, step 203/7134 completed (loss: 0.2061471790075302, acc: 0.944903552532196)
[2025-02-13 02:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31][root][INFO] - Training Epoch: 1/2, step 204/7134 completed (loss: 0.17015166580677032, acc: 0.9528875350952148)
[2025-02-13 02:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:32][root][INFO] - Training Epoch: 1/2, step 205/7134 completed (loss: 0.22650079429149628, acc: 0.9430789351463318)
[2025-02-13 02:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:32][root][INFO] - Training Epoch: 1/2, step 206/7134 completed (loss: 0.18298275768756866, acc: 0.9566613435745239)
[2025-02-13 02:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:33][root][INFO] - Training Epoch: 1/2, step 207/7134 completed (loss: 0.20282740890979767, acc: 0.9488054513931274)
[2025-02-13 02:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:33][root][INFO] - Training Epoch: 1/2, step 208/7134 completed (loss: 0.31715139746665955, acc: 0.9196078181266785)
[2025-02-13 02:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:34][root][INFO] - Training Epoch: 1/2, step 209/7134 completed (loss: 0.32759204506874084, acc: 0.9274611473083496)
[2025-02-13 02:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:34][root][INFO] - Training Epoch: 1/2, step 210/7134 completed (loss: 0.2672024667263031, acc: 0.9351740479469299)
[2025-02-13 02:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:35][root][INFO] - Training Epoch: 1/2, step 211/7134 completed (loss: 0.3227526843547821, acc: 0.9264705777168274)
[2025-02-13 02:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:35][root][INFO] - Training Epoch: 1/2, step 212/7134 completed (loss: 0.2994877099990845, acc: 0.9276859760284424)
[2025-02-13 02:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:35][root][INFO] - Training Epoch: 1/2, step 213/7134 completed (loss: 0.1799522489309311, acc: 0.9530744552612305)
[2025-02-13 02:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:36][root][INFO] - Training Epoch: 1/2, step 214/7134 completed (loss: 0.23560817539691925, acc: 0.9508547186851501)
[2025-02-13 02:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:36][root][INFO] - Training Epoch: 1/2, step 215/7134 completed (loss: 0.21205443143844604, acc: 0.9513888955116272)
[2025-02-13 02:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37][root][INFO] - Training Epoch: 1/2, step 216/7134 completed (loss: 0.17639413475990295, acc: 0.9611111283302307)
[2025-02-13 02:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37][root][INFO] - Training Epoch: 1/2, step 217/7134 completed (loss: 0.15713436901569366, acc: 0.9624242186546326)
[2025-02-13 02:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:38][root][INFO] - Training Epoch: 1/2, step 218/7134 completed (loss: 0.06670635938644409, acc: 0.983460545539856)
[2025-02-13 02:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:38][root][INFO] - Training Epoch: 1/2, step 219/7134 completed (loss: 0.13144321739673615, acc: 0.9727095365524292)
[2025-02-13 02:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:39][root][INFO] - Training Epoch: 1/2, step 220/7134 completed (loss: 0.160968616604805, acc: 0.9686192274093628)
[2025-02-13 02:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:39][root][INFO] - Training Epoch: 1/2, step 221/7134 completed (loss: 0.19760027527809143, acc: 0.9611940383911133)
[2025-02-13 02:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:40][root][INFO] - Training Epoch: 1/2, step 222/7134 completed (loss: 0.11581277847290039, acc: 0.963350772857666)
[2025-02-13 02:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:40][root][INFO] - Training Epoch: 1/2, step 223/7134 completed (loss: 0.19757850468158722, acc: 0.9576380252838135)
[2025-02-13 02:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41][root][INFO] - Training Epoch: 1/2, step 224/7134 completed (loss: 0.23394383490085602, acc: 0.9429312348365784)
[2025-02-13 02:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41][root][INFO] - Training Epoch: 1/2, step 225/7134 completed (loss: 0.2322694957256317, acc: 0.9448010325431824)
[2025-02-13 02:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:42][root][INFO] - Training Epoch: 1/2, step 226/7134 completed (loss: 0.1547253578901291, acc: 0.9621027112007141)
[2025-02-13 02:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:42][root][INFO] - Training Epoch: 1/2, step 227/7134 completed (loss: 0.2847495675086975, acc: 0.9379652738571167)
[2025-02-13 02:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:43][root][INFO] - Training Epoch: 1/2, step 228/7134 completed (loss: 0.1854611486196518, acc: 0.9609261751174927)
[2025-02-13 02:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:43][root][INFO] - Training Epoch: 1/2, step 229/7134 completed (loss: 0.23471447825431824, acc: 0.9443038105964661)
[2025-02-13 02:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44][root][INFO] - Training Epoch: 1/2, step 230/7134 completed (loss: 0.1100161001086235, acc: 0.9705159664154053)
[2025-02-13 02:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44][root][INFO] - Training Epoch: 1/2, step 231/7134 completed (loss: 0.1308644413948059, acc: 0.9585687518119812)
[2025-02-13 02:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:45][root][INFO] - Training Epoch: 1/2, step 232/7134 completed (loss: 0.15068411827087402, acc: 0.9637562036514282)
[2025-02-13 02:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:45][root][INFO] - Training Epoch: 1/2, step 233/7134 completed (loss: 0.16526085138320923, acc: 0.9651346802711487)
[2025-02-13 02:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:45][root][INFO] - Training Epoch: 1/2, step 234/7134 completed (loss: 0.1888117641210556, acc: 0.9543918967247009)
[2025-02-13 02:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:46][root][INFO] - Training Epoch: 1/2, step 235/7134 completed (loss: 0.19427062571048737, acc: 0.9529914259910583)
[2025-02-13 02:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:47][root][INFO] - Training Epoch: 1/2, step 236/7134 completed (loss: 0.07855521887540817, acc: 0.9765625)
[2025-02-13 02:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:47][root][INFO] - Training Epoch: 1/2, step 237/7134 completed (loss: 0.18344618380069733, acc: 0.9538003206253052)
[2025-02-13 02:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:47][root][INFO] - Training Epoch: 1/2, step 238/7134 completed (loss: 0.17234961688518524, acc: 0.9682835936546326)
[2025-02-13 02:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:48][root][INFO] - Training Epoch: 1/2, step 239/7134 completed (loss: 0.09390164911746979, acc: 0.9726890921592712)
[2025-02-13 02:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:48][root][INFO] - Training Epoch: 1/2, step 240/7134 completed (loss: 0.1859356164932251, acc: 0.9555084705352783)
[2025-02-13 02:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:49][root][INFO] - Training Epoch: 1/2, step 241/7134 completed (loss: 0.20007754862308502, acc: 0.9468598961830139)
[2025-02-13 02:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:49][root][INFO] - Training Epoch: 1/2, step 242/7134 completed (loss: 0.27367284893989563, acc: 0.9324089884757996)
[2025-02-13 02:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50][root][INFO] - Training Epoch: 1/2, step 243/7134 completed (loss: 0.28480735421180725, acc: 0.942176878452301)
[2025-02-13 02:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50][root][INFO] - Training Epoch: 1/2, step 244/7134 completed (loss: 0.09696899354457855, acc: 0.9788838624954224)
[2025-02-13 02:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:51][root][INFO] - Training Epoch: 1/2, step 245/7134 completed (loss: 0.1726319044828415, acc: 0.9614710807800293)
[2025-02-13 02:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:51][root][INFO] - Training Epoch: 1/2, step 246/7134 completed (loss: 0.16489751636981964, acc: 0.9589905142784119)
[2025-02-13 02:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:52][root][INFO] - Training Epoch: 1/2, step 247/7134 completed (loss: 0.15369240939617157, acc: 0.963702380657196)
[2025-02-13 02:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:52][root][INFO] - Training Epoch: 1/2, step 248/7134 completed (loss: 0.1645182967185974, acc: 0.9554263353347778)
[2025-02-13 02:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53][root][INFO] - Training Epoch: 1/2, step 249/7134 completed (loss: 0.16322563588619232, acc: 0.959785521030426)
[2025-02-13 02:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53][root][INFO] - Training Epoch: 1/2, step 250/7134 completed (loss: 0.17879754304885864, acc: 0.949367105960846)
[2025-02-13 02:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:54][root][INFO] - Training Epoch: 1/2, step 251/7134 completed (loss: 0.14041519165039062, acc: 0.9589743614196777)
[2025-02-13 02:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:54][root][INFO] - Training Epoch: 1/2, step 252/7134 completed (loss: 0.10703719407320023, acc: 0.965573787689209)
[2025-02-13 02:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:54][root][INFO] - Training Epoch: 1/2, step 253/7134 completed (loss: 0.1372513622045517, acc: 0.9572192430496216)
[2025-02-13 02:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:55][root][INFO] - Training Epoch: 1/2, step 254/7134 completed (loss: 0.09489449113607407, acc: 0.9714285731315613)
[2025-02-13 02:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:55][root][INFO] - Training Epoch: 1/2, step 255/7134 completed (loss: 0.14165227115154266, acc: 0.9634503126144409)
[2025-02-13 02:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56][root][INFO] - Training Epoch: 1/2, step 256/7134 completed (loss: 0.1769738346338272, acc: 0.9448529481887817)
[2025-02-13 02:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56][root][INFO] - Training Epoch: 1/2, step 257/7134 completed (loss: 0.14890271425247192, acc: 0.9631901979446411)
[2025-02-13 02:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:57][root][INFO] - Training Epoch: 1/2, step 258/7134 completed (loss: 0.15041710436344147, acc: 0.968595027923584)
[2025-02-13 02:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:57][root][INFO] - Training Epoch: 1/2, step 259/7134 completed (loss: 0.1532823145389557, acc: 0.9643436074256897)
[2025-02-13 02:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:58][root][INFO] - Training Epoch: 1/2, step 260/7134 completed (loss: 0.13990627229213715, acc: 0.9576988220214844)
[2025-02-13 02:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:58][root][INFO] - Training Epoch: 1/2, step 261/7134 completed (loss: 0.12077927589416504, acc: 0.9672801494598389)
[2025-02-13 02:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:59][root][INFO] - Training Epoch: 1/2, step 262/7134 completed (loss: 0.07452660799026489, acc: 0.9750415682792664)
[2025-02-13 02:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:59][root][INFO] - Training Epoch: 1/2, step 263/7134 completed (loss: 0.06333372741937637, acc: 0.9795657992362976)
[2025-02-13 02:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:59][root][INFO] - Training Epoch: 1/2, step 264/7134 completed (loss: 0.07734298706054688, acc: 0.9766355156898499)
[2025-02-13 02:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00][root][INFO] - Training Epoch: 1/2, step 265/7134 completed (loss: 0.12564708292484283, acc: 0.969072163105011)
[2025-02-13 02:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00][root][INFO] - Training Epoch: 1/2, step 266/7134 completed (loss: 0.08988820016384125, acc: 0.9770641922950745)
[2025-02-13 02:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:01][root][INFO] - Training Epoch: 1/2, step 267/7134 completed (loss: 0.1256582885980606, acc: 0.9701149463653564)
[2025-02-13 02:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02][root][INFO] - Training Epoch: 1/2, step 268/7134 completed (loss: 0.10645251721143723, acc: 0.9688715934753418)
[2025-02-13 02:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02][root][INFO] - Training Epoch: 1/2, step 269/7134 completed (loss: 0.15282151103019714, acc: 0.9624478220939636)
[2025-02-13 02:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02][root][INFO] - Training Epoch: 1/2, step 270/7134 completed (loss: 0.10854051262140274, acc: 0.9731343388557434)
[2025-02-13 02:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03][root][INFO] - Training Epoch: 1/2, step 271/7134 completed (loss: 0.08801520615816116, acc: 0.9760403633117676)
[2025-02-13 02:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03][root][INFO] - Training Epoch: 1/2, step 272/7134 completed (loss: 0.127017080783844, acc: 0.9720101952552795)
[2025-02-13 02:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:04][root][INFO] - Training Epoch: 1/2, step 273/7134 completed (loss: 0.07653114199638367, acc: 0.9784430861473083)
[2025-02-13 02:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:04][root][INFO] - Training Epoch: 1/2, step 274/7134 completed (loss: 0.09331628680229187, acc: 0.9738652110099792)
[2025-02-13 02:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:05][root][INFO] - Training Epoch: 1/2, step 275/7134 completed (loss: 0.15172059834003448, acc: 0.9593967795372009)
[2025-02-13 02:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:05][root][INFO] - Training Epoch: 1/2, step 276/7134 completed (loss: 0.09636418521404266, acc: 0.9732034206390381)
[2025-02-13 02:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06][root][INFO] - Training Epoch: 1/2, step 277/7134 completed (loss: 0.12934260070323944, acc: 0.9678249955177307)
[2025-02-13 02:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06][root][INFO] - Training Epoch: 1/2, step 278/7134 completed (loss: 0.10094960778951645, acc: 0.9782886505126953)
[2025-02-13 02:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:07][root][INFO] - Training Epoch: 1/2, step 279/7134 completed (loss: 0.1545572131872177, acc: 0.9593175649642944)
[2025-02-13 02:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:07][root][INFO] - Training Epoch: 1/2, step 280/7134 completed (loss: 0.15677601099014282, acc: 0.9623655676841736)
[2025-02-13 02:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:08][root][INFO] - Training Epoch: 1/2, step 281/7134 completed (loss: 0.11412953585386276, acc: 0.9679999947547913)
[2025-02-13 02:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:08][root][INFO] - Training Epoch: 1/2, step 282/7134 completed (loss: 0.2324947714805603, acc: 0.943511426448822)
[2025-02-13 02:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:09][root][INFO] - Training Epoch: 1/2, step 283/7134 completed (loss: 0.15136872231960297, acc: 0.9643463492393494)
[2025-02-13 02:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:09][root][INFO] - Training Epoch: 1/2, step 284/7134 completed (loss: 0.16926972568035126, acc: 0.9563758373260498)
[2025-02-13 02:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10][root][INFO] - Training Epoch: 1/2, step 285/7134 completed (loss: 0.13029217720031738, acc: 0.9664903283119202)
[2025-02-13 02:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10][root][INFO] - Training Epoch: 1/2, step 286/7134 completed (loss: 0.12359105795621872, acc: 0.969911515712738)
[2025-02-13 02:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:11][root][INFO] - Training Epoch: 1/2, step 287/7134 completed (loss: 0.1142316684126854, acc: 0.9743177890777588)
[2025-02-13 02:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:11][root][INFO] - Training Epoch: 1/2, step 288/7134 completed (loss: 0.14604443311691284, acc: 0.9650092124938965)
[2025-02-13 02:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:12][root][INFO] - Training Epoch: 1/2, step 289/7134 completed (loss: 0.11157477647066116, acc: 0.9602169990539551)
[2025-02-13 02:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:12][root][INFO] - Training Epoch: 1/2, step 290/7134 completed (loss: 0.10139989107847214, acc: 0.9741641283035278)
[2025-02-13 02:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:13][root][INFO] - Training Epoch: 1/2, step 291/7134 completed (loss: 0.12926478683948517, acc: 0.9621710777282715)
[2025-02-13 02:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:13][root][INFO] - Training Epoch: 1/2, step 292/7134 completed (loss: 0.12871447205543518, acc: 0.9723926186561584)
[2025-02-13 02:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:13][root][INFO] - Training Epoch: 1/2, step 293/7134 completed (loss: 0.12934976816177368, acc: 0.9678714871406555)
[2025-02-13 02:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:14][root][INFO] - Training Epoch: 1/2, step 294/7134 completed (loss: 0.12502309679985046, acc: 0.9571149945259094)
[2025-02-13 02:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:14][root][INFO] - Training Epoch: 1/2, step 295/7134 completed (loss: 0.16220851242542267, acc: 0.9624573588371277)
[2025-02-13 02:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:15][root][INFO] - Training Epoch: 1/2, step 296/7134 completed (loss: 0.08100096136331558, acc: 0.9782945513725281)
[2025-02-13 02:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:15][root][INFO] - Training Epoch: 1/2, step 297/7134 completed (loss: 0.07121908664703369, acc: 0.9769093990325928)
[2025-02-13 02:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16][root][INFO] - Training Epoch: 1/2, step 298/7134 completed (loss: 0.05599045380949974, acc: 0.9872000217437744)
[2025-02-13 02:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16][root][INFO] - Training Epoch: 1/2, step 299/7134 completed (loss: 0.07237029820680618, acc: 0.9774436354637146)
[2025-02-13 02:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:17][root][INFO] - Training Epoch: 1/2, step 300/7134 completed (loss: 0.10706260055303574, acc: 0.9724919199943542)
[2025-02-13 02:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:17][root][INFO] - Training Epoch: 1/2, step 301/7134 completed (loss: 0.05358082801103592, acc: 0.9787836074829102)
[2025-02-13 02:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:18][root][INFO] - Training Epoch: 1/2, step 302/7134 completed (loss: 0.07683449238538742, acc: 0.9824561476707458)
[2025-02-13 02:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:18][root][INFO] - Training Epoch: 1/2, step 303/7134 completed (loss: 0.0641578808426857, acc: 0.9825242757797241)
[2025-02-13 02:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19][root][INFO] - Training Epoch: 1/2, step 304/7134 completed (loss: 0.04589133337140083, acc: 0.9888888597488403)
[2025-02-13 02:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19][root][INFO] - Training Epoch: 1/2, step 305/7134 completed (loss: 0.059325747191905975, acc: 0.9841017723083496)
[2025-02-13 02:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:20][root][INFO] - Training Epoch: 1/2, step 306/7134 completed (loss: 0.029297558590769768, acc: 0.9927536249160767)
[2025-02-13 02:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:20][root][INFO] - Training Epoch: 1/2, step 307/7134 completed (loss: 0.11704115569591522, acc: 0.9697508811950684)
[2025-02-13 02:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:21][root][INFO] - Training Epoch: 1/2, step 308/7134 completed (loss: 0.05195154994726181, acc: 0.9884868264198303)
[2025-02-13 02:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:21][root][INFO] - Training Epoch: 1/2, step 309/7134 completed (loss: 0.08514068275690079, acc: 0.982758641242981)
[2025-02-13 02:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22][root][INFO] - Training Epoch: 1/2, step 310/7134 completed (loss: 0.04682542011141777, acc: 0.9868420958518982)
[2025-02-13 02:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22][root][INFO] - Training Epoch: 1/2, step 311/7134 completed (loss: 0.05686919391155243, acc: 0.9797822833061218)
[2025-02-13 02:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22][root][INFO] - Training Epoch: 1/2, step 312/7134 completed (loss: 0.07172062247991562, acc: 0.9774696826934814)
[2025-02-13 02:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:23][root][INFO] - Training Epoch: 1/2, step 313/7134 completed (loss: 0.0627133920788765, acc: 0.9895651936531067)
[2025-02-13 02:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:23][root][INFO] - Training Epoch: 1/2, step 314/7134 completed (loss: 0.08090575039386749, acc: 0.9781249761581421)
[2025-02-13 02:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:24][root][INFO] - Training Epoch: 1/2, step 315/7134 completed (loss: 0.08081542700529099, acc: 0.9770491719245911)
[2025-02-13 02:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:24][root][INFO] - Training Epoch: 1/2, step 316/7134 completed (loss: 0.06687337905168533, acc: 0.9819494485855103)
[2025-02-13 02:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25][root][INFO] - Training Epoch: 1/2, step 317/7134 completed (loss: 0.03605208173394203, acc: 0.9918808937072754)
[2025-02-13 02:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25][root][INFO] - Training Epoch: 1/2, step 318/7134 completed (loss: 0.09917541593313217, acc: 0.9755799770355225)
[2025-02-13 02:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:26][root][INFO] - Training Epoch: 1/2, step 319/7134 completed (loss: 0.11593615263700485, acc: 0.9691444635391235)
[2025-02-13 02:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:26][root][INFO] - Training Epoch: 1/2, step 320/7134 completed (loss: 0.07972124963998795, acc: 0.9757489562034607)
[2025-02-13 02:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:27][root][INFO] - Training Epoch: 1/2, step 321/7134 completed (loss: 0.14404736459255219, acc: 0.9578543901443481)
[2025-02-13 02:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:27][root][INFO] - Training Epoch: 1/2, step 322/7134 completed (loss: 0.18545375764369965, acc: 0.9564564824104309)
[2025-02-13 02:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28][root][INFO] - Training Epoch: 1/2, step 323/7134 completed (loss: 0.17921103537082672, acc: 0.962772786617279)
[2025-02-13 02:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28][root][INFO] - Training Epoch: 1/2, step 324/7134 completed (loss: 0.17972932755947113, acc: 0.9608745574951172)
[2025-02-13 02:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:29][root][INFO] - Training Epoch: 1/2, step 325/7134 completed (loss: 0.09592956304550171, acc: 0.9755784273147583)
[2025-02-13 02:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:29][root][INFO] - Training Epoch: 1/2, step 326/7134 completed (loss: 0.09996172040700912, acc: 0.9792935252189636)
[2025-02-13 02:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:30][root][INFO] - Training Epoch: 1/2, step 327/7134 completed (loss: 0.13970911502838135, acc: 0.9662162065505981)
[2025-02-13 02:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:30][root][INFO] - Training Epoch: 1/2, step 328/7134 completed (loss: 0.12213706970214844, acc: 0.9792592525482178)
[2025-02-13 02:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:31][root][INFO] - Training Epoch: 1/2, step 329/7134 completed (loss: 0.08176008611917496, acc: 0.9811986088752747)
[2025-02-13 02:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:31][root][INFO] - Training Epoch: 1/2, step 330/7134 completed (loss: 0.19765442609786987, acc: 0.9577039480209351)
[2025-02-13 02:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:32][root][INFO] - Training Epoch: 1/2, step 331/7134 completed (loss: 0.09655293822288513, acc: 0.9783352613449097)
[2025-02-13 02:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:32][root][INFO] - Training Epoch: 1/2, step 332/7134 completed (loss: 0.07298719137907028, acc: 0.9787485003471375)
[2025-02-13 02:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33][root][INFO] - Training Epoch: 1/2, step 333/7134 completed (loss: 0.15163461863994598, acc: 0.9549878239631653)
[2025-02-13 02:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33][root][INFO] - Training Epoch: 1/2, step 334/7134 completed (loss: 0.06484147161245346, acc: 0.979721188545227)
[2025-02-13 02:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:34][root][INFO] - Training Epoch: 1/2, step 335/7134 completed (loss: 0.0848526880145073, acc: 0.976710319519043)
[2025-02-13 02:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:34][root][INFO] - Training Epoch: 1/2, step 336/7134 completed (loss: 0.09037655591964722, acc: 0.9772455096244812)
[2025-02-13 02:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:35][root][INFO] - Training Epoch: 1/2, step 337/7134 completed (loss: 0.09433452785015106, acc: 0.9723926186561584)
[2025-02-13 02:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:35][root][INFO] - Training Epoch: 1/2, step 338/7134 completed (loss: 0.11809087544679642, acc: 0.9697732925415039)
[2025-02-13 02:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:36][root][INFO] - Training Epoch: 1/2, step 339/7134 completed (loss: 0.07983049005270004, acc: 0.9800000190734863)
[2025-02-13 02:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:36][root][INFO] - Training Epoch: 1/2, step 340/7134 completed (loss: 0.12279549241065979, acc: 0.9567430019378662)
[2025-02-13 02:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37][root][INFO] - Training Epoch: 1/2, step 341/7134 completed (loss: 0.0647602304816246, acc: 0.9848675727844238)
[2025-02-13 02:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37][root][INFO] - Training Epoch: 1/2, step 342/7134 completed (loss: 0.07217320054769516, acc: 0.9863636493682861)
[2025-02-13 02:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:38][root][INFO] - Training Epoch: 1/2, step 343/7134 completed (loss: 0.09548796713352203, acc: 0.9792060256004333)
[2025-02-13 02:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:38][root][INFO] - Training Epoch: 1/2, step 344/7134 completed (loss: 0.10135448724031448, acc: 0.9726277589797974)
[2025-02-13 02:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:38][root][INFO] - Training Epoch: 1/2, step 345/7134 completed (loss: 0.06068024784326553, acc: 0.9910714030265808)
[2025-02-13 02:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:39][root][INFO] - Training Epoch: 1/2, step 346/7134 completed (loss: 0.06232508271932602, acc: 0.9793322682380676)
[2025-02-13 02:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:39][root][INFO] - Training Epoch: 1/2, step 347/7134 completed (loss: 0.07452483475208282, acc: 0.9873772859573364)
[2025-02-13 02:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40][root][INFO] - Training Epoch: 1/2, step 348/7134 completed (loss: 0.051197174936532974, acc: 0.9796472191810608)
[2025-02-13 02:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40][root][INFO] - Training Epoch: 1/2, step 349/7134 completed (loss: 0.07373582571744919, acc: 0.9838926196098328)
[2025-02-13 02:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:41][root][INFO] - Training Epoch: 1/2, step 350/7134 completed (loss: 0.10392875224351883, acc: 0.9807460904121399)
[2025-02-13 02:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:42][root][INFO] - Training Epoch: 1/2, step 351/7134 completed (loss: 0.1399422287940979, acc: 0.9656488299369812)
[2025-02-13 02:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:42][root][INFO] - Training Epoch: 1/2, step 352/7134 completed (loss: 0.09645803272724152, acc: 0.97826087474823)
[2025-02-13 02:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43][root][INFO] - Training Epoch: 1/2, step 353/7134 completed (loss: 0.09230953454971313, acc: 0.9747340679168701)
[2025-02-13 02:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43][root][INFO] - Training Epoch: 1/2, step 354/7134 completed (loss: 0.09079625457525253, acc: 0.9778812527656555)
[2025-02-13 02:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43][root][INFO] - Training Epoch: 1/2, step 355/7134 completed (loss: 0.1003207415342331, acc: 0.9818435907363892)
[2025-02-13 02:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:44][root][INFO] - Training Epoch: 1/2, step 356/7134 completed (loss: 0.06637333333492279, acc: 0.9854497313499451)
[2025-02-13 02:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:44][root][INFO] - Training Epoch: 1/2, step 357/7134 completed (loss: 0.05634325370192528, acc: 0.983460545539856)
[2025-02-13 02:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:45][root][INFO] - Training Epoch: 1/2, step 358/7134 completed (loss: 0.07088300585746765, acc: 0.9849931597709656)
[2025-02-13 02:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:45][root][INFO] - Training Epoch: 1/2, step 359/7134 completed (loss: 0.06289202719926834, acc: 0.9837837815284729)
[2025-02-13 02:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46][root][INFO] - Training Epoch: 1/2, step 360/7134 completed (loss: 0.06835559010505676, acc: 0.9849397540092468)
[2025-02-13 02:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46][root][INFO] - Training Epoch: 1/2, step 361/7134 completed (loss: 0.09784115105867386, acc: 0.9735682606697083)
[2025-02-13 02:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:47][root][INFO] - Training Epoch: 1/2, step 362/7134 completed (loss: 0.08568889647722244, acc: 0.9790419340133667)
[2025-02-13 02:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:47][root][INFO] - Training Epoch: 1/2, step 363/7134 completed (loss: 0.08617033809423447, acc: 0.98046875)
[2025-02-13 02:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:48][root][INFO] - Training Epoch: 1/2, step 364/7134 completed (loss: 0.08008130639791489, acc: 0.9766355156898499)
[2025-02-13 02:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:48][root][INFO] - Training Epoch: 1/2, step 365/7134 completed (loss: 0.06238788738846779, acc: 0.9820936918258667)
[2025-02-13 02:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49][root][INFO] - Training Epoch: 1/2, step 366/7134 completed (loss: 0.16589507460594177, acc: 0.9702194333076477)
[2025-02-13 02:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49][root][INFO] - Training Epoch: 1/2, step 367/7134 completed (loss: 0.0846545472741127, acc: 0.9762499928474426)
[2025-02-13 02:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:50][root][INFO] - Training Epoch: 1/2, step 368/7134 completed (loss: 0.06038261577486992, acc: 0.9881129264831543)
[2025-02-13 02:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:50][root][INFO] - Training Epoch: 1/2, step 369/7134 completed (loss: 0.1263887882232666, acc: 0.9663093686103821)
[2025-02-13 02:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:51][root][INFO] - Training Epoch: 1/2, step 370/7134 completed (loss: 0.11155816167593002, acc: 0.9723435044288635)
[2025-02-13 02:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:51][root][INFO] - Training Epoch: 1/2, step 371/7134 completed (loss: 0.10368257761001587, acc: 0.9759398698806763)
[2025-02-13 02:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:51][root][INFO] - Training Epoch: 1/2, step 372/7134 completed (loss: 0.0943971499800682, acc: 0.9702823162078857)
[2025-02-13 02:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52][root][INFO] - Training Epoch: 1/2, step 373/7134 completed (loss: 0.06697429716587067, acc: 0.9790732264518738)
[2025-02-13 02:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52][root][INFO] - Training Epoch: 1/2, step 374/7134 completed (loss: 0.07692272216081619, acc: 0.9855305552482605)
[2025-02-13 02:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:53][root][INFO] - Training Epoch: 1/2, step 375/7134 completed (loss: 0.1287437379360199, acc: 0.9718875288963318)
[2025-02-13 02:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:53][root][INFO] - Training Epoch: 1/2, step 376/7134 completed (loss: 0.1888076215982437, acc: 0.9531034231185913)
[2025-02-13 02:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:54][root][INFO] - Training Epoch: 1/2, step 377/7134 completed (loss: 0.12665608525276184, acc: 0.9669876098632812)
[2025-02-13 02:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:54][root][INFO] - Training Epoch: 1/2, step 378/7134 completed (loss: 0.09023445844650269, acc: 0.972752034664154)
[2025-02-13 02:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:55][root][INFO] - Training Epoch: 1/2, step 379/7134 completed (loss: 0.12030107527971268, acc: 0.9723618030548096)
[2025-02-13 02:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:55][root][INFO] - Training Epoch: 1/2, step 380/7134 completed (loss: 0.1432306319475174, acc: 0.9566360116004944)
[2025-02-13 02:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56][root][INFO] - Training Epoch: 1/2, step 381/7134 completed (loss: 0.1309838443994522, acc: 0.9702842235565186)
[2025-02-13 02:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56][root][INFO] - Training Epoch: 1/2, step 382/7134 completed (loss: 0.1275307536125183, acc: 0.9679075479507446)
[2025-02-13 02:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57][root][INFO] - Training Epoch: 1/2, step 383/7134 completed (loss: 0.0843958780169487, acc: 0.9810963869094849)
[2025-02-13 02:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57][root][INFO] - Training Epoch: 1/2, step 384/7134 completed (loss: 0.09087923169136047, acc: 0.9813084006309509)
[2025-02-13 02:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57][root][INFO] - Training Epoch: 1/2, step 385/7134 completed (loss: 0.08881379663944244, acc: 0.9843013882637024)
[2025-02-13 02:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58][root][INFO] - Training Epoch: 1/2, step 386/7134 completed (loss: 0.07466433942317963, acc: 0.9856850504875183)
[2025-02-13 02:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58][root][INFO] - Training Epoch: 1/2, step 387/7134 completed (loss: 0.16707442700862885, acc: 0.9562682509422302)
[2025-02-13 02:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:59][root][INFO] - Training Epoch: 1/2, step 388/7134 completed (loss: 0.07559280097484589, acc: 0.9835680723190308)
[2025-02-13 02:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:59][root][INFO] - Training Epoch: 1/2, step 389/7134 completed (loss: 0.15048296749591827, acc: 0.9641873240470886)
[2025-02-13 02:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00][root][INFO] - Training Epoch: 1/2, step 390/7134 completed (loss: 0.10721204429864883, acc: 0.9789643883705139)
[2025-02-13 02:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00][root][INFO] - Training Epoch: 1/2, step 391/7134 completed (loss: 0.13120977580547333, acc: 0.9642857313156128)
[2025-02-13 02:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:01][root][INFO] - Training Epoch: 1/2, step 392/7134 completed (loss: 0.12487232685089111, acc: 0.9644444584846497)
[2025-02-13 02:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:01][root][INFO] - Training Epoch: 1/2, step 393/7134 completed (loss: 0.14203494787216187, acc: 0.9611111283302307)
[2025-02-13 02:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02][root][INFO] - Training Epoch: 1/2, step 394/7134 completed (loss: 0.14316044747829437, acc: 0.9610894918441772)
[2025-02-13 02:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02][root][INFO] - Training Epoch: 1/2, step 395/7134 completed (loss: 0.13089141249656677, acc: 0.9670782089233398)
[2025-02-13 02:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02][root][INFO] - Training Epoch: 1/2, step 396/7134 completed (loss: 0.10717791318893433, acc: 0.976344108581543)
[2025-02-13 02:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:03][root][INFO] - Training Epoch: 1/2, step 397/7134 completed (loss: 0.11804592609405518, acc: 0.9642857313156128)
[2025-02-13 02:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:03][root][INFO] - Training Epoch: 1/2, step 398/7134 completed (loss: 0.11772986501455307, acc: 0.9652062058448792)
[2025-02-13 02:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04][root][INFO] - Training Epoch: 1/2, step 399/7134 completed (loss: 0.12951377034187317, acc: 0.9683042764663696)
[2025-02-13 02:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04][root][INFO] - Training Epoch: 1/2, step 400/7134 completed (loss: 0.16358168423175812, acc: 0.9597014784812927)
[2025-02-13 02:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:05][root][INFO] - Training Epoch: 1/2, step 401/7134 completed (loss: 0.0931176170706749, acc: 0.9767156839370728)
[2025-02-13 02:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:05][root][INFO] - Training Epoch: 1/2, step 402/7134 completed (loss: 0.10696543008089066, acc: 0.9670469164848328)
[2025-02-13 02:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:06][root][INFO] - Training Epoch: 1/2, step 403/7134 completed (loss: 0.10401672124862671, acc: 0.9751619696617126)
[2025-02-13 02:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:06][root][INFO] - Training Epoch: 1/2, step 404/7134 completed (loss: 0.08496038615703583, acc: 0.978205144405365)
[2025-02-13 02:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07][root][INFO] - Training Epoch: 1/2, step 405/7134 completed (loss: 0.12129299342632294, acc: 0.9672897458076477)
[2025-02-13 02:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07][root][INFO] - Training Epoch: 1/2, step 406/7134 completed (loss: 0.12915438413619995, acc: 0.966810941696167)
[2025-02-13 02:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:08][root][INFO] - Training Epoch: 1/2, step 407/7134 completed (loss: 0.24422022700309753, acc: 0.9355932474136353)
[2025-02-13 02:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:08][root][INFO] - Training Epoch: 1/2, step 408/7134 completed (loss: 0.18133293092250824, acc: 0.9629629850387573)
[2025-02-13 02:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:09][root][INFO] - Training Epoch: 1/2, step 409/7134 completed (loss: 0.12596264481544495, acc: 0.9660377502441406)
[2025-02-13 02:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:09][root][INFO] - Training Epoch: 1/2, step 410/7134 completed (loss: 0.1687048226594925, acc: 0.9629080295562744)
[2025-02-13 02:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:10][root][INFO] - Training Epoch: 1/2, step 411/7134 completed (loss: 0.1651214361190796, acc: 0.9620253443717957)
[2025-02-13 02:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:10][root][INFO] - Training Epoch: 1/2, step 412/7134 completed (loss: 0.12352949380874634, acc: 0.9633867144584656)
[2025-02-13 02:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:11][root][INFO] - Training Epoch: 1/2, step 413/7134 completed (loss: 0.11533273011445999, acc: 0.9666221737861633)
[2025-02-13 02:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:11][root][INFO] - Training Epoch: 1/2, step 414/7134 completed (loss: 0.10872127115726471, acc: 0.9747774600982666)
[2025-02-13 02:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:12][root][INFO] - Training Epoch: 1/2, step 415/7134 completed (loss: 0.07686711102724075, acc: 0.9849537014961243)
[2025-02-13 02:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:12][root][INFO] - Training Epoch: 1/2, step 416/7134 completed (loss: 0.11055388301610947, acc: 0.9657614827156067)
[2025-02-13 02:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:12][root][INFO] - Training Epoch: 1/2, step 417/7134 completed (loss: 0.14419570565223694, acc: 0.957880437374115)
[2025-02-13 02:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:13][root][INFO] - Training Epoch: 1/2, step 418/7134 completed (loss: 0.08386962860822678, acc: 0.9758898019790649)
[2025-02-13 02:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:14][root][INFO] - Training Epoch: 1/2, step 419/7134 completed (loss: 0.09636106342077255, acc: 0.9727047085762024)
[2025-02-13 02:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:14][root][INFO] - Training Epoch: 1/2, step 420/7134 completed (loss: 0.07990138232707977, acc: 0.9830795526504517)
[2025-02-13 02:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:14][root][INFO] - Training Epoch: 1/2, step 421/7134 completed (loss: 0.05934549495577812, acc: 0.9856687784194946)
[2025-02-13 02:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15][root][INFO] - Training Epoch: 1/2, step 422/7134 completed (loss: 0.07644527405500412, acc: 0.9779411554336548)
[2025-02-13 02:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15][root][INFO] - Training Epoch: 1/2, step 423/7134 completed (loss: 0.06022357568144798, acc: 0.9871428608894348)
[2025-02-13 02:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:16][root][INFO] - Training Epoch: 1/2, step 424/7134 completed (loss: 0.05704713612794876, acc: 0.982594907283783)
[2025-02-13 02:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:16][root][INFO] - Training Epoch: 1/2, step 425/7134 completed (loss: 0.03870345652103424, acc: 0.9898256063461304)
[2025-02-13 02:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:17][root][INFO] - Training Epoch: 1/2, step 426/7134 completed (loss: 0.04931362345814705, acc: 0.9874652028083801)
[2025-02-13 02:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:17][root][INFO] - Training Epoch: 1/2, step 427/7134 completed (loss: 0.05101493000984192, acc: 0.987860381603241)
[2025-02-13 02:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:18][root][INFO] - Training Epoch: 1/2, step 428/7134 completed (loss: 0.07735433429479599, acc: 0.9774096608161926)
[2025-02-13 02:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:18][root][INFO] - Training Epoch: 1/2, step 429/7134 completed (loss: 0.09795553237199783, acc: 0.9745628237724304)
[2025-02-13 02:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19][root][INFO] - Training Epoch: 1/2, step 430/7134 completed (loss: 0.08031398802995682, acc: 0.9741750359535217)
[2025-02-13 02:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19][root][INFO] - Training Epoch: 1/2, step 431/7134 completed (loss: 0.09816140681505203, acc: 0.9740633964538574)
[2025-02-13 02:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:20][root][INFO] - Training Epoch: 1/2, step 432/7134 completed (loss: 0.049046922475099564, acc: 0.9853747487068176)
[2025-02-13 02:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:20][root][INFO] - Training Epoch: 1/2, step 433/7134 completed (loss: 0.06796816736459732, acc: 0.979619562625885)
[2025-02-13 02:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:20][root][INFO] - Training Epoch: 1/2, step 434/7134 completed (loss: 0.06128278374671936, acc: 0.9868228435516357)
[2025-02-13 02:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:21][root][INFO] - Training Epoch: 1/2, step 435/7134 completed (loss: 0.09584847092628479, acc: 0.978723406791687)
[2025-02-13 02:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:21][root][INFO] - Training Epoch: 1/2, step 436/7134 completed (loss: 0.09618935734033585, acc: 0.9685039520263672)
[2025-02-13 02:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:22][root][INFO] - Training Epoch: 1/2, step 437/7134 completed (loss: 0.0852927565574646, acc: 0.9805653691291809)
[2025-02-13 02:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:22][root][INFO] - Training Epoch: 1/2, step 438/7134 completed (loss: 0.0826539471745491, acc: 0.9730769395828247)
[2025-02-13 02:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23][root][INFO] - Training Epoch: 1/2, step 439/7134 completed (loss: 0.06948358565568924, acc: 0.9842932224273682)
[2025-02-13 02:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23][root][INFO] - Training Epoch: 1/2, step 440/7134 completed (loss: 0.08644579350948334, acc: 0.9774696826934814)
[2025-02-13 02:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:24][root][INFO] - Training Epoch: 1/2, step 441/7134 completed (loss: 0.03743451461195946, acc: 0.9880059957504272)
[2025-02-13 02:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:24][root][INFO] - Training Epoch: 1/2, step 442/7134 completed (loss: 0.033400245010852814, acc: 0.9850746393203735)
[2025-02-13 02:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:25][root][INFO] - Training Epoch: 1/2, step 443/7134 completed (loss: 0.08099198341369629, acc: 0.9860464930534363)
[2025-02-13 02:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:25][root][INFO] - Training Epoch: 1/2, step 444/7134 completed (loss: 0.03966059908270836, acc: 0.9866666793823242)
[2025-02-13 02:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:25][root][INFO] - Training Epoch: 1/2, step 445/7134 completed (loss: 0.08145598322153091, acc: 0.9839650392532349)
[2025-02-13 02:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26][root][INFO] - Training Epoch: 1/2, step 446/7134 completed (loss: 0.029283598065376282, acc: 0.9916434288024902)
[2025-02-13 02:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26][root][INFO] - Training Epoch: 1/2, step 447/7134 completed (loss: 0.05454835668206215, acc: 0.9847434163093567)
[2025-02-13 02:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:27][root][INFO] - Training Epoch: 1/2, step 448/7134 completed (loss: 0.11684315651655197, acc: 0.9680306911468506)
[2025-02-13 02:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:27][root][INFO] - Training Epoch: 1/2, step 449/7134 completed (loss: 0.1736169010400772, acc: 0.963350772857666)
[2025-02-13 02:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:28][root][INFO] - Training Epoch: 1/2, step 450/7134 completed (loss: 0.12615378201007843, acc: 0.9670913219451904)
[2025-02-13 02:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:28][root][INFO] - Training Epoch: 1/2, step 451/7134 completed (loss: 0.1868448406457901, acc: 0.9571937918663025)
[2025-02-13 02:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:29][root][INFO] - Training Epoch: 1/2, step 452/7134 completed (loss: 0.18588922917842865, acc: 0.9479451775550842)
[2025-02-13 02:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:29][root][INFO] - Training Epoch: 1/2, step 453/7134 completed (loss: 0.10754285007715225, acc: 0.9779816269874573)
[2025-02-13 02:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30][root][INFO] - Training Epoch: 1/2, step 454/7134 completed (loss: 0.1524445116519928, acc: 0.9655543565750122)
[2025-02-13 02:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30][root][INFO] - Training Epoch: 1/2, step 455/7134 completed (loss: 0.15534351766109467, acc: 0.9542483687400818)
[2025-02-13 02:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:31][root][INFO] - Training Epoch: 1/2, step 456/7134 completed (loss: 0.1100858524441719, acc: 0.9730878472328186)
[2025-02-13 02:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:31][root][INFO] - Training Epoch: 1/2, step 457/7134 completed (loss: 0.11439637094736099, acc: 0.9714764952659607)
[2025-02-13 02:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:31][root][INFO] - Training Epoch: 1/2, step 458/7134 completed (loss: 0.07294245809316635, acc: 0.9821073412895203)
[2025-02-13 02:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:32][root][INFO] - Training Epoch: 1/2, step 459/7134 completed (loss: 0.16223202645778656, acc: 0.9599999785423279)
[2025-02-13 02:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:32][root][INFO] - Training Epoch: 1/2, step 460/7134 completed (loss: 0.07666367292404175, acc: 0.9795275330543518)
[2025-02-13 02:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:33][root][INFO] - Training Epoch: 1/2, step 461/7134 completed (loss: 0.0855511799454689, acc: 0.9789621233940125)
[2025-02-13 02:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:33][root][INFO] - Training Epoch: 1/2, step 462/7134 completed (loss: 0.13823403418064117, acc: 0.961448609828949)
[2025-02-13 02:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:34][root][INFO] - Training Epoch: 1/2, step 463/7134 completed (loss: 0.10671361535787582, acc: 0.9736511707305908)
[2025-02-13 02:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:34][root][INFO] - Training Epoch: 1/2, step 464/7134 completed (loss: 0.1348918080329895, acc: 0.9589235186576843)
[2025-02-13 02:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35][root][INFO] - Training Epoch: 1/2, step 465/7134 completed (loss: 0.12372393906116486, acc: 0.9660574197769165)
[2025-02-13 02:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35][root][INFO] - Training Epoch: 1/2, step 466/7134 completed (loss: 0.15891535580158234, acc: 0.953667938709259)
[2025-02-13 02:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:36][root][INFO] - Training Epoch: 1/2, step 467/7134 completed (loss: 0.1526191383600235, acc: 0.9694223999977112)
[2025-02-13 02:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:36][root][INFO] - Training Epoch: 1/2, step 468/7134 completed (loss: 0.20080554485321045, acc: 0.9518569707870483)
[2025-02-13 02:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:37][root][INFO] - Training Epoch: 1/2, step 469/7134 completed (loss: 0.12947797775268555, acc: 0.9711815714836121)
[2025-02-13 02:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:37][root][INFO] - Training Epoch: 1/2, step 470/7134 completed (loss: 0.13550809025764465, acc: 0.9646258354187012)
[2025-02-13 02:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:38][root][INFO] - Training Epoch: 1/2, step 471/7134 completed (loss: 0.11860077828168869, acc: 0.9656804800033569)
[2025-02-13 02:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:38][root][INFO] - Training Epoch: 1/2, step 472/7134 completed (loss: 0.1354246586561203, acc: 0.9648093581199646)
[2025-02-13 02:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:38][root][INFO] - Training Epoch: 1/2, step 473/7134 completed (loss: 0.11140167713165283, acc: 0.9614197611808777)
[2025-02-13 02:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39][root][INFO] - Training Epoch: 1/2, step 474/7134 completed (loss: 0.10620629042387009, acc: 0.9736495614051819)
[2025-02-13 02:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39][root][INFO] - Training Epoch: 1/2, step 475/7134 completed (loss: 0.16236384212970734, acc: 0.9617646932601929)
[2025-02-13 02:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:40][root][INFO] - Training Epoch: 1/2, step 476/7134 completed (loss: 0.18477685749530792, acc: 0.9570200443267822)
[2025-02-13 02:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:40][root][INFO] - Training Epoch: 1/2, step 477/7134 completed (loss: 0.13355211913585663, acc: 0.9633252024650574)
[2025-02-13 02:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:41][root][INFO] - Training Epoch: 1/2, step 478/7134 completed (loss: 0.1958112269639969, acc: 0.9450317025184631)
[2025-02-13 02:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:41][root][INFO] - Training Epoch: 1/2, step 479/7134 completed (loss: 0.07037080079317093, acc: 0.9784792065620422)
[2025-02-13 02:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:42][root][INFO] - Training Epoch: 1/2, step 480/7134 completed (loss: 0.04712621867656708, acc: 0.9839416146278381)
[2025-02-13 02:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:42][root][INFO] - Training Epoch: 1/2, step 481/7134 completed (loss: 0.07316552847623825, acc: 0.98046875)
[2025-02-13 02:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43][root][INFO] - Training Epoch: 1/2, step 482/7134 completed (loss: 0.05397874489426613, acc: 0.9842857122421265)
[2025-02-13 02:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43][root][INFO] - Training Epoch: 1/2, step 483/7134 completed (loss: 0.04974221810698509, acc: 0.9815725088119507)
[2025-02-13 02:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43][root][INFO] - Training Epoch: 1/2, step 484/7134 completed (loss: 0.0907217413187027, acc: 0.9772727489471436)
[2025-02-13 02:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:44][root][INFO] - Training Epoch: 1/2, step 485/7134 completed (loss: 0.13443268835544586, acc: 0.9727685451507568)
[2025-02-13 02:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:44][root][INFO] - Training Epoch: 1/2, step 486/7134 completed (loss: 0.12449400126934052, acc: 0.9671052694320679)
[2025-02-13 02:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:45][root][INFO] - Training Epoch: 1/2, step 487/7134 completed (loss: 0.06272383779287338, acc: 0.98531574010849)
[2025-02-13 02:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:45][root][INFO] - Training Epoch: 1/2, step 488/7134 completed (loss: 0.1523566097021103, acc: 0.9710144996643066)
[2025-02-13 02:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46][root][INFO] - Training Epoch: 1/2, step 489/7134 completed (loss: 0.10127180069684982, acc: 0.978151261806488)
[2025-02-13 02:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46][root][INFO] - Training Epoch: 1/2, step 490/7134 completed (loss: 0.12445924431085587, acc: 0.9673469662666321)
[2025-02-13 02:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:47][root][INFO] - Training Epoch: 1/2, step 491/7134 completed (loss: 0.08698499947786331, acc: 0.9753289222717285)
[2025-02-13 02:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:47][root][INFO] - Training Epoch: 1/2, step 492/7134 completed (loss: 0.03696407005190849, acc: 0.9924356937408447)
[2025-02-13 02:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:47][root][INFO] - Training Epoch: 1/2, step 493/7134 completed (loss: 0.11251508444547653, acc: 0.9684813618659973)
[2025-02-13 02:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:48][root][INFO] - Training Epoch: 1/2, step 494/7134 completed (loss: 0.05277524143457413, acc: 0.987500011920929)
[2025-02-13 02:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:48][root][INFO] - Training Epoch: 1/2, step 495/7134 completed (loss: 0.07026470452547073, acc: 0.9823529124259949)
[2025-02-13 02:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49][root][INFO] - Training Epoch: 1/2, step 496/7134 completed (loss: 0.11881456524133682, acc: 0.9701896905899048)
[2025-02-13 02:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49][root][INFO] - Training Epoch: 1/2, step 497/7134 completed (loss: 0.11968944221735, acc: 0.9748061895370483)
[2025-02-13 02:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:50][root][INFO] - Training Epoch: 1/2, step 498/7134 completed (loss: 0.06743313372135162, acc: 0.9823718070983887)
[2025-02-13 02:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:50][root][INFO] - Training Epoch: 1/2, step 499/7134 completed (loss: 0.06376691907644272, acc: 0.9819193482398987)
[2025-02-13 02:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:51][root][INFO] - Training Epoch: 1/2, step 500/7134 completed (loss: 0.08304479718208313, acc: 0.981333315372467)
[2025-02-13 02:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:51][root][INFO] - Training Epoch: 1/2, step 501/7134 completed (loss: 0.044161055237054825, acc: 0.9889196753501892)
[2025-02-13 02:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52][root][INFO] - Training Epoch: 1/2, step 502/7134 completed (loss: 0.03577835485339165, acc: 0.9908257126808167)
[2025-02-13 02:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52][root][INFO] - Training Epoch: 1/2, step 503/7134 completed (loss: 0.08383291959762573, acc: 0.9798657894134521)
[2025-02-13 02:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52][root][INFO] - Training Epoch: 1/2, step 504/7134 completed (loss: 0.038690999150276184, acc: 0.9902234673500061)
[2025-02-13 02:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:53][root][INFO] - Training Epoch: 1/2, step 505/7134 completed (loss: 0.03553512692451477, acc: 0.9921466112136841)
[2025-02-13 02:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:53][root][INFO] - Training Epoch: 1/2, step 506/7134 completed (loss: 0.04105300456285477, acc: 0.9888059496879578)
[2025-02-13 02:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54][root][INFO] - Training Epoch: 1/2, step 507/7134 completed (loss: 0.06834007799625397, acc: 0.9815789461135864)
[2025-02-13 02:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54][root][INFO] - Training Epoch: 1/2, step 508/7134 completed (loss: 0.06426247209310532, acc: 0.9832402467727661)
[2025-02-13 02:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:55][root][INFO] - Training Epoch: 1/2, step 509/7134 completed (loss: 0.06663229316473007, acc: 0.9829984307289124)
[2025-02-13 02:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:55][root][INFO] - Training Epoch: 1/2, step 510/7134 completed (loss: 0.167348250746727, acc: 0.9646569490432739)
[2025-02-13 02:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:56][root][INFO] - Training Epoch: 1/2, step 511/7134 completed (loss: 0.11961241066455841, acc: 0.97555011510849)
[2025-02-13 02:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:56][root][INFO] - Training Epoch: 1/2, step 512/7134 completed (loss: 0.14344185590744019, acc: 0.9659318923950195)
[2025-02-13 02:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57][root][INFO] - Training Epoch: 1/2, step 513/7134 completed (loss: 0.12849301099777222, acc: 0.9690189361572266)
[2025-02-13 02:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57][root][INFO] - Training Epoch: 1/2, step 514/7134 completed (loss: 0.1118001714348793, acc: 0.9748743772506714)
[2025-02-13 02:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58][root][INFO] - Training Epoch: 1/2, step 515/7134 completed (loss: 0.07840165495872498, acc: 0.987270176410675)
[2025-02-13 02:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58][root][INFO] - Training Epoch: 1/2, step 516/7134 completed (loss: 0.06930571049451828, acc: 0.986270010471344)
[2025-02-13 02:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58][root][INFO] - Training Epoch: 1/2, step 517/7134 completed (loss: 0.10998497903347015, acc: 0.9749103784561157)
[2025-02-13 02:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:59][root][INFO] - Training Epoch: 1/2, step 518/7134 completed (loss: 0.055105697363615036, acc: 0.9865067601203918)
[2025-02-13 02:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:59][root][INFO] - Training Epoch: 1/2, step 519/7134 completed (loss: 0.08258558809757233, acc: 0.9793814420700073)
[2025-02-13 02:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:00][root][INFO] - Training Epoch: 1/2, step 520/7134 completed (loss: 0.07331328839063644, acc: 0.9794303774833679)
[2025-02-13 02:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:00][root][INFO] - Training Epoch: 1/2, step 521/7134 completed (loss: 0.11942463368177414, acc: 0.9746328592300415)
[2025-02-13 02:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:01][root][INFO] - Training Epoch: 1/2, step 522/7134 completed (loss: 0.05947446823120117, acc: 0.98525470495224)
[2025-02-13 02:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:01][root][INFO] - Training Epoch: 1/2, step 523/7134 completed (loss: 0.07532313466072083, acc: 0.9795321822166443)
[2025-02-13 02:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02][root][INFO] - Training Epoch: 1/2, step 524/7134 completed (loss: 0.04876477271318436, acc: 0.9905914068222046)
[2025-02-13 02:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02][root][INFO] - Training Epoch: 1/2, step 525/7134 completed (loss: 0.11877340823411942, acc: 0.9814528822898865)
[2025-02-13 02:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03][root][INFO] - Training Epoch: 1/2, step 526/7134 completed (loss: 0.07337089627981186, acc: 0.983561635017395)
[2025-02-13 02:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03][root][INFO] - Training Epoch: 1/2, step 527/7134 completed (loss: 0.07307770103216171, acc: 0.986940324306488)
[2025-02-13 02:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03][root][INFO] - Training Epoch: 1/2, step 528/7134 completed (loss: 0.049159880727529526, acc: 0.98591548204422)
[2025-02-13 02:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:04][root][INFO] - Training Epoch: 1/2, step 529/7134 completed (loss: 0.10540059208869934, acc: 0.9746031761169434)
[2025-02-13 02:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:04][root][INFO] - Training Epoch: 1/2, step 530/7134 completed (loss: 0.08074715733528137, acc: 0.9742709994316101)
[2025-02-13 02:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:05][root][INFO] - Training Epoch: 1/2, step 531/7134 completed (loss: 0.08810634166002274, acc: 0.9811965823173523)
[2025-02-13 02:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:05][root][INFO] - Training Epoch: 1/2, step 532/7134 completed (loss: 0.032171521335840225, acc: 0.9916840195655823)
[2025-02-13 02:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:06][root][INFO] - Training Epoch: 1/2, step 533/7134 completed (loss: 0.11714804172515869, acc: 0.9742268323898315)
[2025-02-13 02:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:06][root][INFO] - Training Epoch: 1/2, step 534/7134 completed (loss: 0.027076810598373413, acc: 0.9903846383094788)
[2025-02-13 02:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:07][root][INFO] - Training Epoch: 1/2, step 535/7134 completed (loss: 0.05712336674332619, acc: 0.9789103865623474)
[2025-02-13 02:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:07][root][INFO] - Training Epoch: 1/2, step 536/7134 completed (loss: 0.10366051644086838, acc: 0.9766423106193542)
[2025-02-13 02:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08][root][INFO] - Training Epoch: 1/2, step 537/7134 completed (loss: 0.1328362077474594, acc: 0.9661266803741455)
[2025-02-13 02:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08][root][INFO] - Training Epoch: 1/2, step 538/7134 completed (loss: 0.09497124701738358, acc: 0.9732937812805176)
[2025-02-13 02:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08][root][INFO] - Training Epoch: 1/2, step 539/7134 completed (loss: 0.0650218278169632, acc: 0.9861591458320618)
[2025-02-13 02:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:09][root][INFO] - Training Epoch: 1/2, step 540/7134 completed (loss: 0.07227620482444763, acc: 0.9777777791023254)
[2025-02-13 02:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:09][root][INFO] - Training Epoch: 1/2, step 541/7134 completed (loss: 0.0775592252612114, acc: 0.980053186416626)
[2025-02-13 02:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:10][root][INFO] - Training Epoch: 1/2, step 542/7134 completed (loss: 0.16044215857982635, acc: 0.9624413251876831)
[2025-02-13 02:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:10][root][INFO] - Training Epoch: 1/2, step 543/7134 completed (loss: 0.16291724145412445, acc: 0.9511753916740417)
[2025-02-13 02:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:11][root][INFO] - Training Epoch: 1/2, step 544/7134 completed (loss: 0.09957072883844376, acc: 0.9700315594673157)
[2025-02-13 02:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:11][root][INFO] - Training Epoch: 1/2, step 545/7134 completed (loss: 0.12841250002384186, acc: 0.9645732641220093)
[2025-02-13 02:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12][root][INFO] - Training Epoch: 1/2, step 546/7134 completed (loss: 0.1697893738746643, acc: 0.957446813583374)
[2025-02-13 02:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12][root][INFO] - Training Epoch: 1/2, step 547/7134 completed (loss: 0.10312780737876892, acc: 0.9738751649856567)
[2025-02-13 02:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12][root][INFO] - Training Epoch: 1/2, step 548/7134 completed (loss: 0.05115820840001106, acc: 0.9882659912109375)
[2025-02-13 02:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:13][root][INFO] - Training Epoch: 1/2, step 549/7134 completed (loss: 0.1470152735710144, acc: 0.9642857313156128)
[2025-02-13 02:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:13][root][INFO] - Training Epoch: 1/2, step 550/7134 completed (loss: 0.0750376358628273, acc: 0.9846416115760803)
[2025-02-13 02:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:14][root][INFO] - Training Epoch: 1/2, step 551/7134 completed (loss: 0.08612262457609177, acc: 0.970534086227417)
[2025-02-13 02:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:14][root][INFO] - Training Epoch: 1/2, step 552/7134 completed (loss: 0.07392027974128723, acc: 0.9786324501037598)
[2025-02-13 02:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:15][root][INFO] - Training Epoch: 1/2, step 553/7134 completed (loss: 0.057633303105831146, acc: 0.9802631735801697)
[2025-02-13 02:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:15][root][INFO] - Training Epoch: 1/2, step 554/7134 completed (loss: 0.07872506976127625, acc: 0.9771754741668701)
[2025-02-13 02:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:15][root][INFO] - Training Epoch: 1/2, step 555/7134 completed (loss: 0.10954926162958145, acc: 0.971563994884491)
[2025-02-13 02:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16][root][INFO] - Training Epoch: 1/2, step 556/7134 completed (loss: 0.05247533693909645, acc: 0.9900990128517151)
[2025-02-13 02:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16][root][INFO] - Training Epoch: 1/2, step 557/7134 completed (loss: 0.10009019821882248, acc: 0.9593908786773682)
[2025-02-13 02:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:17][root][INFO] - Training Epoch: 1/2, step 558/7134 completed (loss: 0.06189243867993355, acc: 0.9773049354553223)
[2025-02-13 02:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:17][root][INFO] - Training Epoch: 1/2, step 559/7134 completed (loss: 0.13089966773986816, acc: 0.9629629850387573)
[2025-02-13 02:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:18][root][INFO] - Training Epoch: 1/2, step 560/7134 completed (loss: 0.08672807365655899, acc: 0.9771528840065002)
[2025-02-13 02:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:18][root][INFO] - Training Epoch: 1/2, step 561/7134 completed (loss: 0.10076001286506653, acc: 0.9731800556182861)
[2025-02-13 02:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19][root][INFO] - Training Epoch: 1/2, step 562/7134 completed (loss: 0.06104888394474983, acc: 0.9801324605941772)
[2025-02-13 02:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19][root][INFO] - Training Epoch: 1/2, step 563/7134 completed (loss: 0.1542644500732422, acc: 0.9667721390724182)
[2025-02-13 02:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:20][root][INFO] - Training Epoch: 1/2, step 564/7134 completed (loss: 0.09148257225751877, acc: 0.9752781391143799)
[2025-02-13 02:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:20][root][INFO] - Training Epoch: 1/2, step 565/7134 completed (loss: 0.08681126683950424, acc: 0.9782244563102722)
[2025-02-13 02:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:21][root][INFO] - Training Epoch: 1/2, step 566/7134 completed (loss: 0.18347744643688202, acc: 0.9628770351409912)
[2025-02-13 02:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:21][root][INFO] - Training Epoch: 1/2, step 567/7134 completed (loss: 0.08896849304437637, acc: 0.9747545719146729)
[2025-02-13 02:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:21][root][INFO] - Training Epoch: 1/2, step 568/7134 completed (loss: 0.06848703324794769, acc: 0.9816993474960327)
[2025-02-13 02:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:22][root][INFO] - Training Epoch: 1/2, step 569/7134 completed (loss: 0.04910572990775108, acc: 0.9851852059364319)
[2025-02-13 02:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:22][root][INFO] - Training Epoch: 1/2, step 570/7134 completed (loss: 0.08150900155305862, acc: 0.9706227779388428)
[2025-02-13 02:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23][root][INFO] - Training Epoch: 1/2, step 571/7134 completed (loss: 0.07076052576303482, acc: 0.9773299694061279)
[2025-02-13 02:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23][root][INFO] - Training Epoch: 1/2, step 572/7134 completed (loss: 0.04196017608046532, acc: 0.9858430027961731)
[2025-02-13 02:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:24][root][INFO] - Training Epoch: 1/2, step 573/7134 completed (loss: 0.05020921677350998, acc: 0.9897698163986206)
[2025-02-13 02:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:24][root][INFO] - Training Epoch: 1/2, step 574/7134 completed (loss: 0.05818774923682213, acc: 0.9894598126411438)
[2025-02-13 02:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:25][root][INFO] - Training Epoch: 1/2, step 575/7134 completed (loss: 0.061359405517578125, acc: 0.980719804763794)
[2025-02-13 02:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:25][root][INFO] - Training Epoch: 1/2, step 576/7134 completed (loss: 0.039443239569664, acc: 0.9876237511634827)
[2025-02-13 02:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:26][root][INFO] - Training Epoch: 1/2, step 577/7134 completed (loss: 0.05670282989740372, acc: 0.9802371263504028)
[2025-02-13 02:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:26][root][INFO] - Training Epoch: 1/2, step 578/7134 completed (loss: 0.058721140027046204, acc: 0.9779582619667053)
[2025-02-13 02:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27][root][INFO] - Training Epoch: 1/2, step 579/7134 completed (loss: 0.1504107117652893, acc: 0.9640564918518066)
[2025-02-13 02:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27][root][INFO] - Training Epoch: 1/2, step 580/7134 completed (loss: 0.09789664298295975, acc: 0.9674902558326721)
[2025-02-13 02:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:28][root][INFO] - Training Epoch: 1/2, step 581/7134 completed (loss: 0.06255538761615753, acc: 0.9818689227104187)
[2025-02-13 02:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:28][root][INFO] - Training Epoch: 1/2, step 582/7134 completed (loss: 0.07030349224805832, acc: 0.9832335114479065)
[2025-02-13 02:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:29][root][INFO] - Training Epoch: 1/2, step 583/7134 completed (loss: 0.07720425724983215, acc: 0.9779220819473267)
[2025-02-13 02:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:29][root][INFO] - Training Epoch: 1/2, step 584/7134 completed (loss: 0.04138703644275665, acc: 0.989234447479248)
[2025-02-13 02:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:29][root][INFO] - Training Epoch: 1/2, step 585/7134 completed (loss: 0.05230659991502762, acc: 0.990338146686554)
[2025-02-13 02:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:30][root][INFO] - Training Epoch: 1/2, step 586/7134 completed (loss: 0.0529901348054409, acc: 0.9817470908164978)
[2025-02-13 02:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:30][root][INFO] - Training Epoch: 1/2, step 587/7134 completed (loss: 0.05860458314418793, acc: 0.9860228896141052)
[2025-02-13 02:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31][root][INFO] - Training Epoch: 1/2, step 588/7134 completed (loss: 0.06842184066772461, acc: 0.9843013882637024)
[2025-02-13 02:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31][root][INFO] - Training Epoch: 1/2, step 589/7134 completed (loss: 0.050299886614084244, acc: 0.9838472604751587)
[2025-02-13 02:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:32][root][INFO] - Training Epoch: 1/2, step 590/7134 completed (loss: 0.039058294147253036, acc: 0.9924812316894531)
[2025-02-13 02:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:32][root][INFO] - Training Epoch: 1/2, step 591/7134 completed (loss: 0.052544090896844864, acc: 0.9842615127563477)
[2025-02-13 02:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:33][root][INFO] - Training Epoch: 1/2, step 592/7134 completed (loss: 0.05738256499171257, acc: 0.9835025668144226)
[2025-02-13 02:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:33][root][INFO] - Training Epoch: 1/2, step 593/7134 completed (loss: 0.07104373723268509, acc: 0.9816272854804993)
[2025-02-13 02:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34][root][INFO] - Training Epoch: 1/2, step 594/7134 completed (loss: 0.0682639628648758, acc: 0.9798741936683655)
[2025-02-13 02:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34][root][INFO] - Training Epoch: 1/2, step 595/7134 completed (loss: 0.08820150047540665, acc: 0.9797724485397339)
[2025-02-13 02:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:35][root][INFO] - Training Epoch: 1/2, step 596/7134 completed (loss: 0.05182364210486412, acc: 0.9819587469100952)
[2025-02-13 02:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:35][root][INFO] - Training Epoch: 1/2, step 597/7134 completed (loss: 0.1627713292837143, acc: 0.9525691866874695)
[2025-02-13 02:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36][root][INFO] - Training Epoch: 1/2, step 598/7134 completed (loss: 0.0960071012377739, acc: 0.9661704897880554)
[2025-02-13 02:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36][root][INFO] - Training Epoch: 1/2, step 599/7134 completed (loss: 0.054331179708242416, acc: 0.9837092757225037)
[2025-02-13 02:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36][root][INFO] - Training Epoch: 1/2, step 600/7134 completed (loss: 0.07449531555175781, acc: 0.9819494485855103)
[2025-02-13 02:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:37][root][INFO] - Training Epoch: 1/2, step 601/7134 completed (loss: 0.0620541088283062, acc: 0.9861687421798706)
[2025-02-13 02:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:37][root][INFO] - Training Epoch: 1/2, step 602/7134 completed (loss: 0.07406668365001678, acc: 0.9779582619667053)
[2025-02-13 02:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38][root][INFO] - Training Epoch: 1/2, step 603/7134 completed (loss: 0.07243422418832779, acc: 0.9781491160392761)
[2025-02-13 02:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38][root][INFO] - Training Epoch: 1/2, step 604/7134 completed (loss: 0.061824798583984375, acc: 0.9795022010803223)
[2025-02-13 02:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:39][root][INFO] - Training Epoch: 1/2, step 605/7134 completed (loss: 0.08000365644693375, acc: 0.9805699586868286)
[2025-02-13 02:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:39][root][INFO] - Training Epoch: 1/2, step 606/7134 completed (loss: 0.16647113859653473, acc: 0.9629172086715698)
[2025-02-13 02:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:40][root][INFO] - Training Epoch: 1/2, step 607/7134 completed (loss: 0.08145845681428909, acc: 0.9826435446739197)
[2025-02-13 02:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:40][root][INFO] - Training Epoch: 1/2, step 608/7134 completed (loss: 0.08440737426280975, acc: 0.9766839146614075)
[2025-02-13 02:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41][root][INFO] - Training Epoch: 1/2, step 609/7134 completed (loss: 0.08798572421073914, acc: 0.9718969464302063)
[2025-02-13 02:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41][root][INFO] - Training Epoch: 1/2, step 610/7134 completed (loss: 0.11605720967054367, acc: 0.9696586728096008)
[2025-02-13 02:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:42][root][INFO] - Training Epoch: 1/2, step 611/7134 completed (loss: 0.09324967116117477, acc: 0.9780488014221191)
[2025-02-13 02:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:42][root][INFO] - Training Epoch: 1/2, step 612/7134 completed (loss: 0.08116048574447632, acc: 0.9775725603103638)
[2025-02-13 02:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43][root][INFO] - Training Epoch: 1/2, step 613/7134 completed (loss: 0.08689197897911072, acc: 0.9765533208847046)
[2025-02-13 02:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43][root][INFO] - Training Epoch: 1/2, step 614/7134 completed (loss: 0.05431988835334778, acc: 0.9852774739265442)
[2025-02-13 02:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:44][root][INFO] - Training Epoch: 1/2, step 615/7134 completed (loss: 0.06556476652622223, acc: 0.982822060585022)
[2025-02-13 02:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:44][root][INFO] - Training Epoch: 1/2, step 616/7134 completed (loss: 0.08890438079833984, acc: 0.9761363863945007)
[2025-02-13 02:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45][root][INFO] - Training Epoch: 1/2, step 617/7134 completed (loss: 0.05389245226979256, acc: 0.981776773929596)
[2025-02-13 02:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45][root][INFO] - Training Epoch: 1/2, step 618/7134 completed (loss: 0.04654359072446823, acc: 0.9893048405647278)
[2025-02-13 02:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45][root][INFO] - Training Epoch: 1/2, step 619/7134 completed (loss: 0.12376616150140762, acc: 0.97773277759552)
[2025-02-13 02:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:46][root][INFO] - Training Epoch: 1/2, step 620/7134 completed (loss: 0.12160778790712357, acc: 0.9745222926139832)
[2025-02-13 02:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:46][root][INFO] - Training Epoch: 1/2, step 621/7134 completed (loss: 0.09650955349206924, acc: 0.9756468534469604)
[2025-02-13 02:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:47][root][INFO] - Training Epoch: 1/2, step 622/7134 completed (loss: 0.061257023364305496, acc: 0.9842180609703064)
[2025-02-13 02:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:47][root][INFO] - Training Epoch: 1/2, step 623/7134 completed (loss: 0.10514935851097107, acc: 0.9795918464660645)
[2025-02-13 02:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48][root][INFO] - Training Epoch: 1/2, step 624/7134 completed (loss: 0.04237332567572594, acc: 0.9878378510475159)
[2025-02-13 02:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48][root][INFO] - Training Epoch: 1/2, step 625/7134 completed (loss: 0.07110968977212906, acc: 0.9775725603103638)
[2025-02-13 02:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:49][root][INFO] - Training Epoch: 1/2, step 626/7134 completed (loss: 0.11132700741291046, acc: 0.975944995880127)
[2025-02-13 02:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:49][root][INFO] - Training Epoch: 1/2, step 627/7134 completed (loss: 0.05807018652558327, acc: 0.9778761267662048)
[2025-02-13 02:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50][root][INFO] - Training Epoch: 1/2, step 628/7134 completed (loss: 0.06029943749308586, acc: 0.9875173568725586)
[2025-02-13 02:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50][root][INFO] - Training Epoch: 1/2, step 629/7134 completed (loss: 0.07119147479534149, acc: 0.9814019799232483)
[2025-02-13 02:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50][root][INFO] - Training Epoch: 1/2, step 630/7134 completed (loss: 0.06396333873271942, acc: 0.9805699586868286)
[2025-02-13 02:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:51][root][INFO] - Training Epoch: 1/2, step 631/7134 completed (loss: 0.06258156150579453, acc: 0.9815789461135864)
[2025-02-13 02:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:51][root][INFO] - Training Epoch: 1/2, step 632/7134 completed (loss: 0.09312667697668076, acc: 0.9741100072860718)
[2025-02-13 02:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:52][root][INFO] - Training Epoch: 1/2, step 633/7134 completed (loss: 0.06369899213314056, acc: 0.9840348362922668)
[2025-02-13 02:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:52][root][INFO] - Training Epoch: 1/2, step 634/7134 completed (loss: 0.05590426176786423, acc: 0.9841772317886353)
[2025-02-13 02:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:53][root][INFO] - Training Epoch: 1/2, step 635/7134 completed (loss: 0.07286234200000763, acc: 0.9755725264549255)
[2025-02-13 02:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:53][root][INFO] - Training Epoch: 1/2, step 636/7134 completed (loss: 0.09980278462171555, acc: 0.9765517115592957)
[2025-02-13 02:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54][root][INFO] - Training Epoch: 1/2, step 637/7134 completed (loss: 0.09144823253154755, acc: 0.9757914543151855)
[2025-02-13 02:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54][root][INFO] - Training Epoch: 1/2, step 638/7134 completed (loss: 0.11045963317155838, acc: 0.9719188809394836)
[2025-02-13 02:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54][root][INFO] - Training Epoch: 1/2, step 639/7134 completed (loss: 0.07277161628007889, acc: 0.9816666841506958)
[2025-02-13 02:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:55][root][INFO] - Training Epoch: 1/2, step 640/7134 completed (loss: 0.06492000818252563, acc: 0.9843478202819824)
[2025-02-13 02:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:55][root][INFO] - Training Epoch: 1/2, step 641/7134 completed (loss: 0.07266325503587723, acc: 0.9770867228507996)
[2025-02-13 02:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56][root][INFO] - Training Epoch: 1/2, step 642/7134 completed (loss: 0.05425233766436577, acc: 0.9897959232330322)
[2025-02-13 02:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56][root][INFO] - Training Epoch: 1/2, step 643/7134 completed (loss: 0.053255464881658554, acc: 0.9820282459259033)
[2025-02-13 02:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:57][root][INFO] - Training Epoch: 1/2, step 644/7134 completed (loss: 0.04417026787996292, acc: 0.9884169697761536)
[2025-02-13 02:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:57][root][INFO] - Training Epoch: 1/2, step 645/7134 completed (loss: 0.05093194544315338, acc: 0.9882155060768127)
[2025-02-13 02:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:58][root][INFO] - Training Epoch: 1/2, step 646/7134 completed (loss: 0.10005781799554825, acc: 0.9785932898521423)
[2025-02-13 02:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:58][root][INFO] - Training Epoch: 1/2, step 647/7134 completed (loss: 0.04724728688597679, acc: 0.9830795526504517)
[2025-02-13 02:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59][root][INFO] - Training Epoch: 1/2, step 648/7134 completed (loss: 0.12251601368188858, acc: 0.9650959968566895)
[2025-02-13 02:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59][root][INFO] - Training Epoch: 1/2, step 649/7134 completed (loss: 0.08286292105913162, acc: 0.9769230484962463)
[2025-02-13 02:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59][root][INFO] - Training Epoch: 1/2, step 650/7134 completed (loss: 0.0913710966706276, acc: 0.9733123779296875)
[2025-02-13 02:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00][root][INFO] - Training Epoch: 1/2, step 651/7134 completed (loss: 0.14501197636127472, acc: 0.9602446556091309)
[2025-02-13 02:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00][root][INFO] - Training Epoch: 1/2, step 652/7134 completed (loss: 0.0741693302989006, acc: 0.975944995880127)
[2025-02-13 02:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:01][root][INFO] - Training Epoch: 1/2, step 653/7134 completed (loss: 0.05914163962006569, acc: 0.9758865237236023)
[2025-02-13 02:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:01][root][INFO] - Training Epoch: 1/2, step 654/7134 completed (loss: 0.21720384061336517, acc: 0.948164165019989)
[2025-02-13 02:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02][root][INFO] - Training Epoch: 1/2, step 655/7134 completed (loss: 0.08265130966901779, acc: 0.9778226017951965)
[2025-02-13 02:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02][root][INFO] - Training Epoch: 1/2, step 656/7134 completed (loss: 0.09218089282512665, acc: 0.9656991958618164)
[2025-02-13 02:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:03][root][INFO] - Training Epoch: 1/2, step 657/7134 completed (loss: 0.09627960622310638, acc: 0.9695023894309998)
[2025-02-13 02:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:03][root][INFO] - Training Epoch: 1/2, step 658/7134 completed (loss: 0.045758508145809174, acc: 0.9881423115730286)
[2025-02-13 02:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:04][root][INFO] - Training Epoch: 1/2, step 659/7134 completed (loss: 0.06496412307024002, acc: 0.9824047088623047)
[2025-02-13 02:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:04][root][INFO] - Training Epoch: 1/2, step 660/7134 completed (loss: 0.10161098837852478, acc: 0.9751461744308472)
[2025-02-13 02:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05][root][INFO] - Training Epoch: 1/2, step 661/7134 completed (loss: 0.13414452970027924, acc: 0.9696652889251709)
[2025-02-13 02:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05][root][INFO] - Training Epoch: 1/2, step 662/7134 completed (loss: 0.06412287056446075, acc: 0.9865951538085938)
[2025-02-13 02:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:06][root][INFO] - Training Epoch: 1/2, step 663/7134 completed (loss: 0.061997316777706146, acc: 0.9876543283462524)
[2025-02-13 02:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:06][root][INFO] - Training Epoch: 1/2, step 664/7134 completed (loss: 0.07073628157377243, acc: 0.9816666841506958)
[2025-02-13 02:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:07][root][INFO] - Training Epoch: 1/2, step 665/7134 completed (loss: 0.0833018571138382, acc: 0.9690443873405457)
[2025-02-13 02:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:07][root][INFO] - Training Epoch: 1/2, step 666/7134 completed (loss: 0.09015856683254242, acc: 0.9752747416496277)
[2025-02-13 02:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:07][root][INFO] - Training Epoch: 1/2, step 667/7134 completed (loss: 0.055982183665037155, acc: 0.9837996959686279)
[2025-02-13 02:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:08][root][INFO] - Training Epoch: 1/2, step 668/7134 completed (loss: 0.04883807525038719, acc: 0.9885931611061096)
[2025-02-13 02:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:08][root][INFO] - Training Epoch: 1/2, step 669/7134 completed (loss: 0.071378692984581, acc: 0.9836289286613464)
[2025-02-13 02:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09][root][INFO] - Training Epoch: 1/2, step 670/7134 completed (loss: 0.08289128541946411, acc: 0.9722792506217957)
[2025-02-13 02:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09][root][INFO] - Training Epoch: 1/2, step 671/7134 completed (loss: 0.06999661773443222, acc: 0.9817792177200317)
[2025-02-13 02:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:10][root][INFO] - Training Epoch: 1/2, step 672/7134 completed (loss: 0.09270072728395462, acc: 0.9806259274482727)
[2025-02-13 02:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:10][root][INFO] - Training Epoch: 1/2, step 673/7134 completed (loss: 0.09376244992017746, acc: 0.9815725088119507)
[2025-02-13 02:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:11][root][INFO] - Training Epoch: 1/2, step 674/7134 completed (loss: 0.08188456296920776, acc: 0.9788838624954224)
[2025-02-13 02:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:11][root][INFO] - Training Epoch: 1/2, step 675/7134 completed (loss: 0.08731495589017868, acc: 0.9843546152114868)
[2025-02-13 02:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:12][root][INFO] - Training Epoch: 1/2, step 676/7134 completed (loss: 0.08490464836359024, acc: 0.9798816442489624)
[2025-02-13 02:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:12][root][INFO] - Training Epoch: 1/2, step 677/7134 completed (loss: 0.037860702723264694, acc: 0.9913686513900757)
[2025-02-13 02:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13][root][INFO] - Training Epoch: 1/2, step 678/7134 completed (loss: 0.06831877678632736, acc: 0.9798115491867065)
[2025-02-13 02:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13][root][INFO] - Training Epoch: 1/2, step 679/7134 completed (loss: 0.033821430057287216, acc: 0.9886578321456909)
[2025-02-13 02:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:14][root][INFO] - Training Epoch: 1/2, step 680/7134 completed (loss: 0.09017441421747208, acc: 0.9757412672042847)
[2025-02-13 02:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:14][root][INFO] - Training Epoch: 1/2, step 681/7134 completed (loss: 0.04932686313986778, acc: 0.987730085849762)
[2025-02-13 02:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:15][root][INFO] - Training Epoch: 1/2, step 682/7134 completed (loss: 0.045474693179130554, acc: 0.9884726405143738)
[2025-02-13 02:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:15][root][INFO] - Training Epoch: 1/2, step 683/7134 completed (loss: 0.062179721891880035, acc: 0.9794238805770874)
[2025-02-13 02:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16][root][INFO] - Training Epoch: 1/2, step 684/7134 completed (loss: 0.08932823687791824, acc: 0.9709141254425049)
[2025-02-13 02:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16][root][INFO] - Training Epoch: 1/2, step 685/7134 completed (loss: 0.07042258977890015, acc: 0.9837905168533325)
[2025-02-13 02:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:17][root][INFO] - Training Epoch: 1/2, step 686/7134 completed (loss: 0.025628557428717613, acc: 0.9894319772720337)
[2025-02-13 02:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:17][root][INFO] - Training Epoch: 1/2, step 687/7134 completed (loss: 0.07350751757621765, acc: 0.9748549461364746)
[2025-02-13 02:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:17][root][INFO] - Training Epoch: 1/2, step 688/7134 completed (loss: 0.02663651667535305, acc: 0.9926035404205322)
[2025-02-13 02:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:18][root][INFO] - Training Epoch: 1/2, step 689/7134 completed (loss: 0.03256799280643463, acc: 0.9863247871398926)
[2025-02-13 02:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:18][root][INFO] - Training Epoch: 1/2, step 690/7134 completed (loss: 0.061566971242427826, acc: 0.9809221029281616)
[2025-02-13 02:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:19][root][INFO] - Training Epoch: 1/2, step 691/7134 completed (loss: 0.06743337959051132, acc: 0.982758641242981)
[2025-02-13 02:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:19][root][INFO] - Training Epoch: 1/2, step 692/7134 completed (loss: 0.12911364436149597, acc: 0.9653333425521851)
[2025-02-13 02:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20][root][INFO] - Training Epoch: 1/2, step 693/7134 completed (loss: 0.0835842490196228, acc: 0.9693053364753723)
[2025-02-13 02:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20][root][INFO] - Training Epoch: 1/2, step 694/7134 completed (loss: 0.08417483419179916, acc: 0.9810218811035156)
[2025-02-13 02:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:21][root][INFO] - Training Epoch: 1/2, step 695/7134 completed (loss: 0.12785787880420685, acc: 0.9724612832069397)
[2025-02-13 02:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:21][root][INFO] - Training Epoch: 1/2, step 696/7134 completed (loss: 0.11356668919324875, acc: 0.9676375389099121)
[2025-02-13 02:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:22][root][INFO] - Training Epoch: 1/2, step 697/7134 completed (loss: 0.06028027832508087, acc: 0.9838709831237793)
[2025-02-13 02:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:22][root][INFO] - Training Epoch: 1/2, step 698/7134 completed (loss: 0.04211588576436043, acc: 0.9853420257568359)
[2025-02-13 02:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23][root][INFO] - Training Epoch: 1/2, step 699/7134 completed (loss: 0.05074604973196983, acc: 0.9890965819358826)
[2025-02-13 02:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23][root][INFO] - Training Epoch: 1/2, step 700/7134 completed (loss: 0.09218897670507431, acc: 0.9850746393203735)
[2025-02-13 02:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23][root][INFO] - Training Epoch: 1/2, step 701/7134 completed (loss: 0.049850329756736755, acc: 0.9858155846595764)
[2025-02-13 02:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24][root][INFO] - Training Epoch: 1/2, step 702/7134 completed (loss: 0.06747440248727798, acc: 0.9740259647369385)
[2025-02-13 02:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24][root][INFO] - Training Epoch: 1/2, step 703/7134 completed (loss: 0.05507621541619301, acc: 0.987075924873352)
[2025-02-13 02:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:25][root][INFO] - Training Epoch: 1/2, step 704/7134 completed (loss: 0.0930250734090805, acc: 0.9741641283035278)
[2025-02-13 02:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:25][root][INFO] - Training Epoch: 1/2, step 705/7134 completed (loss: 0.11045565456151962, acc: 0.9675977826118469)
[2025-02-13 02:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:26][root][INFO] - Training Epoch: 1/2, step 706/7134 completed (loss: 0.0444728285074234, acc: 0.9879649877548218)
[2025-02-13 02:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:26][root][INFO] - Training Epoch: 1/2, step 707/7134 completed (loss: 0.05556840822100639, acc: 0.9844124913215637)
[2025-02-13 02:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:27][root][INFO] - Training Epoch: 1/2, step 708/7134 completed (loss: 0.05145351588726044, acc: 0.9851301312446594)
[2025-02-13 02:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:27][root][INFO] - Training Epoch: 1/2, step 709/7134 completed (loss: 0.0684765949845314, acc: 0.9832935333251953)
[2025-02-13 02:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:28][root][INFO] - Training Epoch: 1/2, step 710/7134 completed (loss: 0.05705207958817482, acc: 0.981840193271637)
[2025-02-13 02:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:28][root][INFO] - Training Epoch: 1/2, step 711/7134 completed (loss: 0.040237270295619965, acc: 0.9822904467582703)
[2025-02-13 02:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29][root][INFO] - Training Epoch: 1/2, step 712/7134 completed (loss: 0.04610053077340126, acc: 0.9877899885177612)
[2025-02-13 02:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29][root][INFO] - Training Epoch: 1/2, step 713/7134 completed (loss: 0.03998478502035141, acc: 0.9861591458320618)
[2025-02-13 02:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:30][root][INFO] - Training Epoch: 1/2, step 714/7134 completed (loss: 0.056765031069517136, acc: 0.9806763529777527)
[2025-02-13 02:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:30][root][INFO] - Training Epoch: 1/2, step 715/7134 completed (loss: 0.0340096540749073, acc: 0.9872159361839294)
[2025-02-13 02:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:31][root][INFO] - Training Epoch: 1/2, step 716/7134 completed (loss: 0.04585965722799301, acc: 0.9863481521606445)
[2025-02-13 02:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:31][root][INFO] - Training Epoch: 1/2, step 717/7134 completed (loss: 0.06780125945806503, acc: 0.983565092086792)
[2025-02-13 02:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:31][root][INFO] - Training Epoch: 1/2, step 718/7134 completed (loss: 0.032655540853738785, acc: 0.9874686598777771)
[2025-02-13 02:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32][root][INFO] - Training Epoch: 1/2, step 719/7134 completed (loss: 0.025428010150790215, acc: 0.9948275685310364)
[2025-02-13 02:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32][root][INFO] - Training Epoch: 1/2, step 720/7134 completed (loss: 0.0360378623008728, acc: 0.9874686598777771)
[2025-02-13 02:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:33][root][INFO] - Training Epoch: 1/2, step 721/7134 completed (loss: 0.0750134065747261, acc: 0.9782330393791199)
[2025-02-13 02:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:33][root][INFO] - Training Epoch: 1/2, step 722/7134 completed (loss: 0.061080608516931534, acc: 0.9824970960617065)
[2025-02-13 02:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:34][root][INFO] - Training Epoch: 1/2, step 723/7134 completed (loss: 0.02102157473564148, acc: 0.9963680505752563)
[2025-02-13 02:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:34][root][INFO] - Training Epoch: 1/2, step 724/7134 completed (loss: 0.06796333938837051, acc: 0.9842632412910461)
[2025-02-13 02:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35][root][INFO] - Training Epoch: 1/2, step 725/7134 completed (loss: 0.045039284974336624, acc: 0.9855282306671143)
[2025-02-13 02:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35][root][INFO] - Training Epoch: 1/2, step 726/7134 completed (loss: 0.0457669273018837, acc: 0.9860529899597168)
[2025-02-13 02:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:36][root][INFO] - Training Epoch: 1/2, step 727/7134 completed (loss: 0.03793425112962723, acc: 0.98124098777771)
[2025-02-13 02:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:36][root][INFO] - Training Epoch: 1/2, step 728/7134 completed (loss: 0.035283662378787994, acc: 0.9931129217147827)
[2025-02-13 02:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:37][root][INFO] - Training Epoch: 1/2, step 729/7134 completed (loss: 0.030475705862045288, acc: 0.9888734221458435)
[2025-02-13 02:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:37][root][INFO] - Training Epoch: 1/2, step 730/7134 completed (loss: 0.06017972528934479, acc: 0.9857327938079834)
[2025-02-13 02:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38][root][INFO] - Training Epoch: 1/2, step 731/7134 completed (loss: 0.03940644487738609, acc: 0.9906666874885559)
[2025-02-13 02:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38][root][INFO] - Training Epoch: 1/2, step 732/7134 completed (loss: 0.10614245384931564, acc: 0.9754716753959656)
[2025-02-13 02:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38][root][INFO] - Training Epoch: 1/2, step 733/7134 completed (loss: 0.06941559165716171, acc: 0.9786856174468994)
[2025-02-13 02:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:39][root][INFO] - Training Epoch: 1/2, step 734/7134 completed (loss: 0.03892471268773079, acc: 0.9938555955886841)
[2025-02-13 02:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:39][root][INFO] - Training Epoch: 1/2, step 735/7134 completed (loss: 0.028155464679002762, acc: 0.9919614195823669)
[2025-02-13 02:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40][root][INFO] - Training Epoch: 1/2, step 736/7134 completed (loss: 0.05954371392726898, acc: 0.9758241772651672)
[2025-02-13 02:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40][root][INFO] - Training Epoch: 1/2, step 737/7134 completed (loss: 0.1432432234287262, acc: 0.9634888172149658)
[2025-02-13 02:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:41][root][INFO] - Training Epoch: 1/2, step 738/7134 completed (loss: 0.02585555426776409, acc: 0.9903846383094788)
[2025-02-13 02:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:41][root][INFO] - Training Epoch: 1/2, step 739/7134 completed (loss: 0.04945119470357895, acc: 0.9779816269874573)
[2025-02-13 02:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:41][root][INFO] - Training Epoch: 1/2, step 740/7134 completed (loss: 0.01607208140194416, acc: 0.9952380657196045)
[2025-02-13 02:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:42][root][INFO] - Training Epoch: 1/2, step 741/7134 completed (loss: 0.055941421538591385, acc: 0.9873816967010498)
[2025-02-13 02:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:42][root][INFO] - Training Epoch: 1/2, step 742/7134 completed (loss: 0.04278639703989029, acc: 0.9853249192237854)
[2025-02-13 02:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43][root][INFO] - Training Epoch: 1/2, step 743/7134 completed (loss: 0.08575987070798874, acc: 0.9789156913757324)
[2025-02-13 02:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43][root][INFO] - Training Epoch: 1/2, step 744/7134 completed (loss: 0.055795714259147644, acc: 0.9821428656578064)
[2025-02-13 02:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:44][root][INFO] - Training Epoch: 1/2, step 745/7134 completed (loss: 0.044757310301065445, acc: 0.9924471378326416)
[2025-02-13 02:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:44][root][INFO] - Training Epoch: 1/2, step 746/7134 completed (loss: 0.05258771777153015, acc: 0.9868637323379517)
[2025-02-13 02:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:45][root][INFO] - Training Epoch: 1/2, step 747/7134 completed (loss: 0.11300632357597351, acc: 0.9684385657310486)
[2025-02-13 02:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:45][root][INFO] - Training Epoch: 1/2, step 748/7134 completed (loss: 0.038808487355709076, acc: 0.9879699349403381)
[2025-02-13 02:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:46][root][INFO] - Training Epoch: 1/2, step 749/7134 completed (loss: 0.03370604291558266, acc: 0.9913544654846191)
[2025-02-13 02:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:46][root][INFO] - Training Epoch: 1/2, step 750/7134 completed (loss: 0.05810193717479706, acc: 0.9855855703353882)
[2025-02-13 02:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47][root][INFO] - Training Epoch: 1/2, step 751/7134 completed (loss: 0.08604209125041962, acc: 0.9812606573104858)
[2025-02-13 02:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47][root][INFO] - Training Epoch: 1/2, step 752/7134 completed (loss: 0.06878538429737091, acc: 0.9819819927215576)
[2025-02-13 02:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47][root][INFO] - Training Epoch: 1/2, step 753/7134 completed (loss: 0.057274702936410904, acc: 0.9847972989082336)
[2025-02-13 02:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:48][root][INFO] - Training Epoch: 1/2, step 754/7134 completed (loss: 0.025222988799214363, acc: 0.9924585223197937)
[2025-02-13 02:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:48][root][INFO] - Training Epoch: 1/2, step 755/7134 completed (loss: 0.08751392364501953, acc: 0.9752475023269653)
[2025-02-13 02:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:49][root][INFO] - Training Epoch: 1/2, step 756/7134 completed (loss: 0.09084026515483856, acc: 0.9757009148597717)
[2025-02-13 02:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:49][root][INFO] - Training Epoch: 1/2, step 757/7134 completed (loss: 0.05052396282553673, acc: 0.99042147397995)
[2025-02-13 02:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:50][root][INFO] - Training Epoch: 1/2, step 758/7134 completed (loss: 0.09213485568761826, acc: 0.979345977306366)
[2025-02-13 02:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:50][root][INFO] - Training Epoch: 1/2, step 759/7134 completed (loss: 0.06407993286848068, acc: 0.9786821603775024)
[2025-02-13 02:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:50][root][INFO] - Training Epoch: 1/2, step 760/7134 completed (loss: 0.021524859592318535, acc: 0.9930675625801086)
[2025-02-13 02:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51][root][INFO] - Training Epoch: 1/2, step 761/7134 completed (loss: 0.0334724485874176, acc: 0.9906103014945984)
[2025-02-13 02:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51][root][INFO] - Training Epoch: 1/2, step 762/7134 completed (loss: 0.0416877418756485, acc: 0.9910394549369812)
[2025-02-13 02:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:52][root][INFO] - Training Epoch: 1/2, step 763/7134 completed (loss: 0.06856086850166321, acc: 0.9820846915245056)
[2025-02-13 02:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:52][root][INFO] - Training Epoch: 1/2, step 764/7134 completed (loss: 0.039133720099925995, acc: 0.9885222315788269)
[2025-02-13 02:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53][root][INFO] - Training Epoch: 1/2, step 765/7134 completed (loss: 0.037145014852285385, acc: 0.9897511005401611)
[2025-02-13 02:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53][root][INFO] - Training Epoch: 1/2, step 766/7134 completed (loss: 0.025810951367020607, acc: 0.9910979270935059)
[2025-02-13 02:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:54][root][INFO] - Training Epoch: 1/2, step 767/7134 completed (loss: 0.04123774170875549, acc: 0.9897511005401611)
[2025-02-13 02:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:54][root][INFO] - Training Epoch: 1/2, step 768/7134 completed (loss: 0.05715972185134888, acc: 0.9844192862510681)
[2025-02-13 02:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:54][root][INFO] - Training Epoch: 1/2, step 769/7134 completed (loss: 0.04388304054737091, acc: 0.9871060252189636)
[2025-02-13 02:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:55][root][INFO] - Training Epoch: 1/2, step 770/7134 completed (loss: 0.04033162072300911, acc: 0.9846860766410828)
[2025-02-13 02:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:55][root][INFO] - Training Epoch: 1/2, step 771/7134 completed (loss: 0.05349003151059151, acc: 0.9841583967208862)
[2025-02-13 02:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:56][root][INFO] - Training Epoch: 1/2, step 772/7134 completed (loss: 0.057377904653549194, acc: 0.9865996837615967)
[2025-02-13 02:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:56][root][INFO] - Training Epoch: 1/2, step 773/7134 completed (loss: 0.059249147772789, acc: 0.9845758080482483)
[2025-02-13 02:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57][root][INFO] - Training Epoch: 1/2, step 774/7134 completed (loss: 0.03825892508029938, acc: 0.9900332093238831)
[2025-02-13 02:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57][root][INFO] - Training Epoch: 1/2, step 775/7134 completed (loss: 0.029036518186330795, acc: 0.9946737885475159)
[2025-02-13 02:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:58][root][INFO] - Training Epoch: 1/2, step 776/7134 completed (loss: 0.023082049563527107, acc: 0.9940828680992126)
[2025-02-13 02:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:58][root][INFO] - Training Epoch: 1/2, step 777/7134 completed (loss: 0.04582764208316803, acc: 0.9903692007064819)
[2025-02-13 02:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:58][root][INFO] - Training Epoch: 1/2, step 778/7134 completed (loss: 0.07201837003231049, acc: 0.9871794581413269)
[2025-02-13 02:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59][root][INFO] - Training Epoch: 1/2, step 779/7134 completed (loss: 0.049349695444107056, acc: 0.9839416146278381)
[2025-02-13 02:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59][root][INFO] - Training Epoch: 1/2, step 780/7134 completed (loss: 0.06144268438220024, acc: 0.979721188545227)
[2025-02-13 02:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:00][root][INFO] - Training Epoch: 1/2, step 781/7134 completed (loss: 0.060406193137168884, acc: 0.980463981628418)
[2025-02-13 02:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:00][root][INFO] - Training Epoch: 1/2, step 782/7134 completed (loss: 0.05877698212862015, acc: 0.9841772317886353)
[2025-02-13 02:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:01][root][INFO] - Training Epoch: 1/2, step 783/7134 completed (loss: 0.08185143023729324, acc: 0.9757961630821228)
[2025-02-13 02:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:01][root][INFO] - Training Epoch: 1/2, step 784/7134 completed (loss: 0.04077718034386635, acc: 0.9914737939834595)
[2025-02-13 02:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:02][root][INFO] - Training Epoch: 1/2, step 785/7134 completed (loss: 0.029936280101537704, acc: 0.9899665713310242)
[2025-02-13 02:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:02][root][INFO] - Training Epoch: 1/2, step 786/7134 completed (loss: 0.04417717829346657, acc: 0.9925650358200073)
[2025-02-13 02:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03][root][INFO] - Training Epoch: 1/2, step 787/7134 completed (loss: 0.013044314458966255, acc: 0.9963503479957581)
[2025-02-13 02:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03][root][INFO] - Training Epoch: 1/2, step 788/7134 completed (loss: 0.07802275568246841, acc: 0.9823788404464722)
[2025-02-13 02:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:04][root][INFO] - Training Epoch: 1/2, step 789/7134 completed (loss: 0.0879765972495079, acc: 0.981679379940033)
[2025-02-13 02:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:04][root][INFO] - Training Epoch: 1/2, step 790/7134 completed (loss: 0.12625138461589813, acc: 0.9704142212867737)
[2025-02-13 02:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05][root][INFO] - Training Epoch: 1/2, step 791/7134 completed (loss: 0.08693084120750427, acc: 0.97826087474823)
[2025-02-13 02:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05][root][INFO] - Training Epoch: 1/2, step 792/7134 completed (loss: 0.06710515916347504, acc: 0.984544038772583)
[2025-02-13 02:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05][root][INFO] - Training Epoch: 1/2, step 793/7134 completed (loss: 0.027271589264273643, acc: 0.9881656765937805)
[2025-02-13 02:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:06][root][INFO] - Training Epoch: 1/2, step 794/7134 completed (loss: 0.04656853526830673, acc: 0.9883449673652649)
[2025-02-13 02:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:06][root][INFO] - Training Epoch: 1/2, step 795/7134 completed (loss: 0.10358838737010956, acc: 0.9750778675079346)
[2025-02-13 02:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:07][root][INFO] - Training Epoch: 1/2, step 796/7134 completed (loss: 0.117516428232193, acc: 0.9722222089767456)
[2025-02-13 02:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:07][root][INFO] - Training Epoch: 1/2, step 797/7134 completed (loss: 0.07418820261955261, acc: 0.9839357137680054)
[2025-02-13 02:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08][root][INFO] - Training Epoch: 1/2, step 798/7134 completed (loss: 0.03548089787364006, acc: 0.9894179701805115)
[2025-02-13 02:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08][root][INFO] - Training Epoch: 1/2, step 799/7134 completed (loss: 0.05398110672831535, acc: 0.9806451797485352)
[2025-02-13 02:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08][root][INFO] - Training Epoch: 1/2, step 800/7134 completed (loss: 0.08367812633514404, acc: 0.9813780188560486)
[2025-02-13 02:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09][root][INFO] - Training Epoch: 1/2, step 801/7134 completed (loss: 0.11514487117528915, acc: 0.974399983882904)
[2025-02-13 02:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09][root][INFO] - Training Epoch: 1/2, step 802/7134 completed (loss: 0.05872634798288345, acc: 0.988727867603302)
[2025-02-13 02:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:10][root][INFO] - Training Epoch: 1/2, step 803/7134 completed (loss: 0.048059750348329544, acc: 0.9892857074737549)
[2025-02-13 02:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:10][root][INFO] - Training Epoch: 1/2, step 804/7134 completed (loss: 0.05520261079072952, acc: 0.9857434034347534)
[2025-02-13 02:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:11][root][INFO] - Training Epoch: 1/2, step 805/7134 completed (loss: 0.06778673082590103, acc: 0.9775280952453613)
[2025-02-13 02:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:11][root][INFO] - Training Epoch: 1/2, step 806/7134 completed (loss: 0.03769799694418907, acc: 0.9916201233863831)
[2025-02-13 02:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12][root][INFO] - Training Epoch: 1/2, step 807/7134 completed (loss: 0.06996838003396988, acc: 0.9793187379837036)
[2025-02-13 02:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12][root][INFO] - Training Epoch: 1/2, step 808/7134 completed (loss: 0.04031111299991608, acc: 0.9916167855262756)
[2025-02-13 02:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:13][root][INFO] - Training Epoch: 1/2, step 809/7134 completed (loss: 0.025418076664209366, acc: 0.98959881067276)
[2025-02-13 02:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:13][root][INFO] - Training Epoch: 1/2, step 810/7134 completed (loss: 0.06818372011184692, acc: 0.9770580530166626)
[2025-02-13 02:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:13][root][INFO] - Training Epoch: 1/2, step 811/7134 completed (loss: 0.11981985718011856, acc: 0.9736379384994507)
[2025-02-13 02:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14][root][INFO] - Training Epoch: 1/2, step 812/7134 completed (loss: 0.04226148501038551, acc: 0.9861286282539368)
[2025-02-13 02:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14][root][INFO] - Training Epoch: 1/2, step 813/7134 completed (loss: 0.04142012074589729, acc: 0.9898403286933899)
[2025-02-13 02:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:15][root][INFO] - Training Epoch: 1/2, step 814/7134 completed (loss: 0.07942625135183334, acc: 0.9832258224487305)
[2025-02-13 02:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:15][root][INFO] - Training Epoch: 1/2, step 815/7134 completed (loss: 0.07574214041233063, acc: 0.9752704501152039)
[2025-02-13 02:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:16][root][INFO] - Training Epoch: 1/2, step 816/7134 completed (loss: 0.08197008818387985, acc: 0.9796609878540039)
[2025-02-13 02:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:16][root][INFO] - Training Epoch: 1/2, step 817/7134 completed (loss: 0.08914784342050552, acc: 0.9798902869224548)
[2025-02-13 02:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:17][root][INFO] - Training Epoch: 1/2, step 818/7134 completed (loss: 0.08670596033334732, acc: 0.969072163105011)
[2025-02-13 02:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:17][root][INFO] - Training Epoch: 1/2, step 819/7134 completed (loss: 0.07624999433755875, acc: 0.984000027179718)
[2025-02-13 02:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:18][root][INFO] - Training Epoch: 1/2, step 820/7134 completed (loss: 0.05336154252290726, acc: 0.9829984307289124)
[2025-02-13 02:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:18][root][INFO] - Training Epoch: 1/2, step 821/7134 completed (loss: 0.04177989065647125, acc: 0.9919571280479431)
[2025-02-13 02:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19][root][INFO] - Training Epoch: 1/2, step 822/7134 completed (loss: 0.04558124765753746, acc: 0.9874411225318909)
[2025-02-13 02:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19][root][INFO] - Training Epoch: 1/2, step 823/7134 completed (loss: 0.02459658868610859, acc: 0.9887429475784302)
[2025-02-13 02:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19][root][INFO] - Training Epoch: 1/2, step 824/7134 completed (loss: 0.06759805977344513, acc: 0.971731424331665)
[2025-02-13 02:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:20][root][INFO] - Training Epoch: 1/2, step 825/7134 completed (loss: 0.08373474329710007, acc: 0.9802371263504028)
[2025-02-13 02:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:20][root][INFO] - Training Epoch: 1/2, step 826/7134 completed (loss: 0.05813054367899895, acc: 0.9822888374328613)
[2025-02-13 02:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21][root][INFO] - Training Epoch: 1/2, step 827/7134 completed (loss: 0.03674701973795891, acc: 0.9895012974739075)
[2025-02-13 02:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21][root][INFO] - Training Epoch: 1/2, step 828/7134 completed (loss: 0.06754621863365173, acc: 0.9860031008720398)
[2025-02-13 02:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:22][root][INFO] - Training Epoch: 1/2, step 829/7134 completed (loss: 0.057254042476415634, acc: 0.9830096960067749)
[2025-02-13 02:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:22][root][INFO] - Training Epoch: 1/2, step 830/7134 completed (loss: 0.044026557356119156, acc: 0.9883720874786377)
[2025-02-13 02:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:23][root][INFO] - Training Epoch: 1/2, step 831/7134 completed (loss: 0.04221189394593239, acc: 0.9890410900115967)
[2025-02-13 02:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:23][root][INFO] - Training Epoch: 1/2, step 832/7134 completed (loss: 0.12721428275108337, acc: 0.9624573588371277)
[2025-02-13 02:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24][root][INFO] - Training Epoch: 1/2, step 833/7134 completed (loss: 0.03085898794233799, acc: 0.9887955188751221)
[2025-02-13 02:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24][root][INFO] - Training Epoch: 1/2, step 834/7134 completed (loss: 0.03174842521548271, acc: 0.9930070042610168)
[2025-02-13 02:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24][root][INFO] - Training Epoch: 1/2, step 835/7134 completed (loss: 0.08047259598970413, acc: 0.9790209531784058)
[2025-02-13 02:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:25][root][INFO] - Training Epoch: 1/2, step 836/7134 completed (loss: 0.05348468944430351, acc: 0.982598602771759)
[2025-02-13 02:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:25][root][INFO] - Training Epoch: 1/2, step 837/7134 completed (loss: 0.04813304916024208, acc: 0.9891008138656616)
[2025-02-13 02:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:26][root][INFO] - Training Epoch: 1/2, step 838/7134 completed (loss: 0.03073517605662346, acc: 0.9920508861541748)
[2025-02-13 02:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:26][root][INFO] - Training Epoch: 1/2, step 839/7134 completed (loss: 0.039909884333610535, acc: 0.9856459498405457)
[2025-02-13 02:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:27][root][INFO] - Training Epoch: 1/2, step 840/7134 completed (loss: 0.04208732768893242, acc: 0.9874476790428162)
[2025-02-13 02:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:27][root][INFO] - Training Epoch: 1/2, step 841/7134 completed (loss: 0.08725782483816147, acc: 0.9806362390518188)
[2025-02-13 02:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28][root][INFO] - Training Epoch: 1/2, step 842/7134 completed (loss: 0.051204193383455276, acc: 0.9878706336021423)
[2025-02-13 02:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28][root][INFO] - Training Epoch: 1/2, step 843/7134 completed (loss: 0.041042082011699677, acc: 0.9888734221458435)
[2025-02-13 02:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:29][root][INFO] - Training Epoch: 1/2, step 844/7134 completed (loss: 0.035995207726955414, acc: 0.9873617887496948)
[2025-02-13 02:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:29][root][INFO] - Training Epoch: 1/2, step 845/7134 completed (loss: 0.08688411116600037, acc: 0.9787610769271851)
[2025-02-13 02:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30][root][INFO] - Training Epoch: 1/2, step 846/7134 completed (loss: 0.041731733828783035, acc: 0.9940652847290039)
[2025-02-13 02:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30][root][INFO] - Training Epoch: 1/2, step 847/7134 completed (loss: 0.029347162693738937, acc: 0.9923175573348999)
[2025-02-13 02:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30][root][INFO] - Training Epoch: 1/2, step 848/7134 completed (loss: 0.06090082600712776, acc: 0.9851668477058411)
[2025-02-13 02:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:31][root][INFO] - Training Epoch: 1/2, step 849/7134 completed (loss: 0.038212310522794724, acc: 0.9866989254951477)
[2025-02-13 02:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:31][root][INFO] - Training Epoch: 1/2, step 850/7134 completed (loss: 0.02895362675189972, acc: 0.991946280002594)
[2025-02-13 02:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:32][root][INFO] - Training Epoch: 1/2, step 851/7134 completed (loss: 0.04999161884188652, acc: 0.9868593811988831)
[2025-02-13 02:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:32][root][INFO] - Training Epoch: 1/2, step 852/7134 completed (loss: 0.021892741322517395, acc: 0.9938949942588806)
[2025-02-13 02:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33][root][INFO] - Training Epoch: 1/2, step 853/7134 completed (loss: 0.039428334683179855, acc: 0.984581470489502)
[2025-02-13 02:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33][root][INFO] - Training Epoch: 1/2, step 854/7134 completed (loss: 0.07147450745105743, acc: 0.9875621795654297)
[2025-02-13 02:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:34][root][INFO] - Training Epoch: 1/2, step 855/7134 completed (loss: 0.0540911965072155, acc: 0.9812646508216858)
[2025-02-13 02:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:34][root][INFO] - Training Epoch: 1/2, step 856/7134 completed (loss: 0.030662408098578453, acc: 0.991525411605835)
[2025-02-13 02:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:35][root][INFO] - Training Epoch: 1/2, step 857/7134 completed (loss: 0.05562128126621246, acc: 0.9823788404464722)
[2025-02-13 02:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:35][root][INFO] - Training Epoch: 1/2, step 858/7134 completed (loss: 0.05171378701925278, acc: 0.9850746393203735)
[2025-02-13 02:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:36][root][INFO] - Training Epoch: 1/2, step 859/7134 completed (loss: 0.070149727165699, acc: 0.9798578023910522)
[2025-02-13 02:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:36][root][INFO] - Training Epoch: 1/2, step 860/7134 completed (loss: 0.07494376599788666, acc: 0.9772440195083618)
[2025-02-13 02:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37][root][INFO] - Training Epoch: 1/2, step 861/7134 completed (loss: 0.08431252092123032, acc: 0.9804983735084534)
[2025-02-13 02:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37][root][INFO] - Training Epoch: 1/2, step 862/7134 completed (loss: 0.046745043247938156, acc: 0.9831528067588806)
[2025-02-13 02:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38][root][INFO] - Training Epoch: 1/2, step 863/7134 completed (loss: 0.08484026789665222, acc: 0.9767171144485474)
[2025-02-13 02:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38][root][INFO] - Training Epoch: 1/2, step 864/7134 completed (loss: 0.09173289686441422, acc: 0.9734513163566589)
[2025-02-13 02:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38][root][INFO] - Training Epoch: 1/2, step 865/7134 completed (loss: 0.1074180081486702, acc: 0.9683698415756226)
[2025-02-13 02:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:39][root][INFO] - Training Epoch: 1/2, step 866/7134 completed (loss: 0.09440826624631882, acc: 0.9733333587646484)
[2025-02-13 02:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:39][root][INFO] - Training Epoch: 1/2, step 867/7134 completed (loss: 0.08655345439910889, acc: 0.9749687314033508)
[2025-02-13 02:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40][root][INFO] - Training Epoch: 1/2, step 868/7134 completed (loss: 0.0945068970322609, acc: 0.9738219976425171)
[2025-02-13 02:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40][root][INFO] - Training Epoch: 1/2, step 869/7134 completed (loss: 0.09492458403110504, acc: 0.9681528806686401)
[2025-02-13 02:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:41][root][INFO] - Training Epoch: 1/2, step 870/7134 completed (loss: 0.05168827995657921, acc: 0.9824798107147217)
[2025-02-13 02:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:41][root][INFO] - Training Epoch: 1/2, step 871/7134 completed (loss: 0.10034497827291489, acc: 0.9804560542106628)
[2025-02-13 02:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:42][root][INFO] - Training Epoch: 1/2, step 872/7134 completed (loss: 0.0601533018052578, acc: 0.9858712553977966)
[2025-02-13 02:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:42][root][INFO] - Training Epoch: 1/2, step 873/7134 completed (loss: 0.09203460067510605, acc: 0.9802817106246948)
[2025-02-13 02:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43][root][INFO] - Training Epoch: 1/2, step 874/7134 completed (loss: 0.10089705884456635, acc: 0.9661017060279846)
[2025-02-13 02:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43][root][INFO] - Training Epoch: 1/2, step 875/7134 completed (loss: 0.0754309669137001, acc: 0.9736456871032715)
[2025-02-13 02:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43][root][INFO] - Training Epoch: 1/2, step 876/7134 completed (loss: 0.06621500104665756, acc: 0.9795321822166443)
[2025-02-13 02:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:44][root][INFO] - Training Epoch: 1/2, step 877/7134 completed (loss: 0.11574000865221024, acc: 0.9708608984947205)
[2025-02-13 02:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:44][root][INFO] - Training Epoch: 1/2, step 878/7134 completed (loss: 0.08253052830696106, acc: 0.9694960117340088)
[2025-02-13 02:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45][root][INFO] - Training Epoch: 1/2, step 879/7134 completed (loss: 0.09802460670471191, acc: 0.9752547144889832)
[2025-02-13 02:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45][root][INFO] - Training Epoch: 1/2, step 880/7134 completed (loss: 0.03234795108437538, acc: 0.9868995547294617)
[2025-02-13 02:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:46][root][INFO] - Training Epoch: 1/2, step 881/7134 completed (loss: 0.028022244572639465, acc: 0.9899799823760986)
[2025-02-13 02:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:46][root][INFO] - Training Epoch: 1/2, step 882/7134 completed (loss: 0.05172888562083244, acc: 0.9874100685119629)
[2025-02-13 02:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:47][root][INFO] - Training Epoch: 1/2, step 883/7134 completed (loss: 0.09869320690631866, acc: 0.966360867023468)
[2025-02-13 02:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:47][root][INFO] - Training Epoch: 1/2, step 884/7134 completed (loss: 0.06854454427957535, acc: 0.9758713245391846)
[2025-02-13 02:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:48][root][INFO] - Training Epoch: 1/2, step 885/7134 completed (loss: 0.022569075226783752, acc: 0.9901685118675232)
[2025-02-13 02:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:48][root][INFO] - Training Epoch: 1/2, step 886/7134 completed (loss: 0.08957700431346893, acc: 0.9824561476707458)
[2025-02-13 02:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49][root][INFO] - Training Epoch: 1/2, step 887/7134 completed (loss: 0.05113425478339195, acc: 0.9893805384635925)
[2025-02-13 02:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49][root][INFO] - Training Epoch: 1/2, step 888/7134 completed (loss: 0.07018361985683441, acc: 0.9817906022071838)
[2025-02-13 02:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49][root][INFO] - Training Epoch: 1/2, step 889/7134 completed (loss: 0.028296388685703278, acc: 0.9911110997200012)
[2025-02-13 02:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:50][root][INFO] - Training Epoch: 1/2, step 890/7134 completed (loss: 0.06779070943593979, acc: 0.9847198724746704)
[2025-02-13 02:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:50][root][INFO] - Training Epoch: 1/2, step 891/7134 completed (loss: 0.07511162012815475, acc: 0.9874826073646545)
[2025-02-13 02:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:51][root][INFO] - Training Epoch: 1/2, step 892/7134 completed (loss: 0.06385431438684464, acc: 0.9883720874786377)
[2025-02-13 02:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:51][root][INFO] - Training Epoch: 1/2, step 893/7134 completed (loss: 0.06854087114334106, acc: 0.9876161217689514)
[2025-02-13 02:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52][root][INFO] - Training Epoch: 1/2, step 894/7134 completed (loss: 0.07386554777622223, acc: 0.9860334992408752)
[2025-02-13 02:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52][root][INFO] - Training Epoch: 1/2, step 895/7134 completed (loss: 0.03157590329647064, acc: 0.9931623935699463)
[2025-02-13 02:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:53][root][INFO] - Training Epoch: 1/2, step 896/7134 completed (loss: 0.04996452108025551, acc: 0.9860000014305115)
[2025-02-13 02:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:53][root][INFO] - Training Epoch: 1/2, step 897/7134 completed (loss: 0.04436920955777168, acc: 0.984679639339447)
[2025-02-13 02:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:54][root][INFO] - Training Epoch: 1/2, step 898/7134 completed (loss: 0.03949036821722984, acc: 0.9912827014923096)
[2025-02-13 02:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:54][root][INFO] - Training Epoch: 1/2, step 899/7134 completed (loss: 0.05469142273068428, acc: 0.9863184094429016)
[2025-02-13 02:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:55][root][INFO] - Training Epoch: 1/2, step 900/7134 completed (loss: 0.04665908217430115, acc: 0.9887482523918152)
[2025-02-13 02:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:55][root][INFO] - Training Epoch: 1/2, step 901/7134 completed (loss: 0.04474207013845444, acc: 0.9893333315849304)
[2025-02-13 02:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56][root][INFO] - Training Epoch: 1/2, step 902/7134 completed (loss: 0.018795525655150414, acc: 0.9956958293914795)
[2025-02-13 02:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56][root][INFO] - Training Epoch: 1/2, step 903/7134 completed (loss: 0.013709754683077335, acc: 0.997357964515686)
[2025-02-13 02:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:57][root][INFO] - Training Epoch: 1/2, step 904/7134 completed (loss: 0.029735570773482323, acc: 0.98828125)
[2025-02-13 02:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:57][root][INFO] - Training Epoch: 1/2, step 905/7134 completed (loss: 0.03001381643116474, acc: 0.9892183542251587)
[2025-02-13 02:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58][root][INFO] - Training Epoch: 1/2, step 906/7134 completed (loss: 0.04887787997722626, acc: 0.9839141964912415)
[2025-02-13 02:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58][root][INFO] - Training Epoch: 1/2, step 907/7134 completed (loss: 0.04294179007411003, acc: 0.9868420958518982)
[2025-02-13 02:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59][root][INFO] - Training Epoch: 1/2, step 908/7134 completed (loss: 0.03000645898282528, acc: 0.9921976327896118)
[2025-02-13 02:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59][root][INFO] - Training Epoch: 1/2, step 909/7134 completed (loss: 0.06014498695731163, acc: 0.9789789915084839)
[2025-02-13 02:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59][root][INFO] - Training Epoch: 1/2, step 910/7134 completed (loss: 0.05730769410729408, acc: 0.9826435446739197)
[2025-02-13 02:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:00][root][INFO] - Training Epoch: 1/2, step 911/7134 completed (loss: 0.03617788478732109, acc: 0.9902098178863525)
[2025-02-13 02:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:00][root][INFO] - Training Epoch: 1/2, step 912/7134 completed (loss: 0.059411730617284775, acc: 0.9848675727844238)
[2025-02-13 02:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01][root][INFO] - Training Epoch: 1/2, step 913/7134 completed (loss: 0.033463940024375916, acc: 0.9886934757232666)
[2025-02-13 02:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01][root][INFO] - Training Epoch: 1/2, step 914/7134 completed (loss: 0.02578776702284813, acc: 0.9936224222183228)
[2025-02-13 02:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:02][root][INFO] - Training Epoch: 1/2, step 915/7134 completed (loss: 0.091438427567482, acc: 0.9757281541824341)
[2025-02-13 02:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:02][root][INFO] - Training Epoch: 1/2, step 916/7134 completed (loss: 0.042932700365781784, acc: 0.9860383868217468)
[2025-02-13 02:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:03][root][INFO] - Training Epoch: 1/2, step 917/7134 completed (loss: 0.0712943822145462, acc: 0.9819004535675049)
[2025-02-13 02:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:03][root][INFO] - Training Epoch: 1/2, step 918/7134 completed (loss: 0.03703203424811363, acc: 0.9862174391746521)
[2025-02-13 02:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:04][root][INFO] - Training Epoch: 1/2, step 919/7134 completed (loss: 0.027507442981004715, acc: 0.9927849769592285)
[2025-02-13 02:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:04][root][INFO] - Training Epoch: 1/2, step 920/7134 completed (loss: 0.04397435113787651, acc: 0.9831932783126831)
[2025-02-13 02:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05][root][INFO] - Training Epoch: 1/2, step 921/7134 completed (loss: 0.04216575622558594, acc: 0.9932249188423157)
[2025-02-13 02:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05][root][INFO] - Training Epoch: 1/2, step 922/7134 completed (loss: 0.04785022884607315, acc: 0.9819672107696533)
[2025-02-13 02:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05][root][INFO] - Training Epoch: 1/2, step 923/7134 completed (loss: 0.05754046142101288, acc: 0.9814814925193787)
[2025-02-13 02:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:06][root][INFO] - Training Epoch: 1/2, step 924/7134 completed (loss: 0.024388380348682404, acc: 0.990338146686554)
[2025-02-13 02:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:07][root][INFO] - Training Epoch: 1/2, step 925/7134 completed (loss: 0.06216929852962494, acc: 0.9838472604751587)
[2025-02-13 02:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:07][root][INFO] - Training Epoch: 1/2, step 926/7134 completed (loss: 0.07165678590536118, acc: 0.9837703108787537)
[2025-02-13 02:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:07][root][INFO] - Training Epoch: 1/2, step 927/7134 completed (loss: 0.06013905629515648, acc: 0.983660101890564)
[2025-02-13 02:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08][root][INFO] - Training Epoch: 1/2, step 928/7134 completed (loss: 0.051372140645980835, acc: 0.990138053894043)
[2025-02-13 02:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08][root][INFO] - Training Epoch: 1/2, step 929/7134 completed (loss: 0.02781974896788597, acc: 0.9968404173851013)
[2025-02-13 02:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:09][root][INFO] - Training Epoch: 1/2, step 930/7134 completed (loss: 0.04760831221938133, acc: 0.9879032373428345)
[2025-02-13 02:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:09][root][INFO] - Training Epoch: 1/2, step 931/7134 completed (loss: 0.06746528297662735, acc: 0.9805996417999268)
[2025-02-13 02:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:10][root][INFO] - Training Epoch: 1/2, step 932/7134 completed (loss: 0.0353175550699234, acc: 0.9885386824607849)
[2025-02-13 02:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:10][root][INFO] - Training Epoch: 1/2, step 933/7134 completed (loss: 0.05446081981062889, acc: 0.9816642999649048)
[2025-02-13 02:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:11][root][INFO] - Training Epoch: 1/2, step 934/7134 completed (loss: 0.04595237225294113, acc: 0.9842932224273682)
[2025-02-13 02:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:11][root][INFO] - Training Epoch: 1/2, step 935/7134 completed (loss: 0.03887183591723442, acc: 0.9849749803543091)
[2025-02-13 02:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12][root][INFO] - Training Epoch: 1/2, step 936/7134 completed (loss: 0.05221113562583923, acc: 0.9876977205276489)
[2025-02-13 02:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12][root][INFO] - Training Epoch: 1/2, step 937/7134 completed (loss: 0.03569318354129791, acc: 0.9947183132171631)
[2025-02-13 02:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:13][root][INFO] - Training Epoch: 1/2, step 938/7134 completed (loss: 0.054753851145505905, acc: 0.9856733679771423)
[2025-02-13 02:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:13][root][INFO] - Training Epoch: 1/2, step 939/7134 completed (loss: 0.07487206906080246, acc: 0.9781931638717651)
[2025-02-13 02:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:14][root][INFO] - Training Epoch: 1/2, step 940/7134 completed (loss: 0.050726596266031265, acc: 0.9856938719749451)
[2025-02-13 02:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:14][root][INFO] - Training Epoch: 1/2, step 941/7134 completed (loss: 0.033107511699199677, acc: 0.9848275780677795)
[2025-02-13 02:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:15][root][INFO] - Training Epoch: 1/2, step 942/7134 completed (loss: 0.05130131170153618, acc: 0.9869375824928284)
[2025-02-13 02:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:15][root][INFO] - Training Epoch: 1/2, step 943/7134 completed (loss: 0.04147939383983612, acc: 0.9878234267234802)
[2025-02-13 02:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:15][root][INFO] - Training Epoch: 1/2, step 944/7134 completed (loss: 0.046993087977170944, acc: 0.9865067601203918)
[2025-02-13 02:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16][root][INFO] - Training Epoch: 1/2, step 945/7134 completed (loss: 0.058436062186956406, acc: 0.988811194896698)
[2025-02-13 02:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16][root][INFO] - Training Epoch: 1/2, step 946/7134 completed (loss: 0.019235635176301003, acc: 0.9957173466682434)
[2025-02-13 02:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:17][root][INFO] - Training Epoch: 1/2, step 947/7134 completed (loss: 0.07275845110416412, acc: 0.9845094680786133)
[2025-02-13 02:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:17][root][INFO] - Training Epoch: 1/2, step 948/7134 completed (loss: 0.05459621921181679, acc: 0.9874826073646545)
[2025-02-13 02:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18][root][INFO] - Training Epoch: 1/2, step 949/7134 completed (loss: 0.04325706511735916, acc: 0.9873772859573364)
[2025-02-13 02:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18][root][INFO] - Training Epoch: 1/2, step 950/7134 completed (loss: 0.04421181231737137, acc: 0.9876543283462524)
[2025-02-13 02:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:19][root][INFO] - Training Epoch: 1/2, step 951/7134 completed (loss: 0.041385468095541, acc: 0.98591548204422)
[2025-02-13 02:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:19][root][INFO] - Training Epoch: 1/2, step 952/7134 completed (loss: 0.05472389981150627, acc: 0.9833610653877258)
[2025-02-13 02:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:20][root][INFO] - Training Epoch: 1/2, step 953/7134 completed (loss: 0.09180164337158203, acc: 0.9742331504821777)
[2025-02-13 02:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:20][root][INFO] - Training Epoch: 1/2, step 954/7134 completed (loss: 0.05695844441652298, acc: 0.9838472604751587)
[2025-02-13 02:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21][root][INFO] - Training Epoch: 1/2, step 955/7134 completed (loss: 0.03654291853308678, acc: 0.9892904758453369)
[2025-02-13 02:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21][root][INFO] - Training Epoch: 1/2, step 956/7134 completed (loss: 0.03770589083433151, acc: 0.9853658676147461)
[2025-02-13 02:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:22][root][INFO] - Training Epoch: 1/2, step 957/7134 completed (loss: 0.023747539147734642, acc: 0.9943116903305054)
[2025-02-13 02:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:22][root][INFO] - Training Epoch: 1/2, step 958/7134 completed (loss: 0.07142691314220428, acc: 0.9816176295280457)
[2025-02-13 02:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23][root][INFO] - Training Epoch: 1/2, step 959/7134 completed (loss: 0.12668490409851074, acc: 0.9625668525695801)
[2025-02-13 02:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23][root][INFO] - Training Epoch: 1/2, step 960/7134 completed (loss: 0.11361809819936752, acc: 0.967863917350769)
[2025-02-13 02:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:24][root][INFO] - Training Epoch: 1/2, step 961/7134 completed (loss: 0.11184827983379364, acc: 0.9743935465812683)
[2025-02-13 02:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:24][root][INFO] - Training Epoch: 1/2, step 962/7134 completed (loss: 0.1756969690322876, acc: 0.9586206674575806)
[2025-02-13 02:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:24][root][INFO] - Training Epoch: 1/2, step 963/7134 completed (loss: 0.08643675595521927, acc: 0.9725610017776489)
[2025-02-13 02:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:25][root][INFO] - Training Epoch: 1/2, step 964/7134 completed (loss: 0.16466863453388214, acc: 0.952023983001709)
[2025-02-13 02:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:25][root][INFO] - Training Epoch: 1/2, step 965/7134 completed (loss: 0.13694384694099426, acc: 0.9612625241279602)
[2025-02-13 02:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26][root][INFO] - Training Epoch: 1/2, step 966/7134 completed (loss: 0.06718133389949799, acc: 0.9786885380744934)
[2025-02-13 02:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26][root][INFO] - Training Epoch: 1/2, step 967/7134 completed (loss: 0.07520506531000137, acc: 0.9699863791465759)
[2025-02-13 02:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:27][root][INFO] - Training Epoch: 1/2, step 968/7134 completed (loss: 0.1270679235458374, acc: 0.9662027955055237)
[2025-02-13 02:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:27][root][INFO] - Training Epoch: 1/2, step 969/7134 completed (loss: 0.056994177401065826, acc: 0.9834087491035461)
[2025-02-13 02:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:28][root][INFO] - Training Epoch: 1/2, step 970/7134 completed (loss: 0.09193882346153259, acc: 0.9713740348815918)
[2025-02-13 02:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:28][root][INFO] - Training Epoch: 1/2, step 971/7134 completed (loss: 0.06497851759195328, acc: 0.982822060585022)
[2025-02-13 02:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:29][root][INFO] - Training Epoch: 1/2, step 972/7134 completed (loss: 0.08096425235271454, acc: 0.9775840640068054)
[2025-02-13 02:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:29][root][INFO] - Training Epoch: 1/2, step 973/7134 completed (loss: 0.14899364113807678, acc: 0.964102566242218)
[2025-02-13 02:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30][root][INFO] - Training Epoch: 1/2, step 974/7134 completed (loss: 0.07527565956115723, acc: 0.9806157350540161)
[2025-02-13 02:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30][root][INFO] - Training Epoch: 1/2, step 975/7134 completed (loss: 0.09056555479764938, acc: 0.9772727489471436)
[2025-02-13 02:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:31][root][INFO] - Training Epoch: 1/2, step 976/7134 completed (loss: 0.07574304193258286, acc: 0.9735202789306641)
[2025-02-13 02:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:31][root][INFO] - Training Epoch: 1/2, step 977/7134 completed (loss: 0.09822794795036316, acc: 0.9659090638160706)
[2025-02-13 02:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:32][root][INFO] - Training Epoch: 1/2, step 978/7134 completed (loss: 0.05405101180076599, acc: 0.9831528067588806)
[2025-02-13 02:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:32][root][INFO] - Training Epoch: 1/2, step 979/7134 completed (loss: 0.07512571662664413, acc: 0.9786096215248108)
[2025-02-13 02:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33][root][INFO] - Training Epoch: 1/2, step 980/7134 completed (loss: 0.07962026447057724, acc: 0.9775000214576721)
[2025-02-13 02:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33][root][INFO] - Training Epoch: 1/2, step 981/7134 completed (loss: 0.1640397012233734, acc: 0.9726688265800476)
[2025-02-13 02:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:34][root][INFO] - Training Epoch: 1/2, step 982/7134 completed (loss: 0.056959521025419235, acc: 0.9873595237731934)
[2025-02-13 02:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:34][root][INFO] - Training Epoch: 1/2, step 983/7134 completed (loss: 0.08118480443954468, acc: 0.9726027250289917)
[2025-02-13 02:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:34][root][INFO] - Training Epoch: 1/2, step 984/7134 completed (loss: 0.056845005601644516, acc: 0.9871323704719543)
[2025-02-13 02:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:35][root][INFO] - Training Epoch: 1/2, step 985/7134 completed (loss: 0.05160261690616608, acc: 0.983418345451355)
[2025-02-13 02:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:35][root][INFO] - Training Epoch: 1/2, step 986/7134 completed (loss: 0.07681447267532349, acc: 0.980169951915741)
[2025-02-13 02:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36][root][INFO] - Training Epoch: 1/2, step 987/7134 completed (loss: 0.06145363301038742, acc: 0.979099690914154)
[2025-02-13 02:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36][root][INFO] - Training Epoch: 1/2, step 988/7134 completed (loss: 0.1082124337553978, acc: 0.9667128920555115)
[2025-02-13 02:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:37][root][INFO] - Training Epoch: 1/2, step 989/7134 completed (loss: 0.08012073487043381, acc: 0.9749652147293091)
[2025-02-13 02:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:37][root][INFO] - Training Epoch: 1/2, step 990/7134 completed (loss: 0.07336194813251495, acc: 0.9810366630554199)
[2025-02-13 02:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:38][root][INFO] - Training Epoch: 1/2, step 991/7134 completed (loss: 0.09742089360952377, acc: 0.9733959436416626)
[2025-02-13 02:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:38][root][INFO] - Training Epoch: 1/2, step 992/7134 completed (loss: 0.06859052926301956, acc: 0.9789473414421082)
[2025-02-13 02:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39][root][INFO] - Training Epoch: 1/2, step 993/7134 completed (loss: 0.06487751007080078, acc: 0.9836333990097046)
[2025-02-13 02:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39][root][INFO] - Training Epoch: 1/2, step 994/7134 completed (loss: 0.06362616270780563, acc: 0.9842342138290405)
[2025-02-13 02:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:40][root][INFO] - Training Epoch: 1/2, step 995/7134 completed (loss: 0.05710849165916443, acc: 0.9783950448036194)
[2025-02-13 02:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:40][root][INFO] - Training Epoch: 1/2, step 996/7134 completed (loss: 0.07124055176973343, acc: 0.9748603105545044)
[2025-02-13 02:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:41][root][INFO] - Training Epoch: 1/2, step 997/7134 completed (loss: 0.0703136995434761, acc: 0.984240710735321)
[2025-02-13 02:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:41][root][INFO] - Training Epoch: 1/2, step 998/7134 completed (loss: 0.06495323777198792, acc: 0.9845070242881775)
[2025-02-13 02:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:41][root][INFO] - Training Epoch: 1/2, step 999/7134 completed (loss: 0.02341841161251068, acc: 0.9928315281867981)
[2025-02-13 02:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42][root][INFO] - Training Epoch: 1/2, step 1000/7134 completed (loss: 0.03301567584276199, acc: 0.9897260069847107)
[2025-02-13 02:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42][root][INFO] - Training Epoch: 1/2, step 1001/7134 completed (loss: 0.05740544572472572, acc: 0.9858585596084595)
[2025-02-13 02:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:43][root][INFO] - Training Epoch: 1/2, step 1002/7134 completed (loss: 0.048729877918958664, acc: 0.984308123588562)
[2025-02-13 02:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:43][root][INFO] - Training Epoch: 1/2, step 1003/7134 completed (loss: 0.06796713173389435, acc: 0.9821693897247314)
[2025-02-13 02:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44][root][INFO] - Training Epoch: 1/2, step 1004/7134 completed (loss: 0.08455662429332733, acc: 0.9731958508491516)
[2025-02-13 02:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44][root][INFO] - Training Epoch: 1/2, step 1005/7134 completed (loss: 0.10408440232276917, acc: 0.9735099077224731)
[2025-02-13 02:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45][root][INFO] - Training Epoch: 1/2, step 1006/7134 completed (loss: 0.06968246400356293, acc: 0.9818941354751587)
[2025-02-13 02:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45][root][INFO] - Training Epoch: 1/2, step 1007/7134 completed (loss: 0.08701298385858536, acc: 0.9733123779296875)
[2025-02-13 02:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45][root][INFO] - Training Epoch: 1/2, step 1008/7134 completed (loss: 0.06838254630565643, acc: 0.9872340559959412)
[2025-02-13 02:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:46][root][INFO] - Training Epoch: 1/2, step 1009/7134 completed (loss: 0.0737808495759964, acc: 0.9857904314994812)
[2025-02-13 02:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:46][root][INFO] - Training Epoch: 1/2, step 1010/7134 completed (loss: 0.04628504440188408, acc: 0.97826087474823)
[2025-02-13 02:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47][root][INFO] - Training Epoch: 1/2, step 1011/7134 completed (loss: 0.06157177686691284, acc: 0.9875518679618835)
[2025-02-13 02:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47][root][INFO] - Training Epoch: 1/2, step 1012/7134 completed (loss: 0.0810895785689354, acc: 0.9730878472328186)
[2025-02-13 02:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:48][root][INFO] - Training Epoch: 1/2, step 1013/7134 completed (loss: 0.07508975267410278, acc: 0.9756447076797485)
[2025-02-13 02:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:48][root][INFO] - Training Epoch: 1/2, step 1014/7134 completed (loss: 0.059306465089321136, acc: 0.9868637323379517)
[2025-02-13 02:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:49][root][INFO] - Training Epoch: 1/2, step 1015/7134 completed (loss: 0.06912887096405029, acc: 0.9789156913757324)
[2025-02-13 02:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:49][root][INFO] - Training Epoch: 1/2, step 1016/7134 completed (loss: 0.09776203334331512, acc: 0.9827916026115417)
[2025-02-13 02:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50][root][INFO] - Training Epoch: 1/2, step 1017/7134 completed (loss: 0.02836870774626732, acc: 0.991946280002594)
[2025-02-13 02:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50][root][INFO] - Training Epoch: 1/2, step 1018/7134 completed (loss: 0.08807454258203506, acc: 0.9795022010803223)
[2025-02-13 02:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50][root][INFO] - Training Epoch: 1/2, step 1019/7134 completed (loss: 0.06390117108821869, acc: 0.9823718070983887)
[2025-02-13 02:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:51][root][INFO] - Training Epoch: 1/2, step 1020/7134 completed (loss: 0.054373808205127716, acc: 0.983582079410553)
[2025-02-13 02:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:51][root][INFO] - Training Epoch: 1/2, step 1021/7134 completed (loss: 0.038200072944164276, acc: 0.9904458522796631)
[2025-02-13 02:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:52][root][INFO] - Training Epoch: 1/2, step 1022/7134 completed (loss: 0.052910882979631424, acc: 0.9868247509002686)
[2025-02-13 02:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:52][root][INFO] - Training Epoch: 1/2, step 1023/7134 completed (loss: 0.06546606123447418, acc: 0.9858430027961731)
[2025-02-13 02:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:53][root][INFO] - Training Epoch: 1/2, step 1024/7134 completed (loss: 0.05347013100981712, acc: 0.9874326586723328)
[2025-02-13 02:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:53][root][INFO] - Training Epoch: 1/2, step 1025/7134 completed (loss: 0.03624086454510689, acc: 0.9893238544464111)
[2025-02-13 02:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54][root][INFO] - Training Epoch: 1/2, step 1026/7134 completed (loss: 0.05410885065793991, acc: 0.9852631688117981)
[2025-02-13 02:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54][root][INFO] - Training Epoch: 1/2, step 1027/7134 completed (loss: 0.02334911935031414, acc: 0.9932340979576111)
[2025-02-13 02:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:55][root][INFO] - Training Epoch: 1/2, step 1028/7134 completed (loss: 0.02942132018506527, acc: 0.9895366430282593)
[2025-02-13 02:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:55][root][INFO] - Training Epoch: 1/2, step 1029/7134 completed (loss: 0.045767512172460556, acc: 0.9910846948623657)
[2025-02-13 02:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:55][root][INFO] - Training Epoch: 1/2, step 1030/7134 completed (loss: 0.016768252477049828, acc: 0.9946091771125793)
[2025-02-13 02:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:56][root][INFO] - Training Epoch: 1/2, step 1031/7134 completed (loss: 0.06561000645160675, acc: 0.9817073345184326)
[2025-02-13 02:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:56][root][INFO] - Training Epoch: 1/2, step 1032/7134 completed (loss: 0.07443981617689133, acc: 0.9827188849449158)
[2025-02-13 02:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57][root][INFO] - Training Epoch: 1/2, step 1033/7134 completed (loss: 0.045565541833639145, acc: 0.9870874881744385)
[2025-02-13 02:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57][root][INFO] - Training Epoch: 1/2, step 1034/7134 completed (loss: 0.07228179275989532, acc: 0.9854133129119873)
[2025-02-13 02:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:58][root][INFO] - Training Epoch: 1/2, step 1035/7134 completed (loss: 0.3053116500377655, acc: 0.9385964870452881)
[2025-02-13 02:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:58][root][INFO] - Training Epoch: 1/2, step 1036/7134 completed (loss: 0.3553588390350342, acc: 0.9422222375869751)
[2025-02-13 02:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:59][root][INFO] - Training Epoch: 1/2, step 1037/7134 completed (loss: 0.04444729536771774, acc: 0.9845238327980042)
[2025-02-13 02:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:59][root][INFO] - Training Epoch: 1/2, step 1038/7134 completed (loss: 0.03998945653438568, acc: 0.9882550239562988)
[2025-02-13 02:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:59][root][INFO] - Training Epoch: 1/2, step 1039/7134 completed (loss: 0.04316538944840431, acc: 0.9875776171684265)
[2025-02-13 02:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00][root][INFO] - Training Epoch: 1/2, step 1040/7134 completed (loss: 0.02439091168344021, acc: 0.9948630332946777)
[2025-02-13 02:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00][root][INFO] - Training Epoch: 1/2, step 1041/7134 completed (loss: 0.0432102344930172, acc: 0.9922118186950684)
[2025-02-13 02:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:01][root][INFO] - Training Epoch: 1/2, step 1042/7134 completed (loss: 0.060461487621068954, acc: 0.978205144405365)
[2025-02-13 02:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:01][root][INFO] - Training Epoch: 1/2, step 1043/7134 completed (loss: 0.04934302344918251, acc: 0.9831528067588806)
[2025-02-13 02:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:02][root][INFO] - Training Epoch: 1/2, step 1044/7134 completed (loss: 0.05711504444479942, acc: 0.9890410900115967)
[2025-02-13 02:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:02][root][INFO] - Training Epoch: 1/2, step 1045/7134 completed (loss: 0.11426261067390442, acc: 0.9757673740386963)
[2025-02-13 02:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:03][root][INFO] - Training Epoch: 1/2, step 1046/7134 completed (loss: 0.04414043202996254, acc: 0.9903475046157837)
[2025-02-13 02:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:03][root][INFO] - Training Epoch: 1/2, step 1047/7134 completed (loss: 0.08859298378229141, acc: 0.9807407259941101)
[2025-02-13 02:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:04][root][INFO] - Training Epoch: 1/2, step 1048/7134 completed (loss: 0.032041095197200775, acc: 0.9912023544311523)
[2025-02-13 02:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:04][root][INFO] - Training Epoch: 1/2, step 1049/7134 completed (loss: 0.07513018697500229, acc: 0.980966329574585)
[2025-02-13 02:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:05][root][INFO] - Training Epoch: 1/2, step 1050/7134 completed (loss: 0.06300880014896393, acc: 0.9822404384613037)
[2025-02-13 02:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:05][root][INFO] - Training Epoch: 1/2, step 1051/7134 completed (loss: 0.03375677764415741, acc: 0.9890410900115967)
[2025-02-13 02:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:05][root][INFO] - Training Epoch: 1/2, step 1052/7134 completed (loss: 0.09654586762189865, acc: 0.9789983630180359)
[2025-02-13 02:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06][root][INFO] - Training Epoch: 1/2, step 1053/7134 completed (loss: 0.03465977683663368, acc: 0.9845070242881775)
[2025-02-13 02:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06][root][INFO] - Training Epoch: 1/2, step 1054/7134 completed (loss: 0.04901905357837677, acc: 0.9841059446334839)
[2025-02-13 02:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:07][root][INFO] - Training Epoch: 1/2, step 1055/7134 completed (loss: 0.09301051497459412, acc: 0.9733924865722656)
[2025-02-13 02:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:07][root][INFO] - Training Epoch: 1/2, step 1056/7134 completed (loss: 0.09779104590415955, acc: 0.9731993079185486)
[2025-02-13 02:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:08][root][INFO] - Training Epoch: 1/2, step 1057/7134 completed (loss: 0.06924544274806976, acc: 0.9783236980438232)
[2025-02-13 02:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:08][root][INFO] - Training Epoch: 1/2, step 1058/7134 completed (loss: 0.04832102358341217, acc: 0.9826086759567261)
[2025-02-13 02:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:09][root][INFO] - Training Epoch: 1/2, step 1059/7134 completed (loss: 0.0440739281475544, acc: 0.9883871078491211)
[2025-02-13 02:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:09][root][INFO] - Training Epoch: 1/2, step 1060/7134 completed (loss: 0.07100089639425278, acc: 0.9826498627662659)
[2025-02-13 02:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:10][root][INFO] - Training Epoch: 1/2, step 1061/7134 completed (loss: 0.05509144067764282, acc: 0.979651153087616)
[2025-02-13 02:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:10][root][INFO] - Training Epoch: 1/2, step 1062/7134 completed (loss: 0.05031786859035492, acc: 0.9847457408905029)
[2025-02-13 02:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11][root][INFO] - Training Epoch: 1/2, step 1063/7134 completed (loss: 0.02295869030058384, acc: 0.9935170412063599)
[2025-02-13 02:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11][root][INFO] - Training Epoch: 1/2, step 1064/7134 completed (loss: 0.047329988330602646, acc: 0.9814814925193787)
[2025-02-13 02:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11][root][INFO] - Training Epoch: 1/2, step 1065/7134 completed (loss: 0.025836439803242683, acc: 0.9878048896789551)
[2025-02-13 02:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12][root][INFO] - Training Epoch: 1/2, step 1066/7134 completed (loss: 0.08870299160480499, acc: 0.9765100479125977)
[2025-02-13 02:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12][root][INFO] - Training Epoch: 1/2, step 1067/7134 completed (loss: 0.04994770511984825, acc: 0.9845916628837585)
[2025-02-13 02:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:13][root][INFO] - Training Epoch: 1/2, step 1068/7134 completed (loss: 0.04466751590371132, acc: 0.9849905967712402)
[2025-02-13 02:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:13][root][INFO] - Training Epoch: 1/2, step 1069/7134 completed (loss: 0.03485006093978882, acc: 0.9866443872451782)
[2025-02-13 02:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:14][root][INFO] - Training Epoch: 1/2, step 1070/7134 completed (loss: 0.04576408863067627, acc: 0.9868637323379517)
[2025-02-13 02:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:14][root][INFO] - Training Epoch: 1/2, step 1071/7134 completed (loss: 0.03578267619013786, acc: 0.9892904758453369)
[2025-02-13 02:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15][root][INFO] - Training Epoch: 1/2, step 1072/7134 completed (loss: 0.005844711326062679, acc: 0.9986467957496643)
[2025-02-13 02:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15][root][INFO] - Training Epoch: 1/2, step 1073/7134 completed (loss: 0.01895686611533165, acc: 0.9927954077720642)
[2025-02-13 02:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15][root][INFO] - Training Epoch: 1/2, step 1074/7134 completed (loss: 0.08107789605855942, acc: 0.9775474667549133)
[2025-02-13 02:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:16][root][INFO] - Training Epoch: 1/2, step 1075/7134 completed (loss: 0.13631972670555115, acc: 0.9723756909370422)
[2025-02-13 02:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:16][root][INFO] - Training Epoch: 1/2, step 1076/7134 completed (loss: 0.10958658158779144, acc: 0.9654255509376526)
[2025-02-13 02:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17][root][INFO] - Training Epoch: 1/2, step 1077/7134 completed (loss: 0.1109582781791687, acc: 0.9729729890823364)
[2025-02-13 02:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17][root][INFO] - Training Epoch: 1/2, step 1078/7134 completed (loss: 0.12374655902385712, acc: 0.9644588232040405)
[2025-02-13 02:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:18][root][INFO] - Training Epoch: 1/2, step 1079/7134 completed (loss: 0.053333789110183716, acc: 0.9757785201072693)
[2025-02-13 02:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:18][root][INFO] - Training Epoch: 1/2, step 1080/7134 completed (loss: 0.05022609606385231, acc: 0.9924812316894531)
[2025-02-13 02:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:19][root][INFO] - Training Epoch: 1/2, step 1081/7134 completed (loss: 0.0261142086237669, acc: 0.9901823401451111)
[2025-02-13 02:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:19][root][INFO] - Training Epoch: 1/2, step 1082/7134 completed (loss: 0.026214027777314186, acc: 0.9928057789802551)
[2025-02-13 02:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20][root][INFO] - Training Epoch: 1/2, step 1083/7134 completed (loss: 0.028494568541646004, acc: 0.989830493927002)
[2025-02-13 02:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20][root][INFO] - Training Epoch: 1/2, step 1084/7134 completed (loss: 0.03416304662823677, acc: 0.9900867342948914)
[2025-02-13 02:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20][root][INFO] - Training Epoch: 1/2, step 1085/7134 completed (loss: 0.036626365035772324, acc: 0.9879518151283264)
[2025-02-13 02:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:21][root][INFO] - Training Epoch: 1/2, step 1086/7134 completed (loss: 0.11613252758979797, acc: 0.9754253029823303)
[2025-02-13 02:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:21][root][INFO] - Training Epoch: 1/2, step 1087/7134 completed (loss: 0.05825181305408478, acc: 0.9804878234863281)
[2025-02-13 02:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:22][root][INFO] - Training Epoch: 1/2, step 1088/7134 completed (loss: 0.03821871057152748, acc: 0.9916201233863831)
[2025-02-13 02:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:22][root][INFO] - Training Epoch: 1/2, step 1089/7134 completed (loss: 0.055688776075839996, acc: 0.9836065769195557)
[2025-02-13 02:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23][root][INFO] - Training Epoch: 1/2, step 1090/7134 completed (loss: 0.04828054830431938, acc: 0.9875195026397705)
[2025-02-13 02:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23][root][INFO] - Training Epoch: 1/2, step 1091/7134 completed (loss: 0.04805493354797363, acc: 0.9863013625144958)
[2025-02-13 02:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24][root][INFO] - Training Epoch: 1/2, step 1092/7134 completed (loss: 0.03244897723197937, acc: 0.9884393215179443)
[2025-02-13 02:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24][root][INFO] - Training Epoch: 1/2, step 1093/7134 completed (loss: 0.10756885260343552, acc: 0.9733542203903198)
[2025-02-13 02:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24][root][INFO] - Training Epoch: 1/2, step 1094/7134 completed (loss: 0.0810672864317894, acc: 0.9721189737319946)
[2025-02-13 02:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25][root][INFO] - Training Epoch: 1/2, step 1095/7134 completed (loss: 0.06289713084697723, acc: 0.9792935252189636)
[2025-02-13 02:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25][root][INFO] - Training Epoch: 1/2, step 1096/7134 completed (loss: 0.050367970019578934, acc: 0.9873417615890503)
[2025-02-13 02:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:26][root][INFO] - Training Epoch: 1/2, step 1097/7134 completed (loss: 0.06909602135419846, acc: 0.976123571395874)
[2025-02-13 02:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:26][root][INFO] - Training Epoch: 1/2, step 1098/7134 completed (loss: 0.03399670496582985, acc: 0.9925834536552429)
[2025-02-13 02:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:27][root][INFO] - Training Epoch: 1/2, step 1099/7134 completed (loss: 0.0425097681581974, acc: 0.9833333492279053)
[2025-02-13 02:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:27][root][INFO] - Training Epoch: 1/2, step 1100/7134 completed (loss: 0.05555926263332367, acc: 0.9864130616188049)
[2025-02-13 02:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28][root][INFO] - Training Epoch: 1/2, step 1101/7134 completed (loss: 0.042879004031419754, acc: 0.9909090995788574)
[2025-02-13 02:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28][root][INFO] - Training Epoch: 1/2, step 1102/7134 completed (loss: 0.052987970411777496, acc: 0.9902152419090271)
[2025-02-13 02:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:29][root][INFO] - Training Epoch: 1/2, step 1103/7134 completed (loss: 0.08062141388654709, acc: 0.9800000190734863)
[2025-02-13 02:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:29][root][INFO] - Training Epoch: 1/2, step 1104/7134 completed (loss: 0.05106460303068161, acc: 0.9855907559394836)
[2025-02-13 02:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:30][root][INFO] - Training Epoch: 1/2, step 1105/7134 completed (loss: 0.05638248845934868, acc: 0.9838274717330933)
[2025-02-13 02:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:30][root][INFO] - Training Epoch: 1/2, step 1106/7134 completed (loss: 0.05185624957084656, acc: 0.9793388247489929)
[2025-02-13 02:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:30][root][INFO] - Training Epoch: 1/2, step 1107/7134 completed (loss: 0.04597024247050285, acc: 0.9859319925308228)
[2025-02-13 02:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31][root][INFO] - Training Epoch: 1/2, step 1108/7134 completed (loss: 0.026442330330610275, acc: 0.9896907210350037)
[2025-02-13 02:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31][root][INFO] - Training Epoch: 1/2, step 1109/7134 completed (loss: 0.07484736293554306, acc: 0.9706704020500183)
[2025-02-13 02:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:32][root][INFO] - Training Epoch: 1/2, step 1110/7134 completed (loss: 0.08188740909099579, acc: 0.9764088988304138)
[2025-02-13 02:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:32][root][INFO] - Training Epoch: 1/2, step 1111/7134 completed (loss: 0.05219319835305214, acc: 0.9834254384040833)
[2025-02-13 02:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:33][root][INFO] - Training Epoch: 1/2, step 1112/7134 completed (loss: 0.06283722817897797, acc: 0.9808823466300964)
[2025-02-13 02:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:33][root][INFO] - Training Epoch: 1/2, step 1113/7134 completed (loss: 0.04338814690709114, acc: 0.9871299862861633)
[2025-02-13 02:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:34][root][INFO] - Training Epoch: 1/2, step 1114/7134 completed (loss: 0.045801132917404175, acc: 0.9866342544555664)
[2025-02-13 02:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:34][root][INFO] - Training Epoch: 1/2, step 1115/7134 completed (loss: 0.04817156121134758, acc: 0.9921383857727051)
[2025-02-13 02:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35][root][INFO] - Training Epoch: 1/2, step 1116/7134 completed (loss: 0.03141520172357559, acc: 0.9906716346740723)
[2025-02-13 02:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35][root][INFO] - Training Epoch: 1/2, step 1117/7134 completed (loss: 0.03618815541267395, acc: 0.9923312664031982)
[2025-02-13 02:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35][root][INFO] - Training Epoch: 1/2, step 1118/7134 completed (loss: 0.03950734809041023, acc: 0.9896551966667175)
[2025-02-13 02:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:36][root][INFO] - Training Epoch: 1/2, step 1119/7134 completed (loss: 0.04707495868206024, acc: 0.9857819676399231)
[2025-02-13 02:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:36][root][INFO] - Training Epoch: 1/2, step 1120/7134 completed (loss: 0.055676545947790146, acc: 0.9827044010162354)
[2025-02-13 02:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37][root][INFO] - Training Epoch: 1/2, step 1121/7134 completed (loss: 0.06304008513689041, acc: 0.9837518334388733)
[2025-02-13 02:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37][root][INFO] - Training Epoch: 1/2, step 1122/7134 completed (loss: 0.041862603276968, acc: 0.9883494973182678)
[2025-02-13 02:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37][root][INFO] - Training Epoch: 1/2, step 1123/7134 completed (loss: 0.051585692912340164, acc: 0.9823434948921204)
[2025-02-13 02:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:38][root][INFO] - Training Epoch: 1/2, step 1124/7134 completed (loss: 0.03167039155960083, acc: 0.9922600388526917)
[2025-02-13 02:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:38][root][INFO] - Training Epoch: 1/2, step 1125/7134 completed (loss: 0.03615528345108032, acc: 0.9864681959152222)
[2025-02-13 02:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:39][root][INFO] - Training Epoch: 1/2, step 1126/7134 completed (loss: 0.04656534641981125, acc: 0.9861878156661987)
[2025-02-13 02:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:39][root][INFO] - Training Epoch: 1/2, step 1127/7134 completed (loss: 0.03415472432971001, acc: 0.9919999837875366)
[2025-02-13 02:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:40][root][INFO] - Training Epoch: 1/2, step 1128/7134 completed (loss: 0.04594410955905914, acc: 0.9878472089767456)
[2025-02-13 02:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:40][root][INFO] - Training Epoch: 1/2, step 1129/7134 completed (loss: 0.05394979938864708, acc: 0.9820627570152283)
[2025-02-13 02:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41][root][INFO] - Training Epoch: 1/2, step 1130/7134 completed (loss: 0.03771953284740448, acc: 0.9919224381446838)
[2025-02-13 02:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41][root][INFO] - Training Epoch: 1/2, step 1131/7134 completed (loss: 0.06391038745641708, acc: 0.9823113083839417)
[2025-02-13 02:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42][root][INFO] - Training Epoch: 1/2, step 1132/7134 completed (loss: 0.11011490970849991, acc: 0.9709035158157349)
[2025-02-13 02:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42][root][INFO] - Training Epoch: 1/2, step 1133/7134 completed (loss: 0.1013236716389656, acc: 0.9728958606719971)
[2025-02-13 02:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42][root][INFO] - Training Epoch: 1/2, step 1134/7134 completed (loss: 0.10620417445898056, acc: 0.9700748324394226)
[2025-02-13 02:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:43][root][INFO] - Training Epoch: 1/2, step 1135/7134 completed (loss: 0.10689971596002579, acc: 0.9711999893188477)
[2025-02-13 02:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:43][root][INFO] - Training Epoch: 1/2, step 1136/7134 completed (loss: 0.07543949782848358, acc: 0.9795918464660645)
[2025-02-13 02:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44][root][INFO] - Training Epoch: 1/2, step 1137/7134 completed (loss: 0.06252820789813995, acc: 0.9795454740524292)
[2025-02-13 02:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44][root][INFO] - Training Epoch: 1/2, step 1138/7134 completed (loss: 0.05275614932179451, acc: 0.9836065769195557)
[2025-02-13 02:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:45][root][INFO] - Training Epoch: 1/2, step 1139/7134 completed (loss: 0.08120954036712646, acc: 0.9747048616409302)
[2025-02-13 02:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:45][root][INFO] - Training Epoch: 1/2, step 1140/7134 completed (loss: 0.02868630178272724, acc: 0.9899749159812927)
[2025-02-13 02:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46][root][INFO] - Training Epoch: 1/2, step 1141/7134 completed (loss: 0.022814301773905754, acc: 0.9929742217063904)
[2025-02-13 02:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46][root][INFO] - Training Epoch: 1/2, step 1142/7134 completed (loss: 0.07132149487733841, acc: 0.98828125)
[2025-02-13 02:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46][root][INFO] - Training Epoch: 1/2, step 1143/7134 completed (loss: 0.09512250870466232, acc: 0.9735743999481201)
[2025-02-13 02:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:47][root][INFO] - Training Epoch: 1/2, step 1144/7134 completed (loss: 0.10115620493888855, acc: 0.9725000262260437)
[2025-02-13 02:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:47][root][INFO] - Training Epoch: 1/2, step 1145/7134 completed (loss: 0.03598218038678169, acc: 0.9892086386680603)
[2025-02-13 02:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48][root][INFO] - Training Epoch: 1/2, step 1146/7134 completed (loss: 0.046445462852716446, acc: 0.9855072498321533)
[2025-02-13 02:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48][root][INFO] - Training Epoch: 1/2, step 1147/7134 completed (loss: 0.022573456168174744, acc: 0.9925187230110168)
[2025-02-13 02:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:49][root][INFO] - Training Epoch: 1/2, step 1148/7134 completed (loss: 0.04454074427485466, acc: 0.9914675951004028)
[2025-02-13 02:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:49][root][INFO] - Training Epoch: 1/2, step 1149/7134 completed (loss: 0.14057804644107819, acc: 0.9648351669311523)
[2025-02-13 02:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50][root][INFO] - Training Epoch: 1/2, step 1150/7134 completed (loss: 0.06127268448472023, acc: 0.9860464930534363)
[2025-02-13 02:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50][root][INFO] - Training Epoch: 1/2, step 1151/7134 completed (loss: 0.025004051625728607, acc: 0.9918864369392395)
[2025-02-13 02:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50][root][INFO] - Training Epoch: 1/2, step 1152/7134 completed (loss: 0.040211182087659836, acc: 0.9914529919624329)
[2025-02-13 02:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:51][root][INFO] - Training Epoch: 1/2, step 1153/7134 completed (loss: 0.08976253122091293, acc: 0.9782016277313232)
[2025-02-13 02:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:51][root][INFO] - Training Epoch: 1/2, step 1154/7134 completed (loss: 0.054345108568668365, acc: 0.9858406782150269)
[2025-02-13 02:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:52][root][INFO] - Training Epoch: 1/2, step 1155/7134 completed (loss: 0.07598643004894257, acc: 0.9714912176132202)
[2025-02-13 02:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:52][root][INFO] - Training Epoch: 1/2, step 1156/7134 completed (loss: 0.028420807793736458, acc: 0.9923195242881775)
[2025-02-13 02:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53][root][INFO] - Training Epoch: 1/2, step 1157/7134 completed (loss: 0.07685784250497818, acc: 0.9745330810546875)
[2025-02-13 02:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53][root][INFO] - Training Epoch: 1/2, step 1158/7134 completed (loss: 0.07159878313541412, acc: 0.9783037304878235)
[2025-02-13 02:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54][root][INFO] - Training Epoch: 1/2, step 1159/7134 completed (loss: 0.07518798857927322, acc: 0.978723406791687)
[2025-02-13 02:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54][root][INFO] - Training Epoch: 1/2, step 1160/7134 completed (loss: 0.05168434977531433, acc: 0.9851190447807312)
[2025-02-13 02:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:55][root][INFO] - Training Epoch: 1/2, step 1161/7134 completed (loss: 0.09711484611034393, acc: 0.9771615266799927)
[2025-02-13 02:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:55][root][INFO] - Training Epoch: 1/2, step 1162/7134 completed (loss: 0.0538313053548336, acc: 0.9801653027534485)
[2025-02-13 02:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:55][root][INFO] - Training Epoch: 1/2, step 1163/7134 completed (loss: 0.06528698652982712, acc: 0.9846416115760803)
[2025-02-13 02:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56][root][INFO] - Training Epoch: 1/2, step 1164/7134 completed (loss: 0.07135841250419617, acc: 0.9833101630210876)
[2025-02-13 02:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56][root][INFO] - Training Epoch: 1/2, step 1165/7134 completed (loss: 0.0335114412009716, acc: 0.9891451597213745)
[2025-02-13 02:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:57][root][INFO] - Training Epoch: 1/2, step 1166/7134 completed (loss: 0.03601106256246567, acc: 0.9903978109359741)
[2025-02-13 02:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:57][root][INFO] - Training Epoch: 1/2, step 1167/7134 completed (loss: 0.03227116912603378, acc: 0.994106113910675)
[2025-02-13 02:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58][root][INFO] - Training Epoch: 1/2, step 1168/7134 completed (loss: 0.017821425572037697, acc: 0.9973683953285217)
[2025-02-13 02:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58][root][INFO] - Training Epoch: 1/2, step 1169/7134 completed (loss: 0.05215297266840935, acc: 0.9851351380348206)
[2025-02-13 02:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:59][root][INFO] - Training Epoch: 1/2, step 1170/7134 completed (loss: 0.04856334626674652, acc: 0.992977499961853)
[2025-02-13 02:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:59][root][INFO] - Training Epoch: 1/2, step 1171/7134 completed (loss: 0.04024925455451012, acc: 0.9883138537406921)
[2025-02-13 02:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:59][root][INFO] - Training Epoch: 1/2, step 1172/7134 completed (loss: 0.062299344688653946, acc: 0.9803328514099121)
[2025-02-13 02:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:00][root][INFO] - Training Epoch: 1/2, step 1173/7134 completed (loss: 0.06290792673826218, acc: 0.9811594486236572)
[2025-02-13 02:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:00][root][INFO] - Training Epoch: 1/2, step 1174/7134 completed (loss: 0.0184832401573658, acc: 0.9954954981803894)
[2025-02-13 02:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01][root][INFO] - Training Epoch: 1/2, step 1175/7134 completed (loss: 0.026791086420416832, acc: 0.9942029118537903)
[2025-02-13 02:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01][root][INFO] - Training Epoch: 1/2, step 1176/7134 completed (loss: 0.01292240247130394, acc: 0.9982876777648926)
[2025-02-13 02:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:02][root][INFO] - Training Epoch: 1/2, step 1177/7134 completed (loss: 0.013719732873141766, acc: 0.998039186000824)
[2025-02-13 02:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:02][root][INFO] - Training Epoch: 1/2, step 1178/7134 completed (loss: 0.028903616592288017, acc: 0.9929906725883484)
[2025-02-13 02:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:02][root][INFO] - Training Epoch: 1/2, step 1179/7134 completed (loss: 0.037400972098112106, acc: 0.9923518300056458)
[2025-02-13 02:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:03][root][INFO] - Training Epoch: 1/2, step 1180/7134 completed (loss: 0.02898067981004715, acc: 0.9927007555961609)
[2025-02-13 02:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:03][root][INFO] - Training Epoch: 1/2, step 1181/7134 completed (loss: 0.03280464932322502, acc: 0.9884892106056213)
[2025-02-13 02:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:04][root][INFO] - Training Epoch: 1/2, step 1182/7134 completed (loss: 0.01598191261291504, acc: 0.9965811967849731)
[2025-02-13 02:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:04][root][INFO] - Training Epoch: 1/2, step 1183/7134 completed (loss: 0.02646755240857601, acc: 0.9911894202232361)
[2025-02-13 02:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05][root][INFO] - Training Epoch: 1/2, step 1184/7134 completed (loss: 0.06796091794967651, acc: 0.9871244430541992)
[2025-02-13 02:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05][root][INFO] - Training Epoch: 1/2, step 1185/7134 completed (loss: 0.013040962629020214, acc: 0.995488703250885)
[2025-02-13 02:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05][root][INFO] - Training Epoch: 1/2, step 1186/7134 completed (loss: 0.03241809830069542, acc: 0.9961089491844177)
[2025-02-13 02:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:06][root][INFO] - Training Epoch: 1/2, step 1187/7134 completed (loss: 0.03459883853793144, acc: 0.9916897416114807)
[2025-02-13 02:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:06][root][INFO] - Training Epoch: 1/2, step 1188/7134 completed (loss: 0.006158954463899136, acc: 0.9986807107925415)
[2025-02-13 02:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:07][root][INFO] - Training Epoch: 1/2, step 1189/7134 completed (loss: 0.02251604199409485, acc: 0.9942196607589722)
[2025-02-13 02:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:07][root][INFO] - Training Epoch: 1/2, step 1190/7134 completed (loss: 0.04630008712410927, acc: 0.9898219108581543)
[2025-02-13 02:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08][root][INFO] - Training Epoch: 1/2, step 1191/7134 completed (loss: 0.06175113096833229, acc: 0.9867060780525208)
[2025-02-13 02:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08][root][INFO] - Training Epoch: 1/2, step 1192/7134 completed (loss: 0.023876162245869637, acc: 0.9947575330734253)
[2025-02-13 02:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09][root][INFO] - Training Epoch: 1/2, step 1193/7134 completed (loss: 0.08093181997537613, acc: 0.980861246585846)
[2025-02-13 02:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09][root][INFO] - Training Epoch: 1/2, step 1194/7134 completed (loss: 0.04709045588970184, acc: 0.9903846383094788)
[2025-02-13 02:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09][root][INFO] - Training Epoch: 1/2, step 1195/7134 completed (loss: 0.061760179698467255, acc: 0.983660101890564)
[2025-02-13 02:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10][root][INFO] - Training Epoch: 1/2, step 1196/7134 completed (loss: 0.06927620619535446, acc: 0.9819672107696533)
[2025-02-13 02:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10][root][INFO] - Training Epoch: 1/2, step 1197/7134 completed (loss: 0.04326606169342995, acc: 0.9882698059082031)
[2025-02-13 02:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:11][root][INFO] - Training Epoch: 1/2, step 1198/7134 completed (loss: 0.05088025704026222, acc: 0.9841726422309875)
[2025-02-13 02:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:11][root][INFO] - Training Epoch: 1/2, step 1199/7134 completed (loss: 0.030726447701454163, acc: 0.9911634922027588)
[2025-02-13 02:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12][root][INFO] - Training Epoch: 1/2, step 1200/7134 completed (loss: 0.03448355570435524, acc: 0.9917864203453064)
[2025-02-13 02:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12][root][INFO] - Training Epoch: 1/2, step 1201/7134 completed (loss: 0.05650121718645096, acc: 0.9793814420700073)
[2025-02-13 02:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:13][root][INFO] - Training Epoch: 1/2, step 1202/7134 completed (loss: 0.05086340755224228, acc: 0.9811574816703796)
[2025-02-13 02:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:13][root][INFO] - Training Epoch: 1/2, step 1203/7134 completed (loss: 0.015791112557053566, acc: 0.9969651103019714)
[2025-02-13 02:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:13][root][INFO] - Training Epoch: 1/2, step 1204/7134 completed (loss: 0.06576424092054367, acc: 0.983208954334259)
[2025-02-13 02:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:14][root][INFO] - Training Epoch: 1/2, step 1205/7134 completed (loss: 0.08492425084114075, acc: 0.9788732528686523)
[2025-02-13 02:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:14][root][INFO] - Training Epoch: 1/2, step 1206/7134 completed (loss: 0.04320898279547691, acc: 0.9888337254524231)
[2025-02-13 02:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15][root][INFO] - Training Epoch: 1/2, step 1207/7134 completed (loss: 0.047583192586898804, acc: 0.9861809015274048)
[2025-02-13 02:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15][root][INFO] - Training Epoch: 1/2, step 1208/7134 completed (loss: 0.03747491165995598, acc: 0.986940324306488)
[2025-02-13 02:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:16][root][INFO] - Training Epoch: 1/2, step 1209/7134 completed (loss: 0.09924286603927612, acc: 0.972577691078186)
[2025-02-13 02:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:16][root][INFO] - Training Epoch: 1/2, step 1210/7134 completed (loss: 0.06806403398513794, acc: 0.9824561476707458)
[2025-02-13 02:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17][root][INFO] - Training Epoch: 1/2, step 1211/7134 completed (loss: 0.15796895325183868, acc: 0.9518304467201233)
[2025-02-13 02:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17][root][INFO] - Training Epoch: 1/2, step 1212/7134 completed (loss: 0.0991741493344307, acc: 0.9664633870124817)
[2025-02-13 02:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:18][root][INFO] - Training Epoch: 1/2, step 1213/7134 completed (loss: 0.05393827706575394, acc: 0.9830268621444702)
[2025-02-13 02:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:18][root][INFO] - Training Epoch: 1/2, step 1214/7134 completed (loss: 0.06696650385856628, acc: 0.9800000190734863)
[2025-02-13 02:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19][root][INFO] - Training Epoch: 1/2, step 1215/7134 completed (loss: 0.10579778999090195, acc: 0.9765517115592957)
[2025-02-13 02:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19][root][INFO] - Training Epoch: 1/2, step 1216/7134 completed (loss: 0.05175991728901863, acc: 0.9839416146278381)
[2025-02-13 02:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19][root][INFO] - Training Epoch: 1/2, step 1217/7134 completed (loss: 0.06543739140033722, acc: 0.9870129823684692)
[2025-02-13 02:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:20][root][INFO] - Training Epoch: 1/2, step 1218/7134 completed (loss: 0.10948870331048965, acc: 0.9686567187309265)
[2025-02-13 02:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:20][root][INFO] - Training Epoch: 1/2, step 1219/7134 completed (loss: 0.1332695633172989, acc: 0.9648506045341492)
[2025-02-13 02:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:21][root][INFO] - Training Epoch: 1/2, step 1220/7134 completed (loss: 0.13118579983711243, acc: 0.970812201499939)
[2025-02-13 02:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:21][root][INFO] - Training Epoch: 1/2, step 1221/7134 completed (loss: 0.10922252386808395, acc: 0.9703947305679321)
[2025-02-13 02:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22][root][INFO] - Training Epoch: 1/2, step 1222/7134 completed (loss: 0.08615107089281082, acc: 0.9773539900779724)
[2025-02-13 02:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22][root][INFO] - Training Epoch: 1/2, step 1223/7134 completed (loss: 0.1768663227558136, acc: 0.9643347263336182)
[2025-02-13 02:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:23][root][INFO] - Training Epoch: 1/2, step 1224/7134 completed (loss: 0.06707866489887238, acc: 0.9889624714851379)
[2025-02-13 02:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:23][root][INFO] - Training Epoch: 1/2, step 1225/7134 completed (loss: 0.06988353282213211, acc: 0.9795686602592468)
[2025-02-13 02:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:24][root][INFO] - Training Epoch: 1/2, step 1226/7134 completed (loss: 0.134046271443367, acc: 0.9698188900947571)
[2025-02-13 02:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:24][root][INFO] - Training Epoch: 1/2, step 1227/7134 completed (loss: 0.10037577897310257, acc: 0.97050940990448)
[2025-02-13 02:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:25][root][INFO] - Training Epoch: 1/2, step 1228/7134 completed (loss: 0.12574076652526855, acc: 0.9728183150291443)
[2025-02-13 02:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:25][root][INFO] - Training Epoch: 1/2, step 1229/7134 completed (loss: 0.07681978493928909, acc: 0.981992781162262)
[2025-02-13 02:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26][root][INFO] - Training Epoch: 1/2, step 1230/7134 completed (loss: 0.09620578587055206, acc: 0.9728813767433167)
[2025-02-13 02:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26][root][INFO] - Training Epoch: 1/2, step 1231/7134 completed (loss: 0.08180990070104599, acc: 0.9824198484420776)
[2025-02-13 02:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:27][root][INFO] - Training Epoch: 1/2, step 1232/7134 completed (loss: 0.07462938129901886, acc: 0.9797657132148743)
[2025-02-13 02:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:27][root][INFO] - Training Epoch: 1/2, step 1233/7134 completed (loss: 0.13188743591308594, acc: 0.9641618728637695)
[2025-02-13 02:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:28][root][INFO] - Training Epoch: 1/2, step 1234/7134 completed (loss: 0.05789542198181152, acc: 0.9829192757606506)
[2025-02-13 02:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:28][root][INFO] - Training Epoch: 1/2, step 1235/7134 completed (loss: 0.08390618115663528, acc: 0.9796162843704224)
[2025-02-13 02:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29][root][INFO] - Training Epoch: 1/2, step 1236/7134 completed (loss: 0.1350964605808258, acc: 0.9633967876434326)
[2025-02-13 02:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29][root][INFO] - Training Epoch: 1/2, step 1237/7134 completed (loss: 0.08776752650737762, acc: 0.9767080545425415)
[2025-02-13 02:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:30][root][INFO] - Training Epoch: 1/2, step 1238/7134 completed (loss: 0.14445683360099792, acc: 0.9563318490982056)
[2025-02-13 02:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:30][root][INFO] - Training Epoch: 1/2, step 1239/7134 completed (loss: 0.10555609315633774, acc: 0.9670958518981934)
[2025-02-13 02:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:31][root][INFO] - Training Epoch: 1/2, step 1240/7134 completed (loss: 0.08458272367715836, acc: 0.9722222089767456)
[2025-02-13 02:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:31][root][INFO] - Training Epoch: 1/2, step 1241/7134 completed (loss: 0.10345694422721863, acc: 0.9638554453849792)
[2025-02-13 02:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32][root][INFO] - Training Epoch: 1/2, step 1242/7134 completed (loss: 0.06954853981733322, acc: 0.9807692170143127)
[2025-02-13 02:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32][root][INFO] - Training Epoch: 1/2, step 1243/7134 completed (loss: 0.07268642634153366, acc: 0.9843096137046814)
[2025-02-13 02:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32][root][INFO] - Training Epoch: 1/2, step 1244/7134 completed (loss: 0.07960136979818344, acc: 0.9817880988121033)
[2025-02-13 02:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:33][root][INFO] - Training Epoch: 1/2, step 1245/7134 completed (loss: 0.09173347800970078, acc: 0.9773299694061279)
[2025-02-13 02:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:33][root][INFO] - Training Epoch: 1/2, step 1246/7134 completed (loss: 0.10086604952812195, acc: 0.9756097793579102)
[2025-02-13 02:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:34][root][INFO] - Training Epoch: 1/2, step 1247/7134 completed (loss: 0.051035720854997635, acc: 0.9884225726127625)
[2025-02-13 02:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:34][root][INFO] - Training Epoch: 1/2, step 1248/7134 completed (loss: 0.08155752718448639, acc: 0.9794050455093384)
[2025-02-13 02:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:35][root][INFO] - Training Epoch: 1/2, step 1249/7134 completed (loss: 0.05139659717679024, acc: 0.982206404209137)
[2025-02-13 02:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:35][root][INFO] - Training Epoch: 1/2, step 1250/7134 completed (loss: 0.05443095788359642, acc: 0.9830028414726257)
[2025-02-13 02:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36][root][INFO] - Training Epoch: 1/2, step 1251/7134 completed (loss: 0.052286118268966675, acc: 0.9821673631668091)
[2025-02-13 02:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36][root][INFO] - Training Epoch: 1/2, step 1252/7134 completed (loss: 0.031835976988077164, acc: 0.9922839403152466)
[2025-02-13 02:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:37][root][INFO] - Training Epoch: 1/2, step 1253/7134 completed (loss: 0.08359222114086151, acc: 0.9768339991569519)
[2025-02-13 02:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:37][root][INFO] - Training Epoch: 1/2, step 1254/7134 completed (loss: 0.0917869433760643, acc: 0.9666666388511658)
[2025-02-13 02:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:38][root][INFO] - Training Epoch: 1/2, step 1255/7134 completed (loss: 0.0695122480392456, acc: 0.9868804812431335)
[2025-02-13 02:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:38][root][INFO] - Training Epoch: 1/2, step 1256/7134 completed (loss: 0.053160522133111954, acc: 0.9855305552482605)
[2025-02-13 02:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39][root][INFO] - Training Epoch: 1/2, step 1257/7134 completed (loss: 0.04697246849536896, acc: 0.9886914491653442)
[2025-02-13 02:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39][root][INFO] - Training Epoch: 1/2, step 1258/7134 completed (loss: 0.07723454385995865, acc: 0.9780380725860596)
[2025-02-13 02:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39][root][INFO] - Training Epoch: 1/2, step 1259/7134 completed (loss: 0.08130557090044022, acc: 0.9871244430541992)
[2025-02-13 02:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:40][root][INFO] - Training Epoch: 1/2, step 1260/7134 completed (loss: 0.04270213842391968, acc: 0.9841040372848511)
[2025-02-13 02:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:40][root][INFO] - Training Epoch: 1/2, step 1261/7134 completed (loss: 0.06082965433597565, acc: 0.9777448177337646)
[2025-02-13 02:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41][root][INFO] - Training Epoch: 1/2, step 1262/7134 completed (loss: 0.017784520983695984, acc: 0.9926739931106567)
[2025-02-13 02:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41][root][INFO] - Training Epoch: 1/2, step 1263/7134 completed (loss: 0.037573594599962234, acc: 0.9900000095367432)
[2025-02-13 02:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:42][root][INFO] - Training Epoch: 1/2, step 1264/7134 completed (loss: 0.0407765693962574, acc: 0.9890965819358826)
[2025-02-13 02:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:42][root][INFO] - Training Epoch: 1/2, step 1265/7134 completed (loss: 0.04758969321846962, acc: 0.9887217879295349)
[2025-02-13 02:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43][root][INFO] - Training Epoch: 1/2, step 1266/7134 completed (loss: 0.0448540635406971, acc: 0.9845094680786133)
[2025-02-13 02:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43][root][INFO] - Training Epoch: 1/2, step 1267/7134 completed (loss: 0.03919706493616104, acc: 0.9889655113220215)
[2025-02-13 02:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:44][root][INFO] - Training Epoch: 1/2, step 1268/7134 completed (loss: 0.053073253482580185, acc: 0.9837518334388733)
[2025-02-13 02:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:44][root][INFO] - Training Epoch: 1/2, step 1269/7134 completed (loss: 0.08982113748788834, acc: 0.9724137783050537)
[2025-02-13 02:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:45][root][INFO] - Training Epoch: 1/2, step 1270/7134 completed (loss: 0.04585376754403114, acc: 0.9860529899597168)
[2025-02-13 02:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:45][root][INFO] - Training Epoch: 1/2, step 1271/7134 completed (loss: 0.08359400182962418, acc: 0.9833333492279053)
[2025-02-13 02:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:45][root][INFO] - Training Epoch: 1/2, step 1272/7134 completed (loss: 0.03188489377498627, acc: 0.9894319772720337)
[2025-02-13 02:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46][root][INFO] - Training Epoch: 1/2, step 1273/7134 completed (loss: 0.035019341856241226, acc: 0.9903225898742676)
[2025-02-13 02:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46][root][INFO] - Training Epoch: 1/2, step 1274/7134 completed (loss: 0.04038074240088463, acc: 0.9898648858070374)
[2025-02-13 02:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:47][root][INFO] - Training Epoch: 1/2, step 1275/7134 completed (loss: 0.05608737841248512, acc: 0.9766606688499451)
[2025-02-13 02:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:47][root][INFO] - Training Epoch: 1/2, step 1276/7134 completed (loss: 0.025781406089663506, acc: 0.9919354915618896)
[2025-02-13 02:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:48][root][INFO] - Training Epoch: 1/2, step 1277/7134 completed (loss: 0.03535129129886627, acc: 0.9903069734573364)
[2025-02-13 02:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:48][root][INFO] - Training Epoch: 1/2, step 1278/7134 completed (loss: 0.05660262703895569, acc: 0.9860031008720398)
[2025-02-13 02:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49][root][INFO] - Training Epoch: 1/2, step 1279/7134 completed (loss: 0.043931424617767334, acc: 0.9888337254524231)
[2025-02-13 02:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49][root][INFO] - Training Epoch: 1/2, step 1280/7134 completed (loss: 0.06559525430202484, acc: 0.9855453372001648)
[2025-02-13 02:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:50][root][INFO] - Training Epoch: 1/2, step 1281/7134 completed (loss: 0.05827060341835022, acc: 0.9845094680786133)
[2025-02-13 02:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:50][root][INFO] - Training Epoch: 1/2, step 1282/7134 completed (loss: 0.10989508032798767, acc: 0.9759358167648315)
[2025-02-13 02:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:51][root][INFO] - Training Epoch: 1/2, step 1283/7134 completed (loss: 0.10060884058475494, acc: 0.9823943376541138)
[2025-02-13 02:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:51][root][INFO] - Training Epoch: 1/2, step 1284/7134 completed (loss: 0.07284058630466461, acc: 0.9809264540672302)
[2025-02-13 02:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:51][root][INFO] - Training Epoch: 1/2, step 1285/7134 completed (loss: 0.08135312795639038, acc: 0.98124098777771)
[2025-02-13 02:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52][root][INFO] - Training Epoch: 1/2, step 1286/7134 completed (loss: 0.05943840369582176, acc: 0.9885550737380981)
[2025-02-13 02:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52][root][INFO] - Training Epoch: 1/2, step 1287/7134 completed (loss: 0.07036618888378143, acc: 0.9784052968025208)
[2025-02-13 02:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:53][root][INFO] - Training Epoch: 1/2, step 1288/7134 completed (loss: 0.06283124536275864, acc: 0.9822335243225098)
[2025-02-13 02:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:53][root][INFO] - Training Epoch: 1/2, step 1289/7134 completed (loss: 0.058633193373680115, acc: 0.9849246144294739)
[2025-02-13 02:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:54][root][INFO] - Training Epoch: 1/2, step 1290/7134 completed (loss: 0.06186769902706146, acc: 0.98591548204422)
[2025-02-13 02:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:54][root][INFO] - Training Epoch: 1/2, step 1291/7134 completed (loss: 0.043361470103263855, acc: 0.991631805896759)
[2025-02-13 02:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55][root][INFO] - Training Epoch: 1/2, step 1292/7134 completed (loss: 0.044840727001428604, acc: 0.9935064911842346)
[2025-02-13 02:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55][root][INFO] - Training Epoch: 1/2, step 1293/7134 completed (loss: 0.031048089265823364, acc: 0.99068683385849)
[2025-02-13 02:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:56][root][INFO] - Training Epoch: 1/2, step 1294/7134 completed (loss: 0.050075337290763855, acc: 0.9908972978591919)
[2025-02-13 02:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:56][root][INFO] - Training Epoch: 1/2, step 1295/7134 completed (loss: 0.04249114543199539, acc: 0.9898132681846619)
[2025-02-13 02:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:57][root][INFO] - Training Epoch: 1/2, step 1296/7134 completed (loss: 0.025542020797729492, acc: 0.9933628439903259)
[2025-02-13 02:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:57][root][INFO] - Training Epoch: 1/2, step 1297/7134 completed (loss: 0.05337017402052879, acc: 0.987525999546051)
[2025-02-13 02:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:58][root][INFO] - Training Epoch: 1/2, step 1298/7134 completed (loss: 0.09571173787117004, acc: 0.9766187071800232)
[2025-02-13 02:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:58][root][INFO] - Training Epoch: 1/2, step 1299/7134 completed (loss: 0.033133383840322495, acc: 0.9973958134651184)
[2025-02-13 02:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59][root][INFO] - Training Epoch: 1/2, step 1300/7134 completed (loss: 0.0342707484960556, acc: 0.9902557730674744)
[2025-02-13 02:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59][root][INFO] - Training Epoch: 1/2, step 1301/7134 completed (loss: 0.01993006467819214, acc: 0.9955489635467529)
[2025-02-13 02:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59][root][INFO] - Training Epoch: 1/2, step 1302/7134 completed (loss: 0.05105209723114967, acc: 0.9866071343421936)
[2025-02-13 02:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:00][root][INFO] - Training Epoch: 1/2, step 1303/7134 completed (loss: 0.07024139910936356, acc: 0.9855769276618958)
[2025-02-13 02:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:00][root][INFO] - Training Epoch: 1/2, step 1304/7134 completed (loss: 0.09145311266183853, acc: 0.9841897487640381)
[2025-02-13 02:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:01][root][INFO] - Training Epoch: 1/2, step 1305/7134 completed (loss: 0.07520820200443268, acc: 0.9847715497016907)
[2025-02-13 02:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:01][root][INFO] - Training Epoch: 1/2, step 1306/7134 completed (loss: 0.08700864017009735, acc: 0.9817906022071838)
[2025-02-13 02:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:02][root][INFO] - Training Epoch: 1/2, step 1307/7134 completed (loss: 0.0519896037876606, acc: 0.9857819676399231)
[2025-02-13 02:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:02][root][INFO] - Training Epoch: 1/2, step 1308/7134 completed (loss: 0.04183381050825119, acc: 0.9849624037742615)
[2025-02-13 02:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03][root][INFO] - Training Epoch: 1/2, step 1309/7134 completed (loss: 0.05360453575849533, acc: 0.9868420958518982)
[2025-02-13 02:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03][root][INFO] - Training Epoch: 1/2, step 1310/7134 completed (loss: 0.047022588551044464, acc: 0.9850339889526367)
[2025-02-13 02:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:04][root][INFO] - Training Epoch: 1/2, step 1311/7134 completed (loss: 0.055615413933992386, acc: 0.9909502267837524)
[2025-02-13 02:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:04][root][INFO] - Training Epoch: 1/2, step 1312/7134 completed (loss: 0.07973427325487137, acc: 0.9824798107147217)
[2025-02-13 02:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:05][root][INFO] - Training Epoch: 1/2, step 1313/7134 completed (loss: 0.08391477167606354, acc: 0.977337121963501)
[2025-02-13 02:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:05][root][INFO] - Training Epoch: 1/2, step 1314/7134 completed (loss: 0.03429567441344261, acc: 0.9877192974090576)
[2025-02-13 02:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06][root][INFO] - Training Epoch: 1/2, step 1315/7134 completed (loss: 0.1563749760389328, acc: 0.9599999785423279)
[2025-02-13 02:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06][root][INFO] - Training Epoch: 1/2, step 1316/7134 completed (loss: 0.03870205953717232, acc: 0.9868173003196716)
[2025-02-13 02:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:07][root][INFO] - Training Epoch: 1/2, step 1317/7134 completed (loss: 0.14087514579296112, acc: 0.9552845358848572)
[2025-02-13 02:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:07][root][INFO] - Training Epoch: 1/2, step 1318/7134 completed (loss: 0.10402766615152359, acc: 0.9782016277313232)
[2025-02-13 02:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:07][root][INFO] - Training Epoch: 1/2, step 1319/7134 completed (loss: 0.09371000528335571, acc: 0.9771167039871216)
[2025-02-13 02:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:08][root][INFO] - Training Epoch: 1/2, step 1320/7134 completed (loss: 0.14226286113262177, acc: 0.9690553545951843)
[2025-02-13 02:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09][root][INFO] - Training Epoch: 1/2, step 1321/7134 completed (loss: 0.04450136050581932, acc: 0.9877451062202454)
[2025-02-13 02:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09][root][INFO] - Training Epoch: 1/2, step 1322/7134 completed (loss: 0.05849568545818329, acc: 0.9836333990097046)
[2025-02-13 02:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:10][root][INFO] - Training Epoch: 1/2, step 1323/7134 completed (loss: 0.07981134206056595, acc: 0.9732142686843872)
[2025-02-13 02:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:10][root][INFO] - Training Epoch: 1/2, step 1324/7134 completed (loss: 0.038235630840063095, acc: 0.9917762875556946)
[2025-02-13 02:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:10][root][INFO] - Training Epoch: 1/2, step 1325/7134 completed (loss: 0.06651100516319275, acc: 0.9841269850730896)
[2025-02-13 02:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11][root][INFO] - Training Epoch: 1/2, step 1326/7134 completed (loss: 0.087407685816288, acc: 0.9802414774894714)
[2025-02-13 02:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11][root][INFO] - Training Epoch: 1/2, step 1327/7134 completed (loss: 0.0765274316072464, acc: 0.9790105223655701)
[2025-02-13 02:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:12][root][INFO] - Training Epoch: 1/2, step 1328/7134 completed (loss: 0.06557346880435944, acc: 0.9767441749572754)
[2025-02-13 02:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:12][root][INFO] - Training Epoch: 1/2, step 1329/7134 completed (loss: 0.08595310896635056, acc: 0.9751381278038025)
[2025-02-13 02:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:13][root][INFO] - Training Epoch: 1/2, step 1330/7134 completed (loss: 0.046906787902116776, acc: 0.9862778782844543)
[2025-02-13 02:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:13][root][INFO] - Training Epoch: 1/2, step 1331/7134 completed (loss: 0.015253380872309208, acc: 0.9967426657676697)
[2025-02-13 02:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:14][root][INFO] - Training Epoch: 1/2, step 1332/7134 completed (loss: 0.026165438815951347, acc: 0.9912663698196411)
[2025-02-13 02:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:14][root][INFO] - Training Epoch: 1/2, step 1333/7134 completed (loss: 0.060023218393325806, acc: 0.9795918464660645)
[2025-02-13 02:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15][root][INFO] - Training Epoch: 1/2, step 1334/7134 completed (loss: 0.10750335454940796, acc: 0.9720497131347656)
[2025-02-13 02:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15][root][INFO] - Training Epoch: 1/2, step 1335/7134 completed (loss: 0.11813923716545105, acc: 0.9760319590568542)
[2025-02-13 02:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:16][root][INFO] - Training Epoch: 1/2, step 1336/7134 completed (loss: 0.036760564893484116, acc: 0.9891745448112488)
[2025-02-13 02:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:16][root][INFO] - Training Epoch: 1/2, step 1337/7134 completed (loss: 0.03122769668698311, acc: 0.9923312664031982)
[2025-02-13 02:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:16][root][INFO] - Training Epoch: 1/2, step 1338/7134 completed (loss: 0.0815453976392746, acc: 0.9759036302566528)
[2025-02-13 02:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:17][root][INFO] - Training Epoch: 1/2, step 1339/7134 completed (loss: 0.08660787343978882, acc: 0.9803370833396912)
[2025-02-13 02:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:17][root][INFO] - Training Epoch: 1/2, step 1340/7134 completed (loss: 0.08180267363786697, acc: 0.9761549830436707)
[2025-02-13 02:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18][root][INFO] - Training Epoch: 1/2, step 1341/7134 completed (loss: 0.08330550789833069, acc: 0.9858757257461548)
[2025-02-13 02:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18][root][INFO] - Training Epoch: 1/2, step 1342/7134 completed (loss: 0.030980737879872322, acc: 0.9919484853744507)
[2025-02-13 02:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:19][root][INFO] - Training Epoch: 1/2, step 1343/7134 completed (loss: 0.08455005288124084, acc: 0.9767441749572754)
[2025-02-13 02:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:19][root][INFO] - Training Epoch: 1/2, step 1344/7134 completed (loss: 0.04215135797858238, acc: 0.9886877536773682)
[2025-02-13 02:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:20][root][INFO] - Training Epoch: 1/2, step 1345/7134 completed (loss: 0.048629529774188995, acc: 0.9840849041938782)
[2025-02-13 02:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:20][root][INFO] - Training Epoch: 1/2, step 1346/7134 completed (loss: 0.06702734529972076, acc: 0.9893190860748291)
[2025-02-13 02:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:21][root][INFO] - Training Epoch: 1/2, step 1347/7134 completed (loss: 0.09791586548089981, acc: 0.9721254110336304)
[2025-02-13 02:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:21][root][INFO] - Training Epoch: 1/2, step 1348/7134 completed (loss: 0.03479347378015518, acc: 0.9908257126808167)
[2025-02-13 02:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:22][root][INFO] - Training Epoch: 1/2, step 1349/7134 completed (loss: 0.05289497226476669, acc: 0.988034188747406)
[2025-02-13 02:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:22][root][INFO] - Training Epoch: 1/2, step 1350/7134 completed (loss: 0.045686546713113785, acc: 0.9913169145584106)
[2025-02-13 02:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:22][root][INFO] - Training Epoch: 1/2, step 1351/7134 completed (loss: 0.02320851944386959, acc: 0.9921414256095886)
[2025-02-13 02:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23][root][INFO] - Training Epoch: 1/2, step 1352/7134 completed (loss: 0.03474215418100357, acc: 0.992559552192688)
[2025-02-13 02:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23][root][INFO] - Training Epoch: 1/2, step 1353/7134 completed (loss: 0.057432252913713455, acc: 0.984674334526062)
[2025-02-13 02:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:24][root][INFO] - Training Epoch: 1/2, step 1354/7134 completed (loss: 0.024134673178195953, acc: 0.990275502204895)
[2025-02-13 02:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:24][root][INFO] - Training Epoch: 1/2, step 1355/7134 completed (loss: 0.027765609323978424, acc: 0.9937694668769836)
[2025-02-13 02:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:25][root][INFO] - Training Epoch: 1/2, step 1356/7134 completed (loss: 0.04187373071908951, acc: 0.9868420958518982)
[2025-02-13 02:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:25][root][INFO] - Training Epoch: 1/2, step 1357/7134 completed (loss: 0.015092074871063232, acc: 0.994955837726593)
[2025-02-13 02:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:26][root][INFO] - Training Epoch: 1/2, step 1358/7134 completed (loss: 0.042452484369277954, acc: 0.9896103739738464)
[2025-02-13 02:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:26][root][INFO] - Training Epoch: 1/2, step 1359/7134 completed (loss: 0.031255919486284256, acc: 0.9919028282165527)
[2025-02-13 02:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27][root][INFO] - Training Epoch: 1/2, step 1360/7134 completed (loss: 0.02632788009941578, acc: 0.9905914068222046)
[2025-02-13 02:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27][root][INFO] - Training Epoch: 1/2, step 1361/7134 completed (loss: 0.035568635910749435, acc: 0.9941605925559998)
[2025-02-13 02:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:28][root][INFO] - Training Epoch: 1/2, step 1362/7134 completed (loss: 0.02825586311519146, acc: 0.9890710115432739)
[2025-02-13 02:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:28][root][INFO] - Training Epoch: 1/2, step 1363/7134 completed (loss: 0.025947218760848045, acc: 0.9919571280479431)
[2025-02-13 02:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:29][root][INFO] - Training Epoch: 1/2, step 1364/7134 completed (loss: 0.015050778165459633, acc: 0.9971428513526917)
[2025-02-13 02:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:29][root][INFO] - Training Epoch: 1/2, step 1365/7134 completed (loss: 0.014346244744956493, acc: 0.9981684684753418)
[2025-02-13 02:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30][root][INFO] - Training Epoch: 1/2, step 1366/7134 completed (loss: 0.010291720740497112, acc: 0.9964285492897034)
[2025-02-13 02:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30][root][INFO] - Training Epoch: 1/2, step 1367/7134 completed (loss: 0.012507128529250622, acc: 0.998275876045227)
[2025-02-13 02:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30][root][INFO] - Training Epoch: 1/2, step 1368/7134 completed (loss: 0.04579968750476837, acc: 0.9847561120986938)
[2025-02-13 02:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:31][root][INFO] - Training Epoch: 1/2, step 1369/7134 completed (loss: 0.05007719621062279, acc: 0.985989511013031)
[2025-02-13 02:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:31][root][INFO] - Training Epoch: 1/2, step 1370/7134 completed (loss: 0.049227647483348846, acc: 0.9898862242698669)
[2025-02-13 02:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:32][root][INFO] - Training Epoch: 1/2, step 1371/7134 completed (loss: 0.02597137540578842, acc: 0.9941775798797607)
[2025-02-13 02:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:32][root][INFO] - Training Epoch: 1/2, step 1372/7134 completed (loss: 0.08956806361675262, acc: 0.9775280952453613)
[2025-02-13 02:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:33][root][INFO] - Training Epoch: 1/2, step 1373/7134 completed (loss: 0.03096819669008255, acc: 0.9862204790115356)
[2025-02-13 02:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:33][root][INFO] - Training Epoch: 1/2, step 1374/7134 completed (loss: 0.03593950346112251, acc: 0.9881188273429871)
[2025-02-13 02:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:34][root][INFO] - Training Epoch: 1/2, step 1375/7134 completed (loss: 0.055570151656866074, acc: 0.9855769276618958)
[2025-02-13 02:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:34][root][INFO] - Training Epoch: 1/2, step 1376/7134 completed (loss: 0.07340764254331589, acc: 0.9728600978851318)
[2025-02-13 02:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35][root][INFO] - Training Epoch: 1/2, step 1377/7134 completed (loss: 0.045984428375959396, acc: 0.9835293889045715)
[2025-02-13 02:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35][root][INFO] - Training Epoch: 1/2, step 1378/7134 completed (loss: 0.07245542109012604, acc: 0.9746588468551636)
[2025-02-13 02:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35][root][INFO] - Training Epoch: 1/2, step 1379/7134 completed (loss: 0.04852720722556114, acc: 0.9806451797485352)
[2025-02-13 02:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:36][root][INFO] - Training Epoch: 1/2, step 1380/7134 completed (loss: 0.08059457689523697, acc: 0.9813084006309509)
[2025-02-13 02:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:36][root][INFO] - Training Epoch: 1/2, step 1381/7134 completed (loss: 0.05547013878822327, acc: 0.9850746393203735)
[2025-02-13 02:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:37][root][INFO] - Training Epoch: 1/2, step 1382/7134 completed (loss: 0.051240622997283936, acc: 0.9825396537780762)
[2025-02-13 02:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:37][root][INFO] - Training Epoch: 1/2, step 1383/7134 completed (loss: 0.03395651653409004, acc: 0.9931153059005737)
[2025-02-13 02:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38][root][INFO] - Training Epoch: 1/2, step 1384/7134 completed (loss: 0.09865237772464752, acc: 0.9683859944343567)
[2025-02-13 02:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38][root][INFO] - Training Epoch: 1/2, step 1385/7134 completed (loss: 0.08056996017694473, acc: 0.9798164963722229)
[2025-02-13 02:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:39][root][INFO] - Training Epoch: 1/2, step 1386/7134 completed (loss: 0.047018032521009445, acc: 0.98525071144104)
[2025-02-13 02:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:39][root][INFO] - Training Epoch: 1/2, step 1387/7134 completed (loss: 0.07327666133642197, acc: 0.9799426794052124)
[2025-02-13 02:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:40][root][INFO] - Training Epoch: 1/2, step 1388/7134 completed (loss: 0.061014555394649506, acc: 0.9778534770011902)
[2025-02-13 02:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:40][root][INFO] - Training Epoch: 1/2, step 1389/7134 completed (loss: 0.06506530940532684, acc: 0.9805510640144348)
[2025-02-13 02:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:41][root][INFO] - Training Epoch: 1/2, step 1390/7134 completed (loss: 0.05121404677629471, acc: 0.9875195026397705)
[2025-02-13 02:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:41][root][INFO] - Training Epoch: 1/2, step 1391/7134 completed (loss: 0.09273001551628113, acc: 0.9806678295135498)
[2025-02-13 02:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:41][root][INFO] - Training Epoch: 1/2, step 1392/7134 completed (loss: 0.0509381927549839, acc: 0.9861591458320618)
[2025-02-13 02:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42][root][INFO] - Training Epoch: 1/2, step 1393/7134 completed (loss: 0.06270159780979156, acc: 0.981566846370697)
[2025-02-13 02:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42][root][INFO] - Training Epoch: 1/2, step 1394/7134 completed (loss: 0.04374750703573227, acc: 0.9900793433189392)
[2025-02-13 02:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:43][root][INFO] - Training Epoch: 1/2, step 1395/7134 completed (loss: 0.07226080447435379, acc: 0.9822485446929932)
[2025-02-13 02:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:43][root][INFO] - Training Epoch: 1/2, step 1396/7134 completed (loss: 0.08690184354782104, acc: 0.9679054021835327)
[2025-02-13 02:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44][root][INFO] - Training Epoch: 1/2, step 1397/7134 completed (loss: 0.07154993712902069, acc: 0.9780775904655457)
[2025-02-13 02:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44][root][INFO] - Training Epoch: 1/2, step 1398/7134 completed (loss: 0.049794167280197144, acc: 0.9858012199401855)
[2025-02-13 02:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44][root][INFO] - Training Epoch: 1/2, step 1399/7134 completed (loss: 0.0984332412481308, acc: 0.9764492511749268)
[2025-02-13 02:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45][root][INFO] - Training Epoch: 1/2, step 1400/7134 completed (loss: 0.03943585604429245, acc: 0.9930192232131958)
[2025-02-13 02:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45][root][INFO] - Training Epoch: 1/2, step 1401/7134 completed (loss: 0.02948843315243721, acc: 0.9901153445243835)
[2025-02-13 02:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:46][root][INFO] - Training Epoch: 1/2, step 1402/7134 completed (loss: 0.030443402007222176, acc: 0.9913644194602966)
[2025-02-13 02:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:46][root][INFO] - Training Epoch: 1/2, step 1403/7134 completed (loss: 0.045283664017915726, acc: 0.9839357137680054)
[2025-02-13 02:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:47][root][INFO] - Training Epoch: 1/2, step 1404/7134 completed (loss: 0.027763238176703453, acc: 0.9907161593437195)
[2025-02-13 02:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:47][root][INFO] - Training Epoch: 1/2, step 1405/7134 completed (loss: 0.05233636498451233, acc: 0.9857549667358398)
[2025-02-13 02:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48][root][INFO] - Training Epoch: 1/2, step 1406/7134 completed (loss: 0.016393423080444336, acc: 0.9942528605461121)
[2025-02-13 02:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48][root][INFO] - Training Epoch: 1/2, step 1407/7134 completed (loss: 0.02526862360537052, acc: 0.9934810996055603)
[2025-02-13 02:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:49][root][INFO] - Training Epoch: 1/2, step 1408/7134 completed (loss: 0.03212567791342735, acc: 0.9871794581413269)
[2025-02-13 02:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:49][root][INFO] - Training Epoch: 1/2, step 1409/7134 completed (loss: 0.023174503818154335, acc: 0.992548406124115)
[2025-02-13 02:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:50][root][INFO] - Training Epoch: 1/2, step 1410/7134 completed (loss: 0.04010293260216713, acc: 0.9908854365348816)
[2025-02-13 02:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:50][root][INFO] - Training Epoch: 1/2, step 1411/7134 completed (loss: 0.013762415386736393, acc: 0.9960106611251831)
[2025-02-13 02:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51][root][INFO] - Training Epoch: 1/2, step 1412/7134 completed (loss: 0.021152634173631668, acc: 0.9976246953010559)
[2025-02-13 02:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51][root][INFO] - Training Epoch: 1/2, step 1413/7134 completed (loss: 0.018363971263170242, acc: 0.9942445755004883)
[2025-02-13 02:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:52][root][INFO] - Training Epoch: 1/2, step 1414/7134 completed (loss: 0.03207971528172493, acc: 0.9953271150588989)
[2025-02-13 02:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:52][root][INFO] - Training Epoch: 1/2, step 1415/7134 completed (loss: 0.040715526789426804, acc: 0.9884105920791626)
[2025-02-13 02:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53][root][INFO] - Training Epoch: 1/2, step 1416/7134 completed (loss: 0.020099280402064323, acc: 0.9957864880561829)
[2025-02-13 02:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53][root][INFO] - Training Epoch: 1/2, step 1417/7134 completed (loss: 0.01876698061823845, acc: 0.9931972622871399)
[2025-02-13 02:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53][root][INFO] - Training Epoch: 1/2, step 1418/7134 completed (loss: 0.012654044665396214, acc: 0.9980915784835815)
[2025-02-13 02:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:54][root][INFO] - Training Epoch: 1/2, step 1419/7134 completed (loss: 0.019505929201841354, acc: 0.997300922870636)
[2025-02-13 02:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:54][root][INFO] - Training Epoch: 1/2, step 1420/7134 completed (loss: 0.0174039825797081, acc: 0.9946091771125793)
[2025-02-13 02:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55][root][INFO] - Training Epoch: 1/2, step 1421/7134 completed (loss: 0.032923728227615356, acc: 0.9855769276618958)
[2025-02-13 02:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55][root][INFO] - Training Epoch: 1/2, step 1422/7134 completed (loss: 0.07458551228046417, acc: 0.9800570011138916)
[2025-02-13 02:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:56][root][INFO] - Training Epoch: 1/2, step 1423/7134 completed (loss: 0.046165045350790024, acc: 0.9897959232330322)
[2025-02-13 02:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:56][root][INFO] - Training Epoch: 1/2, step 1424/7134 completed (loss: 0.07368440181016922, acc: 0.980988621711731)
[2025-02-13 02:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:57][root][INFO] - Training Epoch: 1/2, step 1425/7134 completed (loss: 0.054323602467775345, acc: 0.9821162223815918)
[2025-02-13 02:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:57][root][INFO] - Training Epoch: 1/2, step 1426/7134 completed (loss: 0.05523863062262535, acc: 0.9794344305992126)
[2025-02-13 02:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58][root][INFO] - Training Epoch: 1/2, step 1427/7134 completed (loss: 0.06444841623306274, acc: 0.977011501789093)
[2025-02-13 02:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58][root][INFO] - Training Epoch: 1/2, step 1428/7134 completed (loss: 0.28497573733329773, acc: 0.9244713187217712)
[2025-02-13 02:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:59][root][INFO] - Training Epoch: 1/2, step 1429/7134 completed (loss: 0.04419988393783569, acc: 0.9896050095558167)
[2025-02-13 02:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:59][root][INFO] - Training Epoch: 1/2, step 1430/7134 completed (loss: 0.08454806357622147, acc: 0.9815497994422913)
[2025-02-13 02:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00][root][INFO] - Training Epoch: 1/2, step 1431/7134 completed (loss: 0.034143224358558655, acc: 0.9891451597213745)
[2025-02-13 02:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00][root][INFO] - Training Epoch: 1/2, step 1432/7134 completed (loss: 0.021612301468849182, acc: 0.993565022945404)
[2025-02-13 02:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:01][root][INFO] - Training Epoch: 1/2, step 1433/7134 completed (loss: 0.05115973949432373, acc: 0.9842932224273682)
[2025-02-13 02:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:01][root][INFO] - Training Epoch: 1/2, step 1434/7134 completed (loss: 0.07675567269325256, acc: 0.9785810112953186)
[2025-02-13 02:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:01][root][INFO] - Training Epoch: 1/2, step 1435/7134 completed (loss: 0.06619768589735031, acc: 0.9794721603393555)
[2025-02-13 02:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:02][root][INFO] - Training Epoch: 1/2, step 1436/7134 completed (loss: 0.09846441447734833, acc: 0.970812201499939)
[2025-02-13 02:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:02][root][INFO] - Training Epoch: 1/2, step 1437/7134 completed (loss: 0.04758784919977188, acc: 0.9874652028083801)
[2025-02-13 02:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03][root][INFO] - Training Epoch: 1/2, step 1438/7134 completed (loss: 0.06551860272884369, acc: 0.9839416146278381)
[2025-02-13 02:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03][root][INFO] - Training Epoch: 1/2, step 1439/7134 completed (loss: 0.054665740579366684, acc: 0.9851577281951904)
[2025-02-13 02:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:04][root][INFO] - Training Epoch: 1/2, step 1440/7134 completed (loss: 0.09071007370948792, acc: 0.9724612832069397)
[2025-02-13 02:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:04][root][INFO] - Training Epoch: 1/2, step 1441/7134 completed (loss: 0.05978457257151604, acc: 0.9794721603393555)
[2025-02-13 02:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:05][root][INFO] - Training Epoch: 1/2, step 1442/7134 completed (loss: 0.04752122610807419, acc: 0.9832535982131958)
[2025-02-13 02:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:05][root][INFO] - Training Epoch: 1/2, step 1443/7134 completed (loss: 0.08495337516069412, acc: 0.9806678295135498)
[2025-02-13 02:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06][root][INFO] - Training Epoch: 1/2, step 1444/7134 completed (loss: 0.054889194667339325, acc: 0.9844357967376709)
[2025-02-13 02:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06][root][INFO] - Training Epoch: 1/2, step 1445/7134 completed (loss: 0.056130412966012955, acc: 0.981697142124176)
[2025-02-13 02:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:07][root][INFO] - Training Epoch: 1/2, step 1446/7134 completed (loss: 0.02999512478709221, acc: 0.991465151309967)
[2025-02-13 02:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:07][root][INFO] - Training Epoch: 1/2, step 1447/7134 completed (loss: 0.058865416795015335, acc: 0.9773049354553223)
[2025-02-13 02:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:07][root][INFO] - Training Epoch: 1/2, step 1448/7134 completed (loss: 0.08565972745418549, acc: 0.9745762944221497)
[2025-02-13 02:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:08][root][INFO] - Training Epoch: 1/2, step 1449/7134 completed (loss: 0.10544396191835403, acc: 0.983146071434021)
[2025-02-13 02:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:08][root][INFO] - Training Epoch: 1/2, step 1450/7134 completed (loss: 0.03408113494515419, acc: 0.9927431344985962)
[2025-02-13 02:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:09][root][INFO] - Training Epoch: 1/2, step 1451/7134 completed (loss: 0.037571683526039124, acc: 0.985358715057373)
[2025-02-13 02:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:09][root][INFO] - Training Epoch: 1/2, step 1452/7134 completed (loss: 0.07537122070789337, acc: 0.9756944179534912)
[2025-02-13 02:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:10][root][INFO] - Training Epoch: 1/2, step 1453/7134 completed (loss: 0.0901770070195198, acc: 0.9730586409568787)
[2025-02-13 02:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:10][root][INFO] - Training Epoch: 1/2, step 1454/7134 completed (loss: 0.09616058319807053, acc: 0.9730821251869202)
[2025-02-13 02:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11][root][INFO] - Training Epoch: 1/2, step 1455/7134 completed (loss: 0.08072961121797562, acc: 0.9816513657569885)
[2025-02-13 02:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11][root][INFO] - Training Epoch: 1/2, step 1456/7134 completed (loss: 0.03326895833015442, acc: 0.9934498071670532)
[2025-02-13 02:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11][root][INFO] - Training Epoch: 1/2, step 1457/7134 completed (loss: 0.037359826266765594, acc: 0.9891473054885864)
[2025-02-13 02:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:12][root][INFO] - Training Epoch: 1/2, step 1458/7134 completed (loss: 0.016229072585701942, acc: 0.9941089749336243)
[2025-02-13 02:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:12][root][INFO] - Training Epoch: 1/2, step 1459/7134 completed (loss: 0.05360141023993492, acc: 0.9857904314994812)
[2025-02-13 02:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:13][root][INFO] - Training Epoch: 1/2, step 1460/7134 completed (loss: 0.08471184968948364, acc: 0.9723684191703796)
[2025-02-13 02:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:13][root][INFO] - Training Epoch: 1/2, step 1461/7134 completed (loss: 0.04097963497042656, acc: 0.9840510487556458)
[2025-02-13 02:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14][root][INFO] - Training Epoch: 1/2, step 1462/7134 completed (loss: 0.0813555046916008, acc: 0.9742063283920288)
[2025-02-13 02:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14][root][INFO] - Training Epoch: 1/2, step 1463/7134 completed (loss: 0.07219760119915009, acc: 0.9762901067733765)
[2025-02-13 02:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:15][root][INFO] - Training Epoch: 1/2, step 1464/7134 completed (loss: 0.10920708626508713, acc: 0.9699699878692627)
[2025-02-13 02:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:15][root][INFO] - Training Epoch: 1/2, step 1465/7134 completed (loss: 0.05907777324318886, acc: 0.9890282154083252)
[2025-02-13 02:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:16][root][INFO] - Training Epoch: 1/2, step 1466/7134 completed (loss: 0.023869574069976807, acc: 0.9971671104431152)
[2025-02-13 02:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:16][root][INFO] - Training Epoch: 1/2, step 1467/7134 completed (loss: 0.023462902754545212, acc: 0.9927954077720642)
[2025-02-13 02:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:16][root][INFO] - Training Epoch: 1/2, step 1468/7134 completed (loss: 0.03698142245411873, acc: 0.9894319772720337)
[2025-02-13 02:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:17][root][INFO] - Training Epoch: 1/2, step 1469/7134 completed (loss: 0.03213771432638168, acc: 0.9946164488792419)
[2025-02-13 02:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:17][root][INFO] - Training Epoch: 1/2, step 1470/7134 completed (loss: 0.06340528279542923, acc: 0.9812138676643372)
[2025-02-13 02:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:18][root][INFO] - Training Epoch: 1/2, step 1471/7134 completed (loss: 0.05809365212917328, acc: 0.9840142130851746)
[2025-02-13 02:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:18][root][INFO] - Training Epoch: 1/2, step 1472/7134 completed (loss: 0.03879223391413689, acc: 0.9899371266365051)
[2025-02-13 02:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:19][root][INFO] - Training Epoch: 1/2, step 1473/7134 completed (loss: 0.05461614578962326, acc: 0.9851632118225098)
[2025-02-13 02:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:19][root][INFO] - Training Epoch: 1/2, step 1474/7134 completed (loss: 0.06160977855324745, acc: 0.9815436005592346)
[2025-02-13 02:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:20][root][INFO] - Training Epoch: 1/2, step 1475/7134 completed (loss: 0.08369553834199905, acc: 0.9815303683280945)
[2025-02-13 02:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:20][root][INFO] - Training Epoch: 1/2, step 1476/7134 completed (loss: 0.036035697907209396, acc: 0.9875776171684265)
[2025-02-13 02:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21][root][INFO] - Training Epoch: 1/2, step 1477/7134 completed (loss: 0.054251909255981445, acc: 0.9825479984283447)
[2025-02-13 02:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21][root][INFO] - Training Epoch: 1/2, step 1478/7134 completed (loss: 0.04238329827785492, acc: 0.9912408590316772)
[2025-02-13 02:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21][root][INFO] - Training Epoch: 1/2, step 1479/7134 completed (loss: 0.03537813946604729, acc: 0.9899425506591797)
[2025-02-13 02:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:22][root][INFO] - Training Epoch: 1/2, step 1480/7134 completed (loss: 0.01564006134867668, acc: 0.9974026083946228)
[2025-02-13 02:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:22][root][INFO] - Training Epoch: 1/2, step 1481/7134 completed (loss: 0.03545154631137848, acc: 0.9947368502616882)
[2025-02-13 02:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:23][root][INFO] - Training Epoch: 1/2, step 1482/7134 completed (loss: 0.05546695366501808, acc: 0.984375)
[2025-02-13 02:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:23][root][INFO] - Training Epoch: 1/2, step 1483/7134 completed (loss: 0.014081447385251522, acc: 0.9952606558799744)
[2025-02-13 02:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24][root][INFO] - Training Epoch: 1/2, step 1484/7134 completed (loss: 0.06867172569036484, acc: 0.9783236980438232)
[2025-02-13 02:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24][root][INFO] - Training Epoch: 1/2, step 1485/7134 completed (loss: 0.03238428756594658, acc: 0.9898862242698669)
[2025-02-13 02:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:25][root][INFO] - Training Epoch: 1/2, step 1486/7134 completed (loss: 0.06301430612802505, acc: 0.9851149916648865)
[2025-02-13 02:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:25][root][INFO] - Training Epoch: 1/2, step 1487/7134 completed (loss: 0.022953450679779053, acc: 0.9934640526771545)
[2025-02-13 02:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:25][root][INFO] - Training Epoch: 1/2, step 1488/7134 completed (loss: 0.054493218660354614, acc: 0.9818181991577148)
[2025-02-13 02:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26][root][INFO] - Training Epoch: 1/2, step 1489/7134 completed (loss: 0.039860617369413376, acc: 0.985981285572052)
[2025-02-13 02:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26][root][INFO] - Training Epoch: 1/2, step 1490/7134 completed (loss: 0.02347930334508419, acc: 0.991534411907196)
[2025-02-13 02:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:27][root][INFO] - Training Epoch: 1/2, step 1491/7134 completed (loss: 0.027925461530685425, acc: 0.9896551966667175)
[2025-02-13 02:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:27][root][INFO] - Training Epoch: 1/2, step 1492/7134 completed (loss: 0.05015437677502632, acc: 0.988399088382721)
[2025-02-13 02:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:28][root][INFO] - Training Epoch: 1/2, step 1493/7134 completed (loss: 0.04069644212722778, acc: 0.9884792566299438)
[2025-02-13 02:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:28][root][INFO] - Training Epoch: 1/2, step 1494/7134 completed (loss: 0.024665873497724533, acc: 0.9914841651916504)
[2025-02-13 02:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29][root][INFO] - Training Epoch: 1/2, step 1495/7134 completed (loss: 0.024211794137954712, acc: 0.9952095746994019)
[2025-02-13 02:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29][root][INFO] - Training Epoch: 1/2, step 1496/7134 completed (loss: 0.048613253980875015, acc: 0.9871794581413269)
[2025-02-13 02:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:30][root][INFO] - Training Epoch: 1/2, step 1497/7134 completed (loss: 0.04707418009638786, acc: 0.9875565767288208)
[2025-02-13 02:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:30][root][INFO] - Training Epoch: 1/2, step 1498/7134 completed (loss: 0.010528450831770897, acc: 0.9973649382591248)
[2025-02-13 02:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:31][root][INFO] - Training Epoch: 1/2, step 1499/7134 completed (loss: 0.029539844021201134, acc: 0.990510106086731)
[2025-02-13 02:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:31][root][INFO] - Training Epoch: 1/2, step 1500/7134 completed (loss: 0.024326661601662636, acc: 0.9917218685150146)
[2025-02-13 02:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:31][root][INFO] - Training Epoch: 1/2, step 1501/7134 completed (loss: 0.04171153903007507, acc: 0.991150438785553)
[2025-02-13 02:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32][root][INFO] - Training Epoch: 1/2, step 1502/7134 completed (loss: 0.03695445880293846, acc: 0.988726019859314)
[2025-02-13 02:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32][root][INFO] - Training Epoch: 1/2, step 1503/7134 completed (loss: 0.0834972932934761, acc: 0.9758522510528564)
[2025-02-13 02:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33][root][INFO] - Training Epoch: 1/2, step 1504/7134 completed (loss: 0.02370314672589302, acc: 0.99245285987854)
[2025-02-13 02:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33][root][INFO] - Training Epoch: 1/2, step 1505/7134 completed (loss: 0.04455866292119026, acc: 0.9844236969947815)
[2025-02-13 02:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:34][root][INFO] - Training Epoch: 1/2, step 1506/7134 completed (loss: 0.06845659017562866, acc: 0.9772036671638489)
[2025-02-13 02:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:34][root][INFO] - Training Epoch: 1/2, step 1507/7134 completed (loss: 0.03362315893173218, acc: 0.9895287752151489)
[2025-02-13 02:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:35][root][INFO] - Training Epoch: 1/2, step 1508/7134 completed (loss: 0.021997783333063126, acc: 0.9934354424476624)
[2025-02-13 02:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:35][root][INFO] - Training Epoch: 1/2, step 1509/7134 completed (loss: 0.027492959052324295, acc: 0.9915966391563416)
[2025-02-13 02:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36][root][INFO] - Training Epoch: 1/2, step 1510/7134 completed (loss: 0.17340682446956635, acc: 0.9733333587646484)
[2025-02-13 02:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36][root][INFO] - Training Epoch: 1/2, step 1511/7134 completed (loss: 0.09155324101448059, acc: 0.9773049354553223)
[2025-02-13 02:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36][root][INFO] - Training Epoch: 1/2, step 1512/7134 completed (loss: 0.026502983644604683, acc: 0.9923896789550781)
[2025-02-13 02:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:37][root][INFO] - Training Epoch: 1/2, step 1513/7134 completed (loss: 0.027476759627461433, acc: 0.9867724776268005)
[2025-02-13 02:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:37][root][INFO] - Training Epoch: 1/2, step 1514/7134 completed (loss: 0.05413106083869934, acc: 0.992546558380127)
[2025-02-13 02:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38][root][INFO] - Training Epoch: 1/2, step 1515/7134 completed (loss: 0.07387973368167877, acc: 0.9789719581604004)
[2025-02-13 02:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38][root][INFO] - Training Epoch: 1/2, step 1516/7134 completed (loss: 0.07125399261713028, acc: 0.9772079586982727)
[2025-02-13 02:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:39][root][INFO] - Training Epoch: 1/2, step 1517/7134 completed (loss: 0.04518881067633629, acc: 0.9852941036224365)
[2025-02-13 02:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:39][root][INFO] - Training Epoch: 1/2, step 1518/7134 completed (loss: 0.04768703877925873, acc: 0.983132541179657)
[2025-02-13 02:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:39][root][INFO] - Training Epoch: 1/2, step 1519/7134 completed (loss: 0.012175858952105045, acc: 1.0)
[2025-02-13 02:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40][root][INFO] - Training Epoch: 1/2, step 1520/7134 completed (loss: 0.032890092581510544, acc: 0.9906396269798279)
[2025-02-13 02:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40][root][INFO] - Training Epoch: 1/2, step 1521/7134 completed (loss: 0.08963096886873245, acc: 0.9776536226272583)
[2025-02-13 02:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:41][root][INFO] - Training Epoch: 1/2, step 1522/7134 completed (loss: 0.09161166101694107, acc: 0.9691714644432068)
[2025-02-13 02:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:41][root][INFO] - Training Epoch: 1/2, step 1523/7134 completed (loss: 0.12983673810958862, acc: 0.9640179872512817)
[2025-02-13 02:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42][root][INFO] - Training Epoch: 1/2, step 1524/7134 completed (loss: 0.04953416809439659, acc: 0.9899328947067261)
[2025-02-13 02:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42][root][INFO] - Training Epoch: 1/2, step 1525/7134 completed (loss: 0.05432818830013275, acc: 0.9864253401756287)
[2025-02-13 02:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43][root][INFO] - Training Epoch: 1/2, step 1526/7134 completed (loss: 0.11618146300315857, acc: 0.9709724187850952)
[2025-02-13 02:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43][root][INFO] - Training Epoch: 1/2, step 1527/7134 completed (loss: 0.11858725547790527, acc: 0.9666110277175903)
[2025-02-13 02:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:44][root][INFO] - Training Epoch: 1/2, step 1528/7134 completed (loss: 0.05658434331417084, acc: 0.9822866320610046)
[2025-02-13 02:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:44][root][INFO] - Training Epoch: 1/2, step 1529/7134 completed (loss: 0.02781006135046482, acc: 0.9892984628677368)
[2025-02-13 02:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:45][root][INFO] - Training Epoch: 1/2, step 1530/7134 completed (loss: 0.0685422271490097, acc: 0.980879545211792)
[2025-02-13 02:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:45][root][INFO] - Training Epoch: 1/2, step 1531/7134 completed (loss: 0.17878450453281403, acc: 0.945035457611084)
[2025-02-13 02:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:46][root][INFO] - Training Epoch: 1/2, step 1532/7134 completed (loss: 0.05453940108418465, acc: 0.9861271381378174)
[2025-02-13 02:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:46][root][INFO] - Training Epoch: 1/2, step 1533/7134 completed (loss: 0.053724274039268494, acc: 0.9861687421798706)
[2025-02-13 02:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47][root][INFO] - Training Epoch: 1/2, step 1534/7134 completed (loss: 0.054624419659376144, acc: 0.9847645163536072)
[2025-02-13 02:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47][root][INFO] - Training Epoch: 1/2, step 1535/7134 completed (loss: 0.04806889221072197, acc: 0.9880239367485046)
[2025-02-13 02:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48][root][INFO] - Training Epoch: 1/2, step 1536/7134 completed (loss: 0.028230493888258934, acc: 0.991150438785553)
[2025-02-13 02:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48][root][INFO] - Training Epoch: 1/2, step 1537/7134 completed (loss: 0.05134056136012077, acc: 0.9869109988212585)
[2025-02-13 02:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48][root][INFO] - Training Epoch: 1/2, step 1538/7134 completed (loss: 0.06292037665843964, acc: 0.9820742607116699)
[2025-02-13 02:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:49][root][INFO] - Training Epoch: 1/2, step 1539/7134 completed (loss: 0.04501786455512047, acc: 0.9855072498321533)
[2025-02-13 02:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:49][root][INFO] - Training Epoch: 1/2, step 1540/7134 completed (loss: 0.04184586554765701, acc: 0.9839141964912415)
[2025-02-13 02:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:50][root][INFO] - Training Epoch: 1/2, step 1541/7134 completed (loss: 0.08505848050117493, acc: 0.9786163568496704)
[2025-02-13 02:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:50][root][INFO] - Training Epoch: 1/2, step 1542/7134 completed (loss: 0.03383074700832367, acc: 0.993261456489563)
[2025-02-13 02:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:51][root][INFO] - Training Epoch: 1/2, step 1543/7134 completed (loss: 0.02756776474416256, acc: 0.994535505771637)
[2025-02-13 02:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:51][root][INFO] - Training Epoch: 1/2, step 1544/7134 completed (loss: 0.03345511853694916, acc: 0.9927184581756592)
[2025-02-13 02:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52][root][INFO] - Training Epoch: 1/2, step 1545/7134 completed (loss: 0.04550451040267944, acc: 0.9856528043746948)
[2025-02-13 02:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52][root][INFO] - Training Epoch: 1/2, step 1546/7134 completed (loss: 0.0977734699845314, acc: 0.9747545719146729)
[2025-02-13 02:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:53][root][INFO] - Training Epoch: 1/2, step 1547/7134 completed (loss: 0.05003662034869194, acc: 0.9746268391609192)
[2025-02-13 02:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:53][root][INFO] - Training Epoch: 1/2, step 1548/7134 completed (loss: 0.03523851931095123, acc: 0.9886234402656555)
[2025-02-13 02:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:54][root][INFO] - Training Epoch: 1/2, step 1549/7134 completed (loss: 0.03316756710410118, acc: 0.9844961166381836)
[2025-02-13 02:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:54][root][INFO] - Training Epoch: 1/2, step 1550/7134 completed (loss: 0.024920303374528885, acc: 0.9911280274391174)
[2025-02-13 02:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55][root][INFO] - Training Epoch: 1/2, step 1551/7134 completed (loss: 0.03701537847518921, acc: 0.984415590763092)
[2025-02-13 02:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55][root][INFO] - Training Epoch: 1/2, step 1552/7134 completed (loss: 0.0418829470872879, acc: 0.9892241358757019)
[2025-02-13 02:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55][root][INFO] - Training Epoch: 1/2, step 1553/7134 completed (loss: 0.04272482916712761, acc: 0.9882965087890625)
[2025-02-13 02:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:56][root][INFO] - Training Epoch: 1/2, step 1554/7134 completed (loss: 0.05468658730387688, acc: 0.9842164516448975)
[2025-02-13 02:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:56][root][INFO] - Training Epoch: 1/2, step 1555/7134 completed (loss: 0.05189213529229164, acc: 0.9876819849014282)
[2025-02-13 02:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57][root][INFO] - Training Epoch: 1/2, step 1556/7134 completed (loss: 0.05298314616084099, acc: 0.9858989715576172)
[2025-02-13 02:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57][root][INFO] - Training Epoch: 1/2, step 1557/7134 completed (loss: 0.020577356219291687, acc: 0.991051435470581)
[2025-02-13 02:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:58][root][INFO] - Training Epoch: 1/2, step 1558/7134 completed (loss: 0.04837657883763313, acc: 0.9817159175872803)
[2025-02-13 02:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:58][root][INFO] - Training Epoch: 1/2, step 1559/7134 completed (loss: 0.024496786296367645, acc: 0.9892904758453369)
[2025-02-13 02:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:59][root][INFO] - Training Epoch: 1/2, step 1560/7134 completed (loss: 0.04568042606115341, acc: 0.9885641932487488)
[2025-02-13 02:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:59][root][INFO] - Training Epoch: 1/2, step 1561/7134 completed (loss: 0.09021326899528503, acc: 0.9750812649726868)
[2025-02-13 02:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00][root][INFO] - Training Epoch: 1/2, step 1562/7134 completed (loss: 0.046027593314647675, acc: 0.9878970980644226)
[2025-02-13 02:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00][root][INFO] - Training Epoch: 1/2, step 1563/7134 completed (loss: 0.06414429843425751, acc: 0.9782857298851013)
[2025-02-13 02:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:01][root][INFO] - Training Epoch: 1/2, step 1564/7134 completed (loss: 0.07635139673948288, acc: 0.9752475023269653)
[2025-02-13 02:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:01][root][INFO] - Training Epoch: 1/2, step 1565/7134 completed (loss: 0.06795848906040192, acc: 0.9798319339752197)
[2025-02-13 02:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02][root][INFO] - Training Epoch: 1/2, step 1566/7134 completed (loss: 0.05818326398730278, acc: 0.9822161197662354)
[2025-02-13 02:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02][root][INFO] - Training Epoch: 1/2, step 1567/7134 completed (loss: 0.053773678839206696, acc: 0.9837905168533325)
[2025-02-13 02:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:03][root][INFO] - Training Epoch: 1/2, step 1568/7134 completed (loss: 0.06794938445091248, acc: 0.9846153855323792)
[2025-02-13 02:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:03][root][INFO] - Training Epoch: 1/2, step 1569/7134 completed (loss: 0.054918427020311356, acc: 0.9839181303977966)
[2025-02-13 02:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:04][root][INFO] - Training Epoch: 1/2, step 1570/7134 completed (loss: 0.08713742345571518, acc: 0.9757441878318787)
[2025-02-13 02:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:04][root][INFO] - Training Epoch: 1/2, step 1571/7134 completed (loss: 0.058930136263370514, acc: 0.9833926558494568)
[2025-02-13 02:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:05][root][INFO] - Training Epoch: 1/2, step 1572/7134 completed (loss: 0.05376354232430458, acc: 0.9811543226242065)
[2025-02-13 02:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:05][root][INFO] - Training Epoch: 1/2, step 1573/7134 completed (loss: 0.03068532980978489, acc: 0.994194507598877)
[2025-02-13 02:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:05][root][INFO] - Training Epoch: 1/2, step 1574/7134 completed (loss: 0.050902411341667175, acc: 0.9879649877548218)
[2025-02-13 02:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06][root][INFO] - Training Epoch: 1/2, step 1575/7134 completed (loss: 0.054523587226867676, acc: 0.9844789505004883)
[2025-02-13 02:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06][root][INFO] - Training Epoch: 1/2, step 1576/7134 completed (loss: 0.06438620388507843, acc: 0.9871495366096497)
[2025-02-13 02:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:07][root][INFO] - Training Epoch: 1/2, step 1577/7134 completed (loss: 0.04328324645757675, acc: 0.987043559551239)
[2025-02-13 02:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:07][root][INFO] - Training Epoch: 1/2, step 1578/7134 completed (loss: 0.04835015907883644, acc: 0.9824086427688599)
[2025-02-13 02:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08][root][INFO] - Training Epoch: 1/2, step 1579/7134 completed (loss: 0.045780230313539505, acc: 0.9861751198768616)
[2025-02-13 02:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08][root][INFO] - Training Epoch: 1/2, step 1580/7134 completed (loss: 0.08335617929697037, acc: 0.9790748953819275)
[2025-02-13 02:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09][root][INFO] - Training Epoch: 1/2, step 1581/7134 completed (loss: 0.11543190479278564, acc: 0.9629629850387573)
[2025-02-13 02:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09][root][INFO] - Training Epoch: 1/2, step 1582/7134 completed (loss: 0.03945252671837807, acc: 0.98591548204422)
[2025-02-13 02:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10][root][INFO] - Training Epoch: 1/2, step 1583/7134 completed (loss: 0.03138486295938492, acc: 0.9885386824607849)
[2025-02-13 02:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10][root][INFO] - Training Epoch: 1/2, step 1584/7134 completed (loss: 0.042964380234479904, acc: 0.9903640151023865)
[2025-02-13 02:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11][root][INFO] - Training Epoch: 1/2, step 1585/7134 completed (loss: 0.04659492149949074, acc: 0.9835100173950195)
[2025-02-13 02:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11][root][INFO] - Training Epoch: 1/2, step 1586/7134 completed (loss: 0.062495745718479156, acc: 0.9821852445602417)
[2025-02-13 02:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:12][root][INFO] - Training Epoch: 1/2, step 1587/7134 completed (loss: 0.05366423726081848, acc: 0.9858712553977966)
[2025-02-13 02:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:12][root][INFO] - Training Epoch: 1/2, step 1588/7134 completed (loss: 0.058859001845121384, acc: 0.9880159497261047)
[2025-02-13 02:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13][root][INFO] - Training Epoch: 1/2, step 1589/7134 completed (loss: 0.03659629076719284, acc: 0.992443323135376)
[2025-02-13 02:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13][root][INFO] - Training Epoch: 1/2, step 1590/7134 completed (loss: 0.03372327238321304, acc: 0.9924471378326416)
[2025-02-13 02:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14][root][INFO] - Training Epoch: 1/2, step 1591/7134 completed (loss: 0.04790321737527847, acc: 0.9864636063575745)
[2025-02-13 02:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14][root][INFO] - Training Epoch: 1/2, step 1592/7134 completed (loss: 0.033070746809244156, acc: 0.9885877370834351)
[2025-02-13 02:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:15][root][INFO] - Training Epoch: 1/2, step 1593/7134 completed (loss: 0.0522991307079792, acc: 0.9873563051223755)
[2025-02-13 02:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:15][root][INFO] - Training Epoch: 1/2, step 1594/7134 completed (loss: 0.03036232851445675, acc: 0.9917355179786682)
[2025-02-13 02:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16][root][INFO] - Training Epoch: 1/2, step 1595/7134 completed (loss: 0.03594750910997391, acc: 0.9892473220825195)
[2025-02-13 02:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16][root][INFO] - Training Epoch: 1/2, step 1596/7134 completed (loss: 0.0561126209795475, acc: 0.98740154504776)
[2025-02-13 02:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17][root][INFO] - Training Epoch: 1/2, step 1597/7134 completed (loss: 0.016853028908371925, acc: 0.9971346855163574)
[2025-02-13 02:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17][root][INFO] - Training Epoch: 1/2, step 1598/7134 completed (loss: 0.07712426781654358, acc: 0.9780439138412476)
[2025-02-13 02:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17][root][INFO] - Training Epoch: 1/2, step 1599/7134 completed (loss: 0.12271269410848618, acc: 0.9735537171363831)
[2025-02-13 02:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:18][root][INFO] - Training Epoch: 1/2, step 1600/7134 completed (loss: 0.09413442015647888, acc: 0.9838337302207947)
[2025-02-13 02:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:18][root][INFO] - Training Epoch: 1/2, step 1601/7134 completed (loss: 0.022138705477118492, acc: 0.9917469024658203)
[2025-02-13 02:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:19][root][INFO] - Training Epoch: 1/2, step 1602/7134 completed (loss: 0.05097021162509918, acc: 0.9879336357116699)
[2025-02-13 02:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:19][root][INFO] - Training Epoch: 1/2, step 1603/7134 completed (loss: 0.08265124261379242, acc: 0.9795396327972412)
[2025-02-13 02:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20][root][INFO] - Training Epoch: 1/2, step 1604/7134 completed (loss: 0.026930930092930794, acc: 0.9946949481964111)
[2025-02-13 02:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20][root][INFO] - Training Epoch: 1/2, step 1605/7134 completed (loss: 0.04036870226264, acc: 0.9882659912109375)
[2025-02-13 02:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:21][root][INFO] - Training Epoch: 1/2, step 1606/7134 completed (loss: 0.025635290890932083, acc: 0.9938744306564331)
[2025-02-13 02:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:21][root][INFO] - Training Epoch: 1/2, step 1607/7134 completed (loss: 0.015397969633340836, acc: 0.9938900470733643)
[2025-02-13 02:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22][root][INFO] - Training Epoch: 1/2, step 1608/7134 completed (loss: 0.01823529787361622, acc: 0.9962825179100037)
[2025-02-13 02:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22][root][INFO] - Training Epoch: 1/2, step 1609/7134 completed (loss: 0.05006370693445206, acc: 0.9906542301177979)
[2025-02-13 02:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:23][root][INFO] - Training Epoch: 1/2, step 1610/7134 completed (loss: 0.05397731065750122, acc: 0.9916527271270752)
[2025-02-13 02:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:23][root][INFO] - Training Epoch: 1/2, step 1611/7134 completed (loss: 0.07298437505960464, acc: 0.9854497313499451)
[2025-02-13 02:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:23][root][INFO] - Training Epoch: 1/2, step 1612/7134 completed (loss: 0.07145444303750992, acc: 0.9872773289680481)
[2025-02-13 02:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24][root][INFO] - Training Epoch: 1/2, step 1613/7134 completed (loss: 0.05650146305561066, acc: 0.9894598126411438)
[2025-02-13 02:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24][root][INFO] - Training Epoch: 1/2, step 1614/7134 completed (loss: 0.06798476725816727, acc: 0.9882044792175293)
[2025-02-13 02:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25][root][INFO] - Training Epoch: 1/2, step 1615/7134 completed (loss: 0.05116485059261322, acc: 0.9838926196098328)
[2025-02-13 02:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25][root][INFO] - Training Epoch: 1/2, step 1616/7134 completed (loss: 0.02085006795823574, acc: 0.9956709742546082)
[2025-02-13 02:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26][root][INFO] - Training Epoch: 1/2, step 1617/7134 completed (loss: 0.05744929239153862, acc: 0.9906396269798279)
[2025-02-13 02:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26][root][INFO] - Training Epoch: 1/2, step 1618/7134 completed (loss: 0.09098999947309494, acc: 0.9771929979324341)
[2025-02-13 02:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27][root][INFO] - Training Epoch: 1/2, step 1619/7134 completed (loss: 0.06986992061138153, acc: 0.9782016277313232)
[2025-02-13 02:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27][root][INFO] - Training Epoch: 1/2, step 1620/7134 completed (loss: 0.08634910732507706, acc: 0.974281370639801)
[2025-02-13 02:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28][root][INFO] - Training Epoch: 1/2, step 1621/7134 completed (loss: 0.09724244475364685, acc: 0.9770269989967346)
[2025-02-13 02:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28][root][INFO] - Training Epoch: 1/2, step 1622/7134 completed (loss: 0.05902539938688278, acc: 0.984679639339447)
[2025-02-13 02:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29][root][INFO] - Training Epoch: 1/2, step 1623/7134 completed (loss: 0.05309661477804184, acc: 0.9887640476226807)
[2025-02-13 02:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29][root][INFO] - Training Epoch: 1/2, step 1624/7134 completed (loss: 0.060665376484394073, acc: 0.98531574010849)
[2025-02-13 02:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30][root][INFO] - Training Epoch: 1/2, step 1625/7134 completed (loss: 0.05285894125699997, acc: 0.9886040091514587)
[2025-02-13 02:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30][root][INFO] - Training Epoch: 1/2, step 1626/7134 completed (loss: 0.09293149411678314, acc: 0.9768518805503845)
[2025-02-13 02:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30][root][INFO] - Training Epoch: 1/2, step 1627/7134 completed (loss: 0.028201092034578323, acc: 0.9908758997917175)
[2025-02-13 02:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:31][root][INFO] - Training Epoch: 1/2, step 1628/7134 completed (loss: 0.019666597247123718, acc: 0.9927113652229309)
[2025-02-13 02:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:31][root][INFO] - Training Epoch: 1/2, step 1629/7134 completed (loss: 0.034924328327178955, acc: 0.9890909194946289)
[2025-02-13 02:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32][root][INFO] - Training Epoch: 1/2, step 1630/7134 completed (loss: 0.04409852623939514, acc: 0.9925925731658936)
[2025-02-13 02:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32][root][INFO] - Training Epoch: 1/2, step 1631/7134 completed (loss: 0.05439727008342743, acc: 0.9855305552482605)
[2025-02-13 02:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33][root][INFO] - Training Epoch: 1/2, step 1632/7134 completed (loss: 0.01905466802418232, acc: 0.9950248599052429)
[2025-02-13 02:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33][root][INFO] - Training Epoch: 1/2, step 1633/7134 completed (loss: 0.026452334597706795, acc: 0.9890109896659851)
[2025-02-13 02:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33][root][INFO] - Training Epoch: 1/2, step 1634/7134 completed (loss: 0.03360439091920853, acc: 0.9892802238464355)
[2025-02-13 02:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34][root][INFO] - Training Epoch: 1/2, step 1635/7134 completed (loss: 0.029579032212495804, acc: 0.9898167252540588)
[2025-02-13 02:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34][root][INFO] - Training Epoch: 1/2, step 1636/7134 completed (loss: 0.04555444419384003, acc: 0.9809523820877075)
[2025-02-13 02:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35][root][INFO] - Training Epoch: 1/2, step 1637/7134 completed (loss: 0.018969878554344177, acc: 0.9929378628730774)
[2025-02-13 02:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35][root][INFO] - Training Epoch: 1/2, step 1638/7134 completed (loss: 0.07505583018064499, acc: 0.979626476764679)
[2025-02-13 02:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:36][root][INFO] - Training Epoch: 1/2, step 1639/7134 completed (loss: 0.1399315744638443, acc: 0.970457911491394)
[2025-02-13 02:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:36][root][INFO] - Training Epoch: 1/2, step 1640/7134 completed (loss: 0.0658993050456047, acc: 0.9817073345184326)
[2025-02-13 02:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37][root][INFO] - Training Epoch: 1/2, step 1641/7134 completed (loss: 0.027192017063498497, acc: 0.9899425506591797)
[2025-02-13 02:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37][root][INFO] - Training Epoch: 1/2, step 1642/7134 completed (loss: 0.10167970508337021, acc: 0.9736841917037964)
[2025-02-13 02:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:38][root][INFO] - Training Epoch: 1/2, step 1643/7134 completed (loss: 0.11036123335361481, acc: 0.9670710563659668)
[2025-02-13 02:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:38][root][INFO] - Training Epoch: 1/2, step 1644/7134 completed (loss: 0.056391358375549316, acc: 0.9869621992111206)
[2025-02-13 02:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:38][root][INFO] - Training Epoch: 1/2, step 1645/7134 completed (loss: 0.08056768774986267, acc: 0.9841954112052917)
[2025-02-13 02:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39][root][INFO] - Training Epoch: 1/2, step 1646/7134 completed (loss: 0.11507310718297958, acc: 0.9712460041046143)
[2025-02-13 02:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39][root][INFO] - Training Epoch: 1/2, step 1647/7134 completed (loss: 0.052249591797590256, acc: 0.9888017773628235)
[2025-02-13 02:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40][root][INFO] - Training Epoch: 1/2, step 1648/7134 completed (loss: 0.08347523212432861, acc: 0.9760638475418091)
[2025-02-13 02:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40][root][INFO] - Training Epoch: 1/2, step 1649/7134 completed (loss: 0.03436050936579704, acc: 0.9889011979103088)
[2025-02-13 02:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41][root][INFO] - Training Epoch: 1/2, step 1650/7134 completed (loss: 0.051838330924510956, acc: 0.9858247637748718)
[2025-02-13 02:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41][root][INFO] - Training Epoch: 1/2, step 1651/7134 completed (loss: 0.09701815247535706, acc: 0.9811946749687195)
[2025-02-13 02:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42][root][INFO] - Training Epoch: 1/2, step 1652/7134 completed (loss: 0.024848345667123795, acc: 0.9934569001197815)
[2025-02-13 02:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42][root][INFO] - Training Epoch: 1/2, step 1653/7134 completed (loss: 0.052778031677007675, acc: 0.9850560426712036)
[2025-02-13 02:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43][root][INFO] - Training Epoch: 1/2, step 1654/7134 completed (loss: 0.055668484419584274, acc: 0.9857288599014282)
[2025-02-13 02:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43][root][INFO] - Training Epoch: 1/2, step 1655/7134 completed (loss: 0.061758678406476974, acc: 0.983890950679779)
[2025-02-13 02:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44][root][INFO] - Training Epoch: 1/2, step 1656/7134 completed (loss: 0.06089453399181366, acc: 0.9842342138290405)
[2025-02-13 02:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44][root][INFO] - Training Epoch: 1/2, step 1657/7134 completed (loss: 0.04102940484881401, acc: 0.9899543523788452)
[2025-02-13 02:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45][root][INFO] - Training Epoch: 1/2, step 1658/7134 completed (loss: 0.026506546884775162, acc: 0.9938775300979614)
[2025-02-13 02:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45][root][INFO] - Training Epoch: 1/2, step 1659/7134 completed (loss: 0.06188502907752991, acc: 0.9819905161857605)
[2025-02-13 02:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:46][root][INFO] - Training Epoch: 1/2, step 1660/7134 completed (loss: 0.044641800224781036, acc: 0.990338146686554)
[2025-02-13 02:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:46][root][INFO] - Training Epoch: 1/2, step 1661/7134 completed (loss: 0.06733084470033646, acc: 0.9824945330619812)
[2025-02-13 02:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47][root][INFO] - Training Epoch: 1/2, step 1662/7134 completed (loss: 0.06220623105764389, acc: 0.9851537942886353)
[2025-02-13 02:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47][root][INFO] - Training Epoch: 1/2, step 1663/7134 completed (loss: 0.04671954736113548, acc: 0.9876670241355896)
[2025-02-13 02:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48][root][INFO] - Training Epoch: 1/2, step 1664/7134 completed (loss: 0.027191419154405594, acc: 0.9916567206382751)
[2025-02-13 02:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48][root][INFO] - Training Epoch: 1/2, step 1665/7134 completed (loss: 0.04570544511079788, acc: 0.9798561334609985)
[2025-02-13 02:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49][root][INFO] - Training Epoch: 1/2, step 1666/7134 completed (loss: 0.032897330820560455, acc: 0.9900199770927429)
[2025-02-13 02:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49][root][INFO] - Training Epoch: 1/2, step 1667/7134 completed (loss: 0.061462920159101486, acc: 0.9833729267120361)
[2025-02-13 02:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50][root][INFO] - Training Epoch: 1/2, step 1668/7134 completed (loss: 0.026389386504888535, acc: 0.9926470518112183)
[2025-02-13 02:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50][root][INFO] - Training Epoch: 1/2, step 1669/7134 completed (loss: 0.0389779657125473, acc: 0.9887005686759949)
[2025-02-13 02:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51][root][INFO] - Training Epoch: 1/2, step 1670/7134 completed (loss: 0.05846153572201729, acc: 0.9875954389572144)
[2025-02-13 02:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51][root][INFO] - Training Epoch: 1/2, step 1671/7134 completed (loss: 0.05637078732252121, acc: 0.9868578314781189)
[2025-02-13 02:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52][root][INFO] - Training Epoch: 1/2, step 1672/7134 completed (loss: 0.055260069668293, acc: 0.9867197871208191)
[2025-02-13 02:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52][root][INFO] - Training Epoch: 1/2, step 1673/7134 completed (loss: 0.04922540858387947, acc: 0.9919354915618896)
[2025-02-13 02:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:53][root][INFO] - Training Epoch: 1/2, step 1674/7134 completed (loss: 0.051268648356199265, acc: 0.9874301552772522)
[2025-02-13 02:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:53][root][INFO] - Training Epoch: 1/2, step 1675/7134 completed (loss: 0.08221598714590073, acc: 0.9755101799964905)
[2025-02-13 02:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:53][root][INFO] - Training Epoch: 1/2, step 1676/7134 completed (loss: 0.034557122737169266, acc: 0.988950252532959)
[2025-02-13 02:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54][root][INFO] - Training Epoch: 1/2, step 1677/7134 completed (loss: 0.08176914602518082, acc: 0.9803600907325745)
[2025-02-13 02:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54][root][INFO] - Training Epoch: 1/2, step 1678/7134 completed (loss: 0.042960312217473984, acc: 0.9904109835624695)
[2025-02-13 02:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55][root][INFO] - Training Epoch: 1/2, step 1679/7134 completed (loss: 0.05290021002292633, acc: 0.9868228435516357)
[2025-02-13 02:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55][root][INFO] - Training Epoch: 1/2, step 1680/7134 completed (loss: 0.03833844140172005, acc: 0.9932975769042969)
[2025-02-13 02:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56][root][INFO] - Training Epoch: 1/2, step 1681/7134 completed (loss: 0.04420908913016319, acc: 0.9886363744735718)
[2025-02-13 02:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56][root][INFO] - Training Epoch: 1/2, step 1682/7134 completed (loss: 0.05405333265662193, acc: 0.9870316982269287)
[2025-02-13 02:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:57][root][INFO] - Training Epoch: 1/2, step 1683/7134 completed (loss: 0.05871293693780899, acc: 0.9805825352668762)
[2025-02-13 02:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:57][root][INFO] - Training Epoch: 1/2, step 1684/7134 completed (loss: 0.060412295162677765, acc: 0.9845890402793884)
[2025-02-13 02:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:57][root][INFO] - Training Epoch: 1/2, step 1685/7134 completed (loss: 0.043654609471559525, acc: 0.9874826073646545)
[2025-02-13 02:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58][root][INFO] - Training Epoch: 1/2, step 1686/7134 completed (loss: 0.04200323671102524, acc: 0.9864864945411682)
[2025-02-13 02:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58][root][INFO] - Training Epoch: 1/2, step 1687/7134 completed (loss: 0.0342293418943882, acc: 0.9885931611061096)
[2025-02-13 02:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59][root][INFO] - Training Epoch: 1/2, step 1688/7134 completed (loss: 0.06332871317863464, acc: 0.984544038772583)
[2025-02-13 02:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59][root][INFO] - Training Epoch: 1/2, step 1689/7134 completed (loss: 0.041976962238550186, acc: 0.9898989796638489)
[2025-02-13 02:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00][root][INFO] - Training Epoch: 1/2, step 1690/7134 completed (loss: 0.03278835490345955, acc: 0.9939024448394775)
[2025-02-13 02:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00][root][INFO] - Training Epoch: 1/2, step 1691/7134 completed (loss: 0.011030919849872589, acc: 0.996688723564148)
[2025-02-13 02:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:01][root][INFO] - Training Epoch: 1/2, step 1692/7134 completed (loss: 0.06853029131889343, acc: 0.9763912558555603)
[2025-02-13 02:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:01][root][INFO] - Training Epoch: 1/2, step 1693/7134 completed (loss: 0.032365720719099045, acc: 0.9910179376602173)
[2025-02-13 02:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:01][root][INFO] - Training Epoch: 1/2, step 1694/7134 completed (loss: 0.046296365559101105, acc: 0.9874804615974426)
[2025-02-13 02:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02][root][INFO] - Training Epoch: 1/2, step 1695/7134 completed (loss: 0.01650753617286682, acc: 0.9963964223861694)
[2025-02-13 02:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02][root][INFO] - Training Epoch: 1/2, step 1696/7134 completed (loss: 0.050003860145807266, acc: 0.9849246144294739)
[2025-02-13 02:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03][root][INFO] - Training Epoch: 1/2, step 1697/7134 completed (loss: 0.05411630868911743, acc: 0.98591548204422)
[2025-02-13 02:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03][root][INFO] - Training Epoch: 1/2, step 1698/7134 completed (loss: 0.027328556403517723, acc: 0.9921630024909973)
[2025-02-13 02:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04][root][INFO] - Training Epoch: 1/2, step 1699/7134 completed (loss: 0.04333672672510147, acc: 0.9826989769935608)
[2025-02-13 02:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04][root][INFO] - Training Epoch: 1/2, step 1700/7134 completed (loss: 0.06470257043838501, acc: 0.980322003364563)
[2025-02-13 02:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:05][root][INFO] - Training Epoch: 1/2, step 1701/7134 completed (loss: 0.04304470494389534, acc: 0.9847009778022766)
[2025-02-13 02:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:05][root][INFO] - Training Epoch: 1/2, step 1702/7134 completed (loss: 0.044046513736248016, acc: 0.9875776171684265)
[2025-02-13 02:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:05][root][INFO] - Training Epoch: 1/2, step 1703/7134 completed (loss: 0.06833891570568085, acc: 0.9792817831039429)
[2025-02-13 02:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:06][root][INFO] - Training Epoch: 1/2, step 1704/7134 completed (loss: 0.11067821830511093, acc: 0.96875)
[2025-02-13 02:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:06][root][INFO] - Training Epoch: 1/2, step 1705/7134 completed (loss: 0.06205279007554054, acc: 0.9802761077880859)
[2025-02-13 02:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07][root][INFO] - Training Epoch: 1/2, step 1706/7134 completed (loss: 0.052636515349149704, acc: 0.9924242496490479)
[2025-02-13 02:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07][root][INFO] - Training Epoch: 1/2, step 1707/7134 completed (loss: 0.04666588082909584, acc: 0.9915433526039124)
[2025-02-13 02:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08][root][INFO] - Training Epoch: 1/2, step 1708/7134 completed (loss: 0.04726668447256088, acc: 0.9869918823242188)
[2025-02-13 02:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08][root][INFO] - Training Epoch: 1/2, step 1709/7134 completed (loss: 0.06885547935962677, acc: 0.9775862097740173)
[2025-02-13 02:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:09][root][INFO] - Training Epoch: 1/2, step 1710/7134 completed (loss: 0.07002420723438263, acc: 0.982758641242981)
[2025-02-13 02:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:09][root][INFO] - Training Epoch: 1/2, step 1711/7134 completed (loss: 0.060952913016080856, acc: 0.9879102110862732)
[2025-02-13 02:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:09][root][INFO] - Training Epoch: 1/2, step 1712/7134 completed (loss: 0.09460846334695816, acc: 0.9754385948181152)
[2025-02-13 02:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10][root][INFO] - Training Epoch: 1/2, step 1713/7134 completed (loss: 0.04626825451850891, acc: 0.9840319156646729)
[2025-02-13 02:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10][root][INFO] - Training Epoch: 1/2, step 1714/7134 completed (loss: 0.07849949598312378, acc: 0.9769737124443054)
[2025-02-13 02:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11][root][INFO] - Training Epoch: 1/2, step 1715/7134 completed (loss: 0.0494389683008194, acc: 0.9837728142738342)
[2025-02-13 02:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11][root][INFO] - Training Epoch: 1/2, step 1716/7134 completed (loss: 0.09889007359743118, acc: 0.9683333039283752)
[2025-02-13 02:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12][root][INFO] - Training Epoch: 1/2, step 1717/7134 completed (loss: 0.04572447016835213, acc: 0.9824903011322021)
[2025-02-13 02:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12][root][INFO] - Training Epoch: 1/2, step 1718/7134 completed (loss: 0.06785481423139572, acc: 0.9755011200904846)
[2025-02-13 02:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12][root][INFO] - Training Epoch: 1/2, step 1719/7134 completed (loss: 0.0846429094672203, acc: 0.983561635017395)
[2025-02-13 02:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:13][root][INFO] - Training Epoch: 1/2, step 1720/7134 completed (loss: 0.08630053699016571, acc: 0.9648033380508423)
[2025-02-13 02:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:13][root][INFO] - Training Epoch: 1/2, step 1721/7134 completed (loss: 0.040250107645988464, acc: 0.9831365942955017)
[2025-02-13 02:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14][root][INFO] - Training Epoch: 1/2, step 1722/7134 completed (loss: 0.03557971492409706, acc: 0.9868420958518982)
[2025-02-13 02:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14][root][INFO] - Training Epoch: 1/2, step 1723/7134 completed (loss: 0.10636083036661148, acc: 0.9761549830436707)
[2025-02-13 02:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15][root][INFO] - Training Epoch: 1/2, step 1724/7134 completed (loss: 0.11412915587425232, acc: 0.9670329689979553)
[2025-02-13 02:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15][root][INFO] - Training Epoch: 1/2, step 1725/7134 completed (loss: 0.03638709336519241, acc: 0.9935691356658936)
[2025-02-13 02:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15][root][INFO] - Training Epoch: 1/2, step 1726/7134 completed (loss: 0.06189217418432236, acc: 0.9820627570152283)
[2025-02-13 02:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16][root][INFO] - Training Epoch: 1/2, step 1727/7134 completed (loss: 0.030274491757154465, acc: 0.9945255517959595)
[2025-02-13 02:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16][root][INFO] - Training Epoch: 1/2, step 1728/7134 completed (loss: 0.0349004901945591, acc: 0.9884105920791626)
[2025-02-13 02:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17][root][INFO] - Training Epoch: 1/2, step 1729/7134 completed (loss: 0.06907682865858078, acc: 0.9857434034347534)
[2025-02-13 02:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17][root][INFO] - Training Epoch: 1/2, step 1730/7134 completed (loss: 0.0391346774995327, acc: 0.9871520400047302)
[2025-02-13 02:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17][root][INFO] - Training Epoch: 1/2, step 1731/7134 completed (loss: 0.10589616000652313, acc: 0.9797688126564026)
[2025-02-13 02:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18][root][INFO] - Training Epoch: 1/2, step 1732/7134 completed (loss: 0.04441973567008972, acc: 0.987679660320282)
[2025-02-13 02:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18][root][INFO] - Training Epoch: 1/2, step 1733/7134 completed (loss: 0.06160944327712059, acc: 0.9814814925193787)
[2025-02-13 02:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:19][root][INFO] - Training Epoch: 1/2, step 1734/7134 completed (loss: 0.0705627053976059, acc: 0.977886974811554)
[2025-02-13 02:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:19][root][INFO] - Training Epoch: 1/2, step 1735/7134 completed (loss: 0.018394507467746735, acc: 0.9957924485206604)
[2025-02-13 02:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20][root][INFO] - Training Epoch: 1/2, step 1736/7134 completed (loss: 0.04084138944745064, acc: 0.9890909194946289)
[2025-02-13 02:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20][root][INFO] - Training Epoch: 1/2, step 1737/7134 completed (loss: 0.0637548565864563, acc: 0.9845505356788635)
[2025-02-13 02:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21][root][INFO] - Training Epoch: 1/2, step 1738/7134 completed (loss: 0.016764704138040543, acc: 0.9963811635971069)
[2025-02-13 02:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21][root][INFO] - Training Epoch: 1/2, step 1739/7134 completed (loss: 0.07744983583688736, acc: 0.977544903755188)
[2025-02-13 02:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22][root][INFO] - Training Epoch: 1/2, step 1740/7134 completed (loss: 0.024336248636245728, acc: 0.9926900863647461)
[2025-02-13 02:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22][root][INFO] - Training Epoch: 1/2, step 1741/7134 completed (loss: 0.05729033425450325, acc: 0.988095223903656)
[2025-02-13 02:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22][root][INFO] - Training Epoch: 1/2, step 1742/7134 completed (loss: 0.06364600360393524, acc: 0.9843971729278564)
[2025-02-13 02:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:23][root][INFO] - Training Epoch: 1/2, step 1743/7134 completed (loss: 0.08472902327775955, acc: 0.9831932783126831)
[2025-02-13 02:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:23][root][INFO] - Training Epoch: 1/2, step 1744/7134 completed (loss: 0.04096810892224312, acc: 0.9918566942214966)
[2025-02-13 02:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24][root][INFO] - Training Epoch: 1/2, step 1745/7134 completed (loss: 0.041676364839076996, acc: 0.9887217879295349)
[2025-02-13 02:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24][root][INFO] - Training Epoch: 1/2, step 1746/7134 completed (loss: 0.030061990022659302, acc: 0.9883720874786377)
[2025-02-13 02:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25][root][INFO] - Training Epoch: 1/2, step 1747/7134 completed (loss: 0.05173889547586441, acc: 0.9862328171730042)
[2025-02-13 02:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25][root][INFO] - Training Epoch: 1/2, step 1748/7134 completed (loss: 0.03461083769798279, acc: 0.9913151264190674)
[2025-02-13 02:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:26][root][INFO] - Training Epoch: 1/2, step 1749/7134 completed (loss: 0.03977573290467262, acc: 0.9897843599319458)
[2025-02-13 02:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:26][root][INFO] - Training Epoch: 1/2, step 1750/7134 completed (loss: 0.04999570548534393, acc: 0.986810564994812)
[2025-02-13 02:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27][root][INFO] - Training Epoch: 1/2, step 1751/7134 completed (loss: 0.05639084428548813, acc: 0.9842424392700195)
[2025-02-13 02:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27][root][INFO] - Training Epoch: 1/2, step 1752/7134 completed (loss: 0.08220260590314865, acc: 0.9780927896499634)
[2025-02-13 02:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28][root][INFO] - Training Epoch: 1/2, step 1753/7134 completed (loss: 0.06699834764003754, acc: 0.9830949306488037)
[2025-02-13 02:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28][root][INFO] - Training Epoch: 1/2, step 1754/7134 completed (loss: 0.05410417541861534, acc: 0.9899135231971741)
[2025-02-13 02:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29][root][INFO] - Training Epoch: 1/2, step 1755/7134 completed (loss: 0.032046496868133545, acc: 0.988095223903656)
[2025-02-13 02:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29][root][INFO] - Training Epoch: 1/2, step 1756/7134 completed (loss: 0.05729022994637489, acc: 0.981794536113739)
[2025-02-13 02:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29][root][INFO] - Training Epoch: 1/2, step 1757/7134 completed (loss: 0.08038614690303802, acc: 0.9783693552017212)
[2025-02-13 02:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:30][root][INFO] - Training Epoch: 1/2, step 1758/7134 completed (loss: 0.027791080996394157, acc: 0.9911616444587708)
[2025-02-13 02:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:30][root][INFO] - Training Epoch: 1/2, step 1759/7134 completed (loss: 0.042757149785757065, acc: 0.9874826073646545)
[2025-02-13 02:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31][root][INFO] - Training Epoch: 1/2, step 1760/7134 completed (loss: 0.0302906297147274, acc: 0.9932773113250732)
[2025-02-13 02:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31][root][INFO] - Training Epoch: 1/2, step 1761/7134 completed (loss: 0.03104974515736103, acc: 0.9900568127632141)
[2025-02-13 02:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32][root][INFO] - Training Epoch: 1/2, step 1762/7134 completed (loss: 0.02870945632457733, acc: 0.991752564907074)
[2025-02-13 02:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32][root][INFO] - Training Epoch: 1/2, step 1763/7134 completed (loss: 0.05924615636467934, acc: 0.9871794581413269)
[2025-02-13 02:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32][root][INFO] - Training Epoch: 1/2, step 1764/7134 completed (loss: 0.04804427549242973, acc: 0.9869888424873352)
[2025-02-13 02:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33][root][INFO] - Training Epoch: 1/2, step 1765/7134 completed (loss: 0.036597441881895065, acc: 0.9907407164573669)
[2025-02-13 02:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33][root][INFO] - Training Epoch: 1/2, step 1766/7134 completed (loss: 0.03574337437748909, acc: 0.9912023544311523)
[2025-02-13 02:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:34][root][INFO] - Training Epoch: 1/2, step 1767/7134 completed (loss: 0.07218065112829208, acc: 0.9801980257034302)
[2025-02-13 02:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:34][root][INFO] - Training Epoch: 1/2, step 1768/7134 completed (loss: 0.03158349171280861, acc: 0.9865125417709351)
[2025-02-13 02:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35][root][INFO] - Training Epoch: 1/2, step 1769/7134 completed (loss: 0.0392686128616333, acc: 0.9867647290229797)
[2025-02-13 02:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35][root][INFO] - Training Epoch: 1/2, step 1770/7134 completed (loss: 0.03931065648794174, acc: 0.987860381603241)
[2025-02-13 02:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35][root][INFO] - Training Epoch: 1/2, step 1771/7134 completed (loss: 0.030842021107673645, acc: 0.9902234673500061)
[2025-02-13 02:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36][root][INFO] - Training Epoch: 1/2, step 1772/7134 completed (loss: 0.023466285318136215, acc: 0.9899497628211975)
[2025-02-13 02:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36][root][INFO] - Training Epoch: 1/2, step 1773/7134 completed (loss: 0.01924811117351055, acc: 0.9941860437393188)
[2025-02-13 02:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37][root][INFO] - Training Epoch: 1/2, step 1774/7134 completed (loss: 0.019267680123448372, acc: 0.9950166344642639)
[2025-02-13 02:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37][root][INFO] - Training Epoch: 1/2, step 1775/7134 completed (loss: 0.022511441260576248, acc: 0.9921875)
[2025-02-13 02:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:38][root][INFO] - Training Epoch: 1/2, step 1776/7134 completed (loss: 0.04142393916845322, acc: 0.9880239367485046)
[2025-02-13 02:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:38][root][INFO] - Training Epoch: 1/2, step 1777/7134 completed (loss: 0.010089615359902382, acc: 0.9970414042472839)
[2025-02-13 02:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39][root][INFO] - Training Epoch: 1/2, step 1778/7134 completed (loss: 0.05677592381834984, acc: 0.9852070808410645)
[2025-02-13 02:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39][root][INFO] - Training Epoch: 1/2, step 1779/7134 completed (loss: 0.03317348286509514, acc: 0.9898989796638489)
[2025-02-13 02:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39][root][INFO] - Training Epoch: 1/2, step 1780/7134 completed (loss: 0.036352720111608505, acc: 0.9957567453384399)
[2025-02-13 02:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40][root][INFO] - Training Epoch: 1/2, step 1781/7134 completed (loss: 0.02479533664882183, acc: 0.9923664331436157)
[2025-02-13 02:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40][root][INFO] - Training Epoch: 1/2, step 1782/7134 completed (loss: 0.0348823182284832, acc: 0.9939117431640625)
[2025-02-13 02:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0633, device='cuda:0') eval_epoch_loss=tensor(0.0613, device='cuda:0') eval_epoch_acc=tensor(0.9838, device='cuda:0')
[2025-02-13 02:43:15][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 02:43:15][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 02:43:16][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_1783_loss_0.06134800612926483/model.pt
[2025-02-13 02:43:16][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 02:43:16][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.06134800612926483
[2025-02-13 02:43:16][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9837810397148132
[2025-02-13 02:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:16][root][INFO] - Training Epoch: 1/2, step 1783/7134 completed (loss: 0.051213331520557404, acc: 0.9878378510475159)
[2025-02-13 02:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:17][root][INFO] - Training Epoch: 1/2, step 1784/7134 completed (loss: 0.021884961053729057, acc: 0.9956011772155762)
[2025-02-13 02:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:17][root][INFO] - Training Epoch: 1/2, step 1785/7134 completed (loss: 0.036746807396411896, acc: 0.9876203536987305)
[2025-02-13 02:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18][root][INFO] - Training Epoch: 1/2, step 1786/7134 completed (loss: 0.03962773084640503, acc: 0.9906396269798279)
[2025-02-13 02:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18][root][INFO] - Training Epoch: 1/2, step 1787/7134 completed (loss: 0.026690645143389702, acc: 0.9928571581840515)
[2025-02-13 02:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19][root][INFO] - Training Epoch: 1/2, step 1788/7134 completed (loss: 0.0350860096514225, acc: 0.9901800155639648)
[2025-02-13 02:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19][root][INFO] - Training Epoch: 1/2, step 1789/7134 completed (loss: 0.018513528630137444, acc: 0.9953917264938354)
[2025-02-13 02:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19][root][INFO] - Training Epoch: 1/2, step 1790/7134 completed (loss: 0.16243034601211548, acc: 0.9758453965187073)
[2025-02-13 02:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20][root][INFO] - Training Epoch: 1/2, step 1791/7134 completed (loss: 0.1425718069076538, acc: 0.9728958606719971)
[2025-02-13 02:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20][root][INFO] - Training Epoch: 1/2, step 1792/7134 completed (loss: 0.18148289620876312, acc: 0.9688346982002258)
[2025-02-13 02:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:21][root][INFO] - Training Epoch: 1/2, step 1793/7134 completed (loss: 0.11622316390275955, acc: 0.9743243455886841)
[2025-02-13 02:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:21][root][INFO] - Training Epoch: 1/2, step 1794/7134 completed (loss: 0.09982160478830338, acc: 0.9764150977134705)
[2025-02-13 02:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:22][root][INFO] - Training Epoch: 1/2, step 1795/7134 completed (loss: 0.144566610455513, acc: 0.9680851101875305)
[2025-02-13 02:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:22][root][INFO] - Training Epoch: 1/2, step 1796/7134 completed (loss: 0.11335965991020203, acc: 0.9702194333076477)
[2025-02-13 02:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23][root][INFO] - Training Epoch: 1/2, step 1797/7134 completed (loss: 0.07657228410243988, acc: 0.9803921580314636)
[2025-02-13 02:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23][root][INFO] - Training Epoch: 1/2, step 1798/7134 completed (loss: 0.08050866425037384, acc: 0.9710424542427063)
[2025-02-13 02:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24][root][INFO] - Training Epoch: 1/2, step 1799/7134 completed (loss: 0.11125146597623825, acc: 0.9711055159568787)
[2025-02-13 02:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24][root][INFO] - Training Epoch: 1/2, step 1800/7134 completed (loss: 0.130793958902359, acc: 0.9626168012619019)
[2025-02-13 02:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24][root][INFO] - Training Epoch: 1/2, step 1801/7134 completed (loss: 0.08530289679765701, acc: 0.9851694703102112)
[2025-02-13 02:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25][root][INFO] - Training Epoch: 1/2, step 1802/7134 completed (loss: 0.07749617099761963, acc: 0.9887359142303467)
[2025-02-13 02:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25][root][INFO] - Training Epoch: 1/2, step 1803/7134 completed (loss: 0.05928105488419533, acc: 0.9815100431442261)
[2025-02-13 02:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:26][root][INFO] - Training Epoch: 1/2, step 1804/7134 completed (loss: 0.03223403915762901, acc: 0.9889435172080994)
[2025-02-13 02:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:26][root][INFO] - Training Epoch: 1/2, step 1805/7134 completed (loss: 0.06120959296822548, acc: 0.9870129823684692)
[2025-02-13 02:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27][root][INFO] - Training Epoch: 1/2, step 1806/7134 completed (loss: 0.05232958123087883, acc: 0.9894737005233765)
[2025-02-13 02:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27][root][INFO] - Training Epoch: 1/2, step 1807/7134 completed (loss: 0.055853862315416336, acc: 0.9844412803649902)
[2025-02-13 02:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27][root][INFO] - Training Epoch: 1/2, step 1808/7134 completed (loss: 0.029198940843343735, acc: 0.9921976327896118)
[2025-02-13 02:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:28][root][INFO] - Training Epoch: 1/2, step 1809/7134 completed (loss: 0.0404910072684288, acc: 0.9878378510475159)
[2025-02-13 02:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:28][root][INFO] - Training Epoch: 1/2, step 1810/7134 completed (loss: 0.026012836024165154, acc: 0.9935566782951355)
[2025-02-13 02:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:29][root][INFO] - Training Epoch: 1/2, step 1811/7134 completed (loss: 0.06803952157497406, acc: 0.975576639175415)
[2025-02-13 02:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:29][root][INFO] - Training Epoch: 1/2, step 1812/7134 completed (loss: 0.02209555357694626, acc: 0.9948979616165161)
[2025-02-13 02:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30][root][INFO] - Training Epoch: 1/2, step 1813/7134 completed (loss: 0.05564345419406891, acc: 0.9832335114479065)
[2025-02-13 02:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30][root][INFO] - Training Epoch: 1/2, step 1814/7134 completed (loss: 0.030973311513662338, acc: 0.9906759858131409)
[2025-02-13 02:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31][root][INFO] - Training Epoch: 1/2, step 1815/7134 completed (loss: 0.03669237345457077, acc: 0.9899425506591797)
[2025-02-13 02:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31][root][INFO] - Training Epoch: 1/2, step 1816/7134 completed (loss: 0.05151258409023285, acc: 0.9827855825424194)
[2025-02-13 02:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31][root][INFO] - Training Epoch: 1/2, step 1817/7134 completed (loss: 0.045728281140327454, acc: 0.9892601370811462)
[2025-02-13 02:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:32][root][INFO] - Training Epoch: 1/2, step 1818/7134 completed (loss: 0.07317701727151871, acc: 0.9851301312446594)
[2025-02-13 02:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:32][root][INFO] - Training Epoch: 1/2, step 1819/7134 completed (loss: 0.02641463279724121, acc: 0.9925187230110168)
[2025-02-13 02:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:33][root][INFO] - Training Epoch: 1/2, step 1820/7134 completed (loss: 0.03393378108739853, acc: 0.9934498071670532)
[2025-02-13 02:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:33][root][INFO] - Training Epoch: 1/2, step 1821/7134 completed (loss: 0.05783435329794884, acc: 0.9879649877548218)
[2025-02-13 02:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34][root][INFO] - Training Epoch: 1/2, step 1822/7134 completed (loss: 0.019455019384622574, acc: 0.9926900863647461)
[2025-02-13 02:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34][root][INFO] - Training Epoch: 1/2, step 1823/7134 completed (loss: 0.024547215551137924, acc: 0.9904502034187317)
[2025-02-13 02:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:35][root][INFO] - Training Epoch: 1/2, step 1824/7134 completed (loss: 0.03656933829188347, acc: 0.993819534778595)
[2025-02-13 02:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:35][root][INFO] - Training Epoch: 1/2, step 1825/7134 completed (loss: 0.03445253521203995, acc: 0.9887892603874207)
[2025-02-13 02:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36][root][INFO] - Training Epoch: 1/2, step 1826/7134 completed (loss: 0.025918148458003998, acc: 0.9932705163955688)
[2025-02-13 02:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36][root][INFO] - Training Epoch: 1/2, step 1827/7134 completed (loss: 0.05507557839155197, acc: 0.9878721237182617)
[2025-02-13 02:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36][root][INFO] - Training Epoch: 1/2, step 1828/7134 completed (loss: 0.00800518598407507, acc: 0.9974259734153748)
[2025-02-13 02:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:37][root][INFO] - Training Epoch: 1/2, step 1829/7134 completed (loss: 0.04158366098999977, acc: 0.9875776171684265)
[2025-02-13 02:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:37][root][INFO] - Training Epoch: 1/2, step 1830/7134 completed (loss: 0.0666847676038742, acc: 0.9842615127563477)
[2025-02-13 02:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38][root][INFO] - Training Epoch: 1/2, step 1831/7134 completed (loss: 0.02005610801279545, acc: 0.9937499761581421)
[2025-02-13 02:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38][root][INFO] - Training Epoch: 1/2, step 1832/7134 completed (loss: 0.07025670260190964, acc: 0.9840989112854004)
[2025-02-13 02:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:39][root][INFO] - Training Epoch: 1/2, step 1833/7134 completed (loss: 0.23963193595409393, acc: 0.9513742327690125)
[2025-02-13 02:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:39][root][INFO] - Training Epoch: 1/2, step 1834/7134 completed (loss: 0.05780060961842537, acc: 0.9816901683807373)
[2025-02-13 02:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:40][root][INFO] - Training Epoch: 1/2, step 1835/7134 completed (loss: 0.0805407240986824, acc: 0.9869186282157898)
[2025-02-13 02:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:40][root][INFO] - Training Epoch: 1/2, step 1836/7134 completed (loss: 0.039658255875110626, acc: 0.9858155846595764)
[2025-02-13 02:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:40][root][INFO] - Training Epoch: 1/2, step 1837/7134 completed (loss: 0.04303337633609772, acc: 0.9803439974784851)
[2025-02-13 02:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41][root][INFO] - Training Epoch: 1/2, step 1838/7134 completed (loss: 0.022378135472536087, acc: 0.9918367266654968)
[2025-02-13 02:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41][root][INFO] - Training Epoch: 1/2, step 1839/7134 completed (loss: 0.04170392453670502, acc: 0.9870466589927673)
[2025-02-13 02:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:42][root][INFO] - Training Epoch: 1/2, step 1840/7134 completed (loss: 0.04975278303027153, acc: 0.9846583008766174)
[2025-02-13 02:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:42][root][INFO] - Training Epoch: 1/2, step 1841/7134 completed (loss: 0.057410746812820435, acc: 0.978723406791687)
[2025-02-13 02:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43][root][INFO] - Training Epoch: 1/2, step 1842/7134 completed (loss: 0.04484902322292328, acc: 0.9877551198005676)
[2025-02-13 02:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43][root][INFO] - Training Epoch: 1/2, step 1843/7134 completed (loss: 0.060362137854099274, acc: 0.9837499856948853)
[2025-02-13 02:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:44][root][INFO] - Training Epoch: 1/2, step 1844/7134 completed (loss: 0.04036308452486992, acc: 0.9886178970336914)
[2025-02-13 02:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:44][root][INFO] - Training Epoch: 1/2, step 1845/7134 completed (loss: 0.06241831183433533, acc: 0.9812108278274536)
[2025-02-13 02:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:44][root][INFO] - Training Epoch: 1/2, step 1846/7134 completed (loss: 0.03976413235068321, acc: 0.9843304753303528)
[2025-02-13 02:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:45][root][INFO] - Training Epoch: 1/2, step 1847/7134 completed (loss: 0.04704038426280022, acc: 0.9831029176712036)
[2025-02-13 02:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:45][root][INFO] - Training Epoch: 1/2, step 1848/7134 completed (loss: 0.06277631223201752, acc: 0.9844632744789124)
[2025-02-13 02:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46][root][INFO] - Training Epoch: 1/2, step 1849/7134 completed (loss: 0.07793483138084412, acc: 0.9806678295135498)
[2025-02-13 02:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46][root][INFO] - Training Epoch: 1/2, step 1850/7134 completed (loss: 0.040360815823078156, acc: 0.9865525960922241)
[2025-02-13 02:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:47][root][INFO] - Training Epoch: 1/2, step 1851/7134 completed (loss: 0.06149187311530113, acc: 0.9877899885177612)
[2025-02-13 02:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:47][root][INFO] - Training Epoch: 1/2, step 1852/7134 completed (loss: 0.05579666420817375, acc: 0.9829192757606506)
[2025-02-13 02:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:48][root][INFO] - Training Epoch: 1/2, step 1853/7134 completed (loss: 0.049285437911748886, acc: 0.9916267991065979)
[2025-02-13 02:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:48][root][INFO] - Training Epoch: 1/2, step 1854/7134 completed (loss: 0.059305667877197266, acc: 0.9821640849113464)
[2025-02-13 02:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:49][root][INFO] - Training Epoch: 1/2, step 1855/7134 completed (loss: 0.05140650272369385, acc: 0.9849711060523987)
[2025-02-13 02:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:49][root][INFO] - Training Epoch: 1/2, step 1856/7134 completed (loss: 0.0541047602891922, acc: 0.9827798008918762)
[2025-02-13 02:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:49][root][INFO] - Training Epoch: 1/2, step 1857/7134 completed (loss: 0.08465708792209625, acc: 0.9802955389022827)
[2025-02-13 02:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50][root][INFO] - Training Epoch: 1/2, step 1858/7134 completed (loss: 0.02818485163152218, acc: 0.9904305934906006)
[2025-02-13 02:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50][root][INFO] - Training Epoch: 1/2, step 1859/7134 completed (loss: 0.02568592131137848, acc: 0.9931740760803223)
[2025-02-13 02:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:51][root][INFO] - Training Epoch: 1/2, step 1860/7134 completed (loss: 0.030175259336829185, acc: 0.9911392331123352)
[2025-02-13 02:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:51][root][INFO] - Training Epoch: 1/2, step 1861/7134 completed (loss: 0.04493559151887894, acc: 0.9896640777587891)
[2025-02-13 02:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:52][root][INFO] - Training Epoch: 1/2, step 1862/7134 completed (loss: 0.026853512972593307, acc: 0.9945725798606873)
[2025-02-13 02:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:52][root][INFO] - Training Epoch: 1/2, step 1863/7134 completed (loss: 0.0449955128133297, acc: 0.9904076457023621)
[2025-02-13 02:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53][root][INFO] - Training Epoch: 1/2, step 1864/7134 completed (loss: 0.16186851263046265, acc: 0.960544228553772)
[2025-02-13 02:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53][root][INFO] - Training Epoch: 1/2, step 1865/7134 completed (loss: 0.05823048949241638, acc: 0.9830795526504517)
[2025-02-13 02:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53][root][INFO] - Training Epoch: 1/2, step 1866/7134 completed (loss: 0.018170109018683434, acc: 0.9957982897758484)
[2025-02-13 02:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:54][root][INFO] - Training Epoch: 1/2, step 1867/7134 completed (loss: 0.0324086993932724, acc: 0.9886506795883179)
[2025-02-13 02:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:54][root][INFO] - Training Epoch: 1/2, step 1868/7134 completed (loss: 0.07579280436038971, acc: 0.9768977165222168)
[2025-02-13 02:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:55][root][INFO] - Training Epoch: 1/2, step 1869/7134 completed (loss: 0.047191355377435684, acc: 0.9869494438171387)
[2025-02-13 02:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:55][root][INFO] - Training Epoch: 1/2, step 1870/7134 completed (loss: 0.02840523235499859, acc: 0.9921875)
[2025-02-13 02:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56][root][INFO] - Training Epoch: 1/2, step 1871/7134 completed (loss: 0.0863330215215683, acc: 0.9869494438171387)
[2025-02-13 02:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56][root][INFO] - Training Epoch: 1/2, step 1872/7134 completed (loss: 0.5996866226196289, acc: 0.889502763748169)
[2025-02-13 02:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56][root][INFO] - Training Epoch: 1/2, step 1873/7134 completed (loss: 0.14837646484375, acc: 0.95961993932724)
[2025-02-13 02:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:57][root][INFO] - Training Epoch: 1/2, step 1874/7134 completed (loss: 0.046240296214818954, acc: 0.993446946144104)
[2025-02-13 02:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:57][root][INFO] - Training Epoch: 1/2, step 1875/7134 completed (loss: 0.030644860118627548, acc: 0.9912663698196411)
[2025-02-13 02:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:58][root][INFO] - Training Epoch: 1/2, step 1876/7134 completed (loss: 0.16204755008220673, acc: 0.9575551748275757)
[2025-02-13 02:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:58][root][INFO] - Training Epoch: 1/2, step 1877/7134 completed (loss: 0.07108722627162933, acc: 0.9785894155502319)
[2025-02-13 02:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:59][root][INFO] - Training Epoch: 1/2, step 1878/7134 completed (loss: 0.07564815878868103, acc: 0.9799692034721375)
[2025-02-13 02:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:59][root][INFO] - Training Epoch: 1/2, step 1879/7134 completed (loss: 0.07792403548955917, acc: 0.9836868047714233)
[2025-02-13 02:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:00][root][INFO] - Training Epoch: 1/2, step 1880/7134 completed (loss: 0.0791744664311409, acc: 0.9798792600631714)
[2025-02-13 02:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:00][root][INFO] - Training Epoch: 1/2, step 1881/7134 completed (loss: 0.09731125086545944, acc: 0.9722222089767456)
[2025-02-13 02:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:00][root][INFO] - Training Epoch: 1/2, step 1882/7134 completed (loss: 0.08268086612224579, acc: 0.9753694534301758)
[2025-02-13 02:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01][root][INFO] - Training Epoch: 1/2, step 1883/7134 completed (loss: 0.06387437880039215, acc: 0.9783464670181274)
[2025-02-13 02:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01][root][INFO] - Training Epoch: 1/2, step 1884/7134 completed (loss: 0.09069371968507767, acc: 0.9699841737747192)
[2025-02-13 02:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:02][root][INFO] - Training Epoch: 1/2, step 1885/7134 completed (loss: 0.0746253952383995, acc: 0.9737336039543152)
[2025-02-13 02:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:02][root][INFO] - Training Epoch: 1/2, step 1886/7134 completed (loss: 0.09296102076768875, acc: 0.9728600978851318)
[2025-02-13 02:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:02][root][INFO] - Training Epoch: 1/2, step 1887/7134 completed (loss: 0.08482727408409119, acc: 0.9753320813179016)
[2025-02-13 02:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:03][root][INFO] - Training Epoch: 1/2, step 1888/7134 completed (loss: 0.08233332633972168, acc: 0.9808061122894287)
[2025-02-13 02:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:03][root][INFO] - Training Epoch: 1/2, step 1889/7134 completed (loss: 0.05228902027010918, acc: 0.9841040372848511)
[2025-02-13 02:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04][root][INFO] - Training Epoch: 1/2, step 1890/7134 completed (loss: 0.039604008197784424, acc: 0.9891641139984131)
[2025-02-13 02:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04][root][INFO] - Training Epoch: 1/2, step 1891/7134 completed (loss: 0.10770386457443237, acc: 0.9691558480262756)
[2025-02-13 02:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:05][root][INFO] - Training Epoch: 1/2, step 1892/7134 completed (loss: 0.07665304839611053, acc: 0.9783616662025452)
[2025-02-13 02:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:05][root][INFO] - Training Epoch: 1/2, step 1893/7134 completed (loss: 0.04554925486445427, acc: 0.9828392863273621)
[2025-02-13 02:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06][root][INFO] - Training Epoch: 1/2, step 1894/7134 completed (loss: 0.11107455939054489, acc: 0.9774305820465088)
[2025-02-13 02:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06][root][INFO] - Training Epoch: 1/2, step 1895/7134 completed (loss: 0.26040494441986084, acc: 0.9477124214172363)
[2025-02-13 02:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06][root][INFO] - Training Epoch: 1/2, step 1896/7134 completed (loss: 0.07169830054044724, acc: 0.9788235425949097)
[2025-02-13 02:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:07][root][INFO] - Training Epoch: 1/2, step 1897/7134 completed (loss: 0.024415433406829834, acc: 0.9925925731658936)
[2025-02-13 02:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:07][root][INFO] - Training Epoch: 1/2, step 1898/7134 completed (loss: 0.041657593101263046, acc: 0.9850746393203735)
[2025-02-13 02:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08][root][INFO] - Training Epoch: 1/2, step 1899/7134 completed (loss: 0.052932851016521454, acc: 0.9855334758758545)
[2025-02-13 02:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08][root][INFO] - Training Epoch: 1/2, step 1900/7134 completed (loss: 0.027291307225823402, acc: 0.9951456189155579)
[2025-02-13 02:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08][root][INFO] - Training Epoch: 1/2, step 1901/7134 completed (loss: 0.06604225188493729, acc: 0.9872029423713684)
[2025-02-13 02:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:09][root][INFO] - Training Epoch: 1/2, step 1902/7134 completed (loss: 0.041165225207805634, acc: 0.9912126660346985)
[2025-02-13 02:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:09][root][INFO] - Training Epoch: 1/2, step 1903/7134 completed (loss: 0.03233940526843071, acc: 0.9895833134651184)
[2025-02-13 02:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:10][root][INFO] - Training Epoch: 1/2, step 1904/7134 completed (loss: 0.04115673527121544, acc: 0.9851694703102112)
[2025-02-13 02:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:10][root][INFO] - Training Epoch: 1/2, step 1905/7134 completed (loss: 0.05094151571393013, acc: 0.987730085849762)
[2025-02-13 02:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11][root][INFO] - Training Epoch: 1/2, step 1906/7134 completed (loss: 0.056760985404253006, acc: 0.9805068373680115)
[2025-02-13 02:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11][root][INFO] - Training Epoch: 1/2, step 1907/7134 completed (loss: 0.052016597241163254, acc: 0.9848739504814148)
[2025-02-13 02:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11][root][INFO] - Training Epoch: 1/2, step 1908/7134 completed (loss: 0.049696579575538635, acc: 0.989258885383606)
[2025-02-13 02:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:12][root][INFO] - Training Epoch: 1/2, step 1909/7134 completed (loss: 0.05617690086364746, acc: 0.9827814698219299)
[2025-02-13 02:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:12][root][INFO] - Training Epoch: 1/2, step 1910/7134 completed (loss: 0.05301462113857269, acc: 0.984649121761322)
[2025-02-13 02:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13][root][INFO] - Training Epoch: 1/2, step 1911/7134 completed (loss: 0.04253177344799042, acc: 0.987864077091217)
[2025-02-13 02:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13][root][INFO] - Training Epoch: 1/2, step 1912/7134 completed (loss: 0.030667439103126526, acc: 0.9906790852546692)
[2025-02-13 02:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:14][root][INFO] - Training Epoch: 1/2, step 1913/7134 completed (loss: 0.06059619039297104, acc: 0.9851577281951904)
[2025-02-13 02:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:14][root][INFO] - Training Epoch: 1/2, step 1914/7134 completed (loss: 0.06797631829977036, acc: 0.9864681959152222)
[2025-02-13 02:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15][root][INFO] - Training Epoch: 1/2, step 1915/7134 completed (loss: 0.055234551429748535, acc: 0.9885807633399963)
[2025-02-13 02:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15][root][INFO] - Training Epoch: 1/2, step 1916/7134 completed (loss: 0.03953411430120468, acc: 0.9841656684875488)
[2025-02-13 02:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15][root][INFO] - Training Epoch: 1/2, step 1917/7134 completed (loss: 0.04683208838105202, acc: 0.9873949289321899)
[2025-02-13 02:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:16][root][INFO] - Training Epoch: 1/2, step 1918/7134 completed (loss: 0.05133523792028427, acc: 0.9905956387519836)
[2025-02-13 02:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:16][root][INFO] - Training Epoch: 1/2, step 1919/7134 completed (loss: 0.05113893002271652, acc: 0.9856528043746948)
[2025-02-13 02:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:17][root][INFO] - Training Epoch: 1/2, step 1920/7134 completed (loss: 0.039863403886556625, acc: 0.9891975522041321)
[2025-02-13 02:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:17][root][INFO] - Training Epoch: 1/2, step 1921/7134 completed (loss: 0.05040661245584488, acc: 0.9847645163536072)
[2025-02-13 02:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18][root][INFO] - Training Epoch: 1/2, step 1922/7134 completed (loss: 0.035678621381521225, acc: 0.9910537004470825)
[2025-02-13 02:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18][root][INFO] - Training Epoch: 1/2, step 1923/7134 completed (loss: 0.041134029626846313, acc: 0.9899497628211975)
[2025-02-13 02:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:19][root][INFO] - Training Epoch: 1/2, step 1924/7134 completed (loss: 0.05396445840597153, acc: 0.9849785566329956)
[2025-02-13 02:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:19][root][INFO] - Training Epoch: 1/2, step 1925/7134 completed (loss: 0.0738716870546341, acc: 0.9809523820877075)
[2025-02-13 02:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:19][root][INFO] - Training Epoch: 1/2, step 1926/7134 completed (loss: 0.018894542008638382, acc: 0.9928498268127441)
[2025-02-13 02:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20][root][INFO] - Training Epoch: 1/2, step 1927/7134 completed (loss: 0.06199134141206741, acc: 0.9845303893089294)
[2025-02-13 02:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20][root][INFO] - Training Epoch: 1/2, step 1928/7134 completed (loss: 0.03816584497690201, acc: 0.9924012422561646)
[2025-02-13 02:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:21][root][INFO] - Training Epoch: 1/2, step 1929/7134 completed (loss: 0.024086106568574905, acc: 0.994350254535675)
[2025-02-13 02:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:21][root][INFO] - Training Epoch: 1/2, step 1930/7134 completed (loss: 0.06631386280059814, acc: 0.9790025949478149)
[2025-02-13 02:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22][root][INFO] - Training Epoch: 1/2, step 1931/7134 completed (loss: 0.09985793381929398, acc: 0.9734513163566589)
[2025-02-13 02:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22][root][INFO] - Training Epoch: 1/2, step 1932/7134 completed (loss: 0.09432218968868256, acc: 0.9731343388557434)
[2025-02-13 02:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:23][root][INFO] - Training Epoch: 1/2, step 1933/7134 completed (loss: 0.15077006816864014, acc: 0.970588207244873)
[2025-02-13 02:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:23][root][INFO] - Training Epoch: 1/2, step 1934/7134 completed (loss: 0.039109136909246445, acc: 0.9901130199432373)
[2025-02-13 02:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:24][root][INFO] - Training Epoch: 1/2, step 1935/7134 completed (loss: 0.10809776186943054, acc: 0.9752547144889832)
[2025-02-13 02:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:24][root][INFO] - Training Epoch: 1/2, step 1936/7134 completed (loss: 0.027517329901456833, acc: 0.9952606558799744)
[2025-02-13 02:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:24][root][INFO] - Training Epoch: 1/2, step 1937/7134 completed (loss: 0.05349436029791832, acc: 0.9837662577629089)
[2025-02-13 02:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25][root][INFO] - Training Epoch: 1/2, step 1938/7134 completed (loss: 0.07954050600528717, acc: 0.9800570011138916)
[2025-02-13 02:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25][root][INFO] - Training Epoch: 1/2, step 1939/7134 completed (loss: 0.0683639794588089, acc: 0.9809321761131287)
[2025-02-13 02:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:26][root][INFO] - Training Epoch: 1/2, step 1940/7134 completed (loss: 0.0921623706817627, acc: 0.9727891087532043)
[2025-02-13 02:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:26][root][INFO] - Training Epoch: 1/2, step 1941/7134 completed (loss: 0.10183097422122955, acc: 0.9732620120048523)
[2025-02-13 02:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27][root][INFO] - Training Epoch: 1/2, step 1942/7134 completed (loss: 0.06064338609576225, acc: 0.9837398529052734)
[2025-02-13 02:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27][root][INFO] - Training Epoch: 1/2, step 1943/7134 completed (loss: 0.12300097942352295, acc: 0.964634120464325)
[2025-02-13 02:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:28][root][INFO] - Training Epoch: 1/2, step 1944/7134 completed (loss: 0.07506503909826279, acc: 0.9798902869224548)
[2025-02-13 02:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:28][root][INFO] - Training Epoch: 1/2, step 1945/7134 completed (loss: 0.060731153935194016, acc: 0.9783464670181274)
[2025-02-13 02:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:28][root][INFO] - Training Epoch: 1/2, step 1946/7134 completed (loss: 0.09414525330066681, acc: 0.971137523651123)
[2025-02-13 02:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29][root][INFO] - Training Epoch: 1/2, step 1947/7134 completed (loss: 0.07727217674255371, acc: 0.9776315689086914)
[2025-02-13 02:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29][root][INFO] - Training Epoch: 1/2, step 1948/7134 completed (loss: 0.1354106217622757, acc: 0.9677419066429138)
[2025-02-13 02:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:30][root][INFO] - Training Epoch: 1/2, step 1949/7134 completed (loss: 0.052659403532743454, acc: 0.9866220951080322)
[2025-02-13 02:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:30][root][INFO] - Training Epoch: 1/2, step 1950/7134 completed (loss: 0.14060264825820923, acc: 0.974452555179596)
[2025-02-13 02:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31][root][INFO] - Training Epoch: 1/2, step 1951/7134 completed (loss: 0.06811279058456421, acc: 0.9849498271942139)
[2025-02-13 02:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31][root][INFO] - Training Epoch: 1/2, step 1952/7134 completed (loss: 0.07869674265384674, acc: 0.981697142124176)
[2025-02-13 02:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:32][root][INFO] - Training Epoch: 1/2, step 1953/7134 completed (loss: 0.054929591715335846, acc: 0.9829721450805664)
[2025-02-13 02:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:32][root][INFO] - Training Epoch: 1/2, step 1954/7134 completed (loss: 0.06119865924119949, acc: 0.9829268455505371)
[2025-02-13 02:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:32][root][INFO] - Training Epoch: 1/2, step 1955/7134 completed (loss: 0.056049954146146774, acc: 0.982425332069397)
[2025-02-13 02:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33][root][INFO] - Training Epoch: 1/2, step 1956/7134 completed (loss: 0.03796134516596794, acc: 0.9892473220825195)
[2025-02-13 02:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33][root][INFO] - Training Epoch: 1/2, step 1957/7134 completed (loss: 0.05133083462715149, acc: 0.9870370626449585)
[2025-02-13 02:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:34][root][INFO] - Training Epoch: 1/2, step 1958/7134 completed (loss: 0.05240681767463684, acc: 0.9885386824607849)
[2025-02-13 02:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:34][root][INFO] - Training Epoch: 1/2, step 1959/7134 completed (loss: 0.02372847869992256, acc: 0.9978678226470947)
[2025-02-13 02:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:34][root][INFO] - Training Epoch: 1/2, step 1960/7134 completed (loss: 0.02796720154583454, acc: 0.9887217879295349)
[2025-02-13 02:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35][root][INFO] - Training Epoch: 1/2, step 1961/7134 completed (loss: 0.030358508229255676, acc: 0.9899665713310242)
[2025-02-13 02:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35][root][INFO] - Training Epoch: 1/2, step 1962/7134 completed (loss: 0.056493744254112244, acc: 0.9888268113136292)
[2025-02-13 02:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:36][root][INFO] - Training Epoch: 1/2, step 1963/7134 completed (loss: 0.06025370955467224, acc: 0.9740932583808899)
[2025-02-13 02:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:36][root][INFO] - Training Epoch: 1/2, step 1964/7134 completed (loss: 0.06266763806343079, acc: 0.9829059839248657)
[2025-02-13 02:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:36][root][INFO] - Training Epoch: 1/2, step 1965/7134 completed (loss: 0.03437425196170807, acc: 0.991631805896759)
[2025-02-13 02:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:37][root][INFO] - Training Epoch: 1/2, step 1966/7134 completed (loss: 0.04797575995326042, acc: 0.9877675771713257)
[2025-02-13 02:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:37][root][INFO] - Training Epoch: 1/2, step 1967/7134 completed (loss: 0.08352396637201309, acc: 0.9821656346321106)
[2025-02-13 02:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:38][root][INFO] - Training Epoch: 1/2, step 1968/7134 completed (loss: 0.031000781804323196, acc: 0.990123450756073)
[2025-02-13 02:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:38][root][INFO] - Training Epoch: 1/2, step 1969/7134 completed (loss: 0.07466805726289749, acc: 0.9786666631698608)
[2025-02-13 02:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39][root][INFO] - Training Epoch: 1/2, step 1970/7134 completed (loss: 0.04001784324645996, acc: 0.987730085849762)
[2025-02-13 02:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39][root][INFO] - Training Epoch: 1/2, step 1971/7134 completed (loss: 0.047318290919065475, acc: 0.9854111671447754)
[2025-02-13 02:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40][root][INFO] - Training Epoch: 1/2, step 1972/7134 completed (loss: 0.20197519659996033, acc: 0.9385749101638794)
[2025-02-13 02:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40][root][INFO] - Training Epoch: 1/2, step 1973/7134 completed (loss: 0.09944609552621841, acc: 0.9695237874984741)
[2025-02-13 02:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40][root][INFO] - Training Epoch: 1/2, step 1974/7134 completed (loss: 0.05040060356259346, acc: 0.9824120402336121)
[2025-02-13 02:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:41][root][INFO] - Training Epoch: 1/2, step 1975/7134 completed (loss: 0.046085234731435776, acc: 0.9924242496490479)
[2025-02-13 02:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:41][root][INFO] - Training Epoch: 1/2, step 1976/7134 completed (loss: 0.04507854953408241, acc: 0.9864099621772766)
[2025-02-13 02:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42][root][INFO] - Training Epoch: 1/2, step 1977/7134 completed (loss: 0.07562050223350525, acc: 0.9797101616859436)
[2025-02-13 02:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42][root][INFO] - Training Epoch: 1/2, step 1978/7134 completed (loss: 0.024665694683790207, acc: 0.990604043006897)
[2025-02-13 02:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:43][root][INFO] - Training Epoch: 1/2, step 1979/7134 completed (loss: 0.041287168860435486, acc: 0.98828125)
[2025-02-13 02:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:43][root][INFO] - Training Epoch: 1/2, step 1980/7134 completed (loss: 0.11007466912269592, acc: 0.9726206064224243)
[2025-02-13 02:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44][root][INFO] - Training Epoch: 1/2, step 1981/7134 completed (loss: 0.04036736115813255, acc: 0.9874125719070435)
[2025-02-13 02:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44][root][INFO] - Training Epoch: 1/2, step 1982/7134 completed (loss: 0.03273188695311546, acc: 0.9884169697761536)
[2025-02-13 02:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44][root][INFO] - Training Epoch: 1/2, step 1983/7134 completed (loss: 0.024451322853565216, acc: 0.9949109554290771)
[2025-02-13 02:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:45][root][INFO] - Training Epoch: 1/2, step 1984/7134 completed (loss: 0.018505912274122238, acc: 0.9946164488792419)
[2025-02-13 02:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:45][root][INFO] - Training Epoch: 1/2, step 1985/7134 completed (loss: 0.039580367505550385, acc: 0.9874652028083801)
[2025-02-13 02:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46][root][INFO] - Training Epoch: 1/2, step 1986/7134 completed (loss: 0.0649573802947998, acc: 0.9832689762115479)
[2025-02-13 02:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46][root][INFO] - Training Epoch: 1/2, step 1987/7134 completed (loss: 0.0798845887184143, acc: 0.9830028414726257)
[2025-02-13 02:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:47][root][INFO] - Training Epoch: 1/2, step 1988/7134 completed (loss: 0.054744474589824677, acc: 0.980028510093689)
[2025-02-13 02:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:47][root][INFO] - Training Epoch: 1/2, step 1989/7134 completed (loss: 0.0714038759469986, acc: 0.9840510487556458)
[2025-02-13 02:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48][root][INFO] - Training Epoch: 1/2, step 1990/7134 completed (loss: 0.04354005679488182, acc: 0.9890776872634888)
[2025-02-13 02:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48][root][INFO] - Training Epoch: 1/2, step 1991/7134 completed (loss: 0.029472591355443, acc: 0.9900332093238831)
[2025-02-13 02:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48][root][INFO] - Training Epoch: 1/2, step 1992/7134 completed (loss: 0.06549423187971115, acc: 0.9844961166381836)
[2025-02-13 02:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:49][root][INFO] - Training Epoch: 1/2, step 1993/7134 completed (loss: 0.025082817301154137, acc: 0.9929701089859009)
[2025-02-13 02:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:49][root][INFO] - Training Epoch: 1/2, step 1994/7134 completed (loss: 0.022753361612558365, acc: 0.9879310131072998)
[2025-02-13 02:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:50][root][INFO] - Training Epoch: 1/2, step 1995/7134 completed (loss: 0.03438706323504448, acc: 0.9876352548599243)
[2025-02-13 02:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:50][root][INFO] - Training Epoch: 1/2, step 1996/7134 completed (loss: 0.0317302830517292, acc: 0.9900826215744019)
[2025-02-13 02:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51][root][INFO] - Training Epoch: 1/2, step 1997/7134 completed (loss: 0.025578763335943222, acc: 0.9941775798797607)
[2025-02-13 02:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51][root][INFO] - Training Epoch: 1/2, step 1998/7134 completed (loss: 0.028957759961485863, acc: 0.9890109896659851)
[2025-02-13 02:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51][root][INFO] - Training Epoch: 1/2, step 1999/7134 completed (loss: 0.0417795404791832, acc: 0.9864176511764526)
[2025-02-13 02:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:52][root][INFO] - Training Epoch: 1/2, step 2000/7134 completed (loss: 0.0381854847073555, acc: 0.9905362725257874)
[2025-02-13 02:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:52][root][INFO] - Training Epoch: 1/2, step 2001/7134 completed (loss: 0.024588923901319504, acc: 0.9939393997192383)
[2025-02-13 02:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:53][root][INFO] - Training Epoch: 1/2, step 2002/7134 completed (loss: 0.04159647226333618, acc: 0.9878261089324951)
[2025-02-13 02:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:53][root][INFO] - Training Epoch: 1/2, step 2003/7134 completed (loss: 0.023372070863842964, acc: 0.9916666746139526)
[2025-02-13 02:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:54][root][INFO] - Training Epoch: 1/2, step 2004/7134 completed (loss: 0.22524119913578033, acc: 0.9514768123626709)
[2025-02-13 02:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:54][root][INFO] - Training Epoch: 1/2, step 2005/7134 completed (loss: 0.1597055196762085, acc: 0.9698629975318909)
[2025-02-13 02:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:54][root][INFO] - Training Epoch: 1/2, step 2006/7134 completed (loss: 0.02247609570622444, acc: 0.9942362904548645)
[2025-02-13 02:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55][root][INFO] - Training Epoch: 1/2, step 2007/7134 completed (loss: 0.08446186780929565, acc: 0.9819276928901672)
[2025-02-13 02:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55][root][INFO] - Training Epoch: 1/2, step 2008/7134 completed (loss: 0.048487186431884766, acc: 0.9857650995254517)
[2025-02-13 02:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:56][root][INFO] - Training Epoch: 1/2, step 2009/7134 completed (loss: 0.04742075130343437, acc: 0.9909747242927551)
[2025-02-13 02:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:56][root][INFO] - Training Epoch: 1/2, step 2010/7134 completed (loss: 0.034387703984975815, acc: 0.9879931211471558)
[2025-02-13 02:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:56][root][INFO] - Training Epoch: 1/2, step 2011/7134 completed (loss: 0.029511813074350357, acc: 0.9921721816062927)
[2025-02-13 02:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:57][root][INFO] - Training Epoch: 1/2, step 2012/7134 completed (loss: 0.05236784368753433, acc: 0.985200822353363)
[2025-02-13 02:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:57][root][INFO] - Training Epoch: 1/2, step 2013/7134 completed (loss: 0.02986302599310875, acc: 0.9900000095367432)
[2025-02-13 02:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:58][root][INFO] - Training Epoch: 1/2, step 2014/7134 completed (loss: 0.030989855527877808, acc: 0.98740154504776)
[2025-02-13 02:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:58][root][INFO] - Training Epoch: 1/2, step 2015/7134 completed (loss: 0.03783421590924263, acc: 0.9901315569877625)
[2025-02-13 02:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59][root][INFO] - Training Epoch: 1/2, step 2016/7134 completed (loss: 0.06297032535076141, acc: 0.9846153855323792)
[2025-02-13 02:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59][root][INFO] - Training Epoch: 1/2, step 2017/7134 completed (loss: 0.07796469330787659, acc: 0.9837837815284729)
[2025-02-13 02:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59][root][INFO] - Training Epoch: 1/2, step 2018/7134 completed (loss: 0.02726285718381405, acc: 0.9910256266593933)
[2025-02-13 02:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:00][root][INFO] - Training Epoch: 1/2, step 2019/7134 completed (loss: 0.051676176488399506, acc: 0.9862068891525269)
[2025-02-13 02:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:00][root][INFO] - Training Epoch: 1/2, step 2020/7134 completed (loss: 0.013499501161277294, acc: 1.0)
[2025-02-13 02:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:01][root][INFO] - Training Epoch: 1/2, step 2021/7134 completed (loss: 0.06375693529844284, acc: 0.983116865158081)
[2025-02-13 02:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:01][root][INFO] - Training Epoch: 1/2, step 2022/7134 completed (loss: 0.049599539488554, acc: 0.9878683090209961)
[2025-02-13 02:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02][root][INFO] - Training Epoch: 1/2, step 2023/7134 completed (loss: 0.060849983245134354, acc: 0.9839416146278381)
[2025-02-13 02:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02][root][INFO] - Training Epoch: 1/2, step 2024/7134 completed (loss: 0.05321059748530388, acc: 0.9833333492279053)
[2025-02-13 02:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:03][root][INFO] - Training Epoch: 1/2, step 2025/7134 completed (loss: 0.03529250994324684, acc: 0.9906666874885559)
[2025-02-13 02:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:03][root][INFO] - Training Epoch: 1/2, step 2026/7134 completed (loss: 0.07162011414766312, acc: 0.9819276928901672)
[2025-02-13 02:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:03][root][INFO] - Training Epoch: 1/2, step 2027/7134 completed (loss: 0.05070297047495842, acc: 0.9817276000976562)
[2025-02-13 02:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04][root][INFO] - Training Epoch: 1/2, step 2028/7134 completed (loss: 0.0446641780436039, acc: 0.9853801131248474)
[2025-02-13 02:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04][root][INFO] - Training Epoch: 1/2, step 2029/7134 completed (loss: 0.06354314833879471, acc: 0.9786931872367859)
[2025-02-13 02:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:05][root][INFO] - Training Epoch: 1/2, step 2030/7134 completed (loss: 0.06107820197939873, acc: 0.9785276055335999)
[2025-02-13 02:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:05][root][INFO] - Training Epoch: 1/2, step 2031/7134 completed (loss: 0.04867943748831749, acc: 0.9889570474624634)
[2025-02-13 02:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06][root][INFO] - Training Epoch: 1/2, step 2032/7134 completed (loss: 0.041512228548526764, acc: 0.9905914068222046)
[2025-02-13 02:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06][root][INFO] - Training Epoch: 1/2, step 2033/7134 completed (loss: 0.04914960637688637, acc: 0.9879931211471558)
[2025-02-13 02:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06][root][INFO] - Training Epoch: 1/2, step 2034/7134 completed (loss: 0.06187469884753227, acc: 0.9831546545028687)
[2025-02-13 02:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:07][root][INFO] - Training Epoch: 1/2, step 2035/7134 completed (loss: 0.028696661815047264, acc: 0.9925280213356018)
[2025-02-13 02:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:07][root][INFO] - Training Epoch: 1/2, step 2036/7134 completed (loss: 0.045897047966718674, acc: 0.991349458694458)
[2025-02-13 02:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08][root][INFO] - Training Epoch: 1/2, step 2037/7134 completed (loss: 0.01932913064956665, acc: 0.9925816059112549)
[2025-02-13 02:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08][root][INFO] - Training Epoch: 1/2, step 2038/7134 completed (loss: 0.0627051442861557, acc: 0.986997663974762)
[2025-02-13 02:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:09][root][INFO] - Training Epoch: 1/2, step 2039/7134 completed (loss: 0.008482151664793491, acc: 1.0)
[2025-02-13 02:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:09][root][INFO] - Training Epoch: 1/2, step 2040/7134 completed (loss: 0.0610867440700531, acc: 0.9897119402885437)
[2025-02-13 02:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:09][root][INFO] - Training Epoch: 1/2, step 2041/7134 completed (loss: 0.015416419133543968, acc: 0.9931318759918213)
[2025-02-13 02:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:10][root][INFO] - Training Epoch: 1/2, step 2042/7134 completed (loss: 0.019870808348059654, acc: 0.9959677457809448)
[2025-02-13 02:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:10][root][INFO] - Training Epoch: 1/2, step 2043/7134 completed (loss: 0.0378737598657608, acc: 0.989062488079071)
[2025-02-13 02:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11][root][INFO] - Training Epoch: 1/2, step 2044/7134 completed (loss: 0.03372291848063469, acc: 0.9928443431854248)
[2025-02-13 02:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11][root][INFO] - Training Epoch: 1/2, step 2045/7134 completed (loss: 0.04923784360289574, acc: 0.9837251305580139)
[2025-02-13 02:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:12][root][INFO] - Training Epoch: 1/2, step 2046/7134 completed (loss: 0.03667405992746353, acc: 0.9837398529052734)
[2025-02-13 02:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:12][root][INFO] - Training Epoch: 1/2, step 2047/7134 completed (loss: 0.03728150203824043, acc: 0.9866443872451782)
[2025-02-13 02:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:12][root][INFO] - Training Epoch: 1/2, step 2048/7134 completed (loss: 0.05182980000972748, acc: 0.988041877746582)
[2025-02-13 02:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:13][root][INFO] - Training Epoch: 1/2, step 2049/7134 completed (loss: 0.08026861399412155, acc: 0.9717646837234497)
[2025-02-13 02:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:13][root][INFO] - Training Epoch: 1/2, step 2050/7134 completed (loss: 0.0986374095082283, acc: 0.9829059839248657)
[2025-02-13 02:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14][root][INFO] - Training Epoch: 1/2, step 2051/7134 completed (loss: 0.06628242135047913, acc: 0.98046875)
[2025-02-13 02:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14][root][INFO] - Training Epoch: 1/2, step 2052/7134 completed (loss: 0.09659445285797119, acc: 0.9778156876564026)
[2025-02-13 02:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:15][root][INFO] - Training Epoch: 1/2, step 2053/7134 completed (loss: 0.05075320973992348, acc: 0.984000027179718)
[2025-02-13 02:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:15][root][INFO] - Training Epoch: 1/2, step 2054/7134 completed (loss: 0.11602360755205154, acc: 0.9697452187538147)
[2025-02-13 02:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:15][root][INFO] - Training Epoch: 1/2, step 2055/7134 completed (loss: 0.0905839279294014, acc: 0.9795275330543518)
[2025-02-13 02:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16][root][INFO] - Training Epoch: 1/2, step 2056/7134 completed (loss: 0.06328269839286804, acc: 0.9818417429924011)
[2025-02-13 02:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16][root][INFO] - Training Epoch: 1/2, step 2057/7134 completed (loss: 0.06792621314525604, acc: 0.9823608994483948)
[2025-02-13 02:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:17][root][INFO] - Training Epoch: 1/2, step 2058/7134 completed (loss: 0.1292387843132019, acc: 0.9713056087493896)
[2025-02-13 02:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:17][root][INFO] - Training Epoch: 1/2, step 2059/7134 completed (loss: 0.10878700017929077, acc: 0.9792027473449707)
[2025-02-13 02:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:18][root][INFO] - Training Epoch: 1/2, step 2060/7134 completed (loss: 0.05666524916887283, acc: 0.9828326106071472)
[2025-02-13 02:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:18][root][INFO] - Training Epoch: 1/2, step 2061/7134 completed (loss: 0.08369073271751404, acc: 0.9787535667419434)
[2025-02-13 02:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19][root][INFO] - Training Epoch: 1/2, step 2062/7134 completed (loss: 0.09146029502153397, acc: 0.9753320813179016)
[2025-02-13 02:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19][root][INFO] - Training Epoch: 1/2, step 2063/7134 completed (loss: 0.08605387806892395, acc: 0.97947758436203)
[2025-02-13 02:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19][root][INFO] - Training Epoch: 1/2, step 2064/7134 completed (loss: 0.02578635886311531, acc: 0.9921875)
[2025-02-13 02:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:20][root][INFO] - Training Epoch: 1/2, step 2065/7134 completed (loss: 0.08259394019842148, acc: 0.977952778339386)
[2025-02-13 02:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:20][root][INFO] - Training Epoch: 1/2, step 2066/7134 completed (loss: 0.059104934334754944, acc: 0.9839679598808289)
[2025-02-13 02:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:21][root][INFO] - Training Epoch: 1/2, step 2067/7134 completed (loss: 0.042669158428907394, acc: 0.9848197102546692)
[2025-02-13 02:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:21][root][INFO] - Training Epoch: 1/2, step 2068/7134 completed (loss: 0.06970006972551346, acc: 0.9789227247238159)
[2025-02-13 02:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:21][root][INFO] - Training Epoch: 1/2, step 2069/7134 completed (loss: 0.04687467962503433, acc: 0.9830769300460815)
[2025-02-13 02:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22][root][INFO] - Training Epoch: 1/2, step 2070/7134 completed (loss: 0.050475530326366425, acc: 0.9885931611061096)
[2025-02-13 02:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22][root][INFO] - Training Epoch: 1/2, step 2071/7134 completed (loss: 0.020940406247973442, acc: 0.9929742217063904)
[2025-02-13 02:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:23][root][INFO] - Training Epoch: 1/2, step 2072/7134 completed (loss: 0.03706202283501625, acc: 0.9900990128517151)
[2025-02-13 02:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:23][root][INFO] - Training Epoch: 1/2, step 2073/7134 completed (loss: 0.039136845618486404, acc: 0.9903846383094788)
[2025-02-13 02:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:24][root][INFO] - Training Epoch: 1/2, step 2074/7134 completed (loss: 0.04973817989230156, acc: 0.987089216709137)
[2025-02-13 02:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:24][root][INFO] - Training Epoch: 1/2, step 2075/7134 completed (loss: 0.0661318302154541, acc: 0.9699499011039734)
[2025-02-13 02:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:24][root][INFO] - Training Epoch: 1/2, step 2076/7134 completed (loss: 0.04058108106255531, acc: 0.9861538410186768)
[2025-02-13 02:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25][root][INFO] - Training Epoch: 1/2, step 2077/7134 completed (loss: 0.040450360625982285, acc: 0.9853917956352234)
[2025-02-13 02:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25][root][INFO] - Training Epoch: 1/2, step 2078/7134 completed (loss: 0.032733332365751266, acc: 0.9918604493141174)
[2025-02-13 02:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:26][root][INFO] - Training Epoch: 1/2, step 2079/7134 completed (loss: 0.03317469730973244, acc: 0.9841269850730896)
[2025-02-13 02:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:26][root][INFO] - Training Epoch: 1/2, step 2080/7134 completed (loss: 0.040440287441015244, acc: 0.9911764860153198)
[2025-02-13 02:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:27][root][INFO] - Training Epoch: 1/2, step 2081/7134 completed (loss: 0.02012089267373085, acc: 0.994194507598877)
[2025-02-13 02:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:27][root][INFO] - Training Epoch: 1/2, step 2082/7134 completed (loss: 0.09705179929733276, acc: 0.9769392013549805)
[2025-02-13 02:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:27][root][INFO] - Training Epoch: 1/2, step 2083/7134 completed (loss: 0.045497845858335495, acc: 0.9925925731658936)
[2025-02-13 02:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28][root][INFO] - Training Epoch: 1/2, step 2084/7134 completed (loss: 0.03769388422369957, acc: 0.989130437374115)
[2025-02-13 02:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28][root][INFO] - Training Epoch: 1/2, step 2085/7134 completed (loss: 0.02554580196738243, acc: 0.9926062822341919)
[2025-02-13 02:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:29][root][INFO] - Training Epoch: 1/2, step 2086/7134 completed (loss: 0.026654373854398727, acc: 0.9931507110595703)
[2025-02-13 02:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:29][root][INFO] - Training Epoch: 1/2, step 2087/7134 completed (loss: 0.009583307430148125, acc: 1.0)
[2025-02-13 02:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30][root][INFO] - Training Epoch: 1/2, step 2088/7134 completed (loss: 0.03546169772744179, acc: 0.9884726405143738)
[2025-02-13 02:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30][root][INFO] - Training Epoch: 1/2, step 2089/7134 completed (loss: 0.026778558269143105, acc: 0.9921135902404785)
[2025-02-13 02:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30][root][INFO] - Training Epoch: 1/2, step 2090/7134 completed (loss: 0.020223703235387802, acc: 0.9952606558799744)
[2025-02-13 02:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31][root][INFO] - Training Epoch: 1/2, step 2091/7134 completed (loss: 0.035909783095121384, acc: 0.9898648858070374)
[2025-02-13 02:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31][root][INFO] - Training Epoch: 1/2, step 2092/7134 completed (loss: 0.020619399845600128, acc: 0.9936908483505249)
[2025-02-13 02:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:32][root][INFO] - Training Epoch: 1/2, step 2093/7134 completed (loss: 0.029261531308293343, acc: 0.9918588995933533)
[2025-02-13 02:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:32][root][INFO] - Training Epoch: 1/2, step 2094/7134 completed (loss: 0.07327406853437424, acc: 0.9837728142738342)
[2025-02-13 02:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33][root][INFO] - Training Epoch: 1/2, step 2095/7134 completed (loss: 0.04685967043042183, acc: 0.9881266355514526)
[2025-02-13 02:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33][root][INFO] - Training Epoch: 1/2, step 2096/7134 completed (loss: 0.0351669117808342, acc: 0.9913793206214905)
[2025-02-13 02:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33][root][INFO] - Training Epoch: 1/2, step 2097/7134 completed (loss: 0.024398453533649445, acc: 0.991584837436676)
[2025-02-13 02:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34][root][INFO] - Training Epoch: 1/2, step 2098/7134 completed (loss: 0.0277793500572443, acc: 0.9919614195823669)
[2025-02-13 02:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34][root][INFO] - Training Epoch: 1/2, step 2099/7134 completed (loss: 0.03236253187060356, acc: 0.9908257126808167)
[2025-02-13 02:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:35][root][INFO] - Training Epoch: 1/2, step 2100/7134 completed (loss: 0.03322390839457512, acc: 0.9907038807868958)
[2025-02-13 02:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:35][root][INFO] - Training Epoch: 1/2, step 2101/7134 completed (loss: 0.028616271913051605, acc: 0.991183876991272)
[2025-02-13 02:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36][root][INFO] - Training Epoch: 1/2, step 2102/7134 completed (loss: 0.043106429278850555, acc: 0.9859693646430969)
[2025-02-13 02:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36][root][INFO] - Training Epoch: 1/2, step 2103/7134 completed (loss: 0.040215715765953064, acc: 0.9861963391304016)
[2025-02-13 02:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:37][root][INFO] - Training Epoch: 1/2, step 2104/7134 completed (loss: 0.024027515202760696, acc: 0.9922680258750916)
[2025-02-13 02:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:37][root][INFO] - Training Epoch: 1/2, step 2105/7134 completed (loss: 0.057713523507118225, acc: 0.9812206625938416)
[2025-02-13 02:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:37][root][INFO] - Training Epoch: 1/2, step 2106/7134 completed (loss: 0.03768418729305267, acc: 0.9856321811676025)
[2025-02-13 02:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:38][root][INFO] - Training Epoch: 1/2, step 2107/7134 completed (loss: 0.03710663318634033, acc: 0.9905533194541931)
[2025-02-13 02:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:38][root][INFO] - Training Epoch: 1/2, step 2108/7134 completed (loss: 0.04618852213025093, acc: 0.9835575222969055)
[2025-02-13 02:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:39][root][INFO] - Training Epoch: 1/2, step 2109/7134 completed (loss: 0.09546291828155518, acc: 0.9841269850730896)
[2025-02-13 02:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:39][root][INFO] - Training Epoch: 1/2, step 2110/7134 completed (loss: 0.07980983704328537, acc: 0.9781976938247681)
[2025-02-13 02:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40][root][INFO] - Training Epoch: 1/2, step 2111/7134 completed (loss: 0.056161023676395416, acc: 0.9831387996673584)
[2025-02-13 02:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40][root][INFO] - Training Epoch: 1/2, step 2112/7134 completed (loss: 0.11091480404138565, acc: 0.9713375568389893)
[2025-02-13 02:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40][root][INFO] - Training Epoch: 1/2, step 2113/7134 completed (loss: 0.028802303597331047, acc: 0.992548406124115)
[2025-02-13 02:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:41][root][INFO] - Training Epoch: 1/2, step 2114/7134 completed (loss: 0.02817213535308838, acc: 0.9895424842834473)
[2025-02-13 02:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:41][root][INFO] - Training Epoch: 1/2, step 2115/7134 completed (loss: 0.04059024155139923, acc: 0.9887359142303467)
[2025-02-13 02:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42][root][INFO] - Training Epoch: 1/2, step 2116/7134 completed (loss: 0.06438842415809631, acc: 0.9861963391304016)
[2025-02-13 02:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42][root][INFO] - Training Epoch: 1/2, step 2117/7134 completed (loss: 0.030575741082429886, acc: 0.9925373196601868)
[2025-02-13 02:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:43][root][INFO] - Training Epoch: 1/2, step 2118/7134 completed (loss: 0.04491114243865013, acc: 0.9890859723091125)
[2025-02-13 02:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:43][root][INFO] - Training Epoch: 1/2, step 2119/7134 completed (loss: 0.062241341918706894, acc: 0.9816272854804993)
[2025-02-13 02:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44][root][INFO] - Training Epoch: 1/2, step 2120/7134 completed (loss: 0.042390380054712296, acc: 0.9863013625144958)
[2025-02-13 02:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44][root][INFO] - Training Epoch: 1/2, step 2121/7134 completed (loss: 0.05294882133603096, acc: 0.9825396537780762)
[2025-02-13 02:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44][root][INFO] - Training Epoch: 1/2, step 2122/7134 completed (loss: 0.021042170003056526, acc: 0.99609375)
[2025-02-13 02:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45][root][INFO] - Training Epoch: 1/2, step 2123/7134 completed (loss: 0.039140913635492325, acc: 0.9889937043190002)
[2025-02-13 02:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45][root][INFO] - Training Epoch: 1/2, step 2124/7134 completed (loss: 0.019714079797267914, acc: 0.9906542301177979)
[2025-02-13 02:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:46][root][INFO] - Training Epoch: 1/2, step 2125/7134 completed (loss: 0.027509091421961784, acc: 0.9969696998596191)
[2025-02-13 02:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:46][root][INFO] - Training Epoch: 1/2, step 2126/7134 completed (loss: 0.024737874045968056, acc: 0.9932773113250732)
[2025-02-13 02:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:47][root][INFO] - Training Epoch: 1/2, step 2127/7134 completed (loss: 0.03133150562644005, acc: 0.99262535572052)
[2025-02-13 02:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:47][root][INFO] - Training Epoch: 1/2, step 2128/7134 completed (loss: 0.03914400190114975, acc: 0.9842519760131836)
[2025-02-13 02:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48][root][INFO] - Training Epoch: 1/2, step 2129/7134 completed (loss: 0.03559757396578789, acc: 0.9894419312477112)
[2025-02-13 02:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48][root][INFO] - Training Epoch: 1/2, step 2130/7134 completed (loss: 0.06019936501979828, acc: 0.9867987036705017)
[2025-02-13 02:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49][root][INFO] - Training Epoch: 1/2, step 2131/7134 completed (loss: 0.03029707632958889, acc: 0.9922600388526917)
[2025-02-13 02:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49][root][INFO] - Training Epoch: 1/2, step 2132/7134 completed (loss: 0.024655871093273163, acc: 0.9905660152435303)
[2025-02-13 02:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49][root][INFO] - Training Epoch: 1/2, step 2133/7134 completed (loss: 0.03789547458291054, acc: 0.9894319772720337)
[2025-02-13 02:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:50][root][INFO] - Training Epoch: 1/2, step 2134/7134 completed (loss: 0.0396113395690918, acc: 0.9906291961669922)
[2025-02-13 02:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:50][root][INFO] - Training Epoch: 1/2, step 2135/7134 completed (loss: 0.04907985404133797, acc: 0.9864457845687866)
[2025-02-13 02:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51][root][INFO] - Training Epoch: 1/2, step 2136/7134 completed (loss: 0.02409876137971878, acc: 0.9919893145561218)
[2025-02-13 02:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51][root][INFO] - Training Epoch: 1/2, step 2137/7134 completed (loss: 0.0290309339761734, acc: 0.9883871078491211)
[2025-02-13 02:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:52][root][INFO] - Training Epoch: 1/2, step 2138/7134 completed (loss: 0.014807295054197311, acc: 0.9969512224197388)
[2025-02-13 02:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:52][root][INFO] - Training Epoch: 1/2, step 2139/7134 completed (loss: 0.018771817907691002, acc: 0.994452178478241)
[2025-02-13 02:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:53][root][INFO] - Training Epoch: 1/2, step 2140/7134 completed (loss: 0.06297653168439865, acc: 0.9820936918258667)
[2025-02-13 02:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:53][root][INFO] - Training Epoch: 1/2, step 2141/7134 completed (loss: 0.022796548902988434, acc: 0.9941520690917969)
[2025-02-13 02:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:53][root][INFO] - Training Epoch: 1/2, step 2142/7134 completed (loss: 0.029260654002428055, acc: 0.9900850057601929)
[2025-02-13 02:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54][root][INFO] - Training Epoch: 1/2, step 2143/7134 completed (loss: 0.021190593019127846, acc: 0.9929824471473694)
[2025-02-13 02:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54][root][INFO] - Training Epoch: 1/2, step 2144/7134 completed (loss: 0.010074181482195854, acc: 0.9986720085144043)
[2025-02-13 02:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:55][root][INFO] - Training Epoch: 1/2, step 2145/7134 completed (loss: 0.06542006134986877, acc: 0.9814019799232483)
[2025-02-13 02:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:55][root][INFO] - Training Epoch: 1/2, step 2146/7134 completed (loss: 0.04021403566002846, acc: 0.9876760840415955)
[2025-02-13 02:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:56][root][INFO] - Training Epoch: 1/2, step 2147/7134 completed (loss: 0.03911498561501503, acc: 0.9887780547142029)
[2025-02-13 02:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:56][root][INFO] - Training Epoch: 1/2, step 2148/7134 completed (loss: 0.08287464082241058, acc: 0.9781659245491028)
[2025-02-13 02:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57][root][INFO] - Training Epoch: 1/2, step 2149/7134 completed (loss: 0.016050118952989578, acc: 0.9943661689758301)
[2025-02-13 02:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57][root][INFO] - Training Epoch: 1/2, step 2150/7134 completed (loss: 0.04923762381076813, acc: 0.9866666793823242)
[2025-02-13 02:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57][root][INFO] - Training Epoch: 1/2, step 2151/7134 completed (loss: 0.02864924818277359, acc: 0.9928571581840515)
[2025-02-13 02:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:58][root][INFO] - Training Epoch: 1/2, step 2152/7134 completed (loss: 0.09959167242050171, acc: 0.9724264740943909)
[2025-02-13 02:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:58][root][INFO] - Training Epoch: 1/2, step 2153/7134 completed (loss: 0.04798104241490364, acc: 0.9889011979103088)
[2025-02-13 02:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:59][root][INFO] - Training Epoch: 1/2, step 2154/7134 completed (loss: 0.060831468552351, acc: 0.984829306602478)
[2025-02-13 02:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:59][root][INFO] - Training Epoch: 1/2, step 2155/7134 completed (loss: 0.09316606819629669, acc: 0.9835164546966553)
[2025-02-13 02:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00][root][INFO] - Training Epoch: 1/2, step 2156/7134 completed (loss: 0.01847688853740692, acc: 0.9947916865348816)
[2025-02-13 02:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00][root][INFO] - Training Epoch: 1/2, step 2157/7134 completed (loss: 0.029733194038271904, acc: 0.9910614490509033)
[2025-02-13 02:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:01][root][INFO] - Training Epoch: 1/2, step 2158/7134 completed (loss: 0.0515693835914135, acc: 0.9886621236801147)
[2025-02-13 02:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:01][root][INFO] - Training Epoch: 1/2, step 2159/7134 completed (loss: 0.03552986681461334, acc: 0.9936143159866333)
[2025-02-13 02:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:02][root][INFO] - Training Epoch: 1/2, step 2160/7134 completed (loss: 0.033288173377513885, acc: 0.9889958500862122)
[2025-02-13 02:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:02][root][INFO] - Training Epoch: 1/2, step 2161/7134 completed (loss: 0.07678740471601486, acc: 0.984635055065155)
[2025-02-13 02:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03][root][INFO] - Training Epoch: 1/2, step 2162/7134 completed (loss: 0.03350100666284561, acc: 0.9894099831581116)
[2025-02-13 02:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03][root][INFO] - Training Epoch: 1/2, step 2163/7134 completed (loss: 0.039541952311992645, acc: 0.9858267903327942)
[2025-02-13 02:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03][root][INFO] - Training Epoch: 1/2, step 2164/7134 completed (loss: 0.027362557128071785, acc: 0.9931389093399048)
[2025-02-13 02:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:04][root][INFO] - Training Epoch: 1/2, step 2165/7134 completed (loss: 0.03371146693825722, acc: 0.991391658782959)
[2025-02-13 02:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:04][root][INFO] - Training Epoch: 1/2, step 2166/7134 completed (loss: 0.03303941711783409, acc: 0.9908854365348816)
[2025-02-13 02:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05][root][INFO] - Training Epoch: 1/2, step 2167/7134 completed (loss: 0.04092438519001007, acc: 0.9888641238212585)
[2025-02-13 02:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05][root][INFO] - Training Epoch: 1/2, step 2168/7134 completed (loss: 0.026146264746785164, acc: 0.9920634627342224)
[2025-02-13 02:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:06][root][INFO] - Training Epoch: 1/2, step 2169/7134 completed (loss: 0.05275414511561394, acc: 0.9895591735839844)
[2025-02-13 02:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:06][root][INFO] - Training Epoch: 1/2, step 2170/7134 completed (loss: 0.05341946706175804, acc: 0.9825327396392822)
[2025-02-13 02:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07][root][INFO] - Training Epoch: 1/2, step 2171/7134 completed (loss: 0.04445129632949829, acc: 0.9898074865341187)
[2025-02-13 02:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07][root][INFO] - Training Epoch: 1/2, step 2172/7134 completed (loss: 0.022490108385682106, acc: 0.9927685856819153)
[2025-02-13 02:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08][root][INFO] - Training Epoch: 1/2, step 2173/7134 completed (loss: 0.05366881564259529, acc: 0.9864048361778259)
[2025-02-13 02:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08][root][INFO] - Training Epoch: 1/2, step 2174/7134 completed (loss: 0.023356014862656593, acc: 0.9929078221321106)
[2025-02-13 02:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08][root][INFO] - Training Epoch: 1/2, step 2175/7134 completed (loss: 0.08523725718259811, acc: 0.9833794832229614)
[2025-02-13 02:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09][root][INFO] - Training Epoch: 1/2, step 2176/7134 completed (loss: 0.04056873172521591, acc: 0.9845971465110779)
[2025-02-13 02:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09][root][INFO] - Training Epoch: 1/2, step 2177/7134 completed (loss: 0.044808607548475266, acc: 0.9857142567634583)
[2025-02-13 02:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:10][root][INFO] - Training Epoch: 1/2, step 2178/7134 completed (loss: 0.03158515691757202, acc: 0.99227374792099)
[2025-02-13 02:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:10][root][INFO] - Training Epoch: 1/2, step 2179/7134 completed (loss: 0.03862913325428963, acc: 0.9873417615890503)
[2025-02-13 02:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:11][root][INFO] - Training Epoch: 1/2, step 2180/7134 completed (loss: 0.033963605761528015, acc: 0.9929078221321106)
[2025-02-13 02:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:11][root][INFO] - Training Epoch: 1/2, step 2181/7134 completed (loss: 0.013356250710785389, acc: 0.995708167552948)
[2025-02-13 02:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12][root][INFO] - Training Epoch: 1/2, step 2182/7134 completed (loss: 0.05858820676803589, acc: 0.9826989769935608)
[2025-02-13 02:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12][root][INFO] - Training Epoch: 1/2, step 2183/7134 completed (loss: 0.033998195081949234, acc: 0.9883585572242737)
[2025-02-13 02:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:13][root][INFO] - Training Epoch: 1/2, step 2184/7134 completed (loss: 0.03748646751046181, acc: 0.9936102032661438)
[2025-02-13 02:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:13][root][INFO] - Training Epoch: 1/2, step 2185/7134 completed (loss: 0.08022329956293106, acc: 0.980169951915741)
[2025-02-13 02:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:14][root][INFO] - Training Epoch: 1/2, step 2186/7134 completed (loss: 0.09603095054626465, acc: 0.9724473357200623)
[2025-02-13 02:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:14][root][INFO] - Training Epoch: 1/2, step 2187/7134 completed (loss: 0.11892843246459961, acc: 0.9711999893188477)
[2025-02-13 02:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:14][root][INFO] - Training Epoch: 1/2, step 2188/7134 completed (loss: 0.02795461378991604, acc: 0.9917920827865601)
[2025-02-13 02:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15][root][INFO] - Training Epoch: 1/2, step 2189/7134 completed (loss: 0.06390339881181717, acc: 0.9801223278045654)
[2025-02-13 02:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15][root][INFO] - Training Epoch: 1/2, step 2190/7134 completed (loss: 0.05242551863193512, acc: 0.9854881167411804)
[2025-02-13 02:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:16][root][INFO] - Training Epoch: 1/2, step 2191/7134 completed (loss: 0.05486514791846275, acc: 0.9783197641372681)
[2025-02-13 02:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:16][root][INFO] - Training Epoch: 1/2, step 2192/7134 completed (loss: 0.02327946200966835, acc: 0.9905837774276733)
[2025-02-13 02:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17][root][INFO] - Training Epoch: 1/2, step 2193/7134 completed (loss: 0.04083066061139107, acc: 0.9880319237709045)
[2025-02-13 02:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17][root][INFO] - Training Epoch: 1/2, step 2194/7134 completed (loss: 0.04442904517054558, acc: 0.9832496047019958)
[2025-02-13 02:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17][root][INFO] - Training Epoch: 1/2, step 2195/7134 completed (loss: 0.03104822151362896, acc: 0.9919999837875366)
[2025-02-13 02:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18][root][INFO] - Training Epoch: 1/2, step 2196/7134 completed (loss: 0.03490841016173363, acc: 0.9870466589927673)
[2025-02-13 02:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18][root][INFO] - Training Epoch: 1/2, step 2197/7134 completed (loss: 0.034354317933321, acc: 0.9854280352592468)
[2025-02-13 02:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:19][root][INFO] - Training Epoch: 1/2, step 2198/7134 completed (loss: 0.06613458693027496, acc: 0.9819004535675049)
[2025-02-13 02:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:19][root][INFO] - Training Epoch: 1/2, step 2199/7134 completed (loss: 0.02243235521018505, acc: 0.9939576983451843)
[2025-02-13 02:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:19][root][INFO] - Training Epoch: 1/2, step 2200/7134 completed (loss: 0.02757994644343853, acc: 0.9909583926200867)
[2025-02-13 02:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:20][root][INFO] - Training Epoch: 1/2, step 2201/7134 completed (loss: 0.01896219328045845, acc: 0.9939393997192383)
[2025-02-13 02:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:20][root][INFO] - Training Epoch: 1/2, step 2202/7134 completed (loss: 0.05123152956366539, acc: 0.9866443872451782)
[2025-02-13 02:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21][root][INFO] - Training Epoch: 1/2, step 2203/7134 completed (loss: 0.017071403563022614, acc: 0.9917808175086975)
[2025-02-13 02:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21][root][INFO] - Training Epoch: 1/2, step 2204/7134 completed (loss: 0.030412133783102036, acc: 0.9868420958518982)
[2025-02-13 02:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:22][root][INFO] - Training Epoch: 1/2, step 2205/7134 completed (loss: 0.021884828805923462, acc: 0.9947368502616882)
[2025-02-13 02:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:22][root][INFO] - Training Epoch: 1/2, step 2206/7134 completed (loss: 0.06912596523761749, acc: 0.9841827750205994)
[2025-02-13 02:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:22][root][INFO] - Training Epoch: 1/2, step 2207/7134 completed (loss: 0.042791567742824554, acc: 0.993537962436676)
[2025-02-13 02:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:23][root][INFO] - Training Epoch: 1/2, step 2208/7134 completed (loss: 0.0429951548576355, acc: 0.9900826215744019)
[2025-02-13 02:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:23][root][INFO] - Training Epoch: 1/2, step 2209/7134 completed (loss: 0.04435393959283829, acc: 0.9886845946311951)
[2025-02-13 02:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24][root][INFO] - Training Epoch: 1/2, step 2210/7134 completed (loss: 0.04553599655628204, acc: 0.9802731275558472)
[2025-02-13 02:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24][root][INFO] - Training Epoch: 1/2, step 2211/7134 completed (loss: 0.04221677407622337, acc: 0.9944598078727722)
[2025-02-13 02:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25][root][INFO] - Training Epoch: 1/2, step 2212/7134 completed (loss: 0.012209334410727024, acc: 0.9958333373069763)
[2025-02-13 02:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25][root][INFO] - Training Epoch: 1/2, step 2213/7134 completed (loss: 0.026996610686182976, acc: 0.9901130199432373)
[2025-02-13 02:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25][root][INFO] - Training Epoch: 1/2, step 2214/7134 completed (loss: 0.055535171180963516, acc: 0.983582079410553)
[2025-02-13 02:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26][root][INFO] - Training Epoch: 1/2, step 2215/7134 completed (loss: 0.02472657524049282, acc: 0.992977499961853)
[2025-02-13 02:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26][root][INFO] - Training Epoch: 1/2, step 2216/7134 completed (loss: 0.024259649217128754, acc: 0.9893190860748291)
[2025-02-13 02:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:27][root][INFO] - Training Epoch: 1/2, step 2217/7134 completed (loss: 0.02612323686480522, acc: 0.9912827014923096)
[2025-02-13 02:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:27][root][INFO] - Training Epoch: 1/2, step 2218/7134 completed (loss: 0.044293373823165894, acc: 0.9899425506591797)
[2025-02-13 02:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:28][root][INFO] - Training Epoch: 1/2, step 2219/7134 completed (loss: 0.02378777414560318, acc: 0.9922720193862915)
[2025-02-13 02:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:28][root][INFO] - Training Epoch: 1/2, step 2220/7134 completed (loss: 0.024608857929706573, acc: 0.9937106966972351)
[2025-02-13 02:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:28][root][INFO] - Training Epoch: 1/2, step 2221/7134 completed (loss: 0.03073706477880478, acc: 0.9935064911842346)
[2025-02-13 02:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29][root][INFO] - Training Epoch: 1/2, step 2222/7134 completed (loss: 0.0388895645737648, acc: 0.9884393215179443)
[2025-02-13 02:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29][root][INFO] - Training Epoch: 1/2, step 2223/7134 completed (loss: 0.031529758125543594, acc: 0.9902371168136597)
[2025-02-13 02:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:30][root][INFO] - Training Epoch: 1/2, step 2224/7134 completed (loss: 0.0336163155734539, acc: 0.9890260696411133)
[2025-02-13 02:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:30][root][INFO] - Training Epoch: 1/2, step 2225/7134 completed (loss: 0.02704044245183468, acc: 0.9898697733879089)
[2025-02-13 02:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31][root][INFO] - Training Epoch: 1/2, step 2226/7134 completed (loss: 0.028131620958447456, acc: 0.9893454909324646)
[2025-02-13 02:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31][root][INFO] - Training Epoch: 1/2, step 2227/7134 completed (loss: 0.026752008125185966, acc: 0.9926793575286865)
[2025-02-13 02:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31][root][INFO] - Training Epoch: 1/2, step 2228/7134 completed (loss: 0.019197555258870125, acc: 0.9934959411621094)
[2025-02-13 02:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:32][root][INFO] - Training Epoch: 1/2, step 2229/7134 completed (loss: 0.02690650336444378, acc: 0.9900000095367432)
[2025-02-13 02:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:32][root][INFO] - Training Epoch: 1/2, step 2230/7134 completed (loss: 0.0236536655575037, acc: 0.9951456189155579)
[2025-02-13 02:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:33][root][INFO] - Training Epoch: 1/2, step 2231/7134 completed (loss: 0.054017722606658936, acc: 0.9840637445449829)
[2025-02-13 02:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:33][root][INFO] - Training Epoch: 1/2, step 2232/7134 completed (loss: 0.016812225803732872, acc: 0.9959893226623535)
[2025-02-13 02:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34][root][INFO] - Training Epoch: 1/2, step 2233/7134 completed (loss: 0.04095010086894035, acc: 0.9900426864624023)
[2025-02-13 02:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34][root][INFO] - Training Epoch: 1/2, step 2234/7134 completed (loss: 0.018945176154375076, acc: 0.9931694269180298)
[2025-02-13 02:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34][root][INFO] - Training Epoch: 1/2, step 2235/7134 completed (loss: 0.04721065238118172, acc: 0.9890282154083252)
[2025-02-13 02:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:35][root][INFO] - Training Epoch: 1/2, step 2236/7134 completed (loss: 0.030612871050834656, acc: 0.9940029978752136)
[2025-02-13 02:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:35][root][INFO] - Training Epoch: 1/2, step 2237/7134 completed (loss: 0.048369087278842926, acc: 0.987270176410675)
[2025-02-13 02:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36][root][INFO] - Training Epoch: 1/2, step 2238/7134 completed (loss: 0.09077123552560806, acc: 0.9782923460006714)
[2025-02-13 02:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36][root][INFO] - Training Epoch: 1/2, step 2239/7134 completed (loss: 0.07350540906190872, acc: 0.9818181991577148)
[2025-02-13 02:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37][root][INFO] - Training Epoch: 1/2, step 2240/7134 completed (loss: 0.05876040831208229, acc: 0.9870129823684692)
[2025-02-13 02:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37][root][INFO] - Training Epoch: 1/2, step 2241/7134 completed (loss: 0.08839235454797745, acc: 0.9763593673706055)
[2025-02-13 02:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37][root][INFO] - Training Epoch: 1/2, step 2242/7134 completed (loss: 0.12693631649017334, acc: 0.9667458534240723)
[2025-02-13 02:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:38][root][INFO] - Training Epoch: 1/2, step 2243/7134 completed (loss: 0.14231649041175842, acc: 0.9618320465087891)
[2025-02-13 02:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:38][root][INFO] - Training Epoch: 1/2, step 2244/7134 completed (loss: 0.04202783480286598, acc: 0.9842767119407654)
[2025-02-13 02:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:39][root][INFO] - Training Epoch: 1/2, step 2245/7134 completed (loss: 0.07582435756921768, acc: 0.9724919199943542)
[2025-02-13 02:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:39][root][INFO] - Training Epoch: 1/2, step 2246/7134 completed (loss: 0.13447268307209015, acc: 0.9621621370315552)
[2025-02-13 02:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40][root][INFO] - Training Epoch: 1/2, step 2247/7134 completed (loss: 0.12038721144199371, acc: 0.9683908224105835)
[2025-02-13 02:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40][root][INFO] - Training Epoch: 1/2, step 2248/7134 completed (loss: 0.0684555321931839, acc: 0.9875518679618835)
[2025-02-13 02:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41][root][INFO] - Training Epoch: 1/2, step 2249/7134 completed (loss: 0.04994073137640953, acc: 0.9860334992408752)
[2025-02-13 02:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41][root][INFO] - Training Epoch: 1/2, step 2250/7134 completed (loss: 0.06742018461227417, acc: 0.9826202988624573)
[2025-02-13 02:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41][root][INFO] - Training Epoch: 1/2, step 2251/7134 completed (loss: 0.042781420052051544, acc: 0.9841269850730896)
[2025-02-13 02:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42][root][INFO] - Training Epoch: 1/2, step 2252/7134 completed (loss: 0.03635846823453903, acc: 0.994397759437561)
[2025-02-13 02:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42][root][INFO] - Training Epoch: 1/2, step 2253/7134 completed (loss: 0.04497876018285751, acc: 0.9882943034172058)
[2025-02-13 02:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:43][root][INFO] - Training Epoch: 1/2, step 2254/7134 completed (loss: 0.12132967263460159, acc: 0.9601593613624573)
[2025-02-13 02:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:43][root][INFO] - Training Epoch: 1/2, step 2255/7134 completed (loss: 0.0498846098780632, acc: 0.9863429665565491)
[2025-02-13 02:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:43][root][INFO] - Training Epoch: 1/2, step 2256/7134 completed (loss: 0.10903274267911911, acc: 0.9690265655517578)
[2025-02-13 02:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44][root][INFO] - Training Epoch: 1/2, step 2257/7134 completed (loss: 0.041803695261478424, acc: 0.9894259572029114)
[2025-02-13 02:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44][root][INFO] - Training Epoch: 1/2, step 2258/7134 completed (loss: 0.044736094772815704, acc: 0.982692301273346)
[2025-02-13 02:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:45][root][INFO] - Training Epoch: 1/2, step 2259/7134 completed (loss: 0.03330085426568985, acc: 0.9866666793823242)
[2025-02-13 02:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:45][root][INFO] - Training Epoch: 1/2, step 2260/7134 completed (loss: 0.07589828968048096, acc: 0.9833333492279053)
[2025-02-13 02:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46][root][INFO] - Training Epoch: 1/2, step 2261/7134 completed (loss: 0.0944768413901329, acc: 0.974452555179596)
[2025-02-13 02:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46][root][INFO] - Training Epoch: 1/2, step 2262/7134 completed (loss: 0.025437990203499794, acc: 0.9944751262664795)
[2025-02-13 02:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46][root][INFO] - Training Epoch: 1/2, step 2263/7134 completed (loss: 0.05191364511847496, acc: 0.9873873591423035)
[2025-02-13 02:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:47][root][INFO] - Training Epoch: 1/2, step 2264/7134 completed (loss: 0.08716408163309097, acc: 0.9789029359817505)
[2025-02-13 02:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:47][root][INFO] - Training Epoch: 1/2, step 2265/7134 completed (loss: 0.08316271007061005, acc: 0.9725086092948914)
[2025-02-13 02:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48][root][INFO] - Training Epoch: 1/2, step 2266/7134 completed (loss: 0.04932388290762901, acc: 0.9865951538085938)
[2025-02-13 02:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48][root][INFO] - Training Epoch: 1/2, step 2267/7134 completed (loss: 0.03430670499801636, acc: 0.9897119402885437)
[2025-02-13 02:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48][root][INFO] - Training Epoch: 1/2, step 2268/7134 completed (loss: 0.04270540177822113, acc: 0.9870874881744385)
[2025-02-13 02:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49][root][INFO] - Training Epoch: 1/2, step 2269/7134 completed (loss: 0.020683040842413902, acc: 0.99245285987854)
[2025-02-13 02:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49][root][INFO] - Training Epoch: 1/2, step 2270/7134 completed (loss: 0.06724075973033905, acc: 0.9784411191940308)
[2025-02-13 02:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:50][root][INFO] - Training Epoch: 1/2, step 2271/7134 completed (loss: 0.05229524150490761, acc: 0.9781357645988464)
[2025-02-13 02:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:50][root][INFO] - Training Epoch: 1/2, step 2272/7134 completed (loss: 0.03356006741523743, acc: 0.9902200698852539)
[2025-02-13 02:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51][root][INFO] - Training Epoch: 1/2, step 2273/7134 completed (loss: 0.020436543971300125, acc: 0.9940119981765747)
[2025-02-13 02:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51][root][INFO] - Training Epoch: 1/2, step 2274/7134 completed (loss: 0.09583179652690887, acc: 0.9783439636230469)
[2025-02-13 02:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:52][root][INFO] - Training Epoch: 1/2, step 2275/7134 completed (loss: 0.036705225706100464, acc: 0.9894366264343262)
[2025-02-13 02:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:52][root][INFO] - Training Epoch: 1/2, step 2276/7134 completed (loss: 0.029806552454829216, acc: 0.9940758347511292)
[2025-02-13 02:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:53][root][INFO] - Training Epoch: 1/2, step 2277/7134 completed (loss: 0.04388333857059479, acc: 0.986143171787262)
[2025-02-13 02:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:53][root][INFO] - Training Epoch: 1/2, step 2278/7134 completed (loss: 0.1135229617357254, acc: 0.9712460041046143)
[2025-02-13 02:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54][root][INFO] - Training Epoch: 1/2, step 2279/7134 completed (loss: 0.041795339435338974, acc: 0.9846516847610474)
[2025-02-13 02:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54][root][INFO] - Training Epoch: 1/2, step 2280/7134 completed (loss: 0.06284225732088089, acc: 0.9881936311721802)
[2025-02-13 02:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54][root][INFO] - Training Epoch: 1/2, step 2281/7134 completed (loss: 0.10061951726675034, acc: 0.9786666631698608)
[2025-02-13 02:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:55][root][INFO] - Training Epoch: 1/2, step 2282/7134 completed (loss: 0.020497577264904976, acc: 0.9925834536552429)
[2025-02-13 02:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:55][root][INFO] - Training Epoch: 1/2, step 2283/7134 completed (loss: 0.039797015488147736, acc: 0.9841075539588928)
[2025-02-13 02:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:56][root][INFO] - Training Epoch: 1/2, step 2284/7134 completed (loss: 0.04309224709868431, acc: 0.9867060780525208)
[2025-02-13 02:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:56][root][INFO] - Training Epoch: 1/2, step 2285/7134 completed (loss: 0.040827177464962006, acc: 0.9855072498321533)
[2025-02-13 02:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57][root][INFO] - Training Epoch: 1/2, step 2286/7134 completed (loss: 0.054672010242938995, acc: 0.9818417429924011)
[2025-02-13 02:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57][root][INFO] - Training Epoch: 1/2, step 2287/7134 completed (loss: 0.0387132465839386, acc: 0.9839141964912415)
[2025-02-13 02:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:58][root][INFO] - Training Epoch: 1/2, step 2288/7134 completed (loss: 0.03781582787632942, acc: 0.9929328560829163)
[2025-02-13 02:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:58][root][INFO] - Training Epoch: 1/2, step 2289/7134 completed (loss: 0.04719987139105797, acc: 0.9820282459259033)
[2025-02-13 02:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:59][root][INFO] - Training Epoch: 1/2, step 2290/7134 completed (loss: 0.04588669538497925, acc: 0.987261176109314)
[2025-02-13 02:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:59][root][INFO] - Training Epoch: 1/2, step 2291/7134 completed (loss: 0.034510958939790726, acc: 0.9892037510871887)
[2025-02-13 02:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:59][root][INFO] - Training Epoch: 1/2, step 2292/7134 completed (loss: 0.08962708711624146, acc: 0.9819355010986328)
[2025-02-13 02:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00][root][INFO] - Training Epoch: 1/2, step 2293/7134 completed (loss: 0.06658276170492172, acc: 0.9821826219558716)
[2025-02-13 02:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00][root][INFO] - Training Epoch: 1/2, step 2294/7134 completed (loss: 0.043334122747182846, acc: 0.9865016937255859)
[2025-02-13 02:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:01][root][INFO] - Training Epoch: 1/2, step 2295/7134 completed (loss: 0.04232805594801903, acc: 0.9895536303520203)
[2025-02-13 02:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:01][root][INFO] - Training Epoch: 1/2, step 2296/7134 completed (loss: 0.020025653764605522, acc: 0.992553174495697)
[2025-02-13 02:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:02][root][INFO] - Training Epoch: 1/2, step 2297/7134 completed (loss: 0.032075222581624985, acc: 0.9908536672592163)
[2025-02-13 02:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:02][root][INFO] - Training Epoch: 1/2, step 2298/7134 completed (loss: 0.06970589607954025, acc: 0.9779179692268372)
[2025-02-13 02:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:03][root][INFO] - Training Epoch: 1/2, step 2299/7134 completed (loss: 0.022911718115210533, acc: 0.9920364022254944)
[2025-02-13 02:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:03][root][INFO] - Training Epoch: 1/2, step 2300/7134 completed (loss: 0.012421250343322754, acc: 0.9977452158927917)
[2025-02-13 02:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04][root][INFO] - Training Epoch: 1/2, step 2301/7134 completed (loss: 0.04427333176136017, acc: 0.9867549538612366)
[2025-02-13 02:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04][root][INFO] - Training Epoch: 1/2, step 2302/7134 completed (loss: 0.05014871433377266, acc: 0.9816176295280457)
[2025-02-13 02:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:05][root][INFO] - Training Epoch: 1/2, step 2303/7134 completed (loss: 0.04926177114248276, acc: 0.984054684638977)
[2025-02-13 02:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:05][root][INFO] - Training Epoch: 1/2, step 2304/7134 completed (loss: 0.06380559504032135, acc: 0.9856114983558655)
[2025-02-13 02:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:06][root][INFO] - Training Epoch: 1/2, step 2305/7134 completed (loss: 0.03964952006936073, acc: 0.9861555099487305)
[2025-02-13 02:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:06][root][INFO] - Training Epoch: 1/2, step 2306/7134 completed (loss: 0.029879607260227203, acc: 0.9900221824645996)
[2025-02-13 02:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07][root][INFO] - Training Epoch: 1/2, step 2307/7134 completed (loss: 0.05624103918671608, acc: 0.9855371713638306)
[2025-02-13 02:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07][root][INFO] - Training Epoch: 1/2, step 2308/7134 completed (loss: 0.06446816772222519, acc: 0.9795709848403931)
[2025-02-13 02:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08][root][INFO] - Training Epoch: 1/2, step 2309/7134 completed (loss: 0.03425921872258186, acc: 0.9892086386680603)
[2025-02-13 02:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08][root][INFO] - Training Epoch: 1/2, step 2310/7134 completed (loss: 0.027743618935346603, acc: 0.9920904040336609)
[2025-02-13 02:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08][root][INFO] - Training Epoch: 1/2, step 2311/7134 completed (loss: 0.03993295505642891, acc: 0.9896789193153381)
[2025-02-13 02:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:09][root][INFO] - Training Epoch: 1/2, step 2312/7134 completed (loss: 0.04081246629357338, acc: 0.99028080701828)
[2025-02-13 02:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:09][root][INFO] - Training Epoch: 1/2, step 2313/7134 completed (loss: 0.05225035175681114, acc: 0.9874429106712341)
[2025-02-13 02:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10][root][INFO] - Training Epoch: 1/2, step 2314/7134 completed (loss: 0.032616619020700455, acc: 0.9922308325767517)
[2025-02-13 02:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10][root][INFO] - Training Epoch: 1/2, step 2315/7134 completed (loss: 0.03259316459298134, acc: 0.9870588183403015)
[2025-02-13 02:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:11][root][INFO] - Training Epoch: 1/2, step 2316/7134 completed (loss: 0.029255181550979614, acc: 0.9907940030097961)
[2025-02-13 02:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:11][root][INFO] - Training Epoch: 1/2, step 2317/7134 completed (loss: 0.04151744395494461, acc: 0.9894179701805115)
[2025-02-13 02:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:12][root][INFO] - Training Epoch: 1/2, step 2318/7134 completed (loss: 0.01743452437222004, acc: 0.9947698712348938)
[2025-02-13 02:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:12][root][INFO] - Training Epoch: 1/2, step 2319/7134 completed (loss: 0.027089525014162064, acc: 0.9923413395881653)
[2025-02-13 02:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13][root][INFO] - Training Epoch: 1/2, step 2320/7134 completed (loss: 0.023863408714532852, acc: 0.9920454621315002)
[2025-02-13 02:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13][root][INFO] - Training Epoch: 1/2, step 2321/7134 completed (loss: 0.024239011108875275, acc: 0.9916666746139526)
[2025-02-13 02:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14][root][INFO] - Training Epoch: 1/2, step 2322/7134 completed (loss: 0.022717943415045738, acc: 0.9881481528282166)
[2025-02-13 02:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14][root][INFO] - Training Epoch: 1/2, step 2323/7134 completed (loss: 0.048054859042167664, acc: 0.9779179692268372)
[2025-02-13 02:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14][root][INFO] - Training Epoch: 1/2, step 2324/7134 completed (loss: 0.08311952650547028, acc: 0.9800570011138916)
[2025-02-13 02:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:15][root][INFO] - Training Epoch: 1/2, step 2325/7134 completed (loss: 0.06190591678023338, acc: 0.976068377494812)
[2025-02-13 02:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:15][root][INFO] - Training Epoch: 1/2, step 2326/7134 completed (loss: 0.018118591979146004, acc: 0.9926470518112183)
[2025-02-13 02:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16][root][INFO] - Training Epoch: 1/2, step 2327/7134 completed (loss: 0.017575815320014954, acc: 0.9901639223098755)
[2025-02-13 02:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16][root][INFO] - Training Epoch: 1/2, step 2328/7134 completed (loss: 0.07789118587970734, acc: 0.9838709831237793)
[2025-02-13 02:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:17][root][INFO] - Training Epoch: 1/2, step 2329/7134 completed (loss: 0.04373379051685333, acc: 0.9906976819038391)
[2025-02-13 02:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:17][root][INFO] - Training Epoch: 1/2, step 2330/7134 completed (loss: 0.017937852069735527, acc: 0.9954198598861694)
[2025-02-13 02:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:17][root][INFO] - Training Epoch: 1/2, step 2331/7134 completed (loss: 0.015201335772871971, acc: 0.995398759841919)
[2025-02-13 02:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:18][root][INFO] - Training Epoch: 1/2, step 2332/7134 completed (loss: 0.056042034178972244, acc: 0.9910846948623657)
[2025-02-13 02:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:18][root][INFO] - Training Epoch: 1/2, step 2333/7134 completed (loss: 0.018904350697994232, acc: 0.9910714030265808)
[2025-02-13 02:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19][root][INFO] - Training Epoch: 1/2, step 2334/7134 completed (loss: 0.021558260545134544, acc: 0.9895522594451904)
[2025-02-13 02:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19][root][INFO] - Training Epoch: 1/2, step 2335/7134 completed (loss: 0.05881569907069206, acc: 0.9870967864990234)
[2025-02-13 02:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:20][root][INFO] - Training Epoch: 1/2, step 2336/7134 completed (loss: 0.01393953152000904, acc: 0.9953632354736328)
[2025-02-13 02:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:20][root][INFO] - Training Epoch: 1/2, step 2337/7134 completed (loss: 0.005591699853539467, acc: 1.0)
[2025-02-13 02:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21][root][INFO] - Training Epoch: 1/2, step 2338/7134 completed (loss: 0.0309378020465374, acc: 0.9938176274299622)
[2025-02-13 02:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21][root][INFO] - Training Epoch: 1/2, step 2339/7134 completed (loss: 0.040394075214862823, acc: 0.9918434023857117)
[2025-02-13 02:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21][root][INFO] - Training Epoch: 1/2, step 2340/7134 completed (loss: 0.02101929672062397, acc: 0.9954751133918762)
[2025-02-13 02:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:22][root][INFO] - Training Epoch: 1/2, step 2341/7134 completed (loss: 0.013515656813979149, acc: 0.9981203079223633)
[2025-02-13 02:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:22][root][INFO] - Training Epoch: 1/2, step 2342/7134 completed (loss: 0.027465056627988815, acc: 0.9881831407546997)
[2025-02-13 02:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23][root][INFO] - Training Epoch: 1/2, step 2343/7134 completed (loss: 0.018644606694579124, acc: 0.9936000108718872)
[2025-02-13 02:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23][root][INFO] - Training Epoch: 1/2, step 2344/7134 completed (loss: 0.020843787118792534, acc: 0.992175281047821)
[2025-02-13 02:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24][root][INFO] - Training Epoch: 1/2, step 2345/7134 completed (loss: 0.030475864186882973, acc: 0.9902098178863525)
[2025-02-13 02:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24][root][INFO] - Training Epoch: 1/2, step 2346/7134 completed (loss: 0.020902808755636215, acc: 0.9943342804908752)
[2025-02-13 02:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24][root][INFO] - Training Epoch: 1/2, step 2347/7134 completed (loss: 0.03694690391421318, acc: 0.9884868264198303)
[2025-02-13 02:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:25][root][INFO] - Training Epoch: 1/2, step 2348/7134 completed (loss: 0.07306373864412308, acc: 0.9785624146461487)
[2025-02-13 02:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:25][root][INFO] - Training Epoch: 1/2, step 2349/7134 completed (loss: 0.04942301660776138, acc: 0.9861111044883728)
[2025-02-13 02:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26][root][INFO] - Training Epoch: 1/2, step 2350/7134 completed (loss: 0.09548641741275787, acc: 0.980182945728302)
[2025-02-13 02:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26][root][INFO] - Training Epoch: 1/2, step 2351/7134 completed (loss: 0.054707709699869156, acc: 0.9863387942314148)
[2025-02-13 02:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:27][root][INFO] - Training Epoch: 1/2, step 2352/7134 completed (loss: 0.04201939329504967, acc: 0.9887820482254028)
[2025-02-13 02:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:27][root][INFO] - Training Epoch: 1/2, step 2353/7134 completed (loss: 0.043790336698293686, acc: 0.9872340559959412)
[2025-02-13 02:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:28][root][INFO] - Training Epoch: 1/2, step 2354/7134 completed (loss: 0.03451911732554436, acc: 0.9886075854301453)
[2025-02-13 02:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:28][root][INFO] - Training Epoch: 1/2, step 2355/7134 completed (loss: 0.05180540308356285, acc: 0.9864197373390198)
[2025-02-13 02:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:29][root][INFO] - Training Epoch: 1/2, step 2356/7134 completed (loss: 0.06702729314565659, acc: 0.982332170009613)
[2025-02-13 02:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:29][root][INFO] - Training Epoch: 1/2, step 2357/7134 completed (loss: 0.04295465350151062, acc: 0.9841269850730896)
[2025-02-13 02:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:29][root][INFO] - Training Epoch: 1/2, step 2358/7134 completed (loss: 0.04951510578393936, acc: 0.9879518151283264)
[2025-02-13 02:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30][root][INFO] - Training Epoch: 1/2, step 2359/7134 completed (loss: 0.06110956147313118, acc: 0.98531574010849)
[2025-02-13 02:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30][root][INFO] - Training Epoch: 1/2, step 2360/7134 completed (loss: 0.04729890450835228, acc: 0.9912152290344238)
[2025-02-13 02:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:31][root][INFO] - Training Epoch: 1/2, step 2361/7134 completed (loss: 0.03819981589913368, acc: 0.9881656765937805)
[2025-02-13 02:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:31][root][INFO] - Training Epoch: 1/2, step 2362/7134 completed (loss: 0.037513040006160736, acc: 0.9932735562324524)
[2025-02-13 02:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32][root][INFO] - Training Epoch: 1/2, step 2363/7134 completed (loss: 0.05088571459054947, acc: 0.9895712733268738)
[2025-02-13 02:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32][root][INFO] - Training Epoch: 1/2, step 2364/7134 completed (loss: 0.03201555833220482, acc: 0.9893454909324646)
[2025-02-13 02:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:33][root][INFO] - Training Epoch: 1/2, step 2365/7134 completed (loss: 0.03505926951766014, acc: 0.9876033067703247)
[2025-02-13 02:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:33][root][INFO] - Training Epoch: 1/2, step 2366/7134 completed (loss: 0.029307520017027855, acc: 0.9930555820465088)
[2025-02-13 02:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:34][root][INFO] - Training Epoch: 1/2, step 2367/7134 completed (loss: 0.07238714396953583, acc: 0.9886934757232666)
[2025-02-13 02:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:34][root][INFO] - Training Epoch: 1/2, step 2368/7134 completed (loss: 0.031242545694112778, acc: 0.9927272796630859)
[2025-02-13 02:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:34][root][INFO] - Training Epoch: 1/2, step 2369/7134 completed (loss: 0.018905220553278923, acc: 0.9929412007331848)
[2025-02-13 02:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35][root][INFO] - Training Epoch: 1/2, step 2370/7134 completed (loss: 0.04367570951581001, acc: 0.9830247163772583)
[2025-02-13 02:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35][root][INFO] - Training Epoch: 1/2, step 2371/7134 completed (loss: 0.022490378469228745, acc: 0.9956772327423096)
[2025-02-13 02:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:36][root][INFO] - Training Epoch: 1/2, step 2372/7134 completed (loss: 0.03193056583404541, acc: 0.9920634627342224)
[2025-02-13 02:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:36][root][INFO] - Training Epoch: 1/2, step 2373/7134 completed (loss: 0.01650279387831688, acc: 0.9950371980667114)
[2025-02-13 02:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37][root][INFO] - Training Epoch: 1/2, step 2374/7134 completed (loss: 0.021546967327594757, acc: 0.9879518151283264)
[2025-02-13 02:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37][root][INFO] - Training Epoch: 1/2, step 2375/7134 completed (loss: 0.07309594005346298, acc: 0.989393949508667)
[2025-02-13 02:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:38][root][INFO] - Training Epoch: 1/2, step 2376/7134 completed (loss: 0.11904812604188919, acc: 0.9744463562965393)
[2025-02-13 02:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:38][root][INFO] - Training Epoch: 1/2, step 2377/7134 completed (loss: 0.07264779508113861, acc: 0.9820788502693176)
[2025-02-13 02:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:38][root][INFO] - Training Epoch: 1/2, step 2378/7134 completed (loss: 0.059449344873428345, acc: 0.9847418069839478)
[2025-02-13 02:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39][root][INFO] - Training Epoch: 1/2, step 2379/7134 completed (loss: 0.09712540358304977, acc: 0.9696551561355591)
[2025-02-13 02:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39][root][INFO] - Training Epoch: 1/2, step 2380/7134 completed (loss: 0.04527418687939644, acc: 0.9866989254951477)
[2025-02-13 02:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:40][root][INFO] - Training Epoch: 1/2, step 2381/7134 completed (loss: 0.13625678420066833, acc: 0.9661590456962585)
[2025-02-13 02:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:40][root][INFO] - Training Epoch: 1/2, step 2382/7134 completed (loss: 0.05203652009367943, acc: 0.9847095012664795)
[2025-02-13 02:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:41][root][INFO] - Training Epoch: 1/2, step 2383/7134 completed (loss: 0.10188691318035126, acc: 0.9698340892791748)
[2025-02-13 02:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:41][root][INFO] - Training Epoch: 1/2, step 2384/7134 completed (loss: 0.0762774869799614, acc: 0.98562091588974)
[2025-02-13 02:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:41][root][INFO] - Training Epoch: 1/2, step 2385/7134 completed (loss: 0.05212405323982239, acc: 0.9861878156661987)
[2025-02-13 02:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:42][root][INFO] - Training Epoch: 1/2, step 2386/7134 completed (loss: 0.05798695981502533, acc: 0.9842932224273682)
[2025-02-13 02:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:42][root][INFO] - Training Epoch: 1/2, step 2387/7134 completed (loss: 0.05081165209412575, acc: 0.9872390031814575)
[2025-02-13 02:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43][root][INFO] - Training Epoch: 1/2, step 2388/7134 completed (loss: 0.11003361642360687, acc: 0.9678188562393188)
[2025-02-13 02:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43][root][INFO] - Training Epoch: 1/2, step 2389/7134 completed (loss: 0.07811599224805832, acc: 0.9758898019790649)
[2025-02-13 02:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:44][root][INFO] - Training Epoch: 1/2, step 2390/7134 completed (loss: 0.07701551169157028, acc: 0.9772998690605164)
[2025-02-13 02:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:44][root][INFO] - Training Epoch: 1/2, step 2391/7134 completed (loss: 0.0783759132027626, acc: 0.9771283268928528)
[2025-02-13 02:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:45][root][INFO] - Training Epoch: 1/2, step 2392/7134 completed (loss: 0.06113981828093529, acc: 0.9879840016365051)
[2025-02-13 02:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:45][root][INFO] - Training Epoch: 1/2, step 2393/7134 completed (loss: 0.09406235814094543, acc: 0.9792477488517761)
[2025-02-13 02:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46][root][INFO] - Training Epoch: 1/2, step 2394/7134 completed (loss: 0.04548418149352074, acc: 0.9890909194946289)
[2025-02-13 02:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46][root][INFO] - Training Epoch: 1/2, step 2395/7134 completed (loss: 0.1502039134502411, acc: 0.9641693830490112)
[2025-02-13 02:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46][root][INFO] - Training Epoch: 1/2, step 2396/7134 completed (loss: 0.08064877241849899, acc: 0.9765840172767639)
[2025-02-13 02:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47][root][INFO] - Training Epoch: 1/2, step 2397/7134 completed (loss: 0.09115751832723618, acc: 0.9723435044288635)
[2025-02-13 02:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47][root][INFO] - Training Epoch: 1/2, step 2398/7134 completed (loss: 0.034197449684143066, acc: 0.9889958500862122)
[2025-02-13 02:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:48][root][INFO] - Training Epoch: 1/2, step 2399/7134 completed (loss: 0.06183668598532677, acc: 0.9756795167922974)
[2025-02-13 02:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:48][root][INFO] - Training Epoch: 1/2, step 2400/7134 completed (loss: 0.04851173609495163, acc: 0.9820689558982849)
[2025-02-13 02:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:49][root][INFO] - Training Epoch: 1/2, step 2401/7134 completed (loss: 0.11544989049434662, acc: 0.9715189933776855)
[2025-02-13 02:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:49][root][INFO] - Training Epoch: 1/2, step 2402/7134 completed (loss: 0.08922959864139557, acc: 0.9721115827560425)
[2025-02-13 02:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:49][root][INFO] - Training Epoch: 1/2, step 2403/7134 completed (loss: 0.0531001053750515, acc: 0.9829787015914917)
[2025-02-13 02:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50][root][INFO] - Training Epoch: 1/2, step 2404/7134 completed (loss: 0.026110943406820297, acc: 0.9918032884597778)
[2025-02-13 02:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50][root][INFO] - Training Epoch: 1/2, step 2405/7134 completed (loss: 0.05390894412994385, acc: 0.9799330830574036)
[2025-02-13 02:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:51][root][INFO] - Training Epoch: 1/2, step 2406/7134 completed (loss: 0.019864235073328018, acc: 0.9906914830207825)
[2025-02-13 02:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:51][root][INFO] - Training Epoch: 1/2, step 2407/7134 completed (loss: 0.038449160754680634, acc: 0.9893617033958435)
[2025-02-13 02:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52][root][INFO] - Training Epoch: 1/2, step 2408/7134 completed (loss: 0.1241222470998764, acc: 0.9697368144989014)
[2025-02-13 02:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52][root][INFO] - Training Epoch: 1/2, step 2409/7134 completed (loss: 0.030459940433502197, acc: 0.9863387942314148)
[2025-02-13 02:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53][root][INFO] - Training Epoch: 1/2, step 2410/7134 completed (loss: 0.029921289533376694, acc: 0.9885222315788269)
[2025-02-13 02:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53][root][INFO] - Training Epoch: 1/2, step 2411/7134 completed (loss: 0.03286062553524971, acc: 0.9909443855285645)
[2025-02-13 02:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53][root][INFO] - Training Epoch: 1/2, step 2412/7134 completed (loss: 0.040931396186351776, acc: 0.9884615540504456)
[2025-02-13 02:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:54][root][INFO] - Training Epoch: 1/2, step 2413/7134 completed (loss: 0.03223973885178566, acc: 0.9885641932487488)
[2025-02-13 02:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:54][root][INFO] - Training Epoch: 1/2, step 2414/7134 completed (loss: 0.03495762497186661, acc: 0.9900497794151306)
[2025-02-13 02:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55][root][INFO] - Training Epoch: 1/2, step 2415/7134 completed (loss: 0.050025224685668945, acc: 0.9876543283462524)
[2025-02-13 02:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55][root][INFO] - Training Epoch: 1/2, step 2416/7134 completed (loss: 0.0404721163213253, acc: 0.9889349937438965)
[2025-02-13 02:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56][root][INFO] - Training Epoch: 1/2, step 2417/7134 completed (loss: 0.053031083196401596, acc: 0.9843924045562744)
[2025-02-13 02:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56][root][INFO] - Training Epoch: 1/2, step 2418/7134 completed (loss: 0.03480689600110054, acc: 0.9901546835899353)
[2025-02-13 02:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57][root][INFO] - Training Epoch: 1/2, step 2419/7134 completed (loss: 0.022556426003575325, acc: 0.991416335105896)
[2025-02-13 02:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57][root][INFO] - Training Epoch: 1/2, step 2420/7134 completed (loss: 0.021102001890540123, acc: 0.9950248599052429)
[2025-02-13 02:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57][root][INFO] - Training Epoch: 1/2, step 2421/7134 completed (loss: 0.04849721118807793, acc: 0.9863184094429016)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58][root][INFO] - Training Epoch: 1/2, step 2422/7134 completed (loss: 0.030941694974899292, acc: 0.9905437231063843)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58][root][INFO] - Training Epoch: 1/2, step 2423/7134 completed (loss: 0.023636292666196823, acc: 0.9944211840629578)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:59][root][INFO] - Training Epoch: 1/2, step 2424/7134 completed (loss: 0.0338750034570694, acc: 0.9901685118675232)
[2025-02-13 02:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:59][root][INFO] - Training Epoch: 1/2, step 2425/7134 completed (loss: 0.0344596728682518, acc: 0.9919999837875366)
[2025-02-13 02:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00][root][INFO] - Training Epoch: 1/2, step 2426/7134 completed (loss: 0.018386326730251312, acc: 0.9940546751022339)
[2025-02-13 02:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00][root][INFO] - Training Epoch: 1/2, step 2427/7134 completed (loss: 0.021369772031903267, acc: 0.9930394291877747)
[2025-02-13 02:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01][root][INFO] - Training Epoch: 1/2, step 2428/7134 completed (loss: 0.03417780622839928, acc: 0.993630588054657)
[2025-02-13 02:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01][root][INFO] - Training Epoch: 1/2, step 2429/7134 completed (loss: 0.036112233996391296, acc: 0.9906542301177979)
[2025-02-13 02:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01][root][INFO] - Training Epoch: 1/2, step 2430/7134 completed (loss: 0.03721105679869652, acc: 0.9910581111907959)
[2025-02-13 02:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02][root][INFO] - Training Epoch: 1/2, step 2431/7134 completed (loss: 0.053854990750551224, acc: 0.9809384346008301)
[2025-02-13 02:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02][root][INFO] - Training Epoch: 1/2, step 2432/7134 completed (loss: 0.10733092576265335, acc: 0.9779286980628967)
[2025-02-13 02:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:03][root][INFO] - Training Epoch: 1/2, step 2433/7134 completed (loss: 0.06311612576246262, acc: 0.984375)
[2025-02-13 02:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:03][root][INFO] - Training Epoch: 1/2, step 2434/7134 completed (loss: 0.0929923877120018, acc: 0.97826087474823)
[2025-02-13 02:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:04][root][INFO] - Training Epoch: 1/2, step 2435/7134 completed (loss: 0.042563989758491516, acc: 0.9855072498321533)
[2025-02-13 02:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:04][root][INFO] - Training Epoch: 1/2, step 2436/7134 completed (loss: 0.03855850175023079, acc: 0.9895226955413818)
[2025-02-13 02:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05][root][INFO] - Training Epoch: 1/2, step 2437/7134 completed (loss: 0.03940374404191971, acc: 0.9819004535675049)
[2025-02-13 02:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05][root][INFO] - Training Epoch: 1/2, step 2438/7134 completed (loss: 0.05205577239394188, acc: 0.9868420958518982)
[2025-02-13 02:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05][root][INFO] - Training Epoch: 1/2, step 2439/7134 completed (loss: 0.052020516246557236, acc: 0.9849300384521484)
[2025-02-13 02:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06][root][INFO] - Training Epoch: 1/2, step 2440/7134 completed (loss: 0.06810875236988068, acc: 0.9798206090927124)
[2025-02-13 02:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06][root][INFO] - Training Epoch: 1/2, step 2441/7134 completed (loss: 0.12545108795166016, acc: 0.9769673943519592)
[2025-02-13 02:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:07][root][INFO] - Training Epoch: 1/2, step 2442/7134 completed (loss: 0.038405369967222214, acc: 0.9846153855323792)
[2025-02-13 02:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:07][root][INFO] - Training Epoch: 1/2, step 2443/7134 completed (loss: 0.058730341494083405, acc: 0.9819193482398987)
[2025-02-13 02:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:08][root][INFO] - Training Epoch: 1/2, step 2444/7134 completed (loss: 0.05096106603741646, acc: 0.9829545617103577)
[2025-02-13 02:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:08][root][INFO] - Training Epoch: 1/2, step 2445/7134 completed (loss: 0.08186634629964828, acc: 0.9771573543548584)
[2025-02-13 02:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:09][root][INFO] - Training Epoch: 1/2, step 2446/7134 completed (loss: 0.14171433448791504, acc: 0.9680555462837219)
[2025-02-13 02:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:09][root][INFO] - Training Epoch: 1/2, step 2447/7134 completed (loss: 0.05314350873231888, acc: 0.9778024554252625)
[2025-02-13 02:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:09][root][INFO] - Training Epoch: 1/2, step 2448/7134 completed (loss: 0.04116296023130417, acc: 0.9866310358047485)
[2025-02-13 02:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:10][root][INFO] - Training Epoch: 1/2, step 2449/7134 completed (loss: 0.04876159131526947, acc: 0.9854111671447754)
[2025-02-13 02:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:10][root][INFO] - Training Epoch: 1/2, step 2450/7134 completed (loss: 0.05306633934378624, acc: 0.9822109341621399)
[2025-02-13 02:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11][root][INFO] - Training Epoch: 1/2, step 2451/7134 completed (loss: 0.0207749605178833, acc: 0.9910827875137329)
[2025-02-13 02:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11][root][INFO] - Training Epoch: 1/2, step 2452/7134 completed (loss: 0.030182410031557083, acc: 0.9882965087890625)
[2025-02-13 02:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:12][root][INFO] - Training Epoch: 1/2, step 2453/7134 completed (loss: 0.05875067040324211, acc: 0.9819355010986328)
[2025-02-13 02:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:12][root][INFO] - Training Epoch: 1/2, step 2454/7134 completed (loss: 0.037334512919187546, acc: 0.9832776188850403)
[2025-02-13 02:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:13][root][INFO] - Training Epoch: 1/2, step 2455/7134 completed (loss: 0.04432973638176918, acc: 0.9886685609817505)
[2025-02-13 02:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:13][root][INFO] - Training Epoch: 1/2, step 2456/7134 completed (loss: 0.06466435641050339, acc: 0.9792477488517761)
[2025-02-13 02:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:13][root][INFO] - Training Epoch: 1/2, step 2457/7134 completed (loss: 0.05858893692493439, acc: 0.9850249290466309)
[2025-02-13 02:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:14][root][INFO] - Training Epoch: 1/2, step 2458/7134 completed (loss: 0.08544868230819702, acc: 0.9795597195625305)
[2025-02-13 02:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:14][root][INFO] - Training Epoch: 1/2, step 2459/7134 completed (loss: 0.013933205977082253, acc: 0.9970414042472839)
[2025-02-13 02:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15][root][INFO] - Training Epoch: 1/2, step 2460/7134 completed (loss: 0.040192704647779465, acc: 0.9903181195259094)
[2025-02-13 02:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15][root][INFO] - Training Epoch: 1/2, step 2461/7134 completed (loss: 0.027195177972316742, acc: 0.9884058237075806)
[2025-02-13 02:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16][root][INFO] - Training Epoch: 1/2, step 2462/7134 completed (loss: 0.055012691766023636, acc: 0.9834482669830322)
[2025-02-13 02:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16][root][INFO] - Training Epoch: 1/2, step 2463/7134 completed (loss: 0.026319298893213272, acc: 0.9924356937408447)
[2025-02-13 02:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16][root][INFO] - Training Epoch: 1/2, step 2464/7134 completed (loss: 0.05600687488913536, acc: 0.9868913888931274)
[2025-02-13 02:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17][root][INFO] - Training Epoch: 1/2, step 2465/7134 completed (loss: 0.04297586530447006, acc: 0.9867549538612366)
[2025-02-13 02:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17][root][INFO] - Training Epoch: 1/2, step 2466/7134 completed (loss: 0.025803236290812492, acc: 0.9880794882774353)
[2025-02-13 02:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18][root][INFO] - Training Epoch: 1/2, step 2467/7134 completed (loss: 0.03429103270173073, acc: 0.9910025596618652)
[2025-02-13 02:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18][root][INFO] - Training Epoch: 1/2, step 2468/7134 completed (loss: 0.018475137650966644, acc: 0.9930070042610168)
[2025-02-13 02:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:19][root][INFO] - Training Epoch: 1/2, step 2469/7134 completed (loss: 0.022062476724386215, acc: 0.9904502034187317)
[2025-02-13 02:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:19][root][INFO] - Training Epoch: 1/2, step 2470/7134 completed (loss: 0.04837092012166977, acc: 0.9898989796638489)
[2025-02-13 02:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20][root][INFO] - Training Epoch: 1/2, step 2471/7134 completed (loss: 0.03185445815324783, acc: 0.9933110475540161)
[2025-02-13 02:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20][root][INFO] - Training Epoch: 1/2, step 2472/7134 completed (loss: 0.01102039311081171, acc: 0.9969696998596191)
[2025-02-13 02:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20][root][INFO] - Training Epoch: 1/2, step 2473/7134 completed (loss: 0.03178052976727486, acc: 0.9877049326896667)
[2025-02-13 02:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21][root][INFO] - Training Epoch: 1/2, step 2474/7134 completed (loss: 0.023232351988554, acc: 0.9915493130683899)
[2025-02-13 02:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21][root][INFO] - Training Epoch: 1/2, step 2475/7134 completed (loss: 0.020612871274352074, acc: 0.9931389093399048)
[2025-02-13 02:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:22][root][INFO] - Training Epoch: 1/2, step 2476/7134 completed (loss: 0.037952423095703125, acc: 0.9944289922714233)
[2025-02-13 02:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:22][root][INFO] - Training Epoch: 1/2, step 2477/7134 completed (loss: 0.04161117970943451, acc: 0.9903314709663391)
[2025-02-13 02:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:23][root][INFO] - Training Epoch: 1/2, step 2478/7134 completed (loss: 0.011982344090938568, acc: 0.9953271150588989)
[2025-02-13 02:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:23][root][INFO] - Training Epoch: 1/2, step 2479/7134 completed (loss: 0.08629857003688812, acc: 0.9793322682380676)
[2025-02-13 02:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:23][root][INFO] - Training Epoch: 1/2, step 2480/7134 completed (loss: 0.007627991493791342, acc: 0.9977777600288391)
[2025-02-13 02:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24][root][INFO] - Training Epoch: 1/2, step 2481/7134 completed (loss: 0.0413455069065094, acc: 0.9800994992256165)
[2025-02-13 02:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24][root][INFO] - Training Epoch: 1/2, step 2482/7134 completed (loss: 0.053058091551065445, acc: 0.9823434948921204)
[2025-02-13 02:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:25][root][INFO] - Training Epoch: 1/2, step 2483/7134 completed (loss: 0.016343655064702034, acc: 0.994452178478241)
[2025-02-13 02:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:25][root][INFO] - Training Epoch: 1/2, step 2484/7134 completed (loss: 0.02516539953649044, acc: 0.9902777671813965)
[2025-02-13 02:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:26][root][INFO] - Training Epoch: 1/2, step 2485/7134 completed (loss: 0.0245673805475235, acc: 0.9952380657196045)
[2025-02-13 02:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:26][root][INFO] - Training Epoch: 1/2, step 2486/7134 completed (loss: 0.033387936651706696, acc: 0.9928160905838013)
[2025-02-13 02:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:26][root][INFO] - Training Epoch: 1/2, step 2487/7134 completed (loss: 0.08846360445022583, acc: 0.9763779640197754)
[2025-02-13 02:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:27][root][INFO] - Training Epoch: 1/2, step 2488/7134 completed (loss: 0.025712644681334496, acc: 0.9917126893997192)
[2025-02-13 02:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:27][root][INFO] - Training Epoch: 1/2, step 2489/7134 completed (loss: 0.03216422721743584, acc: 0.9916434288024902)
[2025-02-13 02:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28][root][INFO] - Training Epoch: 1/2, step 2490/7134 completed (loss: 0.021763663738965988, acc: 0.9942938685417175)
[2025-02-13 02:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28][root][INFO] - Training Epoch: 1/2, step 2491/7134 completed (loss: 0.12049449980258942, acc: 0.9682539701461792)
[2025-02-13 02:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:29][root][INFO] - Training Epoch: 1/2, step 2492/7134 completed (loss: 0.053795792162418365, acc: 0.985111653804779)
[2025-02-13 02:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:29][root][INFO] - Training Epoch: 1/2, step 2493/7134 completed (loss: 0.06599307805299759, acc: 0.9718309640884399)
[2025-02-13 02:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:29][root][INFO] - Training Epoch: 1/2, step 2494/7134 completed (loss: 0.060471221804618835, acc: 0.9837296605110168)
[2025-02-13 02:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:30][root][INFO] - Training Epoch: 1/2, step 2495/7134 completed (loss: 0.06355924159288406, acc: 0.974219799041748)
[2025-02-13 02:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:30][root][INFO] - Training Epoch: 1/2, step 2496/7134 completed (loss: 0.02638634480535984, acc: 0.9887640476226807)
[2025-02-13 02:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31][root][INFO] - Training Epoch: 1/2, step 2497/7134 completed (loss: 0.03140703961253166, acc: 0.9902676343917847)
[2025-02-13 02:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31][root][INFO] - Training Epoch: 1/2, step 2498/7134 completed (loss: 0.05090383067727089, acc: 0.9852104783058167)
[2025-02-13 02:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:32][root][INFO] - Training Epoch: 1/2, step 2499/7134 completed (loss: 0.024586888030171394, acc: 0.9927536249160767)
[2025-02-13 02:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:32][root][INFO] - Training Epoch: 1/2, step 2500/7134 completed (loss: 0.017625097185373306, acc: 0.9948520064353943)
[2025-02-13 02:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:33][root][INFO] - Training Epoch: 1/2, step 2501/7134 completed (loss: 0.0438200868666172, acc: 0.9874141812324524)
[2025-02-13 02:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:33][root][INFO] - Training Epoch: 1/2, step 2502/7134 completed (loss: 0.022226767614483833, acc: 0.9946996569633484)
[2025-02-13 02:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:33][root][INFO] - Training Epoch: 1/2, step 2503/7134 completed (loss: 0.03473183512687683, acc: 0.9928774833679199)
[2025-02-13 02:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34][root][INFO] - Training Epoch: 1/2, step 2504/7134 completed (loss: 0.021120799705386162, acc: 0.9933333396911621)
[2025-02-13 02:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34][root][INFO] - Training Epoch: 1/2, step 2505/7134 completed (loss: 0.03476608172059059, acc: 0.9908987283706665)
[2025-02-13 02:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:35][root][INFO] - Training Epoch: 1/2, step 2506/7134 completed (loss: 0.028275270015001297, acc: 0.9903846383094788)
[2025-02-13 02:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:35][root][INFO] - Training Epoch: 1/2, step 2507/7134 completed (loss: 0.012715366668999195, acc: 0.9953542351722717)
[2025-02-13 02:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:36][root][INFO] - Training Epoch: 1/2, step 2508/7134 completed (loss: 0.05610114336013794, acc: 0.9873272180557251)
[2025-02-13 02:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:36][root][INFO] - Training Epoch: 1/2, step 2509/7134 completed (loss: 0.032420869916677475, acc: 0.9926560521125793)
[2025-02-13 02:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37][root][INFO] - Training Epoch: 1/2, step 2510/7134 completed (loss: 0.03465622290968895, acc: 0.9908952713012695)
[2025-02-13 02:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37][root][INFO] - Training Epoch: 1/2, step 2511/7134 completed (loss: 0.05912335589528084, acc: 0.9843049049377441)
[2025-02-13 02:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37][root][INFO] - Training Epoch: 1/2, step 2512/7134 completed (loss: 0.07639806717634201, acc: 0.9797570705413818)
[2025-02-13 02:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38][root][INFO] - Training Epoch: 1/2, step 2513/7134 completed (loss: 0.03307167440652847, acc: 0.9905213117599487)
[2025-02-13 02:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38][root][INFO] - Training Epoch: 1/2, step 2514/7134 completed (loss: 0.021368227899074554, acc: 0.9918319582939148)
[2025-02-13 02:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:39][root][INFO] - Training Epoch: 1/2, step 2515/7134 completed (loss: 0.012883353978395462, acc: 0.9973992109298706)
[2025-02-13 02:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:39][root][INFO] - Training Epoch: 1/2, step 2516/7134 completed (loss: 0.017680907621979713, acc: 0.9954545497894287)
[2025-02-13 02:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40][root][INFO] - Training Epoch: 1/2, step 2517/7134 completed (loss: 0.03384662792086601, acc: 0.991963267326355)
[2025-02-13 02:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40][root][INFO] - Training Epoch: 1/2, step 2518/7134 completed (loss: 0.03704432398080826, acc: 0.9885550737380981)
[2025-02-13 02:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:41][root][INFO] - Training Epoch: 1/2, step 2519/7134 completed (loss: 0.015699705109000206, acc: 0.9958506226539612)
[2025-02-13 02:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:41][root][INFO] - Training Epoch: 1/2, step 2520/7134 completed (loss: 0.03232070803642273, acc: 0.9886040091514587)
[2025-02-13 02:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42][root][INFO] - Training Epoch: 1/2, step 2521/7134 completed (loss: 0.06818678230047226, acc: 0.9832689762115479)
[2025-02-13 02:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42][root][INFO] - Training Epoch: 1/2, step 2522/7134 completed (loss: 0.03960825502872467, acc: 0.9871060252189636)
[2025-02-13 02:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42][root][INFO] - Training Epoch: 1/2, step 2523/7134 completed (loss: 0.04836217314004898, acc: 0.9934554696083069)
[2025-02-13 02:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:43][root][INFO] - Training Epoch: 1/2, step 2524/7134 completed (loss: 0.009430897422134876, acc: 0.9986187815666199)
[2025-02-13 02:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:43][root][INFO] - Training Epoch: 1/2, step 2525/7134 completed (loss: 0.028229765594005585, acc: 0.9916527271270752)
[2025-02-13 02:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44][root][INFO] - Training Epoch: 1/2, step 2526/7134 completed (loss: 0.02038392424583435, acc: 0.9929412007331848)
[2025-02-13 02:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44][root][INFO] - Training Epoch: 1/2, step 2527/7134 completed (loss: 0.03688300773501396, acc: 0.9914634227752686)
[2025-02-13 02:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44][root][INFO] - Training Epoch: 1/2, step 2528/7134 completed (loss: 0.010785817168653011, acc: 0.99726402759552)
[2025-02-13 02:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:45][root][INFO] - Training Epoch: 1/2, step 2529/7134 completed (loss: 0.016536977142095566, acc: 0.9946808218955994)
[2025-02-13 02:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:45][root][INFO] - Training Epoch: 1/2, step 2530/7134 completed (loss: 0.03082423284649849, acc: 0.9896103739738464)
[2025-02-13 02:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46][root][INFO] - Training Epoch: 1/2, step 2531/7134 completed (loss: 0.011159193702042103, acc: 0.9944751262664795)
[2025-02-13 02:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46][root][INFO] - Training Epoch: 1/2, step 2532/7134 completed (loss: 0.019224392250180244, acc: 0.9953774809837341)
[2025-02-13 02:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:47][root][INFO] - Training Epoch: 1/2, step 2533/7134 completed (loss: 0.03623789921402931, acc: 0.9954751133918762)
[2025-02-13 02:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:47][root][INFO] - Training Epoch: 1/2, step 2534/7134 completed (loss: 0.03629598766565323, acc: 0.9923567175865173)
[2025-02-13 02:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:47][root][INFO] - Training Epoch: 1/2, step 2535/7134 completed (loss: 0.04222913831472397, acc: 0.9865125417709351)
[2025-02-13 02:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48][root][INFO] - Training Epoch: 1/2, step 2536/7134 completed (loss: 0.05817155912518501, acc: 0.9815497994422913)
[2025-02-13 02:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48][root][INFO] - Training Epoch: 1/2, step 2537/7134 completed (loss: 0.07543458789587021, acc: 0.9801223278045654)
[2025-02-13 02:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:49][root][INFO] - Training Epoch: 1/2, step 2538/7134 completed (loss: 0.06626199930906296, acc: 0.980169951915741)
[2025-02-13 02:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:49][root][INFO] - Training Epoch: 1/2, step 2539/7134 completed (loss: 0.024864226579666138, acc: 0.9928571581840515)
[2025-02-13 02:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50][root][INFO] - Training Epoch: 1/2, step 2540/7134 completed (loss: 0.0680953711271286, acc: 0.9831804037094116)
[2025-02-13 02:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50][root][INFO] - Training Epoch: 1/2, step 2541/7134 completed (loss: 0.019256308674812317, acc: 0.9950330853462219)
[2025-02-13 02:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50][root][INFO] - Training Epoch: 1/2, step 2542/7134 completed (loss: 0.07049047946929932, acc: 0.9852070808410645)
[2025-02-13 02:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:51][root][INFO] - Training Epoch: 1/2, step 2543/7134 completed (loss: 0.07591801136732101, acc: 0.9781420826911926)
[2025-02-13 02:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:51][root][INFO] - Training Epoch: 1/2, step 2544/7134 completed (loss: 0.04310793802142143, acc: 0.9845361113548279)
[2025-02-13 02:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52][root][INFO] - Training Epoch: 1/2, step 2545/7134 completed (loss: 0.03396476432681084, acc: 0.9877836108207703)
[2025-02-13 02:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52][root][INFO] - Training Epoch: 1/2, step 2546/7134 completed (loss: 0.14200706779956818, acc: 0.9603841304779053)
[2025-02-13 02:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53][root][INFO] - Training Epoch: 1/2, step 2547/7134 completed (loss: 0.06403884291648865, acc: 0.9822866320610046)
[2025-02-13 02:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53][root][INFO] - Training Epoch: 1/2, step 2548/7134 completed (loss: 0.03365344554185867, acc: 0.9921259880065918)
[2025-02-13 02:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53][root][INFO] - Training Epoch: 1/2, step 2549/7134 completed (loss: 0.052195362746715546, acc: 0.9834254384040833)
[2025-02-13 02:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54][root][INFO] - Training Epoch: 1/2, step 2550/7134 completed (loss: 0.027996541932225227, acc: 0.990212082862854)
[2025-02-13 02:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54][root][INFO] - Training Epoch: 1/2, step 2551/7134 completed (loss: 0.05880929157137871, acc: 0.9851484894752502)
[2025-02-13 02:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:55][root][INFO] - Training Epoch: 1/2, step 2552/7134 completed (loss: 0.0179765485227108, acc: 0.9905362725257874)
[2025-02-13 02:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:55][root][INFO] - Training Epoch: 1/2, step 2553/7134 completed (loss: 0.05654165521264076, acc: 0.9848942756652832)
[2025-02-13 02:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:55][root][INFO] - Training Epoch: 1/2, step 2554/7134 completed (loss: 0.07532709836959839, acc: 0.981574535369873)
[2025-02-13 02:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56][root][INFO] - Training Epoch: 1/2, step 2555/7134 completed (loss: 0.04450973868370056, acc: 0.9834710955619812)
[2025-02-13 02:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56][root][INFO] - Training Epoch: 1/2, step 2556/7134 completed (loss: 0.06610637903213501, acc: 0.9853249192237854)
[2025-02-13 02:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:57][root][INFO] - Training Epoch: 1/2, step 2557/7134 completed (loss: 0.05826345458626747, acc: 0.9819672107696533)
[2025-02-13 02:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:57][root][INFO] - Training Epoch: 1/2, step 2558/7134 completed (loss: 0.016274480149149895, acc: 0.9967637658119202)
[2025-02-13 02:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:57][root][INFO] - Training Epoch: 1/2, step 2559/7134 completed (loss: 0.02295869030058384, acc: 0.9932340979576111)
[2025-02-13 02:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:58][root][INFO] - Training Epoch: 1/2, step 2560/7134 completed (loss: 0.041564930230379105, acc: 0.9869918823242188)
[2025-02-13 02:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:58][root][INFO] - Training Epoch: 1/2, step 2561/7134 completed (loss: 0.05313737317919731, acc: 0.9825708270072937)
[2025-02-13 02:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59][root][INFO] - Training Epoch: 1/2, step 2562/7134 completed (loss: 0.027930470183491707, acc: 0.9931623935699463)
[2025-02-13 02:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59][root][INFO] - Training Epoch: 1/2, step 2563/7134 completed (loss: 0.0329560711979866, acc: 0.992668628692627)
[2025-02-13 02:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:00][root][INFO] - Training Epoch: 1/2, step 2564/7134 completed (loss: 0.0951070562005043, acc: 0.9832214713096619)
[2025-02-13 02:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:00][root][INFO] - Training Epoch: 1/2, step 2565/7134 completed (loss: 0.03004506789147854, acc: 0.9864048361778259)
[2025-02-13 02:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:00][root][INFO] - Training Epoch: 1/2, step 2566/7134 completed (loss: 0.039332691580057144, acc: 0.9887640476226807)
[2025-02-13 02:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:01][root][INFO] - Training Epoch: 1/2, step 2567/7134 completed (loss: 0.031829990446567535, acc: 0.9876543283462524)
[2025-02-13 02:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:01][root][INFO] - Training Epoch: 1/2, step 2568/7134 completed (loss: 0.05201749876141548, acc: 0.9884393215179443)
[2025-02-13 02:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02][root][INFO] - Training Epoch: 1/2, step 2569/7134 completed (loss: 0.05335366725921631, acc: 0.983433723449707)
[2025-02-13 02:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02][root][INFO] - Training Epoch: 1/2, step 2570/7134 completed (loss: 0.04474041983485222, acc: 0.9895052313804626)
[2025-02-13 02:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:03][root][INFO] - Training Epoch: 1/2, step 2571/7134 completed (loss: 0.02940705604851246, acc: 0.9929701089859009)
[2025-02-13 02:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:03][root][INFO] - Training Epoch: 1/2, step 2572/7134 completed (loss: 0.08631675690412521, acc: 0.9826498627662659)
[2025-02-13 02:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:03][root][INFO] - Training Epoch: 1/2, step 2573/7134 completed (loss: 0.071823351085186, acc: 0.9836065769195557)
[2025-02-13 02:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04][root][INFO] - Training Epoch: 1/2, step 2574/7134 completed (loss: 0.030425550416111946, acc: 0.9890829920768738)
[2025-02-13 02:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04][root][INFO] - Training Epoch: 1/2, step 2575/7134 completed (loss: 0.028049083426594734, acc: 0.9907063245773315)
[2025-02-13 02:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05][root][INFO] - Training Epoch: 1/2, step 2576/7134 completed (loss: 0.03332759812474251, acc: 0.9948275685310364)
[2025-02-13 02:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05][root][INFO] - Training Epoch: 1/2, step 2577/7134 completed (loss: 0.09275837987661362, acc: 0.9829059839248657)
[2025-02-13 02:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05][root][INFO] - Training Epoch: 1/2, step 2578/7134 completed (loss: 0.08280313014984131, acc: 0.9739663004875183)
[2025-02-13 02:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:06][root][INFO] - Training Epoch: 1/2, step 2579/7134 completed (loss: 0.09060858190059662, acc: 0.9746835231781006)
[2025-02-13 02:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:06][root][INFO] - Training Epoch: 1/2, step 2580/7134 completed (loss: 0.05275942012667656, acc: 0.9882506728172302)
[2025-02-13 02:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:07][root][INFO] - Training Epoch: 1/2, step 2581/7134 completed (loss: 0.0442068874835968, acc: 0.9892328381538391)
[2025-02-13 02:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:07][root][INFO] - Training Epoch: 1/2, step 2582/7134 completed (loss: 0.046042393893003464, acc: 0.9864498376846313)
[2025-02-13 02:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08][root][INFO] - Training Epoch: 1/2, step 2583/7134 completed (loss: 0.05301680788397789, acc: 0.9829303026199341)
[2025-02-13 02:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08][root][INFO] - Training Epoch: 1/2, step 2584/7134 completed (loss: 0.06529132276773453, acc: 0.9808027744293213)
[2025-02-13 02:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08][root][INFO] - Training Epoch: 1/2, step 2585/7134 completed (loss: 0.057987529784440994, acc: 0.9870503544807434)
[2025-02-13 02:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:09][root][INFO] - Training Epoch: 1/2, step 2586/7134 completed (loss: 0.05968121439218521, acc: 0.9788732528686523)
[2025-02-13 02:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:09][root][INFO] - Training Epoch: 1/2, step 2587/7134 completed (loss: 0.06027772277593613, acc: 0.9852579832077026)
[2025-02-13 02:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:10][root][INFO] - Training Epoch: 1/2, step 2588/7134 completed (loss: 0.03140714019536972, acc: 0.9890859723091125)
[2025-02-13 02:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:10][root][INFO] - Training Epoch: 1/2, step 2589/7134 completed (loss: 0.04830007627606392, acc: 0.9876325130462646)
[2025-02-13 02:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11][root][INFO] - Training Epoch: 1/2, step 2590/7134 completed (loss: 0.026879392564296722, acc: 0.991830050945282)
[2025-02-13 02:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11][root][INFO] - Training Epoch: 1/2, step 2591/7134 completed (loss: 0.057171471416950226, acc: 0.97826087474823)
[2025-02-13 02:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:12][root][INFO] - Training Epoch: 1/2, step 2592/7134 completed (loss: 0.04677711799740791, acc: 0.9889298677444458)
[2025-02-13 02:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:12][root][INFO] - Training Epoch: 1/2, step 2593/7134 completed (loss: 0.04831702634692192, acc: 0.9908376932144165)
[2025-02-13 02:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:12][root][INFO] - Training Epoch: 1/2, step 2594/7134 completed (loss: 0.048659298568964005, acc: 0.9877675771713257)
[2025-02-13 02:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:13][root][INFO] - Training Epoch: 1/2, step 2595/7134 completed (loss: 0.03662873059511185, acc: 0.9905914068222046)
[2025-02-13 02:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:13][root][INFO] - Training Epoch: 1/2, step 2596/7134 completed (loss: 0.04419111832976341, acc: 0.9899749159812927)
[2025-02-13 02:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:14][root][INFO] - Training Epoch: 1/2, step 2597/7134 completed (loss: 0.05089376121759415, acc: 0.9891892075538635)
[2025-02-13 02:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:14][root][INFO] - Training Epoch: 1/2, step 2598/7134 completed (loss: 0.07970637083053589, acc: 0.9767801761627197)
[2025-02-13 02:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15][root][INFO] - Training Epoch: 1/2, step 2599/7134 completed (loss: 0.011407045647501945, acc: 0.9960421919822693)
[2025-02-13 02:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15][root][INFO] - Training Epoch: 1/2, step 2600/7134 completed (loss: 0.07616356760263443, acc: 0.980793833732605)
[2025-02-13 02:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15][root][INFO] - Training Epoch: 1/2, step 2601/7134 completed (loss: 0.08462916314601898, acc: 0.975642740726471)
[2025-02-13 02:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:16][root][INFO] - Training Epoch: 1/2, step 2602/7134 completed (loss: 0.05187952145934105, acc: 0.980629563331604)
[2025-02-13 02:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:16][root][INFO] - Training Epoch: 1/2, step 2603/7134 completed (loss: 0.05587194859981537, acc: 0.9860464930534363)
[2025-02-13 02:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:17][root][INFO] - Training Epoch: 1/2, step 2604/7134 completed (loss: 0.03396077826619148, acc: 0.9868420958518982)
[2025-02-13 02:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:17][root][INFO] - Training Epoch: 1/2, step 2605/7134 completed (loss: 0.06645984947681427, acc: 0.9855595827102661)
[2025-02-13 02:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18][root][INFO] - Training Epoch: 1/2, step 2606/7134 completed (loss: 0.044944390654563904, acc: 0.9899280667304993)
[2025-02-13 02:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18][root][INFO] - Training Epoch: 1/2, step 2607/7134 completed (loss: 0.05497533828020096, acc: 0.9764705896377563)
[2025-02-13 02:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:19][root][INFO] - Training Epoch: 1/2, step 2608/7134 completed (loss: 0.05676477029919624, acc: 0.978986382484436)
[2025-02-13 02:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:19][root][INFO] - Training Epoch: 1/2, step 2609/7134 completed (loss: 0.07893437147140503, acc: 0.9860724210739136)
[2025-02-13 02:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20][root][INFO] - Training Epoch: 1/2, step 2610/7134 completed (loss: 0.0889703631401062, acc: 0.9847328066825867)
[2025-02-13 02:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20][root][INFO] - Training Epoch: 1/2, step 2611/7134 completed (loss: 0.09061042964458466, acc: 0.9758898019790649)
[2025-02-13 02:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20][root][INFO] - Training Epoch: 1/2, step 2612/7134 completed (loss: 0.06905638426542282, acc: 0.981776773929596)
[2025-02-13 02:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:21][root][INFO] - Training Epoch: 1/2, step 2613/7134 completed (loss: 0.03704981878399849, acc: 0.9848101139068604)
[2025-02-13 02:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:21][root][INFO] - Training Epoch: 1/2, step 2614/7134 completed (loss: 0.09040278196334839, acc: 0.9738041162490845)
[2025-02-13 02:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22][root][INFO] - Training Epoch: 1/2, step 2615/7134 completed (loss: 0.05755891650915146, acc: 0.9820051193237305)
[2025-02-13 02:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22][root][INFO] - Training Epoch: 1/2, step 2616/7134 completed (loss: 0.08341608196496964, acc: 0.9814814925193787)
[2025-02-13 02:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:23][root][INFO] - Training Epoch: 1/2, step 2617/7134 completed (loss: 0.07016706466674805, acc: 0.9811066389083862)
[2025-02-13 02:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:23][root][INFO] - Training Epoch: 1/2, step 2618/7134 completed (loss: 0.0324598103761673, acc: 0.987679660320282)
[2025-02-13 02:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24][root][INFO] - Training Epoch: 1/2, step 2619/7134 completed (loss: 0.06485253572463989, acc: 0.9832214713096619)
[2025-02-13 02:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24][root][INFO] - Training Epoch: 1/2, step 2620/7134 completed (loss: 0.05328303948044777, acc: 0.9851301312446594)
[2025-02-13 02:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24][root][INFO] - Training Epoch: 1/2, step 2621/7134 completed (loss: 0.043477293103933334, acc: 0.9845971465110779)
[2025-02-13 02:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:25][root][INFO] - Training Epoch: 1/2, step 2622/7134 completed (loss: 0.05713663622736931, acc: 0.9819967150688171)
[2025-02-13 02:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:25][root][INFO] - Training Epoch: 1/2, step 2623/7134 completed (loss: 0.05582515150308609, acc: 0.9839080572128296)
[2025-02-13 02:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:26][root][INFO] - Training Epoch: 1/2, step 2624/7134 completed (loss: 0.06473120301961899, acc: 0.9808773994445801)
[2025-02-13 02:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:26][root][INFO] - Training Epoch: 1/2, step 2625/7134 completed (loss: 0.05090707540512085, acc: 0.9833333492279053)
[2025-02-13 02:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27][root][INFO] - Training Epoch: 1/2, step 2626/7134 completed (loss: 0.035316143184900284, acc: 0.9876237511634827)
[2025-02-13 02:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27][root][INFO] - Training Epoch: 1/2, step 2627/7134 completed (loss: 0.03723067417740822, acc: 0.9932340979576111)
[2025-02-13 02:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:28][root][INFO] - Training Epoch: 1/2, step 2628/7134 completed (loss: 0.0702480897307396, acc: 0.9772727489471436)
[2025-02-13 02:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:28][root][INFO] - Training Epoch: 1/2, step 2629/7134 completed (loss: 0.03200096637010574, acc: 0.991150438785553)
[2025-02-13 02:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29][root][INFO] - Training Epoch: 1/2, step 2630/7134 completed (loss: 0.0475202277302742, acc: 0.9840255379676819)
[2025-02-13 02:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29][root][INFO] - Training Epoch: 1/2, step 2631/7134 completed (loss: 0.03889542073011398, acc: 0.9871345162391663)
[2025-02-13 02:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29][root][INFO] - Training Epoch: 1/2, step 2632/7134 completed (loss: 0.022705158218741417, acc: 0.9921414256095886)
[2025-02-13 02:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:30][root][INFO] - Training Epoch: 1/2, step 2633/7134 completed (loss: 0.0468570701777935, acc: 0.9844192862510681)
[2025-02-13 02:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:30][root][INFO] - Training Epoch: 1/2, step 2634/7134 completed (loss: 0.027270570397377014, acc: 0.9910045266151428)
[2025-02-13 02:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:31][root][INFO] - Training Epoch: 1/2, step 2635/7134 completed (loss: 0.13203254342079163, acc: 0.9750000238418579)
[2025-02-13 02:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:31][root][INFO] - Training Epoch: 1/2, step 2636/7134 completed (loss: 0.06609674543142319, acc: 0.9843527674674988)
[2025-02-13 02:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32][root][INFO] - Training Epoch: 1/2, step 2637/7134 completed (loss: 0.22349056601524353, acc: 0.9473684430122375)
[2025-02-13 02:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32][root][INFO] - Training Epoch: 1/2, step 2638/7134 completed (loss: 0.10396035015583038, acc: 0.9674054980278015)
[2025-02-13 02:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:33][root][INFO] - Training Epoch: 1/2, step 2639/7134 completed (loss: 0.095799520611763, acc: 0.9688796401023865)
[2025-02-13 02:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:33][root][INFO] - Training Epoch: 1/2, step 2640/7134 completed (loss: 0.09127998352050781, acc: 0.9800249934196472)
[2025-02-13 02:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:33][root][INFO] - Training Epoch: 1/2, step 2641/7134 completed (loss: 0.08662810921669006, acc: 0.9764359593391418)
[2025-02-13 02:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:34][root][INFO] - Training Epoch: 1/2, step 2642/7134 completed (loss: 0.10896749049425125, acc: 0.9715302586555481)
[2025-02-13 02:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:34][root][INFO] - Training Epoch: 1/2, step 2643/7134 completed (loss: 0.08468659967184067, acc: 0.97508305311203)
[2025-02-13 02:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35][root][INFO] - Training Epoch: 1/2, step 2644/7134 completed (loss: 0.06152144819498062, acc: 0.9781553149223328)
[2025-02-13 02:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35][root][INFO] - Training Epoch: 1/2, step 2645/7134 completed (loss: 0.05640248581767082, acc: 0.9795918464660645)
[2025-02-13 02:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:36][root][INFO] - Training Epoch: 1/2, step 2646/7134 completed (loss: 0.1375824362039566, acc: 0.9681528806686401)
[2025-02-13 02:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:36][root][INFO] - Training Epoch: 1/2, step 2647/7134 completed (loss: 0.046898964792490005, acc: 0.9895969033241272)
[2025-02-13 02:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:37][root][INFO] - Training Epoch: 1/2, step 2648/7134 completed (loss: 0.04041139408946037, acc: 0.9883871078491211)
[2025-02-13 02:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:37][root][INFO] - Training Epoch: 1/2, step 2649/7134 completed (loss: 0.13187429308891296, acc: 0.9597615599632263)
[2025-02-13 02:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:37][root][INFO] - Training Epoch: 1/2, step 2650/7134 completed (loss: 0.11837852746248245, acc: 0.974281370639801)
[2025-02-13 02:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:38][root][INFO] - Training Epoch: 1/2, step 2651/7134 completed (loss: 0.040053460747003555, acc: 0.9833134412765503)
[2025-02-13 02:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:38][root][INFO] - Training Epoch: 1/2, step 2652/7134 completed (loss: 0.060983289033174515, acc: 0.9848675727844238)
[2025-02-13 02:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39][root][INFO] - Training Epoch: 1/2, step 2653/7134 completed (loss: 0.06877666711807251, acc: 0.9769737124443054)
[2025-02-13 02:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39][root][INFO] - Training Epoch: 1/2, step 2654/7134 completed (loss: 0.043260227888822556, acc: 0.9901639223098755)
[2025-02-13 02:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:40][root][INFO] - Training Epoch: 1/2, step 2655/7134 completed (loss: 0.04784887656569481, acc: 0.9894099831581116)
[2025-02-13 02:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:40][root][INFO] - Training Epoch: 1/2, step 2656/7134 completed (loss: 0.05360019952058792, acc: 0.9888734221458435)
[2025-02-13 02:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:40][root][INFO] - Training Epoch: 1/2, step 2657/7134 completed (loss: 0.04462680593132973, acc: 0.9887096881866455)
[2025-02-13 02:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41][root][INFO] - Training Epoch: 1/2, step 2658/7134 completed (loss: 0.11458385735750198, acc: 0.9719626307487488)
[2025-02-13 02:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41][root][INFO] - Training Epoch: 1/2, step 2659/7134 completed (loss: 0.06252092868089676, acc: 0.9865900278091431)
[2025-02-13 02:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:42][root][INFO] - Training Epoch: 1/2, step 2660/7134 completed (loss: 0.04917436093091965, acc: 0.9858871102333069)
[2025-02-13 02:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:42][root][INFO] - Training Epoch: 1/2, step 2661/7134 completed (loss: 0.04822012037038803, acc: 0.9929873943328857)
[2025-02-13 02:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:43][root][INFO] - Training Epoch: 1/2, step 2662/7134 completed (loss: 0.021197998896241188, acc: 0.9943100810050964)
[2025-02-13 02:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:43][root][INFO] - Training Epoch: 1/2, step 2663/7134 completed (loss: 0.0392506904900074, acc: 0.9871345162391663)
[2025-02-13 02:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:44][root][INFO] - Training Epoch: 1/2, step 2664/7134 completed (loss: 0.01929948292672634, acc: 0.9935483932495117)
[2025-02-13 02:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:44][root][INFO] - Training Epoch: 1/2, step 2665/7134 completed (loss: 0.01718408800661564, acc: 0.99609375)
[2025-02-13 02:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:44][root][INFO] - Training Epoch: 1/2, step 2666/7134 completed (loss: 0.03700348362326622, acc: 0.9914320707321167)
[2025-02-13 02:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:45][root][INFO] - Training Epoch: 1/2, step 2667/7134 completed (loss: 0.023661138489842415, acc: 0.9971671104431152)
[2025-02-13 02:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:45][root][INFO] - Training Epoch: 1/2, step 2668/7134 completed (loss: 0.024690816178917885, acc: 0.9959623217582703)
[2025-02-13 02:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46][root][INFO] - Training Epoch: 1/2, step 2669/7134 completed (loss: 0.013730911538004875, acc: 0.9953970313072205)
[2025-02-13 02:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46][root][INFO] - Training Epoch: 1/2, step 2670/7134 completed (loss: 0.03142610192298889, acc: 0.9868938326835632)
[2025-02-13 02:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:47][root][INFO] - Training Epoch: 1/2, step 2671/7134 completed (loss: 0.03951446712017059, acc: 0.9839572310447693)
[2025-02-13 02:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:47][root][INFO] - Training Epoch: 1/2, step 2672/7134 completed (loss: 0.016015902161598206, acc: 0.9970845580101013)
[2025-02-13 02:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:48][root][INFO] - Training Epoch: 1/2, step 2673/7134 completed (loss: 0.02442873828113079, acc: 0.9924242496490479)
[2025-02-13 02:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:48][root][INFO] - Training Epoch: 1/2, step 2674/7134 completed (loss: 0.018443258479237556, acc: 0.9930264949798584)
[2025-02-13 02:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:48][root][INFO] - Training Epoch: 1/2, step 2675/7134 completed (loss: 0.011734183877706528, acc: 0.9972602725028992)
[2025-02-13 02:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49][root][INFO] - Training Epoch: 1/2, step 2676/7134 completed (loss: 0.02679077535867691, acc: 0.9896103739738464)
[2025-02-13 02:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49][root][INFO] - Training Epoch: 1/2, step 2677/7134 completed (loss: 0.01748906634747982, acc: 0.9940476417541504)
[2025-02-13 02:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:50][root][INFO] - Training Epoch: 1/2, step 2678/7134 completed (loss: 0.005110784433782101, acc: 1.0)
[2025-02-13 02:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:50][root][INFO] - Training Epoch: 1/2, step 2679/7134 completed (loss: 0.008867869153618813, acc: 0.9977553486824036)
[2025-02-13 02:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51][root][INFO] - Training Epoch: 1/2, step 2680/7134 completed (loss: 0.019248399883508682, acc: 0.995067834854126)
[2025-02-13 02:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51][root][INFO] - Training Epoch: 1/2, step 2681/7134 completed (loss: 0.008427984081208706, acc: 0.9969651103019714)
[2025-02-13 02:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:52][root][INFO] - Training Epoch: 1/2, step 2682/7134 completed (loss: 0.008886359632015228, acc: 0.9984050989151001)
[2025-02-13 02:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:52][root][INFO] - Training Epoch: 1/2, step 2683/7134 completed (loss: 0.010466175153851509, acc: 0.9954476356506348)
[2025-02-13 02:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:52][root][INFO] - Training Epoch: 1/2, step 2684/7134 completed (loss: 0.04015348106622696, acc: 0.9946236610412598)
[2025-02-13 02:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53][root][INFO] - Training Epoch: 1/2, step 2685/7134 completed (loss: 0.01879267394542694, acc: 0.9947984218597412)
[2025-02-13 02:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53][root][INFO] - Training Epoch: 1/2, step 2686/7134 completed (loss: 0.06342794746160507, acc: 0.9814814925193787)
[2025-02-13 02:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:54][root][INFO] - Training Epoch: 1/2, step 2687/7134 completed (loss: 0.05803492292761803, acc: 0.9840989112854004)
[2025-02-13 02:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:54][root][INFO] - Training Epoch: 1/2, step 2688/7134 completed (loss: 0.05116994306445122, acc: 0.9842767119407654)
[2025-02-13 02:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55][root][INFO] - Training Epoch: 1/2, step 2689/7134 completed (loss: 0.053792305290699005, acc: 0.9873417615890503)
[2025-02-13 02:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55][root][INFO] - Training Epoch: 1/2, step 2690/7134 completed (loss: 0.07361973077058792, acc: 0.9752704501152039)
[2025-02-13 02:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:56][root][INFO] - Training Epoch: 1/2, step 2691/7134 completed (loss: 0.09248987585306168, acc: 0.98046875)
[2025-02-13 02:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:56][root][INFO] - Training Epoch: 1/2, step 2692/7134 completed (loss: 0.14306612312793732, acc: 0.9646182656288147)
[2025-02-13 02:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:56][root][INFO] - Training Epoch: 1/2, step 2693/7134 completed (loss: 0.12634597718715668, acc: 0.9730769395828247)
[2025-02-13 02:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57][root][INFO] - Training Epoch: 1/2, step 2694/7134 completed (loss: 0.11785333603620529, acc: 0.9808823466300964)
[2025-02-13 02:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57][root][INFO] - Training Epoch: 1/2, step 2695/7134 completed (loss: 0.05728983134031296, acc: 0.981574535369873)
[2025-02-13 02:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:58][root][INFO] - Training Epoch: 1/2, step 2696/7134 completed (loss: 0.08678998053073883, acc: 0.9777424335479736)
[2025-02-13 02:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:58][root][INFO] - Training Epoch: 1/2, step 2697/7134 completed (loss: 0.08007188886404037, acc: 0.9722650051116943)
[2025-02-13 02:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:59][root][INFO] - Training Epoch: 1/2, step 2698/7134 completed (loss: 0.03697708249092102, acc: 0.9897435903549194)
[2025-02-13 02:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:59][root][INFO] - Training Epoch: 1/2, step 2699/7134 completed (loss: 0.03873938322067261, acc: 0.9933775067329407)
[2025-02-13 02:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:59][root][INFO] - Training Epoch: 1/2, step 2700/7134 completed (loss: 0.045509908348321915, acc: 0.9821958541870117)
[2025-02-13 02:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00][root][INFO] - Training Epoch: 1/2, step 2701/7134 completed (loss: 0.056175705045461655, acc: 0.9798319339752197)
[2025-02-13 02:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00][root][INFO] - Training Epoch: 1/2, step 2702/7134 completed (loss: 0.07091494649648666, acc: 0.9739583134651184)
[2025-02-13 02:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:01][root][INFO] - Training Epoch: 1/2, step 2703/7134 completed (loss: 0.03259367123246193, acc: 0.9904458522796631)
[2025-02-13 02:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:01][root][INFO] - Training Epoch: 1/2, step 2704/7134 completed (loss: 0.05637839809060097, acc: 0.9878706336021423)
[2025-02-13 02:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:01][root][INFO] - Training Epoch: 1/2, step 2705/7134 completed (loss: 0.09847329556941986, acc: 0.9783950448036194)
[2025-02-13 02:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02][root][INFO] - Training Epoch: 1/2, step 2706/7134 completed (loss: 0.05991031229496002, acc: 0.9854862093925476)
[2025-02-13 02:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02][root][INFO] - Training Epoch: 1/2, step 2707/7134 completed (loss: 0.040523454546928406, acc: 0.9879310131072998)
[2025-02-13 02:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:03][root][INFO] - Training Epoch: 1/2, step 2708/7134 completed (loss: 0.09212498366832733, acc: 0.9705469608306885)
[2025-02-13 02:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:03][root][INFO] - Training Epoch: 1/2, step 2709/7134 completed (loss: 0.041653797030448914, acc: 0.9886363744735718)
[2025-02-13 02:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04][root][INFO] - Training Epoch: 1/2, step 2710/7134 completed (loss: 0.033327218145132065, acc: 0.9906759858131409)
[2025-02-13 02:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04][root][INFO] - Training Epoch: 1/2, step 2711/7134 completed (loss: 0.033297281712293625, acc: 0.9858490824699402)
[2025-02-13 02:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:05][root][INFO] - Training Epoch: 1/2, step 2712/7134 completed (loss: 0.055898141115903854, acc: 0.9888392686843872)
[2025-02-13 02:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:05][root][INFO] - Training Epoch: 1/2, step 2713/7134 completed (loss: 0.05869915708899498, acc: 0.9832869172096252)
[2025-02-13 02:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:06][root][INFO] - Training Epoch: 1/2, step 2714/7134 completed (loss: 0.03931720554828644, acc: 0.9900166392326355)
[2025-02-13 02:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:06][root][INFO] - Training Epoch: 1/2, step 2715/7134 completed (loss: 0.026981649920344353, acc: 0.9909774661064148)
[2025-02-13 02:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:06][root][INFO] - Training Epoch: 1/2, step 2716/7134 completed (loss: 0.047829385846853256, acc: 0.9829620122909546)
[2025-02-13 02:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07][root][INFO] - Training Epoch: 1/2, step 2717/7134 completed (loss: 0.0803736001253128, acc: 0.977412760257721)
[2025-02-13 02:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07][root][INFO] - Training Epoch: 1/2, step 2718/7134 completed (loss: 0.10419170558452606, acc: 0.9700704216957092)
[2025-02-13 02:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:08][root][INFO] - Training Epoch: 1/2, step 2719/7134 completed (loss: 0.06501789391040802, acc: 0.9813874959945679)
[2025-02-13 02:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:08][root][INFO] - Training Epoch: 1/2, step 2720/7134 completed (loss: 0.04356839880347252, acc: 0.9842271208763123)
[2025-02-13 02:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:09][root][INFO] - Training Epoch: 1/2, step 2721/7134 completed (loss: 0.04152287170290947, acc: 0.9898550510406494)
[2025-02-13 02:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:09][root][INFO] - Training Epoch: 1/2, step 2722/7134 completed (loss: 0.14995771646499634, acc: 0.969565212726593)
[2025-02-13 02:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:09][root][INFO] - Training Epoch: 1/2, step 2723/7134 completed (loss: 0.1053028255701065, acc: 0.9766297936439514)
[2025-02-13 02:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10][root][INFO] - Training Epoch: 1/2, step 2724/7134 completed (loss: 0.06071612238883972, acc: 0.9854192137718201)
[2025-02-13 02:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10][root][INFO] - Training Epoch: 1/2, step 2725/7134 completed (loss: 0.03506848216056824, acc: 0.9917898178100586)
[2025-02-13 02:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:11][root][INFO] - Training Epoch: 1/2, step 2726/7134 completed (loss: 0.0715629979968071, acc: 0.9870848655700684)
[2025-02-13 02:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:11][root][INFO] - Training Epoch: 1/2, step 2727/7134 completed (loss: 0.027631115168333054, acc: 0.9925925731658936)
[2025-02-13 02:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:12][root][INFO] - Training Epoch: 1/2, step 2728/7134 completed (loss: 0.11955537647008896, acc: 0.9587156176567078)
[2025-02-13 02:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:12][root][INFO] - Training Epoch: 1/2, step 2729/7134 completed (loss: 0.08013541996479034, acc: 0.9721254110336304)
[2025-02-13 02:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:12][root][INFO] - Training Epoch: 1/2, step 2730/7134 completed (loss: 0.08593463897705078, acc: 0.9750000238418579)
[2025-02-13 02:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13][root][INFO] - Training Epoch: 1/2, step 2731/7134 completed (loss: 0.12668028473854065, acc: 0.9659367203712463)
[2025-02-13 02:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13][root][INFO] - Training Epoch: 1/2, step 2732/7134 completed (loss: 0.06369484215974808, acc: 0.9822335243225098)
[2025-02-13 02:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:14][root][INFO] - Training Epoch: 1/2, step 2733/7134 completed (loss: 0.06525032222270966, acc: 0.9823608994483948)
[2025-02-13 02:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:14][root][INFO] - Training Epoch: 1/2, step 2734/7134 completed (loss: 0.06963851302862167, acc: 0.9818913340568542)
[2025-02-13 02:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:15][root][INFO] - Training Epoch: 1/2, step 2735/7134 completed (loss: 0.05601935833692551, acc: 0.9837398529052734)
[2025-02-13 02:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:15][root][INFO] - Training Epoch: 1/2, step 2736/7134 completed (loss: 0.10594070702791214, acc: 0.9701492786407471)
[2025-02-13 02:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:16][root][INFO] - Training Epoch: 1/2, step 2737/7134 completed (loss: 0.04612494260072708, acc: 0.9884488582611084)
[2025-02-13 02:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:16][root][INFO] - Training Epoch: 1/2, step 2738/7134 completed (loss: 0.04689145088195801, acc: 0.9814285635948181)
[2025-02-13 02:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:16][root][INFO] - Training Epoch: 1/2, step 2739/7134 completed (loss: 0.09572456032037735, acc: 0.9727272987365723)
[2025-02-13 02:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17][root][INFO] - Training Epoch: 1/2, step 2740/7134 completed (loss: 0.03017028607428074, acc: 0.9869375824928284)
[2025-02-13 02:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17][root][INFO] - Training Epoch: 1/2, step 2741/7134 completed (loss: 0.07645022124052048, acc: 0.9742990732192993)
[2025-02-13 02:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18][root][INFO] - Training Epoch: 1/2, step 2742/7134 completed (loss: 0.047234389930963516, acc: 0.9770833253860474)
[2025-02-13 02:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18][root][INFO] - Training Epoch: 1/2, step 2743/7134 completed (loss: 0.02645721845328808, acc: 0.9894737005233765)
[2025-02-13 02:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18][root][INFO] - Training Epoch: 1/2, step 2744/7134 completed (loss: 0.0385848693549633, acc: 0.9885057210922241)
[2025-02-13 02:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19][root][INFO] - Training Epoch: 1/2, step 2745/7134 completed (loss: 0.09404657781124115, acc: 0.9703832864761353)
[2025-02-13 02:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19][root][INFO] - Training Epoch: 1/2, step 2746/7134 completed (loss: 0.04738246649503708, acc: 0.9903846383094788)
[2025-02-13 02:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:20][root][INFO] - Training Epoch: 1/2, step 2747/7134 completed (loss: 0.02632143534719944, acc: 0.9926289916038513)
[2025-02-13 02:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:20][root][INFO] - Training Epoch: 1/2, step 2748/7134 completed (loss: 0.010336149483919144, acc: 0.9968152642250061)
[2025-02-13 02:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:20][root][INFO] - Training Epoch: 1/2, step 2749/7134 completed (loss: 0.0485801063477993, acc: 0.9863387942314148)
[2025-02-13 02:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:21][root][INFO] - Training Epoch: 1/2, step 2750/7134 completed (loss: 0.020782265812158585, acc: 0.9909774661064148)
[2025-02-13 02:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:21][root][INFO] - Training Epoch: 1/2, step 2751/7134 completed (loss: 0.04996692016720772, acc: 0.9848101139068604)
[2025-02-13 02:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:22][root][INFO] - Training Epoch: 1/2, step 2752/7134 completed (loss: 0.04977920651435852, acc: 0.9866920113563538)
[2025-02-13 02:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:22][root][INFO] - Training Epoch: 1/2, step 2753/7134 completed (loss: 0.07262981683015823, acc: 0.9749373197555542)
[2025-02-13 02:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23][root][INFO] - Training Epoch: 1/2, step 2754/7134 completed (loss: 0.05694088712334633, acc: 0.9756592512130737)
[2025-02-13 02:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23][root][INFO] - Training Epoch: 1/2, step 2755/7134 completed (loss: 0.024227650836110115, acc: 0.9941860437393188)
[2025-02-13 02:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23][root][INFO] - Training Epoch: 1/2, step 2756/7134 completed (loss: 0.08972384035587311, acc: 0.9752380847930908)
[2025-02-13 02:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24][root][INFO] - Training Epoch: 1/2, step 2757/7134 completed (loss: 0.09895875304937363, acc: 0.9672130942344666)
[2025-02-13 02:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24][root][INFO] - Training Epoch: 1/2, step 2758/7134 completed (loss: 0.06538234651088715, acc: 0.9840182662010193)
[2025-02-13 02:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:25][root][INFO] - Training Epoch: 1/2, step 2759/7134 completed (loss: 0.03857925906777382, acc: 0.9866220951080322)
[2025-02-13 02:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:25][root][INFO] - Training Epoch: 1/2, step 2760/7134 completed (loss: 0.05294444411993027, acc: 0.9714285731315613)
[2025-02-13 02:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:26][root][INFO] - Training Epoch: 1/2, step 2761/7134 completed (loss: 0.1104489341378212, acc: 0.9709596037864685)
[2025-02-13 02:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:26][root][INFO] - Training Epoch: 1/2, step 2762/7134 completed (loss: 0.057680875062942505, acc: 0.9827387928962708)
[2025-02-13 02:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:26][root][INFO] - Training Epoch: 1/2, step 2763/7134 completed (loss: 0.06690618395805359, acc: 0.9886934757232666)
[2025-02-13 02:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:27][root][INFO] - Training Epoch: 1/2, step 2764/7134 completed (loss: 0.0945173054933548, acc: 0.9762845635414124)
[2025-02-13 02:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:27][root][INFO] - Training Epoch: 1/2, step 2765/7134 completed (loss: 0.04715617373585701, acc: 0.9885057210922241)
[2025-02-13 02:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28][root][INFO] - Training Epoch: 1/2, step 2766/7134 completed (loss: 0.04026338830590248, acc: 0.9903069734573364)
[2025-02-13 02:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28][root][INFO] - Training Epoch: 1/2, step 2767/7134 completed (loss: 0.10247565060853958, acc: 0.9728395342826843)
[2025-02-13 02:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:29][root][INFO] - Training Epoch: 1/2, step 2768/7134 completed (loss: 0.08079993724822998, acc: 0.9787836074829102)
[2025-02-13 02:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:29][root][INFO] - Training Epoch: 1/2, step 2769/7134 completed (loss: 0.05809341371059418, acc: 0.9842342138290405)
[2025-02-13 02:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30][root][INFO] - Training Epoch: 1/2, step 2770/7134 completed (loss: 0.036874473094940186, acc: 0.9902642369270325)
[2025-02-13 02:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30][root][INFO] - Training Epoch: 1/2, step 2771/7134 completed (loss: 0.043091535568237305, acc: 0.986066460609436)
[2025-02-13 02:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:31][root][INFO] - Training Epoch: 1/2, step 2772/7134 completed (loss: 0.038586199283599854, acc: 0.9876957535743713)
[2025-02-13 02:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:31][root][INFO] - Training Epoch: 1/2, step 2773/7134 completed (loss: 0.040015678852796555, acc: 0.9885714054107666)
[2025-02-13 02:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:32][root][INFO] - Training Epoch: 1/2, step 2774/7134 completed (loss: 0.06724482029676437, acc: 0.9813596606254578)
[2025-02-13 02:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:32][root][INFO] - Training Epoch: 1/2, step 2775/7134 completed (loss: 0.04394717141985893, acc: 0.9901840686798096)
[2025-02-13 02:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33][root][INFO] - Training Epoch: 1/2, step 2776/7134 completed (loss: 0.03254540264606476, acc: 0.9897750616073608)
[2025-02-13 02:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33][root][INFO] - Training Epoch: 1/2, step 2777/7134 completed (loss: 0.02708522044122219, acc: 0.9914634227752686)
[2025-02-13 02:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33][root][INFO] - Training Epoch: 1/2, step 2778/7134 completed (loss: 0.041546277701854706, acc: 0.9907299876213074)
[2025-02-13 02:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:34][root][INFO] - Training Epoch: 1/2, step 2779/7134 completed (loss: 0.06126808747649193, acc: 0.9799578189849854)
[2025-02-13 02:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:34][root][INFO] - Training Epoch: 1/2, step 2780/7134 completed (loss: 0.05799434706568718, acc: 0.9894514679908752)
[2025-02-13 02:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:35][root][INFO] - Training Epoch: 1/2, step 2781/7134 completed (loss: 0.035759203135967255, acc: 0.9909326434135437)
[2025-02-13 02:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:35][root][INFO] - Training Epoch: 1/2, step 2782/7134 completed (loss: 0.03769058734178543, acc: 0.9926470518112183)
[2025-02-13 02:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36][root][INFO] - Training Epoch: 1/2, step 2783/7134 completed (loss: 0.036710090935230255, acc: 0.9909909963607788)
[2025-02-13 02:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36][root][INFO] - Training Epoch: 1/2, step 2784/7134 completed (loss: 0.03644724190235138, acc: 0.9905362725257874)
[2025-02-13 02:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:37][root][INFO] - Training Epoch: 1/2, step 2785/7134 completed (loss: 0.023392051458358765, acc: 0.9956331849098206)
[2025-02-13 02:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:37][root][INFO] - Training Epoch: 1/2, step 2786/7134 completed (loss: 0.03804730251431465, acc: 0.9916926026344299)
[2025-02-13 02:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:38][root][INFO] - Training Epoch: 1/2, step 2787/7134 completed (loss: 0.03878797963261604, acc: 0.9853917956352234)
[2025-02-13 02:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:38][root][INFO] - Training Epoch: 1/2, step 2788/7134 completed (loss: 0.043705519288778305, acc: 0.9803664684295654)
[2025-02-13 02:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39][root][INFO] - Training Epoch: 1/2, step 2789/7134 completed (loss: 0.17728836834430695, acc: 0.9481267929077148)
[2025-02-13 02:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39][root][INFO] - Training Epoch: 1/2, step 2790/7134 completed (loss: 0.17204883694648743, acc: 0.9558498859405518)
[2025-02-13 02:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39][root][INFO] - Training Epoch: 1/2, step 2791/7134 completed (loss: 0.056506894528865814, acc: 0.9820627570152283)
[2025-02-13 02:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:40][root][INFO] - Training Epoch: 1/2, step 2792/7134 completed (loss: 0.03890391066670418, acc: 0.99068683385849)
[2025-02-13 02:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:40][root][INFO] - Training Epoch: 1/2, step 2793/7134 completed (loss: 0.06151053309440613, acc: 0.9824561476707458)
[2025-02-13 02:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:41][root][INFO] - Training Epoch: 1/2, step 2794/7134 completed (loss: 0.03826381266117096, acc: 0.990227997303009)
[2025-02-13 02:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:41][root][INFO] - Training Epoch: 1/2, step 2795/7134 completed (loss: 0.1599554419517517, acc: 0.9512761235237122)
[2025-02-13 02:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:41][root][INFO] - Training Epoch: 1/2, step 2796/7134 completed (loss: 0.13991756737232208, acc: 0.960829496383667)
[2025-02-13 02:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42][root][INFO] - Training Epoch: 1/2, step 2797/7134 completed (loss: 0.10092015564441681, acc: 0.9620253443717957)
[2025-02-13 02:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42][root][INFO] - Training Epoch: 1/2, step 2798/7134 completed (loss: 0.10227984935045242, acc: 0.9621848464012146)
[2025-02-13 02:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43][root][INFO] - Training Epoch: 1/2, step 2799/7134 completed (loss: 0.10039541870355606, acc: 0.9719789624214172)
[2025-02-13 02:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43][root][INFO] - Training Epoch: 1/2, step 2800/7134 completed (loss: 0.10002259165048599, acc: 0.9789103865623474)
[2025-02-13 02:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43][root][INFO] - Training Epoch: 1/2, step 2801/7134 completed (loss: 0.059442147612571716, acc: 0.9833759665489197)
[2025-02-13 02:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:44][root][INFO] - Training Epoch: 1/2, step 2802/7134 completed (loss: 0.04404539614915848, acc: 0.9837703108787537)
[2025-02-13 02:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:44][root][INFO] - Training Epoch: 1/2, step 2803/7134 completed (loss: 0.02338373474776745, acc: 0.9929278492927551)
[2025-02-13 02:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:45][root][INFO] - Training Epoch: 1/2, step 2804/7134 completed (loss: 0.04161176085472107, acc: 0.9925742745399475)
[2025-02-13 02:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:45][root][INFO] - Training Epoch: 1/2, step 2805/7134 completed (loss: 0.038089387118816376, acc: 0.990777313709259)
[2025-02-13 02:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46][root][INFO] - Training Epoch: 1/2, step 2806/7134 completed (loss: 0.07019728422164917, acc: 0.9819672107696533)
[2025-02-13 02:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46][root][INFO] - Training Epoch: 1/2, step 2807/7134 completed (loss: 0.048850659281015396, acc: 0.9833679795265198)
[2025-02-13 02:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:47][root][INFO] - Training Epoch: 1/2, step 2808/7134 completed (loss: 0.024180520325899124, acc: 0.9919999837875366)
[2025-02-13 02:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:47][root][INFO] - Training Epoch: 1/2, step 2809/7134 completed (loss: 0.040720537304878235, acc: 0.9821746945381165)
[2025-02-13 02:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48][root][INFO] - Training Epoch: 1/2, step 2810/7134 completed (loss: 0.04113387316465378, acc: 0.9881235361099243)
[2025-02-13 02:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48][root][INFO] - Training Epoch: 1/2, step 2811/7134 completed (loss: 0.015401597134768963, acc: 0.9964200258255005)
[2025-02-13 02:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48][root][INFO] - Training Epoch: 1/2, step 2812/7134 completed (loss: 0.023972008377313614, acc: 0.9953703880310059)
[2025-02-13 02:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:49][root][INFO] - Training Epoch: 1/2, step 2813/7134 completed (loss: 0.02121029607951641, acc: 0.996221661567688)
[2025-02-13 02:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:49][root][INFO] - Training Epoch: 1/2, step 2814/7134 completed (loss: 0.043886926025152206, acc: 0.9909090995788574)
[2025-02-13 02:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50][root][INFO] - Training Epoch: 1/2, step 2815/7134 completed (loss: 0.05054831877350807, acc: 0.9869513511657715)
[2025-02-13 02:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50][root][INFO] - Training Epoch: 1/2, step 2816/7134 completed (loss: 0.06815387308597565, acc: 0.9881423115730286)
[2025-02-13 02:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:51][root][INFO] - Training Epoch: 1/2, step 2817/7134 completed (loss: 0.05253250524401665, acc: 0.9842767119407654)
[2025-02-13 02:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:51][root][INFO] - Training Epoch: 1/2, step 2818/7134 completed (loss: 0.04266748204827309, acc: 0.9857456088066101)
[2025-02-13 02:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52][root][INFO] - Training Epoch: 1/2, step 2819/7134 completed (loss: 0.013445406220853329, acc: 0.9951534867286682)
[2025-02-13 02:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52][root][INFO] - Training Epoch: 1/2, step 2820/7134 completed (loss: 0.10577348619699478, acc: 0.9734904170036316)
[2025-02-13 02:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:53][root][INFO] - Training Epoch: 1/2, step 2821/7134 completed (loss: 0.06124004349112511, acc: 0.9832736253738403)
[2025-02-13 02:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:53][root][INFO] - Training Epoch: 1/2, step 2822/7134 completed (loss: 0.0735928863286972, acc: 0.9810366630554199)
[2025-02-13 02:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:54][root][INFO] - Training Epoch: 1/2, step 2823/7134 completed (loss: 0.07275215536355972, acc: 0.9730496406555176)
[2025-02-13 02:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:54][root][INFO] - Training Epoch: 1/2, step 2824/7134 completed (loss: 0.10948964208364487, acc: 0.9691558480262756)
[2025-02-13 02:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:54][root][INFO] - Training Epoch: 1/2, step 2825/7134 completed (loss: 0.07046207040548325, acc: 0.9852349162101746)
[2025-02-13 02:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55][root][INFO] - Training Epoch: 1/2, step 2826/7134 completed (loss: 0.08764855563640594, acc: 0.9789156913757324)
[2025-02-13 02:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55][root][INFO] - Training Epoch: 1/2, step 2827/7134 completed (loss: 0.06744559109210968, acc: 0.9821200370788574)
[2025-02-13 02:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:56][root][INFO] - Training Epoch: 1/2, step 2828/7134 completed (loss: 0.05399134382605553, acc: 0.9807999730110168)
[2025-02-13 02:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:56][root][INFO] - Training Epoch: 1/2, step 2829/7134 completed (loss: 0.07360666245222092, acc: 0.9849931597709656)
[2025-02-13 02:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57][root][INFO] - Training Epoch: 1/2, step 2830/7134 completed (loss: 0.013855407945811749, acc: 0.9981784820556641)
[2025-02-13 02:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57][root][INFO] - Training Epoch: 1/2, step 2831/7134 completed (loss: 0.052316829562187195, acc: 0.9873617887496948)
[2025-02-13 02:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57][root][INFO] - Training Epoch: 1/2, step 2832/7134 completed (loss: 0.05378004163503647, acc: 0.9917582273483276)
[2025-02-13 02:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:58][root][INFO] - Training Epoch: 1/2, step 2833/7134 completed (loss: 0.059111058712005615, acc: 0.9824561476707458)
[2025-02-13 02:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:58][root][INFO] - Training Epoch: 1/2, step 2834/7134 completed (loss: 0.09469647705554962, acc: 0.9746192693710327)
[2025-02-13 02:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59][root][INFO] - Training Epoch: 1/2, step 2835/7134 completed (loss: 0.06732778251171112, acc: 0.9838449358940125)
[2025-02-13 02:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59][root][INFO] - Training Epoch: 1/2, step 2836/7134 completed (loss: 0.059255361557006836, acc: 0.9881516695022583)
[2025-02-13 02:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:00][root][INFO] - Training Epoch: 1/2, step 2837/7134 completed (loss: 0.028855986893177032, acc: 0.9905405640602112)
[2025-02-13 02:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:00][root][INFO] - Training Epoch: 1/2, step 2838/7134 completed (loss: 0.08368732780218124, acc: 0.9748954176902771)
[2025-02-13 02:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01][root][INFO] - Training Epoch: 1/2, step 2839/7134 completed (loss: 0.05769796296954155, acc: 0.987860381603241)
[2025-02-13 02:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01][root][INFO] - Training Epoch: 1/2, step 2840/7134 completed (loss: 0.027320442721247673, acc: 0.9930939078330994)
[2025-02-13 02:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02][root][INFO] - Training Epoch: 1/2, step 2841/7134 completed (loss: 0.048963602632284164, acc: 0.9884792566299438)
[2025-02-13 02:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02][root][INFO] - Training Epoch: 1/2, step 2842/7134 completed (loss: 0.046902067959308624, acc: 0.983116865158081)
[2025-02-13 02:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02][root][INFO] - Training Epoch: 1/2, step 2843/7134 completed (loss: 0.05378922447562218, acc: 0.9867647290229797)
[2025-02-13 02:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03][root][INFO] - Training Epoch: 1/2, step 2844/7134 completed (loss: 0.13020186126232147, acc: 0.9699042439460754)
[2025-02-13 02:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03][root][INFO] - Training Epoch: 1/2, step 2845/7134 completed (loss: 0.03891846910119057, acc: 0.9878683090209961)
[2025-02-13 02:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:04][root][INFO] - Training Epoch: 1/2, step 2846/7134 completed (loss: 0.10168398171663284, acc: 0.9769357442855835)
[2025-02-13 02:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:04][root][INFO] - Training Epoch: 1/2, step 2847/7134 completed (loss: 0.05586938560009003, acc: 0.9838709831237793)
[2025-02-13 02:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05][root][INFO] - Training Epoch: 1/2, step 2848/7134 completed (loss: 0.0755452811717987, acc: 0.9813519716262817)
[2025-02-13 02:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05][root][INFO] - Training Epoch: 1/2, step 2849/7134 completed (loss: 0.06475630402565002, acc: 0.9875518679618835)
[2025-02-13 02:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:06][root][INFO] - Training Epoch: 1/2, step 2850/7134 completed (loss: 0.03137658163905144, acc: 0.9892473220825195)
[2025-02-13 02:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:06][root][INFO] - Training Epoch: 1/2, step 2851/7134 completed (loss: 0.04087405279278755, acc: 0.9852670431137085)
[2025-02-13 02:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:06][root][INFO] - Training Epoch: 1/2, step 2852/7134 completed (loss: 0.02298024296760559, acc: 0.9920477271080017)
[2025-02-13 02:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07][root][INFO] - Training Epoch: 1/2, step 2853/7134 completed (loss: 0.03824485465884209, acc: 0.9847715497016907)
[2025-02-13 02:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07][root][INFO] - Training Epoch: 1/2, step 2854/7134 completed (loss: 0.058191899210214615, acc: 0.97919762134552)
[2025-02-13 02:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:08][root][INFO] - Training Epoch: 1/2, step 2855/7134 completed (loss: 0.05152960121631622, acc: 0.9886914491653442)
[2025-02-13 02:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:08][root][INFO] - Training Epoch: 1/2, step 2856/7134 completed (loss: 0.021867943927645683, acc: 0.994358241558075)
[2025-02-13 02:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09][root][INFO] - Training Epoch: 1/2, step 2857/7134 completed (loss: 0.06684912741184235, acc: 0.9841040372848511)
[2025-02-13 02:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09][root][INFO] - Training Epoch: 1/2, step 2858/7134 completed (loss: 0.027617499232292175, acc: 0.9921135902404785)
[2025-02-13 02:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10][root][INFO] - Training Epoch: 1/2, step 2859/7134 completed (loss: 0.07792598754167557, acc: 0.9748954176902771)
[2025-02-13 02:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10][root][INFO] - Training Epoch: 1/2, step 2860/7134 completed (loss: 0.03762715682387352, acc: 0.9853211045265198)
[2025-02-13 02:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10][root][INFO] - Training Epoch: 1/2, step 2861/7134 completed (loss: 0.025997716933488846, acc: 0.9918864369392395)
[2025-02-13 02:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11][root][INFO] - Training Epoch: 1/2, step 2862/7134 completed (loss: 0.022478874772787094, acc: 0.989708423614502)
[2025-02-13 02:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11][root][INFO] - Training Epoch: 1/2, step 2863/7134 completed (loss: 0.01638117805123329, acc: 0.9961783289909363)
[2025-02-13 02:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:12][root][INFO] - Training Epoch: 1/2, step 2864/7134 completed (loss: 0.04490659013390541, acc: 0.9896103739738464)
[2025-02-13 02:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:12][root][INFO] - Training Epoch: 1/2, step 2865/7134 completed (loss: 0.03662770986557007, acc: 0.9869565367698669)
[2025-02-13 02:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:12][root][INFO] - Training Epoch: 1/2, step 2866/7134 completed (loss: 0.03886093571782112, acc: 0.9925261735916138)
[2025-02-13 02:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:13][root][INFO] - Training Epoch: 1/2, step 2867/7134 completed (loss: 0.033608365803956985, acc: 0.9938650131225586)
[2025-02-13 02:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:13][root][INFO] - Training Epoch: 1/2, step 2868/7134 completed (loss: 0.02452375739812851, acc: 0.9915730357170105)
[2025-02-13 02:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14][root][INFO] - Training Epoch: 1/2, step 2869/7134 completed (loss: 0.06068291887640953, acc: 0.9882869720458984)
[2025-02-13 02:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14][root][INFO] - Training Epoch: 1/2, step 2870/7134 completed (loss: 0.0683971643447876, acc: 0.9874213933944702)
[2025-02-13 02:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15][root][INFO] - Training Epoch: 1/2, step 2871/7134 completed (loss: 0.04931361973285675, acc: 0.9890795350074768)
[2025-02-13 02:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15][root][INFO] - Training Epoch: 1/2, step 2872/7134 completed (loss: 0.008749803528189659, acc: 0.998046875)
[2025-02-13 02:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15][root][INFO] - Training Epoch: 1/2, step 2873/7134 completed (loss: 0.030791446566581726, acc: 0.9908758997917175)
[2025-02-13 02:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:16][root][INFO] - Training Epoch: 1/2, step 2874/7134 completed (loss: 0.039000269025564194, acc: 0.9890795350074768)
[2025-02-13 02:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:16][root][INFO] - Training Epoch: 1/2, step 2875/7134 completed (loss: 0.03923171013593674, acc: 0.9886202216148376)
[2025-02-13 02:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17][root][INFO] - Training Epoch: 1/2, step 2876/7134 completed (loss: 0.10574263334274292, acc: 0.9746543765068054)
[2025-02-13 02:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17][root][INFO] - Training Epoch: 1/2, step 2877/7134 completed (loss: 0.01402920763939619, acc: 0.9934498071670532)
[2025-02-13 02:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:18][root][INFO] - Training Epoch: 1/2, step 2878/7134 completed (loss: 0.028779741376638412, acc: 0.9978813529014587)
[2025-02-13 02:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:18][root][INFO] - Training Epoch: 1/2, step 2879/7134 completed (loss: 0.01540638878941536, acc: 0.9968652129173279)
[2025-02-13 02:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:18][root][INFO] - Training Epoch: 1/2, step 2880/7134 completed (loss: 0.017116419970989227, acc: 0.9937499761581421)
[2025-02-13 02:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:19][root][INFO] - Training Epoch: 1/2, step 2881/7134 completed (loss: 0.04183749482035637, acc: 0.9874776601791382)
[2025-02-13 02:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:19][root][INFO] - Training Epoch: 1/2, step 2882/7134 completed (loss: 0.06786886602640152, acc: 0.9827373623847961)
[2025-02-13 02:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20][root][INFO] - Training Epoch: 1/2, step 2883/7134 completed (loss: 0.05120162293314934, acc: 0.988095223903656)
[2025-02-13 02:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20][root][INFO] - Training Epoch: 1/2, step 2884/7134 completed (loss: 0.11328373849391937, acc: 0.9728506803512573)
[2025-02-13 02:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20][root][INFO] - Training Epoch: 1/2, step 2885/7134 completed (loss: 0.04670654609799385, acc: 0.9893778562545776)
[2025-02-13 02:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:21][root][INFO] - Training Epoch: 1/2, step 2886/7134 completed (loss: 0.08489292860031128, acc: 0.9731343388557434)
[2025-02-13 02:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:21][root][INFO] - Training Epoch: 1/2, step 2887/7134 completed (loss: 0.05685393139719963, acc: 0.9846153855323792)
[2025-02-13 02:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22][root][INFO] - Training Epoch: 1/2, step 2888/7134 completed (loss: 0.07130168378353119, acc: 0.9778085947036743)
[2025-02-13 02:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22][root][INFO] - Training Epoch: 1/2, step 2889/7134 completed (loss: 0.029304301366209984, acc: 0.9916897416114807)
[2025-02-13 02:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:23][root][INFO] - Training Epoch: 1/2, step 2890/7134 completed (loss: 0.030907073989510536, acc: 0.9920106530189514)
[2025-02-13 02:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:23][root][INFO] - Training Epoch: 1/2, step 2891/7134 completed (loss: 0.05976573005318642, acc: 0.9861538410186768)
[2025-02-13 02:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24][root][INFO] - Training Epoch: 1/2, step 2892/7134 completed (loss: 0.07534529268741608, acc: 0.9785894155502319)
[2025-02-13 02:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24][root][INFO] - Training Epoch: 1/2, step 2893/7134 completed (loss: 0.050316259264945984, acc: 0.9848675727844238)
[2025-02-13 02:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24][root][INFO] - Training Epoch: 1/2, step 2894/7134 completed (loss: 0.04504123702645302, acc: 0.9831932783126831)
[2025-02-13 02:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:25][root][INFO] - Training Epoch: 1/2, step 2895/7134 completed (loss: 0.028410134837031364, acc: 0.9943289160728455)
[2025-02-13 02:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:25][root][INFO] - Training Epoch: 1/2, step 2896/7134 completed (loss: 0.026969101279973984, acc: 0.9961832165718079)
[2025-02-13 02:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26][root][INFO] - Training Epoch: 1/2, step 2897/7134 completed (loss: 0.056186750531196594, acc: 0.9822379946708679)
[2025-02-13 02:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26][root][INFO] - Training Epoch: 1/2, step 2898/7134 completed (loss: 0.03780288249254227, acc: 0.9921568632125854)
[2025-02-13 02:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26][root][INFO] - Training Epoch: 1/2, step 2899/7134 completed (loss: 0.025527050718665123, acc: 0.9912790656089783)
[2025-02-13 02:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:27][root][INFO] - Training Epoch: 1/2, step 2900/7134 completed (loss: 0.038892962038517, acc: 0.9962962865829468)
[2025-02-13 02:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:27][root][INFO] - Training Epoch: 1/2, step 2901/7134 completed (loss: 0.029619688168168068, acc: 0.9888535141944885)
[2025-02-13 02:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:28][root][INFO] - Training Epoch: 1/2, step 2902/7134 completed (loss: 0.04233346879482269, acc: 0.9868420958518982)
[2025-02-13 02:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:28][root][INFO] - Training Epoch: 1/2, step 2903/7134 completed (loss: 0.05435953289270401, acc: 0.9851552248001099)
[2025-02-13 02:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29][root][INFO] - Training Epoch: 1/2, step 2904/7134 completed (loss: 0.029865596443414688, acc: 0.9925093650817871)
[2025-02-13 02:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29][root][INFO] - Training Epoch: 1/2, step 2905/7134 completed (loss: 0.051276884973049164, acc: 0.9824780821800232)
[2025-02-13 02:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:30][root][INFO] - Training Epoch: 1/2, step 2906/7134 completed (loss: 0.016230324283242226, acc: 0.9940119981765747)
[2025-02-13 02:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:30][root][INFO] - Training Epoch: 1/2, step 2907/7134 completed (loss: 0.04826826974749565, acc: 0.9851951599121094)
[2025-02-13 02:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:30][root][INFO] - Training Epoch: 1/2, step 2908/7134 completed (loss: 0.024807294830679893, acc: 0.9935317039489746)
[2025-02-13 02:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31][root][INFO] - Training Epoch: 1/2, step 2909/7134 completed (loss: 0.051890868693590164, acc: 0.9880095720291138)
[2025-02-13 02:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31][root][INFO] - Training Epoch: 1/2, step 2910/7134 completed (loss: 0.06307708472013474, acc: 0.9828693866729736)
[2025-02-13 02:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:32][root][INFO] - Training Epoch: 1/2, step 2911/7134 completed (loss: 0.026184847578406334, acc: 0.990604043006897)
[2025-02-13 02:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:32][root][INFO] - Training Epoch: 1/2, step 2912/7134 completed (loss: 0.031316064298152924, acc: 0.9875862002372742)
[2025-02-13 02:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33][root][INFO] - Training Epoch: 1/2, step 2913/7134 completed (loss: 0.03892693668603897, acc: 0.9928774833679199)
[2025-02-13 02:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33][root][INFO] - Training Epoch: 1/2, step 2914/7134 completed (loss: 0.030735507607460022, acc: 0.9963811635971069)
[2025-02-13 02:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:34][root][INFO] - Training Epoch: 1/2, step 2915/7134 completed (loss: 0.08251536637544632, acc: 0.9783861637115479)
[2025-02-13 02:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:34][root][INFO] - Training Epoch: 1/2, step 2916/7134 completed (loss: 0.04431663081049919, acc: 0.9896907210350037)
[2025-02-13 02:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35][root][INFO] - Training Epoch: 1/2, step 2917/7134 completed (loss: 0.017289049923419952, acc: 0.9949748516082764)
[2025-02-13 02:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35][root][INFO] - Training Epoch: 1/2, step 2918/7134 completed (loss: 0.028354361653327942, acc: 0.9895104765892029)
[2025-02-13 02:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35][root][INFO] - Training Epoch: 1/2, step 2919/7134 completed (loss: 0.06084940582513809, acc: 0.9823736548423767)
[2025-02-13 02:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:36][root][INFO] - Training Epoch: 1/2, step 2920/7134 completed (loss: 0.03966712951660156, acc: 0.9880794882774353)
[2025-02-13 02:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:36][root][INFO] - Training Epoch: 1/2, step 2921/7134 completed (loss: 0.019115310162305832, acc: 0.9956896305084229)
[2025-02-13 02:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37][root][INFO] - Training Epoch: 1/2, step 2922/7134 completed (loss: 0.06444284319877625, acc: 0.9849246144294739)
[2025-02-13 02:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37][root][INFO] - Training Epoch: 1/2, step 2923/7134 completed (loss: 0.15798579156398773, acc: 0.9692780375480652)
[2025-02-13 02:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:38][root][INFO] - Training Epoch: 1/2, step 2924/7134 completed (loss: 0.029026489704847336, acc: 0.9929245114326477)
[2025-02-13 02:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:38][root][INFO] - Training Epoch: 1/2, step 2925/7134 completed (loss: 0.048430413007736206, acc: 0.9889435172080994)
[2025-02-13 02:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:39][root][INFO] - Training Epoch: 1/2, step 2926/7134 completed (loss: 0.1407538801431656, acc: 0.9759615659713745)
[2025-02-13 02:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:39][root][INFO] - Training Epoch: 1/2, step 2927/7134 completed (loss: 0.02488216757774353, acc: 0.9927448630332947)
[2025-02-13 02:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40][root][INFO] - Training Epoch: 1/2, step 2928/7134 completed (loss: 0.03753913938999176, acc: 0.9908536672592163)
[2025-02-13 02:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40][root][INFO] - Training Epoch: 1/2, step 2929/7134 completed (loss: 0.03936314582824707, acc: 0.9869451522827148)
[2025-02-13 02:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40][root][INFO] - Training Epoch: 1/2, step 2930/7134 completed (loss: 0.043392524123191833, acc: 0.98531574010849)
[2025-02-13 02:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:41][root][INFO] - Training Epoch: 1/2, step 2931/7134 completed (loss: 0.07544035464525223, acc: 0.974588930606842)
[2025-02-13 02:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:41][root][INFO] - Training Epoch: 1/2, step 2932/7134 completed (loss: 0.028357794508337975, acc: 0.9895712733268738)
[2025-02-13 02:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42][root][INFO] - Training Epoch: 1/2, step 2933/7134 completed (loss: 0.040461212396621704, acc: 0.9870129823684692)
[2025-02-13 02:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42][root][INFO] - Training Epoch: 1/2, step 2934/7134 completed (loss: 0.022777901962399483, acc: 0.994358241558075)
[2025-02-13 02:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:43][root][INFO] - Training Epoch: 1/2, step 2935/7134 completed (loss: 0.03260786458849907, acc: 0.9889867901802063)
[2025-02-13 02:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:43][root][INFO] - Training Epoch: 1/2, step 2936/7134 completed (loss: 0.04959440603852272, acc: 0.9877924919128418)
[2025-02-13 02:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44][root][INFO] - Training Epoch: 1/2, step 2937/7134 completed (loss: 0.057413749396800995, acc: 0.9842180609703064)
[2025-02-13 02:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44][root][INFO] - Training Epoch: 1/2, step 2938/7134 completed (loss: 0.02585073560476303, acc: 0.9921787977218628)
[2025-02-13 02:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44][root][INFO] - Training Epoch: 1/2, step 2939/7134 completed (loss: 0.021564938127994537, acc: 0.9935897588729858)
[2025-02-13 02:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:45][root][INFO] - Training Epoch: 1/2, step 2940/7134 completed (loss: 0.06730272620916367, acc: 0.9863201379776001)
[2025-02-13 02:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:45][root][INFO] - Training Epoch: 1/2, step 2941/7134 completed (loss: 0.03630587086081505, acc: 0.9898403286933899)
[2025-02-13 02:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46][root][INFO] - Training Epoch: 1/2, step 2942/7134 completed (loss: 0.05787864327430725, acc: 0.98959881067276)
[2025-02-13 02:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46][root][INFO] - Training Epoch: 1/2, step 2943/7134 completed (loss: 0.02971259318292141, acc: 0.9915682673454285)
[2025-02-13 02:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46][root][INFO] - Training Epoch: 1/2, step 2944/7134 completed (loss: 0.03702225908637047, acc: 0.988252580165863)
[2025-02-13 02:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:47][root][INFO] - Training Epoch: 1/2, step 2945/7134 completed (loss: 0.027317022904753685, acc: 0.9906687140464783)
[2025-02-13 02:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:47][root][INFO] - Training Epoch: 1/2, step 2946/7134 completed (loss: 0.04946735128760338, acc: 0.9898256063461304)
[2025-02-13 02:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48][root][INFO] - Training Epoch: 1/2, step 2947/7134 completed (loss: 0.04916568472981453, acc: 0.9827044010162354)
[2025-02-13 02:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48][root][INFO] - Training Epoch: 1/2, step 2948/7134 completed (loss: 0.07040849328041077, acc: 0.9785832166671753)
[2025-02-13 02:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48][root][INFO] - Training Epoch: 1/2, step 2949/7134 completed (loss: 0.007527756970375776, acc: 0.9984709620475769)
[2025-02-13 02:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49][root][INFO] - Training Epoch: 1/2, step 2950/7134 completed (loss: 0.028274569660425186, acc: 0.9944827556610107)
[2025-02-13 02:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49][root][INFO] - Training Epoch: 1/2, step 2951/7134 completed (loss: 0.015361516736447811, acc: 0.9942029118537903)
[2025-02-13 02:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:50][root][INFO] - Training Epoch: 1/2, step 2952/7134 completed (loss: 0.025550462305545807, acc: 0.9940652847290039)
[2025-02-13 02:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:50][root][INFO] - Training Epoch: 1/2, step 2953/7134 completed (loss: 0.03093918040394783, acc: 0.9898648858070374)
[2025-02-13 02:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51][root][INFO] - Training Epoch: 1/2, step 2954/7134 completed (loss: 0.03969939053058624, acc: 0.9897040128707886)
[2025-02-13 02:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51][root][INFO] - Training Epoch: 1/2, step 2955/7134 completed (loss: 0.028482288122177124, acc: 0.9886363744735718)
[2025-02-13 02:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51][root][INFO] - Training Epoch: 1/2, step 2956/7134 completed (loss: 0.024488478899002075, acc: 0.9926793575286865)
[2025-02-13 02:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:52][root][INFO] - Training Epoch: 1/2, step 2957/7134 completed (loss: 0.02070755325257778, acc: 0.9946523904800415)
[2025-02-13 02:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:52][root][INFO] - Training Epoch: 1/2, step 2958/7134 completed (loss: 0.024564679712057114, acc: 0.9931623935699463)
[2025-02-13 02:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53][root][INFO] - Training Epoch: 1/2, step 2959/7134 completed (loss: 0.018407607451081276, acc: 0.9912280440330505)
[2025-02-13 02:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53][root][INFO] - Training Epoch: 1/2, step 2960/7134 completed (loss: 0.023137904703617096, acc: 0.9917355179786682)
[2025-02-13 02:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53][root][INFO] - Training Epoch: 1/2, step 2961/7134 completed (loss: 0.02045500837266445, acc: 0.9931972622871399)
[2025-02-13 02:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:54][root][INFO] - Training Epoch: 1/2, step 2962/7134 completed (loss: 0.017910879105329514, acc: 0.9909443855285645)
[2025-02-13 02:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:54][root][INFO] - Training Epoch: 1/2, step 2963/7134 completed (loss: 0.020939011126756668, acc: 0.9934554696083069)
[2025-02-13 02:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55][root][INFO] - Training Epoch: 1/2, step 2964/7134 completed (loss: 0.023320568725466728, acc: 0.9926793575286865)
[2025-02-13 02:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55][root][INFO] - Training Epoch: 1/2, step 2965/7134 completed (loss: 0.053531743586063385, acc: 0.983582079410553)
[2025-02-13 02:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56][root][INFO] - Training Epoch: 1/2, step 2966/7134 completed (loss: 0.014410728588700294, acc: 0.9936034083366394)
[2025-02-13 02:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56][root][INFO] - Training Epoch: 1/2, step 2967/7134 completed (loss: 0.037886641919612885, acc: 0.9888268113136292)
[2025-02-13 02:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56][root][INFO] - Training Epoch: 1/2, step 2968/7134 completed (loss: 0.16390591859817505, acc: 0.9678456783294678)
[2025-02-13 02:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:57][root][INFO] - Training Epoch: 1/2, step 2969/7134 completed (loss: 0.28738853335380554, acc: 0.9482248425483704)
[2025-02-13 02:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:57][root][INFO] - Training Epoch: 1/2, step 2970/7134 completed (loss: 0.31946954131126404, acc: 0.9368191957473755)
[2025-02-13 02:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58][root][INFO] - Training Epoch: 1/2, step 2971/7134 completed (loss: 0.057384852319955826, acc: 0.9842312932014465)
[2025-02-13 02:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58][root][INFO] - Training Epoch: 1/2, step 2972/7134 completed (loss: 0.042562101036310196, acc: 0.9881305694580078)
[2025-02-13 02:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:59][root][INFO] - Training Epoch: 1/2, step 2973/7134 completed (loss: 0.06789941340684891, acc: 0.9721518754959106)
[2025-02-13 02:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:59][root][INFO] - Training Epoch: 1/2, step 2974/7134 completed (loss: 0.054817717522382736, acc: 0.9831387996673584)
[2025-02-13 02:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:00][root][INFO] - Training Epoch: 1/2, step 2975/7134 completed (loss: 0.04602103307843208, acc: 0.9861878156661987)
[2025-02-13 02:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:00][root][INFO] - Training Epoch: 1/2, step 2976/7134 completed (loss: 0.05894632264971733, acc: 0.98617023229599)
[2025-02-13 02:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:00][root][INFO] - Training Epoch: 1/2, step 2977/7134 completed (loss: 0.041759248822927475, acc: 0.9913294911384583)
[2025-02-13 02:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01][root][INFO] - Training Epoch: 1/2, step 2978/7134 completed (loss: 0.04279365763068199, acc: 0.9886524677276611)
[2025-02-13 02:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01][root][INFO] - Training Epoch: 1/2, step 2979/7134 completed (loss: 0.022693593055009842, acc: 0.9922118186950684)
[2025-02-13 02:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:02][root][INFO] - Training Epoch: 1/2, step 2980/7134 completed (loss: 0.04573625326156616, acc: 0.9885350465774536)
[2025-02-13 02:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:02][root][INFO] - Training Epoch: 1/2, step 2981/7134 completed (loss: 0.08501967787742615, acc: 0.9796472191810608)
[2025-02-13 02:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03][root][INFO] - Training Epoch: 1/2, step 2982/7134 completed (loss: 0.0680091455578804, acc: 0.9829843044281006)
[2025-02-13 02:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03][root][INFO] - Training Epoch: 1/2, step 2983/7134 completed (loss: 0.09393494576215744, acc: 0.976190447807312)
[2025-02-13 02:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:04][root][INFO] - Training Epoch: 1/2, step 2984/7134 completed (loss: 0.03614618256688118, acc: 0.99245285987854)
[2025-02-13 02:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:04][root][INFO] - Training Epoch: 1/2, step 2985/7134 completed (loss: 0.05221031233668327, acc: 0.9880810379981995)
[2025-02-13 02:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:05][root][INFO] - Training Epoch: 1/2, step 2986/7134 completed (loss: 0.030107388272881508, acc: 0.9905437231063843)
[2025-02-13 02:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:05][root][INFO] - Training Epoch: 1/2, step 2987/7134 completed (loss: 0.03679270297288895, acc: 0.9917550086975098)
[2025-02-13 02:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:05][root][INFO] - Training Epoch: 1/2, step 2988/7134 completed (loss: 0.04177539795637131, acc: 0.989051103591919)
[2025-02-13 02:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06][root][INFO] - Training Epoch: 1/2, step 2989/7134 completed (loss: 0.03969288617372513, acc: 0.9921568632125854)
[2025-02-13 02:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06][root][INFO] - Training Epoch: 1/2, step 2990/7134 completed (loss: 0.16233578324317932, acc: 0.9685314893722534)
[2025-02-13 02:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:07][root][INFO] - Training Epoch: 1/2, step 2991/7134 completed (loss: 0.1411944180727005, acc: 0.9597780704498291)
[2025-02-13 02:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:07][root][INFO] - Training Epoch: 1/2, step 2992/7134 completed (loss: 0.08007249236106873, acc: 0.9788838624954224)
[2025-02-13 02:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08][root][INFO] - Training Epoch: 1/2, step 2993/7134 completed (loss: 0.05116851627826691, acc: 0.9877750873565674)
[2025-02-13 02:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08][root][INFO] - Training Epoch: 1/2, step 2994/7134 completed (loss: 0.027230434119701385, acc: 0.9892086386680603)
[2025-02-13 02:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:09][root][INFO] - Training Epoch: 1/2, step 2995/7134 completed (loss: 0.03661219775676727, acc: 0.9920904040336609)
[2025-02-13 02:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:09][root][INFO] - Training Epoch: 1/2, step 2996/7134 completed (loss: 0.04682260379195213, acc: 0.984795331954956)
[2025-02-13 02:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:09][root][INFO] - Training Epoch: 1/2, step 2997/7134 completed (loss: 0.02660905383527279, acc: 0.9927234649658203)
[2025-02-13 02:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10][root][INFO] - Training Epoch: 1/2, step 2998/7134 completed (loss: 0.03915824368596077, acc: 0.9951515197753906)
[2025-02-13 02:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10][root][INFO] - Training Epoch: 1/2, step 2999/7134 completed (loss: 0.03391614183783531, acc: 0.9913169145584106)
[2025-02-13 02:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:11][root][INFO] - Training Epoch: 1/2, step 3000/7134 completed (loss: 0.034537214785814285, acc: 0.9914346933364868)
[2025-02-13 02:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:11][root][INFO] - Training Epoch: 1/2, step 3001/7134 completed (loss: 0.03329414129257202, acc: 0.9893364906311035)
[2025-02-13 02:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12][root][INFO] - Training Epoch: 1/2, step 3002/7134 completed (loss: 0.04105498641729355, acc: 0.9875283241271973)
[2025-02-13 02:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12][root][INFO] - Training Epoch: 1/2, step 3003/7134 completed (loss: 0.017029739916324615, acc: 0.9953595995903015)
[2025-02-13 02:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:13][root][INFO] - Training Epoch: 1/2, step 3004/7134 completed (loss: 0.014408228918910027, acc: 0.994413435459137)
[2025-02-13 02:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:13][root][INFO] - Training Epoch: 1/2, step 3005/7134 completed (loss: 0.01883576437830925, acc: 0.9931585192680359)
[2025-02-13 02:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14][root][INFO] - Training Epoch: 1/2, step 3006/7134 completed (loss: 0.019377509132027626, acc: 0.9936143159866333)
[2025-02-13 02:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14][root][INFO] - Training Epoch: 1/2, step 3007/7134 completed (loss: 0.04218434914946556, acc: 0.989461362361908)
[2025-02-13 02:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14][root][INFO] - Training Epoch: 1/2, step 3008/7134 completed (loss: 0.06585296243429184, acc: 0.9818181991577148)
[2025-02-13 02:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:15][root][INFO] - Training Epoch: 1/2, step 3009/7134 completed (loss: 0.09588050097227097, acc: 0.9708860516548157)
[2025-02-13 02:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:15][root][INFO] - Training Epoch: 1/2, step 3010/7134 completed (loss: 0.0385708212852478, acc: 0.988950252532959)
[2025-02-13 02:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16][root][INFO] - Training Epoch: 1/2, step 3011/7134 completed (loss: 0.09408154338598251, acc: 0.9771754741668701)
[2025-02-13 02:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16][root][INFO] - Training Epoch: 1/2, step 3012/7134 completed (loss: 0.06940429657697678, acc: 0.9762282371520996)
[2025-02-13 02:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:17][root][INFO] - Training Epoch: 1/2, step 3013/7134 completed (loss: 0.01923532783985138, acc: 0.9896313548088074)
[2025-02-13 02:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:17][root][INFO] - Training Epoch: 1/2, step 3014/7134 completed (loss: 0.03811616078019142, acc: 0.9871043562889099)
[2025-02-13 02:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18][root][INFO] - Training Epoch: 1/2, step 3015/7134 completed (loss: 0.0348731093108654, acc: 0.9931129217147827)
[2025-02-13 02:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18][root][INFO] - Training Epoch: 1/2, step 3016/7134 completed (loss: 0.027013711631298065, acc: 0.9929676651954651)
[2025-02-13 02:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:19][root][INFO] - Training Epoch: 1/2, step 3017/7134 completed (loss: 0.05457969382405281, acc: 0.9796609878540039)
[2025-02-13 02:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:19][root][INFO] - Training Epoch: 1/2, step 3018/7134 completed (loss: 0.042416758835315704, acc: 0.9917355179786682)
[2025-02-13 02:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:19][root][INFO] - Training Epoch: 1/2, step 3019/7134 completed (loss: 0.041074447333812714, acc: 0.9869281053543091)
[2025-02-13 02:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:20][root][INFO] - Training Epoch: 1/2, step 3020/7134 completed (loss: 0.05912517011165619, acc: 0.9857397675514221)
[2025-02-13 02:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:20][root][INFO] - Training Epoch: 1/2, step 3021/7134 completed (loss: 0.02189617231488228, acc: 0.9945454597473145)
[2025-02-13 02:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21][root][INFO] - Training Epoch: 1/2, step 3022/7134 completed (loss: 0.054141655564308167, acc: 0.9903537034988403)
[2025-02-13 02:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21][root][INFO] - Training Epoch: 1/2, step 3023/7134 completed (loss: 0.041184160858392715, acc: 0.9860835075378418)
[2025-02-13 02:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21][root][INFO] - Training Epoch: 1/2, step 3024/7134 completed (loss: 0.0250848438590765, acc: 0.992337167263031)
[2025-02-13 02:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:22][root][INFO] - Training Epoch: 1/2, step 3025/7134 completed (loss: 0.07484427839517593, acc: 0.982758641242981)
[2025-02-13 02:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:22][root][INFO] - Training Epoch: 1/2, step 3026/7134 completed (loss: 0.033094439655542374, acc: 0.9874326586723328)
[2025-02-13 02:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:23][root][INFO] - Training Epoch: 1/2, step 3027/7134 completed (loss: 0.06974227726459503, acc: 0.991919219493866)
[2025-02-13 02:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:23][root][INFO] - Training Epoch: 1/2, step 3028/7134 completed (loss: 0.03361428529024124, acc: 0.9909297227859497)
[2025-02-13 02:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24][root][INFO] - Training Epoch: 1/2, step 3029/7134 completed (loss: 0.04487420246005058, acc: 0.9938176274299622)
[2025-02-13 02:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24][root][INFO] - Training Epoch: 1/2, step 3030/7134 completed (loss: 0.03487429395318031, acc: 0.989313006401062)
[2025-02-13 02:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24][root][INFO] - Training Epoch: 1/2, step 3031/7134 completed (loss: 0.014539564028382301, acc: 0.9936440587043762)
[2025-02-13 02:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:25][root][INFO] - Training Epoch: 1/2, step 3032/7134 completed (loss: 0.058828406035900116, acc: 0.9854809641838074)
[2025-02-13 02:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:25][root][INFO] - Training Epoch: 1/2, step 3033/7134 completed (loss: 0.0329493023455143, acc: 0.9858906269073486)
[2025-02-13 02:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26][root][INFO] - Training Epoch: 1/2, step 3034/7134 completed (loss: 0.045707494020462036, acc: 0.9851064085960388)
[2025-02-13 02:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26][root][INFO] - Training Epoch: 1/2, step 3035/7134 completed (loss: 0.02286820113658905, acc: 0.9889763593673706)
[2025-02-13 02:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:27][root][INFO] - Training Epoch: 1/2, step 3036/7134 completed (loss: 0.06423425674438477, acc: 0.9882943034172058)
[2025-02-13 02:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:27][root][INFO] - Training Epoch: 1/2, step 3037/7134 completed (loss: 0.027940530329942703, acc: 0.990755021572113)
[2025-02-13 02:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:27][root][INFO] - Training Epoch: 1/2, step 3038/7134 completed (loss: 0.011485117487609386, acc: 0.9982608556747437)
[2025-02-13 02:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28][root][INFO] - Training Epoch: 1/2, step 3039/7134 completed (loss: 0.04283198341727257, acc: 0.9908116459846497)
[2025-02-13 02:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28][root][INFO] - Training Epoch: 1/2, step 3040/7134 completed (loss: 0.05023245885968208, acc: 0.9851577281951904)
[2025-02-13 02:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:29][root][INFO] - Training Epoch: 1/2, step 3041/7134 completed (loss: 0.07280799001455307, acc: 0.9856459498405457)
[2025-02-13 02:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:29][root][INFO] - Training Epoch: 1/2, step 3042/7134 completed (loss: 0.028586851432919502, acc: 0.9940915703773499)
[2025-02-13 02:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:30][root][INFO] - Training Epoch: 1/2, step 3043/7134 completed (loss: 0.029089516028761864, acc: 0.9885931611061096)
[2025-02-13 02:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:30][root][INFO] - Training Epoch: 1/2, step 3044/7134 completed (loss: 0.028578082099556923, acc: 0.9930915236473083)
[2025-02-13 02:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:30][root][INFO] - Training Epoch: 1/2, step 3045/7134 completed (loss: 0.017395392060279846, acc: 0.993220329284668)
[2025-02-13 02:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31][root][INFO] - Training Epoch: 1/2, step 3046/7134 completed (loss: 0.03254852443933487, acc: 0.9890710115432739)
[2025-02-13 02:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31][root][INFO] - Training Epoch: 1/2, step 3047/7134 completed (loss: 0.009068450890481472, acc: 0.99622642993927)
[2025-02-13 02:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:32][root][INFO] - Training Epoch: 1/2, step 3048/7134 completed (loss: 0.01168395671993494, acc: 0.996221661567688)
[2025-02-13 02:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:32][root][INFO] - Training Epoch: 1/2, step 3049/7134 completed (loss: 0.014103381894528866, acc: 0.992732584476471)
[2025-02-13 02:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33][root][INFO] - Training Epoch: 1/2, step 3050/7134 completed (loss: 0.029919728636741638, acc: 0.989311158657074)
[2025-02-13 02:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33][root][INFO] - Training Epoch: 1/2, step 3051/7134 completed (loss: 0.040114838629961014, acc: 0.9894737005233765)
[2025-02-13 02:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33][root][INFO] - Training Epoch: 1/2, step 3052/7134 completed (loss: 0.030399532988667488, acc: 0.9922680258750916)
[2025-02-13 02:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:34][root][INFO] - Training Epoch: 1/2, step 3053/7134 completed (loss: 0.023890376091003418, acc: 0.9915459156036377)
[2025-02-13 02:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:34][root][INFO] - Training Epoch: 1/2, step 3054/7134 completed (loss: 0.043008193373680115, acc: 0.9883117079734802)
[2025-02-13 02:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:35][root][INFO] - Training Epoch: 1/2, step 3055/7134 completed (loss: 0.028528647497296333, acc: 0.9914529919624329)
[2025-02-13 02:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:35][root][INFO] - Training Epoch: 1/2, step 3056/7134 completed (loss: 0.02064524218440056, acc: 0.9922879338264465)
[2025-02-13 02:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36][root][INFO] - Training Epoch: 1/2, step 3057/7134 completed (loss: 0.01707393303513527, acc: 0.9930651783943176)
[2025-02-13 02:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36][root][INFO] - Training Epoch: 1/2, step 3058/7134 completed (loss: 0.07496214658021927, acc: 0.9818652868270874)
[2025-02-13 02:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:37][root][INFO] - Training Epoch: 1/2, step 3059/7134 completed (loss: 0.022388964891433716, acc: 0.9917808175086975)
[2025-02-13 02:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:37][root][INFO] - Training Epoch: 1/2, step 3060/7134 completed (loss: 0.0294342078268528, acc: 0.9916467666625977)
[2025-02-13 02:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:37][root][INFO] - Training Epoch: 1/2, step 3061/7134 completed (loss: 0.011085154488682747, acc: 0.9984848499298096)
[2025-02-13 02:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38][root][INFO] - Training Epoch: 1/2, step 3062/7134 completed (loss: 0.014753296971321106, acc: 0.9946091771125793)
[2025-02-13 02:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38][root][INFO] - Training Epoch: 1/2, step 3063/7134 completed (loss: 0.029981231316924095, acc: 0.9897304177284241)
[2025-02-13 02:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:39][root][INFO] - Training Epoch: 1/2, step 3064/7134 completed (loss: 0.01661076955497265, acc: 0.9957746267318726)
[2025-02-13 02:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:39][root][INFO] - Training Epoch: 1/2, step 3065/7134 completed (loss: 0.025265129283070564, acc: 0.9932795763015747)
[2025-02-13 02:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:40][root][INFO] - Training Epoch: 1/2, step 3066/7134 completed (loss: 0.020402075722813606, acc: 0.9951612949371338)
[2025-02-13 02:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:40][root][INFO] - Training Epoch: 1/2, step 3067/7134 completed (loss: 0.03262357413768768, acc: 0.9945454597473145)
[2025-02-13 02:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41][root][INFO] - Training Epoch: 1/2, step 3068/7134 completed (loss: 0.032389093190431595, acc: 0.9905277490615845)
[2025-02-13 02:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41][root][INFO] - Training Epoch: 1/2, step 3069/7134 completed (loss: 0.017588436603546143, acc: 0.9930875301361084)
[2025-02-13 02:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42][root][INFO] - Training Epoch: 1/2, step 3070/7134 completed (loss: 0.02271716482937336, acc: 0.9915164113044739)
[2025-02-13 02:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42][root][INFO] - Training Epoch: 1/2, step 3071/7134 completed (loss: 0.034011997282505035, acc: 0.9909909963607788)
[2025-02-13 02:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42][root][INFO] - Training Epoch: 1/2, step 3072/7134 completed (loss: 0.023416023701429367, acc: 0.9929988384246826)
[2025-02-13 02:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:43][root][INFO] - Training Epoch: 1/2, step 3073/7134 completed (loss: 0.012940438464283943, acc: 0.9961832165718079)
[2025-02-13 02:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:43][root][INFO] - Training Epoch: 1/2, step 3074/7134 completed (loss: 0.02257903479039669, acc: 0.9950082898139954)
[2025-02-13 02:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44][root][INFO] - Training Epoch: 1/2, step 3075/7134 completed (loss: 0.010356754995882511, acc: 0.9972222447395325)
[2025-02-13 02:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44][root][INFO] - Training Epoch: 1/2, step 3076/7134 completed (loss: 0.03338886424899101, acc: 0.9911209940910339)
[2025-02-13 02:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45][root][INFO] - Training Epoch: 1/2, step 3077/7134 completed (loss: 0.08335236459970474, acc: 0.9814207553863525)
[2025-02-13 02:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45][root][INFO] - Training Epoch: 1/2, step 3078/7134 completed (loss: 0.06410687416791916, acc: 0.9792349934577942)
[2025-02-13 02:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:46][root][INFO] - Training Epoch: 1/2, step 3079/7134 completed (loss: 0.029950760304927826, acc: 0.9913580417633057)
[2025-02-13 02:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:46][root][INFO] - Training Epoch: 1/2, step 3080/7134 completed (loss: 0.05619648098945618, acc: 0.9825518131256104)
[2025-02-13 02:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:46][root][INFO] - Training Epoch: 1/2, step 3081/7134 completed (loss: 0.031619053333997726, acc: 0.9870689511299133)
[2025-02-13 02:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47][root][INFO] - Training Epoch: 1/2, step 3082/7134 completed (loss: 0.09487047791481018, acc: 0.9759325981140137)
[2025-02-13 02:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47][root][INFO] - Training Epoch: 1/2, step 3083/7134 completed (loss: 0.05739697441458702, acc: 0.9829843044281006)
[2025-02-13 02:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:48][root][INFO] - Training Epoch: 1/2, step 3084/7134 completed (loss: 0.06243225187063217, acc: 0.9833333492279053)
[2025-02-13 02:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:48][root][INFO] - Training Epoch: 1/2, step 3085/7134 completed (loss: 0.07018604129552841, acc: 0.9839416146278381)
[2025-02-13 02:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49][root][INFO] - Training Epoch: 1/2, step 3086/7134 completed (loss: 0.058679621666669846, acc: 0.9797979593276978)
[2025-02-13 02:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49][root][INFO] - Training Epoch: 1/2, step 3087/7134 completed (loss: 0.046748481690883636, acc: 0.9870874881744385)
[2025-02-13 02:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:50][root][INFO] - Training Epoch: 1/2, step 3088/7134 completed (loss: 0.03599294647574425, acc: 0.9875986576080322)
[2025-02-13 02:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:50][root][INFO] - Training Epoch: 1/2, step 3089/7134 completed (loss: 0.03472398594021797, acc: 0.9870550036430359)
[2025-02-13 02:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51][root][INFO] - Training Epoch: 1/2, step 3090/7134 completed (loss: 0.07093833386898041, acc: 0.9755434989929199)
[2025-02-13 02:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51][root][INFO] - Training Epoch: 1/2, step 3091/7134 completed (loss: 0.056388821452856064, acc: 0.981566846370697)
[2025-02-13 02:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:52][root][INFO] - Training Epoch: 1/2, step 3092/7134 completed (loss: 0.07345686852931976, acc: 0.9815157055854797)
[2025-02-13 02:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:52][root][INFO] - Training Epoch: 1/2, step 3093/7134 completed (loss: 0.05545387417078018, acc: 0.9856511950492859)
[2025-02-13 02:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:52][root][INFO] - Training Epoch: 1/2, step 3094/7134 completed (loss: 0.018463412299752235, acc: 0.9959127902984619)
[2025-02-13 02:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:53][root][INFO] - Training Epoch: 1/2, step 3095/7134 completed (loss: 0.060272831469774246, acc: 0.980555534362793)
[2025-02-13 02:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:53][root][INFO] - Training Epoch: 1/2, step 3096/7134 completed (loss: 0.07048013806343079, acc: 0.9781771302223206)
[2025-02-13 02:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54][root][INFO] - Training Epoch: 1/2, step 3097/7134 completed (loss: 0.06521274894475937, acc: 0.9857549667358398)
[2025-02-13 02:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54][root][INFO] - Training Epoch: 1/2, step 3098/7134 completed (loss: 0.026250695809721947, acc: 0.9905660152435303)
[2025-02-13 02:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:55][root][INFO] - Training Epoch: 1/2, step 3099/7134 completed (loss: 0.04023568704724312, acc: 0.987034022808075)
[2025-02-13 02:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:55][root][INFO] - Training Epoch: 1/2, step 3100/7134 completed (loss: 0.024471208453178406, acc: 0.9904255270957947)
[2025-02-13 02:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56][root][INFO] - Training Epoch: 1/2, step 3101/7134 completed (loss: 0.029984453693032265, acc: 0.9866179823875427)
[2025-02-13 02:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56][root][INFO] - Training Epoch: 1/2, step 3102/7134 completed (loss: 0.019640972837805748, acc: 0.9925925731658936)
[2025-02-13 02:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:57][root][INFO] - Training Epoch: 1/2, step 3103/7134 completed (loss: 0.034839238971471786, acc: 0.9904153347015381)
[2025-02-13 02:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:57][root][INFO] - Training Epoch: 1/2, step 3104/7134 completed (loss: 0.03307649493217468, acc: 0.9866412281990051)
[2025-02-13 02:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:57][root][INFO] - Training Epoch: 1/2, step 3105/7134 completed (loss: 0.055638931691646576, acc: 0.9835164546966553)
[2025-02-13 02:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58][root][INFO] - Training Epoch: 1/2, step 3106/7134 completed (loss: 0.029111964628100395, acc: 0.9882943034172058)
[2025-02-13 02:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58][root][INFO] - Training Epoch: 1/2, step 3107/7134 completed (loss: 0.02115531452000141, acc: 0.9947712421417236)
[2025-02-13 02:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:59][root][INFO] - Training Epoch: 1/2, step 3108/7134 completed (loss: 0.05303298309445381, acc: 0.9907692074775696)
[2025-02-13 02:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:59][root][INFO] - Training Epoch: 1/2, step 3109/7134 completed (loss: 0.053105343133211136, acc: 0.9852398633956909)
[2025-02-13 02:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00][root][INFO] - Training Epoch: 1/2, step 3110/7134 completed (loss: 0.022973671555519104, acc: 0.9878048896789551)
[2025-02-13 02:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00][root][INFO] - Training Epoch: 1/2, step 3111/7134 completed (loss: 0.1085110455751419, acc: 0.9725086092948914)
[2025-02-13 02:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00][root][INFO] - Training Epoch: 1/2, step 3112/7134 completed (loss: 0.05433247238397598, acc: 0.9894737005233765)
[2025-02-13 02:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:01][root][INFO] - Training Epoch: 1/2, step 3113/7134 completed (loss: 0.03280451148748398, acc: 0.9915730357170105)
[2025-02-13 02:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:01][root][INFO] - Training Epoch: 1/2, step 3114/7134 completed (loss: 0.03744979202747345, acc: 0.9913793206214905)
[2025-02-13 02:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02][root][INFO] - Training Epoch: 1/2, step 3115/7134 completed (loss: 0.0729152262210846, acc: 0.9876543283462524)
[2025-02-13 02:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02][root][INFO] - Training Epoch: 1/2, step 3116/7134 completed (loss: 0.029445413500070572, acc: 0.9884615540504456)
[2025-02-13 02:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:03][root][INFO] - Training Epoch: 1/2, step 3117/7134 completed (loss: 0.1036166176199913, acc: 0.97826087474823)
[2025-02-13 02:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:03][root][INFO] - Training Epoch: 1/2, step 3118/7134 completed (loss: 0.04781047999858856, acc: 0.9868247509002686)
[2025-02-13 02:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04][root][INFO] - Training Epoch: 1/2, step 3119/7134 completed (loss: 0.06243295967578888, acc: 0.9779506921768188)
[2025-02-13 02:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04][root][INFO] - Training Epoch: 1/2, step 3120/7134 completed (loss: 0.06026340276002884, acc: 0.9915373921394348)
[2025-02-13 02:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04][root][INFO] - Training Epoch: 1/2, step 3121/7134 completed (loss: 0.04100242257118225, acc: 0.9858611822128296)
[2025-02-13 02:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:05][root][INFO] - Training Epoch: 1/2, step 3122/7134 completed (loss: 0.046923186630010605, acc: 0.9872093200683594)
[2025-02-13 02:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:05][root][INFO] - Training Epoch: 1/2, step 3123/7134 completed (loss: 0.06607727706432343, acc: 0.9800994992256165)
[2025-02-13 02:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:06][root][INFO] - Training Epoch: 1/2, step 3124/7134 completed (loss: 0.04458535462617874, acc: 0.9876237511634827)
[2025-02-13 02:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:06][root][INFO] - Training Epoch: 1/2, step 3125/7134 completed (loss: 0.025491833686828613, acc: 0.9909228682518005)
[2025-02-13 02:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07][root][INFO] - Training Epoch: 1/2, step 3126/7134 completed (loss: 0.0449049212038517, acc: 0.987293541431427)
[2025-02-13 02:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07][root][INFO] - Training Epoch: 1/2, step 3127/7134 completed (loss: 0.036445360630750656, acc: 0.9837209582328796)
[2025-02-13 02:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08][root][INFO] - Training Epoch: 1/2, step 3128/7134 completed (loss: 0.023568609729409218, acc: 0.9948387145996094)
[2025-02-13 02:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08][root][INFO] - Training Epoch: 1/2, step 3129/7134 completed (loss: 0.03287487104535103, acc: 0.990111231803894)
[2025-02-13 02:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:09][root][INFO] - Training Epoch: 1/2, step 3130/7134 completed (loss: 0.029753446578979492, acc: 0.9919725060462952)
[2025-02-13 02:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:09][root][INFO] - Training Epoch: 1/2, step 3131/7134 completed (loss: 0.043559979647397995, acc: 0.9826086759567261)
[2025-02-13 02:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:09][root][INFO] - Training Epoch: 1/2, step 3132/7134 completed (loss: 0.02109522558748722, acc: 0.9934924244880676)
[2025-02-13 02:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10][root][INFO] - Training Epoch: 1/2, step 3133/7134 completed (loss: 0.09702566266059875, acc: 0.9807692170143127)
[2025-02-13 02:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10][root][INFO] - Training Epoch: 1/2, step 3134/7134 completed (loss: 0.030263526365160942, acc: 0.9945054650306702)
[2025-02-13 02:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:11][root][INFO] - Training Epoch: 1/2, step 3135/7134 completed (loss: 0.07403212785720825, acc: 0.9838337302207947)
[2025-02-13 02:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:11][root][INFO] - Training Epoch: 1/2, step 3136/7134 completed (loss: 0.02521076612174511, acc: 0.9878261089324951)
[2025-02-13 02:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12][root][INFO] - Training Epoch: 1/2, step 3137/7134 completed (loss: 0.07692910730838776, acc: 0.9742063283920288)
[2025-02-13 02:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12][root][INFO] - Training Epoch: 1/2, step 3138/7134 completed (loss: 0.0645439401268959, acc: 0.9882352948188782)
[2025-02-13 02:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12][root][INFO] - Training Epoch: 1/2, step 3139/7134 completed (loss: 0.09682611376047134, acc: 0.9699570536613464)
[2025-02-13 02:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:13][root][INFO] - Training Epoch: 1/2, step 3140/7134 completed (loss: 0.047385189682245255, acc: 0.9902724027633667)
[2025-02-13 02:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:13][root][INFO] - Training Epoch: 1/2, step 3141/7134 completed (loss: 0.06889337301254272, acc: 0.9807692170143127)
[2025-02-13 02:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:14][root][INFO] - Training Epoch: 1/2, step 3142/7134 completed (loss: 0.019405951723456383, acc: 0.9955157041549683)
[2025-02-13 02:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:14][root][INFO] - Training Epoch: 1/2, step 3143/7134 completed (loss: 0.02469506300985813, acc: 0.9951768517494202)
[2025-02-13 02:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:14][root][INFO] - Training Epoch: 1/2, step 3144/7134 completed (loss: 0.023456411436200142, acc: 0.9905660152435303)
[2025-02-13 02:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15][root][INFO] - Training Epoch: 1/2, step 3145/7134 completed (loss: 0.06096596643328667, acc: 0.9781181812286377)
[2025-02-13 02:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15][root][INFO] - Training Epoch: 1/2, step 3146/7134 completed (loss: 0.03368211165070534, acc: 0.984375)
[2025-02-13 02:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:16][root][INFO] - Training Epoch: 1/2, step 3147/7134 completed (loss: 0.04159478843212128, acc: 0.9871588945388794)
[2025-02-13 02:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:16][root][INFO] - Training Epoch: 1/2, step 3148/7134 completed (loss: 0.03688490763306618, acc: 0.9885203838348389)
[2025-02-13 02:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:17][root][INFO] - Training Epoch: 1/2, step 3149/7134 completed (loss: 0.013373426161706448, acc: 0.9941434860229492)
[2025-02-13 02:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:17][root][INFO] - Training Epoch: 1/2, step 3150/7134 completed (loss: 0.1348239779472351, acc: 0.9704142212867737)
[2025-02-13 02:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:17][root][INFO] - Training Epoch: 1/2, step 3151/7134 completed (loss: 0.11015698313713074, acc: 0.967432975769043)
[2025-02-13 02:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18][root][INFO] - Training Epoch: 1/2, step 3152/7134 completed (loss: 0.04675164818763733, acc: 0.9818181991577148)
[2025-02-13 02:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18][root][INFO] - Training Epoch: 1/2, step 3153/7134 completed (loss: 0.05560136213898659, acc: 0.9799498915672302)
[2025-02-13 02:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:19][root][INFO] - Training Epoch: 1/2, step 3154/7134 completed (loss: 0.05436156317591667, acc: 0.9857904314994812)
[2025-02-13 02:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:19][root][INFO] - Training Epoch: 1/2, step 3155/7134 completed (loss: 0.10627168416976929, acc: 0.9756592512130737)
[2025-02-13 02:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:20][root][INFO] - Training Epoch: 1/2, step 3156/7134 completed (loss: 0.09953643381595612, acc: 0.9702823162078857)
[2025-02-13 02:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:20][root][INFO] - Training Epoch: 1/2, step 3157/7134 completed (loss: 0.035473182797431946, acc: 0.9870967864990234)
[2025-02-13 02:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:20][root][INFO] - Training Epoch: 1/2, step 3158/7134 completed (loss: 0.02558041922748089, acc: 0.991830050945282)
[2025-02-13 02:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21][root][INFO] - Training Epoch: 1/2, step 3159/7134 completed (loss: 0.08486282080411911, acc: 0.970588207244873)
[2025-02-13 02:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21][root][INFO] - Training Epoch: 1/2, step 3160/7134 completed (loss: 0.015328112989664078, acc: 0.9944674968719482)
[2025-02-13 02:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:22][root][INFO] - Training Epoch: 1/2, step 3161/7134 completed (loss: 0.044152501970529556, acc: 0.994397759437561)
[2025-02-13 02:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:22][root][INFO] - Training Epoch: 1/2, step 3162/7134 completed (loss: 0.030879059806466103, acc: 0.9927113652229309)
[2025-02-13 02:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:23][root][INFO] - Training Epoch: 1/2, step 3163/7134 completed (loss: 0.041302286088466644, acc: 0.9916527271270752)
[2025-02-13 02:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:23][root][INFO] - Training Epoch: 1/2, step 3164/7134 completed (loss: 0.049058251082897186, acc: 0.9865996837615967)
[2025-02-13 02:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:23][root][INFO] - Training Epoch: 1/2, step 3165/7134 completed (loss: 0.02860104851424694, acc: 0.994991660118103)
[2025-02-13 02:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:24][root][INFO] - Training Epoch: 1/2, step 3166/7134 completed (loss: 0.07278957217931747, acc: 0.9804511070251465)
[2025-02-13 02:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:24][root][INFO] - Training Epoch: 1/2, step 3167/7134 completed (loss: 0.10897950828075409, acc: 0.96875)
[2025-02-13 02:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25][root][INFO] - Training Epoch: 1/2, step 3168/7134 completed (loss: 0.09315536171197891, acc: 0.9756097793579102)
[2025-02-13 02:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25][root][INFO] - Training Epoch: 1/2, step 3169/7134 completed (loss: 0.07008190453052521, acc: 0.9774590134620667)
[2025-02-13 02:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26][root][INFO] - Training Epoch: 1/2, step 3170/7134 completed (loss: 0.046180155128240585, acc: 0.9800000190734863)
[2025-02-13 02:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26][root][INFO] - Training Epoch: 1/2, step 3171/7134 completed (loss: 0.05967121943831444, acc: 0.9816513657569885)
[2025-02-13 02:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26][root][INFO] - Training Epoch: 1/2, step 3172/7134 completed (loss: 0.022484447807073593, acc: 0.9961685538291931)
[2025-02-13 02:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:27][root][INFO] - Training Epoch: 1/2, step 3173/7134 completed (loss: 0.05871615558862686, acc: 0.9846938848495483)
[2025-02-13 02:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:27][root][INFO] - Training Epoch: 1/2, step 3174/7134 completed (loss: 0.09721210598945618, acc: 0.9718934893608093)
[2025-02-13 02:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28][root][INFO] - Training Epoch: 1/2, step 3175/7134 completed (loss: 0.05935048311948776, acc: 0.9845070242881775)
[2025-02-13 02:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28][root][INFO] - Training Epoch: 1/2, step 3176/7134 completed (loss: 0.09669838845729828, acc: 0.9729323387145996)
[2025-02-13 02:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28][root][INFO] - Training Epoch: 1/2, step 3177/7134 completed (loss: 0.066963329911232, acc: 0.984375)
[2025-02-13 02:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:29][root][INFO] - Training Epoch: 1/2, step 3178/7134 completed (loss: 0.02998991869390011, acc: 0.9950739145278931)
[2025-02-13 02:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:29][root][INFO] - Training Epoch: 1/2, step 3179/7134 completed (loss: 0.07930897921323776, acc: 0.9800994992256165)
[2025-02-13 02:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30][root][INFO] - Training Epoch: 1/2, step 3180/7134 completed (loss: 0.037282560020685196, acc: 0.991428554058075)
[2025-02-13 02:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30][root][INFO] - Training Epoch: 1/2, step 3181/7134 completed (loss: 0.05451151356101036, acc: 0.9859594106674194)
[2025-02-13 02:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30][root][INFO] - Training Epoch: 1/2, step 3182/7134 completed (loss: 0.035573314875364304, acc: 0.9940387606620789)
[2025-02-13 02:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31][root][INFO] - Training Epoch: 1/2, step 3183/7134 completed (loss: 0.05578768253326416, acc: 0.9864636063575745)
[2025-02-13 02:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31][root][INFO] - Training Epoch: 1/2, step 3184/7134 completed (loss: 0.05543742701411247, acc: 0.9866071343421936)
[2025-02-13 02:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:32][root][INFO] - Training Epoch: 1/2, step 3185/7134 completed (loss: 0.010141383856534958, acc: 0.9985755085945129)
[2025-02-13 02:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:32][root][INFO] - Training Epoch: 1/2, step 3186/7134 completed (loss: 0.022142764180898666, acc: 0.9925925731658936)
[2025-02-13 02:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33][root][INFO] - Training Epoch: 1/2, step 3187/7134 completed (loss: 0.043638814240694046, acc: 0.9857594966888428)
[2025-02-13 02:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33][root][INFO] - Training Epoch: 1/2, step 3188/7134 completed (loss: 0.03700797259807587, acc: 0.9896507263183594)
[2025-02-13 02:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33][root][INFO] - Training Epoch: 1/2, step 3189/7134 completed (loss: 0.06420762836933136, acc: 0.9811866879463196)
[2025-02-13 02:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:34][root][INFO] - Training Epoch: 1/2, step 3190/7134 completed (loss: 0.04712046682834625, acc: 0.9898734092712402)
[2025-02-13 02:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:34][root][INFO] - Training Epoch: 1/2, step 3191/7134 completed (loss: 0.031518690288066864, acc: 0.991428554058075)
[2025-02-13 02:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35][root][INFO] - Training Epoch: 1/2, step 3192/7134 completed (loss: 0.055993229150772095, acc: 0.9829867482185364)
[2025-02-13 02:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35][root][INFO] - Training Epoch: 1/2, step 3193/7134 completed (loss: 0.053575485944747925, acc: 0.9838945865631104)
[2025-02-13 02:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35][root][INFO] - Training Epoch: 1/2, step 3194/7134 completed (loss: 0.05148003622889519, acc: 0.983660101890564)
[2025-02-13 02:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36][root][INFO] - Training Epoch: 1/2, step 3195/7134 completed (loss: 0.05359611287713051, acc: 0.9845626354217529)
[2025-02-13 02:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36][root][INFO] - Training Epoch: 1/2, step 3196/7134 completed (loss: 0.025550737977027893, acc: 0.9906542301177979)
[2025-02-13 02:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:37][root][INFO] - Training Epoch: 1/2, step 3197/7134 completed (loss: 0.044494498521089554, acc: 0.9929577708244324)
[2025-02-13 02:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:37][root][INFO] - Training Epoch: 1/2, step 3198/7134 completed (loss: 0.03337116912007332, acc: 0.9894099831581116)
[2025-02-13 02:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:38][root][INFO] - Training Epoch: 1/2, step 3199/7134 completed (loss: 0.038158807903528214, acc: 0.9867841601371765)
[2025-02-13 02:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:38][root][INFO] - Training Epoch: 1/2, step 3200/7134 completed (loss: 0.043035246431827545, acc: 0.9912126660346985)
[2025-02-13 02:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:38][root][INFO] - Training Epoch: 1/2, step 3201/7134 completed (loss: 0.12182541191577911, acc: 0.9739696383476257)
[2025-02-13 02:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39][root][INFO] - Training Epoch: 1/2, step 3202/7134 completed (loss: 0.03184747323393822, acc: 0.9903181195259094)
[2025-02-13 02:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39][root][INFO] - Training Epoch: 1/2, step 3203/7134 completed (loss: 0.04587055370211601, acc: 0.9873617887496948)
[2025-02-13 02:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40][root][INFO] - Training Epoch: 1/2, step 3204/7134 completed (loss: 0.07567259669303894, acc: 0.9774436354637146)
[2025-02-13 02:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40][root][INFO] - Training Epoch: 1/2, step 3205/7134 completed (loss: 0.01106128841638565, acc: 1.0)
[2025-02-13 02:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40][root][INFO] - Training Epoch: 1/2, step 3206/7134 completed (loss: 0.0376906581223011, acc: 0.9927641153335571)
[2025-02-13 02:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41][root][INFO] - Training Epoch: 1/2, step 3207/7134 completed (loss: 0.044684965163469315, acc: 0.9846860766410828)
[2025-02-13 02:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41][root][INFO] - Training Epoch: 1/2, step 3208/7134 completed (loss: 0.09243832528591156, acc: 0.983660101890564)
[2025-02-13 02:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42][root][INFO] - Training Epoch: 1/2, step 3209/7134 completed (loss: 0.017814187332987785, acc: 0.9940476417541504)
[2025-02-13 02:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42][root][INFO] - Training Epoch: 1/2, step 3210/7134 completed (loss: 0.011828272603452206, acc: 0.9969879388809204)
[2025-02-13 02:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43][root][INFO] - Training Epoch: 1/2, step 3211/7134 completed (loss: 0.04843190684914589, acc: 0.9896050095558167)
[2025-02-13 02:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43][root][INFO] - Training Epoch: 1/2, step 3212/7134 completed (loss: 0.004618677776306868, acc: 1.0)
[2025-02-13 02:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43][root][INFO] - Training Epoch: 1/2, step 3213/7134 completed (loss: 0.031165964901447296, acc: 0.9928264021873474)
[2025-02-13 02:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44][root][INFO] - Training Epoch: 1/2, step 3214/7134 completed (loss: 0.031415652483701706, acc: 0.9919678568840027)
[2025-02-13 02:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44][root][INFO] - Training Epoch: 1/2, step 3215/7134 completed (loss: 0.0378444567322731, acc: 0.9901960492134094)
[2025-02-13 02:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45][root][INFO] - Training Epoch: 1/2, step 3216/7134 completed (loss: 0.019719932228326797, acc: 0.9942693114280701)
[2025-02-13 02:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45][root][INFO] - Training Epoch: 1/2, step 3217/7134 completed (loss: 0.02199670299887657, acc: 0.9935483932495117)
[2025-02-13 02:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46][root][INFO] - Training Epoch: 1/2, step 3218/7134 completed (loss: 0.04428965970873833, acc: 0.9890453815460205)
[2025-02-13 02:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46][root][INFO] - Training Epoch: 1/2, step 3219/7134 completed (loss: 0.0343044213950634, acc: 0.9867060780525208)
[2025-02-13 02:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46][root][INFO] - Training Epoch: 1/2, step 3220/7134 completed (loss: 0.041590366512537, acc: 0.9906542301177979)
[2025-02-13 02:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:47][root][INFO] - Training Epoch: 1/2, step 3221/7134 completed (loss: 0.04775339365005493, acc: 0.983561635017395)
[2025-02-13 02:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:47][root][INFO] - Training Epoch: 1/2, step 3222/7134 completed (loss: 0.06667545437812805, acc: 0.9837296605110168)
[2025-02-13 02:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48][root][INFO] - Training Epoch: 1/2, step 3223/7134 completed (loss: 0.06665557622909546, acc: 0.9873015880584717)
[2025-02-13 02:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48][root][INFO] - Training Epoch: 1/2, step 3224/7134 completed (loss: 0.07786143571138382, acc: 0.9826338887214661)
[2025-02-13 02:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49][root][INFO] - Training Epoch: 1/2, step 3225/7134 completed (loss: 0.032396771013736725, acc: 0.991150438785553)
[2025-02-13 02:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49][root][INFO] - Training Epoch: 1/2, step 3226/7134 completed (loss: 0.03851491957902908, acc: 0.9874326586723328)
[2025-02-13 02:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50][root][INFO] - Training Epoch: 1/2, step 3227/7134 completed (loss: 0.07753866910934448, acc: 0.9791208505630493)
[2025-02-13 02:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50][root][INFO] - Training Epoch: 1/2, step 3228/7134 completed (loss: 0.02975372225046158, acc: 0.9908015727996826)
[2025-02-13 02:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50][root][INFO] - Training Epoch: 1/2, step 3229/7134 completed (loss: 0.04096505045890808, acc: 0.9877601265907288)
[2025-02-13 02:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51][root][INFO] - Training Epoch: 1/2, step 3230/7134 completed (loss: 0.047177959233522415, acc: 0.9837278127670288)
[2025-02-13 02:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51][root][INFO] - Training Epoch: 1/2, step 3231/7134 completed (loss: 0.059747371822595596, acc: 0.9848024249076843)
[2025-02-13 02:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52][root][INFO] - Training Epoch: 1/2, step 3232/7134 completed (loss: 0.020659763365983963, acc: 0.9943883419036865)
[2025-02-13 02:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52][root][INFO] - Training Epoch: 1/2, step 3233/7134 completed (loss: 0.07535888254642487, acc: 0.9842209219932556)
[2025-02-13 02:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52][root][INFO] - Training Epoch: 1/2, step 3234/7134 completed (loss: 0.023375459015369415, acc: 0.9968404173851013)
[2025-02-13 02:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:53][root][INFO] - Training Epoch: 1/2, step 3235/7134 completed (loss: 0.03847254067659378, acc: 0.9828571677207947)
[2025-02-13 02:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:53][root][INFO] - Training Epoch: 1/2, step 3236/7134 completed (loss: 0.08920064568519592, acc: 0.9779086709022522)
[2025-02-13 02:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54][root][INFO] - Training Epoch: 1/2, step 3237/7134 completed (loss: 0.02479277364909649, acc: 0.9951691031455994)
[2025-02-13 02:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54][root][INFO] - Training Epoch: 1/2, step 3238/7134 completed (loss: 0.05627324432134628, acc: 0.9831288456916809)
[2025-02-13 02:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:55][root][INFO] - Training Epoch: 1/2, step 3239/7134 completed (loss: 0.018843943253159523, acc: 0.9936908483505249)
[2025-02-13 02:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:55][root][INFO] - Training Epoch: 1/2, step 3240/7134 completed (loss: 0.031184909865260124, acc: 0.9895366430282593)
[2025-02-13 02:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:55][root][INFO] - Training Epoch: 1/2, step 3241/7134 completed (loss: 0.024376410990953445, acc: 0.9955489635467529)
[2025-02-13 02:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56][root][INFO] - Training Epoch: 1/2, step 3242/7134 completed (loss: 0.019651589915156364, acc: 0.9934640526771545)
[2025-02-13 02:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56][root][INFO] - Training Epoch: 1/2, step 3243/7134 completed (loss: 0.026769815012812614, acc: 0.9918256402015686)
[2025-02-13 02:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57][root][INFO] - Training Epoch: 1/2, step 3244/7134 completed (loss: 0.03307515010237694, acc: 0.9899569749832153)
[2025-02-13 02:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57][root][INFO] - Training Epoch: 1/2, step 3245/7134 completed (loss: 0.02057178132236004, acc: 0.9961340427398682)
[2025-02-13 02:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58][root][INFO] - Training Epoch: 1/2, step 3246/7134 completed (loss: 0.04953894019126892, acc: 0.985897421836853)
[2025-02-13 02:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58][root][INFO] - Training Epoch: 1/2, step 3247/7134 completed (loss: 0.027664706110954285, acc: 0.9935170412063599)
[2025-02-13 02:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58][root][INFO] - Training Epoch: 1/2, step 3248/7134 completed (loss: 0.025387784466147423, acc: 0.9934318661689758)
[2025-02-13 02:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59][root][INFO] - Training Epoch: 1/2, step 3249/7134 completed (loss: 0.03461597114801407, acc: 0.9916782379150391)
[2025-02-13 02:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59][root][INFO] - Training Epoch: 1/2, step 3250/7134 completed (loss: 0.043892305344343185, acc: 0.9827337861061096)
[2025-02-13 02:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00][root][INFO] - Training Epoch: 1/2, step 3251/7134 completed (loss: 0.012088671326637268, acc: 0.995720386505127)
[2025-02-13 02:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00][root][INFO] - Training Epoch: 1/2, step 3252/7134 completed (loss: 0.009470910765230656, acc: 0.9971631169319153)
[2025-02-13 02:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00][root][INFO] - Training Epoch: 1/2, step 3253/7134 completed (loss: 0.02476564794778824, acc: 0.9935794472694397)
[2025-02-13 02:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01][root][INFO] - Training Epoch: 1/2, step 3254/7134 completed (loss: 0.01402211282402277, acc: 0.9932546615600586)
[2025-02-13 02:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01][root][INFO] - Training Epoch: 1/2, step 3255/7134 completed (loss: 0.01274953130632639, acc: 0.9960052967071533)
[2025-02-13 02:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02][root][INFO] - Training Epoch: 1/2, step 3256/7134 completed (loss: 0.020043205469846725, acc: 0.9970370531082153)
[2025-02-13 02:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02][root][INFO] - Training Epoch: 1/2, step 3257/7134 completed (loss: 0.022925015538930893, acc: 0.994452178478241)
[2025-02-13 02:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03][root][INFO] - Training Epoch: 1/2, step 3258/7134 completed (loss: 0.035014595836400986, acc: 0.9865384697914124)
[2025-02-13 02:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03][root][INFO] - Training Epoch: 1/2, step 3259/7134 completed (loss: 0.025430331006646156, acc: 0.9897058606147766)
[2025-02-13 02:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03][root][INFO] - Training Epoch: 1/2, step 3260/7134 completed (loss: 0.08297525346279144, acc: 0.9836795330047607)
[2025-02-13 02:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04][root][INFO] - Training Epoch: 1/2, step 3261/7134 completed (loss: 0.0689280778169632, acc: 0.9815436005592346)
[2025-02-13 02:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04][root][INFO] - Training Epoch: 1/2, step 3262/7134 completed (loss: 0.03166817128658295, acc: 0.986975371837616)
[2025-02-13 02:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05][root][INFO] - Training Epoch: 1/2, step 3263/7134 completed (loss: 0.06076415628194809, acc: 0.9867841601371765)
[2025-02-13 02:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05][root][INFO] - Training Epoch: 1/2, step 3264/7134 completed (loss: 0.031015345826745033, acc: 0.9916666746139526)
[2025-02-13 02:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06][root][INFO] - Training Epoch: 1/2, step 3265/7134 completed (loss: 0.09383593499660492, acc: 0.9800570011138916)
[2025-02-13 02:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06][root][INFO] - Training Epoch: 1/2, step 3266/7134 completed (loss: 0.04488878324627876, acc: 0.9894894957542419)
[2025-02-13 02:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06][root][INFO] - Training Epoch: 1/2, step 3267/7134 completed (loss: 0.05875787511467934, acc: 0.9871060252189636)
[2025-02-13 02:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07][root][INFO] - Training Epoch: 1/2, step 3268/7134 completed (loss: 0.049062471836805344, acc: 0.9861325025558472)
[2025-02-13 02:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07][root][INFO] - Training Epoch: 1/2, step 3269/7134 completed (loss: 0.04732363298535347, acc: 0.9873737096786499)
[2025-02-13 02:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08][root][INFO] - Training Epoch: 1/2, step 3270/7134 completed (loss: 0.018836112692952156, acc: 0.9964115023612976)
[2025-02-13 02:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08][root][INFO] - Training Epoch: 1/2, step 3271/7134 completed (loss: 0.00685080885887146, acc: 1.0)
[2025-02-13 02:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08][root][INFO] - Training Epoch: 1/2, step 3272/7134 completed (loss: 0.03086714632809162, acc: 0.9969465732574463)
[2025-02-13 02:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09][root][INFO] - Training Epoch: 1/2, step 3273/7134 completed (loss: 0.020322971045970917, acc: 0.992337167263031)
[2025-02-13 02:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09][root][INFO] - Training Epoch: 1/2, step 3274/7134 completed (loss: 0.05392718315124512, acc: 0.9897210001945496)
[2025-02-13 02:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10][root][INFO] - Training Epoch: 1/2, step 3275/7134 completed (loss: 0.14246676862239838, acc: 0.9736841917037964)
[2025-02-13 02:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10][root][INFO] - Training Epoch: 1/2, step 3276/7134 completed (loss: 0.08128303289413452, acc: 0.9738292098045349)
[2025-02-13 02:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11][root][INFO] - Training Epoch: 1/2, step 3277/7134 completed (loss: 0.04896657168865204, acc: 0.9771341681480408)
[2025-02-13 02:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11][root][INFO] - Training Epoch: 1/2, step 3278/7134 completed (loss: 0.04979415982961655, acc: 0.9847036600112915)
[2025-02-13 02:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12][root][INFO] - Training Epoch: 1/2, step 3279/7134 completed (loss: 0.042192358523607254, acc: 0.991304337978363)
[2025-02-13 02:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12][root][INFO] - Training Epoch: 1/2, step 3280/7134 completed (loss: 0.1682678759098053, acc: 0.9626556038856506)
[2025-02-13 02:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12][root][INFO] - Training Epoch: 1/2, step 3281/7134 completed (loss: 0.05850251764059067, acc: 0.9855072498321533)
[2025-02-13 02:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13][root][INFO] - Training Epoch: 1/2, step 3282/7134 completed (loss: 0.03570571169257164, acc: 0.9920886158943176)
[2025-02-13 02:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13][root][INFO] - Training Epoch: 1/2, step 3283/7134 completed (loss: 0.03468021750450134, acc: 0.9896907210350037)
[2025-02-13 02:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14][root][INFO] - Training Epoch: 1/2, step 3284/7134 completed (loss: 0.06327196210622787, acc: 0.9817351698875427)
[2025-02-13 02:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14][root][INFO] - Training Epoch: 1/2, step 3285/7134 completed (loss: 0.036450136452913284, acc: 0.9876161217689514)
[2025-02-13 02:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15][root][INFO] - Training Epoch: 1/2, step 3286/7134 completed (loss: 0.04022965952754021, acc: 0.9909774661064148)
[2025-02-13 02:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15][root][INFO] - Training Epoch: 1/2, step 3287/7134 completed (loss: 0.10687864571809769, acc: 0.9736841917037964)
[2025-02-13 02:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15][root][INFO] - Training Epoch: 1/2, step 3288/7134 completed (loss: 0.0372389554977417, acc: 0.9880239367485046)
[2025-02-13 02:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16][root][INFO] - Training Epoch: 1/2, step 3289/7134 completed (loss: 0.08009142428636551, acc: 0.9736841917037964)
[2025-02-13 02:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16][root][INFO] - Training Epoch: 1/2, step 3290/7134 completed (loss: 0.05959823355078697, acc: 0.9809027910232544)
[2025-02-13 02:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17][root][INFO] - Training Epoch: 1/2, step 3291/7134 completed (loss: 0.06289055943489075, acc: 0.9688196182250977)
[2025-02-13 02:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17][root][INFO] - Training Epoch: 1/2, step 3292/7134 completed (loss: 0.03344397619366646, acc: 0.9894179701805115)
[2025-02-13 02:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17][root][INFO] - Training Epoch: 1/2, step 3293/7134 completed (loss: 0.04500151798129082, acc: 0.98525071144104)
[2025-02-13 02:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18][root][INFO] - Training Epoch: 1/2, step 3294/7134 completed (loss: 0.07932977378368378, acc: 0.9754977226257324)
[2025-02-13 02:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18][root][INFO] - Training Epoch: 1/2, step 3295/7134 completed (loss: 0.022163566201925278, acc: 0.9948805570602417)
[2025-02-13 02:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19][root][INFO] - Training Epoch: 1/2, step 3296/7134 completed (loss: 0.05491461604833603, acc: 0.9817184805870056)
[2025-02-13 02:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19][root][INFO] - Training Epoch: 1/2, step 3297/7134 completed (loss: 0.03977840393781662, acc: 0.98591548204422)
[2025-02-13 02:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19][root][INFO] - Training Epoch: 1/2, step 3298/7134 completed (loss: 0.09218443930149078, acc: 0.9808306694030762)
[2025-02-13 02:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20][root][INFO] - Training Epoch: 1/2, step 3299/7134 completed (loss: 0.03853030130267143, acc: 0.9899328947067261)
[2025-02-13 02:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20][root][INFO] - Training Epoch: 1/2, step 3300/7134 completed (loss: 0.07512591034173965, acc: 0.976827085018158)
[2025-02-13 02:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21][root][INFO] - Training Epoch: 1/2, step 3301/7134 completed (loss: 0.05717240273952484, acc: 0.9834024906158447)
[2025-02-13 02:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21][root][INFO] - Training Epoch: 1/2, step 3302/7134 completed (loss: 0.06254924088716507, acc: 0.9882155060768127)
[2025-02-13 02:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22][root][INFO] - Training Epoch: 1/2, step 3303/7134 completed (loss: 0.04591995105147362, acc: 0.9889867901802063)
[2025-02-13 02:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22][root][INFO] - Training Epoch: 1/2, step 3304/7134 completed (loss: 0.02875879593193531, acc: 0.9928401112556458)
[2025-02-13 02:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22][root][INFO] - Training Epoch: 1/2, step 3305/7134 completed (loss: 0.04688267037272453, acc: 0.9894419312477112)
[2025-02-13 02:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23][root][INFO] - Training Epoch: 1/2, step 3306/7134 completed (loss: 0.03149735555052757, acc: 0.9896238446235657)
[2025-02-13 02:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23][root][INFO] - Training Epoch: 1/2, step 3307/7134 completed (loss: 0.049454107880592346, acc: 0.9830795526504517)
[2025-02-13 02:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24][root][INFO] - Training Epoch: 1/2, step 3308/7134 completed (loss: 0.06933630257844925, acc: 0.9797688126564026)
[2025-02-13 02:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24][root][INFO] - Training Epoch: 1/2, step 3309/7134 completed (loss: 0.07140576839447021, acc: 0.974926233291626)
[2025-02-13 02:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25][root][INFO] - Training Epoch: 1/2, step 3310/7134 completed (loss: 0.04347681626677513, acc: 0.9936034083366394)
[2025-02-13 02:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25][root][INFO] - Training Epoch: 1/2, step 3311/7134 completed (loss: 0.030462773516774178, acc: 0.9913978576660156)
[2025-02-13 02:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25][root][INFO] - Training Epoch: 1/2, step 3312/7134 completed (loss: 0.03285522386431694, acc: 0.9872727394104004)
[2025-02-13 02:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26][root][INFO] - Training Epoch: 1/2, step 3313/7134 completed (loss: 0.013100296258926392, acc: 0.9976133704185486)
[2025-02-13 02:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26][root][INFO] - Training Epoch: 1/2, step 3314/7134 completed (loss: 0.04132715240120888, acc: 0.9909420013427734)
[2025-02-13 02:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:27][root][INFO] - Training Epoch: 1/2, step 3315/7134 completed (loss: 0.06409440189599991, acc: 0.9850746393203735)
[2025-02-13 02:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:27][root][INFO] - Training Epoch: 1/2, step 3316/7134 completed (loss: 0.022493699565529823, acc: 0.9932773113250732)
[2025-02-13 02:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28][root][INFO] - Training Epoch: 1/2, step 3317/7134 completed (loss: 0.08091632276773453, acc: 0.9769230484962463)
[2025-02-13 02:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28][root][INFO] - Training Epoch: 1/2, step 3318/7134 completed (loss: 0.054146699607372284, acc: 0.9855595827102661)
[2025-02-13 02:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28][root][INFO] - Training Epoch: 1/2, step 3319/7134 completed (loss: 0.048366207629442215, acc: 0.990227997303009)
[2025-02-13 02:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29][root][INFO] - Training Epoch: 1/2, step 3320/7134 completed (loss: 0.07844716310501099, acc: 0.981675386428833)
[2025-02-13 02:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29][root][INFO] - Training Epoch: 1/2, step 3321/7134 completed (loss: 0.09507057070732117, acc: 0.9761468172073364)
[2025-02-13 02:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30][root][INFO] - Training Epoch: 1/2, step 3322/7134 completed (loss: 0.038271717727184296, acc: 0.9924924969673157)
[2025-02-13 02:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30][root][INFO] - Training Epoch: 1/2, step 3323/7134 completed (loss: 0.0444934144616127, acc: 0.9875862002372742)
[2025-02-13 02:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31][root][INFO] - Training Epoch: 1/2, step 3324/7134 completed (loss: 0.08179757744073868, acc: 0.9752650260925293)
[2025-02-13 02:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31][root][INFO] - Training Epoch: 1/2, step 3325/7134 completed (loss: 0.049685582518577576, acc: 0.9838709831237793)
[2025-02-13 02:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32][root][INFO] - Training Epoch: 1/2, step 3326/7134 completed (loss: 0.06201186403632164, acc: 0.9858267903327942)
[2025-02-13 02:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32][root][INFO] - Training Epoch: 1/2, step 3327/7134 completed (loss: 0.045226920396089554, acc: 0.9824086427688599)
[2025-02-13 02:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32][root][INFO] - Training Epoch: 1/2, step 3328/7134 completed (loss: 0.045624807476997375, acc: 0.9887217879295349)
[2025-02-13 02:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33][root][INFO] - Training Epoch: 1/2, step 3329/7134 completed (loss: 0.017421241849660873, acc: 0.9975639581680298)
[2025-02-13 02:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33][root][INFO] - Training Epoch: 1/2, step 3330/7134 completed (loss: 0.04644519463181496, acc: 0.9884792566299438)
[2025-02-13 02:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34][root][INFO] - Training Epoch: 1/2, step 3331/7134 completed (loss: 0.09052284061908722, acc: 0.9738371968269348)
[2025-02-13 02:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34][root][INFO] - Training Epoch: 1/2, step 3332/7134 completed (loss: 0.11898776143789291, acc: 0.970588207244873)
[2025-02-13 02:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35][root][INFO] - Training Epoch: 1/2, step 3333/7134 completed (loss: 0.05852920189499855, acc: 0.9856557250022888)
[2025-02-13 02:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35][root][INFO] - Training Epoch: 1/2, step 3334/7134 completed (loss: 0.08795143663883209, acc: 0.9770269989967346)
[2025-02-13 02:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36][root][INFO] - Training Epoch: 1/2, step 3335/7134 completed (loss: 0.0445360466837883, acc: 0.9841772317886353)
[2025-02-13 02:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36][root][INFO] - Training Epoch: 1/2, step 3336/7134 completed (loss: 0.025008732452988625, acc: 0.991253674030304)
[2025-02-13 02:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36][root][INFO] - Training Epoch: 1/2, step 3337/7134 completed (loss: 0.017805976793169975, acc: 0.9960552453994751)
[2025-02-13 02:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:37][root][INFO] - Training Epoch: 1/2, step 3338/7134 completed (loss: 0.011875479482114315, acc: 0.9975154995918274)
[2025-02-13 02:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:37][root][INFO] - Training Epoch: 1/2, step 3339/7134 completed (loss: 0.0685565248131752, acc: 0.9856801629066467)
[2025-02-13 02:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38][root][INFO] - Training Epoch: 1/2, step 3340/7134 completed (loss: 0.12497256696224213, acc: 0.9668174982070923)
[2025-02-13 02:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38][root][INFO] - Training Epoch: 1/2, step 3341/7134 completed (loss: 0.0653766542673111, acc: 0.9822221994400024)
[2025-02-13 02:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][root][INFO] - Training Epoch: 1/2, step 3342/7134 completed (loss: 0.037298332899808884, acc: 0.9864864945411682)
[2025-02-13 02:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][root][INFO] - Training Epoch: 1/2, step 3343/7134 completed (loss: 0.046299006789922714, acc: 0.9858871102333069)
[2025-02-13 02:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][root][INFO] - Training Epoch: 1/2, step 3344/7134 completed (loss: 0.029705462977290154, acc: 0.9914236664772034)
[2025-02-13 02:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:40][root][INFO] - Training Epoch: 1/2, step 3345/7134 completed (loss: 0.08586367964744568, acc: 0.9821717739105225)
[2025-02-13 02:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:40][root][INFO] - Training Epoch: 1/2, step 3346/7134 completed (loss: 0.028662867844104767, acc: 0.9897660613059998)
[2025-02-13 02:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41][root][INFO] - Training Epoch: 1/2, step 3347/7134 completed (loss: 0.07723896950483322, acc: 0.9771987199783325)
[2025-02-13 02:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41][root][INFO] - Training Epoch: 1/2, step 3348/7134 completed (loss: 0.06222859025001526, acc: 0.9851852059364319)
[2025-02-13 02:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42][root][INFO] - Training Epoch: 1/2, step 3349/7134 completed (loss: 0.022230690345168114, acc: 0.9964726567268372)
[2025-02-13 02:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42][root][INFO] - Training Epoch: 1/2, step 3350/7134 completed (loss: 0.03233485296368599, acc: 0.9898374080657959)
[2025-02-13 02:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42][root][INFO] - Training Epoch: 1/2, step 3351/7134 completed (loss: 0.09404262900352478, acc: 0.9840849041938782)
[2025-02-13 02:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:43][root][INFO] - Training Epoch: 1/2, step 3352/7134 completed (loss: 0.020824072882533073, acc: 0.9947826266288757)
[2025-02-13 02:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:43][root][INFO] - Training Epoch: 1/2, step 3353/7134 completed (loss: 0.05531489476561546, acc: 0.9881656765937805)
[2025-02-13 02:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44][root][INFO] - Training Epoch: 1/2, step 3354/7134 completed (loss: 0.03299112990498543, acc: 0.9922480583190918)
[2025-02-13 02:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44][root][INFO] - Training Epoch: 1/2, step 3355/7134 completed (loss: 0.020121313631534576, acc: 0.994358241558075)
[2025-02-13 02:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45][root][INFO] - Training Epoch: 1/2, step 3356/7134 completed (loss: 0.012638460844755173, acc: 0.9965277910232544)
[2025-02-13 02:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45][root][INFO] - Training Epoch: 1/2, step 3357/7134 completed (loss: 0.03219323605298996, acc: 0.9913420081138611)
[2025-02-13 02:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45][root][INFO] - Training Epoch: 1/2, step 3358/7134 completed (loss: 0.05742167308926582, acc: 0.9930796027183533)
[2025-02-13 02:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46][root][INFO] - Training Epoch: 1/2, step 3359/7134 completed (loss: 0.0317864716053009, acc: 0.991909384727478)
[2025-02-13 02:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46][root][INFO] - Training Epoch: 1/2, step 3360/7134 completed (loss: 0.1012461930513382, acc: 0.9806867241859436)
[2025-02-13 02:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47][root][INFO] - Training Epoch: 1/2, step 3361/7134 completed (loss: 0.06876935809850693, acc: 0.9798657894134521)
[2025-02-13 02:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47][root][INFO] - Training Epoch: 1/2, step 3362/7134 completed (loss: 0.052891805768013, acc: 0.9841269850730896)
[2025-02-13 02:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47][root][INFO] - Training Epoch: 1/2, step 3363/7134 completed (loss: 0.011282000690698624, acc: 0.9976525902748108)
[2025-02-13 02:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48][root][INFO] - Training Epoch: 1/2, step 3364/7134 completed (loss: 0.08396080881357193, acc: 0.9769585132598877)
[2025-02-13 02:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48][root][INFO] - Training Epoch: 1/2, step 3365/7134 completed (loss: 0.07366161793470383, acc: 0.983132541179657)
[2025-02-13 02:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49][root][INFO] - Training Epoch: 1/2, step 3366/7134 completed (loss: 0.03191033750772476, acc: 0.9910314083099365)
[2025-02-13 02:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49][root][INFO] - Training Epoch: 1/2, step 3367/7134 completed (loss: 0.04380204156041145, acc: 0.9876033067703247)
[2025-02-13 02:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49][root][INFO] - Training Epoch: 1/2, step 3368/7134 completed (loss: 0.044619832187891006, acc: 0.9875195026397705)
[2025-02-13 02:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:50][root][INFO] - Training Epoch: 1/2, step 3369/7134 completed (loss: 0.05516150966286659, acc: 0.9870503544807434)
[2025-02-13 02:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:50][root][INFO] - Training Epoch: 1/2, step 3370/7134 completed (loss: 0.029280399903655052, acc: 0.9871244430541992)
[2025-02-13 02:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51][root][INFO] - Training Epoch: 1/2, step 3371/7134 completed (loss: 0.03103923238813877, acc: 0.9950658082962036)
[2025-02-13 02:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51][root][INFO] - Training Epoch: 1/2, step 3372/7134 completed (loss: 0.03537592664361, acc: 0.9890282154083252)
[2025-02-13 02:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52][root][INFO] - Training Epoch: 1/2, step 3373/7134 completed (loss: 0.027920620515942574, acc: 0.9891975522041321)
[2025-02-13 02:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52][root][INFO] - Training Epoch: 1/2, step 3374/7134 completed (loss: 0.034107755869627, acc: 0.9916434288024902)
[2025-02-13 02:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53][root][INFO] - Training Epoch: 1/2, step 3375/7134 completed (loss: 0.029696254059672356, acc: 0.9913169145584106)
[2025-02-13 02:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53][root][INFO] - Training Epoch: 1/2, step 3376/7134 completed (loss: 0.02554754726588726, acc: 0.9910600185394287)
[2025-02-13 02:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53][root][INFO] - Training Epoch: 1/2, step 3377/7134 completed (loss: 0.019778216257691383, acc: 0.9964243173599243)
[2025-02-13 02:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54][root][INFO] - Training Epoch: 1/2, step 3378/7134 completed (loss: 0.016744138672947884, acc: 0.9934895634651184)
[2025-02-13 02:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54][root][INFO] - Training Epoch: 1/2, step 3379/7134 completed (loss: 0.029007554054260254, acc: 0.9894737005233765)
[2025-02-13 02:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55][root][INFO] - Training Epoch: 1/2, step 3380/7134 completed (loss: 0.035219524055719376, acc: 0.9907692074775696)
[2025-02-13 02:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55][root][INFO] - Training Epoch: 1/2, step 3381/7134 completed (loss: 0.04706422984600067, acc: 0.985788106918335)
[2025-02-13 02:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56][root][INFO] - Training Epoch: 1/2, step 3382/7134 completed (loss: 0.01136049348860979, acc: 0.9965397715568542)
[2025-02-13 02:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56][root][INFO] - Training Epoch: 1/2, step 3383/7134 completed (loss: 0.0413961298763752, acc: 0.991391658782959)
[2025-02-13 02:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57][root][INFO] - Training Epoch: 1/2, step 3384/7134 completed (loss: 0.04360588639974594, acc: 0.9923547506332397)
[2025-02-13 02:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57][root][INFO] - Training Epoch: 1/2, step 3385/7134 completed (loss: 0.040704645216464996, acc: 0.9899280667304993)
[2025-02-13 02:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57][root][INFO] - Training Epoch: 1/2, step 3386/7134 completed (loss: 0.02152782864868641, acc: 0.991525411605835)
[2025-02-13 02:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58][root][INFO] - Training Epoch: 1/2, step 3387/7134 completed (loss: 0.05139943212270737, acc: 0.9904761910438538)
[2025-02-13 02:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58][root][INFO] - Training Epoch: 1/2, step 3388/7134 completed (loss: 0.05204750597476959, acc: 0.9873417615890503)
[2025-02-13 02:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59][root][INFO] - Training Epoch: 1/2, step 3389/7134 completed (loss: 0.022397397086024284, acc: 0.9960474371910095)
[2025-02-13 02:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59][root][INFO] - Training Epoch: 1/2, step 3390/7134 completed (loss: 0.022385546937584877, acc: 0.9922279715538025)
[2025-02-13 02:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59][root][INFO] - Training Epoch: 1/2, step 3391/7134 completed (loss: 0.01010768860578537, acc: 0.9972527623176575)
[2025-02-13 02:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00][root][INFO] - Training Epoch: 1/2, step 3392/7134 completed (loss: 0.013609246350824833, acc: 0.9968553185462952)
[2025-02-13 02:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00][root][INFO] - Training Epoch: 1/2, step 3393/7134 completed (loss: 0.03729040175676346, acc: 0.9891975522041321)
[2025-02-13 02:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01][root][INFO] - Training Epoch: 1/2, step 3394/7134 completed (loss: 0.027471201494336128, acc: 0.9889240264892578)
[2025-02-13 02:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01][root][INFO] - Training Epoch: 1/2, step 3395/7134 completed (loss: 0.08825918287038803, acc: 0.9772727489471436)
[2025-02-13 02:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02][root][INFO] - Training Epoch: 1/2, step 3396/7134 completed (loss: 0.0317595899105072, acc: 0.9928143620491028)
[2025-02-13 02:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02][root][INFO] - Training Epoch: 1/2, step 3397/7134 completed (loss: 0.04021276533603668, acc: 0.9926650524139404)
[2025-02-13 02:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02][root][INFO] - Training Epoch: 1/2, step 3398/7134 completed (loss: 0.04365071281790733, acc: 0.9910394549369812)
[2025-02-13 02:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03][root][INFO] - Training Epoch: 1/2, step 3399/7134 completed (loss: 0.09761596471071243, acc: 0.9761029481887817)
[2025-02-13 02:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03][root][INFO] - Training Epoch: 1/2, step 3400/7134 completed (loss: 0.08209102600812912, acc: 0.9817444086074829)
[2025-02-13 02:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04][root][INFO] - Training Epoch: 1/2, step 3401/7134 completed (loss: 0.06266063451766968, acc: 0.9825174808502197)
[2025-02-13 02:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04][root][INFO] - Training Epoch: 1/2, step 3402/7134 completed (loss: 0.0839126780629158, acc: 0.9789196252822876)
[2025-02-13 02:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:05][root][INFO] - Training Epoch: 1/2, step 3403/7134 completed (loss: 0.10037911683320999, acc: 0.9778645634651184)
[2025-02-13 02:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:05][root][INFO] - Training Epoch: 1/2, step 3404/7134 completed (loss: 0.015929408371448517, acc: 0.9955157041549683)
[2025-02-13 02:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:05][root][INFO] - Training Epoch: 1/2, step 3405/7134 completed (loss: 0.09373605996370316, acc: 0.9820144176483154)
[2025-02-13 02:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06][root][INFO] - Training Epoch: 1/2, step 3406/7134 completed (loss: 0.05062449723482132, acc: 0.9845361113548279)
[2025-02-13 02:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06][root][INFO] - Training Epoch: 1/2, step 3407/7134 completed (loss: 0.04622630774974823, acc: 0.9843993782997131)
[2025-02-13 02:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07][root][INFO] - Training Epoch: 1/2, step 3408/7134 completed (loss: 0.051988519728183746, acc: 0.981566846370697)
[2025-02-13 02:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07][root][INFO] - Training Epoch: 1/2, step 3409/7134 completed (loss: 0.04281207174062729, acc: 0.9905277490615845)
[2025-02-13 02:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08][root][INFO] - Training Epoch: 1/2, step 3410/7134 completed (loss: 0.033169377595186234, acc: 0.9917355179786682)
[2025-02-13 02:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08][root][INFO] - Training Epoch: 1/2, step 3411/7134 completed (loss: 0.0458405502140522, acc: 0.9870634078979492)
[2025-02-13 02:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08][root][INFO] - Training Epoch: 1/2, step 3412/7134 completed (loss: 0.04787495732307434, acc: 0.9867674708366394)
[2025-02-13 02:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09][root][INFO] - Training Epoch: 1/2, step 3413/7134 completed (loss: 0.03410528600215912, acc: 0.9900000095367432)
[2025-02-13 02:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09][root][INFO] - Training Epoch: 1/2, step 3414/7134 completed (loss: 0.04992212355136871, acc: 0.9866443872451782)
[2025-02-13 02:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10][root][INFO] - Training Epoch: 1/2, step 3415/7134 completed (loss: 0.02315220609307289, acc: 0.9919871687889099)
[2025-02-13 02:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10][root][INFO] - Training Epoch: 1/2, step 3416/7134 completed (loss: 0.04796604439616203, acc: 0.9867947101593018)
[2025-02-13 02:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:11][root][INFO] - Training Epoch: 1/2, step 3417/7134 completed (loss: 0.02381463535130024, acc: 0.9932523369789124)
[2025-02-13 02:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:11][root][INFO] - Training Epoch: 1/2, step 3418/7134 completed (loss: 0.03069879673421383, acc: 0.9911634922027588)
[2025-02-13 02:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:11][root][INFO] - Training Epoch: 1/2, step 3419/7134 completed (loss: 0.022212866693735123, acc: 0.9927954077720642)
[2025-02-13 02:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12][root][INFO] - Training Epoch: 1/2, step 3420/7134 completed (loss: 0.014606153592467308, acc: 0.9968404173851013)
[2025-02-13 02:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12][root][INFO] - Training Epoch: 1/2, step 3421/7134 completed (loss: 0.022051382809877396, acc: 0.996889591217041)
[2025-02-13 02:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:13][root][INFO] - Training Epoch: 1/2, step 3422/7134 completed (loss: 0.010883430019021034, acc: 0.996497392654419)
[2025-02-13 02:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:13][root][INFO] - Training Epoch: 1/2, step 3423/7134 completed (loss: 0.016668757423758507, acc: 0.9954545497894287)
[2025-02-13 02:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14][root][INFO] - Training Epoch: 1/2, step 3424/7134 completed (loss: 0.07368448376655579, acc: 0.9774096608161926)
[2025-02-13 02:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14][root][INFO] - Training Epoch: 1/2, step 3425/7134 completed (loss: 0.034895673394203186, acc: 0.9897435903549194)
[2025-02-13 02:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14][root][INFO] - Training Epoch: 1/2, step 3426/7134 completed (loss: 0.03950938954949379, acc: 0.9972413778305054)
[2025-02-13 02:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:15][root][INFO] - Training Epoch: 1/2, step 3427/7134 completed (loss: 0.033685747534036636, acc: 0.9936808943748474)
[2025-02-13 02:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:15][root][INFO] - Training Epoch: 1/2, step 3428/7134 completed (loss: 0.02658374048769474, acc: 0.9928366541862488)
[2025-02-13 02:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16][root][INFO] - Training Epoch: 1/2, step 3429/7134 completed (loss: 0.05417661368846893, acc: 0.9854369163513184)
[2025-02-13 02:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16][root][INFO] - Training Epoch: 1/2, step 3430/7134 completed (loss: 0.06741508841514587, acc: 0.987034022808075)
[2025-02-13 02:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17][root][INFO] - Training Epoch: 1/2, step 3431/7134 completed (loss: 0.040688659995794296, acc: 0.9915110468864441)
[2025-02-13 02:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17][root][INFO] - Training Epoch: 1/2, step 3432/7134 completed (loss: 0.04354522004723549, acc: 0.9903978109359741)
[2025-02-13 02:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17][root][INFO] - Training Epoch: 1/2, step 3433/7134 completed (loss: 0.07656286656856537, acc: 0.9787581562995911)
[2025-02-13 02:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:18][root][INFO] - Training Epoch: 1/2, step 3434/7134 completed (loss: 0.03032526932656765, acc: 0.9888712167739868)
[2025-02-13 02:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:18][root][INFO] - Training Epoch: 1/2, step 3435/7134 completed (loss: 0.05262433737516403, acc: 0.9839228391647339)
[2025-02-13 02:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19][root][INFO] - Training Epoch: 1/2, step 3436/7134 completed (loss: 0.037390708923339844, acc: 0.9837398529052734)
[2025-02-13 02:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19][root][INFO] - Training Epoch: 1/2, step 3437/7134 completed (loss: 0.01971430890262127, acc: 0.99609375)
[2025-02-13 02:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:20][root][INFO] - Training Epoch: 1/2, step 3438/7134 completed (loss: 0.04778125882148743, acc: 0.9878787994384766)
[2025-02-13 02:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:20][root][INFO] - Training Epoch: 1/2, step 3439/7134 completed (loss: 0.038555677980184555, acc: 0.9909583926200867)
[2025-02-13 02:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:20][root][INFO] - Training Epoch: 1/2, step 3440/7134 completed (loss: 0.07798969000577927, acc: 0.9650205969810486)
[2025-02-13 02:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21][root][INFO] - Training Epoch: 1/2, step 3441/7134 completed (loss: 0.06059699133038521, acc: 0.9855491518974304)
[2025-02-13 02:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21][root][INFO] - Training Epoch: 1/2, step 3442/7134 completed (loss: 0.01789306476712227, acc: 0.9963503479957581)
[2025-02-13 02:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22][root][INFO] - Training Epoch: 1/2, step 3443/7134 completed (loss: 0.06273134797811508, acc: 0.983146071434021)
[2025-02-13 02:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22][root][INFO] - Training Epoch: 1/2, step 3444/7134 completed (loss: 0.07871554046869278, acc: 0.9889240264892578)
[2025-02-13 02:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22][root][INFO] - Training Epoch: 1/2, step 3445/7134 completed (loss: 0.1162342056632042, acc: 0.9677419066429138)
[2025-02-13 02:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23][root][INFO] - Training Epoch: 1/2, step 3446/7134 completed (loss: 0.10065289586782455, acc: 0.9765886068344116)
[2025-02-13 02:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23][root][INFO] - Training Epoch: 1/2, step 3447/7134 completed (loss: 0.07819943130016327, acc: 0.9771987199783325)
[2025-02-13 02:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24][root][INFO] - Training Epoch: 1/2, step 3448/7134 completed (loss: 0.024593645706772804, acc: 0.9924924969673157)
[2025-02-13 02:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24][root][INFO] - Training Epoch: 1/2, step 3449/7134 completed (loss: 0.05297175794839859, acc: 0.9897959232330322)
[2025-02-13 02:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25][root][INFO] - Training Epoch: 1/2, step 3450/7134 completed (loss: 0.040402814745903015, acc: 0.9898734092712402)
[2025-02-13 02:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25][root][INFO] - Training Epoch: 1/2, step 3451/7134 completed (loss: 0.03966931998729706, acc: 0.9893993139266968)
[2025-02-13 02:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25][root][INFO] - Training Epoch: 1/2, step 3452/7134 completed (loss: 0.05080356076359749, acc: 0.9848155975341797)
[2025-02-13 02:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26][root][INFO] - Training Epoch: 1/2, step 3453/7134 completed (loss: 0.06036187708377838, acc: 0.9881154298782349)
[2025-02-13 02:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26][root][INFO] - Training Epoch: 1/2, step 3454/7134 completed (loss: 0.060163792222738266, acc: 0.9887164831161499)
[2025-02-13 02:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27][root][INFO] - Training Epoch: 1/2, step 3455/7134 completed (loss: 0.01248874980956316, acc: 0.9971014261245728)
[2025-02-13 02:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27][root][INFO] - Training Epoch: 1/2, step 3456/7134 completed (loss: 0.032377470284700394, acc: 0.9929412007331848)
[2025-02-13 02:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28][root][INFO] - Training Epoch: 1/2, step 3457/7134 completed (loss: 0.018641306087374687, acc: 0.993220329284668)
[2025-02-13 02:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28][root][INFO] - Training Epoch: 1/2, step 3458/7134 completed (loss: 0.0641133114695549, acc: 0.9837618470191956)
[2025-02-13 02:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28][root][INFO] - Training Epoch: 1/2, step 3459/7134 completed (loss: 0.032352205365896225, acc: 0.9923896789550781)
[2025-02-13 02:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29][root][INFO] - Training Epoch: 1/2, step 3460/7134 completed (loss: 0.16798867285251617, acc: 0.9583333134651184)
[2025-02-13 02:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29][root][INFO] - Training Epoch: 1/2, step 3461/7134 completed (loss: 0.22878167033195496, acc: 0.9448275566101074)
[2025-02-13 02:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30][root][INFO] - Training Epoch: 1/2, step 3462/7134 completed (loss: 0.09513673186302185, acc: 0.9780219793319702)
[2025-02-13 02:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30][root][INFO] - Training Epoch: 1/2, step 3463/7134 completed (loss: 0.01651216484606266, acc: 0.993966817855835)
[2025-02-13 02:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30][root][INFO] - Training Epoch: 1/2, step 3464/7134 completed (loss: 0.045821692794561386, acc: 0.98525470495224)
[2025-02-13 02:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:31][root][INFO] - Training Epoch: 1/2, step 3465/7134 completed (loss: 0.08381165564060211, acc: 0.9775967597961426)
[2025-02-13 02:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:31][root][INFO] - Training Epoch: 1/2, step 3466/7134 completed (loss: 0.024768376722931862, acc: 0.9904912710189819)
[2025-02-13 02:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32][root][INFO] - Training Epoch: 1/2, step 3467/7134 completed (loss: 0.05316007882356644, acc: 0.9830148816108704)
[2025-02-13 02:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32][root][INFO] - Training Epoch: 1/2, step 3468/7134 completed (loss: 0.010635302402079105, acc: 0.9985527992248535)
[2025-02-13 02:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32][root][INFO] - Training Epoch: 1/2, step 3469/7134 completed (loss: 0.04572344198822975, acc: 0.9867924451828003)
[2025-02-13 02:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33][root][INFO] - Training Epoch: 1/2, step 3470/7134 completed (loss: 0.015501230023801327, acc: 0.9953917264938354)
[2025-02-13 02:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33][root][INFO] - Training Epoch: 1/2, step 3471/7134 completed (loss: 0.07342123985290527, acc: 0.9834254384040833)
[2025-02-13 02:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34][root][INFO] - Training Epoch: 1/2, step 3472/7134 completed (loss: 0.020437803119421005, acc: 0.995529055595398)
[2025-02-13 02:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34][root][INFO] - Training Epoch: 1/2, step 3473/7134 completed (loss: 0.036284755915403366, acc: 0.9900662302970886)
[2025-02-13 02:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:35][root][INFO] - Training Epoch: 1/2, step 3474/7134 completed (loss: 0.0704847052693367, acc: 0.982332170009613)
[2025-02-13 02:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:35][root][INFO] - Training Epoch: 1/2, step 3475/7134 completed (loss: 0.04966627433896065, acc: 0.9911190271377563)
[2025-02-13 02:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:35][root][INFO] - Training Epoch: 1/2, step 3476/7134 completed (loss: 0.08782199025154114, acc: 0.9743150472640991)
[2025-02-13 02:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36][root][INFO] - Training Epoch: 1/2, step 3477/7134 completed (loss: 0.05257606506347656, acc: 0.9901315569877625)
[2025-02-13 02:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36][root][INFO] - Training Epoch: 1/2, step 3478/7134 completed (loss: 0.025178758427500725, acc: 0.9970059990882874)
[2025-02-13 02:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:37][root][INFO] - Training Epoch: 1/2, step 3479/7134 completed (loss: 0.004026550799608231, acc: 1.0)
[2025-02-13 02:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:37][root][INFO] - Training Epoch: 1/2, step 3480/7134 completed (loss: 0.0301353856921196, acc: 0.9945504069328308)
[2025-02-13 02:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38][root][INFO] - Training Epoch: 1/2, step 3481/7134 completed (loss: 0.0257156603038311, acc: 0.9929078221321106)
[2025-02-13 02:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38][root][INFO] - Training Epoch: 1/2, step 3482/7134 completed (loss: 0.02767803706228733, acc: 0.9910072088241577)
[2025-02-13 02:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38][root][INFO] - Training Epoch: 1/2, step 3483/7134 completed (loss: 0.010760647244751453, acc: 0.9984591603279114)
[2025-02-13 02:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:39][root][INFO] - Training Epoch: 1/2, step 3484/7134 completed (loss: 0.022615518420934677, acc: 0.9941002726554871)
[2025-02-13 02:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:39][root][INFO] - Training Epoch: 1/2, step 3485/7134 completed (loss: 0.026097290217876434, acc: 0.9942280054092407)
[2025-02-13 02:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40][root][INFO] - Training Epoch: 1/2, step 3486/7134 completed (loss: 0.020775340497493744, acc: 0.9928057789802551)
[2025-02-13 02:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40][root][INFO] - Training Epoch: 1/2, step 3487/7134 completed (loss: 0.0481315478682518, acc: 0.9837996959686279)
[2025-02-13 02:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40][root][INFO] - Training Epoch: 1/2, step 3488/7134 completed (loss: 0.0299548227339983, acc: 0.9934210777282715)
[2025-02-13 02:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41][root][INFO] - Training Epoch: 1/2, step 3489/7134 completed (loss: 0.06364146620035172, acc: 0.9815837740898132)
[2025-02-13 02:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41][root][INFO] - Training Epoch: 1/2, step 3490/7134 completed (loss: 0.06929370015859604, acc: 0.9867060780525208)
[2025-02-13 02:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42][root][INFO] - Training Epoch: 1/2, step 3491/7134 completed (loss: 0.05194205418229103, acc: 0.9843971729278564)
[2025-02-13 02:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42][root][INFO] - Training Epoch: 1/2, step 3492/7134 completed (loss: 0.03975295647978783, acc: 0.9897330403327942)
[2025-02-13 02:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:43][root][INFO] - Training Epoch: 1/2, step 3493/7134 completed (loss: 0.01640217937529087, acc: 0.994397759437561)
[2025-02-13 02:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:43][root][INFO] - Training Epoch: 1/2, step 3494/7134 completed (loss: 0.03537944704294205, acc: 0.9913793206214905)
[2025-02-13 02:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:43][root][INFO] - Training Epoch: 1/2, step 3495/7134 completed (loss: 0.015579079277813435, acc: 0.9952038526535034)
[2025-02-13 02:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:44][root][INFO] - Training Epoch: 1/2, step 3496/7134 completed (loss: 0.03224506601691246, acc: 0.9864864945411682)
[2025-02-13 02:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:44][root][INFO] - Training Epoch: 1/2, step 3497/7134 completed (loss: 0.04844428971409798, acc: 0.9866468906402588)
[2025-02-13 02:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:45][root][INFO] - Training Epoch: 1/2, step 3498/7134 completed (loss: 0.03746844083070755, acc: 0.9878048896789551)
[2025-02-13 02:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:45][root][INFO] - Training Epoch: 1/2, step 3499/7134 completed (loss: 0.01897222362458706, acc: 0.9983792304992676)
[2025-02-13 02:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:45][root][INFO] - Training Epoch: 1/2, step 3500/7134 completed (loss: 0.03221793472766876, acc: 0.9911894202232361)
[2025-02-13 02:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:46][root][INFO] - Training Epoch: 1/2, step 3501/7134 completed (loss: 0.09700966626405716, acc: 0.9758551120758057)
[2025-02-13 02:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:46][root][INFO] - Training Epoch: 1/2, step 3502/7134 completed (loss: 0.08096159249544144, acc: 0.9789473414421082)
[2025-02-13 02:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47][root][INFO] - Training Epoch: 1/2, step 3503/7134 completed (loss: 0.061958979815244675, acc: 0.9809358716011047)
[2025-02-13 02:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47][root][INFO] - Training Epoch: 1/2, step 3504/7134 completed (loss: 0.013022573664784431, acc: 0.9969834089279175)
[2025-02-13 02:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47][root][INFO] - Training Epoch: 1/2, step 3505/7134 completed (loss: 0.026156354695558548, acc: 0.991055428981781)
[2025-02-13 02:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48][root][INFO] - Training Epoch: 1/2, step 3506/7134 completed (loss: 0.029367856681346893, acc: 0.9943289160728455)
[2025-02-13 02:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48][root][INFO] - Training Epoch: 1/2, step 3507/7134 completed (loss: 0.011272670701146126, acc: 0.9950330853462219)
[2025-02-13 02:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49][root][INFO] - Training Epoch: 1/2, step 3508/7134 completed (loss: 0.03471765294671059, acc: 0.9941520690917969)
[2025-02-13 02:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49][root][INFO] - Training Epoch: 1/2, step 3509/7134 completed (loss: 0.023075906559824944, acc: 0.9930939078330994)
[2025-02-13 02:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50][root][INFO] - Training Epoch: 1/2, step 3510/7134 completed (loss: 0.032713308930397034, acc: 0.9931389093399048)
[2025-02-13 02:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50][root][INFO] - Training Epoch: 1/2, step 3511/7134 completed (loss: 0.009249315597116947, acc: 0.9963964223861694)
[2025-02-13 02:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50][root][INFO] - Training Epoch: 1/2, step 3512/7134 completed (loss: 0.043447695672512054, acc: 0.9857369065284729)
[2025-02-13 02:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51][root][INFO] - Training Epoch: 1/2, step 3513/7134 completed (loss: 0.0344407893717289, acc: 0.9929203391075134)
[2025-02-13 02:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51][root][INFO] - Training Epoch: 1/2, step 3514/7134 completed (loss: 0.033558305352926254, acc: 0.9896313548088074)
[2025-02-13 02:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52][root][INFO] - Training Epoch: 1/2, step 3515/7134 completed (loss: 0.01730775646865368, acc: 0.99301677942276)
[2025-02-13 02:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52][root][INFO] - Training Epoch: 1/2, step 3516/7134 completed (loss: 0.01311948336660862, acc: 0.9956663250923157)
[2025-02-13 02:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53][root][INFO] - Training Epoch: 1/2, step 3517/7134 completed (loss: 0.018828295171260834, acc: 0.9928486347198486)
[2025-02-13 02:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53][root][INFO] - Training Epoch: 1/2, step 3518/7134 completed (loss: 0.020442085340619087, acc: 0.9926062822341919)
[2025-02-13 02:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53][root][INFO] - Training Epoch: 1/2, step 3519/7134 completed (loss: 0.020227236673235893, acc: 0.9943820238113403)
[2025-02-13 02:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54][root][INFO] - Training Epoch: 1/2, step 3520/7134 completed (loss: 0.05861828848719597, acc: 0.9870588183403015)
[2025-02-13 02:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54][root][INFO] - Training Epoch: 1/2, step 3521/7134 completed (loss: 0.016447270289063454, acc: 0.9957325458526611)
[2025-02-13 02:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55][root][INFO] - Training Epoch: 1/2, step 3522/7134 completed (loss: 0.02396983653306961, acc: 0.9946523904800415)
[2025-02-13 02:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55][root][INFO] - Training Epoch: 1/2, step 3523/7134 completed (loss: 0.00691445404663682, acc: 0.9969183206558228)
[2025-02-13 02:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56][root][INFO] - Training Epoch: 1/2, step 3524/7134 completed (loss: 0.03430094197392464, acc: 0.9895833134651184)
[2025-02-13 02:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56][root][INFO] - Training Epoch: 1/2, step 3525/7134 completed (loss: 0.017756083980202675, acc: 0.9917627573013306)
[2025-02-13 02:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57][root][INFO] - Training Epoch: 1/2, step 3526/7134 completed (loss: 0.03564433380961418, acc: 0.9898605942726135)
[2025-02-13 02:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57][root][INFO] - Training Epoch: 1/2, step 3527/7134 completed (loss: 0.03725830838084221, acc: 0.991037130355835)
[2025-02-13 02:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57][root][INFO] - Training Epoch: 1/2, step 3528/7134 completed (loss: 0.011930478736758232, acc: 0.9942330121994019)
[2025-02-13 02:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58][root][INFO] - Training Epoch: 1/2, step 3529/7134 completed (loss: 0.00855882465839386, acc: 0.9976470470428467)
[2025-02-13 02:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58][root][INFO] - Training Epoch: 1/2, step 3530/7134 completed (loss: 0.022097323089838028, acc: 0.9919725060462952)
[2025-02-13 02:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59][root][INFO] - Training Epoch: 1/2, step 3531/7134 completed (loss: 0.012183141894638538, acc: 0.9984939694404602)
[2025-02-13 02:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59][root][INFO] - Training Epoch: 1/2, step 3532/7134 completed (loss: 0.009562375955283642, acc: 0.9983165264129639)
[2025-02-13 02:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00][root][INFO] - Training Epoch: 1/2, step 3533/7134 completed (loss: 0.011183778755366802, acc: 0.9976717233657837)
[2025-02-13 02:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00][root][INFO] - Training Epoch: 1/2, step 3534/7134 completed (loss: 0.03547182306647301, acc: 0.9888734221458435)
[2025-02-13 02:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01][root][INFO] - Training Epoch: 1/2, step 3535/7134 completed (loss: 0.03305789828300476, acc: 0.9938555955886841)
[2025-02-13 02:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01][root][INFO] - Training Epoch: 1/2, step 3536/7134 completed (loss: 0.051315102726221085, acc: 0.9890710115432739)
[2025-02-13 02:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01][root][INFO] - Training Epoch: 1/2, step 3537/7134 completed (loss: 0.03659496456384659, acc: 0.9899328947067261)
[2025-02-13 02:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02][root][INFO] - Training Epoch: 1/2, step 3538/7134 completed (loss: 0.04822385311126709, acc: 0.9923954606056213)
[2025-02-13 02:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02][root][INFO] - Training Epoch: 1/2, step 3539/7134 completed (loss: 0.015474018640816212, acc: 0.9975728392601013)
[2025-02-13 02:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03][root][INFO] - Training Epoch: 1/2, step 3540/7134 completed (loss: 0.04510556161403656, acc: 0.9863429665565491)
[2025-02-13 02:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03][root][INFO] - Training Epoch: 1/2, step 3541/7134 completed (loss: 0.03983406350016594, acc: 0.98531574010849)
[2025-02-13 02:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03][root][INFO] - Training Epoch: 1/2, step 3542/7134 completed (loss: 0.02956918254494667, acc: 0.9876922965049744)
[2025-02-13 02:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04][root][INFO] - Training Epoch: 1/2, step 3543/7134 completed (loss: 0.04161982610821724, acc: 0.9806337952613831)
[2025-02-13 02:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04][root][INFO] - Training Epoch: 1/2, step 3544/7134 completed (loss: 0.0312565378844738, acc: 0.9861538410186768)
[2025-02-13 02:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05][root][INFO] - Training Epoch: 1/2, step 3545/7134 completed (loss: 0.030624836683273315, acc: 0.9953415989875793)
[2025-02-13 02:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05][root][INFO] - Training Epoch: 1/2, step 3546/7134 completed (loss: 0.029544077813625336, acc: 0.9917491674423218)
[2025-02-13 02:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06][root][INFO] - Training Epoch: 1/2, step 3547/7134 completed (loss: 0.059119485318660736, acc: 0.9818181991577148)
[2025-02-13 02:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06][root][INFO] - Training Epoch: 1/2, step 3548/7134 completed (loss: 0.03303687646985054, acc: 0.9845132827758789)
[2025-02-13 02:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07][root][INFO] - Training Epoch: 1/2, step 3549/7134 completed (loss: 0.04717259481549263, acc: 0.9858906269073486)
[2025-02-13 02:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07][root][INFO] - Training Epoch: 1/2, step 3550/7134 completed (loss: 0.020250054076313972, acc: 0.9894921183586121)
[2025-02-13 02:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07][root][INFO] - Training Epoch: 1/2, step 3551/7134 completed (loss: 0.0323403999209404, acc: 0.9898989796638489)
[2025-02-13 02:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08][root][INFO] - Training Epoch: 1/2, step 3552/7134 completed (loss: 0.11394793540239334, acc: 0.966292142868042)
[2025-02-13 02:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08][root][INFO] - Training Epoch: 1/2, step 3553/7134 completed (loss: 0.0503813810646534, acc: 0.9884868264198303)
[2025-02-13 02:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09][root][INFO] - Training Epoch: 1/2, step 3554/7134 completed (loss: 0.05562354996800423, acc: 0.9894578456878662)
[2025-02-13 02:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09][root][INFO] - Training Epoch: 1/2, step 3555/7134 completed (loss: 0.04578171297907829, acc: 0.9858712553977966)
[2025-02-13 02:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10][root][INFO] - Training Epoch: 1/2, step 3556/7134 completed (loss: 0.04769815504550934, acc: 0.9904030561447144)
[2025-02-13 02:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10][root][INFO] - Training Epoch: 1/2, step 3557/7134 completed (loss: 0.0418851263821125, acc: 0.9854809641838074)
[2025-02-13 02:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10][root][INFO] - Training Epoch: 1/2, step 3558/7134 completed (loss: 0.043659694492816925, acc: 0.985567033290863)
[2025-02-13 02:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11][root][INFO] - Training Epoch: 1/2, step 3559/7134 completed (loss: 0.017230868339538574, acc: 0.9912790656089783)
[2025-02-13 02:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11][root][INFO] - Training Epoch: 1/2, step 3560/7134 completed (loss: 0.018316449597477913, acc: 0.9945205450057983)
[2025-02-13 02:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12][root][INFO] - Training Epoch: 1/2, step 3561/7134 completed (loss: 0.02929665520787239, acc: 0.994397759437561)
[2025-02-13 02:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12][root][INFO] - Training Epoch: 1/2, step 3562/7134 completed (loss: 0.04082716628909111, acc: 0.9839285612106323)
[2025-02-13 02:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13][root][INFO] - Training Epoch: 1/2, step 3563/7134 completed (loss: 0.019933877512812614, acc: 0.996820330619812)
[2025-02-13 02:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13][root][INFO] - Training Epoch: 1/2, step 3564/7134 completed (loss: 0.06482484191656113, acc: 0.9887217879295349)
[2025-02-13 02:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13][root][INFO] - Training Epoch: 1/2, step 3565/7134 completed (loss: 0.07684876024723053, acc: 0.9755638837814331)
[2025-02-13 02:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0506, device='cuda:0') eval_epoch_loss=tensor(0.0494, device='cuda:0') eval_epoch_acc=tensor(0.9862, device='cuda:0')
[2025-02-13 03:00:34][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:00:34][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:00:34][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_3566_loss_0.04939722642302513/model.pt
[2025-02-13 03:00:34][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:00:34][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.04939722642302513
[2025-02-13 03:00:34][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9862425923347473
[2025-02-13 03:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:35][root][INFO] - Training Epoch: 1/2, step 3566/7134 completed (loss: 0.04728216305375099, acc: 0.9820716977119446)
[2025-02-13 03:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:35][root][INFO] - Training Epoch: 1/2, step 3567/7134 completed (loss: 0.10533452033996582, acc: 0.9805951118469238)
[2025-02-13 03:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36][root][INFO] - Training Epoch: 1/2, step 3568/7134 completed (loss: 0.08063220232725143, acc: 0.9803600907325745)
[2025-02-13 03:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36][root][INFO] - Training Epoch: 1/2, step 3569/7134 completed (loss: 0.06072946637868881, acc: 0.9874551892280579)
[2025-02-13 03:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:37][root][INFO] - Training Epoch: 1/2, step 3570/7134 completed (loss: 0.09340038150548935, acc: 0.9768595099449158)
[2025-02-13 03:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:37][root][INFO] - Training Epoch: 1/2, step 3571/7134 completed (loss: 0.0733628049492836, acc: 0.9893898963928223)
[2025-02-13 03:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:37][root][INFO] - Training Epoch: 1/2, step 3572/7134 completed (loss: 0.025304671376943588, acc: 0.992094874382019)
[2025-02-13 03:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:38][root][INFO] - Training Epoch: 1/2, step 3573/7134 completed (loss: 0.05691590905189514, acc: 0.9851852059364319)
[2025-02-13 03:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:38][root][INFO] - Training Epoch: 1/2, step 3574/7134 completed (loss: 0.05387178435921669, acc: 0.9865900278091431)
[2025-02-13 03:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39][root][INFO] - Training Epoch: 1/2, step 3575/7134 completed (loss: 0.1043410450220108, acc: 0.9725557565689087)
[2025-02-13 03:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39][root][INFO] - Training Epoch: 1/2, step 3576/7134 completed (loss: 0.0673908069729805, acc: 0.9760589599609375)
[2025-02-13 03:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:40][root][INFO] - Training Epoch: 1/2, step 3577/7134 completed (loss: 0.03760155290365219, acc: 0.984829306602478)
[2025-02-13 03:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:40][root][INFO] - Training Epoch: 1/2, step 3578/7134 completed (loss: 0.032680951058864594, acc: 0.9868995547294617)
[2025-02-13 03:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:41][root][INFO] - Training Epoch: 1/2, step 3579/7134 completed (loss: 0.055759500712156296, acc: 0.9866666793823242)
[2025-02-13 03:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:41][root][INFO] - Training Epoch: 1/2, step 3580/7134 completed (loss: 0.0685587003827095, acc: 0.9732283353805542)
[2025-02-13 03:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:41][root][INFO] - Training Epoch: 1/2, step 3581/7134 completed (loss: 0.09086576849222183, acc: 0.9764359593391418)
[2025-02-13 03:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42][root][INFO] - Training Epoch: 1/2, step 3582/7134 completed (loss: 0.06662171334028244, acc: 0.9810426831245422)
[2025-02-13 03:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42][root][INFO] - Training Epoch: 1/2, step 3583/7134 completed (loss: 0.05605866760015488, acc: 0.9817629456520081)
[2025-02-13 03:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:43][root][INFO] - Training Epoch: 1/2, step 3584/7134 completed (loss: 0.08169747143983841, acc: 0.9841269850730896)
[2025-02-13 03:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:43][root][INFO] - Training Epoch: 1/2, step 3585/7134 completed (loss: 0.07843516021966934, acc: 0.9739583134651184)
[2025-02-13 03:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44][root][INFO] - Training Epoch: 1/2, step 3586/7134 completed (loss: 0.05597776919603348, acc: 0.9791332483291626)
[2025-02-13 03:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44][root][INFO] - Training Epoch: 1/2, step 3587/7134 completed (loss: 0.05147605761885643, acc: 0.989847719669342)
[2025-02-13 03:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44][root][INFO] - Training Epoch: 1/2, step 3588/7134 completed (loss: 0.0862233117222786, acc: 0.9792899489402771)
[2025-02-13 03:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45][root][INFO] - Training Epoch: 1/2, step 3589/7134 completed (loss: 0.07510026544332504, acc: 0.9909228682518005)
[2025-02-13 03:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45][root][INFO] - Training Epoch: 1/2, step 3590/7134 completed (loss: 0.025261646136641502, acc: 0.9915730357170105)
[2025-02-13 03:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:46][root][INFO] - Training Epoch: 1/2, step 3591/7134 completed (loss: 0.033555179834365845, acc: 0.9894894957542419)
[2025-02-13 03:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:46][root][INFO] - Training Epoch: 1/2, step 3592/7134 completed (loss: 0.032126303762197495, acc: 0.9917355179786682)
[2025-02-13 03:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47][root][INFO] - Training Epoch: 1/2, step 3593/7134 completed (loss: 0.06757589429616928, acc: 0.9842382073402405)
[2025-02-13 03:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47][root][INFO] - Training Epoch: 1/2, step 3594/7134 completed (loss: 0.03397202119231224, acc: 0.9886363744735718)
[2025-02-13 03:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48][root][INFO] - Training Epoch: 1/2, step 3595/7134 completed (loss: 0.04092022404074669, acc: 0.9892802238464355)
[2025-02-13 03:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48][root][INFO] - Training Epoch: 1/2, step 3596/7134 completed (loss: 0.03893684968352318, acc: 0.9917469024658203)
[2025-02-13 03:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48][root][INFO] - Training Epoch: 1/2, step 3597/7134 completed (loss: 0.03904648497700691, acc: 0.9884892106056213)
[2025-02-13 03:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:49][root][INFO] - Training Epoch: 1/2, step 3598/7134 completed (loss: 0.06765671819448471, acc: 0.9816933870315552)
[2025-02-13 03:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:49][root][INFO] - Training Epoch: 1/2, step 3599/7134 completed (loss: 0.08274517208337784, acc: 0.9773013591766357)
[2025-02-13 03:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50][root][INFO] - Training Epoch: 1/2, step 3600/7134 completed (loss: 0.04649731144309044, acc: 0.9865319728851318)
[2025-02-13 03:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50][root][INFO] - Training Epoch: 1/2, step 3601/7134 completed (loss: 0.1197446659207344, acc: 0.9696551561355591)
[2025-02-13 03:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:51][root][INFO] - Training Epoch: 1/2, step 3602/7134 completed (loss: 0.09561599791049957, acc: 0.9708284735679626)
[2025-02-13 03:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:51][root][INFO] - Training Epoch: 1/2, step 3603/7134 completed (loss: 0.032288558781147, acc: 0.989062488079071)
[2025-02-13 03:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52][root][INFO] - Training Epoch: 1/2, step 3604/7134 completed (loss: 0.039502888917922974, acc: 0.9849905967712402)
[2025-02-13 03:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52][root][INFO] - Training Epoch: 1/2, step 3605/7134 completed (loss: 0.06957545876502991, acc: 0.9855072498321533)
[2025-02-13 03:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:53][root][INFO] - Training Epoch: 1/2, step 3606/7134 completed (loss: 0.08239138126373291, acc: 0.9822379946708679)
[2025-02-13 03:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:53][root][INFO] - Training Epoch: 1/2, step 3607/7134 completed (loss: 0.04190124571323395, acc: 0.9865771532058716)
[2025-02-13 03:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54][root][INFO] - Training Epoch: 1/2, step 3608/7134 completed (loss: 0.09204310178756714, acc: 0.980088472366333)
[2025-02-13 03:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54][root][INFO] - Training Epoch: 1/2, step 3609/7134 completed (loss: 0.052723728120326996, acc: 0.9873096346855164)
[2025-02-13 03:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54][root][INFO] - Training Epoch: 1/2, step 3610/7134 completed (loss: 0.08024445921182632, acc: 0.9753246903419495)
[2025-02-13 03:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:55][root][INFO] - Training Epoch: 1/2, step 3611/7134 completed (loss: 0.059423159807920456, acc: 0.9838709831237793)
[2025-02-13 03:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:55][root][INFO] - Training Epoch: 1/2, step 3612/7134 completed (loss: 0.10293886065483093, acc: 0.9719917178153992)
[2025-02-13 03:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56][root][INFO] - Training Epoch: 1/2, step 3613/7134 completed (loss: 0.055268388241529465, acc: 0.9821882843971252)
[2025-02-13 03:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56][root][INFO] - Training Epoch: 1/2, step 3614/7134 completed (loss: 0.04558435454964638, acc: 0.9852744340896606)
[2025-02-13 03:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:57][root][INFO] - Training Epoch: 1/2, step 3615/7134 completed (loss: 0.02980152890086174, acc: 0.9954128265380859)
[2025-02-13 03:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:57][root][INFO] - Training Epoch: 1/2, step 3616/7134 completed (loss: 0.04333147406578064, acc: 0.9843013882637024)
[2025-02-13 03:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:58][root][INFO] - Training Epoch: 1/2, step 3617/7134 completed (loss: 0.060621123760938644, acc: 0.9814356565475464)
[2025-02-13 03:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:58][root][INFO] - Training Epoch: 1/2, step 3618/7134 completed (loss: 0.11850833147764206, acc: 0.9656862616539001)
[2025-02-13 03:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59][root][INFO] - Training Epoch: 1/2, step 3619/7134 completed (loss: 0.09569902718067169, acc: 0.9714285731315613)
[2025-02-13 03:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59][root][INFO] - Training Epoch: 1/2, step 3620/7134 completed (loss: 0.051525212824344635, acc: 0.9859353303909302)
[2025-02-13 03:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00][root][INFO] - Training Epoch: 1/2, step 3621/7134 completed (loss: 0.03183579072356224, acc: 0.9875173568725586)
[2025-02-13 03:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00][root][INFO] - Training Epoch: 1/2, step 3622/7134 completed (loss: 0.03836902976036072, acc: 0.9894894957542419)
[2025-02-13 03:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00][root][INFO] - Training Epoch: 1/2, step 3623/7134 completed (loss: 0.0212459247559309, acc: 0.9962825179100037)
[2025-02-13 03:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01][root][INFO] - Training Epoch: 1/2, step 3624/7134 completed (loss: 0.036737751215696335, acc: 0.9933333396911621)
[2025-02-13 03:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01][root][INFO] - Training Epoch: 1/2, step 3625/7134 completed (loss: 0.06552582234144211, acc: 0.9818913340568542)
[2025-02-13 03:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02][root][INFO] - Training Epoch: 1/2, step 3626/7134 completed (loss: 0.26390159130096436, acc: 0.9359999895095825)
[2025-02-13 03:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02][root][INFO] - Training Epoch: 1/2, step 3627/7134 completed (loss: 0.10644049942493439, acc: 0.9739952683448792)
[2025-02-13 03:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02][root][INFO] - Training Epoch: 1/2, step 3628/7134 completed (loss: 0.1530556082725525, acc: 0.9659090638160706)
[2025-02-13 03:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03][root][INFO] - Training Epoch: 1/2, step 3629/7134 completed (loss: 0.11186229437589645, acc: 0.9695122241973877)
[2025-02-13 03:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03][root][INFO] - Training Epoch: 1/2, step 3630/7134 completed (loss: 0.08307746052742004, acc: 0.9715762138366699)
[2025-02-13 03:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:04][root][INFO] - Training Epoch: 1/2, step 3631/7134 completed (loss: 0.06632702052593231, acc: 0.9754098653793335)
[2025-02-13 03:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:04][root][INFO] - Training Epoch: 1/2, step 3632/7134 completed (loss: 0.05753081664443016, acc: 0.9839034080505371)
[2025-02-13 03:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05][root][INFO] - Training Epoch: 1/2, step 3633/7134 completed (loss: 0.08686494082212448, acc: 0.9813486337661743)
[2025-02-13 03:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05][root][INFO] - Training Epoch: 1/2, step 3634/7134 completed (loss: 0.08975155651569366, acc: 0.9838056564331055)
[2025-02-13 03:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05][root][INFO] - Training Epoch: 1/2, step 3635/7134 completed (loss: 0.021527618169784546, acc: 0.9946236610412598)
[2025-02-13 03:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:06][root][INFO] - Training Epoch: 1/2, step 3636/7134 completed (loss: 0.036661650985479355, acc: 0.9921259880065918)
[2025-02-13 03:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:06][root][INFO] - Training Epoch: 1/2, step 3637/7134 completed (loss: 0.03923477604985237, acc: 0.9959758520126343)
[2025-02-13 03:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07][root][INFO] - Training Epoch: 1/2, step 3638/7134 completed (loss: 0.029697176069021225, acc: 0.9882352948188782)
[2025-02-13 03:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07][root][INFO] - Training Epoch: 1/2, step 3639/7134 completed (loss: 0.02256704680621624, acc: 0.9932432174682617)
[2025-02-13 03:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:08][root][INFO] - Training Epoch: 1/2, step 3640/7134 completed (loss: 0.054042600095272064, acc: 0.982824444770813)
[2025-02-13 03:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:08][root][INFO] - Training Epoch: 1/2, step 3641/7134 completed (loss: 0.048191845417022705, acc: 0.9863813519477844)
[2025-02-13 03:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:08][root][INFO] - Training Epoch: 1/2, step 3642/7134 completed (loss: 0.03106522373855114, acc: 0.9876352548599243)
[2025-02-13 03:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09][root][INFO] - Training Epoch: 1/2, step 3643/7134 completed (loss: 0.03205801174044609, acc: 0.9879724979400635)
[2025-02-13 03:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09][root][INFO] - Training Epoch: 1/2, step 3644/7134 completed (loss: 0.06641580164432526, acc: 0.9804878234863281)
[2025-02-13 03:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:10][root][INFO] - Training Epoch: 1/2, step 3645/7134 completed (loss: 0.05698510259389877, acc: 0.98591548204422)
[2025-02-13 03:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:10][root][INFO] - Training Epoch: 1/2, step 3646/7134 completed (loss: 0.11661353707313538, acc: 0.9722222089767456)
[2025-02-13 03:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:11][root][INFO] - Training Epoch: 1/2, step 3647/7134 completed (loss: 0.019292956218123436, acc: 0.9948320388793945)
[2025-02-13 03:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:11][root][INFO] - Training Epoch: 1/2, step 3648/7134 completed (loss: 0.06992883235216141, acc: 0.9780521392822266)
[2025-02-13 03:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:11][root][INFO] - Training Epoch: 1/2, step 3649/7134 completed (loss: 0.0769282877445221, acc: 0.9789915680885315)
[2025-02-13 03:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12][root][INFO] - Training Epoch: 1/2, step 3650/7134 completed (loss: 0.05780312046408653, acc: 0.9803921580314636)
[2025-02-13 03:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12][root][INFO] - Training Epoch: 1/2, step 3651/7134 completed (loss: 0.07497302442789078, acc: 0.9865951538085938)
[2025-02-13 03:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:13][root][INFO] - Training Epoch: 1/2, step 3652/7134 completed (loss: 0.054564766585826874, acc: 0.9834087491035461)
[2025-02-13 03:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:13][root][INFO] - Training Epoch: 1/2, step 3653/7134 completed (loss: 0.09524604678153992, acc: 0.9788838624954224)
[2025-02-13 03:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14][root][INFO] - Training Epoch: 1/2, step 3654/7134 completed (loss: 0.03822430595755577, acc: 0.9879102110862732)
[2025-02-13 03:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14][root][INFO] - Training Epoch: 1/2, step 3655/7134 completed (loss: 0.05546264350414276, acc: 0.9862637519836426)
[2025-02-13 03:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14][root][INFO] - Training Epoch: 1/2, step 3656/7134 completed (loss: 0.09112465381622314, acc: 0.9800000190734863)
[2025-02-13 03:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:15][root][INFO] - Training Epoch: 1/2, step 3657/7134 completed (loss: 0.02120528556406498, acc: 0.995192289352417)
[2025-02-13 03:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:15][root][INFO] - Training Epoch: 1/2, step 3658/7134 completed (loss: 0.0521107092499733, acc: 0.9890965819358826)
[2025-02-13 03:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16][root][INFO] - Training Epoch: 1/2, step 3659/7134 completed (loss: 0.03944649174809456, acc: 0.9835575222969055)
[2025-02-13 03:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16][root][INFO] - Training Epoch: 1/2, step 3660/7134 completed (loss: 0.06137034296989441, acc: 0.9868852496147156)
[2025-02-13 03:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16][root][INFO] - Training Epoch: 1/2, step 3661/7134 completed (loss: 0.06817436963319778, acc: 0.9750000238418579)
[2025-02-13 03:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:17][root][INFO] - Training Epoch: 1/2, step 3662/7134 completed (loss: 0.0758131667971611, acc: 0.9832317233085632)
[2025-02-13 03:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:17][root][INFO] - Training Epoch: 1/2, step 3663/7134 completed (loss: 0.04255147650837898, acc: 0.9853479862213135)
[2025-02-13 03:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:18][root][INFO] - Training Epoch: 1/2, step 3664/7134 completed (loss: 0.06387144327163696, acc: 0.9863221645355225)
[2025-02-13 03:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:18][root][INFO] - Training Epoch: 1/2, step 3665/7134 completed (loss: 0.03761912137269974, acc: 0.9901477694511414)
[2025-02-13 03:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19][root][INFO] - Training Epoch: 1/2, step 3666/7134 completed (loss: 0.0548216812312603, acc: 0.9835766553878784)
[2025-02-13 03:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19][root][INFO] - Training Epoch: 1/2, step 3667/7134 completed (loss: 0.17705751955509186, acc: 0.9589977264404297)
[2025-02-13 03:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19][root][INFO] - Training Epoch: 1/2, step 3668/7134 completed (loss: 0.04159534350037575, acc: 0.9895287752151489)
[2025-02-13 03:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:20][root][INFO] - Training Epoch: 1/2, step 3669/7134 completed (loss: 0.019214870408177376, acc: 0.992438554763794)
[2025-02-13 03:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:20][root][INFO] - Training Epoch: 1/2, step 3670/7134 completed (loss: 0.0272518303245306, acc: 0.9958791136741638)
[2025-02-13 03:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21][root][INFO] - Training Epoch: 1/2, step 3671/7134 completed (loss: 0.02951154299080372, acc: 0.9913294911384583)
[2025-02-13 03:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21][root][INFO] - Training Epoch: 1/2, step 3672/7134 completed (loss: 0.024448147043585777, acc: 0.993686854839325)
[2025-02-13 03:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:22][root][INFO] - Training Epoch: 1/2, step 3673/7134 completed (loss: 0.0517456978559494, acc: 0.9849498271942139)
[2025-02-13 03:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:22][root][INFO] - Training Epoch: 1/2, step 3674/7134 completed (loss: 0.029117338359355927, acc: 0.9921259880065918)
[2025-02-13 03:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:22][root][INFO] - Training Epoch: 1/2, step 3675/7134 completed (loss: 0.018623962998390198, acc: 0.9961734414100647)
[2025-02-13 03:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23][root][INFO] - Training Epoch: 1/2, step 3676/7134 completed (loss: 0.022885823622345924, acc: 0.991253674030304)
[2025-02-13 03:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23][root][INFO] - Training Epoch: 1/2, step 3677/7134 completed (loss: 0.040848616510629654, acc: 0.9876712560653687)
[2025-02-13 03:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24][root][INFO] - Training Epoch: 1/2, step 3678/7134 completed (loss: 0.05019369348883629, acc: 0.9838056564331055)
[2025-02-13 03:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24][root][INFO] - Training Epoch: 1/2, step 3679/7134 completed (loss: 0.02490500919520855, acc: 0.9884297251701355)
[2025-02-13 03:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:25][root][INFO] - Training Epoch: 1/2, step 3680/7134 completed (loss: 0.05440915748476982, acc: 0.9837662577629089)
[2025-02-13 03:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:25][root][INFO] - Training Epoch: 1/2, step 3681/7134 completed (loss: 0.029058845713734627, acc: 0.9874301552772522)
[2025-02-13 03:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26][root][INFO] - Training Epoch: 1/2, step 3682/7134 completed (loss: 0.01968599297106266, acc: 0.9947299361228943)
[2025-02-13 03:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26][root][INFO] - Training Epoch: 1/2, step 3683/7134 completed (loss: 0.019804835319519043, acc: 0.995275616645813)
[2025-02-13 03:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26][root][INFO] - Training Epoch: 1/2, step 3684/7134 completed (loss: 0.033383116126060486, acc: 0.9913420081138611)
[2025-02-13 03:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27][root][INFO] - Training Epoch: 1/2, step 3685/7134 completed (loss: 0.03445689380168915, acc: 0.9884393215179443)
[2025-02-13 03:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27][root][INFO] - Training Epoch: 1/2, step 3686/7134 completed (loss: 0.01854170486330986, acc: 0.9940029978752136)
[2025-02-13 03:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:28][root][INFO] - Training Epoch: 1/2, step 3687/7134 completed (loss: 0.014024872332811356, acc: 0.9923076629638672)
[2025-02-13 03:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:28][root][INFO] - Training Epoch: 1/2, step 3688/7134 completed (loss: 0.01585150510072708, acc: 0.9959431886672974)
[2025-02-13 03:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29][root][INFO] - Training Epoch: 1/2, step 3689/7134 completed (loss: 0.023449072614312172, acc: 0.991830050945282)
[2025-02-13 03:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29][root][INFO] - Training Epoch: 1/2, step 3690/7134 completed (loss: 0.023167317733168602, acc: 0.991416335105896)
[2025-02-13 03:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29][root][INFO] - Training Epoch: 1/2, step 3691/7134 completed (loss: 0.03526681661605835, acc: 0.9925558567047119)
[2025-02-13 03:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30][root][INFO] - Training Epoch: 1/2, step 3692/7134 completed (loss: 0.04258682578802109, acc: 0.9878234267234802)
[2025-02-13 03:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30][root][INFO] - Training Epoch: 1/2, step 3693/7134 completed (loss: 0.022988738492131233, acc: 0.9905362725257874)
[2025-02-13 03:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:31][root][INFO] - Training Epoch: 1/2, step 3694/7134 completed (loss: 0.0277792327105999, acc: 0.9943100810050964)
[2025-02-13 03:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:31][root][INFO] - Training Epoch: 1/2, step 3695/7134 completed (loss: 0.03718399256467819, acc: 0.9886685609817505)
[2025-02-13 03:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32][root][INFO] - Training Epoch: 1/2, step 3696/7134 completed (loss: 0.01841585710644722, acc: 0.9922879338264465)
[2025-02-13 03:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32][root][INFO] - Training Epoch: 1/2, step 3697/7134 completed (loss: 0.01482467446476221, acc: 0.9927113652229309)
[2025-02-13 03:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32][root][INFO] - Training Epoch: 1/2, step 3698/7134 completed (loss: 0.02535671554505825, acc: 0.9903692007064819)
[2025-02-13 03:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33][root][INFO] - Training Epoch: 1/2, step 3699/7134 completed (loss: 0.009251841343939304, acc: 0.9966216087341309)
[2025-02-13 03:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33][root][INFO] - Training Epoch: 1/2, step 3700/7134 completed (loss: 0.02523844502866268, acc: 0.9956268072128296)
[2025-02-13 03:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:34][root][INFO] - Training Epoch: 1/2, step 3701/7134 completed (loss: 0.04224051535129547, acc: 0.9850993156433105)
[2025-02-13 03:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:34][root][INFO] - Training Epoch: 1/2, step 3702/7134 completed (loss: 0.040546707808971405, acc: 0.9903581142425537)
[2025-02-13 03:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:34][root][INFO] - Training Epoch: 1/2, step 3703/7134 completed (loss: 0.025661015883088112, acc: 0.9961685538291931)
[2025-02-13 03:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:35][root][INFO] - Training Epoch: 1/2, step 3704/7134 completed (loss: 0.009039697237312794, acc: 0.9979079365730286)
[2025-02-13 03:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:35][root][INFO] - Training Epoch: 1/2, step 3705/7134 completed (loss: 0.047331005334854126, acc: 0.9840764403343201)
[2025-02-13 03:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36][root][INFO] - Training Epoch: 1/2, step 3706/7134 completed (loss: 0.09208262711763382, acc: 0.9778597950935364)
[2025-02-13 03:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36][root][INFO] - Training Epoch: 1/2, step 3707/7134 completed (loss: 0.025641445070505142, acc: 0.9901823401451111)
[2025-02-13 03:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:37][root][INFO] - Training Epoch: 1/2, step 3708/7134 completed (loss: 0.050723787397146225, acc: 0.9857142567634583)
[2025-02-13 03:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:37][root][INFO] - Training Epoch: 1/2, step 3709/7134 completed (loss: 0.028862494975328445, acc: 0.9888437986373901)
[2025-02-13 03:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38][root][INFO] - Training Epoch: 1/2, step 3710/7134 completed (loss: 0.038169536739587784, acc: 0.9897540807723999)
[2025-02-13 03:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38][root][INFO] - Training Epoch: 1/2, step 3711/7134 completed (loss: 0.038157906383275986, acc: 0.9925558567047119)
[2025-02-13 03:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38][root][INFO] - Training Epoch: 1/2, step 3712/7134 completed (loss: 0.015208419412374496, acc: 0.9964157938957214)
[2025-02-13 03:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:39][root][INFO] - Training Epoch: 1/2, step 3713/7134 completed (loss: 0.05122251808643341, acc: 0.9885057210922241)
[2025-02-13 03:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:39][root][INFO] - Training Epoch: 1/2, step 3714/7134 completed (loss: 0.033987171947956085, acc: 0.9911602139472961)
[2025-02-13 03:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40][root][INFO] - Training Epoch: 1/2, step 3715/7134 completed (loss: 0.03145771101117134, acc: 0.9915356636047363)
[2025-02-13 03:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40][root][INFO] - Training Epoch: 1/2, step 3716/7134 completed (loss: 0.043198224157094955, acc: 0.9852034449577332)
[2025-02-13 03:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:41][root][INFO] - Training Epoch: 1/2, step 3717/7134 completed (loss: 0.027711939066648483, acc: 0.9959016442298889)
[2025-02-13 03:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:41][root][INFO] - Training Epoch: 1/2, step 3718/7134 completed (loss: 0.056069646030664444, acc: 0.9859319925308228)
[2025-02-13 03:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:42][root][INFO] - Training Epoch: 1/2, step 3719/7134 completed (loss: 0.059250522404909134, acc: 0.9852941036224365)
[2025-02-13 03:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:42][root][INFO] - Training Epoch: 1/2, step 3720/7134 completed (loss: 0.016807833686470985, acc: 0.9941349029541016)
[2025-02-13 03:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:43][root][INFO] - Training Epoch: 1/2, step 3721/7134 completed (loss: 0.03608299046754837, acc: 0.9840213060379028)
[2025-02-13 03:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:43][root][INFO] - Training Epoch: 1/2, step 3722/7134 completed (loss: 0.02119331620633602, acc: 0.9926108121871948)
[2025-02-13 03:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:43][root][INFO] - Training Epoch: 1/2, step 3723/7134 completed (loss: 0.05433335155248642, acc: 0.9856114983558655)
[2025-02-13 03:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44][root][INFO] - Training Epoch: 1/2, step 3724/7134 completed (loss: 0.049289703369140625, acc: 0.982758641242981)
[2025-02-13 03:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44][root][INFO] - Training Epoch: 1/2, step 3725/7134 completed (loss: 0.12215271592140198, acc: 0.9690265655517578)
[2025-02-13 03:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:45][root][INFO] - Training Epoch: 1/2, step 3726/7134 completed (loss: 0.03004857338964939, acc: 0.9957491755485535)
[2025-02-13 03:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:45][root][INFO] - Training Epoch: 1/2, step 3727/7134 completed (loss: 0.03184124082326889, acc: 0.9959946870803833)
[2025-02-13 03:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:46][root][INFO] - Training Epoch: 1/2, step 3728/7134 completed (loss: 0.017536386847496033, acc: 0.9954904317855835)
[2025-02-13 03:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:46][root][INFO] - Training Epoch: 1/2, step 3729/7134 completed (loss: 0.03264761343598366, acc: 0.9886363744735718)
[2025-02-13 03:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47][root][INFO] - Training Epoch: 1/2, step 3730/7134 completed (loss: 0.03000110760331154, acc: 0.9932157397270203)
[2025-02-13 03:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47][root][INFO] - Training Epoch: 1/2, step 3731/7134 completed (loss: 0.018381042405962944, acc: 0.9950186610221863)
[2025-02-13 03:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47][root][INFO] - Training Epoch: 1/2, step 3732/7134 completed (loss: 0.020750783383846283, acc: 0.9943246245384216)
[2025-02-13 03:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:48][root][INFO] - Training Epoch: 1/2, step 3733/7134 completed (loss: 0.04406355693936348, acc: 0.9887797832489014)
[2025-02-13 03:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:48][root][INFO] - Training Epoch: 1/2, step 3734/7134 completed (loss: 0.04014242812991142, acc: 0.983818769454956)
[2025-02-13 03:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:49][root][INFO] - Training Epoch: 1/2, step 3735/7134 completed (loss: 0.041792724281549454, acc: 0.9894578456878662)
[2025-02-13 03:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:49][root][INFO] - Training Epoch: 1/2, step 3736/7134 completed (loss: 0.08419876545667648, acc: 0.9791304469108582)
[2025-02-13 03:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50][root][INFO] - Training Epoch: 1/2, step 3737/7134 completed (loss: 0.051414139568805695, acc: 0.9834087491035461)
[2025-02-13 03:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50][root][INFO] - Training Epoch: 1/2, step 3738/7134 completed (loss: 0.01993671990931034, acc: 0.9933993220329285)
[2025-02-13 03:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50][root][INFO] - Training Epoch: 1/2, step 3739/7134 completed (loss: 0.06259714812040329, acc: 0.9859550595283508)
[2025-02-13 03:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51][root][INFO] - Training Epoch: 1/2, step 3740/7134 completed (loss: 0.03712424263358116, acc: 0.9918830990791321)
[2025-02-13 03:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51][root][INFO] - Training Epoch: 1/2, step 3741/7134 completed (loss: 0.04389055445790291, acc: 0.9928673505783081)
[2025-02-13 03:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:52][root][INFO] - Training Epoch: 1/2, step 3742/7134 completed (loss: 0.050003502517938614, acc: 0.9863247871398926)
[2025-02-13 03:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:52][root][INFO] - Training Epoch: 1/2, step 3743/7134 completed (loss: 0.057054419070482254, acc: 0.982300877571106)
[2025-02-13 03:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:52][root][INFO] - Training Epoch: 1/2, step 3744/7134 completed (loss: 0.06142269819974899, acc: 0.9828721880912781)
[2025-02-13 03:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:53][root][INFO] - Training Epoch: 1/2, step 3745/7134 completed (loss: 0.08522259443998337, acc: 0.9767981171607971)
[2025-02-13 03:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:53][root][INFO] - Training Epoch: 1/2, step 3746/7134 completed (loss: 0.05214264616370201, acc: 0.9842932224273682)
[2025-02-13 03:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54][root][INFO] - Training Epoch: 1/2, step 3747/7134 completed (loss: 0.052233848720788956, acc: 0.9825072884559631)
[2025-02-13 03:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54][root][INFO] - Training Epoch: 1/2, step 3748/7134 completed (loss: 0.06960088014602661, acc: 0.9841549396514893)
[2025-02-13 03:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:55][root][INFO] - Training Epoch: 1/2, step 3749/7134 completed (loss: 0.08202921599149704, acc: 0.9835082292556763)
[2025-02-13 03:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:55][root][INFO] - Training Epoch: 1/2, step 3750/7134 completed (loss: 0.017699429765343666, acc: 0.9958158731460571)
[2025-02-13 03:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:55][root][INFO] - Training Epoch: 1/2, step 3751/7134 completed (loss: 0.06808919459581375, acc: 0.9843527674674988)
[2025-02-13 03:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56][root][INFO] - Training Epoch: 1/2, step 3752/7134 completed (loss: 0.037446942180395126, acc: 0.9865671396255493)
[2025-02-13 03:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56][root][INFO] - Training Epoch: 1/2, step 3753/7134 completed (loss: 0.07824994623661041, acc: 0.9790732264518738)
[2025-02-13 03:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:57][root][INFO] - Training Epoch: 1/2, step 3754/7134 completed (loss: 0.02100585587322712, acc: 0.9939302206039429)
[2025-02-13 03:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:57][root][INFO] - Training Epoch: 1/2, step 3755/7134 completed (loss: 0.02944309078156948, acc: 0.994434118270874)
[2025-02-13 03:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:57][root][INFO] - Training Epoch: 1/2, step 3756/7134 completed (loss: 0.052160389721393585, acc: 0.9875346422195435)
[2025-02-13 03:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:58][root][INFO] - Training Epoch: 1/2, step 3757/7134 completed (loss: 0.08780711144208908, acc: 0.9720853567123413)
[2025-02-13 03:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:58][root][INFO] - Training Epoch: 1/2, step 3758/7134 completed (loss: 0.04826851561665535, acc: 0.9855855703353882)
[2025-02-13 03:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59][root][INFO] - Training Epoch: 1/2, step 3759/7134 completed (loss: 0.05551942065358162, acc: 0.9864864945411682)
[2025-02-13 03:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59][root][INFO] - Training Epoch: 1/2, step 3760/7134 completed (loss: 0.05140732228755951, acc: 0.9901130199432373)
[2025-02-13 03:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:00][root][INFO] - Training Epoch: 1/2, step 3761/7134 completed (loss: 0.024278465658426285, acc: 0.9938176274299622)
[2025-02-13 03:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:00][root][INFO] - Training Epoch: 1/2, step 3762/7134 completed (loss: 0.03846277296543121, acc: 0.9889958500862122)
[2025-02-13 03:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:00][root][INFO] - Training Epoch: 1/2, step 3763/7134 completed (loss: 0.024856287986040115, acc: 0.9907407164573669)
[2025-02-13 03:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01][root][INFO] - Training Epoch: 1/2, step 3764/7134 completed (loss: 0.0461476594209671, acc: 0.9863201379776001)
[2025-02-13 03:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01][root][INFO] - Training Epoch: 1/2, step 3765/7134 completed (loss: 0.023480786010622978, acc: 0.9903961420059204)
[2025-02-13 03:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02][root][INFO] - Training Epoch: 1/2, step 3766/7134 completed (loss: 0.08883887529373169, acc: 0.9843304753303528)
[2025-02-13 03:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02][root][INFO] - Training Epoch: 1/2, step 3767/7134 completed (loss: 0.025373108685016632, acc: 0.9925187230110168)
[2025-02-13 03:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:03][root][INFO] - Training Epoch: 1/2, step 3768/7134 completed (loss: 0.0144671481102705, acc: 0.9937759041786194)
[2025-02-13 03:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:03][root][INFO] - Training Epoch: 1/2, step 3769/7134 completed (loss: 0.04595766216516495, acc: 0.986146092414856)
[2025-02-13 03:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:04][root][INFO] - Training Epoch: 1/2, step 3770/7134 completed (loss: 0.04251853749155998, acc: 0.9901130199432373)
[2025-02-13 03:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:04][root][INFO] - Training Epoch: 1/2, step 3771/7134 completed (loss: 0.029920533299446106, acc: 0.9909090995788574)
[2025-02-13 03:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:04][root][INFO] - Training Epoch: 1/2, step 3772/7134 completed (loss: 0.05673831328749657, acc: 0.9820627570152283)
[2025-02-13 03:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05][root][INFO] - Training Epoch: 1/2, step 3773/7134 completed (loss: 0.03383626416325569, acc: 0.9921875)
[2025-02-13 03:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05][root][INFO] - Training Epoch: 1/2, step 3774/7134 completed (loss: 0.04309878125786781, acc: 0.9895591735839844)
[2025-02-13 03:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06][root][INFO] - Training Epoch: 1/2, step 3775/7134 completed (loss: 0.022790517657995224, acc: 0.9942693114280701)
[2025-02-13 03:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06][root][INFO] - Training Epoch: 1/2, step 3776/7134 completed (loss: 0.04209602251648903, acc: 0.9837037324905396)
[2025-02-13 03:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:07][root][INFO] - Training Epoch: 1/2, step 3777/7134 completed (loss: 0.03019750490784645, acc: 0.9937343597412109)
[2025-02-13 03:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:07][root][INFO] - Training Epoch: 1/2, step 3778/7134 completed (loss: 0.05893551930785179, acc: 0.9857752323150635)
[2025-02-13 03:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:07][root][INFO] - Training Epoch: 1/2, step 3779/7134 completed (loss: 0.04204106703400612, acc: 0.9843304753303528)
[2025-02-13 03:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08][root][INFO] - Training Epoch: 1/2, step 3780/7134 completed (loss: 0.026011647656559944, acc: 0.9913544654846191)
[2025-02-13 03:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08][root][INFO] - Training Epoch: 1/2, step 3781/7134 completed (loss: 0.06897163391113281, acc: 0.9862825870513916)
[2025-02-13 03:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:09][root][INFO] - Training Epoch: 1/2, step 3782/7134 completed (loss: 0.04146433621644974, acc: 0.9871794581413269)
[2025-02-13 03:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:09][root][INFO] - Training Epoch: 1/2, step 3783/7134 completed (loss: 0.03708161786198616, acc: 0.9870967864990234)
[2025-02-13 03:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10][root][INFO] - Training Epoch: 1/2, step 3784/7134 completed (loss: 0.057105302810668945, acc: 0.9876352548599243)
[2025-02-13 03:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10][root][INFO] - Training Epoch: 1/2, step 3785/7134 completed (loss: 0.017799045890569687, acc: 0.9937597513198853)
[2025-02-13 03:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10][root][INFO] - Training Epoch: 1/2, step 3786/7134 completed (loss: 0.045400768518447876, acc: 0.9818887710571289)
[2025-02-13 03:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:11][root][INFO] - Training Epoch: 1/2, step 3787/7134 completed (loss: 0.03873236104846001, acc: 0.9840510487556458)
[2025-02-13 03:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:11][root][INFO] - Training Epoch: 1/2, step 3788/7134 completed (loss: 0.034135591238737106, acc: 0.9905533194541931)
[2025-02-13 03:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12][root][INFO] - Training Epoch: 1/2, step 3789/7134 completed (loss: 0.048022251576185226, acc: 0.9831223487854004)
[2025-02-13 03:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12][root][INFO] - Training Epoch: 1/2, step 3790/7134 completed (loss: 0.08324975520372391, acc: 0.9784537553787231)
[2025-02-13 03:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:13][root][INFO] - Training Epoch: 1/2, step 3791/7134 completed (loss: 0.0249097291380167, acc: 0.9935483932495117)
[2025-02-13 03:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:13][root][INFO] - Training Epoch: 1/2, step 3792/7134 completed (loss: 0.03692202270030975, acc: 0.9852216839790344)
[2025-02-13 03:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14][root][INFO] - Training Epoch: 1/2, step 3793/7134 completed (loss: 0.039985645562410355, acc: 0.9878378510475159)
[2025-02-13 03:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14][root][INFO] - Training Epoch: 1/2, step 3794/7134 completed (loss: 0.03947783261537552, acc: 0.9896238446235657)
[2025-02-13 03:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14][root][INFO] - Training Epoch: 1/2, step 3795/7134 completed (loss: 0.04599893465638161, acc: 0.9855421781539917)
[2025-02-13 03:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:15][root][INFO] - Training Epoch: 1/2, step 3796/7134 completed (loss: 0.03377655893564224, acc: 0.9912060499191284)
[2025-02-13 03:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:15][root][INFO] - Training Epoch: 1/2, step 3797/7134 completed (loss: 0.02720831148326397, acc: 0.9921773076057434)
[2025-02-13 03:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16][root][INFO] - Training Epoch: 1/2, step 3798/7134 completed (loss: 0.06447336822748184, acc: 0.9841954112052917)
[2025-02-13 03:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16][root][INFO] - Training Epoch: 1/2, step 3799/7134 completed (loss: 0.0476541593670845, acc: 0.9832689762115479)
[2025-02-13 03:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17][root][INFO] - Training Epoch: 1/2, step 3800/7134 completed (loss: 0.04572620615363121, acc: 0.9877675771713257)
[2025-02-13 03:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17][root][INFO] - Training Epoch: 1/2, step 3801/7134 completed (loss: 0.025576237589120865, acc: 0.9913580417633057)
[2025-02-13 03:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17][root][INFO] - Training Epoch: 1/2, step 3802/7134 completed (loss: 0.033013369888067245, acc: 0.991725742816925)
[2025-02-13 03:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18][root][INFO] - Training Epoch: 1/2, step 3803/7134 completed (loss: 0.04548681527376175, acc: 0.9873949289321899)
[2025-02-13 03:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18][root][INFO] - Training Epoch: 1/2, step 3804/7134 completed (loss: 0.02680187299847603, acc: 0.9906687140464783)
[2025-02-13 03:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:19][root][INFO] - Training Epoch: 1/2, step 3805/7134 completed (loss: 0.01665579527616501, acc: 0.9947643876075745)
[2025-02-13 03:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:19][root][INFO] - Training Epoch: 1/2, step 3806/7134 completed (loss: 0.04586745798587799, acc: 0.9842209219932556)
[2025-02-13 03:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20][root][INFO] - Training Epoch: 1/2, step 3807/7134 completed (loss: 0.012726343236863613, acc: 0.9985380172729492)
[2025-02-13 03:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20][root][INFO] - Training Epoch: 1/2, step 3808/7134 completed (loss: 0.0437166728079319, acc: 0.9911392331123352)
[2025-02-13 03:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20][root][INFO] - Training Epoch: 1/2, step 3809/7134 completed (loss: 0.060769449919462204, acc: 0.9835025668144226)
[2025-02-13 03:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:21][root][INFO] - Training Epoch: 1/2, step 3810/7134 completed (loss: 0.03279458358883858, acc: 0.9940476417541504)
[2025-02-13 03:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:21][root][INFO] - Training Epoch: 1/2, step 3811/7134 completed (loss: 0.05202305316925049, acc: 0.9834482669830322)
[2025-02-13 03:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22][root][INFO] - Training Epoch: 1/2, step 3812/7134 completed (loss: 0.05412278696894646, acc: 0.9828473329544067)
[2025-02-13 03:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22][root][INFO] - Training Epoch: 1/2, step 3813/7134 completed (loss: 0.04195781424641609, acc: 0.9868074059486389)
[2025-02-13 03:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:23][root][INFO] - Training Epoch: 1/2, step 3814/7134 completed (loss: 0.031616441905498505, acc: 0.9906166195869446)
[2025-02-13 03:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:23][root][INFO] - Training Epoch: 1/2, step 3815/7134 completed (loss: 0.0247579924762249, acc: 0.9911110997200012)
[2025-02-13 03:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:24][root][INFO] - Training Epoch: 1/2, step 3816/7134 completed (loss: 0.024117864668369293, acc: 0.9936908483505249)
[2025-02-13 03:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:24][root][INFO] - Training Epoch: 1/2, step 3817/7134 completed (loss: 0.05563417077064514, acc: 0.9832134246826172)
[2025-02-13 03:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:24][root][INFO] - Training Epoch: 1/2, step 3818/7134 completed (loss: 0.08945751190185547, acc: 0.9838383793830872)
[2025-02-13 03:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25][root][INFO] - Training Epoch: 1/2, step 3819/7134 completed (loss: 0.08338390290737152, acc: 0.9718875288963318)
[2025-02-13 03:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25][root][INFO] - Training Epoch: 1/2, step 3820/7134 completed (loss: 0.04724743589758873, acc: 0.980997622013092)
[2025-02-13 03:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26][root][INFO] - Training Epoch: 1/2, step 3821/7134 completed (loss: 0.04167115315794945, acc: 0.9847715497016907)
[2025-02-13 03:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26][root][INFO] - Training Epoch: 1/2, step 3822/7134 completed (loss: 0.05496257171034813, acc: 0.9823182821273804)
[2025-02-13 03:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26][root][INFO] - Training Epoch: 1/2, step 3823/7134 completed (loss: 0.024277912452816963, acc: 0.9917012453079224)
[2025-02-13 03:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:27][root][INFO] - Training Epoch: 1/2, step 3824/7134 completed (loss: 0.08306603133678436, acc: 0.9831365942955017)
[2025-02-13 03:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:27][root][INFO] - Training Epoch: 1/2, step 3825/7134 completed (loss: 0.027010075747966766, acc: 0.9900199770927429)
[2025-02-13 03:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:28][root][INFO] - Training Epoch: 1/2, step 3826/7134 completed (loss: 0.05586428940296173, acc: 0.9867647290229797)
[2025-02-13 03:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:28][root][INFO] - Training Epoch: 1/2, step 3827/7134 completed (loss: 0.033652178943157196, acc: 0.9929971694946289)
[2025-02-13 03:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29][root][INFO] - Training Epoch: 1/2, step 3828/7134 completed (loss: 0.07618387043476105, acc: 0.9867374300956726)
[2025-02-13 03:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29][root][INFO] - Training Epoch: 1/2, step 3829/7134 completed (loss: 0.03150436282157898, acc: 0.9924585223197937)
[2025-02-13 03:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:30][root][INFO] - Training Epoch: 1/2, step 3830/7134 completed (loss: 0.05736978352069855, acc: 0.9849246144294739)
[2025-02-13 03:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:30][root][INFO] - Training Epoch: 1/2, step 3831/7134 completed (loss: 0.03766831010580063, acc: 0.9849849939346313)
[2025-02-13 03:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:30][root][INFO] - Training Epoch: 1/2, step 3832/7134 completed (loss: 0.034893717616796494, acc: 0.9921414256095886)
[2025-02-13 03:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:31][root][INFO] - Training Epoch: 1/2, step 3833/7134 completed (loss: 0.013582000508904457, acc: 0.9950494766235352)
[2025-02-13 03:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:31][root][INFO] - Training Epoch: 1/2, step 3834/7134 completed (loss: 0.012017926201224327, acc: 0.9948052167892456)
[2025-02-13 03:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32][root][INFO] - Training Epoch: 1/2, step 3835/7134 completed (loss: 0.012503997422754765, acc: 0.9951456189155579)
[2025-02-13 03:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32][root][INFO] - Training Epoch: 1/2, step 3836/7134 completed (loss: 0.048601310700178146, acc: 0.9884058237075806)
[2025-02-13 03:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33][root][INFO] - Training Epoch: 1/2, step 3837/7134 completed (loss: 0.026201313361525536, acc: 0.9956896305084229)
[2025-02-13 03:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33][root][INFO] - Training Epoch: 1/2, step 3838/7134 completed (loss: 0.025189824402332306, acc: 0.9933599233627319)
[2025-02-13 03:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33][root][INFO] - Training Epoch: 1/2, step 3839/7134 completed (loss: 0.01843273639678955, acc: 0.9930939078330994)
[2025-02-13 03:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:34][root][INFO] - Training Epoch: 1/2, step 3840/7134 completed (loss: 0.04466736316680908, acc: 0.9866130948066711)
[2025-02-13 03:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:34][root][INFO] - Training Epoch: 1/2, step 3841/7134 completed (loss: 0.0691756010055542, acc: 0.9877913594245911)
[2025-02-13 03:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35][root][INFO] - Training Epoch: 1/2, step 3842/7134 completed (loss: 0.006511917803436518, acc: 0.9975489974021912)
[2025-02-13 03:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35][root][INFO] - Training Epoch: 1/2, step 3843/7134 completed (loss: 0.030023861676454544, acc: 0.9906152486801147)
[2025-02-13 03:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36][root][INFO] - Training Epoch: 1/2, step 3844/7134 completed (loss: 0.027290327474474907, acc: 0.9936467409133911)
[2025-02-13 03:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36][root][INFO] - Training Epoch: 1/2, step 3845/7134 completed (loss: 0.04646652564406395, acc: 0.9898989796638489)
[2025-02-13 03:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36][root][INFO] - Training Epoch: 1/2, step 3846/7134 completed (loss: 0.016496963798999786, acc: 0.9932088255882263)
[2025-02-13 03:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:37][root][INFO] - Training Epoch: 1/2, step 3847/7134 completed (loss: 0.02376423589885235, acc: 0.9919999837875366)
[2025-02-13 03:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:37][root][INFO] - Training Epoch: 1/2, step 3848/7134 completed (loss: 0.02722024731338024, acc: 0.9961832165718079)
[2025-02-13 03:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38][root][INFO] - Training Epoch: 1/2, step 3849/7134 completed (loss: 0.060985226184129715, acc: 0.9839572310447693)
[2025-02-13 03:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38][root][INFO] - Training Epoch: 1/2, step 3850/7134 completed (loss: 0.0625988021492958, acc: 0.9853300452232361)
[2025-02-13 03:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:39][root][INFO] - Training Epoch: 1/2, step 3851/7134 completed (loss: 0.057003192603588104, acc: 0.9804741740226746)
[2025-02-13 03:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:39][root][INFO] - Training Epoch: 1/2, step 3852/7134 completed (loss: 0.027342744171619415, acc: 0.9919614195823669)
[2025-02-13 03:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40][root][INFO] - Training Epoch: 1/2, step 3853/7134 completed (loss: 0.05446339398622513, acc: 0.9817159175872803)
[2025-02-13 03:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40][root][INFO] - Training Epoch: 1/2, step 3854/7134 completed (loss: 0.01848321221768856, acc: 0.992977499961853)
[2025-02-13 03:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40][root][INFO] - Training Epoch: 1/2, step 3855/7134 completed (loss: 0.055316317826509476, acc: 0.9900249242782593)
[2025-02-13 03:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:41][root][INFO] - Training Epoch: 1/2, step 3856/7134 completed (loss: 0.04463839530944824, acc: 0.9860529899597168)
[2025-02-13 03:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:41][root][INFO] - Training Epoch: 1/2, step 3857/7134 completed (loss: 0.035131845623254776, acc: 0.9855305552482605)
[2025-02-13 03:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:42][root][INFO] - Training Epoch: 1/2, step 3858/7134 completed (loss: 0.05608256533741951, acc: 0.980169951915741)
[2025-02-13 03:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:42][root][INFO] - Training Epoch: 1/2, step 3859/7134 completed (loss: 0.04446392133831978, acc: 0.9872773289680481)
[2025-02-13 03:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:43][root][INFO] - Training Epoch: 1/2, step 3860/7134 completed (loss: 0.03333354368805885, acc: 0.9910614490509033)
[2025-02-13 03:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:43][root][INFO] - Training Epoch: 1/2, step 3861/7134 completed (loss: 0.04616783931851387, acc: 0.9863945841789246)
[2025-02-13 03:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:43][root][INFO] - Training Epoch: 1/2, step 3862/7134 completed (loss: 0.048582401126623154, acc: 0.9835796356201172)
[2025-02-13 03:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44][root][INFO] - Training Epoch: 1/2, step 3863/7134 completed (loss: 0.028778886422514915, acc: 0.9913580417633057)
[2025-02-13 03:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44][root][INFO] - Training Epoch: 1/2, step 3864/7134 completed (loss: 0.032983459532260895, acc: 0.9921671152114868)
[2025-02-13 03:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:45][root][INFO] - Training Epoch: 1/2, step 3865/7134 completed (loss: 0.03266056627035141, acc: 0.9860557913780212)
[2025-02-13 03:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:45][root][INFO] - Training Epoch: 1/2, step 3866/7134 completed (loss: 0.05078805983066559, acc: 0.9853479862213135)
[2025-02-13 03:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46][root][INFO] - Training Epoch: 1/2, step 3867/7134 completed (loss: 0.04322972893714905, acc: 0.9886363744735718)
[2025-02-13 03:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46][root][INFO] - Training Epoch: 1/2, step 3868/7134 completed (loss: 0.02361399307847023, acc: 0.9915397763252258)
[2025-02-13 03:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:47][root][INFO] - Training Epoch: 1/2, step 3869/7134 completed (loss: 0.0380617119371891, acc: 0.9863842725753784)
[2025-02-13 03:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:47][root][INFO] - Training Epoch: 1/2, step 3870/7134 completed (loss: 0.07034922391176224, acc: 0.9819444417953491)
[2025-02-13 03:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:47][root][INFO] - Training Epoch: 1/2, step 3871/7134 completed (loss: 0.043959878385066986, acc: 0.9887780547142029)
[2025-02-13 03:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:48][root][INFO] - Training Epoch: 1/2, step 3872/7134 completed (loss: 0.04695349559187889, acc: 0.9903640151023865)
[2025-02-13 03:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:48][root][INFO] - Training Epoch: 1/2, step 3873/7134 completed (loss: 0.027062280103564262, acc: 0.9884910583496094)
[2025-02-13 03:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49][root][INFO] - Training Epoch: 1/2, step 3874/7134 completed (loss: 0.06065676361322403, acc: 0.9834024906158447)
[2025-02-13 03:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49][root][INFO] - Training Epoch: 1/2, step 3875/7134 completed (loss: 0.0749807357788086, acc: 0.9827127456665039)
[2025-02-13 03:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:50][root][INFO] - Training Epoch: 1/2, step 3876/7134 completed (loss: 0.03268548473715782, acc: 0.9906191229820251)
[2025-02-13 03:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:50][root][INFO] - Training Epoch: 1/2, step 3877/7134 completed (loss: 0.05934013053774834, acc: 0.9882506728172302)
[2025-02-13 03:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:50][root][INFO] - Training Epoch: 1/2, step 3878/7134 completed (loss: 0.06812308728694916, acc: 0.9824798107147217)
[2025-02-13 03:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51][root][INFO] - Training Epoch: 1/2, step 3879/7134 completed (loss: 0.023106323555111885, acc: 0.996303141117096)
[2025-02-13 03:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51][root][INFO] - Training Epoch: 1/2, step 3880/7134 completed (loss: 0.10812070965766907, acc: 0.9750367403030396)
[2025-02-13 03:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52][root][INFO] - Training Epoch: 1/2, step 3881/7134 completed (loss: 0.11414897441864014, acc: 0.972000002861023)
[2025-02-13 03:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52][root][INFO] - Training Epoch: 1/2, step 3882/7134 completed (loss: 0.07387123256921768, acc: 0.9825000166893005)
[2025-02-13 03:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:53][root][INFO] - Training Epoch: 1/2, step 3883/7134 completed (loss: 0.01815704256296158, acc: 0.9944055676460266)
[2025-02-13 03:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:53][root][INFO] - Training Epoch: 1/2, step 3884/7134 completed (loss: 0.022873861715197563, acc: 0.994557797908783)
[2025-02-13 03:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:54][root][INFO] - Training Epoch: 1/2, step 3885/7134 completed (loss: 0.02936382219195366, acc: 0.9895561337471008)
[2025-02-13 03:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:54][root][INFO] - Training Epoch: 1/2, step 3886/7134 completed (loss: 0.03414737433195114, acc: 0.987922728061676)
[2025-02-13 03:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:54][root][INFO] - Training Epoch: 1/2, step 3887/7134 completed (loss: 0.028167713433504105, acc: 0.993966817855835)
[2025-02-13 03:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55][root][INFO] - Training Epoch: 1/2, step 3888/7134 completed (loss: 0.04666898399591446, acc: 0.9850000143051147)
[2025-02-13 03:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55][root][INFO] - Training Epoch: 1/2, step 3889/7134 completed (loss: 0.02803697995841503, acc: 0.992337167263031)
[2025-02-13 03:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:56][root][INFO] - Training Epoch: 1/2, step 3890/7134 completed (loss: 0.020767299458384514, acc: 0.9951865077018738)
[2025-02-13 03:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:56][root][INFO] - Training Epoch: 1/2, step 3891/7134 completed (loss: 0.019118407741189003, acc: 0.9934640526771545)
[2025-02-13 03:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57][root][INFO] - Training Epoch: 1/2, step 3892/7134 completed (loss: 0.03372996672987938, acc: 0.989230751991272)
[2025-02-13 03:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57][root][INFO] - Training Epoch: 1/2, step 3893/7134 completed (loss: 0.0702655240893364, acc: 0.9742268323898315)
[2025-02-13 03:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:58][root][INFO] - Training Epoch: 1/2, step 3894/7134 completed (loss: 0.02198687382042408, acc: 0.9957864880561829)
[2025-02-13 03:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:58][root][INFO] - Training Epoch: 1/2, step 3895/7134 completed (loss: 0.029774373397231102, acc: 0.9922580718994141)
[2025-02-13 03:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:58][root][INFO] - Training Epoch: 1/2, step 3896/7134 completed (loss: 0.02623942494392395, acc: 0.993228018283844)
[2025-02-13 03:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59][root][INFO] - Training Epoch: 1/2, step 3897/7134 completed (loss: 0.05455104261636734, acc: 0.9887323975563049)
[2025-02-13 03:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59][root][INFO] - Training Epoch: 1/2, step 3898/7134 completed (loss: 0.04594389721751213, acc: 0.9847328066825867)
[2025-02-13 03:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:00][root][INFO] - Training Epoch: 1/2, step 3899/7134 completed (loss: 0.028537463396787643, acc: 0.9961190223693848)
[2025-02-13 03:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:00][root][INFO] - Training Epoch: 1/2, step 3900/7134 completed (loss: 0.04003150388598442, acc: 0.9872537851333618)
[2025-02-13 03:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01][root][INFO] - Training Epoch: 1/2, step 3901/7134 completed (loss: 0.08271470665931702, acc: 0.9824798107147217)
[2025-02-13 03:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01][root][INFO] - Training Epoch: 1/2, step 3902/7134 completed (loss: 0.09781671315431595, acc: 0.9748344421386719)
[2025-02-13 03:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:02][root][INFO] - Training Epoch: 1/2, step 3903/7134 completed (loss: 0.09384073317050934, acc: 0.9718498587608337)
[2025-02-13 03:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:02][root][INFO] - Training Epoch: 1/2, step 3904/7134 completed (loss: 0.06512722373008728, acc: 0.9817351698875427)
[2025-02-13 03:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:02][root][INFO] - Training Epoch: 1/2, step 3905/7134 completed (loss: 0.020459327846765518, acc: 0.994358241558075)
[2025-02-13 03:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:03][root][INFO] - Training Epoch: 1/2, step 3906/7134 completed (loss: 0.019212232902646065, acc: 0.9956896305084229)
[2025-02-13 03:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:03][root][INFO] - Training Epoch: 1/2, step 3907/7134 completed (loss: 0.03873297944664955, acc: 0.9897172451019287)
[2025-02-13 03:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04][root][INFO] - Training Epoch: 1/2, step 3908/7134 completed (loss: 0.09092121571302414, acc: 0.9785605072975159)
[2025-02-13 03:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04][root][INFO] - Training Epoch: 1/2, step 3909/7134 completed (loss: 0.040510065853595734, acc: 0.9838449358940125)
[2025-02-13 03:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:05][root][INFO] - Training Epoch: 1/2, step 3910/7134 completed (loss: 0.05715634301304817, acc: 0.9847009778022766)
[2025-02-13 03:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:05][root][INFO] - Training Epoch: 1/2, step 3911/7134 completed (loss: 0.028953691944479942, acc: 0.991909384727478)
[2025-02-13 03:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06][root][INFO] - Training Epoch: 1/2, step 3912/7134 completed (loss: 0.016174528747797012, acc: 0.9940476417541504)
[2025-02-13 03:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06][root][INFO] - Training Epoch: 1/2, step 3913/7134 completed (loss: 0.03131856769323349, acc: 0.9884615540504456)
[2025-02-13 03:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06][root][INFO] - Training Epoch: 1/2, step 3914/7134 completed (loss: 0.03480932489037514, acc: 0.9932659864425659)
[2025-02-13 03:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07][root][INFO] - Training Epoch: 1/2, step 3915/7134 completed (loss: 0.026571448892354965, acc: 0.9929478168487549)
[2025-02-13 03:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07][root][INFO] - Training Epoch: 1/2, step 3916/7134 completed (loss: 0.022989030927419662, acc: 0.9910827875137329)
[2025-02-13 03:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:08][root][INFO] - Training Epoch: 1/2, step 3917/7134 completed (loss: 0.016133667901158333, acc: 0.9963898658752441)
[2025-02-13 03:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:08][root][INFO] - Training Epoch: 1/2, step 3918/7134 completed (loss: 0.03914967551827431, acc: 0.9869791865348816)
[2025-02-13 03:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:09][root][INFO] - Training Epoch: 1/2, step 3919/7134 completed (loss: 0.07086818665266037, acc: 0.9804560542106628)
[2025-02-13 03:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:09][root][INFO] - Training Epoch: 1/2, step 3920/7134 completed (loss: 0.05618787184357643, acc: 0.9813084006309509)
[2025-02-13 03:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10][root][INFO] - Training Epoch: 1/2, step 3921/7134 completed (loss: 0.04330335557460785, acc: 0.9860000014305115)
[2025-02-13 03:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10][root][INFO] - Training Epoch: 1/2, step 3922/7134 completed (loss: 0.09795556962490082, acc: 0.9580712914466858)
[2025-02-13 03:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10][root][INFO] - Training Epoch: 1/2, step 3923/7134 completed (loss: 0.13385485112667084, acc: 0.9743150472640991)
[2025-02-13 03:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:11][root][INFO] - Training Epoch: 1/2, step 3924/7134 completed (loss: 0.052059490233659744, acc: 0.977011501789093)
[2025-02-13 03:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:11][root][INFO] - Training Epoch: 1/2, step 3925/7134 completed (loss: 0.07195410877466202, acc: 0.9751098155975342)
[2025-02-13 03:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:12][root][INFO] - Training Epoch: 1/2, step 3926/7134 completed (loss: 0.07388117909431458, acc: 0.9827337861061096)
[2025-02-13 03:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:12][root][INFO] - Training Epoch: 1/2, step 3927/7134 completed (loss: 0.035210128873586655, acc: 0.9876203536987305)
[2025-02-13 03:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13][root][INFO] - Training Epoch: 1/2, step 3928/7134 completed (loss: 0.06084504723548889, acc: 0.982758641242981)
[2025-02-13 03:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13][root][INFO] - Training Epoch: 1/2, step 3929/7134 completed (loss: 0.029827261343598366, acc: 0.9936407208442688)
[2025-02-13 03:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13][root][INFO] - Training Epoch: 1/2, step 3930/7134 completed (loss: 0.055127937346696854, acc: 0.9768907427787781)
[2025-02-13 03:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:14][root][INFO] - Training Epoch: 1/2, step 3931/7134 completed (loss: 0.03775941953063011, acc: 0.9835575222969055)
[2025-02-13 03:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:14][root][INFO] - Training Epoch: 1/2, step 3932/7134 completed (loss: 0.04371966794133186, acc: 0.9861660003662109)
[2025-02-13 03:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:15][root][INFO] - Training Epoch: 1/2, step 3933/7134 completed (loss: 0.030861422419548035, acc: 0.9934210777282715)
[2025-02-13 03:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:15][root][INFO] - Training Epoch: 1/2, step 3934/7134 completed (loss: 0.0527973398566246, acc: 0.9882903695106506)
[2025-02-13 03:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16][root][INFO] - Training Epoch: 1/2, step 3935/7134 completed (loss: 0.004465394653379917, acc: 1.0)
[2025-02-13 03:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16][root][INFO] - Training Epoch: 1/2, step 3936/7134 completed (loss: 0.04932926967740059, acc: 0.9886792302131653)
[2025-02-13 03:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16][root][INFO] - Training Epoch: 1/2, step 3937/7134 completed (loss: 0.058149296790361404, acc: 0.9857142567634583)
[2025-02-13 03:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:17][root][INFO] - Training Epoch: 1/2, step 3938/7134 completed (loss: 0.02012651599943638, acc: 0.9932126402854919)
[2025-02-13 03:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:17][root][INFO] - Training Epoch: 1/2, step 3939/7134 completed (loss: 0.07597590237855911, acc: 0.9847715497016907)
[2025-02-13 03:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:18][root][INFO] - Training Epoch: 1/2, step 3940/7134 completed (loss: 0.026956802234053612, acc: 0.9949748516082764)
[2025-02-13 03:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:18][root][INFO] - Training Epoch: 1/2, step 3941/7134 completed (loss: 0.027723373845219612, acc: 0.9908536672592163)
[2025-02-13 03:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:18][root][INFO] - Training Epoch: 1/2, step 3942/7134 completed (loss: 0.02448883280158043, acc: 0.9918032884597778)
[2025-02-13 03:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19][root][INFO] - Training Epoch: 1/2, step 3943/7134 completed (loss: 0.025541117414832115, acc: 0.9884169697761536)
[2025-02-13 03:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19][root][INFO] - Training Epoch: 1/2, step 3944/7134 completed (loss: 0.0344182550907135, acc: 0.990774929523468)
[2025-02-13 03:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:20][root][INFO] - Training Epoch: 1/2, step 3945/7134 completed (loss: 0.03158794343471527, acc: 0.9878048896789551)
[2025-02-13 03:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:20][root][INFO] - Training Epoch: 1/2, step 3946/7134 completed (loss: 0.04647646099328995, acc: 0.9937369227409363)
[2025-02-13 03:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:20][root][INFO] - Training Epoch: 1/2, step 3947/7134 completed (loss: 0.06011779233813286, acc: 0.9896373152732849)
[2025-02-13 03:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21][root][INFO] - Training Epoch: 1/2, step 3948/7134 completed (loss: 0.018239185214042664, acc: 0.9974874258041382)
[2025-02-13 03:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21][root][INFO] - Training Epoch: 1/2, step 3949/7134 completed (loss: 0.03744874894618988, acc: 0.9906716346740723)
[2025-02-13 03:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:22][root][INFO] - Training Epoch: 1/2, step 3950/7134 completed (loss: 0.044325362890958786, acc: 0.9910714030265808)
[2025-02-13 03:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:22][root][INFO] - Training Epoch: 1/2, step 3951/7134 completed (loss: 0.041374750435352325, acc: 0.9899749159812927)
[2025-02-13 03:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:22][root][INFO] - Training Epoch: 1/2, step 3952/7134 completed (loss: 0.04250505939126015, acc: 0.9897435903549194)
[2025-02-13 03:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:23][root][INFO] - Training Epoch: 1/2, step 3953/7134 completed (loss: 0.04189475253224373, acc: 0.9905123114585876)
[2025-02-13 03:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:23][root][INFO] - Training Epoch: 1/2, step 3954/7134 completed (loss: 0.04965749382972717, acc: 0.9819587469100952)
[2025-02-13 03:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:24][root][INFO] - Training Epoch: 1/2, step 3955/7134 completed (loss: 0.0312328077852726, acc: 0.9898819327354431)
[2025-02-13 03:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:24][root][INFO] - Training Epoch: 1/2, step 3956/7134 completed (loss: 0.023834098130464554, acc: 0.9936000108718872)
[2025-02-13 03:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25][root][INFO] - Training Epoch: 1/2, step 3957/7134 completed (loss: 0.035815611481666565, acc: 0.9887387156486511)
[2025-02-13 03:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25][root][INFO] - Training Epoch: 1/2, step 3958/7134 completed (loss: 0.21134842932224274, acc: 0.9487179517745972)
[2025-02-13 03:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25][root][INFO] - Training Epoch: 1/2, step 3959/7134 completed (loss: 0.016164658591151237, acc: 0.995604395866394)
[2025-02-13 03:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:26][root][INFO] - Training Epoch: 1/2, step 3960/7134 completed (loss: 0.03946046531200409, acc: 0.9871794581413269)
[2025-02-13 03:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:26][root][INFO] - Training Epoch: 1/2, step 3961/7134 completed (loss: 0.014623606577515602, acc: 0.9925037622451782)
[2025-02-13 03:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27][root][INFO] - Training Epoch: 1/2, step 3962/7134 completed (loss: 0.01851736009120941, acc: 0.9902912378311157)
[2025-02-13 03:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27][root][INFO] - Training Epoch: 1/2, step 3963/7134 completed (loss: 0.055454518646001816, acc: 0.9853372573852539)
[2025-02-13 03:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27][root][INFO] - Training Epoch: 1/2, step 3964/7134 completed (loss: 0.07421629130840302, acc: 0.9814814925193787)
[2025-02-13 03:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:28][root][INFO] - Training Epoch: 1/2, step 3965/7134 completed (loss: 0.043440770357847214, acc: 0.9885550737380981)
[2025-02-13 03:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:28][root][INFO] - Training Epoch: 1/2, step 3966/7134 completed (loss: 0.024373088032007217, acc: 0.9927797913551331)
[2025-02-13 03:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:29][root][INFO] - Training Epoch: 1/2, step 3967/7134 completed (loss: 0.1191350445151329, acc: 0.9705372452735901)
[2025-02-13 03:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:29][root][INFO] - Training Epoch: 1/2, step 3968/7134 completed (loss: 0.01951332576572895, acc: 0.9921976327896118)
[2025-02-13 03:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:30][root][INFO] - Training Epoch: 1/2, step 3969/7134 completed (loss: 0.054432813078165054, acc: 0.9842932224273682)
[2025-02-13 03:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:30][root][INFO] - Training Epoch: 1/2, step 3970/7134 completed (loss: 0.0757562518119812, acc: 0.9899280667304993)
[2025-02-13 03:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:30][root][INFO] - Training Epoch: 1/2, step 3971/7134 completed (loss: 0.11429478228092194, acc: 0.9731457829475403)
[2025-02-13 03:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31][root][INFO] - Training Epoch: 1/2, step 3972/7134 completed (loss: 0.14250224828720093, acc: 0.9639175534248352)
[2025-02-13 03:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31][root][INFO] - Training Epoch: 1/2, step 3973/7134 completed (loss: 0.04085676372051239, acc: 0.9827288389205933)
[2025-02-13 03:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:32][root][INFO] - Training Epoch: 1/2, step 3974/7134 completed (loss: 0.11411979049444199, acc: 0.9701896905899048)
[2025-02-13 03:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:32][root][INFO] - Training Epoch: 1/2, step 3975/7134 completed (loss: 0.03712254762649536, acc: 0.9848942756652832)
[2025-02-13 03:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33][root][INFO] - Training Epoch: 1/2, step 3976/7134 completed (loss: 0.12235652655363083, acc: 0.9719763994216919)
[2025-02-13 03:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33][root][INFO] - Training Epoch: 1/2, step 3977/7134 completed (loss: 0.021889768540859222, acc: 0.9965217113494873)
[2025-02-13 03:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34][root][INFO] - Training Epoch: 1/2, step 3978/7134 completed (loss: 0.031002813950181007, acc: 0.9935317039489746)
[2025-02-13 03:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34][root][INFO] - Training Epoch: 1/2, step 3979/7134 completed (loss: 0.0562601201236248, acc: 0.9836065769195557)
[2025-02-13 03:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34][root][INFO] - Training Epoch: 1/2, step 3980/7134 completed (loss: 0.03270356357097626, acc: 0.9923780560493469)
[2025-02-13 03:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35][root][INFO] - Training Epoch: 1/2, step 3981/7134 completed (loss: 0.06522353738546371, acc: 0.9847095012664795)
[2025-02-13 03:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35][root][INFO] - Training Epoch: 1/2, step 3982/7134 completed (loss: 0.09048810601234436, acc: 0.9770444631576538)
[2025-02-13 03:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36][root][INFO] - Training Epoch: 1/2, step 3983/7134 completed (loss: 0.27118128538131714, acc: 0.9446808695793152)
[2025-02-13 03:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36][root][INFO] - Training Epoch: 1/2, step 3984/7134 completed (loss: 0.08938895165920258, acc: 0.9792208075523376)
[2025-02-13 03:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36][root][INFO] - Training Epoch: 1/2, step 3985/7134 completed (loss: 0.02236456796526909, acc: 0.9946523904800415)
[2025-02-13 03:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:37][root][INFO] - Training Epoch: 1/2, step 3986/7134 completed (loss: 0.18807202577590942, acc: 0.9575757384300232)
[2025-02-13 03:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:37][root][INFO] - Training Epoch: 1/2, step 3987/7134 completed (loss: 0.059589169919490814, acc: 0.9865642786026001)
[2025-02-13 03:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38][root][INFO] - Training Epoch: 1/2, step 3988/7134 completed (loss: 0.07768270373344421, acc: 0.9797724485397339)
[2025-02-13 03:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38][root][INFO] - Training Epoch: 1/2, step 3989/7134 completed (loss: 0.16497741639614105, acc: 0.9637526869773865)
[2025-02-13 03:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39][root][INFO] - Training Epoch: 1/2, step 3990/7134 completed (loss: 0.05512531101703644, acc: 0.9850746393203735)
[2025-02-13 03:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39][root][INFO] - Training Epoch: 1/2, step 3991/7134 completed (loss: 0.0502978079020977, acc: 0.9900990128517151)
[2025-02-13 03:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39][root][INFO] - Training Epoch: 1/2, step 3992/7134 completed (loss: 0.08703672885894775, acc: 0.9864864945411682)
[2025-02-13 03:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:40][root][INFO] - Training Epoch: 1/2, step 3993/7134 completed (loss: 0.08265544474124908, acc: 0.9793577790260315)
[2025-02-13 03:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:40][root][INFO] - Training Epoch: 1/2, step 3994/7134 completed (loss: 0.06955553591251373, acc: 0.9833333492279053)
[2025-02-13 03:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41][root][INFO] - Training Epoch: 1/2, step 3995/7134 completed (loss: 0.11737983673810959, acc: 0.9806201457977295)
[2025-02-13 03:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41][root][INFO] - Training Epoch: 1/2, step 3996/7134 completed (loss: 0.049801282584667206, acc: 0.9897330403327942)
[2025-02-13 03:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41][root][INFO] - Training Epoch: 1/2, step 3997/7134 completed (loss: 0.1675584763288498, acc: 0.9648351669311523)
[2025-02-13 03:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:42][root][INFO] - Training Epoch: 1/2, step 3998/7134 completed (loss: 0.10701411217451096, acc: 0.9737417697906494)
[2025-02-13 03:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:42][root][INFO] - Training Epoch: 1/2, step 3999/7134 completed (loss: 0.046383559703826904, acc: 0.9860279560089111)
[2025-02-13 03:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43][root][INFO] - Training Epoch: 1/2, step 4000/7134 completed (loss: 0.049518898129463196, acc: 0.9864864945411682)
[2025-02-13 03:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43][root][INFO] - Training Epoch: 1/2, step 4001/7134 completed (loss: 0.07802547514438629, acc: 0.9761431217193604)
[2025-02-13 03:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43][root][INFO] - Training Epoch: 1/2, step 4002/7134 completed (loss: 0.09093368798494339, acc: 0.9734939932823181)
[2025-02-13 03:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44][root][INFO] - Training Epoch: 1/2, step 4003/7134 completed (loss: 0.10254008322954178, acc: 0.9741697311401367)
[2025-02-13 03:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44][root][INFO] - Training Epoch: 1/2, step 4004/7134 completed (loss: 0.05529739707708359, acc: 0.9813519716262817)
[2025-02-13 03:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:45][root][INFO] - Training Epoch: 1/2, step 4005/7134 completed (loss: 0.02003335766494274, acc: 0.9952903985977173)
[2025-02-13 03:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:45][root][INFO] - Training Epoch: 1/2, step 4006/7134 completed (loss: 0.0418119952082634, acc: 0.9906347393989563)
[2025-02-13 03:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:45][root][INFO] - Training Epoch: 1/2, step 4007/7134 completed (loss: 0.04884324595332146, acc: 0.9910045266151428)
[2025-02-13 03:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:46][root][INFO] - Training Epoch: 1/2, step 4008/7134 completed (loss: 0.02708965539932251, acc: 0.9936575293540955)
[2025-02-13 03:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:46][root][INFO] - Training Epoch: 1/2, step 4009/7134 completed (loss: 0.017440753057599068, acc: 0.9960052967071533)
[2025-02-13 03:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47][root][INFO] - Training Epoch: 1/2, step 4010/7134 completed (loss: 0.013943277299404144, acc: 0.9955257177352905)
[2025-02-13 03:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47][root][INFO] - Training Epoch: 1/2, step 4011/7134 completed (loss: 0.03217267245054245, acc: 0.9933599233627319)
[2025-02-13 03:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:48][root][INFO] - Training Epoch: 1/2, step 4012/7134 completed (loss: 0.03112938441336155, acc: 0.9887005686759949)
[2025-02-13 03:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:48][root][INFO] - Training Epoch: 1/2, step 4013/7134 completed (loss: 0.005144516937434673, acc: 1.0)
[2025-02-13 03:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:49][root][INFO] - Training Epoch: 1/2, step 4014/7134 completed (loss: 0.010181465186178684, acc: 0.9952977895736694)
[2025-02-13 03:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:49][root][INFO] - Training Epoch: 1/2, step 4015/7134 completed (loss: 0.03500788286328316, acc: 0.9877111911773682)
[2025-02-13 03:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:49][root][INFO] - Training Epoch: 1/2, step 4016/7134 completed (loss: 0.03796760365366936, acc: 0.9885203838348389)
[2025-02-13 03:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:50][root][INFO] - Training Epoch: 1/2, step 4017/7134 completed (loss: 0.04018497094511986, acc: 0.991916835308075)
[2025-02-13 03:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:50][root][INFO] - Training Epoch: 1/2, step 4018/7134 completed (loss: 0.04530395567417145, acc: 0.9903030395507812)
[2025-02-13 03:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51][root][INFO] - Training Epoch: 1/2, step 4019/7134 completed (loss: 0.04873058572411537, acc: 0.9873096346855164)
[2025-02-13 03:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51][root][INFO] - Training Epoch: 1/2, step 4020/7134 completed (loss: 0.04407607018947601, acc: 0.9849726557731628)
[2025-02-13 03:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:52][root][INFO] - Training Epoch: 1/2, step 4021/7134 completed (loss: 0.029454905539751053, acc: 0.9893048405647278)
[2025-02-13 03:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:52][root][INFO] - Training Epoch: 1/2, step 4022/7134 completed (loss: 0.03183235973119736, acc: 0.9901840686798096)
[2025-02-13 03:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:52][root][INFO] - Training Epoch: 1/2, step 4023/7134 completed (loss: 0.055886149406433105, acc: 0.9836478233337402)
[2025-02-13 03:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:53][root][INFO] - Training Epoch: 1/2, step 4024/7134 completed (loss: 0.03173050656914711, acc: 0.990813672542572)
[2025-02-13 03:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:53][root][INFO] - Training Epoch: 1/2, step 4025/7134 completed (loss: 0.06392624974250793, acc: 0.9841449856758118)
[2025-02-13 03:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54][root][INFO] - Training Epoch: 1/2, step 4026/7134 completed (loss: 0.07698370516300201, acc: 0.9778761267662048)
[2025-02-13 03:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54][root][INFO] - Training Epoch: 1/2, step 4027/7134 completed (loss: 0.07464749366044998, acc: 0.9820144176483154)
[2025-02-13 03:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:55][root][INFO] - Training Epoch: 1/2, step 4028/7134 completed (loss: 0.04734225571155548, acc: 0.9862306118011475)
[2025-02-13 03:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:55][root][INFO] - Training Epoch: 1/2, step 4029/7134 completed (loss: 0.02217843011021614, acc: 0.9926199316978455)
[2025-02-13 03:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56][root][INFO] - Training Epoch: 1/2, step 4030/7134 completed (loss: 0.01973019354045391, acc: 0.9958932399749756)
[2025-02-13 03:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56][root][INFO] - Training Epoch: 1/2, step 4031/7134 completed (loss: 0.021909475326538086, acc: 0.992668628692627)
[2025-02-13 03:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56][root][INFO] - Training Epoch: 1/2, step 4032/7134 completed (loss: 0.06395305693149567, acc: 0.9825970530509949)
[2025-02-13 03:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57][root][INFO] - Training Epoch: 1/2, step 4033/7134 completed (loss: 0.016753245145082474, acc: 0.9949685335159302)
[2025-02-13 03:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57][root][INFO] - Training Epoch: 1/2, step 4034/7134 completed (loss: 0.00838226918131113, acc: 0.9975639581680298)
[2025-02-13 03:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58][root][INFO] - Training Epoch: 1/2, step 4035/7134 completed (loss: 0.047311920672655106, acc: 0.9858323335647583)
[2025-02-13 03:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58][root][INFO] - Training Epoch: 1/2, step 4036/7134 completed (loss: 0.04422912746667862, acc: 0.9890109896659851)
[2025-02-13 03:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:59][root][INFO] - Training Epoch: 1/2, step 4037/7134 completed (loss: 0.015067337080836296, acc: 0.9951865077018738)
[2025-02-13 03:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:59][root][INFO] - Training Epoch: 1/2, step 4038/7134 completed (loss: 0.039255540817976, acc: 0.9862448573112488)
[2025-02-13 03:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:59][root][INFO] - Training Epoch: 1/2, step 4039/7134 completed (loss: 0.036138083785772324, acc: 0.9900709390640259)
[2025-02-13 03:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:00][root][INFO] - Training Epoch: 1/2, step 4040/7134 completed (loss: 0.018134262412786484, acc: 0.9950739145278931)
[2025-02-13 03:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:00][root][INFO] - Training Epoch: 1/2, step 4041/7134 completed (loss: 0.03175768256187439, acc: 0.9932795763015747)
[2025-02-13 03:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01][root][INFO] - Training Epoch: 1/2, step 4042/7134 completed (loss: 0.03233158588409424, acc: 0.9926470518112183)
[2025-02-13 03:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01][root][INFO] - Training Epoch: 1/2, step 4043/7134 completed (loss: 0.02068120613694191, acc: 0.9911894202232361)
[2025-02-13 03:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:02][root][INFO] - Training Epoch: 1/2, step 4044/7134 completed (loss: 0.01647893525660038, acc: 0.9924242496490479)
[2025-02-13 03:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:02][root][INFO] - Training Epoch: 1/2, step 4045/7134 completed (loss: 0.0505029559135437, acc: 0.9847221970558167)
[2025-02-13 03:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:02][root][INFO] - Training Epoch: 1/2, step 4046/7134 completed (loss: 0.0360398143529892, acc: 0.9903846383094788)
[2025-02-13 03:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03][root][INFO] - Training Epoch: 1/2, step 4047/7134 completed (loss: 0.049946378916502, acc: 0.9916467666625977)
[2025-02-13 03:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03][root][INFO] - Training Epoch: 1/2, step 4048/7134 completed (loss: 0.022926995530724525, acc: 0.9941725134849548)
[2025-02-13 03:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:04][root][INFO] - Training Epoch: 1/2, step 4049/7134 completed (loss: 0.032732557505369186, acc: 0.9896551966667175)
[2025-02-13 03:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:04][root][INFO] - Training Epoch: 1/2, step 4050/7134 completed (loss: 0.051133837550878525, acc: 0.9886524677276611)
[2025-02-13 03:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05][root][INFO] - Training Epoch: 1/2, step 4051/7134 completed (loss: 0.07038744539022446, acc: 0.989234447479248)
[2025-02-13 03:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05][root][INFO] - Training Epoch: 1/2, step 4052/7134 completed (loss: 0.02194852940738201, acc: 0.9935400485992432)
[2025-02-13 03:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:06][root][INFO] - Training Epoch: 1/2, step 4053/7134 completed (loss: 0.049202632158994675, acc: 0.9840849041938782)
[2025-02-13 03:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:06][root][INFO] - Training Epoch: 1/2, step 4054/7134 completed (loss: 0.04472744092345238, acc: 0.9879999756813049)
[2025-02-13 03:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:06][root][INFO] - Training Epoch: 1/2, step 4055/7134 completed (loss: 0.03285924717783928, acc: 0.987864077091217)
[2025-02-13 03:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:07][root][INFO] - Training Epoch: 1/2, step 4056/7134 completed (loss: 0.026862164959311485, acc: 0.9878787994384766)
[2025-02-13 03:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:07][root][INFO] - Training Epoch: 1/2, step 4057/7134 completed (loss: 0.06068198010325432, acc: 0.9874826073646545)
[2025-02-13 03:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08][root][INFO] - Training Epoch: 1/2, step 4058/7134 completed (loss: 0.11749377846717834, acc: 0.971319317817688)
[2025-02-13 03:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08][root][INFO] - Training Epoch: 1/2, step 4059/7134 completed (loss: 0.040312789380550385, acc: 0.989051103591919)
[2025-02-13 03:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:09][root][INFO] - Training Epoch: 1/2, step 4060/7134 completed (loss: 0.03455870598554611, acc: 0.989180862903595)
[2025-02-13 03:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:09][root][INFO] - Training Epoch: 1/2, step 4061/7134 completed (loss: 0.02086111158132553, acc: 0.9928774833679199)
[2025-02-13 03:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:09][root][INFO] - Training Epoch: 1/2, step 4062/7134 completed (loss: 0.013665137812495232, acc: 0.995275616645813)
[2025-02-13 03:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:10][root][INFO] - Training Epoch: 1/2, step 4063/7134 completed (loss: 0.015064259991049767, acc: 0.9964664578437805)
[2025-02-13 03:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:10][root][INFO] - Training Epoch: 1/2, step 4064/7134 completed (loss: 0.015757327899336815, acc: 0.9954614043235779)
[2025-02-13 03:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11][root][INFO] - Training Epoch: 1/2, step 4065/7134 completed (loss: 0.02451157383620739, acc: 0.9944827556610107)
[2025-02-13 03:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11][root][INFO] - Training Epoch: 1/2, step 4066/7134 completed (loss: 0.009571828879415989, acc: 0.9986110925674438)
[2025-02-13 03:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:12][root][INFO] - Training Epoch: 1/2, step 4067/7134 completed (loss: 0.04741574451327324, acc: 0.9901960492134094)
[2025-02-13 03:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:12][root][INFO] - Training Epoch: 1/2, step 4068/7134 completed (loss: 0.05115879327058792, acc: 0.9888888597488403)
[2025-02-13 03:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:12][root][INFO] - Training Epoch: 1/2, step 4069/7134 completed (loss: 0.0344826877117157, acc: 0.9937402009963989)
[2025-02-13 03:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:13][root][INFO] - Training Epoch: 1/2, step 4070/7134 completed (loss: 0.027647679671645164, acc: 0.9923896789550781)
[2025-02-13 03:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:13][root][INFO] - Training Epoch: 1/2, step 4071/7134 completed (loss: 0.048903778195381165, acc: 0.9851729869842529)
[2025-02-13 03:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:14][root][INFO] - Training Epoch: 1/2, step 4072/7134 completed (loss: 0.012356709688901901, acc: 0.9985569715499878)
[2025-02-13 03:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:14][root][INFO] - Training Epoch: 1/2, step 4073/7134 completed (loss: 0.02472897246479988, acc: 0.9942528605461121)
[2025-02-13 03:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:14][root][INFO] - Training Epoch: 1/2, step 4074/7134 completed (loss: 0.03256756812334061, acc: 0.9942775368690491)
[2025-02-13 03:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15][root][INFO] - Training Epoch: 1/2, step 4075/7134 completed (loss: 0.020197754725813866, acc: 0.9925037622451782)
[2025-02-13 03:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15][root][INFO] - Training Epoch: 1/2, step 4076/7134 completed (loss: 0.013400526717305183, acc: 0.9956584572792053)
[2025-02-13 03:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:16][root][INFO] - Training Epoch: 1/2, step 4077/7134 completed (loss: 0.04458508640527725, acc: 0.9881556630134583)
[2025-02-13 03:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:16][root][INFO] - Training Epoch: 1/2, step 4078/7134 completed (loss: 0.029727740213274956, acc: 0.9928774833679199)
[2025-02-13 03:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:17][root][INFO] - Training Epoch: 1/2, step 4079/7134 completed (loss: 0.02377278357744217, acc: 0.9964349269866943)
[2025-02-13 03:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:17][root][INFO] - Training Epoch: 1/2, step 4080/7134 completed (loss: 0.019103191792964935, acc: 0.9920886158943176)
[2025-02-13 03:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:17][root][INFO] - Training Epoch: 1/2, step 4081/7134 completed (loss: 0.008690795861184597, acc: 0.9983498454093933)
[2025-02-13 03:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18][root][INFO] - Training Epoch: 1/2, step 4082/7134 completed (loss: 0.061589669436216354, acc: 0.975970447063446)
[2025-02-13 03:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18][root][INFO] - Training Epoch: 1/2, step 4083/7134 completed (loss: 0.05376475304365158, acc: 0.985029935836792)
[2025-02-13 03:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:19][root][INFO] - Training Epoch: 1/2, step 4084/7134 completed (loss: 0.08452469110488892, acc: 0.9647058844566345)
[2025-02-13 03:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:19][root][INFO] - Training Epoch: 1/2, step 4085/7134 completed (loss: 0.0463411770761013, acc: 0.9851751923561096)
[2025-02-13 03:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:20][root][INFO] - Training Epoch: 1/2, step 4086/7134 completed (loss: 0.07481527328491211, acc: 0.978723406791687)
[2025-02-13 03:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:20][root][INFO] - Training Epoch: 1/2, step 4087/7134 completed (loss: 0.021850453689694405, acc: 0.9974160194396973)
[2025-02-13 03:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:20][root][INFO] - Training Epoch: 1/2, step 4088/7134 completed (loss: 0.08579398691654205, acc: 0.9751243591308594)
[2025-02-13 03:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21][root][INFO] - Training Epoch: 1/2, step 4089/7134 completed (loss: 0.0776689350605011, acc: 0.9733570218086243)
[2025-02-13 03:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21][root][INFO] - Training Epoch: 1/2, step 4090/7134 completed (loss: 0.10078240185976028, acc: 0.9775725603103638)
[2025-02-13 03:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:22][root][INFO] - Training Epoch: 1/2, step 4091/7134 completed (loss: 0.12452605366706848, acc: 0.9713321924209595)
[2025-02-13 03:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:22][root][INFO] - Training Epoch: 1/2, step 4092/7134 completed (loss: 0.05349990725517273, acc: 0.9871299862861633)
[2025-02-13 03:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23][root][INFO] - Training Epoch: 1/2, step 4093/7134 completed (loss: 0.03149646520614624, acc: 0.9927140474319458)
[2025-02-13 03:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23][root][INFO] - Training Epoch: 1/2, step 4094/7134 completed (loss: 0.0813010185956955, acc: 0.9804804921150208)
[2025-02-13 03:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23][root][INFO] - Training Epoch: 1/2, step 4095/7134 completed (loss: 0.029499385505914688, acc: 0.9913169145584106)
[2025-02-13 03:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:24][root][INFO] - Training Epoch: 1/2, step 4096/7134 completed (loss: 0.10194222629070282, acc: 0.9850746393203735)
[2025-02-13 03:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:24][root][INFO] - Training Epoch: 1/2, step 4097/7134 completed (loss: 0.06410597264766693, acc: 0.9870967864990234)
[2025-02-13 03:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25][root][INFO] - Training Epoch: 1/2, step 4098/7134 completed (loss: 0.04554017633199692, acc: 0.9878048896789551)
[2025-02-13 03:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25][root][INFO] - Training Epoch: 1/2, step 4099/7134 completed (loss: 0.05546003580093384, acc: 0.98591548204422)
[2025-02-13 03:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26][root][INFO] - Training Epoch: 1/2, step 4100/7134 completed (loss: 0.010006507858633995, acc: 0.9954614043235779)
[2025-02-13 03:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26][root][INFO] - Training Epoch: 1/2, step 4101/7134 completed (loss: 0.016168585047125816, acc: 0.9942965507507324)
[2025-02-13 03:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26][root][INFO] - Training Epoch: 1/2, step 4102/7134 completed (loss: 0.03179294243454933, acc: 0.9853747487068176)
[2025-02-13 03:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:27][root][INFO] - Training Epoch: 1/2, step 4103/7134 completed (loss: 0.009043159894645214, acc: 0.9985052347183228)
[2025-02-13 03:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:27][root][INFO] - Training Epoch: 1/2, step 4104/7134 completed (loss: 0.018349552527070045, acc: 0.9949238300323486)
[2025-02-13 03:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28][root][INFO] - Training Epoch: 1/2, step 4105/7134 completed (loss: 0.027825482189655304, acc: 0.995468258857727)
[2025-02-13 03:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28][root][INFO] - Training Epoch: 1/2, step 4106/7134 completed (loss: 0.027353404089808464, acc: 0.9923312664031982)
[2025-02-13 03:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:29][root][INFO] - Training Epoch: 1/2, step 4107/7134 completed (loss: 0.03187539055943489, acc: 0.9949579834938049)
[2025-02-13 03:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:29][root][INFO] - Training Epoch: 1/2, step 4108/7134 completed (loss: 0.03437712416052818, acc: 0.9902912378311157)
[2025-02-13 03:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:29][root][INFO] - Training Epoch: 1/2, step 4109/7134 completed (loss: 0.016942864283919334, acc: 0.9935794472694397)
[2025-02-13 03:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30][root][INFO] - Training Epoch: 1/2, step 4110/7134 completed (loss: 0.019718782976269722, acc: 0.9948979616165161)
[2025-02-13 03:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30][root][INFO] - Training Epoch: 1/2, step 4111/7134 completed (loss: 0.017747916281223297, acc: 0.9921135902404785)
[2025-02-13 03:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:31][root][INFO] - Training Epoch: 1/2, step 4112/7134 completed (loss: 0.036084357649087906, acc: 0.9928315281867981)
[2025-02-13 03:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:31][root][INFO] - Training Epoch: 1/2, step 4113/7134 completed (loss: 0.040299639105796814, acc: 0.9906976819038391)
[2025-02-13 03:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32][root][INFO] - Training Epoch: 1/2, step 4114/7134 completed (loss: 0.11087509989738464, acc: 0.9735772609710693)
[2025-02-13 03:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32][root][INFO] - Training Epoch: 1/2, step 4115/7134 completed (loss: 0.08469818532466888, acc: 0.9813432693481445)
[2025-02-13 03:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32][root][INFO] - Training Epoch: 1/2, step 4116/7134 completed (loss: 0.019656125456094742, acc: 0.9951140284538269)
[2025-02-13 03:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:33][root][INFO] - Training Epoch: 1/2, step 4117/7134 completed (loss: 0.029200134798884392, acc: 0.988959014415741)
[2025-02-13 03:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:33][root][INFO] - Training Epoch: 1/2, step 4118/7134 completed (loss: 0.01967742294073105, acc: 0.9950860142707825)
[2025-02-13 03:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34][root][INFO] - Training Epoch: 1/2, step 4119/7134 completed (loss: 0.04675043001770973, acc: 0.9854604005813599)
[2025-02-13 03:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34][root][INFO] - Training Epoch: 1/2, step 4120/7134 completed (loss: 0.04164695367217064, acc: 0.9934102296829224)
[2025-02-13 03:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34][root][INFO] - Training Epoch: 1/2, step 4121/7134 completed (loss: 0.050408829003572464, acc: 0.9886363744735718)
[2025-02-13 03:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35][root][INFO] - Training Epoch: 1/2, step 4122/7134 completed (loss: 0.024804428219795227, acc: 0.9923664331436157)
[2025-02-13 03:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35][root][INFO] - Training Epoch: 1/2, step 4123/7134 completed (loss: 0.03951132670044899, acc: 0.9924585223197937)
[2025-02-13 03:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:36][root][INFO] - Training Epoch: 1/2, step 4124/7134 completed (loss: 0.03267495706677437, acc: 0.9929278492927551)
[2025-02-13 03:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:36][root][INFO] - Training Epoch: 1/2, step 4125/7134 completed (loss: 0.02183358557522297, acc: 0.9941176176071167)
[2025-02-13 03:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37][root][INFO] - Training Epoch: 1/2, step 4126/7134 completed (loss: 0.012534487061202526, acc: 0.9957507252693176)
[2025-02-13 03:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37][root][INFO] - Training Epoch: 1/2, step 4127/7134 completed (loss: 0.033926986157894135, acc: 0.9921875)
[2025-02-13 03:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37][root][INFO] - Training Epoch: 1/2, step 4128/7134 completed (loss: 0.02566594071686268, acc: 0.9937402009963989)
[2025-02-13 03:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:38][root][INFO] - Training Epoch: 1/2, step 4129/7134 completed (loss: 0.019453784450888634, acc: 0.9940652847290039)
[2025-02-13 03:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:38][root][INFO] - Training Epoch: 1/2, step 4130/7134 completed (loss: 0.03233795613050461, acc: 0.9922928810119629)
[2025-02-13 03:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39][root][INFO] - Training Epoch: 1/2, step 4131/7134 completed (loss: 0.02069879323244095, acc: 0.9944547414779663)
[2025-02-13 03:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39][root][INFO] - Training Epoch: 1/2, step 4132/7134 completed (loss: 0.02738899178802967, acc: 0.9866071343421936)
[2025-02-13 03:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39][root][INFO] - Training Epoch: 1/2, step 4133/7134 completed (loss: 0.04335523396730423, acc: 0.9833333492279053)
[2025-02-13 03:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:40][root][INFO] - Training Epoch: 1/2, step 4134/7134 completed (loss: 0.06475581973791122, acc: 0.9691942930221558)
[2025-02-13 03:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:40][root][INFO] - Training Epoch: 1/2, step 4135/7134 completed (loss: 0.020953459665179253, acc: 0.9911190271377563)
[2025-02-13 03:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41][root][INFO] - Training Epoch: 1/2, step 4136/7134 completed (loss: 0.019867531955242157, acc: 0.9929203391075134)
[2025-02-13 03:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41][root][INFO] - Training Epoch: 1/2, step 4137/7134 completed (loss: 0.04015064612030983, acc: 0.986328125)
[2025-02-13 03:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41][root][INFO] - Training Epoch: 1/2, step 4138/7134 completed (loss: 0.04618515446782112, acc: 0.9843260049819946)
[2025-02-13 03:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42][root][INFO] - Training Epoch: 1/2, step 4139/7134 completed (loss: 0.021739251911640167, acc: 0.9935622215270996)
[2025-02-13 03:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42][root][INFO] - Training Epoch: 1/2, step 4140/7134 completed (loss: 0.04655058681964874, acc: 0.9834482669830322)
[2025-02-13 03:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:43][root][INFO] - Training Epoch: 1/2, step 4141/7134 completed (loss: 0.07188980281352997, acc: 0.9799426794052124)
[2025-02-13 03:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:43][root][INFO] - Training Epoch: 1/2, step 4142/7134 completed (loss: 0.09210282564163208, acc: 0.9771167039871216)
[2025-02-13 03:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:44][root][INFO] - Training Epoch: 1/2, step 4143/7134 completed (loss: 0.04877367243170738, acc: 0.9887754917144775)
[2025-02-13 03:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:44][root][INFO] - Training Epoch: 1/2, step 4144/7134 completed (loss: 0.08848048001527786, acc: 0.9738041162490845)
[2025-02-13 03:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45][root][INFO] - Training Epoch: 1/2, step 4145/7134 completed (loss: 0.044576406478881836, acc: 0.9855595827102661)
[2025-02-13 03:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45][root][INFO] - Training Epoch: 1/2, step 4146/7134 completed (loss: 0.10329732298851013, acc: 0.9797979593276978)
[2025-02-13 03:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45][root][INFO] - Training Epoch: 1/2, step 4147/7134 completed (loss: 0.11658972501754761, acc: 0.9655172228813171)
[2025-02-13 03:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:46][root][INFO] - Training Epoch: 1/2, step 4148/7134 completed (loss: 0.13534729182720184, acc: 0.96138995885849)
[2025-02-13 03:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:46][root][INFO] - Training Epoch: 1/2, step 4149/7134 completed (loss: 0.1380331665277481, acc: 0.9599999785423279)
[2025-02-13 03:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47][root][INFO] - Training Epoch: 1/2, step 4150/7134 completed (loss: 0.134373277425766, acc: 0.9678068161010742)
[2025-02-13 03:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47][root][INFO] - Training Epoch: 1/2, step 4151/7134 completed (loss: 0.17366862297058105, acc: 0.9655796885490417)
[2025-02-13 03:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:48][root][INFO] - Training Epoch: 1/2, step 4152/7134 completed (loss: 0.05591390281915665, acc: 0.9783616662025452)
[2025-02-13 03:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:48][root][INFO] - Training Epoch: 1/2, step 4153/7134 completed (loss: 0.09608176350593567, acc: 0.9785276055335999)
[2025-02-13 03:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49][root][INFO] - Training Epoch: 1/2, step 4154/7134 completed (loss: 0.06555412709712982, acc: 0.9814814925193787)
[2025-02-13 03:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49][root][INFO] - Training Epoch: 1/2, step 4155/7134 completed (loss: 0.08553026616573334, acc: 0.9725557565689087)
[2025-02-13 03:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49][root][INFO] - Training Epoch: 1/2, step 4156/7134 completed (loss: 0.08490084111690521, acc: 0.977746844291687)
[2025-02-13 03:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:50][root][INFO] - Training Epoch: 1/2, step 4157/7134 completed (loss: 0.06292149424552917, acc: 0.9820359349250793)
[2025-02-13 03:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:50][root][INFO] - Training Epoch: 1/2, step 4158/7134 completed (loss: 0.20598049461841583, acc: 0.9562981724739075)
[2025-02-13 03:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:51][root][INFO] - Training Epoch: 1/2, step 4159/7134 completed (loss: 0.051532067358493805, acc: 0.9813242554664612)
[2025-02-13 03:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:51][root][INFO] - Training Epoch: 1/2, step 4160/7134 completed (loss: 0.08311371505260468, acc: 0.9660377502441406)
[2025-02-13 03:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52][root][INFO] - Training Epoch: 1/2, step 4161/7134 completed (loss: 0.04303690046072006, acc: 0.9899598360061646)
[2025-02-13 03:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52][root][INFO] - Training Epoch: 1/2, step 4162/7134 completed (loss: 0.07616803050041199, acc: 0.9836065769195557)
[2025-02-13 03:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52][root][INFO] - Training Epoch: 1/2, step 4163/7134 completed (loss: 0.0736326277256012, acc: 0.9848713874816895)
[2025-02-13 03:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:53][root][INFO] - Training Epoch: 1/2, step 4164/7134 completed (loss: 0.05599291995167732, acc: 0.9825834631919861)
[2025-02-13 03:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:53][root][INFO] - Training Epoch: 1/2, step 4165/7134 completed (loss: 0.061715953052043915, acc: 0.9825218319892883)
[2025-02-13 03:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54][root][INFO] - Training Epoch: 1/2, step 4166/7134 completed (loss: 0.05695762112736702, acc: 0.9794420003890991)
[2025-02-13 03:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54][root][INFO] - Training Epoch: 1/2, step 4167/7134 completed (loss: 0.017127640545368195, acc: 0.9965277910232544)
[2025-02-13 03:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:55][root][INFO] - Training Epoch: 1/2, step 4168/7134 completed (loss: 0.04082178324460983, acc: 0.9895833134651184)
[2025-02-13 03:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:55][root][INFO] - Training Epoch: 1/2, step 4169/7134 completed (loss: 0.0684833899140358, acc: 0.9862724542617798)
[2025-02-13 03:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:56][root][INFO] - Training Epoch: 1/2, step 4170/7134 completed (loss: 0.06161505728960037, acc: 0.9808917045593262)
[2025-02-13 03:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:56][root][INFO] - Training Epoch: 1/2, step 4171/7134 completed (loss: 0.037060197442770004, acc: 0.9896193742752075)
[2025-02-13 03:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:57][root][INFO] - Training Epoch: 1/2, step 4172/7134 completed (loss: 0.0621919222176075, acc: 0.9887780547142029)
[2025-02-13 03:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:57][root][INFO] - Training Epoch: 1/2, step 4173/7134 completed (loss: 0.07877010852098465, acc: 0.9828326106071472)
[2025-02-13 03:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:57][root][INFO] - Training Epoch: 1/2, step 4174/7134 completed (loss: 0.013970864936709404, acc: 0.9961685538291931)
[2025-02-13 03:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58][root][INFO] - Training Epoch: 1/2, step 4175/7134 completed (loss: 0.0324447862803936, acc: 0.993127167224884)
[2025-02-13 03:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58][root][INFO] - Training Epoch: 1/2, step 4176/7134 completed (loss: 0.024952037259936333, acc: 0.9910581111907959)
[2025-02-13 03:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59][root][INFO] - Training Epoch: 1/2, step 4177/7134 completed (loss: 0.02050253190100193, acc: 0.9947437644004822)
[2025-02-13 03:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59][root][INFO] - Training Epoch: 1/2, step 4178/7134 completed (loss: 0.02739657461643219, acc: 0.9924471378326416)
[2025-02-13 03:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:00][root][INFO] - Training Epoch: 1/2, step 4179/7134 completed (loss: 0.016994696110486984, acc: 0.9921259880065918)
[2025-02-13 03:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:00][root][INFO] - Training Epoch: 1/2, step 4180/7134 completed (loss: 0.012400621548295021, acc: 0.9957627058029175)
[2025-02-13 03:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01][root][INFO] - Training Epoch: 1/2, step 4181/7134 completed (loss: 0.009044816717505455, acc: 0.9967373609542847)
[2025-02-13 03:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01][root][INFO] - Training Epoch: 1/2, step 4182/7134 completed (loss: 0.005927633494138718, acc: 1.0)
[2025-02-13 03:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01][root][INFO] - Training Epoch: 1/2, step 4183/7134 completed (loss: 0.022198881953954697, acc: 0.9916550517082214)
[2025-02-13 03:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:02][root][INFO] - Training Epoch: 1/2, step 4184/7134 completed (loss: 0.046352315694093704, acc: 0.988063633441925)
[2025-02-13 03:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:02][root][INFO] - Training Epoch: 1/2, step 4185/7134 completed (loss: 0.013177141547203064, acc: 0.9977169036865234)
[2025-02-13 03:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03][root][INFO] - Training Epoch: 1/2, step 4186/7134 completed (loss: 0.07399328798055649, acc: 0.9879662990570068)
[2025-02-13 03:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03][root][INFO] - Training Epoch: 1/2, step 4187/7134 completed (loss: 0.017238561064004898, acc: 0.99609375)
[2025-02-13 03:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:04][root][INFO] - Training Epoch: 1/2, step 4188/7134 completed (loss: 0.01192526612430811, acc: 0.9950000047683716)
[2025-02-13 03:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:04][root][INFO] - Training Epoch: 1/2, step 4189/7134 completed (loss: 0.023619119077920914, acc: 0.9958041906356812)
[2025-02-13 03:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:05][root][INFO] - Training Epoch: 1/2, step 4190/7134 completed (loss: 0.02573676034808159, acc: 0.9943181872367859)
[2025-02-13 03:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:05][root][INFO] - Training Epoch: 1/2, step 4191/7134 completed (loss: 0.03755790367722511, acc: 0.9907833933830261)
[2025-02-13 03:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:05][root][INFO] - Training Epoch: 1/2, step 4192/7134 completed (loss: 0.019977226853370667, acc: 0.9936102032661438)
[2025-02-13 03:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:06][root][INFO] - Training Epoch: 1/2, step 4193/7134 completed (loss: 0.02035403996706009, acc: 0.9959893226623535)
[2025-02-13 03:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:06][root][INFO] - Training Epoch: 1/2, step 4194/7134 completed (loss: 0.012284258380532265, acc: 0.9954614043235779)
[2025-02-13 03:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07][root][INFO] - Training Epoch: 1/2, step 4195/7134 completed (loss: 0.04792659357190132, acc: 0.9878048896789551)
[2025-02-13 03:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07][root][INFO] - Training Epoch: 1/2, step 4196/7134 completed (loss: 0.023591775447130203, acc: 0.9930796027183533)
[2025-02-13 03:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08][root][INFO] - Training Epoch: 1/2, step 4197/7134 completed (loss: 0.028980271890759468, acc: 0.9922380447387695)
[2025-02-13 03:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08][root][INFO] - Training Epoch: 1/2, step 4198/7134 completed (loss: 0.022524375468492508, acc: 0.9944444298744202)
[2025-02-13 03:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08][root][INFO] - Training Epoch: 1/2, step 4199/7134 completed (loss: 0.04957563430070877, acc: 0.9872262477874756)
[2025-02-13 03:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:09][root][INFO] - Training Epoch: 1/2, step 4200/7134 completed (loss: 0.05015832558274269, acc: 0.9874551892280579)
[2025-02-13 03:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:09][root][INFO] - Training Epoch: 1/2, step 4201/7134 completed (loss: 0.01136722881346941, acc: 0.9965753555297852)
[2025-02-13 03:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10][root][INFO] - Training Epoch: 1/2, step 4202/7134 completed (loss: 0.05682564154267311, acc: 0.9864341020584106)
[2025-02-13 03:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10][root][INFO] - Training Epoch: 1/2, step 4203/7134 completed (loss: 0.05639823526144028, acc: 0.9859594106674194)
[2025-02-13 03:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11][root][INFO] - Training Epoch: 1/2, step 4204/7134 completed (loss: 0.09055563062429428, acc: 0.9795321822166443)
[2025-02-13 03:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11][root][INFO] - Training Epoch: 1/2, step 4205/7134 completed (loss: 0.051309987902641296, acc: 0.9870503544807434)
[2025-02-13 03:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11][root][INFO] - Training Epoch: 1/2, step 4206/7134 completed (loss: 0.07155025005340576, acc: 0.9830747246742249)
[2025-02-13 03:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12][root][INFO] - Training Epoch: 1/2, step 4207/7134 completed (loss: 0.051555853337049484, acc: 0.9920381903648376)
[2025-02-13 03:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12][root][INFO] - Training Epoch: 1/2, step 4208/7134 completed (loss: 0.06548627465963364, acc: 0.9872408509254456)
[2025-02-13 03:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:13][root][INFO] - Training Epoch: 1/2, step 4209/7134 completed (loss: 0.0683027058839798, acc: 0.980169951915741)
[2025-02-13 03:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:13][root][INFO] - Training Epoch: 1/2, step 4210/7134 completed (loss: 0.011087998747825623, acc: 0.9979550242424011)
[2025-02-13 03:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14][root][INFO] - Training Epoch: 1/2, step 4211/7134 completed (loss: 0.044100698083639145, acc: 0.9861751198768616)
[2025-02-13 03:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14][root][INFO] - Training Epoch: 1/2, step 4212/7134 completed (loss: 0.03398929536342621, acc: 0.9901800155639648)
[2025-02-13 03:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14][root][INFO] - Training Epoch: 1/2, step 4213/7134 completed (loss: 0.01837734505534172, acc: 0.9942775368690491)
[2025-02-13 03:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:15][root][INFO] - Training Epoch: 1/2, step 4214/7134 completed (loss: 0.061875391751527786, acc: 0.9820895791053772)
[2025-02-13 03:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:15][root][INFO] - Training Epoch: 1/2, step 4215/7134 completed (loss: 0.05467101186513901, acc: 0.9890109896659851)
[2025-02-13 03:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:16][root][INFO] - Training Epoch: 1/2, step 4216/7134 completed (loss: 0.054654255509376526, acc: 0.9872029423713684)
[2025-02-13 03:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:16][root][INFO] - Training Epoch: 1/2, step 4217/7134 completed (loss: 0.02501554600894451, acc: 0.9902098178863525)
[2025-02-13 03:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:16][root][INFO] - Training Epoch: 1/2, step 4218/7134 completed (loss: 0.05389404296875, acc: 0.985567033290863)
[2025-02-13 03:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17][root][INFO] - Training Epoch: 1/2, step 4219/7134 completed (loss: 0.08322745561599731, acc: 0.9839357137680054)
[2025-02-13 03:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17][root][INFO] - Training Epoch: 1/2, step 4220/7134 completed (loss: 0.04614030569791794, acc: 0.9913669228553772)
[2025-02-13 03:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:18][root][INFO] - Training Epoch: 1/2, step 4221/7134 completed (loss: 0.03811207413673401, acc: 0.9858406782150269)
[2025-02-13 03:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:18][root][INFO] - Training Epoch: 1/2, step 4222/7134 completed (loss: 0.1300463229417801, acc: 0.9800000190734863)
[2025-02-13 03:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19][root][INFO] - Training Epoch: 1/2, step 4223/7134 completed (loss: 0.0466318354010582, acc: 0.9867549538612366)
[2025-02-13 03:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19][root][INFO] - Training Epoch: 1/2, step 4224/7134 completed (loss: 0.07612580060958862, acc: 0.9827213883399963)
[2025-02-13 03:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19][root][INFO] - Training Epoch: 1/2, step 4225/7134 completed (loss: 0.09184058010578156, acc: 0.979626476764679)
[2025-02-13 03:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:20][root][INFO] - Training Epoch: 1/2, step 4226/7134 completed (loss: 0.05816050246357918, acc: 0.9859514832496643)
[2025-02-13 03:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:20][root][INFO] - Training Epoch: 1/2, step 4227/7134 completed (loss: 0.12658460438251495, acc: 0.9631449580192566)
[2025-02-13 03:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21][root][INFO] - Training Epoch: 1/2, step 4228/7134 completed (loss: 0.14567549526691437, acc: 0.9741379022598267)
[2025-02-13 03:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21][root][INFO] - Training Epoch: 1/2, step 4229/7134 completed (loss: 0.09010838717222214, acc: 0.9769392013549805)
[2025-02-13 03:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21][root][INFO] - Training Epoch: 1/2, step 4230/7134 completed (loss: 0.08159109950065613, acc: 0.9764705896377563)
[2025-02-13 03:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22][root][INFO] - Training Epoch: 1/2, step 4231/7134 completed (loss: 0.17001372575759888, acc: 0.9631676077842712)
[2025-02-13 03:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22][root][INFO] - Training Epoch: 1/2, step 4232/7134 completed (loss: 0.11579468846321106, acc: 0.9725190997123718)
[2025-02-13 03:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:23][root][INFO] - Training Epoch: 1/2, step 4233/7134 completed (loss: 0.09690185636281967, acc: 0.9799666404724121)
[2025-02-13 03:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:23][root][INFO] - Training Epoch: 1/2, step 4234/7134 completed (loss: 0.06923498213291168, acc: 0.9841017723083496)
[2025-02-13 03:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24][root][INFO] - Training Epoch: 1/2, step 4235/7134 completed (loss: 0.04835144057869911, acc: 0.9873617887496948)
[2025-02-13 03:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24][root][INFO] - Training Epoch: 1/2, step 4236/7134 completed (loss: 0.020551754161715508, acc: 1.0)
[2025-02-13 03:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24][root][INFO] - Training Epoch: 1/2, step 4237/7134 completed (loss: 0.04473912715911865, acc: 0.9876288771629333)
[2025-02-13 03:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25][root][INFO] - Training Epoch: 1/2, step 4238/7134 completed (loss: 0.06149635463953018, acc: 0.9918166995048523)
[2025-02-13 03:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25][root][INFO] - Training Epoch: 1/2, step 4239/7134 completed (loss: 0.03170343488454819, acc: 0.9919999837875366)
[2025-02-13 03:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26][root][INFO] - Training Epoch: 1/2, step 4240/7134 completed (loss: 0.050655022263526917, acc: 0.987075924873352)
[2025-02-13 03:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26][root][INFO] - Training Epoch: 1/2, step 4241/7134 completed (loss: 0.018787620589137077, acc: 0.9956616163253784)
[2025-02-13 03:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26][root][INFO] - Training Epoch: 1/2, step 4242/7134 completed (loss: 0.05543327331542969, acc: 0.9866369962692261)
[2025-02-13 03:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:27][root][INFO] - Training Epoch: 1/2, step 4243/7134 completed (loss: 0.07084362953901291, acc: 0.9891501069068909)
[2025-02-13 03:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:27][root][INFO] - Training Epoch: 1/2, step 4244/7134 completed (loss: 0.049454085528850555, acc: 0.9866962432861328)
[2025-02-13 03:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:28][root][INFO] - Training Epoch: 1/2, step 4245/7134 completed (loss: 0.037512894719839096, acc: 0.9926650524139404)
[2025-02-13 03:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:28][root][INFO] - Training Epoch: 1/2, step 4246/7134 completed (loss: 0.05171927437186241, acc: 0.9889065027236938)
[2025-02-13 03:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:28][root][INFO] - Training Epoch: 1/2, step 4247/7134 completed (loss: 0.030154312029480934, acc: 0.987522304058075)
[2025-02-13 03:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29][root][INFO] - Training Epoch: 1/2, step 4248/7134 completed (loss: 0.06246575340628624, acc: 0.9839743375778198)
[2025-02-13 03:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29][root][INFO] - Training Epoch: 1/2, step 4249/7134 completed (loss: 0.0618472695350647, acc: 0.9873015880584717)
[2025-02-13 03:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:30][root][INFO] - Training Epoch: 1/2, step 4250/7134 completed (loss: 0.0793280377984047, acc: 0.9830148816108704)
[2025-02-13 03:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:30][root][INFO] - Training Epoch: 1/2, step 4251/7134 completed (loss: 0.02119293436408043, acc: 0.9950658082962036)
[2025-02-13 03:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:31][root][INFO] - Training Epoch: 1/2, step 4252/7134 completed (loss: 0.052366435527801514, acc: 0.9825119376182556)
[2025-02-13 03:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:31][root][INFO] - Training Epoch: 1/2, step 4253/7134 completed (loss: 0.047483786940574646, acc: 0.9913232326507568)
[2025-02-13 03:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:31][root][INFO] - Training Epoch: 1/2, step 4254/7134 completed (loss: 0.028228381648659706, acc: 0.9925187230110168)
[2025-02-13 03:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32][root][INFO] - Training Epoch: 1/2, step 4255/7134 completed (loss: 0.03634626045823097, acc: 0.9897260069847107)
[2025-02-13 03:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32][root][INFO] - Training Epoch: 1/2, step 4256/7134 completed (loss: 0.0424199253320694, acc: 0.9886877536773682)
[2025-02-13 03:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:33][root][INFO] - Training Epoch: 1/2, step 4257/7134 completed (loss: 0.027113061398267746, acc: 0.9910873174667358)
[2025-02-13 03:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:33][root][INFO] - Training Epoch: 1/2, step 4258/7134 completed (loss: 0.013796796090900898, acc: 0.9961832165718079)
[2025-02-13 03:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34][root][INFO] - Training Epoch: 1/2, step 4259/7134 completed (loss: 0.09363623708486557, acc: 0.9756447076797485)
[2025-02-13 03:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34][root][INFO] - Training Epoch: 1/2, step 4260/7134 completed (loss: 0.023261506110429764, acc: 0.9917184114456177)
[2025-02-13 03:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34][root][INFO] - Training Epoch: 1/2, step 4261/7134 completed (loss: 0.030800864100456238, acc: 0.9923518300056458)
[2025-02-13 03:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:35][root][INFO] - Training Epoch: 1/2, step 4262/7134 completed (loss: 0.028723515570163727, acc: 0.9906103014945984)
[2025-02-13 03:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:35][root][INFO] - Training Epoch: 1/2, step 4263/7134 completed (loss: 0.07595311105251312, acc: 0.9864864945411682)
[2025-02-13 03:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36][root][INFO] - Training Epoch: 1/2, step 4264/7134 completed (loss: 0.08485963195562363, acc: 0.9774718284606934)
[2025-02-13 03:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36][root][INFO] - Training Epoch: 1/2, step 4265/7134 completed (loss: 0.038832779973745346, acc: 0.9898989796638489)
[2025-02-13 03:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:37][root][INFO] - Training Epoch: 1/2, step 4266/7134 completed (loss: 0.034173596650362015, acc: 0.9940119981765747)
[2025-02-13 03:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:37][root][INFO] - Training Epoch: 1/2, step 4267/7134 completed (loss: 0.03127109259366989, acc: 0.9915151596069336)
[2025-02-13 03:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38][root][INFO] - Training Epoch: 1/2, step 4268/7134 completed (loss: 0.08856654167175293, acc: 0.9759519100189209)
[2025-02-13 03:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38][root][INFO] - Training Epoch: 1/2, step 4269/7134 completed (loss: 0.10622136294841766, acc: 0.971222996711731)
[2025-02-13 03:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:39][root][INFO] - Training Epoch: 1/2, step 4270/7134 completed (loss: 0.04355823993682861, acc: 0.9817708134651184)
[2025-02-13 03:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:39][root][INFO] - Training Epoch: 1/2, step 4271/7134 completed (loss: 0.039319492876529694, acc: 0.9887920022010803)
[2025-02-13 03:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:39][root][INFO] - Training Epoch: 1/2, step 4272/7134 completed (loss: 0.03637642413377762, acc: 0.9882628917694092)
[2025-02-13 03:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:40][root][INFO] - Training Epoch: 1/2, step 4273/7134 completed (loss: 0.033512961119413376, acc: 0.9874529242515564)
[2025-02-13 03:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:40][root][INFO] - Training Epoch: 1/2, step 4274/7134 completed (loss: 0.0690465122461319, acc: 0.9778037667274475)
[2025-02-13 03:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41][root][INFO] - Training Epoch: 1/2, step 4275/7134 completed (loss: 0.03270479291677475, acc: 0.9886363744735718)
[2025-02-13 03:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41][root][INFO] - Training Epoch: 1/2, step 4276/7134 completed (loss: 0.059321023523807526, acc: 0.9871134161949158)
[2025-02-13 03:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:42][root][INFO] - Training Epoch: 1/2, step 4277/7134 completed (loss: 0.03254720941185951, acc: 0.9888888597488403)
[2025-02-13 03:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:42][root][INFO] - Training Epoch: 1/2, step 4278/7134 completed (loss: 0.0414067804813385, acc: 0.9851852059364319)
[2025-02-13 03:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43][root][INFO] - Training Epoch: 1/2, step 4279/7134 completed (loss: 0.06128396466374397, acc: 0.988095223903656)
[2025-02-13 03:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43][root][INFO] - Training Epoch: 1/2, step 4280/7134 completed (loss: 0.011901208199560642, acc: 0.9952380657196045)
[2025-02-13 03:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43][root][INFO] - Training Epoch: 1/2, step 4281/7134 completed (loss: 0.06295140832662582, acc: 0.9801980257034302)
[2025-02-13 03:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44][root][INFO] - Training Epoch: 1/2, step 4282/7134 completed (loss: 0.011898207478225231, acc: 0.9981481432914734)
[2025-02-13 03:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44][root][INFO] - Training Epoch: 1/2, step 4283/7134 completed (loss: 0.025125721469521523, acc: 0.9896789193153381)
[2025-02-13 03:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:45][root][INFO] - Training Epoch: 1/2, step 4284/7134 completed (loss: 0.06398642063140869, acc: 0.9873188138008118)
[2025-02-13 03:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:45][root][INFO] - Training Epoch: 1/2, step 4285/7134 completed (loss: 0.03671997785568237, acc: 0.9863013625144958)
[2025-02-13 03:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46][root][INFO] - Training Epoch: 1/2, step 4286/7134 completed (loss: 0.05699354410171509, acc: 0.9838129281997681)
[2025-02-13 03:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46][root][INFO] - Training Epoch: 1/2, step 4287/7134 completed (loss: 0.035670436918735504, acc: 0.9896907210350037)
[2025-02-13 03:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:47][root][INFO] - Training Epoch: 1/2, step 4288/7134 completed (loss: 0.04976992309093475, acc: 0.9872340559959412)
[2025-02-13 03:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:47][root][INFO] - Training Epoch: 1/2, step 4289/7134 completed (loss: 0.05814730376005173, acc: 0.9804511070251465)
[2025-02-13 03:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:47][root][INFO] - Training Epoch: 1/2, step 4290/7134 completed (loss: 0.022538788616657257, acc: 0.9945873022079468)
[2025-02-13 03:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:48][root][INFO] - Training Epoch: 1/2, step 4291/7134 completed (loss: 0.02693411149084568, acc: 0.9904648661613464)
[2025-02-13 03:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:48][root][INFO] - Training Epoch: 1/2, step 4292/7134 completed (loss: 0.030441734939813614, acc: 0.9939758777618408)
[2025-02-13 03:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49][root][INFO] - Training Epoch: 1/2, step 4293/7134 completed (loss: 0.022252248600125313, acc: 0.9919354915618896)
[2025-02-13 03:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49][root][INFO] - Training Epoch: 1/2, step 4294/7134 completed (loss: 0.019770218059420586, acc: 0.9941775798797607)
[2025-02-13 03:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50][root][INFO] - Training Epoch: 1/2, step 4295/7134 completed (loss: 0.035233523696660995, acc: 0.989051103591919)
[2025-02-13 03:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50][root][INFO] - Training Epoch: 1/2, step 4296/7134 completed (loss: 0.03457842767238617, acc: 0.9951338171958923)
[2025-02-13 03:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50][root][INFO] - Training Epoch: 1/2, step 4297/7134 completed (loss: 0.03346464782953262, acc: 0.9929577708244324)
[2025-02-13 03:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:51][root][INFO] - Training Epoch: 1/2, step 4298/7134 completed (loss: 0.045547571033239365, acc: 0.9903714060783386)
[2025-02-13 03:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:51][root][INFO] - Training Epoch: 1/2, step 4299/7134 completed (loss: 0.03351958096027374, acc: 0.9899713397026062)
[2025-02-13 03:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52][root][INFO] - Training Epoch: 1/2, step 4300/7134 completed (loss: 0.019262205809354782, acc: 0.9934210777282715)
[2025-02-13 03:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52][root][INFO] - Training Epoch: 1/2, step 4301/7134 completed (loss: 0.048127662390470505, acc: 0.9901639223098755)
[2025-02-13 03:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52][root][INFO] - Training Epoch: 1/2, step 4302/7134 completed (loss: 0.015448713675141335, acc: 0.9955223798751831)
[2025-02-13 03:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:53][root][INFO] - Training Epoch: 1/2, step 4303/7134 completed (loss: 0.026965027675032616, acc: 0.9916467666625977)
[2025-02-13 03:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:53][root][INFO] - Training Epoch: 1/2, step 4304/7134 completed (loss: 0.029536515474319458, acc: 0.9910846948623657)
[2025-02-13 03:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54][root][INFO] - Training Epoch: 1/2, step 4305/7134 completed (loss: 0.03730444610118866, acc: 0.9894366264343262)
[2025-02-13 03:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54][root][INFO] - Training Epoch: 1/2, step 4306/7134 completed (loss: 0.07273988425731659, acc: 0.9855453372001648)
[2025-02-13 03:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:55][root][INFO] - Training Epoch: 1/2, step 4307/7134 completed (loss: 0.10548960417509079, acc: 0.9759229421615601)
[2025-02-13 03:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:55][root][INFO] - Training Epoch: 1/2, step 4308/7134 completed (loss: 0.0401277132332325, acc: 0.9872390031814575)
[2025-02-13 03:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56][root][INFO] - Training Epoch: 1/2, step 4309/7134 completed (loss: 0.03145454823970795, acc: 0.9913686513900757)
[2025-02-13 03:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56][root][INFO] - Training Epoch: 1/2, step 4310/7134 completed (loss: 0.026546206325292587, acc: 0.9917743802070618)
[2025-02-13 03:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:57][root][INFO] - Training Epoch: 1/2, step 4311/7134 completed (loss: 0.01591736078262329, acc: 0.9971014261245728)
[2025-02-13 03:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:57][root][INFO] - Training Epoch: 1/2, step 4312/7134 completed (loss: 0.016841787844896317, acc: 0.9959127902984619)
[2025-02-13 03:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:57][root][INFO] - Training Epoch: 1/2, step 4313/7134 completed (loss: 0.015153888612985611, acc: 0.998701274394989)
[2025-02-13 03:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58][root][INFO] - Training Epoch: 1/2, step 4314/7134 completed (loss: 0.02515179105103016, acc: 0.9909365773200989)
[2025-02-13 03:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58][root][INFO] - Training Epoch: 1/2, step 4315/7134 completed (loss: 0.03614191710948944, acc: 0.9921568632125854)
[2025-02-13 03:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:59][root][INFO] - Training Epoch: 1/2, step 4316/7134 completed (loss: 0.033958423882722855, acc: 0.9923469424247742)
[2025-02-13 03:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:59][root][INFO] - Training Epoch: 1/2, step 4317/7134 completed (loss: 0.034724511206150055, acc: 0.9913259148597717)
[2025-02-13 03:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:59][root][INFO] - Training Epoch: 1/2, step 4318/7134 completed (loss: 0.04199206084012985, acc: 0.9910314083099365)
[2025-02-13 03:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:00][root][INFO] - Training Epoch: 1/2, step 4319/7134 completed (loss: 0.016309332102537155, acc: 0.997770369052887)
[2025-02-13 03:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:00][root][INFO] - Training Epoch: 1/2, step 4320/7134 completed (loss: 0.031413882970809937, acc: 0.9932432174682617)
[2025-02-13 03:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01][root][INFO] - Training Epoch: 1/2, step 4321/7134 completed (loss: 0.04336242005228996, acc: 0.988452672958374)
[2025-02-13 03:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01][root][INFO] - Training Epoch: 1/2, step 4322/7134 completed (loss: 0.035503264516592026, acc: 0.9889937043190002)
[2025-02-13 03:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:02][root][INFO] - Training Epoch: 1/2, step 4323/7134 completed (loss: 0.06404228508472443, acc: 0.9845857620239258)
[2025-02-13 03:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:02][root][INFO] - Training Epoch: 1/2, step 4324/7134 completed (loss: 0.0164651982486248, acc: 0.9941860437393188)
[2025-02-13 03:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:02][root][INFO] - Training Epoch: 1/2, step 4325/7134 completed (loss: 0.0333428755402565, acc: 0.9904305934906006)
[2025-02-13 03:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03][root][INFO] - Training Epoch: 1/2, step 4326/7134 completed (loss: 0.025655794888734818, acc: 0.9894366264343262)
[2025-02-13 03:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03][root][INFO] - Training Epoch: 1/2, step 4327/7134 completed (loss: 0.03406583145260811, acc: 0.9877622127532959)
[2025-02-13 03:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04][root][INFO] - Training Epoch: 1/2, step 4328/7134 completed (loss: 0.061188954859972, acc: 0.9868420958518982)
[2025-02-13 03:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04][root][INFO] - Training Epoch: 1/2, step 4329/7134 completed (loss: 0.041511524468660355, acc: 0.993446946144104)
[2025-02-13 03:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04][root][INFO] - Training Epoch: 1/2, step 4330/7134 completed (loss: 0.06389014422893524, acc: 0.9872068166732788)
[2025-02-13 03:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05][root][INFO] - Training Epoch: 1/2, step 4331/7134 completed (loss: 0.01882590726017952, acc: 0.9915013909339905)
[2025-02-13 03:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05][root][INFO] - Training Epoch: 1/2, step 4332/7134 completed (loss: 0.054168228060007095, acc: 0.9790874719619751)
[2025-02-13 03:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:06][root][INFO] - Training Epoch: 1/2, step 4333/7134 completed (loss: 0.038606323301792145, acc: 0.9924127459526062)
[2025-02-13 03:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:06][root][INFO] - Training Epoch: 1/2, step 4334/7134 completed (loss: 0.05930788069963455, acc: 0.9820359349250793)
[2025-02-13 03:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:07][root][INFO] - Training Epoch: 1/2, step 4335/7134 completed (loss: 0.04335084185004234, acc: 0.9899193644523621)
[2025-02-13 03:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:07][root][INFO] - Training Epoch: 1/2, step 4336/7134 completed (loss: 0.017821496352553368, acc: 0.991909384727478)
[2025-02-13 03:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:07][root][INFO] - Training Epoch: 1/2, step 4337/7134 completed (loss: 0.024800896644592285, acc: 0.9920886158943176)
[2025-02-13 03:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08][root][INFO] - Training Epoch: 1/2, step 4338/7134 completed (loss: 0.015287503600120544, acc: 0.9921011328697205)
[2025-02-13 03:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08][root][INFO] - Training Epoch: 1/2, step 4339/7134 completed (loss: 0.04862268269062042, acc: 0.9825072884559631)
[2025-02-13 03:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:09][root][INFO] - Training Epoch: 1/2, step 4340/7134 completed (loss: 0.03145769611001015, acc: 0.9898648858070374)
[2025-02-13 03:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:09][root][INFO] - Training Epoch: 1/2, step 4341/7134 completed (loss: 0.05294273793697357, acc: 0.9866270422935486)
[2025-02-13 03:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10][root][INFO] - Training Epoch: 1/2, step 4342/7134 completed (loss: 0.037191446870565414, acc: 0.9871794581413269)
[2025-02-13 03:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10][root][INFO] - Training Epoch: 1/2, step 4343/7134 completed (loss: 0.025035424157977104, acc: 0.9950082898139954)
[2025-02-13 03:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10][root][INFO] - Training Epoch: 1/2, step 4344/7134 completed (loss: 0.009332261979579926, acc: 1.0)
[2025-02-13 03:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:11][root][INFO] - Training Epoch: 1/2, step 4345/7134 completed (loss: 0.019920172169804573, acc: 0.9937888383865356)
[2025-02-13 03:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:11][root][INFO] - Training Epoch: 1/2, step 4346/7134 completed (loss: 0.011165500618517399, acc: 0.9979296326637268)
[2025-02-13 03:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12][root][INFO] - Training Epoch: 1/2, step 4347/7134 completed (loss: 0.05324844270944595, acc: 0.987270176410675)
[2025-02-13 03:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12][root][INFO] - Training Epoch: 1/2, step 4348/7134 completed (loss: 0.023900292813777924, acc: 0.9936000108718872)
[2025-02-13 03:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12][root][INFO] - Training Epoch: 1/2, step 4349/7134 completed (loss: 0.01536746509373188, acc: 0.9953810572624207)
[2025-02-13 03:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:13][root][INFO] - Training Epoch: 1/2, step 4350/7134 completed (loss: 0.03427073359489441, acc: 0.9921466112136841)
[2025-02-13 03:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:13][root][INFO] - Training Epoch: 1/2, step 4351/7134 completed (loss: 0.05550191551446915, acc: 0.9849108457565308)
[2025-02-13 03:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14][root][INFO] - Training Epoch: 1/2, step 4352/7134 completed (loss: 0.03707490116357803, acc: 0.9873257279396057)
[2025-02-13 03:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14][root][INFO] - Training Epoch: 1/2, step 4353/7134 completed (loss: 0.05113311484456062, acc: 0.9854304790496826)
[2025-02-13 03:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:15][root][INFO] - Training Epoch: 1/2, step 4354/7134 completed (loss: 0.03272519260644913, acc: 0.9906666874885559)
[2025-02-13 03:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:15][root][INFO] - Training Epoch: 1/2, step 4355/7134 completed (loss: 0.04274619370698929, acc: 0.987484335899353)
[2025-02-13 03:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:15][root][INFO] - Training Epoch: 1/2, step 4356/7134 completed (loss: 0.03808685764670372, acc: 0.9820788502693176)
[2025-02-13 03:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:16][root][INFO] - Training Epoch: 1/2, step 4357/7134 completed (loss: 0.055104177445173264, acc: 0.9858267903327942)
[2025-02-13 03:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:16][root][INFO] - Training Epoch: 1/2, step 4358/7134 completed (loss: 0.04752129316329956, acc: 0.9856287240982056)
[2025-02-13 03:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17][root][INFO] - Training Epoch: 1/2, step 4359/7134 completed (loss: 0.04348227009177208, acc: 0.9857369065284729)
[2025-02-13 03:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17][root][INFO] - Training Epoch: 1/2, step 4360/7134 completed (loss: 0.06216065585613251, acc: 0.9804469347000122)
[2025-02-13 03:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:18][root][INFO] - Training Epoch: 1/2, step 4361/7134 completed (loss: 0.03209143504500389, acc: 0.9884393215179443)
[2025-02-13 03:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:18][root][INFO] - Training Epoch: 1/2, step 4362/7134 completed (loss: 0.043285418301820755, acc: 0.9839704036712646)
[2025-02-13 03:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19][root][INFO] - Training Epoch: 1/2, step 4363/7134 completed (loss: 0.033310409635305405, acc: 0.9884297251701355)
[2025-02-13 03:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19][root][INFO] - Training Epoch: 1/2, step 4364/7134 completed (loss: 0.021639438346028328, acc: 0.9921875)
[2025-02-13 03:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19][root][INFO] - Training Epoch: 1/2, step 4365/7134 completed (loss: 0.04078858345746994, acc: 0.9842767119407654)
[2025-02-13 03:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20][root][INFO] - Training Epoch: 1/2, step 4366/7134 completed (loss: 0.06553881615400314, acc: 0.9780927896499634)
[2025-02-13 03:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20][root][INFO] - Training Epoch: 1/2, step 4367/7134 completed (loss: 0.029773658141493797, acc: 0.9899713397026062)
[2025-02-13 03:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:21][root][INFO] - Training Epoch: 1/2, step 4368/7134 completed (loss: 0.017298603430390358, acc: 0.99370276927948)
[2025-02-13 03:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:21][root][INFO] - Training Epoch: 1/2, step 4369/7134 completed (loss: 0.02532757818698883, acc: 0.9950000047683716)
[2025-02-13 03:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:22][root][INFO] - Training Epoch: 1/2, step 4370/7134 completed (loss: 0.051380690187215805, acc: 0.9911727905273438)
[2025-02-13 03:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:22][root][INFO] - Training Epoch: 1/2, step 4371/7134 completed (loss: 0.04480799287557602, acc: 0.9881266355514526)
[2025-02-13 03:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:22][root][INFO] - Training Epoch: 1/2, step 4372/7134 completed (loss: 0.04485629126429558, acc: 0.9913941621780396)
[2025-02-13 03:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23][root][INFO] - Training Epoch: 1/2, step 4373/7134 completed (loss: 0.025049010291695595, acc: 0.9916765689849854)
[2025-02-13 03:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23][root][INFO] - Training Epoch: 1/2, step 4374/7134 completed (loss: 0.04076944291591644, acc: 0.9870503544807434)
[2025-02-13 03:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:24][root][INFO] - Training Epoch: 1/2, step 4375/7134 completed (loss: 0.020010801032185555, acc: 0.993565022945404)
[2025-02-13 03:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:24][root][INFO] - Training Epoch: 1/2, step 4376/7134 completed (loss: 0.03460807353258133, acc: 0.9932126402854919)
[2025-02-13 03:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25][root][INFO] - Training Epoch: 1/2, step 4377/7134 completed (loss: 0.061760298907756805, acc: 0.9851668477058411)
[2025-02-13 03:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25][root][INFO] - Training Epoch: 1/2, step 4378/7134 completed (loss: 0.04495704546570778, acc: 0.9885621070861816)
[2025-02-13 03:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:26][root][INFO] - Training Epoch: 1/2, step 4379/7134 completed (loss: 0.08160827308893204, acc: 0.9772727489471436)
[2025-02-13 03:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:26][root][INFO] - Training Epoch: 1/2, step 4380/7134 completed (loss: 0.07526592165231705, acc: 0.9807355403900146)
[2025-02-13 03:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:26][root][INFO] - Training Epoch: 1/2, step 4381/7134 completed (loss: 0.02709471806883812, acc: 0.9917241334915161)
[2025-02-13 03:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27][root][INFO] - Training Epoch: 1/2, step 4382/7134 completed (loss: 0.014637969434261322, acc: 0.996820330619812)
[2025-02-13 03:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27][root][INFO] - Training Epoch: 1/2, step 4383/7134 completed (loss: 0.03840979188680649, acc: 0.9862448573112488)
[2025-02-13 03:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:28][root][INFO] - Training Epoch: 1/2, step 4384/7134 completed (loss: 0.027609933167696, acc: 0.9912060499191284)
[2025-02-13 03:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:28][root][INFO] - Training Epoch: 1/2, step 4385/7134 completed (loss: 0.05053975433111191, acc: 0.9804432988166809)
[2025-02-13 03:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29][root][INFO] - Training Epoch: 1/2, step 4386/7134 completed (loss: 0.04969837889075279, acc: 0.9845971465110779)
[2025-02-13 03:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29][root][INFO] - Training Epoch: 1/2, step 4387/7134 completed (loss: 0.06060045212507248, acc: 0.9838969111442566)
[2025-02-13 03:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29][root][INFO] - Training Epoch: 1/2, step 4388/7134 completed (loss: 0.03613121435046196, acc: 0.9875776171684265)
[2025-02-13 03:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:30][root][INFO] - Training Epoch: 1/2, step 4389/7134 completed (loss: 0.08234875649213791, acc: 0.980424165725708)
[2025-02-13 03:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:30][root][INFO] - Training Epoch: 1/2, step 4390/7134 completed (loss: 0.09380708634853363, acc: 0.9761570692062378)
[2025-02-13 03:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:31][root][INFO] - Training Epoch: 1/2, step 4391/7134 completed (loss: 0.08562525361776352, acc: 0.975570023059845)
[2025-02-13 03:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:31][root][INFO] - Training Epoch: 1/2, step 4392/7134 completed (loss: 0.026572514325380325, acc: 0.9926470518112183)
[2025-02-13 03:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32][root][INFO] - Training Epoch: 1/2, step 4393/7134 completed (loss: 0.023595809936523438, acc: 0.9957805871963501)
[2025-02-13 03:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32][root][INFO] - Training Epoch: 1/2, step 4394/7134 completed (loss: 0.03582901507616043, acc: 0.9915373921394348)
[2025-02-13 03:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:33][root][INFO] - Training Epoch: 1/2, step 4395/7134 completed (loss: 0.05419855937361717, acc: 0.9795918464660645)
[2025-02-13 03:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:33][root][INFO] - Training Epoch: 1/2, step 4396/7134 completed (loss: 0.07780773192644119, acc: 0.9769647717475891)
[2025-02-13 03:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:33][root][INFO] - Training Epoch: 1/2, step 4397/7134 completed (loss: 0.06026550754904747, acc: 0.9835293889045715)
[2025-02-13 03:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:34][root][INFO] - Training Epoch: 1/2, step 4398/7134 completed (loss: 0.018967026844620705, acc: 0.9924242496490479)
[2025-02-13 03:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:34][root][INFO] - Training Epoch: 1/2, step 4399/7134 completed (loss: 0.05167519673705101, acc: 0.9887459874153137)
[2025-02-13 03:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35][root][INFO] - Training Epoch: 1/2, step 4400/7134 completed (loss: 0.08937328308820724, acc: 0.9776951670646667)
[2025-02-13 03:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35][root][INFO] - Training Epoch: 1/2, step 4401/7134 completed (loss: 0.082071952521801, acc: 0.9811594486236572)
[2025-02-13 03:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:36][root][INFO] - Training Epoch: 1/2, step 4402/7134 completed (loss: 0.11193829029798508, acc: 0.9636628031730652)
[2025-02-13 03:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:36][root][INFO] - Training Epoch: 1/2, step 4403/7134 completed (loss: 0.15560096502304077, acc: 0.9585492014884949)
[2025-02-13 03:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:36][root][INFO] - Training Epoch: 1/2, step 4404/7134 completed (loss: 0.14325426518917084, acc: 0.9627659320831299)
[2025-02-13 03:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37][root][INFO] - Training Epoch: 1/2, step 4405/7134 completed (loss: 0.03312411531805992, acc: 0.9913606643676758)
[2025-02-13 03:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37][root][INFO] - Training Epoch: 1/2, step 4406/7134 completed (loss: 0.03494441509246826, acc: 0.9855234026908875)
[2025-02-13 03:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:38][root][INFO] - Training Epoch: 1/2, step 4407/7134 completed (loss: 0.042503099888563156, acc: 0.9884393215179443)
[2025-02-13 03:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:38][root][INFO] - Training Epoch: 1/2, step 4408/7134 completed (loss: 0.08976644277572632, acc: 0.9767171144485474)
[2025-02-13 03:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:39][root][INFO] - Training Epoch: 1/2, step 4409/7134 completed (loss: 0.08611761778593063, acc: 0.9743276238441467)
[2025-02-13 03:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:39][root][INFO] - Training Epoch: 1/2, step 4410/7134 completed (loss: 0.06992121785879135, acc: 0.9838709831237793)
[2025-02-13 03:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40][root][INFO] - Training Epoch: 1/2, step 4411/7134 completed (loss: 0.1328308880329132, acc: 0.9640287756919861)
[2025-02-13 03:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40][root][INFO] - Training Epoch: 1/2, step 4412/7134 completed (loss: 0.18500542640686035, acc: 0.9491018056869507)
[2025-02-13 03:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40][root][INFO] - Training Epoch: 1/2, step 4413/7134 completed (loss: 0.10733050107955933, acc: 0.9741496443748474)
[2025-02-13 03:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:41][root][INFO] - Training Epoch: 1/2, step 4414/7134 completed (loss: 0.046230483800172806, acc: 0.9834123253822327)
[2025-02-13 03:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:41][root][INFO] - Training Epoch: 1/2, step 4415/7134 completed (loss: 0.061986494809389114, acc: 0.9775840640068054)
[2025-02-13 03:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:42][root][INFO] - Training Epoch: 1/2, step 4416/7134 completed (loss: 0.06308542191982269, acc: 0.9836639165878296)
[2025-02-13 03:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:42][root][INFO] - Training Epoch: 1/2, step 4417/7134 completed (loss: 0.0763639360666275, acc: 0.9807692170143127)
[2025-02-13 03:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43][root][INFO] - Training Epoch: 1/2, step 4418/7134 completed (loss: 0.07013656944036484, acc: 0.9817708134651184)
[2025-02-13 03:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43][root][INFO] - Training Epoch: 1/2, step 4419/7134 completed (loss: 0.08710198104381561, acc: 0.9759615659713745)
[2025-02-13 03:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:44][root][INFO] - Training Epoch: 1/2, step 4420/7134 completed (loss: 0.09271226823329926, acc: 0.9751037359237671)
[2025-02-13 03:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:44][root][INFO] - Training Epoch: 1/2, step 4421/7134 completed (loss: 0.05804050341248512, acc: 0.9865016937255859)
[2025-02-13 03:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:44][root][INFO] - Training Epoch: 1/2, step 4422/7134 completed (loss: 0.08829670399427414, acc: 0.9748822450637817)
[2025-02-13 03:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45][root][INFO] - Training Epoch: 1/2, step 4423/7134 completed (loss: 0.04901969060301781, acc: 0.9851598143577576)
[2025-02-13 03:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45][root][INFO] - Training Epoch: 1/2, step 4424/7134 completed (loss: 0.04372740909457207, acc: 0.9924924969673157)
[2025-02-13 03:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:46][root][INFO] - Training Epoch: 1/2, step 4425/7134 completed (loss: 0.04474272206425667, acc: 0.9859648942947388)
[2025-02-13 03:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:46][root][INFO] - Training Epoch: 1/2, step 4426/7134 completed (loss: 0.055828966200351715, acc: 0.9855222105979919)
[2025-02-13 03:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:47][root][INFO] - Training Epoch: 1/2, step 4427/7134 completed (loss: 0.1756390929222107, acc: 0.9590361714363098)
[2025-02-13 03:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:47][root][INFO] - Training Epoch: 1/2, step 4428/7134 completed (loss: 0.027217471972107887, acc: 0.9929577708244324)
[2025-02-13 03:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48][root][INFO] - Training Epoch: 1/2, step 4429/7134 completed (loss: 0.07179740071296692, acc: 0.969072163105011)
[2025-02-13 03:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48][root][INFO] - Training Epoch: 1/2, step 4430/7134 completed (loss: 0.08750441670417786, acc: 0.9740853905677795)
[2025-02-13 03:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48][root][INFO] - Training Epoch: 1/2, step 4431/7134 completed (loss: 0.06356225907802582, acc: 0.9797822833061218)
[2025-02-13 03:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:49][root][INFO] - Training Epoch: 1/2, step 4432/7134 completed (loss: 0.02014959044754505, acc: 0.9938555955886841)
[2025-02-13 03:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:49][root][INFO] - Training Epoch: 1/2, step 4433/7134 completed (loss: 0.04671013355255127, acc: 0.9895104765892029)
[2025-02-13 03:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:50][root][INFO] - Training Epoch: 1/2, step 4434/7134 completed (loss: 0.050809796899557114, acc: 0.9819819927215576)
[2025-02-13 03:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:50][root][INFO] - Training Epoch: 1/2, step 4435/7134 completed (loss: 0.0473334863781929, acc: 0.9815157055854797)
[2025-02-13 03:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51][root][INFO] - Training Epoch: 1/2, step 4436/7134 completed (loss: 0.07202378660440445, acc: 0.9818435907363892)
[2025-02-13 03:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51][root][INFO] - Training Epoch: 1/2, step 4437/7134 completed (loss: 0.023093320429325104, acc: 0.9895012974739075)
[2025-02-13 03:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51][root][INFO] - Training Epoch: 1/2, step 4438/7134 completed (loss: 0.035593174397945404, acc: 0.991525411605835)
[2025-02-13 03:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:52][root][INFO] - Training Epoch: 1/2, step 4439/7134 completed (loss: 0.022562310099601746, acc: 0.9902439117431641)
[2025-02-13 03:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:52][root][INFO] - Training Epoch: 1/2, step 4440/7134 completed (loss: 0.033335987478494644, acc: 0.9903314709663391)
[2025-02-13 03:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:53][root][INFO] - Training Epoch: 1/2, step 4441/7134 completed (loss: 0.043176453560590744, acc: 0.9892473220825195)
[2025-02-13 03:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:53][root][INFO] - Training Epoch: 1/2, step 4442/7134 completed (loss: 0.07065703719854355, acc: 0.9832689762115479)
[2025-02-13 03:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54][root][INFO] - Training Epoch: 1/2, step 4443/7134 completed (loss: 0.04596302658319473, acc: 0.9846153855323792)
[2025-02-13 03:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54][root][INFO] - Training Epoch: 1/2, step 4444/7134 completed (loss: 0.03774069622159004, acc: 0.990212082862854)
[2025-02-13 03:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54][root][INFO] - Training Epoch: 1/2, step 4445/7134 completed (loss: 0.05179062485694885, acc: 0.9818435907363892)
[2025-02-13 03:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:55][root][INFO] - Training Epoch: 1/2, step 4446/7134 completed (loss: 0.034260910004377365, acc: 0.9822866320610046)
[2025-02-13 03:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:55][root][INFO] - Training Epoch: 1/2, step 4447/7134 completed (loss: 0.04212565720081329, acc: 0.989386796951294)
[2025-02-13 03:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56][root][INFO] - Training Epoch: 1/2, step 4448/7134 completed (loss: 0.052599579095840454, acc: 0.9798902869224548)
[2025-02-13 03:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56][root][INFO] - Training Epoch: 1/2, step 4449/7134 completed (loss: 0.05641244351863861, acc: 0.9795538783073425)
[2025-02-13 03:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:57][root][INFO] - Training Epoch: 1/2, step 4450/7134 completed (loss: 0.038713037967681885, acc: 0.9851852059364319)
[2025-02-13 03:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:57][root][INFO] - Training Epoch: 1/2, step 4451/7134 completed (loss: 0.10123343765735626, acc: 0.9784411191940308)
[2025-02-13 03:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:57][root][INFO] - Training Epoch: 1/2, step 4452/7134 completed (loss: 0.01566964015364647, acc: 0.9936000108718872)
[2025-02-13 03:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:58][root][INFO] - Training Epoch: 1/2, step 4453/7134 completed (loss: 0.05061814561486244, acc: 0.9877049326896667)
[2025-02-13 03:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:58][root][INFO] - Training Epoch: 1/2, step 4454/7134 completed (loss: 0.04151906073093414, acc: 0.9915013909339905)
[2025-02-13 03:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59][root][INFO] - Training Epoch: 1/2, step 4455/7134 completed (loss: 0.12183503061532974, acc: 0.9640883803367615)
[2025-02-13 03:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59][root][INFO] - Training Epoch: 1/2, step 4456/7134 completed (loss: 0.011630335822701454, acc: 0.9967637658119202)
[2025-02-13 03:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00][root][INFO] - Training Epoch: 1/2, step 4457/7134 completed (loss: 0.031947631388902664, acc: 0.9887640476226807)
[2025-02-13 03:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00][root][INFO] - Training Epoch: 1/2, step 4458/7134 completed (loss: 0.013056721538305283, acc: 0.9950900077819824)
[2025-02-13 03:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00][root][INFO] - Training Epoch: 1/2, step 4459/7134 completed (loss: 0.02202274464070797, acc: 0.9931318759918213)
[2025-02-13 03:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01][root][INFO] - Training Epoch: 1/2, step 4460/7134 completed (loss: 0.025892242789268494, acc: 0.9914425611495972)
[2025-02-13 03:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01][root][INFO] - Training Epoch: 1/2, step 4461/7134 completed (loss: 0.06273963302373886, acc: 0.9901599287986755)
[2025-02-13 03:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:02][root][INFO] - Training Epoch: 1/2, step 4462/7134 completed (loss: 0.003982994239777327, acc: 1.0)
[2025-02-13 03:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:02][root][INFO] - Training Epoch: 1/2, step 4463/7134 completed (loss: 0.01787322573363781, acc: 0.9949685335159302)
[2025-02-13 03:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:03][root][INFO] - Training Epoch: 1/2, step 4464/7134 completed (loss: 0.04801853746175766, acc: 0.9865092635154724)
[2025-02-13 03:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:03][root][INFO] - Training Epoch: 1/2, step 4465/7134 completed (loss: 0.010969816707074642, acc: 0.9974226951599121)
[2025-02-13 03:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04][root][INFO] - Training Epoch: 1/2, step 4466/7134 completed (loss: 0.017314722761511803, acc: 0.9950617551803589)
[2025-02-13 03:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04][root][INFO] - Training Epoch: 1/2, step 4467/7134 completed (loss: 0.031626831740140915, acc: 0.9912060499191284)
[2025-02-13 03:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04][root][INFO] - Training Epoch: 1/2, step 4468/7134 completed (loss: 0.01200884860008955, acc: 0.9964328408241272)
[2025-02-13 03:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:05][root][INFO] - Training Epoch: 1/2, step 4469/7134 completed (loss: 0.03525243699550629, acc: 0.989595353603363)
[2025-02-13 03:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:05][root][INFO] - Training Epoch: 1/2, step 4470/7134 completed (loss: 0.01860111951828003, acc: 0.9911602139472961)
[2025-02-13 03:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06][root][INFO] - Training Epoch: 1/2, step 4471/7134 completed (loss: 0.01485529262572527, acc: 0.996314525604248)
[2025-02-13 03:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06][root][INFO] - Training Epoch: 1/2, step 4472/7134 completed (loss: 0.040989816188812256, acc: 0.992094874382019)
[2025-02-13 03:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:07][root][INFO] - Training Epoch: 1/2, step 4473/7134 completed (loss: 0.08071165531873703, acc: 0.9733893275260925)
[2025-02-13 03:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:07][root][INFO] - Training Epoch: 1/2, step 4474/7134 completed (loss: 0.07212723791599274, acc: 0.9748954176902771)
[2025-02-13 03:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08][root][INFO] - Training Epoch: 1/2, step 4475/7134 completed (loss: 0.09893105924129486, acc: 0.9727047085762024)
[2025-02-13 03:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08][root][INFO] - Training Epoch: 1/2, step 4476/7134 completed (loss: 0.06585061550140381, acc: 0.9821673631668091)
[2025-02-13 03:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08][root][INFO] - Training Epoch: 1/2, step 4477/7134 completed (loss: 0.0065338569693267345, acc: 0.9987775087356567)
[2025-02-13 03:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:09][root][INFO] - Training Epoch: 1/2, step 4478/7134 completed (loss: 0.017660489305853844, acc: 0.9975278377532959)
[2025-02-13 03:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:09][root][INFO] - Training Epoch: 1/2, step 4479/7134 completed (loss: 0.024363374337553978, acc: 0.993261456489563)
[2025-02-13 03:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10][root][INFO] - Training Epoch: 1/2, step 4480/7134 completed (loss: 0.016290055587887764, acc: 0.9953271150588989)
[2025-02-13 03:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10][root][INFO] - Training Epoch: 1/2, step 4481/7134 completed (loss: 0.037019457668066025, acc: 0.9918224215507507)
[2025-02-13 03:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11][root][INFO] - Training Epoch: 1/2, step 4482/7134 completed (loss: 0.011774440295994282, acc: 0.9964028596878052)
[2025-02-13 03:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11][root][INFO] - Training Epoch: 1/2, step 4483/7134 completed (loss: 0.0316288024187088, acc: 0.9925705790519714)
[2025-02-13 03:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:12][root][INFO] - Training Epoch: 1/2, step 4484/7134 completed (loss: 0.023958368226885796, acc: 0.9933862686157227)
[2025-02-13 03:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:12][root][INFO] - Training Epoch: 1/2, step 4485/7134 completed (loss: 0.01896943710744381, acc: 0.9938744306564331)
[2025-02-13 03:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:12][root][INFO] - Training Epoch: 1/2, step 4486/7134 completed (loss: 0.011771252378821373, acc: 0.9970149397850037)
[2025-02-13 03:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:13][root][INFO] - Training Epoch: 1/2, step 4487/7134 completed (loss: 0.017833514139056206, acc: 0.9931034445762634)
[2025-02-13 03:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:13][root][INFO] - Training Epoch: 1/2, step 4488/7134 completed (loss: 0.016547270119190216, acc: 0.9928057789802551)
[2025-02-13 03:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14][root][INFO] - Training Epoch: 1/2, step 4489/7134 completed (loss: 0.019191628322005272, acc: 0.9948453903198242)
[2025-02-13 03:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14][root][INFO] - Training Epoch: 1/2, step 4490/7134 completed (loss: 0.028053024783730507, acc: 0.9907407164573669)
[2025-02-13 03:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:15][root][INFO] - Training Epoch: 1/2, step 4491/7134 completed (loss: 0.006232178304344416, acc: 0.9981378316879272)
[2025-02-13 03:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:15][root][INFO] - Training Epoch: 1/2, step 4492/7134 completed (loss: 0.014206275343894958, acc: 0.9944853186607361)
[2025-02-13 03:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:15][root][INFO] - Training Epoch: 1/2, step 4493/7134 completed (loss: 0.014619160443544388, acc: 0.9944979548454285)
[2025-02-13 03:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16][root][INFO] - Training Epoch: 1/2, step 4494/7134 completed (loss: 0.03339351341128349, acc: 0.9913169145584106)
[2025-02-13 03:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16][root][INFO] - Training Epoch: 1/2, step 4495/7134 completed (loss: 0.017421068623661995, acc: 0.9936143159866333)
[2025-02-13 03:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:17][root][INFO] - Training Epoch: 1/2, step 4496/7134 completed (loss: 0.03289046883583069, acc: 0.9871794581413269)
[2025-02-13 03:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:17][root][INFO] - Training Epoch: 1/2, step 4497/7134 completed (loss: 0.013627691194415092, acc: 0.998675525188446)
[2025-02-13 03:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:17][root][INFO] - Training Epoch: 1/2, step 4498/7134 completed (loss: 0.059658415615558624, acc: 0.9927431344985962)
[2025-02-13 03:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18][root][INFO] - Training Epoch: 1/2, step 4499/7134 completed (loss: 0.04996208846569061, acc: 0.991304337978363)
[2025-02-13 03:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18][root][INFO] - Training Epoch: 1/2, step 4500/7134 completed (loss: 0.029843902215361595, acc: 0.9944055676460266)
[2025-02-13 03:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:19][root][INFO] - Training Epoch: 1/2, step 4501/7134 completed (loss: 0.02608938328921795, acc: 0.9931600689888)
[2025-02-13 03:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:19][root][INFO] - Training Epoch: 1/2, step 4502/7134 completed (loss: 0.019343536347150803, acc: 0.995502233505249)
[2025-02-13 03:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20][root][INFO] - Training Epoch: 1/2, step 4503/7134 completed (loss: 0.013433653861284256, acc: 0.995184600353241)
[2025-02-13 03:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20][root][INFO] - Training Epoch: 1/2, step 4504/7134 completed (loss: 0.04972521588206291, acc: 0.9835575222969055)
[2025-02-13 03:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20][root][INFO] - Training Epoch: 1/2, step 4505/7134 completed (loss: 0.032347869127988815, acc: 0.9864864945411682)
[2025-02-13 03:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:21][root][INFO] - Training Epoch: 1/2, step 4506/7134 completed (loss: 0.03301820904016495, acc: 0.9897040128707886)
[2025-02-13 03:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:21][root][INFO] - Training Epoch: 1/2, step 4507/7134 completed (loss: 0.029401391744613647, acc: 0.9928876161575317)
[2025-02-13 03:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22][root][INFO] - Training Epoch: 1/2, step 4508/7134 completed (loss: 0.03262421116232872, acc: 0.9900285005569458)
[2025-02-13 03:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22][root][INFO] - Training Epoch: 1/2, step 4509/7134 completed (loss: 0.035151366144418716, acc: 0.9852349162101746)
[2025-02-13 03:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23][root][INFO] - Training Epoch: 1/2, step 4510/7134 completed (loss: 0.0449666865170002, acc: 0.9901269674301147)
[2025-02-13 03:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23][root][INFO] - Training Epoch: 1/2, step 4511/7134 completed (loss: 0.07001274824142456, acc: 0.9803094267845154)
[2025-02-13 03:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:24][root][INFO] - Training Epoch: 1/2, step 4512/7134 completed (loss: 0.05173185467720032, acc: 0.980286717414856)
[2025-02-13 03:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:24][root][INFO] - Training Epoch: 1/2, step 4513/7134 completed (loss: 0.038565073162317276, acc: 0.9873595237731934)
[2025-02-13 03:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:24][root][INFO] - Training Epoch: 1/2, step 4514/7134 completed (loss: 0.08121662586927414, acc: 0.9795620441436768)
[2025-02-13 03:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25][root][INFO] - Training Epoch: 1/2, step 4515/7134 completed (loss: 0.04466676712036133, acc: 0.9895522594451904)
[2025-02-13 03:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25][root][INFO] - Training Epoch: 1/2, step 4516/7134 completed (loss: 0.02695297636091709, acc: 0.9914320707321167)
[2025-02-13 03:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26][root][INFO] - Training Epoch: 1/2, step 4517/7134 completed (loss: 0.03598429262638092, acc: 0.9871959090232849)
[2025-02-13 03:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26][root][INFO] - Training Epoch: 1/2, step 4518/7134 completed (loss: 0.06279734522104263, acc: 0.9821428656578064)
[2025-02-13 03:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:27][root][INFO] - Training Epoch: 1/2, step 4519/7134 completed (loss: 0.038066521286964417, acc: 0.9866989254951477)
[2025-02-13 03:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:27][root][INFO] - Training Epoch: 1/2, step 4520/7134 completed (loss: 0.03540833666920662, acc: 0.991525411605835)
[2025-02-13 03:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28][root][INFO] - Training Epoch: 1/2, step 4521/7134 completed (loss: 0.04672476649284363, acc: 0.9867629408836365)
[2025-02-13 03:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28][root][INFO] - Training Epoch: 1/2, step 4522/7134 completed (loss: 0.04628182575106621, acc: 0.983627200126648)
[2025-02-13 03:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28][root][INFO] - Training Epoch: 1/2, step 4523/7134 completed (loss: 0.02941540814936161, acc: 0.9905325174331665)
[2025-02-13 03:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:29][root][INFO] - Training Epoch: 1/2, step 4524/7134 completed (loss: 0.04612003639340401, acc: 0.9865047335624695)
[2025-02-13 03:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:29][root][INFO] - Training Epoch: 1/2, step 4525/7134 completed (loss: 0.037148963660001755, acc: 0.9900000095367432)
[2025-02-13 03:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30][root][INFO] - Training Epoch: 1/2, step 4526/7134 completed (loss: 0.05323778837919235, acc: 0.9819355010986328)
[2025-02-13 03:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30][root][INFO] - Training Epoch: 1/2, step 4527/7134 completed (loss: 0.029852602630853653, acc: 0.9910813570022583)
[2025-02-13 03:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31][root][INFO] - Training Epoch: 1/2, step 4528/7134 completed (loss: 0.048366744071245193, acc: 0.9871465563774109)
[2025-02-13 03:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31][root][INFO] - Training Epoch: 1/2, step 4529/7134 completed (loss: 0.09672550112009048, acc: 0.9766839146614075)
[2025-02-13 03:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:32][root][INFO] - Training Epoch: 1/2, step 4530/7134 completed (loss: 0.022425811737775803, acc: 0.9971988797187805)
[2025-02-13 03:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:32][root][INFO] - Training Epoch: 1/2, step 4531/7134 completed (loss: 0.03812077268958092, acc: 0.9910233616828918)
[2025-02-13 03:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:32][root][INFO] - Training Epoch: 1/2, step 4532/7134 completed (loss: 0.045968346297740936, acc: 0.9840637445449829)
[2025-02-13 03:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33][root][INFO] - Training Epoch: 1/2, step 4533/7134 completed (loss: 0.025959108024835587, acc: 0.990510106086731)
[2025-02-13 03:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33][root][INFO] - Training Epoch: 1/2, step 4534/7134 completed (loss: 0.025941239669919014, acc: 0.9929577708244324)
[2025-02-13 03:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:34][root][INFO] - Training Epoch: 1/2, step 4535/7134 completed (loss: 0.051453057676553726, acc: 0.9863325953483582)
[2025-02-13 03:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:34][root][INFO] - Training Epoch: 1/2, step 4536/7134 completed (loss: 0.022480200976133347, acc: 0.9952107071876526)
[2025-02-13 03:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35][root][INFO] - Training Epoch: 1/2, step 4537/7134 completed (loss: 0.014681779779493809, acc: 0.9955307245254517)
[2025-02-13 03:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35][root][INFO] - Training Epoch: 1/2, step 4538/7134 completed (loss: 0.05469856783747673, acc: 0.9862068891525269)
[2025-02-13 03:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:36][root][INFO] - Training Epoch: 1/2, step 4539/7134 completed (loss: 0.04132399708032608, acc: 0.9902912378311157)
[2025-02-13 03:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:36][root][INFO] - Training Epoch: 1/2, step 4540/7134 completed (loss: 0.034041766077280045, acc: 0.9913294911384583)
[2025-02-13 03:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37][root][INFO] - Training Epoch: 1/2, step 4541/7134 completed (loss: 0.06288685649633408, acc: 0.9815195202827454)
[2025-02-13 03:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37][root][INFO] - Training Epoch: 1/2, step 4542/7134 completed (loss: 0.06791278719902039, acc: 0.9848484992980957)
[2025-02-13 03:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37][root][INFO] - Training Epoch: 1/2, step 4543/7134 completed (loss: 0.0718013271689415, acc: 0.976827085018158)
[2025-02-13 03:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:38][root][INFO] - Training Epoch: 1/2, step 4544/7134 completed (loss: 0.04879704490303993, acc: 0.9817073345184326)
[2025-02-13 03:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:38][root][INFO] - Training Epoch: 1/2, step 4545/7134 completed (loss: 0.06728225201368332, acc: 0.9807322025299072)
[2025-02-13 03:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:39][root][INFO] - Training Epoch: 1/2, step 4546/7134 completed (loss: 0.060053061693906784, acc: 0.9865546226501465)
[2025-02-13 03:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:39][root][INFO] - Training Epoch: 1/2, step 4547/7134 completed (loss: 0.023779043927788734, acc: 0.9930796027183533)
[2025-02-13 03:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40][root][INFO] - Training Epoch: 1/2, step 4548/7134 completed (loss: 0.039745915681123734, acc: 0.9900497794151306)
[2025-02-13 03:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40][root][INFO] - Training Epoch: 1/2, step 4549/7134 completed (loss: 0.04815647751092911, acc: 0.9849785566329956)
[2025-02-13 03:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40][root][INFO] - Training Epoch: 1/2, step 4550/7134 completed (loss: 0.05083627253770828, acc: 0.9899497628211975)
[2025-02-13 03:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:41][root][INFO] - Training Epoch: 1/2, step 4551/7134 completed (loss: 0.03414519503712654, acc: 0.991428554058075)
[2025-02-13 03:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:41][root][INFO] - Training Epoch: 1/2, step 4552/7134 completed (loss: 0.02016020193696022, acc: 0.9942857027053833)
[2025-02-13 03:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42][root][INFO] - Training Epoch: 1/2, step 4553/7134 completed (loss: 0.03300260007381439, acc: 0.989847719669342)
[2025-02-13 03:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42][root][INFO] - Training Epoch: 1/2, step 4554/7134 completed (loss: 0.06092817336320877, acc: 0.9819819927215576)
[2025-02-13 03:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42][root][INFO] - Training Epoch: 1/2, step 4555/7134 completed (loss: 0.05261687561869621, acc: 0.9806094169616699)
[2025-02-13 03:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:43][root][INFO] - Training Epoch: 1/2, step 4556/7134 completed (loss: 0.06023446097970009, acc: 0.9834586381912231)
[2025-02-13 03:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:43][root][INFO] - Training Epoch: 1/2, step 4557/7134 completed (loss: 0.0926482155919075, acc: 0.9817517995834351)
[2025-02-13 03:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44][root][INFO] - Training Epoch: 1/2, step 4558/7134 completed (loss: 0.05040257051587105, acc: 0.9865319728851318)
[2025-02-13 03:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44][root][INFO] - Training Epoch: 1/2, step 4559/7134 completed (loss: 0.0791960060596466, acc: 0.978151261806488)
[2025-02-13 03:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:45][root][INFO] - Training Epoch: 1/2, step 4560/7134 completed (loss: 0.041889600455760956, acc: 0.9873188138008118)
[2025-02-13 03:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:45][root][INFO] - Training Epoch: 1/2, step 4561/7134 completed (loss: 0.06166373938322067, acc: 0.9803921580314636)
[2025-02-13 03:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:45][root][INFO] - Training Epoch: 1/2, step 4562/7134 completed (loss: 0.046591997146606445, acc: 0.9852670431137085)
[2025-02-13 03:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46][root][INFO] - Training Epoch: 1/2, step 4563/7134 completed (loss: 0.04243384674191475, acc: 0.989313006401062)
[2025-02-13 03:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46][root][INFO] - Training Epoch: 1/2, step 4564/7134 completed (loss: 0.04728364571928978, acc: 0.9901269674301147)
[2025-02-13 03:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47][root][INFO] - Training Epoch: 1/2, step 4565/7134 completed (loss: 0.04514027759432793, acc: 0.9847036600112915)
[2025-02-13 03:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47][root][INFO] - Training Epoch: 1/2, step 4566/7134 completed (loss: 0.036052629351615906, acc: 0.9892473220825195)
[2025-02-13 03:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47][root][INFO] - Training Epoch: 1/2, step 4567/7134 completed (loss: 0.02337305247783661, acc: 0.9963235259056091)
[2025-02-13 03:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:48][root][INFO] - Training Epoch: 1/2, step 4568/7134 completed (loss: 0.06427103281021118, acc: 0.9862385392189026)
[2025-02-13 03:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:48][root][INFO] - Training Epoch: 1/2, step 4569/7134 completed (loss: 0.024787500500679016, acc: 0.9901960492134094)
[2025-02-13 03:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49][root][INFO] - Training Epoch: 1/2, step 4570/7134 completed (loss: 0.026728691533207893, acc: 0.9883381724357605)
[2025-02-13 03:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49][root][INFO] - Training Epoch: 1/2, step 4571/7134 completed (loss: 0.040960539132356644, acc: 0.9910141229629517)
[2025-02-13 03:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:50][root][INFO] - Training Epoch: 1/2, step 4572/7134 completed (loss: 0.03478797897696495, acc: 0.9906542301177979)
[2025-02-13 03:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:50][root][INFO] - Training Epoch: 1/2, step 4573/7134 completed (loss: 0.03329279273748398, acc: 0.9915966391563416)
[2025-02-13 03:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:50][root][INFO] - Training Epoch: 1/2, step 4574/7134 completed (loss: 0.011001176200807095, acc: 0.9964664578437805)
[2025-02-13 03:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51][root][INFO] - Training Epoch: 1/2, step 4575/7134 completed (loss: 0.04150364547967911, acc: 0.9867021441459656)
[2025-02-13 03:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51][root][INFO] - Training Epoch: 1/2, step 4576/7134 completed (loss: 0.018485624343156815, acc: 0.9949811697006226)
[2025-02-13 03:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:52][root][INFO] - Training Epoch: 1/2, step 4577/7134 completed (loss: 0.05286452919244766, acc: 0.9887640476226807)
[2025-02-13 03:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:52][root][INFO] - Training Epoch: 1/2, step 4578/7134 completed (loss: 0.026379909366369247, acc: 0.9927219748497009)
[2025-02-13 03:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:53][root][INFO] - Training Epoch: 1/2, step 4579/7134 completed (loss: 0.061268579214811325, acc: 0.9904912710189819)
[2025-02-13 03:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:53][root][INFO] - Training Epoch: 1/2, step 4580/7134 completed (loss: 0.02245377004146576, acc: 0.9925373196601868)
[2025-02-13 03:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:53][root][INFO] - Training Epoch: 1/2, step 4581/7134 completed (loss: 0.0158474612981081, acc: 0.9964726567268372)
[2025-02-13 03:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54][root][INFO] - Training Epoch: 1/2, step 4582/7134 completed (loss: 0.0282729584723711, acc: 0.9917762875556946)
[2025-02-13 03:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54][root][INFO] - Training Epoch: 1/2, step 4583/7134 completed (loss: 0.03455885499715805, acc: 0.991830050945282)
[2025-02-13 03:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:55][root][INFO] - Training Epoch: 1/2, step 4584/7134 completed (loss: 0.014069109223783016, acc: 0.9951691031455994)
[2025-02-13 03:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:55][root][INFO] - Training Epoch: 1/2, step 4585/7134 completed (loss: 0.018035391345620155, acc: 0.9951456189155579)
[2025-02-13 03:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56][root][INFO] - Training Epoch: 1/2, step 4586/7134 completed (loss: 0.029700852930545807, acc: 0.9908972978591919)
[2025-02-13 03:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56][root][INFO] - Training Epoch: 1/2, step 4587/7134 completed (loss: 0.037817347794771194, acc: 0.9890410900115967)
[2025-02-13 03:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56][root][INFO] - Training Epoch: 1/2, step 4588/7134 completed (loss: 0.041836075484752655, acc: 0.979651153087616)
[2025-02-13 03:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:57][root][INFO] - Training Epoch: 1/2, step 4589/7134 completed (loss: 0.05201724171638489, acc: 0.985029935836792)
[2025-02-13 03:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:57][root][INFO] - Training Epoch: 1/2, step 4590/7134 completed (loss: 0.04150771722197533, acc: 0.9917126893997192)
[2025-02-13 03:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58][root][INFO] - Training Epoch: 1/2, step 4591/7134 completed (loss: 0.030230330303311348, acc: 0.9920844435691833)
[2025-02-13 03:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58][root][INFO] - Training Epoch: 1/2, step 4592/7134 completed (loss: 0.03742065280675888, acc: 0.9883889555931091)
[2025-02-13 03:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58][root][INFO] - Training Epoch: 1/2, step 4593/7134 completed (loss: 0.04105305299162865, acc: 0.9896296262741089)
[2025-02-13 03:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:59][root][INFO] - Training Epoch: 1/2, step 4594/7134 completed (loss: 0.043261438608169556, acc: 0.9866666793823242)
[2025-02-13 03:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:59][root][INFO] - Training Epoch: 1/2, step 4595/7134 completed (loss: 0.02985520102083683, acc: 0.9881129264831543)
[2025-02-13 03:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:00][root][INFO] - Training Epoch: 1/2, step 4596/7134 completed (loss: 0.03371893987059593, acc: 0.9888888597488403)
[2025-02-13 03:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:00][root][INFO] - Training Epoch: 1/2, step 4597/7134 completed (loss: 0.03014121949672699, acc: 0.9927095770835876)
[2025-02-13 03:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01][root][INFO] - Training Epoch: 1/2, step 4598/7134 completed (loss: 0.05679565295577049, acc: 0.9865030646324158)
[2025-02-13 03:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01][root][INFO] - Training Epoch: 1/2, step 4599/7134 completed (loss: 0.022654525935649872, acc: 0.9920508861541748)
[2025-02-13 03:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:02][root][INFO] - Training Epoch: 1/2, step 4600/7134 completed (loss: 0.025115683674812317, acc: 0.9932340979576111)
[2025-02-13 03:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:02][root][INFO] - Training Epoch: 1/2, step 4601/7134 completed (loss: 0.048121094703674316, acc: 0.9895697236061096)
[2025-02-13 03:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:02][root][INFO] - Training Epoch: 1/2, step 4602/7134 completed (loss: 0.04106401279568672, acc: 0.9904631972312927)
[2025-02-13 03:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:03][root][INFO] - Training Epoch: 1/2, step 4603/7134 completed (loss: 0.045634761452674866, acc: 0.9834482669830322)
[2025-02-13 03:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:03][root][INFO] - Training Epoch: 1/2, step 4604/7134 completed (loss: 0.021854273974895477, acc: 0.9887640476226807)
[2025-02-13 03:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:04][root][INFO] - Training Epoch: 1/2, step 4605/7134 completed (loss: 0.06309803575277328, acc: 0.9825327396392822)
[2025-02-13 03:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:04][root][INFO] - Training Epoch: 1/2, step 4606/7134 completed (loss: 0.01619187742471695, acc: 0.9954128265380859)
[2025-02-13 03:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05][root][INFO] - Training Epoch: 1/2, step 4607/7134 completed (loss: 0.021934248507022858, acc: 0.9882155060768127)
[2025-02-13 03:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05][root][INFO] - Training Epoch: 1/2, step 4608/7134 completed (loss: 0.03504527732729912, acc: 0.9881481528282166)
[2025-02-13 03:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05][root][INFO] - Training Epoch: 1/2, step 4609/7134 completed (loss: 0.02963918261229992, acc: 0.9944953918457031)
[2025-02-13 03:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:06][root][INFO] - Training Epoch: 1/2, step 4610/7134 completed (loss: 0.02591729164123535, acc: 0.9907038807868958)
[2025-02-13 03:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:06][root][INFO] - Training Epoch: 1/2, step 4611/7134 completed (loss: 0.011115793138742447, acc: 0.9968701004981995)
[2025-02-13 03:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07][root][INFO] - Training Epoch: 1/2, step 4612/7134 completed (loss: 0.020902050659060478, acc: 0.9907063245773315)
[2025-02-13 03:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07][root][INFO] - Training Epoch: 1/2, step 4613/7134 completed (loss: 0.023270176723599434, acc: 0.9930555820465088)
[2025-02-13 03:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07][root][INFO] - Training Epoch: 1/2, step 4614/7134 completed (loss: 0.032024405896663666, acc: 0.9924242496490479)
[2025-02-13 03:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:08][root][INFO] - Training Epoch: 1/2, step 4615/7134 completed (loss: 0.05002467334270477, acc: 0.9850746393203735)
[2025-02-13 03:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:08][root][INFO] - Training Epoch: 1/2, step 4616/7134 completed (loss: 0.05402996018528938, acc: 0.9837278127670288)
[2025-02-13 03:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:09][root][INFO] - Training Epoch: 1/2, step 4617/7134 completed (loss: 0.03012685477733612, acc: 0.9894894957542419)
[2025-02-13 03:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:09][root][INFO] - Training Epoch: 1/2, step 4618/7134 completed (loss: 0.053190574049949646, acc: 0.984466016292572)
[2025-02-13 03:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:10][root][INFO] - Training Epoch: 1/2, step 4619/7134 completed (loss: 0.016645850613713264, acc: 0.993966817855835)
[2025-02-13 03:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:10][root][INFO] - Training Epoch: 1/2, step 4620/7134 completed (loss: 0.013838227838277817, acc: 0.9942029118537903)
[2025-02-13 03:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11][root][INFO] - Training Epoch: 1/2, step 4621/7134 completed (loss: 0.018390942364931107, acc: 0.9922839403152466)
[2025-02-13 03:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11][root][INFO] - Training Epoch: 1/2, step 4622/7134 completed (loss: 0.03044809028506279, acc: 0.9912609457969666)
[2025-02-13 03:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11][root][INFO] - Training Epoch: 1/2, step 4623/7134 completed (loss: 0.05660868063569069, acc: 0.9830065369606018)
[2025-02-13 03:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:12][root][INFO] - Training Epoch: 1/2, step 4624/7134 completed (loss: 0.07656113058328629, acc: 0.9849108457565308)
[2025-02-13 03:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:12][root][INFO] - Training Epoch: 1/2, step 4625/7134 completed (loss: 0.09188994020223618, acc: 0.977011501789093)
[2025-02-13 03:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:13][root][INFO] - Training Epoch: 1/2, step 4626/7134 completed (loss: 0.047858670353889465, acc: 0.9879194498062134)
[2025-02-13 03:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:13][root][INFO] - Training Epoch: 1/2, step 4627/7134 completed (loss: 0.08243123441934586, acc: 0.9802731275558472)
[2025-02-13 03:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14][root][INFO] - Training Epoch: 1/2, step 4628/7134 completed (loss: 0.06791310757398605, acc: 0.9797101616859436)
[2025-02-13 03:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14][root][INFO] - Training Epoch: 1/2, step 4629/7134 completed (loss: 0.10108392685651779, acc: 0.9709401726722717)
[2025-02-13 03:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:15][root][INFO] - Training Epoch: 1/2, step 4630/7134 completed (loss: 0.06670793145895004, acc: 0.9815059304237366)
[2025-02-13 03:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:15][root][INFO] - Training Epoch: 1/2, step 4631/7134 completed (loss: 0.038832888007164, acc: 0.9867256879806519)
[2025-02-13 03:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:16][root][INFO] - Training Epoch: 1/2, step 4632/7134 completed (loss: 0.04275336116552353, acc: 0.9926470518112183)
[2025-02-13 03:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:16][root][INFO] - Training Epoch: 1/2, step 4633/7134 completed (loss: 0.0446724072098732, acc: 0.989595353603363)
[2025-02-13 03:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:16][root][INFO] - Training Epoch: 1/2, step 4634/7134 completed (loss: 0.038705311715602875, acc: 0.9864077568054199)
[2025-02-13 03:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17][root][INFO] - Training Epoch: 1/2, step 4635/7134 completed (loss: 0.06425033509731293, acc: 0.9862805008888245)
[2025-02-13 03:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17][root][INFO] - Training Epoch: 1/2, step 4636/7134 completed (loss: 0.025025062263011932, acc: 0.9948453903198242)
[2025-02-13 03:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:18][root][INFO] - Training Epoch: 1/2, step 4637/7134 completed (loss: 0.03342121094465256, acc: 0.9919354915618896)
[2025-02-13 03:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:18][root][INFO] - Training Epoch: 1/2, step 4638/7134 completed (loss: 0.057177312672138214, acc: 0.9815059304237366)
[2025-02-13 03:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19][root][INFO] - Training Epoch: 1/2, step 4639/7134 completed (loss: 0.06698127835988998, acc: 0.9866179823875427)
[2025-02-13 03:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19][root][INFO] - Training Epoch: 1/2, step 4640/7134 completed (loss: 0.06210300326347351, acc: 0.978205144405365)
[2025-02-13 03:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19][root][INFO] - Training Epoch: 1/2, step 4641/7134 completed (loss: 0.04116698354482651, acc: 0.9872159361839294)
[2025-02-13 03:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:20][root][INFO] - Training Epoch: 1/2, step 4642/7134 completed (loss: 0.012518221512436867, acc: 0.9947090148925781)
[2025-02-13 03:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:20][root][INFO] - Training Epoch: 1/2, step 4643/7134 completed (loss: 0.029736405238509178, acc: 0.9940828680992126)
[2025-02-13 03:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:21][root][INFO] - Training Epoch: 1/2, step 4644/7134 completed (loss: 0.024634214118123055, acc: 0.9928994178771973)
[2025-02-13 03:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:21][root][INFO] - Training Epoch: 1/2, step 4645/7134 completed (loss: 0.025700604543089867, acc: 0.9902642369270325)
[2025-02-13 03:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22][root][INFO] - Training Epoch: 1/2, step 4646/7134 completed (loss: 0.019840655848383904, acc: 0.9921507239341736)
[2025-02-13 03:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22][root][INFO] - Training Epoch: 1/2, step 4647/7134 completed (loss: 0.06131846085190773, acc: 0.9856770634651184)
[2025-02-13 03:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:23][root][INFO] - Training Epoch: 1/2, step 4648/7134 completed (loss: 0.040917105972766876, acc: 0.9862825870513916)
[2025-02-13 03:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:23][root][INFO] - Training Epoch: 1/2, step 4649/7134 completed (loss: 0.05944447964429855, acc: 0.9789842367172241)
[2025-02-13 03:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:24][root][INFO] - Training Epoch: 1/2, step 4650/7134 completed (loss: 0.04548826068639755, acc: 0.9836309552192688)
[2025-02-13 03:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:24][root][INFO] - Training Epoch: 1/2, step 4651/7134 completed (loss: 0.09329875558614731, acc: 0.9733840227127075)
[2025-02-13 03:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:24][root][INFO] - Training Epoch: 1/2, step 4652/7134 completed (loss: 0.041684191673994064, acc: 0.9862805008888245)
[2025-02-13 03:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25][root][INFO] - Training Epoch: 1/2, step 4653/7134 completed (loss: 0.054562151432037354, acc: 0.9838709831237793)
[2025-02-13 03:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25][root][INFO] - Training Epoch: 1/2, step 4654/7134 completed (loss: 0.06144976243376732, acc: 0.9841656684875488)
[2025-02-13 03:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:26][root][INFO] - Training Epoch: 1/2, step 4655/7134 completed (loss: 0.04590797796845436, acc: 0.9904240965843201)
[2025-02-13 03:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:26][root][INFO] - Training Epoch: 1/2, step 4656/7134 completed (loss: 0.03444748371839523, acc: 0.9943181872367859)
[2025-02-13 03:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27][root][INFO] - Training Epoch: 1/2, step 4657/7134 completed (loss: 0.0562351755797863, acc: 0.9844192862510681)
[2025-02-13 03:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27][root][INFO] - Training Epoch: 1/2, step 4658/7134 completed (loss: 0.05773426219820976, acc: 0.9797022938728333)
[2025-02-13 03:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:28][root][INFO] - Training Epoch: 1/2, step 4659/7134 completed (loss: 0.0773729532957077, acc: 0.9820051193237305)
[2025-02-13 03:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:28][root][INFO] - Training Epoch: 1/2, step 4660/7134 completed (loss: 0.07513510435819626, acc: 0.9749216437339783)
[2025-02-13 03:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:28][root][INFO] - Training Epoch: 1/2, step 4661/7134 completed (loss: 0.03923046588897705, acc: 0.9908536672592163)
[2025-02-13 03:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:29][root][INFO] - Training Epoch: 1/2, step 4662/7134 completed (loss: 0.09223318099975586, acc: 0.976190447807312)
[2025-02-13 03:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:29][root][INFO] - Training Epoch: 1/2, step 4663/7134 completed (loss: 0.06565333902835846, acc: 0.9852941036224365)
[2025-02-13 03:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30][root][INFO] - Training Epoch: 1/2, step 4664/7134 completed (loss: 0.03763161972165108, acc: 0.9907529950141907)
[2025-02-13 03:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30][root][INFO] - Training Epoch: 1/2, step 4665/7134 completed (loss: 0.03261175379157066, acc: 0.9918144345283508)
[2025-02-13 03:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:31][root][INFO] - Training Epoch: 1/2, step 4666/7134 completed (loss: 0.06307176500558853, acc: 0.9843546152114868)
[2025-02-13 03:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:31][root][INFO] - Training Epoch: 1/2, step 4667/7134 completed (loss: 0.06360378116369247, acc: 0.9862306118011475)
[2025-02-13 03:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:31][root][INFO] - Training Epoch: 1/2, step 4668/7134 completed (loss: 0.03115362487733364, acc: 0.9856630563735962)
[2025-02-13 03:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:32][root][INFO] - Training Epoch: 1/2, step 4669/7134 completed (loss: 0.02124413102865219, acc: 0.9970414042472839)
[2025-02-13 03:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:32][root][INFO] - Training Epoch: 1/2, step 4670/7134 completed (loss: 0.0646088570356369, acc: 0.9844054579734802)
[2025-02-13 03:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33][root][INFO] - Training Epoch: 1/2, step 4671/7134 completed (loss: 0.04877787083387375, acc: 0.9858956336975098)
[2025-02-13 03:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33][root][INFO] - Training Epoch: 1/2, step 4672/7134 completed (loss: 0.07165231555700302, acc: 0.9808362126350403)
[2025-02-13 03:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:34][root][INFO] - Training Epoch: 1/2, step 4673/7134 completed (loss: 0.0522821880877018, acc: 0.9875195026397705)
[2025-02-13 03:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:34][root][INFO] - Training Epoch: 1/2, step 4674/7134 completed (loss: 0.05174273997545242, acc: 0.9857369065284729)
[2025-02-13 03:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:35][root][INFO] - Training Epoch: 1/2, step 4675/7134 completed (loss: 0.03815387189388275, acc: 0.9910827875137329)
[2025-02-13 03:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:35][root][INFO] - Training Epoch: 1/2, step 4676/7134 completed (loss: 0.03879246488213539, acc: 0.9894578456878662)
[2025-02-13 03:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:35][root][INFO] - Training Epoch: 1/2, step 4677/7134 completed (loss: 0.04735739901661873, acc: 0.9824561476707458)
[2025-02-13 03:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36][root][INFO] - Training Epoch: 1/2, step 4678/7134 completed (loss: 0.04890033230185509, acc: 0.9881129264831543)
[2025-02-13 03:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36][root][INFO] - Training Epoch: 1/2, step 4679/7134 completed (loss: 0.03883614018559456, acc: 0.9903448224067688)
[2025-02-13 03:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:37][root][INFO] - Training Epoch: 1/2, step 4680/7134 completed (loss: 0.051781512796878815, acc: 0.9906542301177979)
[2025-02-13 03:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:37][root][INFO] - Training Epoch: 1/2, step 4681/7134 completed (loss: 0.04296020418405533, acc: 0.9907692074775696)
[2025-02-13 03:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38][root][INFO] - Training Epoch: 1/2, step 4682/7134 completed (loss: 0.022426247596740723, acc: 0.9939172863960266)
[2025-02-13 03:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38][root][INFO] - Training Epoch: 1/2, step 4683/7134 completed (loss: 0.02654234878718853, acc: 0.9900142550468445)
[2025-02-13 03:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38][root][INFO] - Training Epoch: 1/2, step 4684/7134 completed (loss: 0.07166431099176407, acc: 0.9863731861114502)
[2025-02-13 03:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39][root][INFO] - Training Epoch: 1/2, step 4685/7134 completed (loss: 0.02655717171728611, acc: 0.993852436542511)
[2025-02-13 03:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39][root][INFO] - Training Epoch: 1/2, step 4686/7134 completed (loss: 0.039094291627407074, acc: 0.9895226955413818)
[2025-02-13 03:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:40][root][INFO] - Training Epoch: 1/2, step 4687/7134 completed (loss: 0.17596270143985748, acc: 0.9601227045059204)
[2025-02-13 03:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:40][root][INFO] - Training Epoch: 1/2, step 4688/7134 completed (loss: 0.22432361543178558, acc: 0.9507187008857727)
[2025-02-13 03:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41][root][INFO] - Training Epoch: 1/2, step 4689/7134 completed (loss: 0.11848387122154236, acc: 0.9720176458358765)
[2025-02-13 03:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41][root][INFO] - Training Epoch: 1/2, step 4690/7134 completed (loss: 0.09241849929094315, acc: 0.9790356159210205)
[2025-02-13 03:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:42][root][INFO] - Training Epoch: 1/2, step 4691/7134 completed (loss: 0.03848787769675255, acc: 0.9863945841789246)
[2025-02-13 03:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:42][root][INFO] - Training Epoch: 1/2, step 4692/7134 completed (loss: 0.15177208185195923, acc: 0.9617834687232971)
[2025-02-13 03:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:42][root][INFO] - Training Epoch: 1/2, step 4693/7134 completed (loss: 0.14786502718925476, acc: 0.965413510799408)
[2025-02-13 03:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:43][root][INFO] - Training Epoch: 1/2, step 4694/7134 completed (loss: 0.1449364721775055, acc: 0.9655172228813171)
[2025-02-13 03:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:43][root][INFO] - Training Epoch: 1/2, step 4695/7134 completed (loss: 0.05572736635804176, acc: 0.9868637323379517)
[2025-02-13 03:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44][root][INFO] - Training Epoch: 1/2, step 4696/7134 completed (loss: 0.026854457333683968, acc: 0.9851411581039429)
[2025-02-13 03:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44][root][INFO] - Training Epoch: 1/2, step 4697/7134 completed (loss: 0.03322822228074074, acc: 0.9906014800071716)
[2025-02-13 03:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45][root][INFO] - Training Epoch: 1/2, step 4698/7134 completed (loss: 0.13628661632537842, acc: 0.9656488299369812)
[2025-02-13 03:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45][root][INFO] - Training Epoch: 1/2, step 4699/7134 completed (loss: 0.05692625045776367, acc: 0.9798902869224548)
[2025-02-13 03:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:46][root][INFO] - Training Epoch: 1/2, step 4700/7134 completed (loss: 0.07018474489450455, acc: 0.9821162223815918)
[2025-02-13 03:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:46][root][INFO] - Training Epoch: 1/2, step 4701/7134 completed (loss: 0.134510800242424, acc: 0.9584664702415466)
[2025-02-13 03:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:46][root][INFO] - Training Epoch: 1/2, step 4702/7134 completed (loss: 0.032776061445474625, acc: 0.9888535141944885)
[2025-02-13 03:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47][root][INFO] - Training Epoch: 1/2, step 4703/7134 completed (loss: 0.043474145233631134, acc: 0.9878378510475159)
[2025-02-13 03:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47][root][INFO] - Training Epoch: 1/2, step 4704/7134 completed (loss: 0.031626638025045395, acc: 0.991793692111969)
[2025-02-13 03:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:48][root][INFO] - Training Epoch: 1/2, step 4705/7134 completed (loss: 0.030474049970507622, acc: 0.988304078578949)
[2025-02-13 03:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:48][root][INFO] - Training Epoch: 1/2, step 4706/7134 completed (loss: 0.0700407326221466, acc: 0.9766355156898499)
[2025-02-13 03:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49][root][INFO] - Training Epoch: 1/2, step 4707/7134 completed (loss: 0.19906824827194214, acc: 0.9523809552192688)
[2025-02-13 03:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49][root][INFO] - Training Epoch: 1/2, step 4708/7134 completed (loss: 0.03494805842638016, acc: 0.9850373864173889)
[2025-02-13 03:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49][root][INFO] - Training Epoch: 1/2, step 4709/7134 completed (loss: 0.024137884378433228, acc: 0.9934123754501343)
[2025-02-13 03:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50][root][INFO] - Training Epoch: 1/2, step 4710/7134 completed (loss: 0.04445933178067207, acc: 0.9868995547294617)
[2025-02-13 03:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50][root][INFO] - Training Epoch: 1/2, step 4711/7134 completed (loss: 0.080733522772789, acc: 0.9813242554664612)
[2025-02-13 03:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:51][root][INFO] - Training Epoch: 1/2, step 4712/7134 completed (loss: 0.10303386300802231, acc: 0.9696969985961914)
[2025-02-13 03:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:51][root][INFO] - Training Epoch: 1/2, step 4713/7134 completed (loss: 0.02296682447195053, acc: 0.9946428537368774)
[2025-02-13 03:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:52][root][INFO] - Training Epoch: 1/2, step 4714/7134 completed (loss: 0.06433885544538498, acc: 0.9813084006309509)
[2025-02-13 03:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:52][root][INFO] - Training Epoch: 1/2, step 4715/7134 completed (loss: 0.11740106344223022, acc: 0.9722814559936523)
[2025-02-13 03:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53][root][INFO] - Training Epoch: 1/2, step 4716/7134 completed (loss: 0.014040954411029816, acc: 0.9923780560493469)
[2025-02-13 03:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53][root][INFO] - Training Epoch: 1/2, step 4717/7134 completed (loss: 0.03510129451751709, acc: 0.9940740466117859)
[2025-02-13 03:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53][root][INFO] - Training Epoch: 1/2, step 4718/7134 completed (loss: 0.036423444747924805, acc: 0.9850993156433105)
[2025-02-13 03:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54][root][INFO] - Training Epoch: 1/2, step 4719/7134 completed (loss: 0.02762874960899353, acc: 0.99262535572052)
[2025-02-13 03:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54][root][INFO] - Training Epoch: 1/2, step 4720/7134 completed (loss: 0.017243362963199615, acc: 0.9957924485206604)
[2025-02-13 03:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55][root][INFO] - Training Epoch: 1/2, step 4721/7134 completed (loss: 0.0066756317391991615, acc: 0.9971181750297546)
[2025-02-13 03:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55][root][INFO] - Training Epoch: 1/2, step 4722/7134 completed (loss: 0.02049136534333229, acc: 0.9940828680992126)
[2025-02-13 03:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55][root][INFO] - Training Epoch: 1/2, step 4723/7134 completed (loss: 0.01579616777598858, acc: 0.9954268336296082)
[2025-02-13 03:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:56][root][INFO] - Training Epoch: 1/2, step 4724/7134 completed (loss: 0.03605515882372856, acc: 0.9896449446678162)
[2025-02-13 03:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:56][root][INFO] - Training Epoch: 1/2, step 4725/7134 completed (loss: 0.017582720145583153, acc: 0.9947229623794556)
[2025-02-13 03:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57][root][INFO] - Training Epoch: 1/2, step 4726/7134 completed (loss: 0.042403921484947205, acc: 0.9829787015914917)
[2025-02-13 03:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57][root][INFO] - Training Epoch: 1/2, step 4727/7134 completed (loss: 0.039005402475595474, acc: 0.9874607920646667)
[2025-02-13 03:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58][root][INFO] - Training Epoch: 1/2, step 4728/7134 completed (loss: 0.04095790162682533, acc: 0.9835255146026611)
[2025-02-13 03:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58][root][INFO] - Training Epoch: 1/2, step 4729/7134 completed (loss: 0.01691528968513012, acc: 0.9957746267318726)
[2025-02-13 03:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58][root][INFO] - Training Epoch: 1/2, step 4730/7134 completed (loss: 0.020795738324522972, acc: 0.9908116459846497)
[2025-02-13 03:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59][root][INFO] - Training Epoch: 1/2, step 4731/7134 completed (loss: 0.04720832034945488, acc: 0.989313006401062)
[2025-02-13 03:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59][root][INFO] - Training Epoch: 1/2, step 4732/7134 completed (loss: 0.07676060497760773, acc: 0.9800363183021545)
[2025-02-13 03:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00][root][INFO] - Training Epoch: 1/2, step 4733/7134 completed (loss: 0.011007961817085743, acc: 0.9967690110206604)
[2025-02-13 03:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00][root][INFO] - Training Epoch: 1/2, step 4734/7134 completed (loss: 0.04608066380023956, acc: 0.9841017723083496)
[2025-02-13 03:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:01][root][INFO] - Training Epoch: 1/2, step 4735/7134 completed (loss: 0.029678093269467354, acc: 0.9924924969673157)
[2025-02-13 03:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:01][root][INFO] - Training Epoch: 1/2, step 4736/7134 completed (loss: 0.05655711516737938, acc: 0.990138053894043)
[2025-02-13 03:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:01][root][INFO] - Training Epoch: 1/2, step 4737/7134 completed (loss: 0.025914756581187248, acc: 0.9952977895736694)
[2025-02-13 03:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:02][root][INFO] - Training Epoch: 1/2, step 4738/7134 completed (loss: 0.023319708183407784, acc: 0.9924127459526062)
[2025-02-13 03:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:02][root][INFO] - Training Epoch: 1/2, step 4739/7134 completed (loss: 0.09507331997156143, acc: 0.9728434681892395)
[2025-02-13 03:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03][root][INFO] - Training Epoch: 1/2, step 4740/7134 completed (loss: 0.08696053922176361, acc: 0.9745509028434753)
[2025-02-13 03:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03][root][INFO] - Training Epoch: 1/2, step 4741/7134 completed (loss: 0.06415818631649017, acc: 0.9778188467025757)
[2025-02-13 03:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03][root][INFO] - Training Epoch: 1/2, step 4742/7134 completed (loss: 0.11786247789859772, acc: 0.9663716554641724)
[2025-02-13 03:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:04][root][INFO] - Training Epoch: 1/2, step 4743/7134 completed (loss: 0.04117059335112572, acc: 0.9875444769859314)
[2025-02-13 03:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:04][root][INFO] - Training Epoch: 1/2, step 4744/7134 completed (loss: 0.04981675744056702, acc: 0.9876033067703247)
[2025-02-13 03:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:05][root][INFO] - Training Epoch: 1/2, step 4745/7134 completed (loss: 0.05567405745387077, acc: 0.9832776188850403)
[2025-02-13 03:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:05][root][INFO] - Training Epoch: 1/2, step 4746/7134 completed (loss: 0.038780923932790756, acc: 0.9908088445663452)
[2025-02-13 03:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06][root][INFO] - Training Epoch: 1/2, step 4747/7134 completed (loss: 0.0422527976334095, acc: 0.9878542423248291)
[2025-02-13 03:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06][root][INFO] - Training Epoch: 1/2, step 4748/7134 completed (loss: 0.06402283906936646, acc: 0.9778761267662048)
[2025-02-13 03:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06][root][INFO] - Training Epoch: 1/2, step 4749/7134 completed (loss: 0.046855513006448746, acc: 0.9853333234786987)
[2025-02-13 03:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:07][root][INFO] - Training Epoch: 1/2, step 4750/7134 completed (loss: 0.09236421436071396, acc: 0.978805422782898)
[2025-02-13 03:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:07][root][INFO] - Training Epoch: 1/2, step 4751/7134 completed (loss: 0.031952958554029465, acc: 0.9915134310722351)
[2025-02-13 03:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08][root][INFO] - Training Epoch: 1/2, step 4752/7134 completed (loss: 0.06199265271425247, acc: 0.9877111911773682)
[2025-02-13 03:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08][root][INFO] - Training Epoch: 1/2, step 4753/7134 completed (loss: 0.10134457051753998, acc: 0.9819276928901672)
[2025-02-13 03:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:09][root][INFO] - Training Epoch: 1/2, step 4754/7134 completed (loss: 0.03289349004626274, acc: 0.9941262602806091)
[2025-02-13 03:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:09][root][INFO] - Training Epoch: 1/2, step 4755/7134 completed (loss: 0.05724572390317917, acc: 0.9875173568725586)
[2025-02-13 03:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:10][root][INFO] - Training Epoch: 1/2, step 4756/7134 completed (loss: 0.06683874875307083, acc: 0.9895397424697876)
[2025-02-13 03:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:10][root][INFO] - Training Epoch: 1/2, step 4757/7134 completed (loss: 0.027132583782076836, acc: 0.9942594766616821)
[2025-02-13 03:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11][root][INFO] - Training Epoch: 1/2, step 4758/7134 completed (loss: 0.04371710494160652, acc: 0.9873949289321899)
[2025-02-13 03:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11][root][INFO] - Training Epoch: 1/2, step 4759/7134 completed (loss: 0.022210435941815376, acc: 0.9948275685310364)
[2025-02-13 03:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11][root][INFO] - Training Epoch: 1/2, step 4760/7134 completed (loss: 0.04954688996076584, acc: 0.9875776171684265)
[2025-02-13 03:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:12][root][INFO] - Training Epoch: 1/2, step 4761/7134 completed (loss: 0.05565335974097252, acc: 0.9908397197723389)
[2025-02-13 03:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:12][root][INFO] - Training Epoch: 1/2, step 4762/7134 completed (loss: 0.05191626772284508, acc: 0.9878378510475159)
[2025-02-13 03:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13][root][INFO] - Training Epoch: 1/2, step 4763/7134 completed (loss: 0.05425511673092842, acc: 0.9878419637680054)
[2025-02-13 03:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13][root][INFO] - Training Epoch: 1/2, step 4764/7134 completed (loss: 0.057984232902526855, acc: 0.982694685459137)
[2025-02-13 03:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:14][root][INFO] - Training Epoch: 1/2, step 4765/7134 completed (loss: 0.07790574431419373, acc: 0.9884892106056213)
[2025-02-13 03:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:14][root][INFO] - Training Epoch: 1/2, step 4766/7134 completed (loss: 0.040712080895900726, acc: 0.9884169697761536)
[2025-02-13 03:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:14][root][INFO] - Training Epoch: 1/2, step 4767/7134 completed (loss: 0.056992121040821075, acc: 0.9821882843971252)
[2025-02-13 03:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:15][root][INFO] - Training Epoch: 1/2, step 4768/7134 completed (loss: 0.06109675392508507, acc: 0.9837398529052734)
[2025-02-13 03:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:15][root][INFO] - Training Epoch: 1/2, step 4769/7134 completed (loss: 0.07637359946966171, acc: 0.9871588945388794)
[2025-02-13 03:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16][root][INFO] - Training Epoch: 1/2, step 4770/7134 completed (loss: 0.023612437769770622, acc: 0.991919219493866)
[2025-02-13 03:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16][root][INFO] - Training Epoch: 1/2, step 4771/7134 completed (loss: 0.07356055825948715, acc: 0.9781976938247681)
[2025-02-13 03:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:17][root][INFO] - Training Epoch: 1/2, step 4772/7134 completed (loss: 0.05288597196340561, acc: 0.9904305934906006)
[2025-02-13 03:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:17][root][INFO] - Training Epoch: 1/2, step 4773/7134 completed (loss: 0.02306576631963253, acc: 0.9945205450057983)
[2025-02-13 03:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:17][root][INFO] - Training Epoch: 1/2, step 4774/7134 completed (loss: 0.04037431627511978, acc: 0.9915730357170105)
[2025-02-13 03:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:18][root][INFO] - Training Epoch: 1/2, step 4775/7134 completed (loss: 0.04950705170631409, acc: 0.988727867603302)
[2025-02-13 03:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:18][root][INFO] - Training Epoch: 1/2, step 4776/7134 completed (loss: 0.05308601260185242, acc: 0.9855538010597229)
[2025-02-13 03:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19][root][INFO] - Training Epoch: 1/2, step 4777/7134 completed (loss: 0.021396318450570107, acc: 0.994991660118103)
[2025-02-13 03:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19][root][INFO] - Training Epoch: 1/2, step 4778/7134 completed (loss: 0.033724892884492874, acc: 0.9912472367286682)
[2025-02-13 03:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:20][root][INFO] - Training Epoch: 1/2, step 4779/7134 completed (loss: 0.014645390212535858, acc: 0.9944751262664795)
[2025-02-13 03:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:20][root][INFO] - Training Epoch: 1/2, step 4780/7134 completed (loss: 0.011731420643627644, acc: 0.9953271150588989)
[2025-02-13 03:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:20][root][INFO] - Training Epoch: 1/2, step 4781/7134 completed (loss: 0.018613040447235107, acc: 0.9921721816062927)
[2025-02-13 03:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21][root][INFO] - Training Epoch: 1/2, step 4782/7134 completed (loss: 0.01871632970869541, acc: 0.9935170412063599)
[2025-02-13 03:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21][root][INFO] - Training Epoch: 1/2, step 4783/7134 completed (loss: 0.06231111288070679, acc: 0.9786477088928223)
[2025-02-13 03:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:22][root][INFO] - Training Epoch: 1/2, step 4784/7134 completed (loss: 0.018788553774356842, acc: 0.9986357688903809)
[2025-02-13 03:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:22][root][INFO] - Training Epoch: 1/2, step 4785/7134 completed (loss: 0.046004150062799454, acc: 0.9887640476226807)
[2025-02-13 03:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:23][root][INFO] - Training Epoch: 1/2, step 4786/7134 completed (loss: 0.03679598495364189, acc: 0.9922360181808472)
[2025-02-13 03:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:23][root][INFO] - Training Epoch: 1/2, step 4787/7134 completed (loss: 0.019460497424006462, acc: 0.9952531456947327)
[2025-02-13 03:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:23][root][INFO] - Training Epoch: 1/2, step 4788/7134 completed (loss: 0.020679721608757973, acc: 0.9946808218955994)
[2025-02-13 03:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24][root][INFO] - Training Epoch: 1/2, step 4789/7134 completed (loss: 0.03277747705578804, acc: 0.9920381903648376)
[2025-02-13 03:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24][root][INFO] - Training Epoch: 1/2, step 4790/7134 completed (loss: 0.024991994723677635, acc: 0.9959239363670349)
[2025-02-13 03:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:25][root][INFO] - Training Epoch: 1/2, step 4791/7134 completed (loss: 0.01571791060268879, acc: 0.9962756037712097)
[2025-02-13 03:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:25][root][INFO] - Training Epoch: 1/2, step 4792/7134 completed (loss: 0.037658415734767914, acc: 0.9882943034172058)
[2025-02-13 03:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:25][root][INFO] - Training Epoch: 1/2, step 4793/7134 completed (loss: 0.05394429713487625, acc: 0.9808219075202942)
[2025-02-13 03:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26][root][INFO] - Training Epoch: 1/2, step 4794/7134 completed (loss: 0.07933393120765686, acc: 0.977142870426178)
[2025-02-13 03:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26][root][INFO] - Training Epoch: 1/2, step 4795/7134 completed (loss: 0.018570026382803917, acc: 0.9929378628730774)
[2025-02-13 03:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:27][root][INFO] - Training Epoch: 1/2, step 4796/7134 completed (loss: 0.02858002297580242, acc: 0.9905660152435303)
[2025-02-13 03:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:27][root][INFO] - Training Epoch: 1/2, step 4797/7134 completed (loss: 0.03247975558042526, acc: 0.992546558380127)
[2025-02-13 03:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:28][root][INFO] - Training Epoch: 1/2, step 4798/7134 completed (loss: 0.03205835446715355, acc: 0.9920106530189514)
[2025-02-13 03:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:28][root][INFO] - Training Epoch: 1/2, step 4799/7134 completed (loss: 0.02251342125236988, acc: 0.9886845946311951)
[2025-02-13 03:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29][root][INFO] - Training Epoch: 1/2, step 4800/7134 completed (loss: 0.03323342651128769, acc: 0.9895678162574768)
[2025-02-13 03:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29][root][INFO] - Training Epoch: 1/2, step 4801/7134 completed (loss: 0.01968988962471485, acc: 0.9954751133918762)
[2025-02-13 03:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29][root][INFO] - Training Epoch: 1/2, step 4802/7134 completed (loss: 0.04010945186018944, acc: 0.9842932224273682)
[2025-02-13 03:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:30][root][INFO] - Training Epoch: 1/2, step 4803/7134 completed (loss: 0.04730001837015152, acc: 0.9838235378265381)
[2025-02-13 03:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:30][root][INFO] - Training Epoch: 1/2, step 4804/7134 completed (loss: 0.0289393812417984, acc: 0.9938461780548096)
[2025-02-13 03:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:31][root][INFO] - Training Epoch: 1/2, step 4805/7134 completed (loss: 0.04857954382896423, acc: 0.9886731505393982)
[2025-02-13 03:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:31][root][INFO] - Training Epoch: 1/2, step 4806/7134 completed (loss: 0.03796844929456711, acc: 0.9916805028915405)
[2025-02-13 03:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32][root][INFO] - Training Epoch: 1/2, step 4807/7134 completed (loss: 0.05043459311127663, acc: 0.9908814430236816)
[2025-02-13 03:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32][root][INFO] - Training Epoch: 1/2, step 4808/7134 completed (loss: 0.03520367294549942, acc: 0.9893617033958435)
[2025-02-13 03:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32][root][INFO] - Training Epoch: 1/2, step 4809/7134 completed (loss: 0.034884434193372726, acc: 0.9920634627342224)
[2025-02-13 03:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:33][root][INFO] - Training Epoch: 1/2, step 4810/7134 completed (loss: 0.06885140389204025, acc: 0.9857346415519714)
[2025-02-13 03:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:33][root][INFO] - Training Epoch: 1/2, step 4811/7134 completed (loss: 0.04588901251554489, acc: 0.9864341020584106)
[2025-02-13 03:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:34][root][INFO] - Training Epoch: 1/2, step 4812/7134 completed (loss: 0.0771193653345108, acc: 0.9833101630210876)
[2025-02-13 03:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:34][root][INFO] - Training Epoch: 1/2, step 4813/7134 completed (loss: 0.0074178739450871944, acc: 0.9974026083946228)
[2025-02-13 03:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35][root][INFO] - Training Epoch: 1/2, step 4814/7134 completed (loss: 0.03503098711371422, acc: 0.9923469424247742)
[2025-02-13 03:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35][root][INFO] - Training Epoch: 1/2, step 4815/7134 completed (loss: 0.0051428587175905704, acc: 1.0)
[2025-02-13 03:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35][root][INFO] - Training Epoch: 1/2, step 4816/7134 completed (loss: 0.017967285588383675, acc: 0.9962871074676514)
[2025-02-13 03:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:36][root][INFO] - Training Epoch: 1/2, step 4817/7134 completed (loss: 0.021632689982652664, acc: 0.9927710890769958)
[2025-02-13 03:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:36][root][INFO] - Training Epoch: 1/2, step 4818/7134 completed (loss: 0.018301500007510185, acc: 0.9969230890274048)
[2025-02-13 03:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37][root][INFO] - Training Epoch: 1/2, step 4819/7134 completed (loss: 0.022175345569849014, acc: 0.9918864369392395)
[2025-02-13 03:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37][root][INFO] - Training Epoch: 1/2, step 4820/7134 completed (loss: 0.015615523792803288, acc: 0.993565022945404)
[2025-02-13 03:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:38][root][INFO] - Training Epoch: 1/2, step 4821/7134 completed (loss: 0.02667931281030178, acc: 0.9958100318908691)
[2025-02-13 03:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:38][root][INFO] - Training Epoch: 1/2, step 4822/7134 completed (loss: 0.03560531884431839, acc: 0.9943289160728455)
[2025-02-13 03:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39][root][INFO] - Training Epoch: 1/2, step 4823/7134 completed (loss: 0.061440013349056244, acc: 0.9857819676399231)
[2025-02-13 03:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39][root][INFO] - Training Epoch: 1/2, step 4824/7134 completed (loss: 0.05358482897281647, acc: 0.9840182662010193)
[2025-02-13 03:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39][root][INFO] - Training Epoch: 1/2, step 4825/7134 completed (loss: 0.013771822676062584, acc: 0.9963503479957581)
[2025-02-13 03:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:40][root][INFO] - Training Epoch: 1/2, step 4826/7134 completed (loss: 0.02088000625371933, acc: 0.995708167552948)
[2025-02-13 03:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:40][root][INFO] - Training Epoch: 1/2, step 4827/7134 completed (loss: 0.026278646662831306, acc: 0.9930796027183533)
[2025-02-13 03:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41][root][INFO] - Training Epoch: 1/2, step 4828/7134 completed (loss: 0.014620652422308922, acc: 0.9951397180557251)
[2025-02-13 03:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41][root][INFO] - Training Epoch: 1/2, step 4829/7134 completed (loss: 0.010701687075197697, acc: 0.9987179636955261)
[2025-02-13 03:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:42][root][INFO] - Training Epoch: 1/2, step 4830/7134 completed (loss: 0.013418967835605145, acc: 0.9955621361732483)
[2025-02-13 03:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:42][root][INFO] - Training Epoch: 1/2, step 4831/7134 completed (loss: 0.008490551263093948, acc: 0.9956140518188477)
[2025-02-13 03:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43][root][INFO] - Training Epoch: 1/2, step 4832/7134 completed (loss: 0.022135112434625626, acc: 0.9967426657676697)
[2025-02-13 03:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43][root][INFO] - Training Epoch: 1/2, step 4833/7134 completed (loss: 0.010815752670168877, acc: 0.9974586963653564)
[2025-02-13 03:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43][root][INFO] - Training Epoch: 1/2, step 4834/7134 completed (loss: 0.014011344872415066, acc: 0.9964285492897034)
[2025-02-13 03:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:44][root][INFO] - Training Epoch: 1/2, step 4835/7134 completed (loss: 0.015815867111086845, acc: 0.9918919205665588)
[2025-02-13 03:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:44][root][INFO] - Training Epoch: 1/2, step 4836/7134 completed (loss: 0.018164344131946564, acc: 0.9954198598861694)
[2025-02-13 03:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45][root][INFO] - Training Epoch: 1/2, step 4837/7134 completed (loss: 0.09502439200878143, acc: 0.9797794222831726)
[2025-02-13 03:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45][root][INFO] - Training Epoch: 1/2, step 4838/7134 completed (loss: 0.020056311041116714, acc: 0.9967793822288513)
[2025-02-13 03:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:46][root][INFO] - Training Epoch: 1/2, step 4839/7134 completed (loss: 0.0657646581530571, acc: 0.9862306118011475)
[2025-02-13 03:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:46][root][INFO] - Training Epoch: 1/2, step 4840/7134 completed (loss: 0.025146018713712692, acc: 0.9913294911384583)
[2025-02-13 03:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:46][root][INFO] - Training Epoch: 1/2, step 4841/7134 completed (loss: 0.044836003333330154, acc: 0.9880775213241577)
[2025-02-13 03:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47][root][INFO] - Training Epoch: 1/2, step 4842/7134 completed (loss: 0.05493598058819771, acc: 0.9859693646430969)
[2025-02-13 03:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47][root][INFO] - Training Epoch: 1/2, step 4843/7134 completed (loss: 0.035312920808792114, acc: 0.9881129264831543)
[2025-02-13 03:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:48][root][INFO] - Training Epoch: 1/2, step 4844/7134 completed (loss: 0.023904697969555855, acc: 0.993537962436676)
[2025-02-13 03:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:48][root][INFO] - Training Epoch: 1/2, step 4845/7134 completed (loss: 0.04082754626870155, acc: 0.9877049326896667)
[2025-02-13 03:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:49][root][INFO] - Training Epoch: 1/2, step 4846/7134 completed (loss: 0.03257140517234802, acc: 0.9905405640602112)
[2025-02-13 03:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:49][root][INFO] - Training Epoch: 1/2, step 4847/7134 completed (loss: 0.026809312403202057, acc: 0.993261456489563)
[2025-02-13 03:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:50][root][INFO] - Training Epoch: 1/2, step 4848/7134 completed (loss: 0.03051636554300785, acc: 0.9944751262664795)
[2025-02-13 03:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:50][root][INFO] - Training Epoch: 1/2, step 4849/7134 completed (loss: 0.02417573519051075, acc: 0.9940564632415771)
[2025-02-13 03:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:50][root][INFO] - Training Epoch: 1/2, step 4850/7134 completed (loss: 0.037227775901556015, acc: 0.9889841079711914)
[2025-02-13 03:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51][root][INFO] - Training Epoch: 1/2, step 4851/7134 completed (loss: 0.04156147316098213, acc: 0.985981285572052)
[2025-02-13 03:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51][root][INFO] - Training Epoch: 1/2, step 4852/7134 completed (loss: 0.038126010447740555, acc: 0.9880159497261047)
[2025-02-13 03:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:52][root][INFO] - Training Epoch: 1/2, step 4853/7134 completed (loss: 0.08268262445926666, acc: 0.980028510093689)
[2025-02-13 03:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:52][root][INFO] - Training Epoch: 1/2, step 4854/7134 completed (loss: 0.02090795338153839, acc: 0.9933333396911621)
[2025-02-13 03:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:53][root][INFO] - Training Epoch: 1/2, step 4855/7134 completed (loss: 0.028574706986546516, acc: 0.9891172647476196)
[2025-02-13 03:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:53][root][INFO] - Training Epoch: 1/2, step 4856/7134 completed (loss: 0.031735677272081375, acc: 0.9898219108581543)
[2025-02-13 03:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:53][root][INFO] - Training Epoch: 1/2, step 4857/7134 completed (loss: 0.015931284055113792, acc: 0.9956834316253662)
[2025-02-13 03:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54][root][INFO] - Training Epoch: 1/2, step 4858/7134 completed (loss: 0.027915168553590775, acc: 0.9928774833679199)
[2025-02-13 03:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54][root][INFO] - Training Epoch: 1/2, step 4859/7134 completed (loss: 0.02873913384974003, acc: 0.9909909963607788)
[2025-02-13 03:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55][root][INFO] - Training Epoch: 1/2, step 4860/7134 completed (loss: 0.022918233647942543, acc: 0.9945504069328308)
[2025-02-13 03:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55][root][INFO] - Training Epoch: 1/2, step 4861/7134 completed (loss: 0.013455118052661419, acc: 0.9959568977355957)
[2025-02-13 03:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:56][root][INFO] - Training Epoch: 1/2, step 4862/7134 completed (loss: 0.022100482136011124, acc: 0.995708167552948)
[2025-02-13 03:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:56][root][INFO] - Training Epoch: 1/2, step 4863/7134 completed (loss: 0.026375960558652878, acc: 0.9908758997917175)
[2025-02-13 03:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:56][root][INFO] - Training Epoch: 1/2, step 4864/7134 completed (loss: 0.03078542836010456, acc: 0.989230751991272)
[2025-02-13 03:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57][root][INFO] - Training Epoch: 1/2, step 4865/7134 completed (loss: 0.041439980268478394, acc: 0.990231990814209)
[2025-02-13 03:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57][root][INFO] - Training Epoch: 1/2, step 4866/7134 completed (loss: 0.027408767491579056, acc: 0.9922178983688354)
[2025-02-13 03:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58][root][INFO] - Training Epoch: 1/2, step 4867/7134 completed (loss: 0.04776596277952194, acc: 0.9907407164573669)
[2025-02-13 03:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58][root][INFO] - Training Epoch: 1/2, step 4868/7134 completed (loss: 0.05405724421143532, acc: 0.9844961166381836)
[2025-02-13 03:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59][root][INFO] - Training Epoch: 1/2, step 4869/7134 completed (loss: 0.05303440988063812, acc: 0.9911110997200012)
[2025-02-13 03:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59][root][INFO] - Training Epoch: 1/2, step 4870/7134 completed (loss: 0.019733205437660217, acc: 0.9953051805496216)
[2025-02-13 03:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59][root][INFO] - Training Epoch: 1/2, step 4871/7134 completed (loss: 0.021646080538630486, acc: 0.9938271641731262)
[2025-02-13 03:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00][root][INFO] - Training Epoch: 1/2, step 4872/7134 completed (loss: 0.02298661507666111, acc: 0.9944367408752441)
[2025-02-13 03:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00][root][INFO] - Training Epoch: 1/2, step 4873/7134 completed (loss: 0.029079429805278778, acc: 0.98828125)
[2025-02-13 03:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01][root][INFO] - Training Epoch: 1/2, step 4874/7134 completed (loss: 0.02436714433133602, acc: 0.9897142648696899)
[2025-02-13 03:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01][root][INFO] - Training Epoch: 1/2, step 4875/7134 completed (loss: 0.02769126370549202, acc: 0.9930651783943176)
[2025-02-13 03:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02][root][INFO] - Training Epoch: 1/2, step 4876/7134 completed (loss: 0.07544120401144028, acc: 0.9830867052078247)
[2025-02-13 03:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02][root][INFO] - Training Epoch: 1/2, step 4877/7134 completed (loss: 0.04614369943737984, acc: 0.9858064651489258)
[2025-02-13 03:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03][root][INFO] - Training Epoch: 1/2, step 4878/7134 completed (loss: 0.03641451150178909, acc: 0.9837996959686279)
[2025-02-13 03:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03][root][INFO] - Training Epoch: 1/2, step 4879/7134 completed (loss: 0.06910588592290878, acc: 0.983433723449707)
[2025-02-13 03:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03][root][INFO] - Training Epoch: 1/2, step 4880/7134 completed (loss: 0.009820890612900257, acc: 0.9987030029296875)
[2025-02-13 03:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:04][root][INFO] - Training Epoch: 1/2, step 4881/7134 completed (loss: 0.04369685798883438, acc: 0.9883211851119995)
[2025-02-13 03:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:04][root][INFO] - Training Epoch: 1/2, step 4882/7134 completed (loss: 0.03962068632245064, acc: 0.9868420958518982)
[2025-02-13 03:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05][root][INFO] - Training Epoch: 1/2, step 4883/7134 completed (loss: 0.030187111347913742, acc: 0.9868612885475159)
[2025-02-13 03:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05][root][INFO] - Training Epoch: 1/2, step 4884/7134 completed (loss: 0.01224414724856615, acc: 0.9953917264938354)
[2025-02-13 03:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06][root][INFO] - Training Epoch: 1/2, step 4885/7134 completed (loss: 0.012998605147004128, acc: 0.9973614811897278)
[2025-02-13 03:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06][root][INFO] - Training Epoch: 1/2, step 4886/7134 completed (loss: 0.028250426054000854, acc: 0.9948717951774597)
[2025-02-13 03:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06][root][INFO] - Training Epoch: 1/2, step 4887/7134 completed (loss: 0.012984138913452625, acc: 0.9964157938957214)
[2025-02-13 03:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07][root][INFO] - Training Epoch: 1/2, step 4888/7134 completed (loss: 0.03553193435072899, acc: 0.9917582273483276)
[2025-02-13 03:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07][root][INFO] - Training Epoch: 1/2, step 4889/7134 completed (loss: 0.014053340069949627, acc: 0.9951865077018738)
[2025-02-13 03:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08][root][INFO] - Training Epoch: 1/2, step 4890/7134 completed (loss: 0.03676615282893181, acc: 0.9976190328598022)
[2025-02-13 03:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08][root][INFO] - Training Epoch: 1/2, step 4891/7134 completed (loss: 0.05612046271562576, acc: 0.9888059496879578)
[2025-02-13 03:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:09][root][INFO] - Training Epoch: 1/2, step 4892/7134 completed (loss: 0.058492161333560944, acc: 0.9848993420600891)
[2025-02-13 03:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:09][root][INFO] - Training Epoch: 1/2, step 4893/7134 completed (loss: 0.21030384302139282, acc: 0.9498680830001831)
[2025-02-13 03:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:09][root][INFO] - Training Epoch: 1/2, step 4894/7134 completed (loss: 0.021920185536146164, acc: 0.9942362904548645)
[2025-02-13 03:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10][root][INFO] - Training Epoch: 1/2, step 4895/7134 completed (loss: 0.018076689913868904, acc: 0.9937499761581421)
[2025-02-13 03:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10][root][INFO] - Training Epoch: 1/2, step 4896/7134 completed (loss: 0.007119426503777504, acc: 0.9985954761505127)
[2025-02-13 03:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11][root][INFO] - Training Epoch: 1/2, step 4897/7134 completed (loss: 0.04071807861328125, acc: 0.9894737005233765)
[2025-02-13 03:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11][root][INFO] - Training Epoch: 1/2, step 4898/7134 completed (loss: 0.04244741424918175, acc: 0.9886792302131653)
[2025-02-13 03:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12][root][INFO] - Training Epoch: 1/2, step 4899/7134 completed (loss: 0.026191217824816704, acc: 0.9941775798797607)
[2025-02-13 03:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12][root][INFO] - Training Epoch: 1/2, step 4900/7134 completed (loss: 0.0332195982336998, acc: 0.99210524559021)
[2025-02-13 03:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12][root][INFO] - Training Epoch: 1/2, step 4901/7134 completed (loss: 0.017206400632858276, acc: 0.995488703250885)
[2025-02-13 03:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13][root][INFO] - Training Epoch: 1/2, step 4902/7134 completed (loss: 0.011249789036810398, acc: 0.9946949481964111)
[2025-02-13 03:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13][root][INFO] - Training Epoch: 1/2, step 4903/7134 completed (loss: 0.01609210856258869, acc: 0.9946236610412598)
[2025-02-13 03:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14][root][INFO] - Training Epoch: 1/2, step 4904/7134 completed (loss: 0.01150213461369276, acc: 0.9940298795700073)
[2025-02-13 03:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14][root][INFO] - Training Epoch: 1/2, step 4905/7134 completed (loss: 0.006566352676600218, acc: 0.9984076619148254)
[2025-02-13 03:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15][root][INFO] - Training Epoch: 1/2, step 4906/7134 completed (loss: 0.022690119221806526, acc: 0.9915682673454285)
[2025-02-13 03:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15][root][INFO] - Training Epoch: 1/2, step 4907/7134 completed (loss: 0.014676680788397789, acc: 0.9984177350997925)
[2025-02-13 03:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15][root][INFO] - Training Epoch: 1/2, step 4908/7134 completed (loss: 0.008779067546129227, acc: 0.995199978351593)
[2025-02-13 03:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16][root][INFO] - Training Epoch: 1/2, step 4909/7134 completed (loss: 0.016250241547822952, acc: 0.9983792304992676)
[2025-02-13 03:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16][root][INFO] - Training Epoch: 1/2, step 4910/7134 completed (loss: 0.02829337865114212, acc: 0.9905956387519836)
[2025-02-13 03:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17][root][INFO] - Training Epoch: 1/2, step 4911/7134 completed (loss: 0.02657257951796055, acc: 0.9907651543617249)
[2025-02-13 03:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17][root][INFO] - Training Epoch: 1/2, step 4912/7134 completed (loss: 0.021422257646918297, acc: 0.9939467310905457)
[2025-02-13 03:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18][root][INFO] - Training Epoch: 1/2, step 4913/7134 completed (loss: 0.017346197739243507, acc: 0.9949324131011963)
[2025-02-13 03:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18][root][INFO] - Training Epoch: 1/2, step 4914/7134 completed (loss: 0.021108316257596016, acc: 0.9923547506332397)
[2025-02-13 03:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18][root][INFO] - Training Epoch: 1/2, step 4915/7134 completed (loss: 0.0031518307514488697, acc: 1.0)
[2025-02-13 03:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19][root][INFO] - Training Epoch: 1/2, step 4916/7134 completed (loss: 0.05363115668296814, acc: 0.9854369163513184)
[2025-02-13 03:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19][root][INFO] - Training Epoch: 1/2, step 4917/7134 completed (loss: 0.042334672063589096, acc: 0.9902642369270325)
[2025-02-13 03:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20][root][INFO] - Training Epoch: 1/2, step 4918/7134 completed (loss: 0.005949188023805618, acc: 0.998633861541748)
[2025-02-13 03:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20][root][INFO] - Training Epoch: 1/2, step 4919/7134 completed (loss: 0.03487054258584976, acc: 0.9894737005233765)
[2025-02-13 03:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21][root][INFO] - Training Epoch: 1/2, step 4920/7134 completed (loss: 0.07036321610212326, acc: 0.9832041263580322)
[2025-02-13 03:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21][root][INFO] - Training Epoch: 1/2, step 4921/7134 completed (loss: 0.03511015698313713, acc: 0.9899569749832153)
[2025-02-13 03:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21][root][INFO] - Training Epoch: 1/2, step 4922/7134 completed (loss: 0.023548224940896034, acc: 0.9918367266654968)
[2025-02-13 03:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22][root][INFO] - Training Epoch: 1/2, step 4923/7134 completed (loss: 0.014743823558092117, acc: 0.9955947399139404)
[2025-02-13 03:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22][root][INFO] - Training Epoch: 1/2, step 4924/7134 completed (loss: 0.026846570894122124, acc: 0.9879999756813049)
[2025-02-13 03:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:23][root][INFO] - Training Epoch: 1/2, step 4925/7134 completed (loss: 0.02643156237900257, acc: 0.9885550737380981)
[2025-02-13 03:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:23][root][INFO] - Training Epoch: 1/2, step 4926/7134 completed (loss: 0.015812117606401443, acc: 0.9956958293914795)
[2025-02-13 03:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24][root][INFO] - Training Epoch: 1/2, step 4927/7134 completed (loss: 0.027506820857524872, acc: 0.994397759437561)
[2025-02-13 03:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24][root][INFO] - Training Epoch: 1/2, step 4928/7134 completed (loss: 0.008237415924668312, acc: 0.9988412261009216)
[2025-02-13 03:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24][root][INFO] - Training Epoch: 1/2, step 4929/7134 completed (loss: 0.017284665256738663, acc: 0.9953488111495972)
[2025-02-13 03:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25][root][INFO] - Training Epoch: 1/2, step 4930/7134 completed (loss: 0.030349431559443474, acc: 0.9909502267837524)
[2025-02-13 03:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25][root][INFO] - Training Epoch: 1/2, step 4931/7134 completed (loss: 0.07475525140762329, acc: 0.9863013625144958)
[2025-02-13 03:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26][root][INFO] - Training Epoch: 1/2, step 4932/7134 completed (loss: 0.026219883933663368, acc: 0.9905533194541931)
[2025-02-13 03:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26][root][INFO] - Training Epoch: 1/2, step 4933/7134 completed (loss: 0.03310948982834816, acc: 0.988875150680542)
[2025-02-13 03:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27][root][INFO] - Training Epoch: 1/2, step 4934/7134 completed (loss: 0.015146064572036266, acc: 0.9918808937072754)
[2025-02-13 03:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27][root][INFO] - Training Epoch: 1/2, step 4935/7134 completed (loss: 0.019946878775954247, acc: 0.9904240965843201)
[2025-02-13 03:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27][root][INFO] - Training Epoch: 1/2, step 4936/7134 completed (loss: 0.036435164511203766, acc: 0.9876543283462524)
[2025-02-13 03:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28][root][INFO] - Training Epoch: 1/2, step 4937/7134 completed (loss: 0.03465724736452103, acc: 0.9895366430282593)
[2025-02-13 03:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28][root][INFO] - Training Epoch: 1/2, step 4938/7134 completed (loss: 0.022929944097995758, acc: 0.9905533194541931)
[2025-02-13 03:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29][root][INFO] - Training Epoch: 1/2, step 4939/7134 completed (loss: 0.028939487412571907, acc: 0.9885807633399963)
[2025-02-13 03:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29][root][INFO] - Training Epoch: 1/2, step 4940/7134 completed (loss: 0.039233047515153885, acc: 0.9924242496490479)
[2025-02-13 03:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30][root][INFO] - Training Epoch: 1/2, step 4941/7134 completed (loss: 0.02317713014781475, acc: 0.9946164488792419)
[2025-02-13 03:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30][root][INFO] - Training Epoch: 1/2, step 4942/7134 completed (loss: 0.029166249558329582, acc: 0.9919571280479431)
[2025-02-13 03:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30][root][INFO] - Training Epoch: 1/2, step 4943/7134 completed (loss: 0.04097069054841995, acc: 0.9890561103820801)
[2025-02-13 03:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31][root][INFO] - Training Epoch: 1/2, step 4944/7134 completed (loss: 0.04647209495306015, acc: 0.9873684048652649)
[2025-02-13 03:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31][root][INFO] - Training Epoch: 1/2, step 4945/7134 completed (loss: 0.0471995547413826, acc: 0.9798387289047241)
[2025-02-13 03:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32][root][INFO] - Training Epoch: 1/2, step 4946/7134 completed (loss: 0.062007032334804535, acc: 0.9802197813987732)
[2025-02-13 03:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32][root][INFO] - Training Epoch: 1/2, step 4947/7134 completed (loss: 0.027949130162596703, acc: 0.9866962432861328)
[2025-02-13 03:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32][root][INFO] - Training Epoch: 1/2, step 4948/7134 completed (loss: 0.03763493895530701, acc: 0.9893048405647278)
[2025-02-13 03:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:33][root][INFO] - Training Epoch: 1/2, step 4949/7134 completed (loss: 0.03902839869260788, acc: 0.9936507940292358)
[2025-02-13 03:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:33][root][INFO] - Training Epoch: 1/2, step 4950/7134 completed (loss: 0.014738819561898708, acc: 0.996497392654419)
[2025-02-13 03:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34][root][INFO] - Training Epoch: 1/2, step 4951/7134 completed (loss: 0.04103146120905876, acc: 0.9857549667358398)
[2025-02-13 03:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34][root][INFO] - Training Epoch: 1/2, step 4952/7134 completed (loss: 0.017802484333515167, acc: 0.996688723564148)
[2025-02-13 03:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35][root][INFO] - Training Epoch: 1/2, step 4953/7134 completed (loss: 0.014600474387407303, acc: 0.9978213310241699)
[2025-02-13 03:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35][root][INFO] - Training Epoch: 1/2, step 4954/7134 completed (loss: 0.009731185622513294, acc: 0.9971428513526917)
[2025-02-13 03:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35][root][INFO] - Training Epoch: 1/2, step 4955/7134 completed (loss: 0.04052741825580597, acc: 0.9882903695106506)
[2025-02-13 03:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36][root][INFO] - Training Epoch: 1/2, step 4956/7134 completed (loss: 0.020153315737843513, acc: 0.9928876161575317)
[2025-02-13 03:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36][root][INFO] - Training Epoch: 1/2, step 4957/7134 completed (loss: 0.03299250453710556, acc: 0.9894179701805115)
[2025-02-13 03:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37][root][INFO] - Training Epoch: 1/2, step 4958/7134 completed (loss: 0.015161600895226002, acc: 0.9971222877502441)
[2025-02-13 03:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37][root][INFO] - Training Epoch: 1/2, step 4959/7134 completed (loss: 0.015046964399516582, acc: 0.9955882430076599)
[2025-02-13 03:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37][root][INFO] - Training Epoch: 1/2, step 4960/7134 completed (loss: 0.013658595271408558, acc: 0.9965517520904541)
[2025-02-13 03:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38][root][INFO] - Training Epoch: 1/2, step 4961/7134 completed (loss: 0.008297432214021683, acc: 0.9981784820556641)
[2025-02-13 03:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38][root][INFO] - Training Epoch: 1/2, step 4962/7134 completed (loss: 0.008228825405240059, acc: 0.9984447956085205)
[2025-02-13 03:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39][root][INFO] - Training Epoch: 1/2, step 4963/7134 completed (loss: 0.03270362317562103, acc: 0.9918830990791321)
[2025-02-13 03:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39][root][INFO] - Training Epoch: 1/2, step 4964/7134 completed (loss: 0.031365156173706055, acc: 0.9902439117431641)
[2025-02-13 03:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40][root][INFO] - Training Epoch: 1/2, step 4965/7134 completed (loss: 0.05812760069966316, acc: 0.9894958138465881)
[2025-02-13 03:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40][root][INFO] - Training Epoch: 1/2, step 4966/7134 completed (loss: 0.0669725239276886, acc: 0.9778393507003784)
[2025-02-13 03:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40][root][INFO] - Training Epoch: 1/2, step 4967/7134 completed (loss: 0.044920507818460464, acc: 0.98828125)
[2025-02-13 03:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41][root][INFO] - Training Epoch: 1/2, step 4968/7134 completed (loss: 0.09248748421669006, acc: 0.9750889539718628)
[2025-02-13 03:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41][root][INFO] - Training Epoch: 1/2, step 4969/7134 completed (loss: 0.07536797970533371, acc: 0.9845361113548279)
[2025-02-13 03:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42][root][INFO] - Training Epoch: 1/2, step 4970/7134 completed (loss: 0.02180839329957962, acc: 0.9882943034172058)
[2025-02-13 03:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42][root][INFO] - Training Epoch: 1/2, step 4971/7134 completed (loss: 0.032740604132413864, acc: 0.9847457408905029)
[2025-02-13 03:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42][root][INFO] - Training Epoch: 1/2, step 4972/7134 completed (loss: 0.014754338189959526, acc: 0.995768666267395)
[2025-02-13 03:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:43][root][INFO] - Training Epoch: 1/2, step 4973/7134 completed (loss: 0.059672046452760696, acc: 0.9876543283462524)
[2025-02-13 03:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:43][root][INFO] - Training Epoch: 1/2, step 4974/7134 completed (loss: 0.05025066062808037, acc: 0.9859719276428223)
[2025-02-13 03:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][root][INFO] - Training Epoch: 1/2, step 4975/7134 completed (loss: 0.06244072690606117, acc: 0.9872204661369324)
[2025-02-13 03:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][root][INFO] - Training Epoch: 1/2, step 4976/7134 completed (loss: 0.03220813721418381, acc: 0.9929742217063904)
[2025-02-13 03:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][root][INFO] - Training Epoch: 1/2, step 4977/7134 completed (loss: 0.09474819898605347, acc: 0.9741935729980469)
[2025-02-13 03:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:45][root][INFO] - Training Epoch: 1/2, step 4978/7134 completed (loss: 0.028947725892066956, acc: 0.9888198971748352)
[2025-02-13 03:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:45][root][INFO] - Training Epoch: 1/2, step 4979/7134 completed (loss: 0.11819379776716232, acc: 0.9769357442855835)
[2025-02-13 03:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46][root][INFO] - Training Epoch: 1/2, step 4980/7134 completed (loss: 0.05029461532831192, acc: 0.9855072498321533)
[2025-02-13 03:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46][root][INFO] - Training Epoch: 1/2, step 4981/7134 completed (loss: 0.03653300553560257, acc: 0.9904305934906006)
[2025-02-13 03:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47][root][INFO] - Training Epoch: 1/2, step 4982/7134 completed (loss: 0.07258139550685883, acc: 0.9838056564331055)
[2025-02-13 03:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47][root][INFO] - Training Epoch: 1/2, step 4983/7134 completed (loss: 0.04207636043429375, acc: 0.9937694668769836)
[2025-02-13 03:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47][root][INFO] - Training Epoch: 1/2, step 4984/7134 completed (loss: 0.08936340361833572, acc: 0.987500011920929)
[2025-02-13 03:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48][root][INFO] - Training Epoch: 1/2, step 4985/7134 completed (loss: 0.05493602529168129, acc: 0.9877675771713257)
[2025-02-13 03:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48][root][INFO] - Training Epoch: 1/2, step 4986/7134 completed (loss: 0.07320284843444824, acc: 0.9806835055351257)
[2025-02-13 03:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49][root][INFO] - Training Epoch: 1/2, step 4987/7134 completed (loss: 0.030612556263804436, acc: 0.9940357804298401)
[2025-02-13 03:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49][root][INFO] - Training Epoch: 1/2, step 4988/7134 completed (loss: 0.02057694084942341, acc: 0.9911660552024841)
[2025-02-13 03:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49][root][INFO] - Training Epoch: 1/2, step 4989/7134 completed (loss: 0.041647687554359436, acc: 0.9876237511634827)
[2025-02-13 03:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50][root][INFO] - Training Epoch: 1/2, step 4990/7134 completed (loss: 0.04171789810061455, acc: 0.9915540814399719)
[2025-02-13 03:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50][root][INFO] - Training Epoch: 1/2, step 4991/7134 completed (loss: 0.023778684437274933, acc: 0.9916247725486755)
[2025-02-13 03:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:51][root][INFO] - Training Epoch: 1/2, step 4992/7134 completed (loss: 0.14900410175323486, acc: 0.9606127142906189)
[2025-02-13 03:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:51][root][INFO] - Training Epoch: 1/2, step 4993/7134 completed (loss: 0.02165672741830349, acc: 0.9909502267837524)
[2025-02-13 03:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:51][root][INFO] - Training Epoch: 1/2, step 4994/7134 completed (loss: 0.020666981115937233, acc: 0.9943262338638306)
[2025-02-13 03:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52][root][INFO] - Training Epoch: 1/2, step 4995/7134 completed (loss: 0.0453559048473835, acc: 0.9836734533309937)
[2025-02-13 03:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52][root][INFO] - Training Epoch: 1/2, step 4996/7134 completed (loss: 0.03849707916378975, acc: 0.9889298677444458)
[2025-02-13 03:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53][root][INFO] - Training Epoch: 1/2, step 4997/7134 completed (loss: 0.039193637669086456, acc: 0.9897260069847107)
[2025-02-13 03:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53][root][INFO] - Training Epoch: 1/2, step 4998/7134 completed (loss: 0.08498793840408325, acc: 0.9836065769195557)
[2025-02-13 03:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53][root][INFO] - Training Epoch: 1/2, step 4999/7134 completed (loss: 0.021171992644667625, acc: 0.9931153059005737)
[2025-02-13 03:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54][root][INFO] - Training Epoch: 1/2, step 5000/7134 completed (loss: 0.019440850242972374, acc: 0.9930232763290405)
[2025-02-13 03:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54][root][INFO] - Training Epoch: 1/2, step 5001/7134 completed (loss: 0.044266942888498306, acc: 0.9869186282157898)
[2025-02-13 03:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55][root][INFO] - Training Epoch: 1/2, step 5002/7134 completed (loss: 0.04574044421315193, acc: 0.9925650358200073)
[2025-02-13 03:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55][root][INFO] - Training Epoch: 1/2, step 5003/7134 completed (loss: 0.017152417451143265, acc: 0.9936000108718872)
[2025-02-13 03:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55][root][INFO] - Training Epoch: 1/2, step 5004/7134 completed (loss: 0.012741964310407639, acc: 0.9955849647521973)
[2025-02-13 03:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56][root][INFO] - Training Epoch: 1/2, step 5005/7134 completed (loss: 0.03156903386116028, acc: 0.9902439117431641)
[2025-02-13 03:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56][root][INFO] - Training Epoch: 1/2, step 5006/7134 completed (loss: 0.02025817334651947, acc: 0.997063159942627)
[2025-02-13 03:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57][root][INFO] - Training Epoch: 1/2, step 5007/7134 completed (loss: 0.015867454931139946, acc: 0.9971751570701599)
[2025-02-13 03:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57][root][INFO] - Training Epoch: 1/2, step 5008/7134 completed (loss: 0.056401800364255905, acc: 0.9862204790115356)
[2025-02-13 03:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58][root][INFO] - Training Epoch: 1/2, step 5009/7134 completed (loss: 0.060414914041757584, acc: 0.9900744557380676)
[2025-02-13 03:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58][root][INFO] - Training Epoch: 1/2, step 5010/7134 completed (loss: 0.039603982120752335, acc: 0.9939271211624146)
[2025-02-13 03:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58][root][INFO] - Training Epoch: 1/2, step 5011/7134 completed (loss: 0.04352615401148796, acc: 0.9886178970336914)
[2025-02-13 03:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:59][root][INFO] - Training Epoch: 1/2, step 5012/7134 completed (loss: 0.0636991485953331, acc: 0.9856321811676025)
[2025-02-13 03:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:59][root][INFO] - Training Epoch: 1/2, step 5013/7134 completed (loss: 0.006485532037913799, acc: 1.0)
[2025-02-13 03:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00][root][INFO] - Training Epoch: 1/2, step 5014/7134 completed (loss: 0.027796167880296707, acc: 0.9959016442298889)
[2025-02-13 03:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00][root][INFO] - Training Epoch: 1/2, step 5015/7134 completed (loss: 0.07778055965900421, acc: 0.9862068891525269)
[2025-02-13 03:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00][root][INFO] - Training Epoch: 1/2, step 5016/7134 completed (loss: 0.026133636012673378, acc: 0.9927667379379272)
[2025-02-13 03:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01][root][INFO] - Training Epoch: 1/2, step 5017/7134 completed (loss: 0.016924375668168068, acc: 0.9963964223861694)
[2025-02-13 03:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01][root][INFO] - Training Epoch: 1/2, step 5018/7134 completed (loss: 0.03033592738211155, acc: 0.9858155846595764)
[2025-02-13 03:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02][root][INFO] - Training Epoch: 1/2, step 5019/7134 completed (loss: 0.022023841738700867, acc: 0.9971510171890259)
[2025-02-13 03:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02][root][INFO] - Training Epoch: 1/2, step 5020/7134 completed (loss: 0.025893278419971466, acc: 0.9925000071525574)
[2025-02-13 03:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03][root][INFO] - Training Epoch: 1/2, step 5021/7134 completed (loss: 0.0403791144490242, acc: 0.9908466935157776)
[2025-02-13 03:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03][root][INFO] - Training Epoch: 1/2, step 5022/7134 completed (loss: 0.03678536042571068, acc: 0.9922839403152466)
[2025-02-13 03:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04][root][INFO] - Training Epoch: 1/2, step 5023/7134 completed (loss: 0.04247863590717316, acc: 0.9863221645355225)
[2025-02-13 03:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04][root][INFO] - Training Epoch: 1/2, step 5024/7134 completed (loss: 0.04012881964445114, acc: 0.993565022945404)
[2025-02-13 03:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04][root][INFO] - Training Epoch: 1/2, step 5025/7134 completed (loss: 0.01369461603462696, acc: 0.9981883764266968)
[2025-02-13 03:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05][root][INFO] - Training Epoch: 1/2, step 5026/7134 completed (loss: 0.029318638145923615, acc: 0.9894921183586121)
[2025-02-13 03:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05][root][INFO] - Training Epoch: 1/2, step 5027/7134 completed (loss: 0.028011171147227287, acc: 0.994535505771637)
[2025-02-13 03:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06][root][INFO] - Training Epoch: 1/2, step 5028/7134 completed (loss: 0.015189077705144882, acc: 0.9952977895736694)
[2025-02-13 03:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06][root][INFO] - Training Epoch: 1/2, step 5029/7134 completed (loss: 0.016248643398284912, acc: 0.9962121248245239)
[2025-02-13 03:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06][root][INFO] - Training Epoch: 1/2, step 5030/7134 completed (loss: 0.009888248518109322, acc: 0.9972714781761169)
[2025-02-13 03:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07][root][INFO] - Training Epoch: 1/2, step 5031/7134 completed (loss: 0.06578405201435089, acc: 0.9863945841789246)
[2025-02-13 03:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07][root][INFO] - Training Epoch: 1/2, step 5032/7134 completed (loss: 0.024994220584630966, acc: 0.9951298832893372)
[2025-02-13 03:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08][root][INFO] - Training Epoch: 1/2, step 5033/7134 completed (loss: 0.013189432211220264, acc: 0.9967948794364929)
[2025-02-13 03:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08][root][INFO] - Training Epoch: 1/2, step 5034/7134 completed (loss: 0.01767982915043831, acc: 0.9946808218955994)
[2025-02-13 03:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09][root][INFO] - Training Epoch: 1/2, step 5035/7134 completed (loss: 0.0022623271215707064, acc: 1.0)
[2025-02-13 03:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09][root][INFO] - Training Epoch: 1/2, step 5036/7134 completed (loss: 0.05059387534856796, acc: 0.9866071343421936)
[2025-02-13 03:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09][root][INFO] - Training Epoch: 1/2, step 5037/7134 completed (loss: 0.05308336764574051, acc: 0.9919571280479431)
[2025-02-13 03:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:10][root][INFO] - Training Epoch: 1/2, step 5038/7134 completed (loss: 0.03663710504770279, acc: 0.9910913109779358)
[2025-02-13 03:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:10][root][INFO] - Training Epoch: 1/2, step 5039/7134 completed (loss: 0.03494686260819435, acc: 0.9873417615890503)
[2025-02-13 03:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11][root][INFO] - Training Epoch: 1/2, step 5040/7134 completed (loss: 0.010742953978478909, acc: 0.9961758852005005)
[2025-02-13 03:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11][root][INFO] - Training Epoch: 1/2, step 5041/7134 completed (loss: 0.01190139725804329, acc: 0.9960212111473083)
[2025-02-13 03:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11][root][INFO] - Training Epoch: 1/2, step 5042/7134 completed (loss: 0.04330233484506607, acc: 0.9883449673652649)
[2025-02-13 03:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12][root][INFO] - Training Epoch: 1/2, step 5043/7134 completed (loss: 0.03739204257726669, acc: 0.9901185631752014)
[2025-02-13 03:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12][root][INFO] - Training Epoch: 1/2, step 5044/7134 completed (loss: 0.01757366955280304, acc: 0.9956140518188477)
[2025-02-13 03:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13][root][INFO] - Training Epoch: 1/2, step 5045/7134 completed (loss: 0.055188000202178955, acc: 0.9811574816703796)
[2025-02-13 03:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13][root][INFO] - Training Epoch: 1/2, step 5046/7134 completed (loss: 0.07476439327001572, acc: 0.9789473414421082)
[2025-02-13 03:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13][root][INFO] - Training Epoch: 1/2, step 5047/7134 completed (loss: 0.06841342896223068, acc: 0.9801653027534485)
[2025-02-13 03:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:14][root][INFO] - Training Epoch: 1/2, step 5048/7134 completed (loss: 0.04290149360895157, acc: 0.9939117431640625)
[2025-02-13 03:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:14][root][INFO] - Training Epoch: 1/2, step 5049/7134 completed (loss: 0.07028976827859879, acc: 0.9822888374328613)
[2025-02-13 03:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15][root][INFO] - Training Epoch: 1/2, step 5050/7134 completed (loss: 0.05282248929142952, acc: 0.9887482523918152)
[2025-02-13 03:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15][root][INFO] - Training Epoch: 1/2, step 5051/7134 completed (loss: 0.04260898381471634, acc: 0.991304337978363)
[2025-02-13 03:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15][root][INFO] - Training Epoch: 1/2, step 5052/7134 completed (loss: 0.023387955501675606, acc: 0.9924242496490479)
[2025-02-13 03:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16][root][INFO] - Training Epoch: 1/2, step 5053/7134 completed (loss: 0.08571493625640869, acc: 0.9811320900917053)
[2025-02-13 03:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16][root][INFO] - Training Epoch: 1/2, step 5054/7134 completed (loss: 0.06385095417499542, acc: 0.9794952869415283)
[2025-02-13 03:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17][root][INFO] - Training Epoch: 1/2, step 5055/7134 completed (loss: 0.030517810955643654, acc: 0.9918367266654968)
[2025-02-13 03:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17][root][INFO] - Training Epoch: 1/2, step 5056/7134 completed (loss: 0.07231894880533218, acc: 0.9857142567634583)
[2025-02-13 03:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:18][root][INFO] - Training Epoch: 1/2, step 5057/7134 completed (loss: 0.09017869085073471, acc: 0.9772403836250305)
[2025-02-13 03:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:18][root][INFO] - Training Epoch: 1/2, step 5058/7134 completed (loss: 0.05787575617432594, acc: 0.9840579628944397)
[2025-02-13 03:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:18][root][INFO] - Training Epoch: 1/2, step 5059/7134 completed (loss: 0.03738252446055412, acc: 0.9895150661468506)
[2025-02-13 03:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:19][root][INFO] - Training Epoch: 1/2, step 5060/7134 completed (loss: 0.024984315037727356, acc: 0.9940564632415771)
[2025-02-13 03:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:19][root][INFO] - Training Epoch: 1/2, step 5061/7134 completed (loss: 0.04360130429267883, acc: 0.9827213883399963)
[2025-02-13 03:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20][root][INFO] - Training Epoch: 1/2, step 5062/7134 completed (loss: 0.07957956939935684, acc: 0.9811320900917053)
[2025-02-13 03:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20][root][INFO] - Training Epoch: 1/2, step 5063/7134 completed (loss: 0.040146827697753906, acc: 0.9883138537406921)
[2025-02-13 03:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21][root][INFO] - Training Epoch: 1/2, step 5064/7134 completed (loss: 0.019818942993879318, acc: 0.9931507110595703)
[2025-02-13 03:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21][root][INFO] - Training Epoch: 1/2, step 5065/7134 completed (loss: 0.04971158504486084, acc: 0.9873060584068298)
[2025-02-13 03:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21][root][INFO] - Training Epoch: 1/2, step 5066/7134 completed (loss: 0.04554831609129906, acc: 0.9863842725753784)
[2025-02-13 03:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22][root][INFO] - Training Epoch: 1/2, step 5067/7134 completed (loss: 0.0227829422801733, acc: 0.9961013793945312)
[2025-02-13 03:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22][root][INFO] - Training Epoch: 1/2, step 5068/7134 completed (loss: 0.03178907558321953, acc: 0.9889415502548218)
[2025-02-13 03:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23][root][INFO] - Training Epoch: 1/2, step 5069/7134 completed (loss: 0.02983858250081539, acc: 0.989062488079071)
[2025-02-13 03:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23][root][INFO] - Training Epoch: 1/2, step 5070/7134 completed (loss: 0.026429587975144386, acc: 0.9942528605461121)
[2025-02-13 03:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23][root][INFO] - Training Epoch: 1/2, step 5071/7134 completed (loss: 0.05333087965846062, acc: 0.9803646802902222)
[2025-02-13 03:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24][root][INFO] - Training Epoch: 1/2, step 5072/7134 completed (loss: 0.04251222684979439, acc: 0.9861982464790344)
[2025-02-13 03:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24][root][INFO] - Training Epoch: 1/2, step 5073/7134 completed (loss: 0.05888162925839424, acc: 0.9845260977745056)
[2025-02-13 03:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25][root][INFO] - Training Epoch: 1/2, step 5074/7134 completed (loss: 0.04740648716688156, acc: 0.9867330193519592)
[2025-02-13 03:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25][root][INFO] - Training Epoch: 1/2, step 5075/7134 completed (loss: 0.031865671277046204, acc: 0.9941860437393188)
[2025-02-13 03:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26][root][INFO] - Training Epoch: 1/2, step 5076/7134 completed (loss: 0.0335557796061039, acc: 0.9880383014678955)
[2025-02-13 03:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26][root][INFO] - Training Epoch: 1/2, step 5077/7134 completed (loss: 0.02699785679578781, acc: 0.9936708807945251)
[2025-02-13 03:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:27][root][INFO] - Training Epoch: 1/2, step 5078/7134 completed (loss: 0.08556724339723587, acc: 0.9790794849395752)
[2025-02-13 03:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:27][root][INFO] - Training Epoch: 1/2, step 5079/7134 completed (loss: 0.017485162243247032, acc: 0.9931972622871399)
[2025-02-13 03:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:27][root][INFO] - Training Epoch: 1/2, step 5080/7134 completed (loss: 0.023395167663693428, acc: 0.9940898418426514)
[2025-02-13 03:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:28][root][INFO] - Training Epoch: 1/2, step 5081/7134 completed (loss: 0.013523930683732033, acc: 0.9959239363670349)
[2025-02-13 03:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:28][root][INFO] - Training Epoch: 1/2, step 5082/7134 completed (loss: 0.028827911242842674, acc: 0.9914966225624084)
[2025-02-13 03:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29][root][INFO] - Training Epoch: 1/2, step 5083/7134 completed (loss: 0.039518021047115326, acc: 0.9943740963935852)
[2025-02-13 03:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29][root][INFO] - Training Epoch: 1/2, step 5084/7134 completed (loss: 0.019561704248189926, acc: 0.9930555820465088)
[2025-02-13 03:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30][root][INFO] - Training Epoch: 1/2, step 5085/7134 completed (loss: 0.033470865339040756, acc: 0.9869822263717651)
[2025-02-13 03:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30][root][INFO] - Training Epoch: 1/2, step 5086/7134 completed (loss: 0.03776748850941658, acc: 0.9885641932487488)
[2025-02-13 03:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:31][root][INFO] - Training Epoch: 1/2, step 5087/7134 completed (loss: 0.009723260067403316, acc: 0.9959839582443237)
[2025-02-13 03:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:31][root][INFO] - Training Epoch: 1/2, step 5088/7134 completed (loss: 0.006204877980053425, acc: 0.9985358715057373)
[2025-02-13 03:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32][root][INFO] - Training Epoch: 1/2, step 5089/7134 completed (loss: 0.019270222634077072, acc: 0.9950124621391296)
[2025-02-13 03:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32][root][INFO] - Training Epoch: 1/2, step 5090/7134 completed (loss: 0.016892343759536743, acc: 0.9941657185554504)
[2025-02-13 03:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32][root][INFO] - Training Epoch: 1/2, step 5091/7134 completed (loss: 0.008573772385716438, acc: 1.0)
[2025-02-13 03:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:33][root][INFO] - Training Epoch: 1/2, step 5092/7134 completed (loss: 0.03672315925359726, acc: 0.9917012453079224)
[2025-02-13 03:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:33][root][INFO] - Training Epoch: 1/2, step 5093/7134 completed (loss: 0.0337107852101326, acc: 0.9917126893997192)
[2025-02-13 03:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34][root][INFO] - Training Epoch: 1/2, step 5094/7134 completed (loss: 0.03161098062992096, acc: 0.9945454597473145)
[2025-02-13 03:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34][root][INFO] - Training Epoch: 1/2, step 5095/7134 completed (loss: 0.11777077615261078, acc: 0.9781591296195984)
[2025-02-13 03:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:35][root][INFO] - Training Epoch: 1/2, step 5096/7134 completed (loss: 0.03762397542595863, acc: 0.989595353603363)
[2025-02-13 03:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:35][root][INFO] - Training Epoch: 1/2, step 5097/7134 completed (loss: 0.037234630435705185, acc: 0.9877899885177612)
[2025-02-13 03:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:35][root][INFO] - Training Epoch: 1/2, step 5098/7134 completed (loss: 0.031895510852336884, acc: 0.9865016937255859)
[2025-02-13 03:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36][root][INFO] - Training Epoch: 1/2, step 5099/7134 completed (loss: 0.03334204480051994, acc: 0.9896073937416077)
[2025-02-13 03:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36][root][INFO] - Training Epoch: 1/2, step 5100/7134 completed (loss: 0.03506620600819588, acc: 0.9889867901802063)
[2025-02-13 03:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:37][root][INFO] - Training Epoch: 1/2, step 5101/7134 completed (loss: 0.024219101294875145, acc: 0.9906759858131409)
[2025-02-13 03:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:37][root][INFO] - Training Epoch: 1/2, step 5102/7134 completed (loss: 0.049247436225414276, acc: 0.9849931597709656)
[2025-02-13 03:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38][root][INFO] - Training Epoch: 1/2, step 5103/7134 completed (loss: 0.039554011076688766, acc: 0.9908362030982971)
[2025-02-13 03:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38][root][INFO] - Training Epoch: 1/2, step 5104/7134 completed (loss: 0.019649188965559006, acc: 0.9955703020095825)
[2025-02-13 03:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39][root][INFO] - Training Epoch: 1/2, step 5105/7134 completed (loss: 0.027164442464709282, acc: 0.9910581111907959)
[2025-02-13 03:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39][root][INFO] - Training Epoch: 1/2, step 5106/7134 completed (loss: 0.033474262803792953, acc: 0.9881481528282166)
[2025-02-13 03:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40][root][INFO] - Training Epoch: 1/2, step 5107/7134 completed (loss: 0.09076487272977829, acc: 0.9825673699378967)
[2025-02-13 03:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40][root][INFO] - Training Epoch: 1/2, step 5108/7134 completed (loss: 0.06270984560251236, acc: 0.9825783967971802)
[2025-02-13 03:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40][root][INFO] - Training Epoch: 1/2, step 5109/7134 completed (loss: 0.06031814217567444, acc: 0.9856194853782654)
[2025-02-13 03:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41][root][INFO] - Training Epoch: 1/2, step 5110/7134 completed (loss: 0.047147855162620544, acc: 0.9846153855323792)
[2025-02-13 03:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41][root][INFO] - Training Epoch: 1/2, step 5111/7134 completed (loss: 0.029149387031793594, acc: 0.9890410900115967)
[2025-02-13 03:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42][root][INFO] - Training Epoch: 1/2, step 5112/7134 completed (loss: 0.05526500940322876, acc: 0.9900373816490173)
[2025-02-13 03:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42][root][INFO] - Training Epoch: 1/2, step 5113/7134 completed (loss: 0.05554484948515892, acc: 0.983582079410553)
[2025-02-13 03:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43][root][INFO] - Training Epoch: 1/2, step 5114/7134 completed (loss: 0.022459611296653748, acc: 0.9944367408752441)
[2025-02-13 03:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43][root][INFO] - Training Epoch: 1/2, step 5115/7134 completed (loss: 0.02533489093184471, acc: 0.9922580718994141)
[2025-02-13 03:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:44][root][INFO] - Training Epoch: 1/2, step 5116/7134 completed (loss: 0.020706385374069214, acc: 0.9948052167892456)
[2025-02-13 03:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:44][root][INFO] - Training Epoch: 1/2, step 5117/7134 completed (loss: 0.02864931896328926, acc: 0.9930264949798584)
[2025-02-13 03:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:44][root][INFO] - Training Epoch: 1/2, step 5118/7134 completed (loss: 0.055884961038827896, acc: 0.9893778562545776)
[2025-02-13 03:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45][root][INFO] - Training Epoch: 1/2, step 5119/7134 completed (loss: 0.03541223704814911, acc: 0.993122398853302)
[2025-02-13 03:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45][root][INFO] - Training Epoch: 1/2, step 5120/7134 completed (loss: 0.07147382199764252, acc: 0.985602080821991)
[2025-02-13 03:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46][root][INFO] - Training Epoch: 1/2, step 5121/7134 completed (loss: 0.027400776743888855, acc: 0.994452178478241)
[2025-02-13 03:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46][root][INFO] - Training Epoch: 1/2, step 5122/7134 completed (loss: 0.035301242023706436, acc: 0.9887482523918152)
[2025-02-13 03:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:47][root][INFO] - Training Epoch: 1/2, step 5123/7134 completed (loss: 0.026840845122933388, acc: 0.9927623867988586)
[2025-02-13 03:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:47][root][INFO] - Training Epoch: 1/2, step 5124/7134 completed (loss: 0.04265018180012703, acc: 0.9863353967666626)
[2025-02-13 03:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:47][root][INFO] - Training Epoch: 1/2, step 5125/7134 completed (loss: 0.022522516548633575, acc: 0.9952606558799744)
[2025-02-13 03:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48][root][INFO] - Training Epoch: 1/2, step 5126/7134 completed (loss: 0.012044131755828857, acc: 0.9965811967849731)
[2025-02-13 03:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48][root][INFO] - Training Epoch: 1/2, step 5127/7134 completed (loss: 0.030134767293930054, acc: 0.9930070042610168)
[2025-02-13 03:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49][root][INFO] - Training Epoch: 1/2, step 5128/7134 completed (loss: 0.01266025472432375, acc: 0.997802197933197)
[2025-02-13 03:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49][root][INFO] - Training Epoch: 1/2, step 5129/7134 completed (loss: 0.06663170456886292, acc: 0.9803921580314636)
[2025-02-13 03:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:50][root][INFO] - Training Epoch: 1/2, step 5130/7134 completed (loss: 0.014303927309811115, acc: 0.9937597513198853)
[2025-02-13 03:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:50][root][INFO] - Training Epoch: 1/2, step 5131/7134 completed (loss: 0.055042315274477005, acc: 0.9886363744735718)
[2025-02-13 03:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:50][root][INFO] - Training Epoch: 1/2, step 5132/7134 completed (loss: 0.022594686597585678, acc: 0.9900990128517151)
[2025-02-13 03:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51][root][INFO] - Training Epoch: 1/2, step 5133/7134 completed (loss: 0.05008430778980255, acc: 0.9829192757606506)
[2025-02-13 03:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51][root][INFO] - Training Epoch: 1/2, step 5134/7134 completed (loss: 0.017159435898065567, acc: 0.998123824596405)
[2025-02-13 03:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:52][root][INFO] - Training Epoch: 1/2, step 5135/7134 completed (loss: 0.04266967996954918, acc: 0.9900000095367432)
[2025-02-13 03:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:52][root][INFO] - Training Epoch: 1/2, step 5136/7134 completed (loss: 0.02903125062584877, acc: 0.9890710115432739)
[2025-02-13 03:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:53][root][INFO] - Training Epoch: 1/2, step 5137/7134 completed (loss: 0.030507586896419525, acc: 0.9926605224609375)
[2025-02-13 03:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:53][root][INFO] - Training Epoch: 1/2, step 5138/7134 completed (loss: 0.017509035766124725, acc: 0.9962499737739563)
[2025-02-13 03:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54][root][INFO] - Training Epoch: 1/2, step 5139/7134 completed (loss: 0.018799452111124992, acc: 0.991416335105896)
[2025-02-13 03:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54][root][INFO] - Training Epoch: 1/2, step 5140/7134 completed (loss: 0.01901405304670334, acc: 0.9946977496147156)
[2025-02-13 03:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54][root][INFO] - Training Epoch: 1/2, step 5141/7134 completed (loss: 0.04172482714056969, acc: 0.992277979850769)
[2025-02-13 03:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:55][root][INFO] - Training Epoch: 1/2, step 5142/7134 completed (loss: 0.023956365883350372, acc: 0.9917647242546082)
[2025-02-13 03:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:55][root][INFO] - Training Epoch: 1/2, step 5143/7134 completed (loss: 0.021385814994573593, acc: 0.9936548471450806)
[2025-02-13 03:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56][root][INFO] - Training Epoch: 1/2, step 5144/7134 completed (loss: 0.016512848436832428, acc: 0.9964994192123413)
[2025-02-13 03:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56][root][INFO] - Training Epoch: 1/2, step 5145/7134 completed (loss: 0.017897862941026688, acc: 0.9959058165550232)
[2025-02-13 03:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57][root][INFO] - Training Epoch: 1/2, step 5146/7134 completed (loss: 0.05235884338617325, acc: 0.9855072498321533)
[2025-02-13 03:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57][root][INFO] - Training Epoch: 1/2, step 5147/7134 completed (loss: 0.020348863676190376, acc: 0.9931694269180298)
[2025-02-13 03:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58][root][INFO] - Training Epoch: 1/2, step 5148/7134 completed (loss: 0.06979619711637497, acc: 0.9826897382736206)
[2025-02-13 03:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58][root][INFO] - Training Epoch: 1/2, step 5149/7134 completed (loss: 0.04104116931557655, acc: 0.9890547394752502)
[2025-02-13 03:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59][root][INFO] - Training Epoch: 1/2, step 5150/7134 completed (loss: 0.05738746374845505, acc: 0.9817470908164978)
[2025-02-13 03:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59][root][INFO] - Training Epoch: 1/2, step 5151/7134 completed (loss: 0.04085826873779297, acc: 0.9925742745399475)
[2025-02-13 03:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59][root][INFO] - Training Epoch: 1/2, step 5152/7134 completed (loss: 0.02937469817698002, acc: 0.989276111125946)
[2025-02-13 03:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:00][root][INFO] - Training Epoch: 1/2, step 5153/7134 completed (loss: 0.025163212791085243, acc: 0.991769552230835)
[2025-02-13 03:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:00][root][INFO] - Training Epoch: 1/2, step 5154/7134 completed (loss: 0.029127204790711403, acc: 0.9920364022254944)
[2025-02-13 03:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:01][root][INFO] - Training Epoch: 1/2, step 5155/7134 completed (loss: 0.03453831002116203, acc: 0.9954819083213806)
[2025-02-13 03:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:01][root][INFO] - Training Epoch: 1/2, step 5156/7134 completed (loss: 0.020134808495640755, acc: 0.9920555949211121)
[2025-02-13 03:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:02][root][INFO] - Training Epoch: 1/2, step 5157/7134 completed (loss: 0.07274725288152695, acc: 0.9767726063728333)
[2025-02-13 03:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:02][root][INFO] - Training Epoch: 1/2, step 5158/7134 completed (loss: 0.06627149879932404, acc: 0.9821958541870117)
[2025-02-13 03:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:03][root][INFO] - Training Epoch: 1/2, step 5159/7134 completed (loss: 0.030015241354703903, acc: 0.9921976327896118)
[2025-02-13 03:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:03][root][INFO] - Training Epoch: 1/2, step 5160/7134 completed (loss: 0.056299686431884766, acc: 0.9798271059989929)
[2025-02-13 03:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04][root][INFO] - Training Epoch: 1/2, step 5161/7134 completed (loss: 0.08213294297456741, acc: 0.9748252034187317)
[2025-02-13 03:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04][root][INFO] - Training Epoch: 1/2, step 5162/7134 completed (loss: 0.023781340569257736, acc: 0.9886547923088074)
[2025-02-13 03:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04][root][INFO] - Training Epoch: 1/2, step 5163/7134 completed (loss: 0.019580330699682236, acc: 0.9943342804908752)
[2025-02-13 03:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05][root][INFO] - Training Epoch: 1/2, step 5164/7134 completed (loss: 0.021454058587551117, acc: 0.9966386556625366)
[2025-02-13 03:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05][root][INFO] - Training Epoch: 1/2, step 5165/7134 completed (loss: 0.040889766067266464, acc: 0.9924699068069458)
[2025-02-13 03:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06][root][INFO] - Training Epoch: 1/2, step 5166/7134 completed (loss: 0.017715228721499443, acc: 0.9954338073730469)
[2025-02-13 03:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06][root][INFO] - Training Epoch: 1/2, step 5167/7134 completed (loss: 0.06537783145904541, acc: 0.9791666865348816)
[2025-02-13 03:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07][root][INFO] - Training Epoch: 1/2, step 5168/7134 completed (loss: 0.04890426620841026, acc: 0.9813664555549622)
[2025-02-13 03:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07][root][INFO] - Training Epoch: 1/2, step 5169/7134 completed (loss: 0.06918871402740479, acc: 0.9828660488128662)
[2025-02-13 03:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07][root][INFO] - Training Epoch: 1/2, step 5170/7134 completed (loss: 0.043880395591259, acc: 0.9854304790496826)
[2025-02-13 03:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08][root][INFO] - Training Epoch: 1/2, step 5171/7134 completed (loss: 0.031055597588419914, acc: 0.9914529919624329)
[2025-02-13 03:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08][root][INFO] - Training Epoch: 1/2, step 5172/7134 completed (loss: 0.08745027333498001, acc: 0.9727427363395691)
[2025-02-13 03:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09][root][INFO] - Training Epoch: 1/2, step 5173/7134 completed (loss: 0.04348403587937355, acc: 0.9842519760131836)
[2025-02-13 03:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09][root][INFO] - Training Epoch: 1/2, step 5174/7134 completed (loss: 0.01646432653069496, acc: 0.9961685538291931)
[2025-02-13 03:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10][root][INFO] - Training Epoch: 1/2, step 5175/7134 completed (loss: 0.017764201387763023, acc: 0.9930939078330994)
[2025-02-13 03:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10][root][INFO] - Training Epoch: 1/2, step 5176/7134 completed (loss: 0.021230177953839302, acc: 0.9954476356506348)
[2025-02-13 03:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10][root][INFO] - Training Epoch: 1/2, step 5177/7134 completed (loss: 0.05544039607048035, acc: 0.9843478202819824)
[2025-02-13 03:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11][root][INFO] - Training Epoch: 1/2, step 5178/7134 completed (loss: 0.1297476887702942, acc: 0.9738675951957703)
[2025-02-13 03:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11][root][INFO] - Training Epoch: 1/2, step 5179/7134 completed (loss: 0.027493581175804138, acc: 0.9926035404205322)
[2025-02-13 03:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12][root][INFO] - Training Epoch: 1/2, step 5180/7134 completed (loss: 0.047605693340301514, acc: 0.9849905967712402)
[2025-02-13 03:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12][root][INFO] - Training Epoch: 1/2, step 5181/7134 completed (loss: 0.08739537745714188, acc: 0.978723406791687)
[2025-02-13 03:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12][root][INFO] - Training Epoch: 1/2, step 5182/7134 completed (loss: 0.15135420858860016, acc: 0.9520833492279053)
[2025-02-13 03:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13][root][INFO] - Training Epoch: 1/2, step 5183/7134 completed (loss: 0.05792981758713722, acc: 0.9884488582611084)
[2025-02-13 03:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13][root][INFO] - Training Epoch: 1/2, step 5184/7134 completed (loss: 0.054466523230075836, acc: 0.9853300452232361)
[2025-02-13 03:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14][root][INFO] - Training Epoch: 1/2, step 5185/7134 completed (loss: 0.04771491512656212, acc: 0.9879518151283264)
[2025-02-13 03:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14][root][INFO] - Training Epoch: 1/2, step 5186/7134 completed (loss: 0.051117222756147385, acc: 0.9877800345420837)
[2025-02-13 03:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15][root][INFO] - Training Epoch: 1/2, step 5187/7134 completed (loss: 0.020093686878681183, acc: 0.9923809766769409)
[2025-02-13 03:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15][root][INFO] - Training Epoch: 1/2, step 5188/7134 completed (loss: 0.2067931890487671, acc: 0.9545454382896423)
[2025-02-13 03:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15][root][INFO] - Training Epoch: 1/2, step 5189/7134 completed (loss: 0.1607915461063385, acc: 0.9591836929321289)
[2025-02-13 03:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16][root][INFO] - Training Epoch: 1/2, step 5190/7134 completed (loss: 0.10086382925510406, acc: 0.9667458534240723)
[2025-02-13 03:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16][root][INFO] - Training Epoch: 1/2, step 5191/7134 completed (loss: 0.11664807051420212, acc: 0.966325044631958)
[2025-02-13 03:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17][root][INFO] - Training Epoch: 1/2, step 5192/7134 completed (loss: 0.06308773905038834, acc: 0.9857312440872192)
[2025-02-13 03:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17][root][INFO] - Training Epoch: 1/2, step 5193/7134 completed (loss: 0.05040120333433151, acc: 0.9862204790115356)
[2025-02-13 03:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18][root][INFO] - Training Epoch: 1/2, step 5194/7134 completed (loss: 0.08272887021303177, acc: 0.9702970385551453)
[2025-02-13 03:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18][root][INFO] - Training Epoch: 1/2, step 5195/7134 completed (loss: 0.058795977383852005, acc: 0.9890350699424744)
[2025-02-13 03:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18][root][INFO] - Training Epoch: 1/2, step 5196/7134 completed (loss: 0.04733932390809059, acc: 0.9856459498405457)
[2025-02-13 03:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19][root][INFO] - Training Epoch: 1/2, step 5197/7134 completed (loss: 0.02938777580857277, acc: 0.9897119402885437)
[2025-02-13 03:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19][root][INFO] - Training Epoch: 1/2, step 5198/7134 completed (loss: 0.04856962710618973, acc: 0.9833679795265198)
[2025-02-13 03:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20][root][INFO] - Training Epoch: 1/2, step 5199/7134 completed (loss: 0.10778035968542099, acc: 0.9665697813034058)
[2025-02-13 03:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20][root][INFO] - Training Epoch: 1/2, step 5200/7134 completed (loss: 0.04702165350317955, acc: 0.9882121682167053)
[2025-02-13 03:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21][root][INFO] - Training Epoch: 1/2, step 5201/7134 completed (loss: 0.06984888017177582, acc: 0.9855247139930725)
[2025-02-13 03:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21][root][INFO] - Training Epoch: 1/2, step 5202/7134 completed (loss: 0.08109597116708755, acc: 0.973901093006134)
[2025-02-13 03:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21][root][INFO] - Training Epoch: 1/2, step 5203/7134 completed (loss: 0.06665029376745224, acc: 0.9850339889526367)
[2025-02-13 03:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22][root][INFO] - Training Epoch: 1/2, step 5204/7134 completed (loss: 0.04263588786125183, acc: 0.983505129814148)
[2025-02-13 03:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22][root][INFO] - Training Epoch: 1/2, step 5205/7134 completed (loss: 0.05823797360062599, acc: 0.9812734127044678)
[2025-02-13 03:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23][root][INFO] - Training Epoch: 1/2, step 5206/7134 completed (loss: 0.05692986771464348, acc: 0.9903692007064819)
[2025-02-13 03:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23][root][INFO] - Training Epoch: 1/2, step 5207/7134 completed (loss: 0.0771416574716568, acc: 0.9758551120758057)
[2025-02-13 03:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24][root][INFO] - Training Epoch: 1/2, step 5208/7134 completed (loss: 0.028445454314351082, acc: 0.9907651543617249)
[2025-02-13 03:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24][root][INFO] - Training Epoch: 1/2, step 5209/7134 completed (loss: 0.03622574731707573, acc: 0.9919678568840027)
[2025-02-13 03:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24][root][INFO] - Training Epoch: 1/2, step 5210/7134 completed (loss: 0.027125494554638863, acc: 0.9919893145561218)
[2025-02-13 03:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25][root][INFO] - Training Epoch: 1/2, step 5211/7134 completed (loss: 0.03718706965446472, acc: 0.9913194179534912)
[2025-02-13 03:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25][root][INFO] - Training Epoch: 1/2, step 5212/7134 completed (loss: 0.08039635419845581, acc: 0.979522168636322)
[2025-02-13 03:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26][root][INFO] - Training Epoch: 1/2, step 5213/7134 completed (loss: 0.062109727412462234, acc: 0.9856114983558655)
[2025-02-13 03:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26][root][INFO] - Training Epoch: 1/2, step 5214/7134 completed (loss: 0.11658400297164917, acc: 0.9699570536613464)
[2025-02-13 03:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][root][INFO] - Training Epoch: 1/2, step 5215/7134 completed (loss: 0.07446553558111191, acc: 0.9833971858024597)
[2025-02-13 03:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][root][INFO] - Training Epoch: 1/2, step 5216/7134 completed (loss: 0.06919344514608383, acc: 0.9771863222122192)
[2025-02-13 03:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][root][INFO] - Training Epoch: 1/2, step 5217/7134 completed (loss: 0.07668409496545792, acc: 0.9756097793579102)
[2025-02-13 03:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:28][root][INFO] - Training Epoch: 1/2, step 5218/7134 completed (loss: 0.03673158586025238, acc: 0.9910112619400024)
[2025-02-13 03:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:28][root][INFO] - Training Epoch: 1/2, step 5219/7134 completed (loss: 0.046033114194869995, acc: 0.9927361011505127)
[2025-02-13 03:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29][root][INFO] - Training Epoch: 1/2, step 5220/7134 completed (loss: 0.058900605887174606, acc: 0.9785330891609192)
[2025-02-13 03:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29][root][INFO] - Training Epoch: 1/2, step 5221/7134 completed (loss: 0.06943690776824951, acc: 0.9828660488128662)
[2025-02-13 03:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29][root][INFO] - Training Epoch: 1/2, step 5222/7134 completed (loss: 0.0481095165014267, acc: 0.9813084006309509)
[2025-02-13 03:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30][root][INFO] - Training Epoch: 1/2, step 5223/7134 completed (loss: 0.0489678792655468, acc: 0.9884792566299438)
[2025-02-13 03:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30][root][INFO] - Training Epoch: 1/2, step 5224/7134 completed (loss: 0.05892207473516464, acc: 0.9841827750205994)
[2025-02-13 03:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31][root][INFO] - Training Epoch: 1/2, step 5225/7134 completed (loss: 0.06295929849147797, acc: 0.9806138873100281)
[2025-02-13 03:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31][root][INFO] - Training Epoch: 1/2, step 5226/7134 completed (loss: 0.04787864536046982, acc: 0.9934102296829224)
[2025-02-13 03:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32][root][INFO] - Training Epoch: 1/2, step 5227/7134 completed (loss: 0.03469917178153992, acc: 0.984415590763092)
[2025-02-13 03:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32][root][INFO] - Training Epoch: 1/2, step 5228/7134 completed (loss: 0.02262948639690876, acc: 0.9941860437393188)
[2025-02-13 03:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32][root][INFO] - Training Epoch: 1/2, step 5229/7134 completed (loss: 0.021299276500940323, acc: 0.9940564632415771)
[2025-02-13 03:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33][root][INFO] - Training Epoch: 1/2, step 5230/7134 completed (loss: 0.029798798263072968, acc: 0.9892183542251587)
[2025-02-13 03:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33][root][INFO] - Training Epoch: 1/2, step 5231/7134 completed (loss: 0.0287071131169796, acc: 0.9924050569534302)
[2025-02-13 03:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34][root][INFO] - Training Epoch: 1/2, step 5232/7134 completed (loss: 0.024066990241408348, acc: 0.9910314083099365)
[2025-02-13 03:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34][root][INFO] - Training Epoch: 1/2, step 5233/7134 completed (loss: 0.030752232298254967, acc: 0.9909909963607788)
[2025-02-13 03:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35][root][INFO] - Training Epoch: 1/2, step 5234/7134 completed (loss: 0.032654743641614914, acc: 0.993880033493042)
[2025-02-13 03:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35][root][INFO] - Training Epoch: 1/2, step 5235/7134 completed (loss: 0.02372867427766323, acc: 0.9949579834938049)
[2025-02-13 03:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36][root][INFO] - Training Epoch: 1/2, step 5236/7134 completed (loss: 0.01529536023736, acc: 0.9947916865348816)
[2025-02-13 03:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36][root][INFO] - Training Epoch: 1/2, step 5237/7134 completed (loss: 0.00957478303462267, acc: 0.9982078671455383)
[2025-02-13 03:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36][root][INFO] - Training Epoch: 1/2, step 5238/7134 completed (loss: 0.018562592566013336, acc: 0.9929577708244324)
[2025-02-13 03:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37][root][INFO] - Training Epoch: 1/2, step 5239/7134 completed (loss: 0.0418718159198761, acc: 0.9903475046157837)
[2025-02-13 03:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37][root][INFO] - Training Epoch: 1/2, step 5240/7134 completed (loss: 0.02414461225271225, acc: 0.9944674968719482)
[2025-02-13 03:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38][root][INFO] - Training Epoch: 1/2, step 5241/7134 completed (loss: 0.06543254852294922, acc: 0.9858934283256531)
[2025-02-13 03:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38][root][INFO] - Training Epoch: 1/2, step 5242/7134 completed (loss: 0.011199934408068657, acc: 0.9983471035957336)
[2025-02-13 03:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39][root][INFO] - Training Epoch: 1/2, step 5243/7134 completed (loss: 0.026705056428909302, acc: 0.9948275685310364)
[2025-02-13 03:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39][root][INFO] - Training Epoch: 1/2, step 5244/7134 completed (loss: 0.015728948637843132, acc: 0.9958620667457581)
[2025-02-13 03:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39][root][INFO] - Training Epoch: 1/2, step 5245/7134 completed (loss: 0.023558037355542183, acc: 0.9912739992141724)
[2025-02-13 03:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40][root][INFO] - Training Epoch: 1/2, step 5246/7134 completed (loss: 0.02075718343257904, acc: 0.9907975196838379)
[2025-02-13 03:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40][root][INFO] - Training Epoch: 1/2, step 5247/7134 completed (loss: 0.01355702243745327, acc: 0.9962962865829468)
[2025-02-13 03:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41][root][INFO] - Training Epoch: 1/2, step 5248/7134 completed (loss: 0.07010072469711304, acc: 0.9868913888931274)
[2025-02-13 03:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41][root][INFO] - Training Epoch: 1/2, step 5249/7134 completed (loss: 0.012279181741178036, acc: 0.9937106966972351)
[2025-02-13 03:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:42][root][INFO] - Training Epoch: 1/2, step 5250/7134 completed (loss: 0.025107339024543762, acc: 0.994955837726593)
[2025-02-13 03:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:42][root][INFO] - Training Epoch: 1/2, step 5251/7134 completed (loss: 0.03450539708137512, acc: 0.9866666793823242)
[2025-02-13 03:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:42][root][INFO] - Training Epoch: 1/2, step 5252/7134 completed (loss: 0.05075960233807564, acc: 0.985981285572052)
[2025-02-13 03:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43][root][INFO] - Training Epoch: 1/2, step 5253/7134 completed (loss: 0.017817450687289238, acc: 0.9934640526771545)
[2025-02-13 03:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43][root][INFO] - Training Epoch: 1/2, step 5254/7134 completed (loss: 0.04950978234410286, acc: 0.9830220937728882)
[2025-02-13 03:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:44][root][INFO] - Training Epoch: 1/2, step 5255/7134 completed (loss: 0.029312457889318466, acc: 0.9945205450057983)
[2025-02-13 03:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:44][root][INFO] - Training Epoch: 1/2, step 5256/7134 completed (loss: 0.06527716666460037, acc: 0.9829984307289124)
[2025-02-13 03:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45][root][INFO] - Training Epoch: 1/2, step 5257/7134 completed (loss: 0.07083339989185333, acc: 0.9878234267234802)
[2025-02-13 03:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45][root][INFO] - Training Epoch: 1/2, step 5258/7134 completed (loss: 0.04147946462035179, acc: 0.9846938848495483)
[2025-02-13 03:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45][root][INFO] - Training Epoch: 1/2, step 5259/7134 completed (loss: 0.0687529519200325, acc: 0.9857594966888428)
[2025-02-13 03:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:46][root][INFO] - Training Epoch: 1/2, step 5260/7134 completed (loss: 0.03536652773618698, acc: 0.9914634227752686)
[2025-02-13 03:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:46][root][INFO] - Training Epoch: 1/2, step 5261/7134 completed (loss: 0.04632653668522835, acc: 0.9865384697914124)
[2025-02-13 03:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47][root][INFO] - Training Epoch: 1/2, step 5262/7134 completed (loss: 0.025126995518803596, acc: 0.9917241334915161)
[2025-02-13 03:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47][root][INFO] - Training Epoch: 1/2, step 5263/7134 completed (loss: 0.05104043707251549, acc: 0.9824561476707458)
[2025-02-13 03:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48][root][INFO] - Training Epoch: 1/2, step 5264/7134 completed (loss: 0.02796567790210247, acc: 0.9890643954277039)
[2025-02-13 03:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48][root][INFO] - Training Epoch: 1/2, step 5265/7134 completed (loss: 0.03249053284525871, acc: 0.9885641932487488)
[2025-02-13 03:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49][root][INFO] - Training Epoch: 1/2, step 5266/7134 completed (loss: 0.01854874938726425, acc: 0.9931507110595703)
[2025-02-13 03:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49][root][INFO] - Training Epoch: 1/2, step 5267/7134 completed (loss: 0.09074041992425919, acc: 0.9782270789146423)
[2025-02-13 03:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49][root][INFO] - Training Epoch: 1/2, step 5268/7134 completed (loss: 0.04251039773225784, acc: 0.9827255010604858)
[2025-02-13 03:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:50][root][INFO] - Training Epoch: 1/2, step 5269/7134 completed (loss: 0.08731251209974289, acc: 0.9731663465499878)
[2025-02-13 03:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:50][root][INFO] - Training Epoch: 1/2, step 5270/7134 completed (loss: 0.03590739518404007, acc: 0.990641713142395)
[2025-02-13 03:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:51][root][INFO] - Training Epoch: 1/2, step 5271/7134 completed (loss: 0.06847816705703735, acc: 0.9805996417999268)
[2025-02-13 03:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:51][root][INFO] - Training Epoch: 1/2, step 5272/7134 completed (loss: 0.03547346964478493, acc: 0.9912023544311523)
[2025-02-13 03:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52][root][INFO] - Training Epoch: 1/2, step 5273/7134 completed (loss: 0.02719908580183983, acc: 0.9898132681846619)
[2025-02-13 03:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52][root][INFO] - Training Epoch: 1/2, step 5274/7134 completed (loss: 0.042997993528842926, acc: 0.9850746393203735)
[2025-02-13 03:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52][root][INFO] - Training Epoch: 1/2, step 5275/7134 completed (loss: 0.02673623524606228, acc: 0.9908592104911804)
[2025-02-13 03:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53][root][INFO] - Training Epoch: 1/2, step 5276/7134 completed (loss: 0.04849979281425476, acc: 0.9850522875785828)
[2025-02-13 03:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53][root][INFO] - Training Epoch: 1/2, step 5277/7134 completed (loss: 0.033322837203741074, acc: 0.9870316982269287)
[2025-02-13 03:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:54][root][INFO] - Training Epoch: 1/2, step 5278/7134 completed (loss: 0.07317999750375748, acc: 0.9831804037094116)
[2025-02-13 03:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:54][root][INFO] - Training Epoch: 1/2, step 5279/7134 completed (loss: 0.017582518979907036, acc: 0.9961089491844177)
[2025-02-13 03:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55][root][INFO] - Training Epoch: 1/2, step 5280/7134 completed (loss: 0.04180601239204407, acc: 0.9850746393203735)
[2025-02-13 03:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55][root][INFO] - Training Epoch: 1/2, step 5281/7134 completed (loss: 0.028616303578019142, acc: 0.9916805028915405)
[2025-02-13 03:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55][root][INFO] - Training Epoch: 1/2, step 5282/7134 completed (loss: 0.025169985368847847, acc: 0.9942938685417175)
[2025-02-13 03:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56][root][INFO] - Training Epoch: 1/2, step 5283/7134 completed (loss: 0.03074704110622406, acc: 0.9904240965843201)
[2025-02-13 03:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56][root][INFO] - Training Epoch: 1/2, step 5284/7134 completed (loss: 0.02788172848522663, acc: 0.9944674968719482)
[2025-02-13 03:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57][root][INFO] - Training Epoch: 1/2, step 5285/7134 completed (loss: 0.0453050471842289, acc: 0.9869822263717651)
[2025-02-13 03:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57][root][INFO] - Training Epoch: 1/2, step 5286/7134 completed (loss: 0.04295876622200012, acc: 0.9897058606147766)
[2025-02-13 03:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58][root][INFO] - Training Epoch: 1/2, step 5287/7134 completed (loss: 0.05288654565811157, acc: 0.9810298085212708)
[2025-02-13 03:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58][root][INFO] - Training Epoch: 1/2, step 5288/7134 completed (loss: 0.01509800087660551, acc: 0.9969325065612793)
[2025-02-13 03:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58][root][INFO] - Training Epoch: 1/2, step 5289/7134 completed (loss: 0.03414047509431839, acc: 0.9886685609817505)
[2025-02-13 03:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59][root][INFO] - Training Epoch: 1/2, step 5290/7134 completed (loss: 0.041630715131759644, acc: 0.9898734092712402)
[2025-02-13 03:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59][root][INFO] - Training Epoch: 1/2, step 5291/7134 completed (loss: 0.032237112522125244, acc: 0.9939393997192383)
[2025-02-13 03:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00][root][INFO] - Training Epoch: 1/2, step 5292/7134 completed (loss: 0.028395652770996094, acc: 0.9923664331436157)
[2025-02-13 03:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00][root][INFO] - Training Epoch: 1/2, step 5293/7134 completed (loss: 0.03038618341088295, acc: 0.9912408590316772)
[2025-02-13 03:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01][root][INFO] - Training Epoch: 1/2, step 5294/7134 completed (loss: 0.03132699057459831, acc: 0.9902371168136597)
[2025-02-13 03:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01][root][INFO] - Training Epoch: 1/2, step 5295/7134 completed (loss: 0.029913993552327156, acc: 0.9926470518112183)
[2025-02-13 03:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01][root][INFO] - Training Epoch: 1/2, step 5296/7134 completed (loss: 0.030253218486905098, acc: 0.9916550517082214)
[2025-02-13 03:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02][root][INFO] - Training Epoch: 1/2, step 5297/7134 completed (loss: 0.03143690526485443, acc: 0.9928160905838013)
[2025-02-13 03:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02][root][INFO] - Training Epoch: 1/2, step 5298/7134 completed (loss: 0.03650209680199623, acc: 0.9914529919624329)
[2025-02-13 03:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03][root][INFO] - Training Epoch: 1/2, step 5299/7134 completed (loss: 0.017251262441277504, acc: 0.995184600353241)
[2025-02-13 03:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03][root][INFO] - Training Epoch: 1/2, step 5300/7134 completed (loss: 0.029501011595129967, acc: 0.9911894202232361)
[2025-02-13 03:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:04][root][INFO] - Training Epoch: 1/2, step 5301/7134 completed (loss: 0.019774606451392174, acc: 0.9926578402519226)
[2025-02-13 03:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:04][root][INFO] - Training Epoch: 1/2, step 5302/7134 completed (loss: 0.0935620591044426, acc: 0.9772036671638489)
[2025-02-13 03:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:04][root][INFO] - Training Epoch: 1/2, step 5303/7134 completed (loss: 0.054508987814188004, acc: 0.991150438785553)
[2025-02-13 03:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05][root][INFO] - Training Epoch: 1/2, step 5304/7134 completed (loss: 0.07582350820302963, acc: 0.980033278465271)
[2025-02-13 03:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05][root][INFO] - Training Epoch: 1/2, step 5305/7134 completed (loss: 0.05657176673412323, acc: 0.985401451587677)
[2025-02-13 03:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06][root][INFO] - Training Epoch: 1/2, step 5306/7134 completed (loss: 0.015320531092584133, acc: 0.9972222447395325)
[2025-02-13 03:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06][root][INFO] - Training Epoch: 1/2, step 5307/7134 completed (loss: 0.06563972681760788, acc: 0.9814814925193787)
[2025-02-13 03:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06][root][INFO] - Training Epoch: 1/2, step 5308/7134 completed (loss: 0.04494911804795265, acc: 0.9864048361778259)
[2025-02-13 03:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07][root][INFO] - Training Epoch: 1/2, step 5309/7134 completed (loss: 0.09570889919996262, acc: 0.9785330891609192)
[2025-02-13 03:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07][root][INFO] - Training Epoch: 1/2, step 5310/7134 completed (loss: 0.0525093711912632, acc: 0.9860529899597168)
[2025-02-13 03:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08][root][INFO] - Training Epoch: 1/2, step 5311/7134 completed (loss: 0.07686643302440643, acc: 0.9807383418083191)
[2025-02-13 03:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08][root][INFO] - Training Epoch: 1/2, step 5312/7134 completed (loss: 0.03855052962899208, acc: 0.9887429475784302)
[2025-02-13 03:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09][root][INFO] - Training Epoch: 1/2, step 5313/7134 completed (loss: 0.039637722074985504, acc: 0.9854280352592468)
[2025-02-13 03:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09][root][INFO] - Training Epoch: 1/2, step 5314/7134 completed (loss: 0.045249324291944504, acc: 0.9912891983985901)
[2025-02-13 03:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09][root][INFO] - Training Epoch: 1/2, step 5315/7134 completed (loss: 0.030433882027864456, acc: 0.9901800155639648)
[2025-02-13 03:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10][root][INFO] - Training Epoch: 1/2, step 5316/7134 completed (loss: 0.04138966649770737, acc: 0.9932773113250732)
[2025-02-13 03:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10][root][INFO] - Training Epoch: 1/2, step 5317/7134 completed (loss: 0.020033597946166992, acc: 0.9946236610412598)
[2025-02-13 03:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11][root][INFO] - Training Epoch: 1/2, step 5318/7134 completed (loss: 0.019647007808089256, acc: 0.9930915236473083)
[2025-02-13 03:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11][root][INFO] - Training Epoch: 1/2, step 5319/7134 completed (loss: 0.04579726234078407, acc: 0.9927007555961609)
[2025-02-13 03:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11][root][INFO] - Training Epoch: 1/2, step 5320/7134 completed (loss: 0.031637731939554214, acc: 0.9922360181808472)
[2025-02-13 03:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12][root][INFO] - Training Epoch: 1/2, step 5321/7134 completed (loss: 0.02631620690226555, acc: 0.9891473054885864)
[2025-02-13 03:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12][root][INFO] - Training Epoch: 1/2, step 5322/7134 completed (loss: 0.019039487466216087, acc: 0.993966817855835)
[2025-02-13 03:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13][root][INFO] - Training Epoch: 1/2, step 5323/7134 completed (loss: 0.03921236842870712, acc: 0.9878787994384766)
[2025-02-13 03:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13][root][INFO] - Training Epoch: 1/2, step 5324/7134 completed (loss: 0.02794661931693554, acc: 0.989180862903595)
[2025-02-13 03:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14][root][INFO] - Training Epoch: 1/2, step 5325/7134 completed (loss: 0.02313614822924137, acc: 0.9965928196907043)
[2025-02-13 03:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14][root][INFO] - Training Epoch: 1/2, step 5326/7134 completed (loss: 0.049988314509391785, acc: 0.9854838848114014)
[2025-02-13 03:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14][root][INFO] - Training Epoch: 1/2, step 5327/7134 completed (loss: 0.02313081920146942, acc: 0.9927431344985962)
[2025-02-13 03:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15][root][INFO] - Training Epoch: 1/2, step 5328/7134 completed (loss: 0.018829509615898132, acc: 0.9963302612304688)
[2025-02-13 03:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15][root][INFO] - Training Epoch: 1/2, step 5329/7134 completed (loss: 0.0626198947429657, acc: 0.9837925434112549)
[2025-02-13 03:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:16][root][INFO] - Training Epoch: 1/2, step 5330/7134 completed (loss: 0.043196771293878555, acc: 0.984375)
[2025-02-13 03:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:16][root][INFO] - Training Epoch: 1/2, step 5331/7134 completed (loss: 0.036111652851104736, acc: 0.9929947257041931)
[2025-02-13 03:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:16][root][INFO] - Training Epoch: 1/2, step 5332/7134 completed (loss: 0.03505835682153702, acc: 0.9967479705810547)
[2025-02-13 03:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17][root][INFO] - Training Epoch: 1/2, step 5333/7134 completed (loss: 0.017353955656290054, acc: 0.9953271150588989)
[2025-02-13 03:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17][root][INFO] - Training Epoch: 1/2, step 5334/7134 completed (loss: 0.06939080357551575, acc: 0.9846860766410828)
[2025-02-13 03:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:18][root][INFO] - Training Epoch: 1/2, step 5335/7134 completed (loss: 0.03967135399580002, acc: 0.9904153347015381)
[2025-02-13 03:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:18][root][INFO] - Training Epoch: 1/2, step 5336/7134 completed (loss: 0.027318498119711876, acc: 0.9906250238418579)
[2025-02-13 03:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:18][root][INFO] - Training Epoch: 1/2, step 5337/7134 completed (loss: 0.025258542969822884, acc: 0.9922049045562744)
[2025-02-13 03:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19][root][INFO] - Training Epoch: 1/2, step 5338/7134 completed (loss: 0.028835786506533623, acc: 0.9954338073730469)
[2025-02-13 03:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19][root][INFO] - Training Epoch: 1/2, step 5339/7134 completed (loss: 0.04758067429065704, acc: 0.9883419871330261)
[2025-02-13 03:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20][root][INFO] - Training Epoch: 1/2, step 5340/7134 completed (loss: 0.0577932633459568, acc: 0.9792027473449707)
[2025-02-13 03:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20][root][INFO] - Training Epoch: 1/2, step 5341/7134 completed (loss: 0.019687045365571976, acc: 0.9920904040336609)
[2025-02-13 03:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:21][root][INFO] - Training Epoch: 1/2, step 5342/7134 completed (loss: 0.05046284198760986, acc: 0.9813581705093384)
[2025-02-13 03:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:21][root][INFO] - Training Epoch: 1/2, step 5343/7134 completed (loss: 0.05605586990714073, acc: 0.9841688871383667)
[2025-02-13 03:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22][root][INFO] - Training Epoch: 1/2, step 5344/7134 completed (loss: 0.039469655603170395, acc: 0.9873684048652649)
[2025-02-13 03:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22][root][INFO] - Training Epoch: 1/2, step 5345/7134 completed (loss: 0.03482341393828392, acc: 0.9896551966667175)
[2025-02-13 03:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22][root][INFO] - Training Epoch: 1/2, step 5346/7134 completed (loss: 0.051421720534563065, acc: 0.9853095412254333)
[2025-02-13 03:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23][root][INFO] - Training Epoch: 1/2, step 5347/7134 completed (loss: 0.02503134496510029, acc: 0.9893898963928223)
[2025-02-13 03:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23][root][INFO] - Training Epoch: 1/2, step 5348/7134 completed (loss: 0.03974970057606697, acc: 0.9875346422195435)
[2025-02-13 03:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:45][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0506, device='cuda:0') eval_epoch_loss=tensor(0.0493, device='cuda:0') eval_epoch_acc=tensor(0.9862, device='cuda:0')
[2025-02-13 03:17:45][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:17:45][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:17:45][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_5349_loss_0.049329716712236404/model.pt
[2025-02-13 03:17:45][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:17:45][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.049329716712236404
[2025-02-13 03:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:45][root][INFO] - Training Epoch: 1/2, step 5349/7134 completed (loss: 0.0475429892539978, acc: 0.9870800971984863)
[2025-02-13 03:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:46][root][INFO] - Training Epoch: 1/2, step 5350/7134 completed (loss: 0.035459451377391815, acc: 0.9881109595298767)
[2025-02-13 03:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:46][root][INFO] - Training Epoch: 1/2, step 5351/7134 completed (loss: 0.05053061619400978, acc: 0.9900744557380676)
[2025-02-13 03:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:47][root][INFO] - Training Epoch: 1/2, step 5352/7134 completed (loss: 0.030909297987818718, acc: 0.9935317039489746)
[2025-02-13 03:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:47][root][INFO] - Training Epoch: 1/2, step 5353/7134 completed (loss: 0.014300326816737652, acc: 0.9954904317855835)
[2025-02-13 03:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48][root][INFO] - Training Epoch: 1/2, step 5354/7134 completed (loss: 0.04413497447967529, acc: 0.9856828451156616)
[2025-02-13 03:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48][root][INFO] - Training Epoch: 1/2, step 5355/7134 completed (loss: 0.07177465409040451, acc: 0.9810874462127686)
[2025-02-13 03:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48][root][INFO] - Training Epoch: 1/2, step 5356/7134 completed (loss: 0.055846136063337326, acc: 0.9838107228279114)
[2025-02-13 03:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:49][root][INFO] - Training Epoch: 1/2, step 5357/7134 completed (loss: 0.04224398732185364, acc: 0.9878048896789551)
[2025-02-13 03:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:49][root][INFO] - Training Epoch: 1/2, step 5358/7134 completed (loss: 0.047695595771074295, acc: 0.9875862002372742)
[2025-02-13 03:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:50][root][INFO] - Training Epoch: 1/2, step 5359/7134 completed (loss: 0.02176598273217678, acc: 0.9918414950370789)
[2025-02-13 03:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:50][root][INFO] - Training Epoch: 1/2, step 5360/7134 completed (loss: 0.016126958653330803, acc: 0.9931318759918213)
[2025-02-13 03:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51][root][INFO] - Training Epoch: 1/2, step 5361/7134 completed (loss: 0.03795669600367546, acc: 0.9880239367485046)
[2025-02-13 03:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51][root][INFO] - Training Epoch: 1/2, step 5362/7134 completed (loss: 0.03462802991271019, acc: 0.9910447597503662)
[2025-02-13 03:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:52][root][INFO] - Training Epoch: 1/2, step 5363/7134 completed (loss: 0.07171324640512466, acc: 0.9767441749572754)
[2025-02-13 03:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:52][root][INFO] - Training Epoch: 1/2, step 5364/7134 completed (loss: 0.03124040924012661, acc: 0.9883117079734802)
[2025-02-13 03:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53][root][INFO] - Training Epoch: 1/2, step 5365/7134 completed (loss: 0.02226840704679489, acc: 0.9929453134536743)
[2025-02-13 03:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53][root][INFO] - Training Epoch: 1/2, step 5366/7134 completed (loss: 0.08228761702775955, acc: 0.9791666865348816)
[2025-02-13 03:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53][root][INFO] - Training Epoch: 1/2, step 5367/7134 completed (loss: 0.0302771907299757, acc: 0.9912060499191284)
[2025-02-13 03:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:54][root][INFO] - Training Epoch: 1/2, step 5368/7134 completed (loss: 0.04290251433849335, acc: 0.9832776188850403)
[2025-02-13 03:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:54][root][INFO] - Training Epoch: 1/2, step 5369/7134 completed (loss: 0.03761371597647667, acc: 0.987261176109314)
[2025-02-13 03:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55][root][INFO] - Training Epoch: 1/2, step 5370/7134 completed (loss: 0.06094016134738922, acc: 0.9843013882637024)
[2025-02-13 03:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55][root][INFO] - Training Epoch: 1/2, step 5371/7134 completed (loss: 0.0746135264635086, acc: 0.9817276000976562)
[2025-02-13 03:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56][root][INFO] - Training Epoch: 1/2, step 5372/7134 completed (loss: 0.015550738200545311, acc: 0.9929873943328857)
[2025-02-13 03:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56][root][INFO] - Training Epoch: 1/2, step 5373/7134 completed (loss: 0.06340615451335907, acc: 0.9769874215126038)
[2025-02-13 03:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56][root][INFO] - Training Epoch: 1/2, step 5374/7134 completed (loss: 0.055379677563905716, acc: 0.9839228391647339)
[2025-02-13 03:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:57][root][INFO] - Training Epoch: 1/2, step 5375/7134 completed (loss: 0.06659775227308273, acc: 0.982758641242981)
[2025-02-13 03:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:57][root][INFO] - Training Epoch: 1/2, step 5376/7134 completed (loss: 0.025393405929207802, acc: 0.9912891983985901)
[2025-02-13 03:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58][root][INFO] - Training Epoch: 1/2, step 5377/7134 completed (loss: 0.03962273150682449, acc: 0.9924127459526062)
[2025-02-13 03:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58][root][INFO] - Training Epoch: 1/2, step 5378/7134 completed (loss: 0.020413437858223915, acc: 0.9939320683479309)
[2025-02-13 03:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:59][root][INFO] - Training Epoch: 1/2, step 5379/7134 completed (loss: 0.04032370448112488, acc: 0.990920901298523)
[2025-02-13 03:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:59][root][INFO] - Training Epoch: 1/2, step 5380/7134 completed (loss: 0.037967827171087265, acc: 0.9858906269073486)
[2025-02-13 03:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:59][root][INFO] - Training Epoch: 1/2, step 5381/7134 completed (loss: 0.02957191877067089, acc: 0.991631805896759)
[2025-02-13 03:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00][root][INFO] - Training Epoch: 1/2, step 5382/7134 completed (loss: 0.016805635765194893, acc: 0.9948805570602417)
[2025-02-13 03:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00][root][INFO] - Training Epoch: 1/2, step 5383/7134 completed (loss: 0.03264137730002403, acc: 0.9867424368858337)
[2025-02-13 03:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:01][root][INFO] - Training Epoch: 1/2, step 5384/7134 completed (loss: 0.02261713519692421, acc: 0.9940758347511292)
[2025-02-13 03:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:01][root][INFO] - Training Epoch: 1/2, step 5385/7134 completed (loss: 0.03556446731090546, acc: 0.9920739531517029)
[2025-02-13 03:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:02][root][INFO] - Training Epoch: 1/2, step 5386/7134 completed (loss: 0.02020459808409214, acc: 0.9915730357170105)
[2025-02-13 03:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:02][root][INFO] - Training Epoch: 1/2, step 5387/7134 completed (loss: 0.012834864668548107, acc: 0.9980952143669128)
[2025-02-13 03:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03][root][INFO] - Training Epoch: 1/2, step 5388/7134 completed (loss: 0.02673349902033806, acc: 0.9898989796638489)
[2025-02-13 03:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03][root][INFO] - Training Epoch: 1/2, step 5389/7134 completed (loss: 0.015562737360596657, acc: 0.9968051314353943)
[2025-02-13 03:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:04][root][INFO] - Training Epoch: 1/2, step 5390/7134 completed (loss: 0.037058550864458084, acc: 0.9904912710189819)
[2025-02-13 03:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:04][root][INFO] - Training Epoch: 1/2, step 5391/7134 completed (loss: 0.05386705696582794, acc: 0.9879931211471558)
[2025-02-13 03:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:04][root][INFO] - Training Epoch: 1/2, step 5392/7134 completed (loss: 0.0510052889585495, acc: 0.9817073345184326)
[2025-02-13 03:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05][root][INFO] - Training Epoch: 1/2, step 5393/7134 completed (loss: 0.023996438831090927, acc: 0.990234375)
[2025-02-13 03:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05][root][INFO] - Training Epoch: 1/2, step 5394/7134 completed (loss: 0.051082074642181396, acc: 0.9821428656578064)
[2025-02-13 03:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06][root][INFO] - Training Epoch: 1/2, step 5395/7134 completed (loss: 0.02173863723874092, acc: 0.9894419312477112)
[2025-02-13 03:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06][root][INFO] - Training Epoch: 1/2, step 5396/7134 completed (loss: 0.0440589040517807, acc: 0.9864661693572998)
[2025-02-13 03:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06][root][INFO] - Training Epoch: 1/2, step 5397/7134 completed (loss: 0.02921544387936592, acc: 0.989847719669342)
[2025-02-13 03:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:07][root][INFO] - Training Epoch: 1/2, step 5398/7134 completed (loss: 0.03013421781361103, acc: 0.9882006049156189)
[2025-02-13 03:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:07][root][INFO] - Training Epoch: 1/2, step 5399/7134 completed (loss: 0.027591893449425697, acc: 0.9917920827865601)
[2025-02-13 03:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08][root][INFO] - Training Epoch: 1/2, step 5400/7134 completed (loss: 0.10221603512763977, acc: 0.9717868566513062)
[2025-02-13 03:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08][root][INFO] - Training Epoch: 1/2, step 5401/7134 completed (loss: 0.022279489785432816, acc: 0.995184600353241)
[2025-02-13 03:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09][root][INFO] - Training Epoch: 1/2, step 5402/7134 completed (loss: 0.028638966381549835, acc: 0.9897210001945496)
[2025-02-13 03:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09][root][INFO] - Training Epoch: 1/2, step 5403/7134 completed (loss: 0.06380622833967209, acc: 0.9885807633399963)
[2025-02-13 03:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09][root][INFO] - Training Epoch: 1/2, step 5404/7134 completed (loss: 0.07056229561567307, acc: 0.9838926196098328)
[2025-02-13 03:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:10][root][INFO] - Training Epoch: 1/2, step 5405/7134 completed (loss: 0.029382210224866867, acc: 0.9930192232131958)
[2025-02-13 03:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:10][root][INFO] - Training Epoch: 1/2, step 5406/7134 completed (loss: 0.024453729391098022, acc: 0.9941520690917969)
[2025-02-13 03:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11][root][INFO] - Training Epoch: 1/2, step 5407/7134 completed (loss: 0.04698798805475235, acc: 0.987922728061676)
[2025-02-13 03:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11][root][INFO] - Training Epoch: 1/2, step 5408/7134 completed (loss: 0.06620384007692337, acc: 0.9843993782997131)
[2025-02-13 03:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:12][root][INFO] - Training Epoch: 1/2, step 5409/7134 completed (loss: 0.08967261761426926, acc: 0.9833585619926453)
[2025-02-13 03:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:12][root][INFO] - Training Epoch: 1/2, step 5410/7134 completed (loss: 0.060039959847927094, acc: 0.9829396605491638)
[2025-02-13 03:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13][root][INFO] - Training Epoch: 1/2, step 5411/7134 completed (loss: 0.044949162751436234, acc: 0.9883889555931091)
[2025-02-13 03:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13][root][INFO] - Training Epoch: 1/2, step 5412/7134 completed (loss: 0.10160672664642334, acc: 0.9788461327552795)
[2025-02-13 03:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13][root][INFO] - Training Epoch: 1/2, step 5413/7134 completed (loss: 0.07871323823928833, acc: 0.9785714149475098)
[2025-02-13 03:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14][root][INFO] - Training Epoch: 1/2, step 5414/7134 completed (loss: 0.13036903738975525, acc: 0.9717223644256592)
[2025-02-13 03:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14][root][INFO] - Training Epoch: 1/2, step 5415/7134 completed (loss: 0.05845436081290245, acc: 0.9799426794052124)
[2025-02-13 03:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:15][root][INFO] - Training Epoch: 1/2, step 5416/7134 completed (loss: 0.059608008712530136, acc: 0.9837398529052734)
[2025-02-13 03:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:15][root][INFO] - Training Epoch: 1/2, step 5417/7134 completed (loss: 0.024051452055573463, acc: 0.9937577843666077)
[2025-02-13 03:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16][root][INFO] - Training Epoch: 1/2, step 5418/7134 completed (loss: 0.06085760146379471, acc: 0.9793205261230469)
[2025-02-13 03:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16][root][INFO] - Training Epoch: 1/2, step 5419/7134 completed (loss: 0.1190563291311264, acc: 0.9644067883491516)
[2025-02-13 03:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:17][root][INFO] - Training Epoch: 1/2, step 5420/7134 completed (loss: 0.10023742914199829, acc: 0.9737762212753296)
[2025-02-13 03:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:17][root][INFO] - Training Epoch: 1/2, step 5421/7134 completed (loss: 0.08893341571092606, acc: 0.975039005279541)
[2025-02-13 03:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:17][root][INFO] - Training Epoch: 1/2, step 5422/7134 completed (loss: 0.04649819806218147, acc: 0.9866864085197449)
[2025-02-13 03:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:18][root][INFO] - Training Epoch: 1/2, step 5423/7134 completed (loss: 0.06764145195484161, acc: 0.9882698059082031)
[2025-02-13 03:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:18][root][INFO] - Training Epoch: 1/2, step 5424/7134 completed (loss: 0.02822166122496128, acc: 0.9911971688270569)
[2025-02-13 03:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19][root][INFO] - Training Epoch: 1/2, step 5425/7134 completed (loss: 0.03372849524021149, acc: 0.9961165189743042)
[2025-02-13 03:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19][root][INFO] - Training Epoch: 1/2, step 5426/7134 completed (loss: 0.06799918413162231, acc: 0.9859437942504883)
[2025-02-13 03:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:20][root][INFO] - Training Epoch: 1/2, step 5427/7134 completed (loss: 0.026131480932235718, acc: 0.9936808943748474)
[2025-02-13 03:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:20][root][INFO] - Training Epoch: 1/2, step 5428/7134 completed (loss: 0.04471810162067413, acc: 0.9863713979721069)
[2025-02-13 03:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:20][root][INFO] - Training Epoch: 1/2, step 5429/7134 completed (loss: 0.09525506943464279, acc: 0.9819494485855103)
[2025-02-13 03:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21][root][INFO] - Training Epoch: 1/2, step 5430/7134 completed (loss: 0.026983976364135742, acc: 0.9905956387519836)
[2025-02-13 03:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21][root][INFO] - Training Epoch: 1/2, step 5431/7134 completed (loss: 0.033087652176618576, acc: 0.9905481934547424)
[2025-02-13 03:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:22][root][INFO] - Training Epoch: 1/2, step 5432/7134 completed (loss: 0.023633411154150963, acc: 0.9924670457839966)
[2025-02-13 03:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:22][root][INFO] - Training Epoch: 1/2, step 5433/7134 completed (loss: 0.0569627583026886, acc: 0.9878787994384766)
[2025-02-13 03:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:22][root][INFO] - Training Epoch: 1/2, step 5434/7134 completed (loss: 0.02567043900489807, acc: 0.9939637780189514)
[2025-02-13 03:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:23][root][INFO] - Training Epoch: 1/2, step 5435/7134 completed (loss: 0.0269317664206028, acc: 0.9895397424697876)
[2025-02-13 03:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:23][root][INFO] - Training Epoch: 1/2, step 5436/7134 completed (loss: 0.02826545014977455, acc: 0.9896142482757568)
[2025-02-13 03:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24][root][INFO] - Training Epoch: 1/2, step 5437/7134 completed (loss: 0.023945556953549385, acc: 0.9930192232131958)
[2025-02-13 03:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24][root][INFO] - Training Epoch: 1/2, step 5438/7134 completed (loss: 0.028273222967982292, acc: 0.9922480583190918)
[2025-02-13 03:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:25][root][INFO] - Training Epoch: 1/2, step 5439/7134 completed (loss: 0.04422161355614662, acc: 0.9840116500854492)
[2025-02-13 03:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:25][root][INFO] - Training Epoch: 1/2, step 5440/7134 completed (loss: 0.05227552354335785, acc: 0.984674334526062)
[2025-02-13 03:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:26][root][INFO] - Training Epoch: 1/2, step 5441/7134 completed (loss: 0.02758237160742283, acc: 0.9926560521125793)
[2025-02-13 03:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:26][root][INFO] - Training Epoch: 1/2, step 5442/7134 completed (loss: 0.024004610255360603, acc: 0.9959404468536377)
[2025-02-13 03:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:26][root][INFO] - Training Epoch: 1/2, step 5443/7134 completed (loss: 0.03272624313831329, acc: 0.9900568127632141)
[2025-02-13 03:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27][root][INFO] - Training Epoch: 1/2, step 5444/7134 completed (loss: 0.03382416069507599, acc: 0.9908758997917175)
[2025-02-13 03:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27][root][INFO] - Training Epoch: 1/2, step 5445/7134 completed (loss: 0.047876112163066864, acc: 0.9890310764312744)
[2025-02-13 03:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:28][root][INFO] - Training Epoch: 1/2, step 5446/7134 completed (loss: 0.03687858209013939, acc: 0.994397759437561)
[2025-02-13 03:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:28][root][INFO] - Training Epoch: 1/2, step 5447/7134 completed (loss: 0.03145433962345123, acc: 0.988727867603302)
[2025-02-13 03:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:29][root][INFO] - Training Epoch: 1/2, step 5448/7134 completed (loss: 0.04100550338625908, acc: 0.9846368432044983)
[2025-02-13 03:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:29][root][INFO] - Training Epoch: 1/2, step 5449/7134 completed (loss: 0.015352223999798298, acc: 0.9964912533760071)
[2025-02-13 03:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:29][root][INFO] - Training Epoch: 1/2, step 5450/7134 completed (loss: 0.020923206582665443, acc: 0.992277979850769)
[2025-02-13 03:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30][root][INFO] - Training Epoch: 1/2, step 5451/7134 completed (loss: 0.028864676132798195, acc: 0.993446946144104)
[2025-02-13 03:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30][root][INFO] - Training Epoch: 1/2, step 5452/7134 completed (loss: 0.050100162625312805, acc: 0.9896640777587891)
[2025-02-13 03:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31][root][INFO] - Training Epoch: 1/2, step 5453/7134 completed (loss: 0.04779135435819626, acc: 0.987730085849762)
[2025-02-13 03:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31][root][INFO] - Training Epoch: 1/2, step 5454/7134 completed (loss: 0.034177035093307495, acc: 0.9921773076057434)
[2025-02-13 03:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32][root][INFO] - Training Epoch: 1/2, step 5455/7134 completed (loss: 0.08666377514600754, acc: 0.9823232293128967)
[2025-02-13 03:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32][root][INFO] - Training Epoch: 1/2, step 5456/7134 completed (loss: 0.034817177802324295, acc: 0.991183876991272)
[2025-02-13 03:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33][root][INFO] - Training Epoch: 1/2, step 5457/7134 completed (loss: 0.06908068805932999, acc: 0.9838926196098328)
[2025-02-13 03:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33][root][INFO] - Training Epoch: 1/2, step 5458/7134 completed (loss: 0.032219741493463516, acc: 0.9910827875137329)
[2025-02-13 03:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34][root][INFO] - Training Epoch: 1/2, step 5459/7134 completed (loss: 0.009997249580919743, acc: 0.997019350528717)
[2025-02-13 03:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34][root][INFO] - Training Epoch: 1/2, step 5460/7134 completed (loss: 0.032896578311920166, acc: 0.9944367408752441)
[2025-02-13 03:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34][root][INFO] - Training Epoch: 1/2, step 5461/7134 completed (loss: 0.05120963603258133, acc: 0.9867549538612366)
[2025-02-13 03:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:35][root][INFO] - Training Epoch: 1/2, step 5462/7134 completed (loss: 0.009007057175040245, acc: 0.9985693693161011)
[2025-02-13 03:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:35][root][INFO] - Training Epoch: 1/2, step 5463/7134 completed (loss: 0.026902269572019577, acc: 0.9923664331436157)
[2025-02-13 03:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36][root][INFO] - Training Epoch: 1/2, step 5464/7134 completed (loss: 0.034761328250169754, acc: 0.9867899417877197)
[2025-02-13 03:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36][root][INFO] - Training Epoch: 1/2, step 5465/7134 completed (loss: 0.042294569313526154, acc: 0.9895366430282593)
[2025-02-13 03:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:37][root][INFO] - Training Epoch: 1/2, step 5466/7134 completed (loss: 0.020539574325084686, acc: 0.9930955171585083)
[2025-02-13 03:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:37][root][INFO] - Training Epoch: 1/2, step 5467/7134 completed (loss: 0.018119411543011665, acc: 0.9941314458847046)
[2025-02-13 03:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:38][root][INFO] - Training Epoch: 1/2, step 5468/7134 completed (loss: 0.0324070006608963, acc: 0.9941995143890381)
[2025-02-13 03:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:38][root][INFO] - Training Epoch: 1/2, step 5469/7134 completed (loss: 0.02714628353714943, acc: 0.9926380515098572)
[2025-02-13 03:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:38][root][INFO] - Training Epoch: 1/2, step 5470/7134 completed (loss: 0.0073745702393352985, acc: 0.9987951517105103)
[2025-02-13 03:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39][root][INFO] - Training Epoch: 1/2, step 5471/7134 completed (loss: 0.011719189584255219, acc: 0.995945930480957)
[2025-02-13 03:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39][root][INFO] - Training Epoch: 1/2, step 5472/7134 completed (loss: 0.02569233626127243, acc: 0.9950124621391296)
[2025-02-13 03:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:40][root][INFO] - Training Epoch: 1/2, step 5473/7134 completed (loss: 0.016310837119817734, acc: 0.991411030292511)
[2025-02-13 03:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:40][root][INFO] - Training Epoch: 1/2, step 5474/7134 completed (loss: 0.02704692631959915, acc: 0.991428554058075)
[2025-02-13 03:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:41][root][INFO] - Training Epoch: 1/2, step 5475/7134 completed (loss: 0.018357230350375175, acc: 0.9943100810050964)
[2025-02-13 03:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:41][root][INFO] - Training Epoch: 1/2, step 5476/7134 completed (loss: 0.014677370898425579, acc: 0.9927641153335571)
[2025-02-13 03:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42][root][INFO] - Training Epoch: 1/2, step 5477/7134 completed (loss: 0.015020922757685184, acc: 0.9950739145278931)
[2025-02-13 03:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42][root][INFO] - Training Epoch: 1/2, step 5478/7134 completed (loss: 0.04864497855305672, acc: 0.9862778782844543)
[2025-02-13 03:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42][root][INFO] - Training Epoch: 1/2, step 5479/7134 completed (loss: 0.0434783436357975, acc: 0.9823369383811951)
[2025-02-13 03:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:43][root][INFO] - Training Epoch: 1/2, step 5480/7134 completed (loss: 0.056009311228990555, acc: 0.9855263233184814)
[2025-02-13 03:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:43][root][INFO] - Training Epoch: 1/2, step 5481/7134 completed (loss: 0.019032875075936317, acc: 0.9939393997192383)
[2025-02-13 03:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:44][root][INFO] - Training Epoch: 1/2, step 5482/7134 completed (loss: 0.04514534771442413, acc: 0.987908124923706)
[2025-02-13 03:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45][root][INFO] - Training Epoch: 1/2, step 5483/7134 completed (loss: 0.038000695407390594, acc: 0.9888579249382019)
[2025-02-13 03:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45][root][INFO] - Training Epoch: 1/2, step 5484/7134 completed (loss: 0.04666205495595932, acc: 0.9833101630210876)
[2025-02-13 03:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46][root][INFO] - Training Epoch: 1/2, step 5485/7134 completed (loss: 0.061783384531736374, acc: 0.9854809641838074)
[2025-02-13 03:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46][root][INFO] - Training Epoch: 1/2, step 5486/7134 completed (loss: 0.05616006627678871, acc: 0.9826989769935608)
[2025-02-13 03:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46][root][INFO] - Training Epoch: 1/2, step 5487/7134 completed (loss: 0.015448655933141708, acc: 0.995121955871582)
[2025-02-13 03:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:47][root][INFO] - Training Epoch: 1/2, step 5488/7134 completed (loss: 0.057803988456726074, acc: 0.9763157963752747)
[2025-02-13 03:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:47][root][INFO] - Training Epoch: 1/2, step 5489/7134 completed (loss: 0.042226605117321014, acc: 0.9845201373100281)
[2025-02-13 03:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48][root][INFO] - Training Epoch: 1/2, step 5490/7134 completed (loss: 0.02184450253844261, acc: 0.9931662678718567)
[2025-02-13 03:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48][root][INFO] - Training Epoch: 1/2, step 5491/7134 completed (loss: 0.02084832824766636, acc: 0.9941349029541016)
[2025-02-13 03:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49][root][INFO] - Training Epoch: 1/2, step 5492/7134 completed (loss: 0.0449967198073864, acc: 0.9915397763252258)
[2025-02-13 03:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49][root][INFO] - Training Epoch: 1/2, step 5493/7134 completed (loss: 0.04641120508313179, acc: 0.979619562625885)
[2025-02-13 03:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49][root][INFO] - Training Epoch: 1/2, step 5494/7134 completed (loss: 0.01925797201693058, acc: 0.9928876161575317)
[2025-02-13 03:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:50][root][INFO] - Training Epoch: 1/2, step 5495/7134 completed (loss: 0.022818244993686676, acc: 0.9910394549369812)
[2025-02-13 03:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:50][root][INFO] - Training Epoch: 1/2, step 5496/7134 completed (loss: 0.09935829788446426, acc: 0.9766536951065063)
[2025-02-13 03:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51][root][INFO] - Training Epoch: 1/2, step 5497/7134 completed (loss: 0.06062909588217735, acc: 0.980966329574585)
[2025-02-13 03:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51][root][INFO] - Training Epoch: 1/2, step 5498/7134 completed (loss: 0.031389761716127396, acc: 0.988063633441925)
[2025-02-13 03:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:52][root][INFO] - Training Epoch: 1/2, step 5499/7134 completed (loss: 0.03397734463214874, acc: 0.9874371886253357)
[2025-02-13 03:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:52][root][INFO] - Training Epoch: 1/2, step 5500/7134 completed (loss: 0.05197049304842949, acc: 0.9877216815948486)
[2025-02-13 03:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:52][root][INFO] - Training Epoch: 1/2, step 5501/7134 completed (loss: 0.02419617958366871, acc: 0.990123450756073)
[2025-02-13 03:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53][root][INFO] - Training Epoch: 1/2, step 5502/7134 completed (loss: 0.03615078330039978, acc: 0.9851552248001099)
[2025-02-13 03:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53][root][INFO] - Training Epoch: 1/2, step 5503/7134 completed (loss: 0.07005400210618973, acc: 0.9801192879676819)
[2025-02-13 03:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:54][root][INFO] - Training Epoch: 1/2, step 5504/7134 completed (loss: 0.021786760538816452, acc: 0.9975786805152893)
[2025-02-13 03:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:54][root][INFO] - Training Epoch: 1/2, step 5505/7134 completed (loss: 0.029614584520459175, acc: 0.9929078221321106)
[2025-02-13 03:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:55][root][INFO] - Training Epoch: 1/2, step 5506/7134 completed (loss: 0.047106482088565826, acc: 0.9873125553131104)
[2025-02-13 03:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:55][root][INFO] - Training Epoch: 1/2, step 5507/7134 completed (loss: 0.04115976020693779, acc: 0.9873949289321899)
[2025-02-13 03:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56][root][INFO] - Training Epoch: 1/2, step 5508/7134 completed (loss: 0.03713621944189072, acc: 0.9848130941390991)
[2025-02-13 03:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56][root][INFO] - Training Epoch: 1/2, step 5509/7134 completed (loss: 0.03353183716535568, acc: 0.9906432628631592)
[2025-02-13 03:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56][root][INFO] - Training Epoch: 1/2, step 5510/7134 completed (loss: 0.054430533200502396, acc: 0.9845758080482483)
[2025-02-13 03:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:57][root][INFO] - Training Epoch: 1/2, step 5511/7134 completed (loss: 0.04787864536046982, acc: 0.9852941036224365)
[2025-02-13 03:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:57][root][INFO] - Training Epoch: 1/2, step 5512/7134 completed (loss: 0.01776079274713993, acc: 0.9960474371910095)
[2025-02-13 03:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:58][root][INFO] - Training Epoch: 1/2, step 5513/7134 completed (loss: 0.03888219594955444, acc: 0.9836956262588501)
[2025-02-13 03:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:58][root][INFO] - Training Epoch: 1/2, step 5514/7134 completed (loss: 0.09416905045509338, acc: 0.9751243591308594)
[2025-02-13 03:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59][root][INFO] - Training Epoch: 1/2, step 5515/7134 completed (loss: 0.04671575129032135, acc: 0.984308123588562)
[2025-02-13 03:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59][root][INFO] - Training Epoch: 1/2, step 5516/7134 completed (loss: 0.03353889286518097, acc: 0.9957805871963501)
[2025-02-13 03:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00][root][INFO] - Training Epoch: 1/2, step 5517/7134 completed (loss: 0.03721802309155464, acc: 0.9880319237709045)
[2025-02-13 03:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00][root][INFO] - Training Epoch: 1/2, step 5518/7134 completed (loss: 0.04289734363555908, acc: 0.989130437374115)
[2025-02-13 03:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00][root][INFO] - Training Epoch: 1/2, step 5519/7134 completed (loss: 0.019217582419514656, acc: 0.9944827556610107)
[2025-02-13 03:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01][root][INFO] - Training Epoch: 1/2, step 5520/7134 completed (loss: 0.027155177667737007, acc: 0.9920724630355835)
[2025-02-13 03:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01][root][INFO] - Training Epoch: 1/2, step 5521/7134 completed (loss: 0.03991125151515007, acc: 0.9861809015274048)
[2025-02-13 03:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:02][root][INFO] - Training Epoch: 1/2, step 5522/7134 completed (loss: 0.05998019501566887, acc: 0.9803921580314636)
[2025-02-13 03:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:02][root][INFO] - Training Epoch: 1/2, step 5523/7134 completed (loss: 0.03606202080845833, acc: 0.992977499961853)
[2025-02-13 03:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03][root][INFO] - Training Epoch: 1/2, step 5524/7134 completed (loss: 0.04764583334326744, acc: 0.9860681295394897)
[2025-02-13 03:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03][root][INFO] - Training Epoch: 1/2, step 5525/7134 completed (loss: 0.03877881541848183, acc: 0.9838945865631104)
[2025-02-13 03:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:04][root][INFO] - Training Epoch: 1/2, step 5526/7134 completed (loss: 0.03492126241326332, acc: 0.9882179498672485)
[2025-02-13 03:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:04][root][INFO] - Training Epoch: 1/2, step 5527/7134 completed (loss: 0.05392865091562271, acc: 0.9815126061439514)
[2025-02-13 03:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:04][root][INFO] - Training Epoch: 1/2, step 5528/7134 completed (loss: 0.03234957531094551, acc: 0.9857988357543945)
[2025-02-13 03:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05][root][INFO] - Training Epoch: 1/2, step 5529/7134 completed (loss: 0.04898684471845627, acc: 0.9858155846595764)
[2025-02-13 03:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05][root][INFO] - Training Epoch: 1/2, step 5530/7134 completed (loss: 0.03445883095264435, acc: 0.9910581111907959)
[2025-02-13 03:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:06][root][INFO] - Training Epoch: 1/2, step 5531/7134 completed (loss: 0.051257017999887466, acc: 0.9850560426712036)
[2025-02-13 03:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:06][root][INFO] - Training Epoch: 1/2, step 5532/7134 completed (loss: 0.04567496106028557, acc: 0.9851668477058411)
[2025-02-13 03:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07][root][INFO] - Training Epoch: 1/2, step 5533/7134 completed (loss: 0.011415657587349415, acc: 0.9965986609458923)
[2025-02-13 03:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07][root][INFO] - Training Epoch: 1/2, step 5534/7134 completed (loss: 0.039800308644771576, acc: 0.9848130941390991)
[2025-02-13 03:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:08][root][INFO] - Training Epoch: 1/2, step 5535/7134 completed (loss: 0.032971274107694626, acc: 0.9891186356544495)
[2025-02-13 03:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:08][root][INFO] - Training Epoch: 1/2, step 5536/7134 completed (loss: 0.023636365309357643, acc: 0.9930232763290405)
[2025-02-13 03:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09][root][INFO] - Training Epoch: 1/2, step 5537/7134 completed (loss: 0.036523666232824326, acc: 0.9848307967185974)
[2025-02-13 03:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09][root][INFO] - Training Epoch: 1/2, step 5538/7134 completed (loss: 0.03545309603214264, acc: 0.9931740760803223)
[2025-02-13 03:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09][root][INFO] - Training Epoch: 1/2, step 5539/7134 completed (loss: 0.038752321153879166, acc: 0.9898648858070374)
[2025-02-13 03:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:10][root][INFO] - Training Epoch: 1/2, step 5540/7134 completed (loss: 0.0364583358168602, acc: 0.9907894730567932)
[2025-02-13 03:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:10][root][INFO] - Training Epoch: 1/2, step 5541/7134 completed (loss: 0.014490650035440922, acc: 0.998670220375061)
[2025-02-13 03:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11][root][INFO] - Training Epoch: 1/2, step 5542/7134 completed (loss: 0.01745782606303692, acc: 0.995720386505127)
[2025-02-13 03:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11][root][INFO] - Training Epoch: 1/2, step 5543/7134 completed (loss: 0.03564407303929329, acc: 0.9911308288574219)
[2025-02-13 03:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:12][root][INFO] - Training Epoch: 1/2, step 5544/7134 completed (loss: 0.024509189650416374, acc: 0.9935483932495117)
[2025-02-13 03:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:12][root][INFO] - Training Epoch: 1/2, step 5545/7134 completed (loss: 0.03636324405670166, acc: 0.9901639223098755)
[2025-02-13 03:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13][root][INFO] - Training Epoch: 1/2, step 5546/7134 completed (loss: 0.03833523765206337, acc: 0.9911406636238098)
[2025-02-13 03:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13][root][INFO] - Training Epoch: 1/2, step 5547/7134 completed (loss: 0.01957605592906475, acc: 0.9942396283149719)
[2025-02-13 03:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:14][root][INFO] - Training Epoch: 1/2, step 5548/7134 completed (loss: 0.021135827526450157, acc: 0.9918604493141174)
[2025-02-13 03:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:14][root][INFO] - Training Epoch: 1/2, step 5549/7134 completed (loss: 0.027486611157655716, acc: 0.9900000095367432)
[2025-02-13 03:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:14][root][INFO] - Training Epoch: 1/2, step 5550/7134 completed (loss: 0.017387179657816887, acc: 0.9917743802070618)
[2025-02-13 03:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15][root][INFO] - Training Epoch: 1/2, step 5551/7134 completed (loss: 0.06433428823947906, acc: 0.9837618470191956)
[2025-02-13 03:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15][root][INFO] - Training Epoch: 1/2, step 5552/7134 completed (loss: 0.011359683237969875, acc: 0.9964328408241272)
[2025-02-13 03:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:16][root][INFO] - Training Epoch: 1/2, step 5553/7134 completed (loss: 0.03173799812793732, acc: 0.990111231803894)
[2025-02-13 03:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:16][root][INFO] - Training Epoch: 1/2, step 5554/7134 completed (loss: 0.021516311913728714, acc: 0.9914425611495972)
[2025-02-13 03:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17][root][INFO] - Training Epoch: 1/2, step 5555/7134 completed (loss: 0.029747743159532547, acc: 0.9879679083824158)
[2025-02-13 03:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17][root][INFO] - Training Epoch: 1/2, step 5556/7134 completed (loss: 0.04167167842388153, acc: 0.9857327938079834)
[2025-02-13 03:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18][root][INFO] - Training Epoch: 1/2, step 5557/7134 completed (loss: 0.023840270936489105, acc: 0.994397759437561)
[2025-02-13 03:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18][root][INFO] - Training Epoch: 1/2, step 5558/7134 completed (loss: 0.017930109053850174, acc: 0.9950310587882996)
[2025-02-13 03:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18][root][INFO] - Training Epoch: 1/2, step 5559/7134 completed (loss: 0.036523014307022095, acc: 0.991304337978363)
[2025-02-13 03:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19][root][INFO] - Training Epoch: 1/2, step 5560/7134 completed (loss: 0.05066709220409393, acc: 0.9883419871330261)
[2025-02-13 03:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19][root][INFO] - Training Epoch: 1/2, step 5561/7134 completed (loss: 0.04008449986577034, acc: 0.992438554763794)
[2025-02-13 03:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:20][root][INFO] - Training Epoch: 1/2, step 5562/7134 completed (loss: 0.03821833059191704, acc: 0.9948805570602417)
[2025-02-13 03:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:20][root][INFO] - Training Epoch: 1/2, step 5563/7134 completed (loss: 0.04066925123333931, acc: 0.9909909963607788)
[2025-02-13 03:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21][root][INFO] - Training Epoch: 1/2, step 5564/7134 completed (loss: 0.02654225192964077, acc: 0.9944211840629578)
[2025-02-13 03:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21][root][INFO] - Training Epoch: 1/2, step 5565/7134 completed (loss: 0.008289719931781292, acc: 0.996927797794342)
[2025-02-13 03:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21][root][INFO] - Training Epoch: 1/2, step 5566/7134 completed (loss: 0.01647249050438404, acc: 0.9965556859970093)
[2025-02-13 03:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:22][root][INFO] - Training Epoch: 1/2, step 5567/7134 completed (loss: 0.0142845893278718, acc: 0.9942922592163086)
[2025-02-13 03:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:22][root][INFO] - Training Epoch: 1/2, step 5568/7134 completed (loss: 0.023625841364264488, acc: 0.9948453903198242)
[2025-02-13 03:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23][root][INFO] - Training Epoch: 1/2, step 5569/7134 completed (loss: 0.007233599666506052, acc: 0.9977169036865234)
[2025-02-13 03:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23][root][INFO] - Training Epoch: 1/2, step 5570/7134 completed (loss: 0.028631819412112236, acc: 0.9900621175765991)
[2025-02-13 03:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:24][root][INFO] - Training Epoch: 1/2, step 5571/7134 completed (loss: 0.03698047250509262, acc: 0.9910846948623657)
[2025-02-13 03:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:24][root][INFO] - Training Epoch: 1/2, step 5572/7134 completed (loss: 0.04858699068427086, acc: 0.9917582273483276)
[2025-02-13 03:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25][root][INFO] - Training Epoch: 1/2, step 5573/7134 completed (loss: 0.03657685965299606, acc: 0.9927219748497009)
[2025-02-13 03:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25][root][INFO] - Training Epoch: 1/2, step 5574/7134 completed (loss: 0.009357757866382599, acc: 0.9984126687049866)
[2025-02-13 03:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25][root][INFO] - Training Epoch: 1/2, step 5575/7134 completed (loss: 0.03239622712135315, acc: 0.9888357520103455)
[2025-02-13 03:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26][root][INFO] - Training Epoch: 1/2, step 5576/7134 completed (loss: 0.02587832137942314, acc: 0.9899713397026062)
[2025-02-13 03:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26][root][INFO] - Training Epoch: 1/2, step 5577/7134 completed (loss: 0.03999194875359535, acc: 0.9895287752151489)
[2025-02-13 03:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:27][root][INFO] - Training Epoch: 1/2, step 5578/7134 completed (loss: 0.01759956404566765, acc: 0.9956958293914795)
[2025-02-13 03:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:27][root][INFO] - Training Epoch: 1/2, step 5579/7134 completed (loss: 0.029457826167345047, acc: 0.9889349937438965)
[2025-02-13 03:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:28][root][INFO] - Training Epoch: 1/2, step 5580/7134 completed (loss: 0.015764432027935982, acc: 0.9945429563522339)
[2025-02-13 03:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:28][root][INFO] - Training Epoch: 1/2, step 5581/7134 completed (loss: 0.02282772585749626, acc: 0.9920212626457214)
[2025-02-13 03:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29][root][INFO] - Training Epoch: 1/2, step 5582/7134 completed (loss: 0.015283041633665562, acc: 0.9947848916053772)
[2025-02-13 03:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29][root][INFO] - Training Epoch: 1/2, step 5583/7134 completed (loss: 0.026679396629333496, acc: 0.99370276927948)
[2025-02-13 03:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29][root][INFO] - Training Epoch: 1/2, step 5584/7134 completed (loss: 0.01923847571015358, acc: 0.9940828680992126)
[2025-02-13 03:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:30][root][INFO] - Training Epoch: 1/2, step 5585/7134 completed (loss: 0.042659759521484375, acc: 0.9890909194946289)
[2025-02-13 03:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:30][root][INFO] - Training Epoch: 1/2, step 5586/7134 completed (loss: 0.019912956282496452, acc: 0.9946714043617249)
[2025-02-13 03:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31][root][INFO] - Training Epoch: 1/2, step 5587/7134 completed (loss: 0.017219971865415573, acc: 0.9972260594367981)
[2025-02-13 03:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31][root][INFO] - Training Epoch: 1/2, step 5588/7134 completed (loss: 0.014506424777209759, acc: 0.9954614043235779)
[2025-02-13 03:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32][root][INFO] - Training Epoch: 1/2, step 5589/7134 completed (loss: 0.025791531428694725, acc: 0.9968454241752625)
[2025-02-13 03:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32][root][INFO] - Training Epoch: 1/2, step 5590/7134 completed (loss: 0.027476485818624496, acc: 0.9901408553123474)
[2025-02-13 03:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32][root][INFO] - Training Epoch: 1/2, step 5591/7134 completed (loss: 0.01597478985786438, acc: 0.9934554696083069)
[2025-02-13 03:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33][root][INFO] - Training Epoch: 1/2, step 5592/7134 completed (loss: 0.01887354627251625, acc: 0.9958847761154175)
[2025-02-13 03:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33][root][INFO] - Training Epoch: 1/2, step 5593/7134 completed (loss: 0.027663609012961388, acc: 0.9935232996940613)
[2025-02-13 03:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:34][root][INFO] - Training Epoch: 1/2, step 5594/7134 completed (loss: 0.012154076248407364, acc: 0.9973045587539673)
[2025-02-13 03:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:34][root][INFO] - Training Epoch: 1/2, step 5595/7134 completed (loss: 0.02753620035946369, acc: 0.9919571280479431)
[2025-02-13 03:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35][root][INFO] - Training Epoch: 1/2, step 5596/7134 completed (loss: 0.009026693180203438, acc: 0.9986559152603149)
[2025-02-13 03:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35][root][INFO] - Training Epoch: 1/2, step 5597/7134 completed (loss: 0.026561396196484566, acc: 0.9903581142425537)
[2025-02-13 03:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35][root][INFO] - Training Epoch: 1/2, step 5598/7134 completed (loss: 0.024520881474018097, acc: 0.99301677942276)
[2025-02-13 03:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:36][root][INFO] - Training Epoch: 1/2, step 5599/7134 completed (loss: 0.06487560272216797, acc: 0.9877049326896667)
[2025-02-13 03:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:36][root][INFO] - Training Epoch: 1/2, step 5600/7134 completed (loss: 0.012198480777442455, acc: 0.9957020282745361)
[2025-02-13 03:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37][root][INFO] - Training Epoch: 1/2, step 5601/7134 completed (loss: 0.04690060764551163, acc: 0.9878261089324951)
[2025-02-13 03:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37][root][INFO] - Training Epoch: 1/2, step 5602/7134 completed (loss: 0.04412141814827919, acc: 0.9855072498321533)
[2025-02-13 03:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:38][root][INFO] - Training Epoch: 1/2, step 5603/7134 completed (loss: 0.005794673226773739, acc: 1.0)
[2025-02-13 03:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:38][root][INFO] - Training Epoch: 1/2, step 5604/7134 completed (loss: 0.04887378215789795, acc: 0.9839228391647339)
[2025-02-13 03:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:38][root][INFO] - Training Epoch: 1/2, step 5605/7134 completed (loss: 0.027103576809167862, acc: 0.9915110468864441)
[2025-02-13 03:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39][root][INFO] - Training Epoch: 1/2, step 5606/7134 completed (loss: 0.04780491441488266, acc: 0.9871794581413269)
[2025-02-13 03:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39][root][INFO] - Training Epoch: 1/2, step 5607/7134 completed (loss: 0.021020639687776566, acc: 0.9939849376678467)
[2025-02-13 03:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:40][root][INFO] - Training Epoch: 1/2, step 5608/7134 completed (loss: 0.022315993905067444, acc: 0.9901477694511414)
[2025-02-13 03:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:40][root][INFO] - Training Epoch: 1/2, step 5609/7134 completed (loss: 0.03824503347277641, acc: 0.9860869646072388)
[2025-02-13 03:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:41][root][INFO] - Training Epoch: 1/2, step 5610/7134 completed (loss: 0.014315868727862835, acc: 0.9939637780189514)
[2025-02-13 03:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:41][root][INFO] - Training Epoch: 1/2, step 5611/7134 completed (loss: 0.027146417647600174, acc: 0.9889240264892578)
[2025-02-13 03:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:41][root][INFO] - Training Epoch: 1/2, step 5612/7134 completed (loss: 0.03730478510260582, acc: 0.987730085849762)
[2025-02-13 03:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42][root][INFO] - Training Epoch: 1/2, step 5613/7134 completed (loss: 0.02892230451107025, acc: 0.991055428981781)
[2025-02-13 03:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42][root][INFO] - Training Epoch: 1/2, step 5614/7134 completed (loss: 0.04584817215800285, acc: 0.9828571677207947)
[2025-02-13 03:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:43][root][INFO] - Training Epoch: 1/2, step 5615/7134 completed (loss: 0.05079925060272217, acc: 0.982758641242981)
[2025-02-13 03:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:43][root][INFO] - Training Epoch: 1/2, step 5616/7134 completed (loss: 0.015717223286628723, acc: 0.9970282316207886)
[2025-02-13 03:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:44][root][INFO] - Training Epoch: 1/2, step 5617/7134 completed (loss: 0.010812236927449703, acc: 0.9967532753944397)
[2025-02-13 03:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:44][root][INFO] - Training Epoch: 1/2, step 5618/7134 completed (loss: 0.04225209355354309, acc: 0.9853658676147461)
[2025-02-13 03:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:44][root][INFO] - Training Epoch: 1/2, step 5619/7134 completed (loss: 0.048633113503456116, acc: 0.9797160029411316)
[2025-02-13 03:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45][root][INFO] - Training Epoch: 1/2, step 5620/7134 completed (loss: 0.03000945970416069, acc: 0.9925233721733093)
[2025-02-13 03:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45][root][INFO] - Training Epoch: 1/2, step 5621/7134 completed (loss: 0.031111156567931175, acc: 0.9898348450660706)
[2025-02-13 03:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:46][root][INFO] - Training Epoch: 1/2, step 5622/7134 completed (loss: 0.04397055134177208, acc: 0.9918256402015686)
[2025-02-13 03:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:46][root][INFO] - Training Epoch: 1/2, step 5623/7134 completed (loss: 0.031222190707921982, acc: 0.9878048896789551)
[2025-02-13 03:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47][root][INFO] - Training Epoch: 1/2, step 5624/7134 completed (loss: 0.03695687651634216, acc: 0.9885433912277222)
[2025-02-13 03:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47][root][INFO] - Training Epoch: 1/2, step 5625/7134 completed (loss: 0.010753416456282139, acc: 0.9976470470428467)
[2025-02-13 03:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47][root][INFO] - Training Epoch: 1/2, step 5626/7134 completed (loss: 0.013073191978037357, acc: 0.9949238300323486)
[2025-02-13 03:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:48][root][INFO] - Training Epoch: 1/2, step 5627/7134 completed (loss: 0.1049744039773941, acc: 0.9795918464660645)
[2025-02-13 03:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:48][root][INFO] - Training Epoch: 1/2, step 5628/7134 completed (loss: 0.03244258090853691, acc: 0.9890710115432739)
[2025-02-13 03:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:49][root][INFO] - Training Epoch: 1/2, step 5629/7134 completed (loss: 0.04271484166383743, acc: 0.9894551634788513)
[2025-02-13 03:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:49][root][INFO] - Training Epoch: 1/2, step 5630/7134 completed (loss: 0.0590970516204834, acc: 0.987679660320282)
[2025-02-13 03:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50][root][INFO] - Training Epoch: 1/2, step 5631/7134 completed (loss: 0.06162421405315399, acc: 0.9834162592887878)
[2025-02-13 03:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50][root][INFO] - Training Epoch: 1/2, step 5632/7134 completed (loss: 0.02834840677678585, acc: 0.9927404522895813)
[2025-02-13 03:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50][root][INFO] - Training Epoch: 1/2, step 5633/7134 completed (loss: 0.03632199019193649, acc: 0.9869706630706787)
[2025-02-13 03:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:51][root][INFO] - Training Epoch: 1/2, step 5634/7134 completed (loss: 0.051827117800712585, acc: 0.9860383868217468)
[2025-02-13 03:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:51][root][INFO] - Training Epoch: 1/2, step 5635/7134 completed (loss: 0.015042267739772797, acc: 0.9966216087341309)
[2025-02-13 03:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52][root][INFO] - Training Epoch: 1/2, step 5636/7134 completed (loss: 0.028353800997138023, acc: 0.9872204661369324)
[2025-02-13 03:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52][root][INFO] - Training Epoch: 1/2, step 5637/7134 completed (loss: 0.031085608527064323, acc: 0.9915682673454285)
[2025-02-13 03:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:53][root][INFO] - Training Epoch: 1/2, step 5638/7134 completed (loss: 0.04645979404449463, acc: 0.98828125)
[2025-02-13 03:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:53][root][INFO] - Training Epoch: 1/2, step 5639/7134 completed (loss: 0.023599717766046524, acc: 0.9928774833679199)
[2025-02-13 03:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:53][root][INFO] - Training Epoch: 1/2, step 5640/7134 completed (loss: 0.03484809026122093, acc: 0.9902439117431641)
[2025-02-13 03:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:54][root][INFO] - Training Epoch: 1/2, step 5641/7134 completed (loss: 0.044256724417209625, acc: 0.9873737096786499)
[2025-02-13 03:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:54][root][INFO] - Training Epoch: 1/2, step 5642/7134 completed (loss: 0.04708343371748924, acc: 0.9909090995788574)
[2025-02-13 03:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:54][root][INFO] - Training Epoch: 1/2, step 5643/7134 completed (loss: 0.059941187500953674, acc: 0.9845132827758789)
[2025-02-13 03:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55][root][INFO] - Training Epoch: 1/2, step 5644/7134 completed (loss: 0.038648318499326706, acc: 0.9857723712921143)
[2025-02-13 03:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55][root][INFO] - Training Epoch: 1/2, step 5645/7134 completed (loss: 0.015460866503417492, acc: 0.9955817461013794)
[2025-02-13 03:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:56][root][INFO] - Training Epoch: 1/2, step 5646/7134 completed (loss: 0.03531401604413986, acc: 0.987261176109314)
[2025-02-13 03:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:56][root][INFO] - Training Epoch: 1/2, step 5647/7134 completed (loss: 0.044682200998067856, acc: 0.987596869468689)
[2025-02-13 03:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:57][root][INFO] - Training Epoch: 1/2, step 5648/7134 completed (loss: 0.09715676307678223, acc: 0.9776119589805603)
[2025-02-13 03:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:57][root][INFO] - Training Epoch: 1/2, step 5649/7134 completed (loss: 0.047595854848623276, acc: 0.9908257126808167)
[2025-02-13 03:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:57][root][INFO] - Training Epoch: 1/2, step 5650/7134 completed (loss: 0.025287995114922523, acc: 0.989708423614502)
[2025-02-13 03:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58][root][INFO] - Training Epoch: 1/2, step 5651/7134 completed (loss: 0.01494938787072897, acc: 0.9940828680992126)
[2025-02-13 03:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58][root][INFO] - Training Epoch: 1/2, step 5652/7134 completed (loss: 0.026538429781794548, acc: 0.9887820482254028)
[2025-02-13 03:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:59][root][INFO] - Training Epoch: 1/2, step 5653/7134 completed (loss: 0.08361045271158218, acc: 0.9791666865348816)
[2025-02-13 03:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:59][root][INFO] - Training Epoch: 1/2, step 5654/7134 completed (loss: 0.017216555774211884, acc: 0.9939939975738525)
[2025-02-13 03:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00][root][INFO] - Training Epoch: 1/2, step 5655/7134 completed (loss: 0.0300106443464756, acc: 0.988095223903656)
[2025-02-13 03:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00][root][INFO] - Training Epoch: 1/2, step 5656/7134 completed (loss: 0.034866418689489365, acc: 0.9885714054107666)
[2025-02-13 03:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00][root][INFO] - Training Epoch: 1/2, step 5657/7134 completed (loss: 0.04146462306380272, acc: 0.9926793575286865)
[2025-02-13 03:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:01][root][INFO] - Training Epoch: 1/2, step 5658/7134 completed (loss: 0.046201031655073166, acc: 0.9820442199707031)
[2025-02-13 03:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:01][root][INFO] - Training Epoch: 1/2, step 5659/7134 completed (loss: 0.04545694589614868, acc: 0.9900709390640259)
[2025-02-13 03:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:02][root][INFO] - Training Epoch: 1/2, step 5660/7134 completed (loss: 0.018586009740829468, acc: 0.9933884143829346)
[2025-02-13 03:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:02][root][INFO] - Training Epoch: 1/2, step 5661/7134 completed (loss: 0.008688651025295258, acc: 0.9963503479957581)
[2025-02-13 03:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03][root][INFO] - Training Epoch: 1/2, step 5662/7134 completed (loss: 0.021692493930459023, acc: 0.9967948794364929)
[2025-02-13 03:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03][root][INFO] - Training Epoch: 1/2, step 5663/7134 completed (loss: 0.012295178137719631, acc: 0.9953774809837341)
[2025-02-13 03:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03][root][INFO] - Training Epoch: 1/2, step 5664/7134 completed (loss: 0.06682627648115158, acc: 0.9851411581039429)
[2025-02-13 03:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:04][root][INFO] - Training Epoch: 1/2, step 5665/7134 completed (loss: 0.022693650797009468, acc: 0.9928673505783081)
[2025-02-13 03:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:04][root][INFO] - Training Epoch: 1/2, step 5666/7134 completed (loss: 0.014753682538866997, acc: 0.9944827556610107)
[2025-02-13 03:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05][root][INFO] - Training Epoch: 1/2, step 5667/7134 completed (loss: 0.022646835073828697, acc: 0.996363639831543)
[2025-02-13 03:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05][root][INFO] - Training Epoch: 1/2, step 5668/7134 completed (loss: 0.041653409600257874, acc: 0.9900332093238831)
[2025-02-13 03:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06][root][INFO] - Training Epoch: 1/2, step 5669/7134 completed (loss: 0.023208018392324448, acc: 0.9941349029541016)
[2025-02-13 03:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06][root][INFO] - Training Epoch: 1/2, step 5670/7134 completed (loss: 0.021210379898548126, acc: 0.9933333396911621)
[2025-02-13 03:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06][root][INFO] - Training Epoch: 1/2, step 5671/7134 completed (loss: 0.01713871769607067, acc: 0.9950799345970154)
[2025-02-13 03:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07][root][INFO] - Training Epoch: 1/2, step 5672/7134 completed (loss: 0.01493793074041605, acc: 0.9936908483505249)
[2025-02-13 03:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07][root][INFO] - Training Epoch: 1/2, step 5673/7134 completed (loss: 0.016111502423882484, acc: 0.9970458149909973)
[2025-02-13 03:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:08][root][INFO] - Training Epoch: 1/2, step 5674/7134 completed (loss: 0.01693424955010414, acc: 0.9923547506332397)
[2025-02-13 03:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:08][root][INFO] - Training Epoch: 1/2, step 5675/7134 completed (loss: 0.04588394612073898, acc: 0.9900497794151306)
[2025-02-13 03:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09][root][INFO] - Training Epoch: 1/2, step 5676/7134 completed (loss: 0.08283445984125137, acc: 0.9807692170143127)
[2025-02-13 03:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09][root][INFO] - Training Epoch: 1/2, step 5677/7134 completed (loss: 0.045919325202703476, acc: 0.9886363744735718)
[2025-02-13 03:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09][root][INFO] - Training Epoch: 1/2, step 5678/7134 completed (loss: 0.027800293639302254, acc: 0.9911764860153198)
[2025-02-13 03:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:10][root][INFO] - Training Epoch: 1/2, step 5679/7134 completed (loss: 0.07643073052167892, acc: 0.9765739440917969)
[2025-02-13 03:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:10][root][INFO] - Training Epoch: 1/2, step 5680/7134 completed (loss: 0.0798930674791336, acc: 0.9701937437057495)
[2025-02-13 03:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:11][root][INFO] - Training Epoch: 1/2, step 5681/7134 completed (loss: 0.052246060222387314, acc: 0.9849170446395874)
[2025-02-13 03:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:11][root][INFO] - Training Epoch: 1/2, step 5682/7134 completed (loss: 0.033404313027858734, acc: 0.9870129823684692)
[2025-02-13 03:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12][root][INFO] - Training Epoch: 1/2, step 5683/7134 completed (loss: 0.058544352650642395, acc: 0.9765739440917969)
[2025-02-13 03:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12][root][INFO] - Training Epoch: 1/2, step 5684/7134 completed (loss: 0.0839722752571106, acc: 0.9865092635154724)
[2025-02-13 03:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13][root][INFO] - Training Epoch: 1/2, step 5685/7134 completed (loss: 0.07322096824645996, acc: 0.9834558963775635)
[2025-02-13 03:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13][root][INFO] - Training Epoch: 1/2, step 5686/7134 completed (loss: 0.05969062075018883, acc: 0.9864636063575745)
[2025-02-13 03:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13][root][INFO] - Training Epoch: 1/2, step 5687/7134 completed (loss: 0.05309504643082619, acc: 0.9871794581413269)
[2025-02-13 03:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14][root][INFO] - Training Epoch: 1/2, step 5688/7134 completed (loss: 0.07566577196121216, acc: 0.9786381721496582)
[2025-02-13 03:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14][root][INFO] - Training Epoch: 1/2, step 5689/7134 completed (loss: 0.08096645027399063, acc: 0.9847792983055115)
[2025-02-13 03:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:15][root][INFO] - Training Epoch: 1/2, step 5690/7134 completed (loss: 0.037141352891922, acc: 0.9905660152435303)
[2025-02-13 03:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:15][root][INFO] - Training Epoch: 1/2, step 5691/7134 completed (loss: 0.05554502457380295, acc: 0.9826086759567261)
[2025-02-13 03:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16][root][INFO] - Training Epoch: 1/2, step 5692/7134 completed (loss: 0.06352374702692032, acc: 0.9825737476348877)
[2025-02-13 03:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16][root][INFO] - Training Epoch: 1/2, step 5693/7134 completed (loss: 0.019359761849045753, acc: 0.9940029978752136)
[2025-02-13 03:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16][root][INFO] - Training Epoch: 1/2, step 5694/7134 completed (loss: 0.03864096850156784, acc: 0.9927641153335571)
[2025-02-13 03:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:17][root][INFO] - Training Epoch: 1/2, step 5695/7134 completed (loss: 0.0246379766613245, acc: 0.9899713397026062)
[2025-02-13 03:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:17][root][INFO] - Training Epoch: 1/2, step 5696/7134 completed (loss: 0.03178452327847481, acc: 0.9929412007331848)
[2025-02-13 03:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18][root][INFO] - Training Epoch: 1/2, step 5697/7134 completed (loss: 0.04003326594829559, acc: 0.9901269674301147)
[2025-02-13 03:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18][root][INFO] - Training Epoch: 1/2, step 5698/7134 completed (loss: 0.03412861377000809, acc: 0.9931972622871399)
[2025-02-13 03:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:19][root][INFO] - Training Epoch: 1/2, step 5699/7134 completed (loss: 0.041150033473968506, acc: 0.9847133755683899)
[2025-02-13 03:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:19][root][INFO] - Training Epoch: 1/2, step 5700/7134 completed (loss: 0.017640268430113792, acc: 0.9957447052001953)
[2025-02-13 03:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20][root][INFO] - Training Epoch: 1/2, step 5701/7134 completed (loss: 0.03330232575535774, acc: 0.9974968433380127)
[2025-02-13 03:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20][root][INFO] - Training Epoch: 1/2, step 5702/7134 completed (loss: 0.03676899895071983, acc: 0.9932340979576111)
[2025-02-13 03:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21][root][INFO] - Training Epoch: 1/2, step 5703/7134 completed (loss: 0.017270579934120178, acc: 0.997187077999115)
[2025-02-13 03:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21][root][INFO] - Training Epoch: 1/2, step 5704/7134 completed (loss: 0.05084393918514252, acc: 0.9857346415519714)
[2025-02-13 03:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21][root][INFO] - Training Epoch: 1/2, step 5705/7134 completed (loss: 0.014136042445898056, acc: 0.9964285492897034)
[2025-02-13 03:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:22][root][INFO] - Training Epoch: 1/2, step 5706/7134 completed (loss: 0.01717604696750641, acc: 0.9955257177352905)
[2025-02-13 03:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:22][root][INFO] - Training Epoch: 1/2, step 5707/7134 completed (loss: 0.036180052906274796, acc: 0.9876760840415955)
[2025-02-13 03:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23][root][INFO] - Training Epoch: 1/2, step 5708/7134 completed (loss: 0.017012275755405426, acc: 0.9970370531082153)
[2025-02-13 03:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23][root][INFO] - Training Epoch: 1/2, step 5709/7134 completed (loss: 0.05421627685427666, acc: 0.9876881241798401)
[2025-02-13 03:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:24][root][INFO] - Training Epoch: 1/2, step 5710/7134 completed (loss: 0.024447210133075714, acc: 0.991094172000885)
[2025-02-13 03:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:24][root][INFO] - Training Epoch: 1/2, step 5711/7134 completed (loss: 0.04987958446145058, acc: 0.9869109988212585)
[2025-02-13 03:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:24][root][INFO] - Training Epoch: 1/2, step 5712/7134 completed (loss: 0.05091942101716995, acc: 0.984000027179718)
[2025-02-13 03:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25][root][INFO] - Training Epoch: 1/2, step 5713/7134 completed (loss: 0.014562155120074749, acc: 0.9961880445480347)
[2025-02-13 03:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25][root][INFO] - Training Epoch: 1/2, step 5714/7134 completed (loss: 0.02781515009701252, acc: 0.993773341178894)
[2025-02-13 03:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:26][root][INFO] - Training Epoch: 1/2, step 5715/7134 completed (loss: 0.0708228349685669, acc: 0.9869646430015564)
[2025-02-13 03:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:26][root][INFO] - Training Epoch: 1/2, step 5716/7134 completed (loss: 0.12366551905870438, acc: 0.980461835861206)
[2025-02-13 03:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27][root][INFO] - Training Epoch: 1/2, step 5717/7134 completed (loss: 0.09175963699817657, acc: 0.9813374876976013)
[2025-02-13 03:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27][root][INFO] - Training Epoch: 1/2, step 5718/7134 completed (loss: 0.06578375399112701, acc: 0.9858044385910034)
[2025-02-13 03:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27][root][INFO] - Training Epoch: 1/2, step 5719/7134 completed (loss: 0.042855922132730484, acc: 0.9880059957504272)
[2025-02-13 03:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:28][root][INFO] - Training Epoch: 1/2, step 5720/7134 completed (loss: 0.03146010637283325, acc: 0.9879102110862732)
[2025-02-13 03:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:28][root][INFO] - Training Epoch: 1/2, step 5721/7134 completed (loss: 0.039187557995319366, acc: 0.9907894730567932)
[2025-02-13 03:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29][root][INFO] - Training Epoch: 1/2, step 5722/7134 completed (loss: 0.04943021014332771, acc: 0.9911308288574219)
[2025-02-13 03:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29][root][INFO] - Training Epoch: 1/2, step 5723/7134 completed (loss: 0.05063469707965851, acc: 0.9863842725753784)
[2025-02-13 03:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:30][root][INFO] - Training Epoch: 1/2, step 5724/7134 completed (loss: 0.02589564211666584, acc: 0.9904000163078308)
[2025-02-13 03:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:30][root][INFO] - Training Epoch: 1/2, step 5725/7134 completed (loss: 0.022740671411156654, acc: 0.9957143068313599)
[2025-02-13 03:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:30][root][INFO] - Training Epoch: 1/2, step 5726/7134 completed (loss: 0.06174749135971069, acc: 0.9877675771713257)
[2025-02-13 03:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31][root][INFO] - Training Epoch: 1/2, step 5727/7134 completed (loss: 0.011969953775405884, acc: 0.9986631274223328)
[2025-02-13 03:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31][root][INFO] - Training Epoch: 1/2, step 5728/7134 completed (loss: 0.011557151563465595, acc: 1.0)
[2025-02-13 03:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:32][root][INFO] - Training Epoch: 1/2, step 5729/7134 completed (loss: 0.018154093995690346, acc: 0.9950000047683716)
[2025-02-13 03:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:32][root][INFO] - Training Epoch: 1/2, step 5730/7134 completed (loss: 0.0715537741780281, acc: 0.9838383793830872)
[2025-02-13 03:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:32][root][INFO] - Training Epoch: 1/2, step 5731/7134 completed (loss: 0.0353206992149353, acc: 0.9883720874786377)
[2025-02-13 03:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:33][root][INFO] - Training Epoch: 1/2, step 5732/7134 completed (loss: 0.14258445799350739, acc: 0.9650485515594482)
[2025-02-13 03:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:33][root][INFO] - Training Epoch: 1/2, step 5733/7134 completed (loss: 0.03660907596349716, acc: 0.9908925294876099)
[2025-02-13 03:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34][root][INFO] - Training Epoch: 1/2, step 5734/7134 completed (loss: 0.009539060294628143, acc: 0.9983333349227905)
[2025-02-13 03:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34][root][INFO] - Training Epoch: 1/2, step 5735/7134 completed (loss: 0.06431303918361664, acc: 0.9784946441650391)
[2025-02-13 03:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:35][root][INFO] - Training Epoch: 1/2, step 5736/7134 completed (loss: 0.07410634309053421, acc: 0.9804511070251465)
[2025-02-13 03:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:35][root][INFO] - Training Epoch: 1/2, step 5737/7134 completed (loss: 0.05686816945672035, acc: 0.9788519740104675)
[2025-02-13 03:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:35][root][INFO] - Training Epoch: 1/2, step 5738/7134 completed (loss: 0.04439070448279381, acc: 0.984280526638031)
[2025-02-13 03:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:36][root][INFO] - Training Epoch: 1/2, step 5739/7134 completed (loss: 0.060646891593933105, acc: 0.9831578731536865)
[2025-02-13 03:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:36][root][INFO] - Training Epoch: 1/2, step 5740/7134 completed (loss: 0.034423477947711945, acc: 0.9884910583496094)
[2025-02-13 03:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37][root][INFO] - Training Epoch: 1/2, step 5741/7134 completed (loss: 0.053901154547929764, acc: 0.9845956563949585)
[2025-02-13 03:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37][root][INFO] - Training Epoch: 1/2, step 5742/7134 completed (loss: 0.037440791726112366, acc: 0.9894875288009644)
[2025-02-13 03:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:38][root][INFO] - Training Epoch: 1/2, step 5743/7134 completed (loss: 0.017899034544825554, acc: 0.9926470518112183)
[2025-02-13 03:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:38][root][INFO] - Training Epoch: 1/2, step 5744/7134 completed (loss: 0.007307764608412981, acc: 1.0)
[2025-02-13 03:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:38][root][INFO] - Training Epoch: 1/2, step 5745/7134 completed (loss: 0.04075991362333298, acc: 0.984455943107605)
[2025-02-13 03:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39][root][INFO] - Training Epoch: 1/2, step 5746/7134 completed (loss: 0.06906072050333023, acc: 0.9882352948188782)
[2025-02-13 03:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39][root][INFO] - Training Epoch: 1/2, step 5747/7134 completed (loss: 0.02621992491185665, acc: 0.990304708480835)
[2025-02-13 03:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:40][root][INFO] - Training Epoch: 1/2, step 5748/7134 completed (loss: 0.01929115504026413, acc: 0.9945155382156372)
[2025-02-13 03:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:40][root][INFO] - Training Epoch: 1/2, step 5749/7134 completed (loss: 0.029793057590723038, acc: 0.9934554696083069)
[2025-02-13 03:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41][root][INFO] - Training Epoch: 1/2, step 5750/7134 completed (loss: 0.04644709452986717, acc: 0.9851484894752502)
[2025-02-13 03:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41][root][INFO] - Training Epoch: 1/2, step 5751/7134 completed (loss: 0.03452843427658081, acc: 0.9897304177284241)
[2025-02-13 03:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42][root][INFO] - Training Epoch: 1/2, step 5752/7134 completed (loss: 0.10601009428501129, acc: 0.9791183471679688)
[2025-02-13 03:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42][root][INFO] - Training Epoch: 1/2, step 5753/7134 completed (loss: 0.05172652006149292, acc: 0.9859374761581421)
[2025-02-13 03:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42][root][INFO] - Training Epoch: 1/2, step 5754/7134 completed (loss: 0.05729551240801811, acc: 0.987261176109314)
[2025-02-13 03:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:43][root][INFO] - Training Epoch: 1/2, step 5755/7134 completed (loss: 0.04121551662683487, acc: 0.9868228435516357)
[2025-02-13 03:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:43][root][INFO] - Training Epoch: 1/2, step 5756/7134 completed (loss: 0.050078097730875015, acc: 0.978723406791687)
[2025-02-13 03:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44][root][INFO] - Training Epoch: 1/2, step 5757/7134 completed (loss: 0.030316920951008797, acc: 0.9872286319732666)
[2025-02-13 03:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44][root][INFO] - Training Epoch: 1/2, step 5758/7134 completed (loss: 0.05400776118040085, acc: 0.9808823466300964)
[2025-02-13 03:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:45][root][INFO] - Training Epoch: 1/2, step 5759/7134 completed (loss: 0.037214260548353195, acc: 0.9838274717330933)
[2025-02-13 03:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:45][root][INFO] - Training Epoch: 1/2, step 5760/7134 completed (loss: 0.029891151934862137, acc: 0.9908397197723389)
[2025-02-13 03:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46][root][INFO] - Training Epoch: 1/2, step 5761/7134 completed (loss: 0.030324002727866173, acc: 0.9890109896659851)
[2025-02-13 03:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46][root][INFO] - Training Epoch: 1/2, step 5762/7134 completed (loss: 0.057495564222335815, acc: 0.9835766553878784)
[2025-02-13 03:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46][root][INFO] - Training Epoch: 1/2, step 5763/7134 completed (loss: 0.032884955406188965, acc: 0.9924717545509338)
[2025-02-13 03:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:47][root][INFO] - Training Epoch: 1/2, step 5764/7134 completed (loss: 0.018913105130195618, acc: 0.9948717951774597)
[2025-02-13 03:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:47][root][INFO] - Training Epoch: 1/2, step 5765/7134 completed (loss: 0.02018270455300808, acc: 0.9929971694946289)
[2025-02-13 03:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48][root][INFO] - Training Epoch: 1/2, step 5766/7134 completed (loss: 0.025421764701604843, acc: 0.994020938873291)
[2025-02-13 03:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48][root][INFO] - Training Epoch: 1/2, step 5767/7134 completed (loss: 0.014791752211749554, acc: 0.9944367408752441)
[2025-02-13 03:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:49][root][INFO] - Training Epoch: 1/2, step 5768/7134 completed (loss: 0.012570721097290516, acc: 0.9955089688301086)
[2025-02-13 03:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:49][root][INFO] - Training Epoch: 1/2, step 5769/7134 completed (loss: 0.00901874154806137, acc: 0.9967266917228699)
[2025-02-13 03:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:49][root][INFO] - Training Epoch: 1/2, step 5770/7134 completed (loss: 0.0325709693133831, acc: 0.9926560521125793)
[2025-02-13 03:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:50][root][INFO] - Training Epoch: 1/2, step 5771/7134 completed (loss: 0.048379238694906235, acc: 0.99048912525177)
[2025-02-13 03:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:50][root][INFO] - Training Epoch: 1/2, step 5772/7134 completed (loss: 0.02953381836414337, acc: 0.9930434823036194)
[2025-02-13 03:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51][root][INFO] - Training Epoch: 1/2, step 5773/7134 completed (loss: 0.011395414359867573, acc: 0.9983277320861816)
[2025-02-13 03:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51][root][INFO] - Training Epoch: 1/2, step 5774/7134 completed (loss: 0.011734655126929283, acc: 0.997032642364502)
[2025-02-13 03:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:52][root][INFO] - Training Epoch: 1/2, step 5775/7134 completed (loss: 0.01440323144197464, acc: 0.9950576424598694)
[2025-02-13 03:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:52][root][INFO] - Training Epoch: 1/2, step 5776/7134 completed (loss: 0.025685179978609085, acc: 0.9915730357170105)
[2025-02-13 03:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53][root][INFO] - Training Epoch: 1/2, step 5777/7134 completed (loss: 0.03930681198835373, acc: 0.9938119053840637)
[2025-02-13 03:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53][root][INFO] - Training Epoch: 1/2, step 5778/7134 completed (loss: 0.07061810046434402, acc: 0.9791666865348816)
[2025-02-13 03:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53][root][INFO] - Training Epoch: 1/2, step 5779/7134 completed (loss: 0.03946985304355621, acc: 0.9889958500862122)
[2025-02-13 03:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:54][root][INFO] - Training Epoch: 1/2, step 5780/7134 completed (loss: 0.03279992938041687, acc: 0.9892601370811462)
[2025-02-13 03:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:54][root][INFO] - Training Epoch: 1/2, step 5781/7134 completed (loss: 0.0216356348246336, acc: 0.9930939078330994)
[2025-02-13 03:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:55][root][INFO] - Training Epoch: 1/2, step 5782/7134 completed (loss: 0.017437327653169632, acc: 0.9948186278343201)
[2025-02-13 03:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:55][root][INFO] - Training Epoch: 1/2, step 5783/7134 completed (loss: 0.02091040462255478, acc: 0.9946879148483276)
[2025-02-13 03:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56][root][INFO] - Training Epoch: 1/2, step 5784/7134 completed (loss: 0.026940906420350075, acc: 0.9932705163955688)
[2025-02-13 03:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56][root][INFO] - Training Epoch: 1/2, step 5785/7134 completed (loss: 0.03638286888599396, acc: 0.9878706336021423)
[2025-02-13 03:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56][root][INFO] - Training Epoch: 1/2, step 5786/7134 completed (loss: 0.03153730928897858, acc: 0.9897540807723999)
[2025-02-13 03:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:57][root][INFO] - Training Epoch: 1/2, step 5787/7134 completed (loss: 0.024003176018595695, acc: 0.9937810897827148)
[2025-02-13 03:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:57][root][INFO] - Training Epoch: 1/2, step 5788/7134 completed (loss: 0.015304644592106342, acc: 0.9953846335411072)
[2025-02-13 03:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58][root][INFO] - Training Epoch: 1/2, step 5789/7134 completed (loss: 0.03606221824884415, acc: 0.9900744557380676)
[2025-02-13 03:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58][root][INFO] - Training Epoch: 1/2, step 5790/7134 completed (loss: 0.03591828793287277, acc: 0.9903846383094788)
[2025-02-13 03:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59][root][INFO] - Training Epoch: 1/2, step 5791/7134 completed (loss: 0.04346919432282448, acc: 0.9845361113548279)
[2025-02-13 03:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59][root][INFO] - Training Epoch: 1/2, step 5792/7134 completed (loss: 0.06664635241031647, acc: 0.9830220937728882)
[2025-02-13 03:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59][root][INFO] - Training Epoch: 1/2, step 5793/7134 completed (loss: 0.05548401176929474, acc: 0.9842767119407654)
[2025-02-13 03:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:00][root][INFO] - Training Epoch: 1/2, step 5794/7134 completed (loss: 0.029147740453481674, acc: 0.989393949508667)
[2025-02-13 03:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:00][root][INFO] - Training Epoch: 1/2, step 5795/7134 completed (loss: 0.10320008546113968, acc: 0.9768907427787781)
[2025-02-13 03:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:01][root][INFO] - Training Epoch: 1/2, step 5796/7134 completed (loss: 0.04512502998113632, acc: 0.9881556630134583)
[2025-02-13 03:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:01][root][INFO] - Training Epoch: 1/2, step 5797/7134 completed (loss: 0.030299510806798935, acc: 0.9872000217437744)
[2025-02-13 03:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02][root][INFO] - Training Epoch: 1/2, step 5798/7134 completed (loss: 0.16325245797634125, acc: 0.9634782671928406)
[2025-02-13 03:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02][root][INFO] - Training Epoch: 1/2, step 5799/7134 completed (loss: 0.06876673549413681, acc: 0.9802761077880859)
[2025-02-13 03:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02][root][INFO] - Training Epoch: 1/2, step 5800/7134 completed (loss: 0.05829409137368202, acc: 0.9761029481887817)
[2025-02-13 03:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:03][root][INFO] - Training Epoch: 1/2, step 5801/7134 completed (loss: 0.048548534512519836, acc: 0.9832317233085632)
[2025-02-13 03:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:03][root][INFO] - Training Epoch: 1/2, step 5802/7134 completed (loss: 0.035189393907785416, acc: 0.9889094233512878)
[2025-02-13 03:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:04][root][INFO] - Training Epoch: 1/2, step 5803/7134 completed (loss: 0.02076692320406437, acc: 0.9974489808082581)
[2025-02-13 03:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:04][root][INFO] - Training Epoch: 1/2, step 5804/7134 completed (loss: 0.10770253092050552, acc: 0.9729363918304443)
[2025-02-13 03:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:05][root][INFO] - Training Epoch: 1/2, step 5805/7134 completed (loss: 0.049058981239795685, acc: 0.9884892106056213)
[2025-02-13 03:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:05][root][INFO] - Training Epoch: 1/2, step 5806/7134 completed (loss: 0.040016818791627884, acc: 0.9877049326896667)
[2025-02-13 03:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:05][root][INFO] - Training Epoch: 1/2, step 5807/7134 completed (loss: 0.04913751780986786, acc: 0.9894099831581116)
[2025-02-13 03:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06][root][INFO] - Training Epoch: 1/2, step 5808/7134 completed (loss: 0.032241784036159515, acc: 0.9901546835899353)
[2025-02-13 03:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06][root][INFO] - Training Epoch: 1/2, step 5809/7134 completed (loss: 0.03150911629199982, acc: 0.9934210777282715)
[2025-02-13 03:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:07][root][INFO] - Training Epoch: 1/2, step 5810/7134 completed (loss: 0.03418644517660141, acc: 0.9878296256065369)
[2025-02-13 03:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:07][root][INFO] - Training Epoch: 1/2, step 5811/7134 completed (loss: 0.03129866346716881, acc: 0.9903614521026611)
[2025-02-13 03:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:08][root][INFO] - Training Epoch: 1/2, step 5812/7134 completed (loss: 0.01643119379878044, acc: 0.9966996908187866)
[2025-02-13 03:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:08][root][INFO] - Training Epoch: 1/2, step 5813/7134 completed (loss: 0.029672274366021156, acc: 0.99245285987854)
[2025-02-13 03:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:08][root][INFO] - Training Epoch: 1/2, step 5814/7134 completed (loss: 0.016722209751605988, acc: 0.9949238300323486)
[2025-02-13 03:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:09][root][INFO] - Training Epoch: 1/2, step 5815/7134 completed (loss: 0.029404670000076294, acc: 0.9905837774276733)
[2025-02-13 03:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:09][root][INFO] - Training Epoch: 1/2, step 5816/7134 completed (loss: 0.006812837440520525, acc: 0.9975062608718872)
[2025-02-13 03:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10][root][INFO] - Training Epoch: 1/2, step 5817/7134 completed (loss: 0.0628395676612854, acc: 0.98740553855896)
[2025-02-13 03:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10][root][INFO] - Training Epoch: 1/2, step 5818/7134 completed (loss: 0.028498761355876923, acc: 0.9881481528282166)
[2025-02-13 03:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:11][root][INFO] - Training Epoch: 1/2, step 5819/7134 completed (loss: 0.028606010600924492, acc: 0.9948275685310364)
[2025-02-13 03:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:11][root][INFO] - Training Epoch: 1/2, step 5820/7134 completed (loss: 0.04943720996379852, acc: 0.9834710955619812)
[2025-02-13 03:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:11][root][INFO] - Training Epoch: 1/2, step 5821/7134 completed (loss: 0.01341590378433466, acc: 0.9953774809837341)
[2025-02-13 03:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12][root][INFO] - Training Epoch: 1/2, step 5822/7134 completed (loss: 0.024975137785077095, acc: 0.9935483932495117)
[2025-02-13 03:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12][root][INFO] - Training Epoch: 1/2, step 5823/7134 completed (loss: 0.024151870980858803, acc: 0.9922239780426025)
[2025-02-13 03:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:13][root][INFO] - Training Epoch: 1/2, step 5824/7134 completed (loss: 0.016374101862311363, acc: 0.993779182434082)
[2025-02-13 03:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:13][root][INFO] - Training Epoch: 1/2, step 5825/7134 completed (loss: 0.036313530057668686, acc: 0.991349458694458)
[2025-02-13 03:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:13][root][INFO] - Training Epoch: 1/2, step 5826/7134 completed (loss: 0.026360010728240013, acc: 0.9964538812637329)
[2025-02-13 03:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:14][root][INFO] - Training Epoch: 1/2, step 5827/7134 completed (loss: 0.03047698736190796, acc: 0.9884615540504456)
[2025-02-13 03:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:14][root][INFO] - Training Epoch: 1/2, step 5828/7134 completed (loss: 0.042590271681547165, acc: 0.9881129264831543)
[2025-02-13 03:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15][root][INFO] - Training Epoch: 1/2, step 5829/7134 completed (loss: 0.03484971821308136, acc: 0.9934123754501343)
[2025-02-13 03:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15][root][INFO] - Training Epoch: 1/2, step 5830/7134 completed (loss: 0.04481791704893112, acc: 0.9848771095275879)
[2025-02-13 03:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15][root][INFO] - Training Epoch: 1/2, step 5831/7134 completed (loss: 0.015187308192253113, acc: 0.9976744055747986)
[2025-02-13 03:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:16][root][INFO] - Training Epoch: 1/2, step 5832/7134 completed (loss: 0.044942352920770645, acc: 0.9864864945411682)
[2025-02-13 03:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:16][root][INFO] - Training Epoch: 1/2, step 5833/7134 completed (loss: 0.023073431104421616, acc: 0.9946236610412598)
[2025-02-13 03:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17][root][INFO] - Training Epoch: 1/2, step 5834/7134 completed (loss: 0.03021441213786602, acc: 0.9958333373069763)
[2025-02-13 03:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17][root][INFO] - Training Epoch: 1/2, step 5835/7134 completed (loss: 0.016585061326622963, acc: 0.9956647157669067)
[2025-02-13 03:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:18][root][INFO] - Training Epoch: 1/2, step 5836/7134 completed (loss: 0.03238799795508385, acc: 0.9927954077720642)
[2025-02-13 03:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:18][root][INFO] - Training Epoch: 1/2, step 5837/7134 completed (loss: 0.06053420156240463, acc: 0.9898648858070374)
[2025-02-13 03:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:18][root][INFO] - Training Epoch: 1/2, step 5838/7134 completed (loss: 0.04329237714409828, acc: 0.9884678721427917)
[2025-02-13 03:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:19][root][INFO] - Training Epoch: 1/2, step 5839/7134 completed (loss: 0.034172847867012024, acc: 0.987500011920929)
[2025-02-13 03:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:19][root][INFO] - Training Epoch: 1/2, step 5840/7134 completed (loss: 0.015720980241894722, acc: 0.9936407208442688)
[2025-02-13 03:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:20][root][INFO] - Training Epoch: 1/2, step 5841/7134 completed (loss: 0.029433097690343857, acc: 0.9936203956604004)
[2025-02-13 03:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:20][root][INFO] - Training Epoch: 1/2, step 5842/7134 completed (loss: 0.036334048956632614, acc: 0.9909420013427734)
[2025-02-13 03:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21][root][INFO] - Training Epoch: 1/2, step 5843/7134 completed (loss: 0.03902854397892952, acc: 0.9892904758453369)
[2025-02-13 03:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21][root][INFO] - Training Epoch: 1/2, step 5844/7134 completed (loss: 0.019680878147482872, acc: 0.9971550703048706)
[2025-02-13 03:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21][root][INFO] - Training Epoch: 1/2, step 5845/7134 completed (loss: 0.03320736810564995, acc: 0.9886040091514587)
[2025-02-13 03:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:22][root][INFO] - Training Epoch: 1/2, step 5846/7134 completed (loss: 0.05359484627842903, acc: 0.9876881241798401)
[2025-02-13 03:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:22][root][INFO] - Training Epoch: 1/2, step 5847/7134 completed (loss: 0.014394218102097511, acc: 0.9986263513565063)
[2025-02-13 03:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23][root][INFO] - Training Epoch: 1/2, step 5848/7134 completed (loss: 0.04527333006262779, acc: 0.9895833134651184)
[2025-02-13 03:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23][root][INFO] - Training Epoch: 1/2, step 5849/7134 completed (loss: 0.06783244013786316, acc: 0.9793510437011719)
[2025-02-13 03:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:24][root][INFO] - Training Epoch: 1/2, step 5850/7134 completed (loss: 0.01531200110912323, acc: 0.9938931465148926)
[2025-02-13 03:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:24][root][INFO] - Training Epoch: 1/2, step 5851/7134 completed (loss: 0.037224072962999344, acc: 0.9884868264198303)
[2025-02-13 03:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:24][root][INFO] - Training Epoch: 1/2, step 5852/7134 completed (loss: 0.02169402688741684, acc: 0.9912408590316772)
[2025-02-13 03:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:25][root][INFO] - Training Epoch: 1/2, step 5853/7134 completed (loss: 0.04863886162638664, acc: 0.9891745448112488)
[2025-02-13 03:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:25][root][INFO] - Training Epoch: 1/2, step 5854/7134 completed (loss: 0.01972559466958046, acc: 0.9946902394294739)
[2025-02-13 03:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:26][root][INFO] - Training Epoch: 1/2, step 5855/7134 completed (loss: 0.04152931645512581, acc: 0.9896373152732849)
[2025-02-13 03:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:26][root][INFO] - Training Epoch: 1/2, step 5856/7134 completed (loss: 0.08024629950523376, acc: 0.9833837151527405)
[2025-02-13 03:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:26][root][INFO] - Training Epoch: 1/2, step 5857/7134 completed (loss: 0.03408830240368843, acc: 0.9889415502548218)
[2025-02-13 03:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:27][root][INFO] - Training Epoch: 1/2, step 5858/7134 completed (loss: 0.018413932994008064, acc: 0.9944547414779663)
[2025-02-13 03:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:27][root][INFO] - Training Epoch: 1/2, step 5859/7134 completed (loss: 0.03617479279637337, acc: 0.990234375)
[2025-02-13 03:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28][root][INFO] - Training Epoch: 1/2, step 5860/7134 completed (loss: 0.024350350722670555, acc: 0.9889807105064392)
[2025-02-13 03:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28][root][INFO] - Training Epoch: 1/2, step 5861/7134 completed (loss: 0.03277922794222832, acc: 0.9878296256065369)
[2025-02-13 03:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:29][root][INFO] - Training Epoch: 1/2, step 5862/7134 completed (loss: 0.024656280875205994, acc: 0.991416335105896)
[2025-02-13 03:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:29][root][INFO] - Training Epoch: 1/2, step 5863/7134 completed (loss: 0.04006069898605347, acc: 0.9898989796638489)
[2025-02-13 03:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:29][root][INFO] - Training Epoch: 1/2, step 5864/7134 completed (loss: 0.048909254372119904, acc: 0.9865269660949707)
[2025-02-13 03:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:30][root][INFO] - Training Epoch: 1/2, step 5865/7134 completed (loss: 0.045435551553964615, acc: 0.9873015880584717)
[2025-02-13 03:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:30][root][INFO] - Training Epoch: 1/2, step 5866/7134 completed (loss: 0.016014330089092255, acc: 0.9960317611694336)
[2025-02-13 03:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:31][root][INFO] - Training Epoch: 1/2, step 5867/7134 completed (loss: 0.014476741664111614, acc: 0.9950413107872009)
[2025-02-13 03:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:31][root][INFO] - Training Epoch: 1/2, step 5868/7134 completed (loss: 0.03462343290448189, acc: 0.9879310131072998)
[2025-02-13 03:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32][root][INFO] - Training Epoch: 1/2, step 5869/7134 completed (loss: 0.030088996514678, acc: 0.9925373196601868)
[2025-02-13 03:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32][root][INFO] - Training Epoch: 1/2, step 5870/7134 completed (loss: 0.05076232925057411, acc: 0.9853556752204895)
[2025-02-13 03:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32][root][INFO] - Training Epoch: 1/2, step 5871/7134 completed (loss: 0.023689206689596176, acc: 0.9893842935562134)
[2025-02-13 03:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:33][root][INFO] - Training Epoch: 1/2, step 5872/7134 completed (loss: 0.02475244365632534, acc: 0.9939117431640625)
[2025-02-13 03:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:33][root][INFO] - Training Epoch: 1/2, step 5873/7134 completed (loss: 0.035117026418447495, acc: 0.9893333315849304)
[2025-02-13 03:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:34][root][INFO] - Training Epoch: 1/2, step 5874/7134 completed (loss: 0.027189094573259354, acc: 0.9939758777618408)
[2025-02-13 03:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:34][root][INFO] - Training Epoch: 1/2, step 5875/7134 completed (loss: 0.040237851440906525, acc: 0.9876106381416321)
[2025-02-13 03:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35][root][INFO] - Training Epoch: 1/2, step 5876/7134 completed (loss: 0.023041056469082832, acc: 0.9927184581756592)
[2025-02-13 03:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35][root][INFO] - Training Epoch: 1/2, step 5877/7134 completed (loss: 0.04528283700346947, acc: 0.989847719669342)
[2025-02-13 03:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35][root][INFO] - Training Epoch: 1/2, step 5878/7134 completed (loss: 0.022387638688087463, acc: 0.9934210777282715)
[2025-02-13 03:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:36][root][INFO] - Training Epoch: 1/2, step 5879/7134 completed (loss: 0.01097006257623434, acc: 0.9974619150161743)
[2025-02-13 03:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:36][root][INFO] - Training Epoch: 1/2, step 5880/7134 completed (loss: 0.018690461292862892, acc: 0.9911110997200012)
[2025-02-13 03:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37][root][INFO] - Training Epoch: 1/2, step 5881/7134 completed (loss: 0.011718333698809147, acc: 0.9961612224578857)
[2025-02-13 03:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37][root][INFO] - Training Epoch: 1/2, step 5882/7134 completed (loss: 0.015067537315189838, acc: 0.9976470470428467)
[2025-02-13 03:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37][root][INFO] - Training Epoch: 1/2, step 5883/7134 completed (loss: 0.04560083523392677, acc: 0.9887640476226807)
[2025-02-13 03:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:38][root][INFO] - Training Epoch: 1/2, step 5884/7134 completed (loss: 0.052274759858846664, acc: 0.9781022071838379)
[2025-02-13 03:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:38][root][INFO] - Training Epoch: 1/2, step 5885/7134 completed (loss: 0.10581281781196594, acc: 0.97826087474823)
[2025-02-13 03:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:39][root][INFO] - Training Epoch: 1/2, step 5886/7134 completed (loss: 0.018095064908266068, acc: 0.991428554058075)
[2025-02-13 03:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:39][root][INFO] - Training Epoch: 1/2, step 5887/7134 completed (loss: 0.03361015021800995, acc: 0.993630588054657)
[2025-02-13 03:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40][root][INFO] - Training Epoch: 1/2, step 5888/7134 completed (loss: 0.03406761959195137, acc: 0.9915540814399719)
[2025-02-13 03:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40][root][INFO] - Training Epoch: 1/2, step 5889/7134 completed (loss: 0.012328469194471836, acc: 0.9955357313156128)
[2025-02-13 03:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40][root][INFO] - Training Epoch: 1/2, step 5890/7134 completed (loss: 0.013250283896923065, acc: 0.996503472328186)
[2025-02-13 03:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:41][root][INFO] - Training Epoch: 1/2, step 5891/7134 completed (loss: 0.02497444860637188, acc: 0.988950252532959)
[2025-02-13 03:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:41][root][INFO] - Training Epoch: 1/2, step 5892/7134 completed (loss: 0.052472516894340515, acc: 0.9868804812431335)
[2025-02-13 03:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42][root][INFO] - Training Epoch: 1/2, step 5893/7134 completed (loss: 0.03218972682952881, acc: 0.9921259880065918)
[2025-02-13 03:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42][root][INFO] - Training Epoch: 1/2, step 5894/7134 completed (loss: 0.012146119959652424, acc: 0.9967213273048401)
[2025-02-13 03:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43][root][INFO] - Training Epoch: 1/2, step 5895/7134 completed (loss: 0.010093512944877148, acc: 0.9976190328598022)
[2025-02-13 03:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43][root][INFO] - Training Epoch: 1/2, step 5896/7134 completed (loss: 0.034819915890693665, acc: 0.9914966225624084)
[2025-02-13 03:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43][root][INFO] - Training Epoch: 1/2, step 5897/7134 completed (loss: 0.014799012802541256, acc: 0.994490385055542)
[2025-02-13 03:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44][root][INFO] - Training Epoch: 1/2, step 5898/7134 completed (loss: 0.017878128215670586, acc: 0.9956896305084229)
[2025-02-13 03:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44][root][INFO] - Training Epoch: 1/2, step 5899/7134 completed (loss: 0.014912798069417477, acc: 0.9940476417541504)
[2025-02-13 03:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:45][root][INFO] - Training Epoch: 1/2, step 5900/7134 completed (loss: 0.02382419817149639, acc: 0.9940476417541504)
[2025-02-13 03:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:45][root][INFO] - Training Epoch: 1/2, step 5901/7134 completed (loss: 0.05417609214782715, acc: 0.9850000143051147)
[2025-02-13 03:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46][root][INFO] - Training Epoch: 1/2, step 5902/7134 completed (loss: 0.059266455471515656, acc: 0.9793814420700073)
[2025-02-13 03:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46][root][INFO] - Training Epoch: 1/2, step 5903/7134 completed (loss: 0.04046987369656563, acc: 0.989051103591919)
[2025-02-13 03:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46][root][INFO] - Training Epoch: 1/2, step 5904/7134 completed (loss: 0.03410826250910759, acc: 0.9890710115432739)
[2025-02-13 03:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:47][root][INFO] - Training Epoch: 1/2, step 5905/7134 completed (loss: 0.03282085061073303, acc: 0.9916550517082214)
[2025-02-13 03:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:47][root][INFO] - Training Epoch: 1/2, step 5906/7134 completed (loss: 0.0471307598054409, acc: 0.9828009605407715)
[2025-02-13 03:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:48][root][INFO] - Training Epoch: 1/2, step 5907/7134 completed (loss: 0.04356972500681877, acc: 0.9896507263183594)
[2025-02-13 03:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:48][root][INFO] - Training Epoch: 1/2, step 5908/7134 completed (loss: 0.028432374820113182, acc: 0.9884910583496094)
[2025-02-13 03:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49][root][INFO] - Training Epoch: 1/2, step 5909/7134 completed (loss: 0.050650496035814285, acc: 0.9855907559394836)
[2025-02-13 03:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49][root][INFO] - Training Epoch: 1/2, step 5910/7134 completed (loss: 0.04710249975323677, acc: 0.9847434163093567)
[2025-02-13 03:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:50][root][INFO] - Training Epoch: 1/2, step 5911/7134 completed (loss: 0.05053370073437691, acc: 0.9900826215744019)
[2025-02-13 03:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:50][root][INFO] - Training Epoch: 1/2, step 5912/7134 completed (loss: 0.028105871751904488, acc: 0.9929676651954651)
[2025-02-13 03:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:51][root][INFO] - Training Epoch: 1/2, step 5913/7134 completed (loss: 0.08109505474567413, acc: 0.98097825050354)
[2025-02-13 03:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:51][root][INFO] - Training Epoch: 1/2, step 5914/7134 completed (loss: 0.019459379836916924, acc: 0.995184600353241)
[2025-02-13 03:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52][root][INFO] - Training Epoch: 1/2, step 5915/7134 completed (loss: 0.03575650602579117, acc: 0.9860464930534363)
[2025-02-13 03:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52][root][INFO] - Training Epoch: 1/2, step 5916/7134 completed (loss: 0.037033069878816605, acc: 0.9921362996101379)
[2025-02-13 03:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52][root][INFO] - Training Epoch: 1/2, step 5917/7134 completed (loss: 0.028363775461912155, acc: 0.9913344979286194)
[2025-02-13 03:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:53][root][INFO] - Training Epoch: 1/2, step 5918/7134 completed (loss: 0.10030804574489594, acc: 0.9796807169914246)
[2025-02-13 03:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:53][root][INFO] - Training Epoch: 1/2, step 5919/7134 completed (loss: 0.03555593639612198, acc: 0.9893617033958435)
[2025-02-13 03:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:54][root][INFO] - Training Epoch: 1/2, step 5920/7134 completed (loss: 0.03524365276098251, acc: 0.9900166392326355)
[2025-02-13 03:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:54][root][INFO] - Training Epoch: 1/2, step 5921/7134 completed (loss: 0.037005290389060974, acc: 0.9895615577697754)
[2025-02-13 03:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55][root][INFO] - Training Epoch: 1/2, step 5922/7134 completed (loss: 0.20061928033828735, acc: 0.966312050819397)
[2025-02-13 03:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55][root][INFO] - Training Epoch: 1/2, step 5923/7134 completed (loss: 0.09928393363952637, acc: 0.980966329574585)
[2025-02-13 03:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:56][root][INFO] - Training Epoch: 1/2, step 5924/7134 completed (loss: 0.04028527811169624, acc: 0.9832935333251953)
[2025-02-13 03:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:56][root][INFO] - Training Epoch: 1/2, step 5925/7134 completed (loss: 0.17772436141967773, acc: 0.9560117125511169)
[2025-02-13 03:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:56][root][INFO] - Training Epoch: 1/2, step 5926/7134 completed (loss: 0.0651574656367302, acc: 0.9833333492279053)
[2025-02-13 03:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:57][root][INFO] - Training Epoch: 1/2, step 5927/7134 completed (loss: 0.09452922642230988, acc: 0.9688196182250977)
[2025-02-13 03:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:57][root][INFO] - Training Epoch: 1/2, step 5928/7134 completed (loss: 0.18151870369911194, acc: 0.948630154132843)
[2025-02-13 03:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58][root][INFO] - Training Epoch: 1/2, step 5929/7134 completed (loss: 0.05237467959523201, acc: 0.97947758436203)
[2025-02-13 03:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58][root][INFO] - Training Epoch: 1/2, step 5930/7134 completed (loss: 0.08833131194114685, acc: 0.9844098091125488)
[2025-02-13 03:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59][root][INFO] - Training Epoch: 1/2, step 5931/7134 completed (loss: 0.04231295734643936, acc: 0.9892008900642395)
[2025-02-13 03:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59][root][INFO] - Training Epoch: 1/2, step 5932/7134 completed (loss: 0.09974408149719238, acc: 0.9666666388511658)
[2025-02-13 03:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59][root][INFO] - Training Epoch: 1/2, step 5933/7134 completed (loss: 0.14065466821193695, acc: 0.9622641801834106)
[2025-02-13 03:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00][root][INFO] - Training Epoch: 1/2, step 5934/7134 completed (loss: 0.042972758412361145, acc: 0.9893428087234497)
[2025-02-13 03:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00][root][INFO] - Training Epoch: 1/2, step 5935/7134 completed (loss: 0.021820375695824623, acc: 0.9941747784614563)
[2025-02-13 03:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:01][root][INFO] - Training Epoch: 1/2, step 5936/7134 completed (loss: 0.09148754179477692, acc: 0.9770290851593018)
[2025-02-13 03:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:01][root][INFO] - Training Epoch: 1/2, step 5937/7134 completed (loss: 0.02300836145877838, acc: 0.9936203956604004)
[2025-02-13 03:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:01][root][INFO] - Training Epoch: 1/2, step 5938/7134 completed (loss: 0.06866145879030228, acc: 0.9732313752174377)
[2025-02-13 03:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:02][root][INFO] - Training Epoch: 1/2, step 5939/7134 completed (loss: 0.12094447016716003, acc: 0.9638009071350098)
[2025-02-13 03:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:02][root][INFO] - Training Epoch: 1/2, step 5940/7134 completed (loss: 0.04578171297907829, acc: 0.9839650392532349)
[2025-02-13 03:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03][root][INFO] - Training Epoch: 1/2, step 5941/7134 completed (loss: 0.06699230521917343, acc: 0.987500011920929)
[2025-02-13 03:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03][root][INFO] - Training Epoch: 1/2, step 5942/7134 completed (loss: 0.011981368996202946, acc: 0.9961758852005005)
[2025-02-13 03:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03][root][INFO] - Training Epoch: 1/2, step 5943/7134 completed (loss: 0.021264318376779556, acc: 0.995708167552948)
[2025-02-13 03:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:04][root][INFO] - Training Epoch: 1/2, step 5944/7134 completed (loss: 0.039417434483766556, acc: 0.9842932224273682)
[2025-02-13 03:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:04][root][INFO] - Training Epoch: 1/2, step 5945/7134 completed (loss: 0.031277384608983994, acc: 0.9909909963607788)
[2025-02-13 03:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05][root][INFO] - Training Epoch: 1/2, step 5946/7134 completed (loss: 0.03518417850136757, acc: 0.989130437374115)
[2025-02-13 03:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05][root][INFO] - Training Epoch: 1/2, step 5947/7134 completed (loss: 0.03530395030975342, acc: 0.9876084327697754)
[2025-02-13 03:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06][root][INFO] - Training Epoch: 1/2, step 5948/7134 completed (loss: 0.06725297123193741, acc: 0.984000027179718)
[2025-02-13 03:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06][root][INFO] - Training Epoch: 1/2, step 5949/7134 completed (loss: 0.01402831356972456, acc: 0.9974905848503113)
[2025-02-13 03:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:07][root][INFO] - Training Epoch: 1/2, step 5950/7134 completed (loss: 0.10445361584424973, acc: 0.9714964628219604)
[2025-02-13 03:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:07][root][INFO] - Training Epoch: 1/2, step 5951/7134 completed (loss: 0.04921820014715195, acc: 0.9870874881744385)
[2025-02-13 03:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:07][root][INFO] - Training Epoch: 1/2, step 5952/7134 completed (loss: 0.1549968719482422, acc: 0.9609375)
[2025-02-13 03:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:08][root][INFO] - Training Epoch: 1/2, step 5953/7134 completed (loss: 0.05499092489480972, acc: 0.9807162284851074)
[2025-02-13 03:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:08][root][INFO] - Training Epoch: 1/2, step 5954/7134 completed (loss: 0.01849166303873062, acc: 0.9923858046531677)
[2025-02-13 03:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09][root][INFO] - Training Epoch: 1/2, step 5955/7134 completed (loss: 0.044368330389261246, acc: 0.9952380657196045)
[2025-02-13 03:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09][root][INFO] - Training Epoch: 1/2, step 5956/7134 completed (loss: 0.05702068284153938, acc: 0.9843137264251709)
[2025-02-13 03:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09][root][INFO] - Training Epoch: 1/2, step 5957/7134 completed (loss: 0.05594071000814438, acc: 0.9865067601203918)
[2025-02-13 03:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10][root][INFO] - Training Epoch: 1/2, step 5958/7134 completed (loss: 0.03796204552054405, acc: 0.9892617464065552)
[2025-02-13 03:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10][root][INFO] - Training Epoch: 1/2, step 5959/7134 completed (loss: 0.054008472710847855, acc: 0.9879840016365051)
[2025-02-13 03:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:11][root][INFO] - Training Epoch: 1/2, step 5960/7134 completed (loss: 0.009243237785995007, acc: 0.9985954761505127)
[2025-02-13 03:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:11][root][INFO] - Training Epoch: 1/2, step 5961/7134 completed (loss: 0.052067674696445465, acc: 0.9815573692321777)
[2025-02-13 03:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12][root][INFO] - Training Epoch: 1/2, step 5962/7134 completed (loss: 0.01334586925804615, acc: 0.995121955871582)
[2025-02-13 03:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12][root][INFO] - Training Epoch: 1/2, step 5963/7134 completed (loss: 0.01643817499279976, acc: 0.994413435459137)
[2025-02-13 03:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12][root][INFO] - Training Epoch: 1/2, step 5964/7134 completed (loss: 0.02588704042136669, acc: 0.9944547414779663)
[2025-02-13 03:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:13][root][INFO] - Training Epoch: 1/2, step 5965/7134 completed (loss: 0.043327633291482925, acc: 0.9880059957504272)
[2025-02-13 03:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:13][root][INFO] - Training Epoch: 1/2, step 5966/7134 completed (loss: 0.04295283555984497, acc: 0.9889042973518372)
[2025-02-13 03:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:14][root][INFO] - Training Epoch: 1/2, step 5967/7134 completed (loss: 0.02389751747250557, acc: 0.9940298795700073)
[2025-02-13 03:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:14][root][INFO] - Training Epoch: 1/2, step 5968/7134 completed (loss: 0.038200926035642624, acc: 0.9931507110595703)
[2025-02-13 03:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:15][root][INFO] - Training Epoch: 1/2, step 5969/7134 completed (loss: 0.03741208091378212, acc: 0.9922600388526917)
[2025-02-13 03:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:15][root][INFO] - Training Epoch: 1/2, step 5970/7134 completed (loss: 0.05016745999455452, acc: 0.9897540807723999)
[2025-02-13 03:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16][root][INFO] - Training Epoch: 1/2, step 5971/7134 completed (loss: 0.014660462737083435, acc: 0.9949874877929688)
[2025-02-13 03:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16][root][INFO] - Training Epoch: 1/2, step 5972/7134 completed (loss: 0.07476401329040527, acc: 0.98531574010849)
[2025-02-13 03:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16][root][INFO] - Training Epoch: 1/2, step 5973/7134 completed (loss: 0.06521987169981003, acc: 0.9784172773361206)
[2025-02-13 03:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:17][root][INFO] - Training Epoch: 1/2, step 5974/7134 completed (loss: 0.010219910182058811, acc: 0.9965338110923767)
[2025-02-13 03:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:17][root][INFO] - Training Epoch: 1/2, step 5975/7134 completed (loss: 0.020073136314749718, acc: 0.99262535572052)
[2025-02-13 03:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:18][root][INFO] - Training Epoch: 1/2, step 5976/7134 completed (loss: 0.06770914047956467, acc: 0.9815950989723206)
[2025-02-13 03:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:18][root][INFO] - Training Epoch: 1/2, step 5977/7134 completed (loss: 0.019871866330504417, acc: 0.9950433969497681)
[2025-02-13 03:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:19][root][INFO] - Training Epoch: 1/2, step 5978/7134 completed (loss: 0.030285801738500595, acc: 0.995192289352417)
[2025-02-13 03:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:19][root][INFO] - Training Epoch: 1/2, step 5979/7134 completed (loss: 0.061735257506370544, acc: 0.9851852059364319)
[2025-02-13 03:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20][root][INFO] - Training Epoch: 1/2, step 5980/7134 completed (loss: 0.028675802052021027, acc: 0.991037130355835)
[2025-02-13 03:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20][root][INFO] - Training Epoch: 1/2, step 5981/7134 completed (loss: 0.021534519270062447, acc: 0.9942196607589722)
[2025-02-13 03:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20][root][INFO] - Training Epoch: 1/2, step 5982/7134 completed (loss: 0.032334499061107635, acc: 0.9940357804298401)
[2025-02-13 03:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:21][root][INFO] - Training Epoch: 1/2, step 5983/7134 completed (loss: 0.05372298136353493, acc: 0.9819444417953491)
[2025-02-13 03:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:21][root][INFO] - Training Epoch: 1/2, step 5984/7134 completed (loss: 0.019539939239621162, acc: 0.9914893507957458)
[2025-02-13 03:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:22][root][INFO] - Training Epoch: 1/2, step 5985/7134 completed (loss: 0.02268950827419758, acc: 0.9962453246116638)
[2025-02-13 03:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:22][root][INFO] - Training Epoch: 1/2, step 5986/7134 completed (loss: 0.036937542259693146, acc: 0.987261176109314)
[2025-02-13 03:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:23][root][INFO] - Training Epoch: 1/2, step 5987/7134 completed (loss: 0.05817421153187752, acc: 0.9825174808502197)
[2025-02-13 03:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:23][root][INFO] - Training Epoch: 1/2, step 5988/7134 completed (loss: 0.011492665857076645, acc: 0.9959999918937683)
[2025-02-13 03:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24][root][INFO] - Training Epoch: 1/2, step 5989/7134 completed (loss: 0.04983818158507347, acc: 0.9852398633956909)
[2025-02-13 03:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24][root][INFO] - Training Epoch: 1/2, step 5990/7134 completed (loss: 0.03621676191687584, acc: 0.983561635017395)
[2025-02-13 03:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24][root][INFO] - Training Epoch: 1/2, step 5991/7134 completed (loss: 0.11526405811309814, acc: 0.9748603105545044)
[2025-02-13 03:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:25][root][INFO] - Training Epoch: 1/2, step 5992/7134 completed (loss: 0.022860541939735413, acc: 0.9941927790641785)
[2025-02-13 03:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:25][root][INFO] - Training Epoch: 1/2, step 5993/7134 completed (loss: 0.029989102855324745, acc: 0.9930651783943176)
[2025-02-13 03:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26][root][INFO] - Training Epoch: 1/2, step 5994/7134 completed (loss: 0.0222808625549078, acc: 0.9961977005004883)
[2025-02-13 03:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26][root][INFO] - Training Epoch: 1/2, step 5995/7134 completed (loss: 0.011604144237935543, acc: 0.997183084487915)
[2025-02-13 03:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:27][root][INFO] - Training Epoch: 1/2, step 5996/7134 completed (loss: 0.05502010136842728, acc: 0.9827855825424194)
[2025-02-13 03:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:27][root][INFO] - Training Epoch: 1/2, step 5997/7134 completed (loss: 0.03634888306260109, acc: 0.9946595430374146)
[2025-02-13 03:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28][root][INFO] - Training Epoch: 1/2, step 5998/7134 completed (loss: 0.03324170783162117, acc: 0.9887780547142029)
[2025-02-13 03:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28][root][INFO] - Training Epoch: 1/2, step 5999/7134 completed (loss: 0.022812100127339363, acc: 0.9925650358200073)
[2025-02-13 03:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28][root][INFO] - Training Epoch: 1/2, step 6000/7134 completed (loss: 0.03968851640820503, acc: 0.9883527159690857)
[2025-02-13 03:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29][root][INFO] - Training Epoch: 1/2, step 6001/7134 completed (loss: 0.015521910041570663, acc: 0.9948849081993103)
[2025-02-13 03:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29][root][INFO] - Training Epoch: 1/2, step 6002/7134 completed (loss: 0.04799644276499748, acc: 0.9844054579734802)
[2025-02-13 03:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:30][root][INFO] - Training Epoch: 1/2, step 6003/7134 completed (loss: 0.03175981342792511, acc: 0.9897210001945496)
[2025-02-13 03:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:30][root][INFO] - Training Epoch: 1/2, step 6004/7134 completed (loss: 0.02734810672700405, acc: 0.9916550517082214)
[2025-02-13 03:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:31][root][INFO] - Training Epoch: 1/2, step 6005/7134 completed (loss: 0.03495892882347107, acc: 0.9878234267234802)
[2025-02-13 03:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:31][root][INFO] - Training Epoch: 1/2, step 6006/7134 completed (loss: 0.04818613827228546, acc: 0.9864603281021118)
[2025-02-13 03:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:31][root][INFO] - Training Epoch: 1/2, step 6007/7134 completed (loss: 0.016709616407752037, acc: 0.9966611266136169)
[2025-02-13 03:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32][root][INFO] - Training Epoch: 1/2, step 6008/7134 completed (loss: 0.03472200408577919, acc: 0.9851632118225098)
[2025-02-13 03:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32][root][INFO] - Training Epoch: 1/2, step 6009/7134 completed (loss: 0.05354076996445656, acc: 0.9860248565673828)
[2025-02-13 03:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:33][root][INFO] - Training Epoch: 1/2, step 6010/7134 completed (loss: 0.06024884805083275, acc: 0.9864498376846313)
[2025-02-13 03:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:33][root][INFO] - Training Epoch: 1/2, step 6011/7134 completed (loss: 0.04175359010696411, acc: 0.9896373152732849)
[2025-02-13 03:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:34][root][INFO] - Training Epoch: 1/2, step 6012/7134 completed (loss: 0.04212832823395729, acc: 0.9850746393203735)
[2025-02-13 03:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:34][root][INFO] - Training Epoch: 1/2, step 6013/7134 completed (loss: 0.051486194133758545, acc: 0.9852941036224365)
[2025-02-13 03:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35][root][INFO] - Training Epoch: 1/2, step 6014/7134 completed (loss: 0.026303624734282494, acc: 0.9927404522895813)
[2025-02-13 03:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35][root][INFO] - Training Epoch: 1/2, step 6015/7134 completed (loss: 0.06352857500314713, acc: 0.9793388247489929)
[2025-02-13 03:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35][root][INFO] - Training Epoch: 1/2, step 6016/7134 completed (loss: 0.019110020250082016, acc: 0.9964157938957214)
[2025-02-13 03:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:36][root][INFO] - Training Epoch: 1/2, step 6017/7134 completed (loss: 0.037940461188554764, acc: 0.9900990128517151)
[2025-02-13 03:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:36][root][INFO] - Training Epoch: 1/2, step 6018/7134 completed (loss: 0.013627380132675171, acc: 1.0)
[2025-02-13 03:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:37][root][INFO] - Training Epoch: 1/2, step 6019/7134 completed (loss: 0.04958151653409004, acc: 0.9913793206214905)
[2025-02-13 03:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:37][root][INFO] - Training Epoch: 1/2, step 6020/7134 completed (loss: 0.005972990300506353, acc: 1.0)
[2025-02-13 03:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:37][root][INFO] - Training Epoch: 1/2, step 6021/7134 completed (loss: 0.014882910065352917, acc: 0.9934640526771545)
[2025-02-13 03:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38][root][INFO] - Training Epoch: 1/2, step 6022/7134 completed (loss: 0.023248564451932907, acc: 0.9889298677444458)
[2025-02-13 03:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38][root][INFO] - Training Epoch: 1/2, step 6023/7134 completed (loss: 0.02461668848991394, acc: 0.9918699264526367)
[2025-02-13 03:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38][root][INFO] - Training Epoch: 1/2, step 6024/7134 completed (loss: 0.014765385538339615, acc: 0.9953488111495972)
[2025-02-13 03:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:39][root][INFO] - Training Epoch: 1/2, step 6025/7134 completed (loss: 0.011672476306557655, acc: 0.9979079365730286)
[2025-02-13 03:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:39][root][INFO] - Training Epoch: 1/2, step 6026/7134 completed (loss: 0.06244971230626106, acc: 0.9885714054107666)
[2025-02-13 03:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:40][root][INFO] - Training Epoch: 1/2, step 6027/7134 completed (loss: 0.014238434843719006, acc: 0.9974293112754822)
[2025-02-13 03:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:40][root][INFO] - Training Epoch: 1/2, step 6028/7134 completed (loss: 0.005418400280177593, acc: 1.0)
[2025-02-13 03:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:40][root][INFO] - Training Epoch: 1/2, step 6029/7134 completed (loss: 0.03658287227153778, acc: 0.9891696572303772)
[2025-02-13 03:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:41][root][INFO] - Training Epoch: 1/2, step 6030/7134 completed (loss: 0.02436516061425209, acc: 0.992337167263031)
[2025-02-13 03:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:41][root][INFO] - Training Epoch: 1/2, step 6031/7134 completed (loss: 0.04222637042403221, acc: 0.988304078578949)
[2025-02-13 03:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:41][root][INFO] - Training Epoch: 1/2, step 6032/7134 completed (loss: 0.019624732434749603, acc: 0.9974683523178101)
[2025-02-13 03:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42][root][INFO] - Training Epoch: 1/2, step 6033/7134 completed (loss: 0.06008275970816612, acc: 0.9836065769195557)
[2025-02-13 03:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42][root][INFO] - Training Epoch: 1/2, step 6034/7134 completed (loss: 0.026146961376070976, acc: 0.9918032884597778)
[2025-02-13 03:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:43][root][INFO] - Training Epoch: 1/2, step 6035/7134 completed (loss: 0.08616171777248383, acc: 0.9719298481941223)
[2025-02-13 03:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:43][root][INFO] - Training Epoch: 1/2, step 6036/7134 completed (loss: 0.058533135801553726, acc: 0.9846153855323792)
[2025-02-13 03:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:44][root][INFO] - Training Epoch: 1/2, step 6037/7134 completed (loss: 0.04197051003575325, acc: 0.989830493927002)
[2025-02-13 03:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:44][root][INFO] - Training Epoch: 1/2, step 6038/7134 completed (loss: 0.03865601122379303, acc: 0.9888142943382263)
[2025-02-13 03:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45][root][INFO] - Training Epoch: 1/2, step 6039/7134 completed (loss: 0.027857553213834763, acc: 0.9940688014030457)
[2025-02-13 03:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45][root][INFO] - Training Epoch: 1/2, step 6040/7134 completed (loss: 0.06878836452960968, acc: 0.9844074845314026)
[2025-02-13 03:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45][root][INFO] - Training Epoch: 1/2, step 6041/7134 completed (loss: 0.04005955159664154, acc: 0.9882044792175293)
[2025-02-13 03:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:46][root][INFO] - Training Epoch: 1/2, step 6042/7134 completed (loss: 0.019677696749567986, acc: 0.9940000176429749)
[2025-02-13 03:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:46][root][INFO] - Training Epoch: 1/2, step 6043/7134 completed (loss: 0.03014436922967434, acc: 0.9916067123413086)
[2025-02-13 03:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:47][root][INFO] - Training Epoch: 1/2, step 6044/7134 completed (loss: 0.06491705030202866, acc: 0.9827160239219666)
[2025-02-13 03:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:47][root][INFO] - Training Epoch: 1/2, step 6045/7134 completed (loss: 0.03191099315881729, acc: 0.9881656765937805)
[2025-02-13 03:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48][root][INFO] - Training Epoch: 1/2, step 6046/7134 completed (loss: 0.05029726028442383, acc: 0.984402060508728)
[2025-02-13 03:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48][root][INFO] - Training Epoch: 1/2, step 6047/7134 completed (loss: 0.11669543385505676, acc: 0.9645868539810181)
[2025-02-13 03:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:49][root][INFO] - Training Epoch: 1/2, step 6048/7134 completed (loss: 0.06118365749716759, acc: 0.9859719276428223)
[2025-02-13 03:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:49][root][INFO] - Training Epoch: 1/2, step 6049/7134 completed (loss: 0.017405299469828606, acc: 0.9965870380401611)
[2025-02-13 03:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:49][root][INFO] - Training Epoch: 1/2, step 6050/7134 completed (loss: 0.06719356775283813, acc: 0.9794801473617554)
[2025-02-13 03:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:50][root][INFO] - Training Epoch: 1/2, step 6051/7134 completed (loss: 0.09360301494598389, acc: 0.9675036668777466)
[2025-02-13 03:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:50][root][INFO] - Training Epoch: 1/2, step 6052/7134 completed (loss: 0.07172849774360657, acc: 0.9780219793319702)
[2025-02-13 03:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:51][root][INFO] - Training Epoch: 1/2, step 6053/7134 completed (loss: 0.05746205523610115, acc: 0.9813596606254578)
[2025-02-13 03:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:51][root][INFO] - Training Epoch: 1/2, step 6054/7134 completed (loss: 0.04373595118522644, acc: 0.989130437374115)
[2025-02-13 03:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52][root][INFO] - Training Epoch: 1/2, step 6055/7134 completed (loss: 0.036507293581962585, acc: 0.9889415502548218)
[2025-02-13 03:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52][root][INFO] - Training Epoch: 1/2, step 6056/7134 completed (loss: 0.07553336024284363, acc: 0.9746543765068054)
[2025-02-13 03:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52][root][INFO] - Training Epoch: 1/2, step 6057/7134 completed (loss: 0.05574074387550354, acc: 0.9835526347160339)
[2025-02-13 03:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:53][root][INFO] - Training Epoch: 1/2, step 6058/7134 completed (loss: 0.013161170296370983, acc: 0.9954954981803894)
[2025-02-13 03:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:53][root][INFO] - Training Epoch: 1/2, step 6059/7134 completed (loss: 0.034448787569999695, acc: 0.982332170009613)
[2025-02-13 03:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:54][root][INFO] - Training Epoch: 1/2, step 6060/7134 completed (loss: 0.023328550159931183, acc: 0.9909774661064148)
[2025-02-13 03:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:54][root][INFO] - Training Epoch: 1/2, step 6061/7134 completed (loss: 0.05964895710349083, acc: 0.9861325025558472)
[2025-02-13 03:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55][root][INFO] - Training Epoch: 1/2, step 6062/7134 completed (loss: 0.08382396399974823, acc: 0.9767441749572754)
[2025-02-13 03:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55][root][INFO] - Training Epoch: 1/2, step 6063/7134 completed (loss: 0.1120186373591423, acc: 0.9801084995269775)
[2025-02-13 03:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55][root][INFO] - Training Epoch: 1/2, step 6064/7134 completed (loss: 0.030036423355340958, acc: 0.9927641153335571)
[2025-02-13 03:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:56][root][INFO] - Training Epoch: 1/2, step 6065/7134 completed (loss: 0.038249485194683075, acc: 0.9890643954277039)
[2025-02-13 03:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:56][root][INFO] - Training Epoch: 1/2, step 6066/7134 completed (loss: 0.09234417229890823, acc: 0.9703459739685059)
[2025-02-13 03:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57][root][INFO] - Training Epoch: 1/2, step 6067/7134 completed (loss: 0.06749892979860306, acc: 0.9783549904823303)
[2025-02-13 03:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57][root][INFO] - Training Epoch: 1/2, step 6068/7134 completed (loss: 0.10487429797649384, acc: 0.9762202501296997)
[2025-02-13 03:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58][root][INFO] - Training Epoch: 1/2, step 6069/7134 completed (loss: 0.07730010151863098, acc: 0.9751824736595154)
[2025-02-13 03:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58][root][INFO] - Training Epoch: 1/2, step 6070/7134 completed (loss: 0.08437629789113998, acc: 0.9733688235282898)
[2025-02-13 03:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58][root][INFO] - Training Epoch: 1/2, step 6071/7134 completed (loss: 0.05037766322493553, acc: 0.9865410327911377)
[2025-02-13 03:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:59][root][INFO] - Training Epoch: 1/2, step 6072/7134 completed (loss: 0.12923839688301086, acc: 0.9642857313156128)
[2025-02-13 03:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:59][root][INFO] - Training Epoch: 1/2, step 6073/7134 completed (loss: 0.07547686249017715, acc: 0.9818181991577148)
[2025-02-13 03:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:00][root][INFO] - Training Epoch: 1/2, step 6074/7134 completed (loss: 0.08735927939414978, acc: 0.9823529124259949)
[2025-02-13 03:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:00][root][INFO] - Training Epoch: 1/2, step 6075/7134 completed (loss: 0.06555168330669403, acc: 0.9856114983558655)
[2025-02-13 03:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01][root][INFO] - Training Epoch: 1/2, step 6076/7134 completed (loss: 0.10164310038089752, acc: 0.9741379022598267)
[2025-02-13 03:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01][root][INFO] - Training Epoch: 1/2, step 6077/7134 completed (loss: 0.06855769455432892, acc: 0.9776358008384705)
[2025-02-13 03:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01][root][INFO] - Training Epoch: 1/2, step 6078/7134 completed (loss: 0.09602362662553787, acc: 0.9788732528686523)
[2025-02-13 03:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:02][root][INFO] - Training Epoch: 1/2, step 6079/7134 completed (loss: 0.026766110211610794, acc: 0.9927007555961609)
[2025-02-13 03:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:02][root][INFO] - Training Epoch: 1/2, step 6080/7134 completed (loss: 0.035799697041511536, acc: 0.9886845946311951)
[2025-02-13 03:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03][root][INFO] - Training Epoch: 1/2, step 6081/7134 completed (loss: 0.035152725875377655, acc: 0.9877862334251404)
[2025-02-13 03:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03][root][INFO] - Training Epoch: 1/2, step 6082/7134 completed (loss: 0.02836725302040577, acc: 0.9904458522796631)
[2025-02-13 03:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:04][root][INFO] - Training Epoch: 1/2, step 6083/7134 completed (loss: 0.015111061744391918, acc: 0.9950860142707825)
[2025-02-13 03:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:04][root][INFO] - Training Epoch: 1/2, step 6084/7134 completed (loss: 0.04040394723415375, acc: 0.9905660152435303)
[2025-02-13 03:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:04][root][INFO] - Training Epoch: 1/2, step 6085/7134 completed (loss: 0.04361923038959503, acc: 0.9838998317718506)
[2025-02-13 03:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05][root][INFO] - Training Epoch: 1/2, step 6086/7134 completed (loss: 0.024685174226760864, acc: 0.992977499961853)
[2025-02-13 03:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05][root][INFO] - Training Epoch: 1/2, step 6087/7134 completed (loss: 0.059301845729351044, acc: 0.9798741936683655)
[2025-02-13 03:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:06][root][INFO] - Training Epoch: 1/2, step 6088/7134 completed (loss: 0.03280971199274063, acc: 0.9906166195869446)
[2025-02-13 03:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:06][root][INFO] - Training Epoch: 1/2, step 6089/7134 completed (loss: 0.03173056244850159, acc: 0.9917241334915161)
[2025-02-13 03:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07][root][INFO] - Training Epoch: 1/2, step 6090/7134 completed (loss: 0.019531968981027603, acc: 0.9958847761154175)
[2025-02-13 03:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07][root][INFO] - Training Epoch: 1/2, step 6091/7134 completed (loss: 0.029503148049116135, acc: 0.993630588054657)
[2025-02-13 03:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08][root][INFO] - Training Epoch: 1/2, step 6092/7134 completed (loss: 0.03275727480649948, acc: 0.9952380657196045)
[2025-02-13 03:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08][root][INFO] - Training Epoch: 1/2, step 6093/7134 completed (loss: 0.04994126409292221, acc: 0.9888682961463928)
[2025-02-13 03:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08][root][INFO] - Training Epoch: 1/2, step 6094/7134 completed (loss: 0.04291383922100067, acc: 0.9887955188751221)
[2025-02-13 03:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:09][root][INFO] - Training Epoch: 1/2, step 6095/7134 completed (loss: 0.02869756519794464, acc: 0.9913793206214905)
[2025-02-13 03:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:09][root][INFO] - Training Epoch: 1/2, step 6096/7134 completed (loss: 0.03995843604207039, acc: 0.9904534816741943)
[2025-02-13 03:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10][root][INFO] - Training Epoch: 1/2, step 6097/7134 completed (loss: 0.044282447546720505, acc: 0.9861496090888977)
[2025-02-13 03:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10][root][INFO] - Training Epoch: 1/2, step 6098/7134 completed (loss: 0.03962717950344086, acc: 0.9909909963607788)
[2025-02-13 03:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:11][root][INFO] - Training Epoch: 1/2, step 6099/7134 completed (loss: 0.07034333050251007, acc: 0.9831804037094116)
[2025-02-13 03:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:11][root][INFO] - Training Epoch: 1/2, step 6100/7134 completed (loss: 0.09219340234994888, acc: 0.9695122241973877)
[2025-02-13 03:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:11][root][INFO] - Training Epoch: 1/2, step 6101/7134 completed (loss: 0.0166742205619812, acc: 0.9968051314353943)
[2025-02-13 03:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12][root][INFO] - Training Epoch: 1/2, step 6102/7134 completed (loss: 0.03881261125206947, acc: 0.9929577708244324)
[2025-02-13 03:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12][root][INFO] - Training Epoch: 1/2, step 6103/7134 completed (loss: 0.04482881352305412, acc: 0.9851852059364319)
[2025-02-13 03:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:13][root][INFO] - Training Epoch: 1/2, step 6104/7134 completed (loss: 0.09105028212070465, acc: 0.9733333587646484)
[2025-02-13 03:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:13][root][INFO] - Training Epoch: 1/2, step 6105/7134 completed (loss: 0.0713004469871521, acc: 0.9866071343421936)
[2025-02-13 03:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14][root][INFO] - Training Epoch: 1/2, step 6106/7134 completed (loss: 0.057557858526706696, acc: 0.9821109175682068)
[2025-02-13 03:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14][root][INFO] - Training Epoch: 1/2, step 6107/7134 completed (loss: 0.06351309269666672, acc: 0.9852941036224365)
[2025-02-13 03:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14][root][INFO] - Training Epoch: 1/2, step 6108/7134 completed (loss: 0.04659675061702728, acc: 0.9890710115432739)
[2025-02-13 03:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:15][root][INFO] - Training Epoch: 1/2, step 6109/7134 completed (loss: 0.01856354810297489, acc: 0.9957627058029175)
[2025-02-13 03:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:15][root][INFO] - Training Epoch: 1/2, step 6110/7134 completed (loss: 0.11135099828243256, acc: 0.9658119678497314)
[2025-02-13 03:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16][root][INFO] - Training Epoch: 1/2, step 6111/7134 completed (loss: 0.08146736025810242, acc: 0.9812332391738892)
[2025-02-13 03:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16][root][INFO] - Training Epoch: 1/2, step 6112/7134 completed (loss: 0.04570160433650017, acc: 0.9832776188850403)
[2025-02-13 03:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16][root][INFO] - Training Epoch: 1/2, step 6113/7134 completed (loss: 0.021217888221144676, acc: 0.9924127459526062)
[2025-02-13 03:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:17][root][INFO] - Training Epoch: 1/2, step 6114/7134 completed (loss: 0.0710422620177269, acc: 0.9801653027534485)
[2025-02-13 03:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:17][root][INFO] - Training Epoch: 1/2, step 6115/7134 completed (loss: 0.057652294635772705, acc: 0.984674334526062)
[2025-02-13 03:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:18][root][INFO] - Training Epoch: 1/2, step 6116/7134 completed (loss: 0.08677709102630615, acc: 0.9822134375572205)
[2025-02-13 03:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:18][root][INFO] - Training Epoch: 1/2, step 6117/7134 completed (loss: 0.09305205941200256, acc: 0.9822404384613037)
[2025-02-13 03:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19][root][INFO] - Training Epoch: 1/2, step 6118/7134 completed (loss: 0.08879253268241882, acc: 0.9797794222831726)
[2025-02-13 03:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19][root][INFO] - Training Epoch: 1/2, step 6119/7134 completed (loss: 0.03621431440114975, acc: 0.9867947101593018)
[2025-02-13 03:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19][root][INFO] - Training Epoch: 1/2, step 6120/7134 completed (loss: 0.04763513430953026, acc: 0.989266574382782)
[2025-02-13 03:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:20][root][INFO] - Training Epoch: 1/2, step 6121/7134 completed (loss: 0.05179517716169357, acc: 0.9856114983558655)
[2025-02-13 03:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:20][root][INFO] - Training Epoch: 1/2, step 6122/7134 completed (loss: 0.0432431623339653, acc: 0.9811320900917053)
[2025-02-13 03:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:21][root][INFO] - Training Epoch: 1/2, step 6123/7134 completed (loss: 0.1026061549782753, acc: 0.9771309494972229)
[2025-02-13 03:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:21][root][INFO] - Training Epoch: 1/2, step 6124/7134 completed (loss: 0.026618873700499535, acc: 0.9884615540504456)
[2025-02-13 03:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22][root][INFO] - Training Epoch: 1/2, step 6125/7134 completed (loss: 0.13108059763908386, acc: 0.9759036302566528)
[2025-02-13 03:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22][root][INFO] - Training Epoch: 1/2, step 6126/7134 completed (loss: 0.06670106202363968, acc: 0.982758641242981)
[2025-02-13 03:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22][root][INFO] - Training Epoch: 1/2, step 6127/7134 completed (loss: 0.044808413833379745, acc: 0.9857142567634583)
[2025-02-13 03:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:23][root][INFO] - Training Epoch: 1/2, step 6128/7134 completed (loss: 0.03808888792991638, acc: 0.9878542423248291)
[2025-02-13 03:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:23][root][INFO] - Training Epoch: 1/2, step 6129/7134 completed (loss: 0.06369618326425552, acc: 0.9774965047836304)
[2025-02-13 03:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24][root][INFO] - Training Epoch: 1/2, step 6130/7134 completed (loss: 0.03247079253196716, acc: 0.9910394549369812)
[2025-02-13 03:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24][root][INFO] - Training Epoch: 1/2, step 6131/7134 completed (loss: 0.042036186903715134, acc: 0.9928143620491028)
[2025-02-13 03:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:25][root][INFO] - Training Epoch: 1/2, step 6132/7134 completed (loss: 0.04953717067837715, acc: 0.987500011920929)
[2025-02-13 03:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:25][root][INFO] - Training Epoch: 1/2, step 6133/7134 completed (loss: 0.051025230437517166, acc: 0.9915048480033875)
[2025-02-13 03:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:25][root][INFO] - Training Epoch: 1/2, step 6134/7134 completed (loss: 0.08204159885644913, acc: 0.9739663004875183)
[2025-02-13 03:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26][root][INFO] - Training Epoch: 1/2, step 6135/7134 completed (loss: 0.017076123505830765, acc: 0.996515691280365)
[2025-02-13 03:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26][root][INFO] - Training Epoch: 1/2, step 6136/7134 completed (loss: 0.06108861044049263, acc: 0.9901960492134094)
[2025-02-13 03:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27][root][INFO] - Training Epoch: 1/2, step 6137/7134 completed (loss: 0.07772911339998245, acc: 0.9859594106674194)
[2025-02-13 03:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27][root][INFO] - Training Epoch: 1/2, step 6138/7134 completed (loss: 0.06508579850196838, acc: 0.9877111911773682)
[2025-02-13 03:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27][root][INFO] - Training Epoch: 1/2, step 6139/7134 completed (loss: 0.06034078821539879, acc: 0.9833333492279053)
[2025-02-13 03:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28][root][INFO] - Training Epoch: 1/2, step 6140/7134 completed (loss: 0.03465186804533005, acc: 0.9911660552024841)
[2025-02-13 03:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28][root][INFO] - Training Epoch: 1/2, step 6141/7134 completed (loss: 0.06888557970523834, acc: 0.9938837885856628)
[2025-02-13 03:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:29][root][INFO] - Training Epoch: 1/2, step 6142/7134 completed (loss: 0.04510480538010597, acc: 0.9841726422309875)
[2025-02-13 03:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:29][root][INFO] - Training Epoch: 1/2, step 6143/7134 completed (loss: 0.03682059049606323, acc: 0.9932432174682617)
[2025-02-13 03:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30][root][INFO] - Training Epoch: 1/2, step 6144/7134 completed (loss: 0.03143277019262314, acc: 0.9899159669876099)
[2025-02-13 03:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30][root][INFO] - Training Epoch: 1/2, step 6145/7134 completed (loss: 0.06484810262918472, acc: 0.9867899417877197)
[2025-02-13 03:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31][root][INFO] - Training Epoch: 1/2, step 6146/7134 completed (loss: 0.06909403949975967, acc: 0.9834087491035461)
[2025-02-13 03:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31][root][INFO] - Training Epoch: 1/2, step 6147/7134 completed (loss: 0.05563066527247429, acc: 0.988095223903656)
[2025-02-13 03:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31][root][INFO] - Training Epoch: 1/2, step 6148/7134 completed (loss: 0.05718205124139786, acc: 0.9832496047019958)
[2025-02-13 03:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32][root][INFO] - Training Epoch: 1/2, step 6149/7134 completed (loss: 0.04519423097372055, acc: 0.9887459874153137)
[2025-02-13 03:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32][root][INFO] - Training Epoch: 1/2, step 6150/7134 completed (loss: 0.04248247668147087, acc: 0.9859648942947388)
[2025-02-13 03:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:33][root][INFO] - Training Epoch: 1/2, step 6151/7134 completed (loss: 0.019749093800783157, acc: 0.9921507239341736)
[2025-02-13 03:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:33][root][INFO] - Training Epoch: 1/2, step 6152/7134 completed (loss: 0.03337092697620392, acc: 0.9879336357116699)
[2025-02-13 03:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:33][root][INFO] - Training Epoch: 1/2, step 6153/7134 completed (loss: 0.021908165886998177, acc: 0.9913169145584106)
[2025-02-13 03:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34][root][INFO] - Training Epoch: 1/2, step 6154/7134 completed (loss: 0.045315831899642944, acc: 0.9874476790428162)
[2025-02-13 03:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34][root][INFO] - Training Epoch: 1/2, step 6155/7134 completed (loss: 0.04714992269873619, acc: 0.9866468906402588)
[2025-02-13 03:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:35][root][INFO] - Training Epoch: 1/2, step 6156/7134 completed (loss: 0.022914085537195206, acc: 0.9920760989189148)
[2025-02-13 03:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:35][root][INFO] - Training Epoch: 1/2, step 6157/7134 completed (loss: 0.02926384098827839, acc: 0.9921630024909973)
[2025-02-13 03:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:36][root][INFO] - Training Epoch: 1/2, step 6158/7134 completed (loss: 0.0760008692741394, acc: 0.9819168448448181)
[2025-02-13 03:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:36][root][INFO] - Training Epoch: 1/2, step 6159/7134 completed (loss: 0.057817667722702026, acc: 0.9855247139930725)
[2025-02-13 03:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37][root][INFO] - Training Epoch: 1/2, step 6160/7134 completed (loss: 0.08298476040363312, acc: 0.9803030490875244)
[2025-02-13 03:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37][root][INFO] - Training Epoch: 1/2, step 6161/7134 completed (loss: 0.06782836467027664, acc: 0.982425332069397)
[2025-02-13 03:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37][root][INFO] - Training Epoch: 1/2, step 6162/7134 completed (loss: 0.05667645484209061, acc: 0.9836333990097046)
[2025-02-13 03:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38][root][INFO] - Training Epoch: 1/2, step 6163/7134 completed (loss: 0.10771071165800095, acc: 0.9785714149475098)
[2025-02-13 03:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38][root][INFO] - Training Epoch: 1/2, step 6164/7134 completed (loss: 0.0954965278506279, acc: 0.9787535667419434)
[2025-02-13 03:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:39][root][INFO] - Training Epoch: 1/2, step 6165/7134 completed (loss: 0.07736269384622574, acc: 0.9760147333145142)
[2025-02-13 03:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:39][root][INFO] - Training Epoch: 1/2, step 6166/7134 completed (loss: 0.03916021063923836, acc: 0.9912126660346985)
[2025-02-13 03:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40][root][INFO] - Training Epoch: 1/2, step 6167/7134 completed (loss: 0.03613423556089401, acc: 0.9929278492927551)
[2025-02-13 03:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40][root][INFO] - Training Epoch: 1/2, step 6168/7134 completed (loss: 0.04252159968018532, acc: 0.9884169697761536)
[2025-02-13 03:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40][root][INFO] - Training Epoch: 1/2, step 6169/7134 completed (loss: 0.03930088132619858, acc: 0.991253674030304)
[2025-02-13 03:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:41][root][INFO] - Training Epoch: 1/2, step 6170/7134 completed (loss: 0.03051801025867462, acc: 0.9943289160728455)
[2025-02-13 03:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:41][root][INFO] - Training Epoch: 1/2, step 6171/7134 completed (loss: 0.01703571155667305, acc: 0.9921156167984009)
[2025-02-13 03:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42][root][INFO] - Training Epoch: 1/2, step 6172/7134 completed (loss: 0.025657061487436295, acc: 0.9899799823760986)
[2025-02-13 03:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42][root][INFO] - Training Epoch: 1/2, step 6173/7134 completed (loss: 0.010892171412706375, acc: 0.9970545172691345)
[2025-02-13 03:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42][root][INFO] - Training Epoch: 1/2, step 6174/7134 completed (loss: 0.0052233291789889336, acc: 0.9985337257385254)
[2025-02-13 03:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:43][root][INFO] - Training Epoch: 1/2, step 6175/7134 completed (loss: 0.03656512871384621, acc: 0.9911242723464966)
[2025-02-13 03:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:43][root][INFO] - Training Epoch: 1/2, step 6176/7134 completed (loss: 0.02713681012392044, acc: 0.9898697733879089)
[2025-02-13 03:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44][root][INFO] - Training Epoch: 1/2, step 6177/7134 completed (loss: 0.026779528707265854, acc: 0.9910485744476318)
[2025-02-13 03:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44][root][INFO] - Training Epoch: 1/2, step 6178/7134 completed (loss: 0.017666134983301163, acc: 0.9941002726554871)
[2025-02-13 03:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:45][root][INFO] - Training Epoch: 1/2, step 6179/7134 completed (loss: 0.04829413443803787, acc: 0.9922360181808472)
[2025-02-13 03:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:45][root][INFO] - Training Epoch: 1/2, step 6180/7134 completed (loss: 0.13995283842086792, acc: 0.9585987329483032)
[2025-02-13 03:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:45][root][INFO] - Training Epoch: 1/2, step 6181/7134 completed (loss: 0.2745099663734436, acc: 0.9173553586006165)
[2025-02-13 03:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46][root][INFO] - Training Epoch: 1/2, step 6182/7134 completed (loss: 0.0527002178132534, acc: 0.9795918464660645)
[2025-02-13 03:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46][root][INFO] - Training Epoch: 1/2, step 6183/7134 completed (loss: 0.07708905637264252, acc: 0.9754977226257324)
[2025-02-13 03:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47][root][INFO] - Training Epoch: 1/2, step 6184/7134 completed (loss: 0.12395472079515457, acc: 0.9697580933570862)
[2025-02-13 03:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47][root][INFO] - Training Epoch: 1/2, step 6185/7134 completed (loss: 0.028190195560455322, acc: 0.9889705777168274)
[2025-02-13 03:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47][root][INFO] - Training Epoch: 1/2, step 6186/7134 completed (loss: 0.03185666725039482, acc: 0.9877622127532959)
[2025-02-13 03:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:48][root][INFO] - Training Epoch: 1/2, step 6187/7134 completed (loss: 0.03712272271513939, acc: 0.9894179701805115)
[2025-02-13 03:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:48][root][INFO] - Training Epoch: 1/2, step 6188/7134 completed (loss: 0.0651221051812172, acc: 0.9816642999649048)
[2025-02-13 03:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49][root][INFO] - Training Epoch: 1/2, step 6189/7134 completed (loss: 0.10717234015464783, acc: 0.9754689931869507)
[2025-02-13 03:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49][root][INFO] - Training Epoch: 1/2, step 6190/7134 completed (loss: 0.14071908593177795, acc: 0.960132896900177)
[2025-02-13 03:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:50][root][INFO] - Training Epoch: 1/2, step 6191/7134 completed (loss: 0.060444410890340805, acc: 0.9860140085220337)
[2025-02-13 03:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:50][root][INFO] - Training Epoch: 1/2, step 6192/7134 completed (loss: 0.039363760501146317, acc: 0.986066460609436)
[2025-02-13 03:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51][root][INFO] - Training Epoch: 1/2, step 6193/7134 completed (loss: 0.05984608083963394, acc: 0.9833531379699707)
[2025-02-13 03:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51][root][INFO] - Training Epoch: 1/2, step 6194/7134 completed (loss: 0.05056053400039673, acc: 0.9805447459220886)
[2025-02-13 03:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:52][root][INFO] - Training Epoch: 1/2, step 6195/7134 completed (loss: 0.06649072468280792, acc: 0.9760765433311462)
[2025-02-13 03:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:52][root][INFO] - Training Epoch: 1/2, step 6196/7134 completed (loss: 0.02462889440357685, acc: 0.9917355179786682)
[2025-02-13 03:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:52][root][INFO] - Training Epoch: 1/2, step 6197/7134 completed (loss: 0.08090038597583771, acc: 0.9765517115592957)
[2025-02-13 03:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:53][root][INFO] - Training Epoch: 1/2, step 6198/7134 completed (loss: 0.03718460351228714, acc: 0.9875518679618835)
[2025-02-13 03:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:53][root][INFO] - Training Epoch: 1/2, step 6199/7134 completed (loss: 0.042812567204236984, acc: 0.9872390031814575)
[2025-02-13 03:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54][root][INFO] - Training Epoch: 1/2, step 6200/7134 completed (loss: 0.029202070087194443, acc: 0.9914634227752686)
[2025-02-13 03:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54][root][INFO] - Training Epoch: 1/2, step 6201/7134 completed (loss: 0.02213391847908497, acc: 0.994425892829895)
[2025-02-13 03:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:55][root][INFO] - Training Epoch: 1/2, step 6202/7134 completed (loss: 0.04472784698009491, acc: 0.989154040813446)
[2025-02-13 03:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:55][root][INFO] - Training Epoch: 1/2, step 6203/7134 completed (loss: 0.03169870004057884, acc: 0.9910714030265808)
[2025-02-13 03:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:55][root][INFO] - Training Epoch: 1/2, step 6204/7134 completed (loss: 0.025924453511834145, acc: 0.9960707426071167)
[2025-02-13 03:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56][root][INFO] - Training Epoch: 1/2, step 6205/7134 completed (loss: 0.030973242595791817, acc: 0.9914675951004028)
[2025-02-13 03:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56][root][INFO] - Training Epoch: 1/2, step 6206/7134 completed (loss: 0.02622304856777191, acc: 0.9898697733879089)
[2025-02-13 03:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:57][root][INFO] - Training Epoch: 1/2, step 6207/7134 completed (loss: 0.12024062126874924, acc: 0.9719626307487488)
[2025-02-13 03:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:57][root][INFO] - Training Epoch: 1/2, step 6208/7134 completed (loss: 0.044879250228405, acc: 0.9872685074806213)
[2025-02-13 03:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:58][root][INFO] - Training Epoch: 1/2, step 6209/7134 completed (loss: 0.041826680302619934, acc: 0.9883570671081543)
[2025-02-13 03:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:58][root][INFO] - Training Epoch: 1/2, step 6210/7134 completed (loss: 0.05460132285952568, acc: 0.9835164546966553)
[2025-02-13 03:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59][root][INFO] - Training Epoch: 1/2, step 6211/7134 completed (loss: 0.03764111176133156, acc: 0.9905277490615845)
[2025-02-13 03:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59][root][INFO] - Training Epoch: 1/2, step 6212/7134 completed (loss: 0.03381464630365372, acc: 0.9894737005233765)
[2025-02-13 03:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59][root][INFO] - Training Epoch: 1/2, step 6213/7134 completed (loss: 0.01653851568698883, acc: 0.9937629699707031)
[2025-02-13 03:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:00][root][INFO] - Training Epoch: 1/2, step 6214/7134 completed (loss: 0.02671031281352043, acc: 0.9922978281974792)
[2025-02-13 03:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:00][root][INFO] - Training Epoch: 1/2, step 6215/7134 completed (loss: 0.04565901681780815, acc: 0.985023021697998)
[2025-02-13 03:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:01][root][INFO] - Training Epoch: 1/2, step 6216/7134 completed (loss: 0.02630198374390602, acc: 0.9872340559959412)
[2025-02-13 03:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:01][root][INFO] - Training Epoch: 1/2, step 6217/7134 completed (loss: 0.03443882241845131, acc: 0.9892617464065552)
[2025-02-13 03:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02][root][INFO] - Training Epoch: 1/2, step 6218/7134 completed (loss: 0.05003099888563156, acc: 0.9878048896789551)
[2025-02-13 03:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02][root][INFO] - Training Epoch: 1/2, step 6219/7134 completed (loss: 0.016234226524829865, acc: 0.9957805871963501)
[2025-02-13 03:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02][root][INFO] - Training Epoch: 1/2, step 6220/7134 completed (loss: 0.06726374477148056, acc: 0.9785714149475098)
[2025-02-13 03:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:03][root][INFO] - Training Epoch: 1/2, step 6221/7134 completed (loss: 0.021229349076747894, acc: 0.9912280440330505)
[2025-02-13 03:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:03][root][INFO] - Training Epoch: 1/2, step 6222/7134 completed (loss: 0.04916631802916527, acc: 0.9877049326896667)
[2025-02-13 03:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:04][root][INFO] - Training Epoch: 1/2, step 6223/7134 completed (loss: 0.04167786240577698, acc: 0.9853479862213135)
[2025-02-13 03:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:04][root][INFO] - Training Epoch: 1/2, step 6224/7134 completed (loss: 0.009088661521673203, acc: 0.9970972537994385)
[2025-02-13 03:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05][root][INFO] - Training Epoch: 1/2, step 6225/7134 completed (loss: 0.027130628004670143, acc: 0.9885203838348389)
[2025-02-13 03:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05][root][INFO] - Training Epoch: 1/2, step 6226/7134 completed (loss: 0.04102589562535286, acc: 0.9790301322937012)
[2025-02-13 03:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05][root][INFO] - Training Epoch: 1/2, step 6227/7134 completed (loss: 0.016383621841669083, acc: 0.9959839582443237)
[2025-02-13 03:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:06][root][INFO] - Training Epoch: 1/2, step 6228/7134 completed (loss: 0.02891395427286625, acc: 0.9901719689369202)
[2025-02-13 03:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:06][root][INFO] - Training Epoch: 1/2, step 6229/7134 completed (loss: 0.07243656367063522, acc: 0.9802784323692322)
[2025-02-13 03:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07][root][INFO] - Training Epoch: 1/2, step 6230/7134 completed (loss: 0.03923073410987854, acc: 0.9887780547142029)
[2025-02-13 03:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07][root][INFO] - Training Epoch: 1/2, step 6231/7134 completed (loss: 0.018657077103853226, acc: 0.9948320388793945)
[2025-02-13 03:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08][root][INFO] - Training Epoch: 1/2, step 6232/7134 completed (loss: 0.022084925323724747, acc: 0.9962121248245239)
[2025-02-13 03:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08][root][INFO] - Training Epoch: 1/2, step 6233/7134 completed (loss: 0.02227717824280262, acc: 0.9932975769042969)
[2025-02-13 03:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:09][root][INFO] - Training Epoch: 1/2, step 6234/7134 completed (loss: 0.05284504592418671, acc: 0.9826202988624573)
[2025-02-13 03:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:09][root][INFO] - Training Epoch: 1/2, step 6235/7134 completed (loss: 0.024785377085208893, acc: 0.9934810996055603)
[2025-02-13 03:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:10][root][INFO] - Training Epoch: 1/2, step 6236/7134 completed (loss: 0.037914689630270004, acc: 0.9836289286613464)
[2025-02-13 03:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:10][root][INFO] - Training Epoch: 1/2, step 6237/7134 completed (loss: 0.0382736437022686, acc: 0.9847095012664795)
[2025-02-13 03:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:10][root][INFO] - Training Epoch: 1/2, step 6238/7134 completed (loss: 0.011611168272793293, acc: 0.9967690110206604)
[2025-02-13 03:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11][root][INFO] - Training Epoch: 1/2, step 6239/7134 completed (loss: 0.031366992741823196, acc: 0.9952210187911987)
[2025-02-13 03:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11][root][INFO] - Training Epoch: 1/2, step 6240/7134 completed (loss: 0.004705739673227072, acc: 1.0)
[2025-02-13 03:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:12][root][INFO] - Training Epoch: 1/2, step 6241/7134 completed (loss: 0.020177876576781273, acc: 0.9920886158943176)
[2025-02-13 03:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:12][root][INFO] - Training Epoch: 1/2, step 6242/7134 completed (loss: 0.014878441579639912, acc: 0.9982993006706238)
[2025-02-13 03:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13][root][INFO] - Training Epoch: 1/2, step 6243/7134 completed (loss: 0.020284628495573997, acc: 0.9920381903648376)
[2025-02-13 03:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13][root][INFO] - Training Epoch: 1/2, step 6244/7134 completed (loss: 0.017728080973029137, acc: 0.9958847761154175)
[2025-02-13 03:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13][root][INFO] - Training Epoch: 1/2, step 6245/7134 completed (loss: 0.02374218963086605, acc: 0.99589604139328)
[2025-02-13 03:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:14][root][INFO] - Training Epoch: 1/2, step 6246/7134 completed (loss: 0.022330747917294502, acc: 0.9947299361228943)
[2025-02-13 03:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:14][root][INFO] - Training Epoch: 1/2, step 6247/7134 completed (loss: 0.04038530960679054, acc: 0.9867469668388367)
[2025-02-13 03:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15][root][INFO] - Training Epoch: 1/2, step 6248/7134 completed (loss: 0.06342960149049759, acc: 0.9797507524490356)
[2025-02-13 03:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15][root][INFO] - Training Epoch: 1/2, step 6249/7134 completed (loss: 0.010229331441223621, acc: 0.9968553185462952)
[2025-02-13 03:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15][root][INFO] - Training Epoch: 1/2, step 6250/7134 completed (loss: 0.004389793612062931, acc: 0.9982993006706238)
[2025-02-13 03:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:16][root][INFO] - Training Epoch: 1/2, step 6251/7134 completed (loss: 0.02348935604095459, acc: 0.9923312664031982)
[2025-02-13 03:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:16][root][INFO] - Training Epoch: 1/2, step 6252/7134 completed (loss: 0.017571598291397095, acc: 0.9951456189155579)
[2025-02-13 03:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17][root][INFO] - Training Epoch: 1/2, step 6253/7134 completed (loss: 0.021363254636526108, acc: 0.9948275685310364)
[2025-02-13 03:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17][root][INFO] - Training Epoch: 1/2, step 6254/7134 completed (loss: 0.03270856663584709, acc: 0.9930796027183533)
[2025-02-13 03:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:18][root][INFO] - Training Epoch: 1/2, step 6255/7134 completed (loss: 0.024356946349143982, acc: 0.9905149340629578)
[2025-02-13 03:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:18][root][INFO] - Training Epoch: 1/2, step 6256/7134 completed (loss: 0.016276700422167778, acc: 0.9900709390640259)
[2025-02-13 03:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:18][root][INFO] - Training Epoch: 1/2, step 6257/7134 completed (loss: 0.015843668952584267, acc: 0.9899280667304993)
[2025-02-13 03:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:19][root][INFO] - Training Epoch: 1/2, step 6258/7134 completed (loss: 0.025951864197850227, acc: 0.9936000108718872)
[2025-02-13 03:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:19][root][INFO] - Training Epoch: 1/2, step 6259/7134 completed (loss: 0.014911383390426636, acc: 0.9939939975738525)
[2025-02-13 03:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20][root][INFO] - Training Epoch: 1/2, step 6260/7134 completed (loss: 0.01794646866619587, acc: 0.994966447353363)
[2025-02-13 03:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20][root][INFO] - Training Epoch: 1/2, step 6261/7134 completed (loss: 0.0236198753118515, acc: 0.9954268336296082)
[2025-02-13 03:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21][root][INFO] - Training Epoch: 1/2, step 6262/7134 completed (loss: 0.007011209614574909, acc: 0.9985652565956116)
[2025-02-13 03:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21][root][INFO] - Training Epoch: 1/2, step 6263/7134 completed (loss: 0.008547507226467133, acc: 0.9970717430114746)
[2025-02-13 03:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21][root][INFO] - Training Epoch: 1/2, step 6264/7134 completed (loss: 0.015504013746976852, acc: 0.9950739145278931)
[2025-02-13 03:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22][root][INFO] - Training Epoch: 1/2, step 6265/7134 completed (loss: 0.010878331959247589, acc: 0.9970414042472839)
[2025-02-13 03:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22][root][INFO] - Training Epoch: 1/2, step 6266/7134 completed (loss: 0.006791349966078997, acc: 0.998251736164093)
[2025-02-13 03:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:23][root][INFO] - Training Epoch: 1/2, step 6267/7134 completed (loss: 0.02036246284842491, acc: 0.9967426657676697)
[2025-02-13 03:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:23][root][INFO] - Training Epoch: 1/2, step 6268/7134 completed (loss: 0.021277273073792458, acc: 0.9970238208770752)
[2025-02-13 03:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24][root][INFO] - Training Epoch: 1/2, step 6269/7134 completed (loss: 0.02178282104432583, acc: 0.9957355856895447)
[2025-02-13 03:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24][root][INFO] - Training Epoch: 1/2, step 6270/7134 completed (loss: 0.013601171784102917, acc: 0.995398759841919)
[2025-02-13 03:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24][root][INFO] - Training Epoch: 1/2, step 6271/7134 completed (loss: 0.026575159281492233, acc: 0.9919785857200623)
[2025-02-13 03:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:25][root][INFO] - Training Epoch: 1/2, step 6272/7134 completed (loss: 0.0026454173494130373, acc: 1.0)
[2025-02-13 03:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:25][root][INFO] - Training Epoch: 1/2, step 6273/7134 completed (loss: 0.008525409735739231, acc: 0.99863201379776)
[2025-02-13 03:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26][root][INFO] - Training Epoch: 1/2, step 6274/7134 completed (loss: 0.01458757370710373, acc: 0.9971469044685364)
[2025-02-13 03:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26][root][INFO] - Training Epoch: 1/2, step 6275/7134 completed (loss: 0.03268783539533615, acc: 0.9939117431640625)
[2025-02-13 03:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27][root][INFO] - Training Epoch: 1/2, step 6276/7134 completed (loss: 0.01935967057943344, acc: 0.9912434220314026)
[2025-02-13 03:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27][root][INFO] - Training Epoch: 1/2, step 6277/7134 completed (loss: 0.06623303890228271, acc: 0.9819004535675049)
[2025-02-13 03:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27][root][INFO] - Training Epoch: 1/2, step 6278/7134 completed (loss: 0.00551524618640542, acc: 0.9983552694320679)
[2025-02-13 03:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28][root][INFO] - Training Epoch: 1/2, step 6279/7134 completed (loss: 0.011342670768499374, acc: 0.9948630332946777)
[2025-02-13 03:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28][root][INFO] - Training Epoch: 1/2, step 6280/7134 completed (loss: 0.0245596282184124, acc: 0.9947643876075745)
[2025-02-13 03:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:29][root][INFO] - Training Epoch: 1/2, step 6281/7134 completed (loss: 0.02838202193379402, acc: 0.9948006868362427)
[2025-02-13 03:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:29][root][INFO] - Training Epoch: 1/2, step 6282/7134 completed (loss: 0.010518387891352177, acc: 0.9952830076217651)
[2025-02-13 03:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:29][root][INFO] - Training Epoch: 1/2, step 6283/7134 completed (loss: 0.03647847846150398, acc: 0.9912663698196411)
[2025-02-13 03:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30][root][INFO] - Training Epoch: 1/2, step 6284/7134 completed (loss: 0.056899357587099075, acc: 0.9824945330619812)
[2025-02-13 03:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30][root][INFO] - Training Epoch: 1/2, step 6285/7134 completed (loss: 0.021122628822922707, acc: 0.9921259880065918)
[2025-02-13 03:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:31][root][INFO] - Training Epoch: 1/2, step 6286/7134 completed (loss: 0.011297465302050114, acc: 0.9967105388641357)
[2025-02-13 03:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:31][root][INFO] - Training Epoch: 1/2, step 6287/7134 completed (loss: 0.03655419498682022, acc: 0.9908088445663452)
[2025-02-13 03:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:31][root][INFO] - Training Epoch: 1/2, step 6288/7134 completed (loss: 0.007388581521809101, acc: 0.9950980544090271)
[2025-02-13 03:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32][root][INFO] - Training Epoch: 1/2, step 6289/7134 completed (loss: 0.02551945671439171, acc: 0.9909909963607788)
[2025-02-13 03:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32][root][INFO] - Training Epoch: 1/2, step 6290/7134 completed (loss: 0.0200120210647583, acc: 0.9936000108718872)
[2025-02-13 03:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33][root][INFO] - Training Epoch: 1/2, step 6291/7134 completed (loss: 0.01267203502357006, acc: 0.9927272796630859)
[2025-02-13 03:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33][root][INFO] - Training Epoch: 1/2, step 6292/7134 completed (loss: 0.04032015800476074, acc: 0.9886105060577393)
[2025-02-13 03:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33][root][INFO] - Training Epoch: 1/2, step 6293/7134 completed (loss: 0.06105032563209534, acc: 0.9778156876564026)
[2025-02-13 03:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:34][root][INFO] - Training Epoch: 1/2, step 6294/7134 completed (loss: 0.021499119699001312, acc: 0.9935170412063599)
[2025-02-13 03:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:34][root][INFO] - Training Epoch: 1/2, step 6295/7134 completed (loss: 0.017373880371451378, acc: 0.994575023651123)
[2025-02-13 03:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:35][root][INFO] - Training Epoch: 1/2, step 6296/7134 completed (loss: 0.06302694976329803, acc: 0.9862204790115356)
[2025-02-13 03:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:35][root][INFO] - Training Epoch: 1/2, step 6297/7134 completed (loss: 0.03285954147577286, acc: 0.9901639223098755)
[2025-02-13 03:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36][root][INFO] - Training Epoch: 1/2, step 6298/7134 completed (loss: 0.049001436680555344, acc: 0.9828178882598877)
[2025-02-13 03:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36][root][INFO] - Training Epoch: 1/2, step 6299/7134 completed (loss: 0.08401653170585632, acc: 0.9792332053184509)
[2025-02-13 03:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36][root][INFO] - Training Epoch: 1/2, step 6300/7134 completed (loss: 0.052149754017591476, acc: 0.9882746934890747)
[2025-02-13 03:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:37][root][INFO] - Training Epoch: 1/2, step 6301/7134 completed (loss: 0.01902667246758938, acc: 0.9931034445762634)
[2025-02-13 03:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:37][root][INFO] - Training Epoch: 1/2, step 6302/7134 completed (loss: 0.01224891934543848, acc: 0.9972375631332397)
[2025-02-13 03:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38][root][INFO] - Training Epoch: 1/2, step 6303/7134 completed (loss: 0.015629447996616364, acc: 0.9957401752471924)
[2025-02-13 03:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38][root][INFO] - Training Epoch: 1/2, step 6304/7134 completed (loss: 0.017264174297451973, acc: 0.9918919205665588)
[2025-02-13 03:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:39][root][INFO] - Training Epoch: 1/2, step 6305/7134 completed (loss: 0.010951118543744087, acc: 0.9967266917228699)
[2025-02-13 03:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:39][root][INFO] - Training Epoch: 1/2, step 6306/7134 completed (loss: 0.0433926060795784, acc: 0.989195704460144)
[2025-02-13 03:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:40][root][INFO] - Training Epoch: 1/2, step 6307/7134 completed (loss: 0.057810042053461075, acc: 0.9911816716194153)
[2025-02-13 03:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:40][root][INFO] - Training Epoch: 1/2, step 6308/7134 completed (loss: 0.018578210845589638, acc: 0.9975429773330688)
[2025-02-13 03:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:40][root][INFO] - Training Epoch: 1/2, step 6309/7134 completed (loss: 0.014633647166192532, acc: 0.9936000108718872)
[2025-02-13 03:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:41][root][INFO] - Training Epoch: 1/2, step 6310/7134 completed (loss: 0.059805311262607574, acc: 0.9802197813987732)
[2025-02-13 03:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:41][root][INFO] - Training Epoch: 1/2, step 6311/7134 completed (loss: 0.02835795283317566, acc: 0.9913899302482605)
[2025-02-13 03:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42][root][INFO] - Training Epoch: 1/2, step 6312/7134 completed (loss: 0.034799475222826004, acc: 0.9921383857727051)
[2025-02-13 03:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42][root][INFO] - Training Epoch: 1/2, step 6313/7134 completed (loss: 0.017809897661209106, acc: 0.9975460171699524)
[2025-02-13 03:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:43][root][INFO] - Training Epoch: 1/2, step 6314/7134 completed (loss: 0.016433361917734146, acc: 0.994955837726593)
[2025-02-13 03:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:43][root][INFO] - Training Epoch: 1/2, step 6315/7134 completed (loss: 0.0345999151468277, acc: 0.9934425950050354)
[2025-02-13 03:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:43][root][INFO] - Training Epoch: 1/2, step 6316/7134 completed (loss: 0.027798349037766457, acc: 0.990275502204895)
[2025-02-13 03:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:44][root][INFO] - Training Epoch: 1/2, step 6317/7134 completed (loss: 0.009767319075763226, acc: 0.9967690110206604)
[2025-02-13 03:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:44][root][INFO] - Training Epoch: 1/2, step 6318/7134 completed (loss: 0.009397350251674652, acc: 0.996820330619812)
[2025-02-13 03:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:45][root][INFO] - Training Epoch: 1/2, step 6319/7134 completed (loss: 0.011137068271636963, acc: 0.9958847761154175)
[2025-02-13 03:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:45][root][INFO] - Training Epoch: 1/2, step 6320/7134 completed (loss: 0.024728624150156975, acc: 0.9905213117599487)
[2025-02-13 03:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46][root][INFO] - Training Epoch: 1/2, step 6321/7134 completed (loss: 0.009233078919351101, acc: 0.9966517686843872)
[2025-02-13 03:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46][root][INFO] - Training Epoch: 1/2, step 6322/7134 completed (loss: 0.0395391471683979, acc: 0.992732584476471)
[2025-02-13 03:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:47][root][INFO] - Training Epoch: 1/2, step 6323/7134 completed (loss: 0.03632686287164688, acc: 0.9909909963607788)
[2025-02-13 03:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:47][root][INFO] - Training Epoch: 1/2, step 6324/7134 completed (loss: 0.023462414741516113, acc: 0.9960052967071533)
[2025-02-13 03:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:47][root][INFO] - Training Epoch: 1/2, step 6325/7134 completed (loss: 0.026282357051968575, acc: 0.994339644908905)
[2025-02-13 03:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48][root][INFO] - Training Epoch: 1/2, step 6326/7134 completed (loss: 0.0448899120092392, acc: 0.9887640476226807)
[2025-02-13 03:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48][root][INFO] - Training Epoch: 1/2, step 6327/7134 completed (loss: 0.021872196346521378, acc: 0.9947229623794556)
[2025-02-13 03:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:49][root][INFO] - Training Epoch: 1/2, step 6328/7134 completed (loss: 0.02574784681200981, acc: 0.9904305934906006)
[2025-02-13 03:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:49][root][INFO] - Training Epoch: 1/2, step 6329/7134 completed (loss: 0.04750610888004303, acc: 0.9848484992980957)
[2025-02-13 03:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:50][root][INFO] - Training Epoch: 1/2, step 6330/7134 completed (loss: 0.02985196001827717, acc: 0.9875930547714233)
[2025-02-13 03:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:50][root][INFO] - Training Epoch: 1/2, step 6331/7134 completed (loss: 0.0171514879912138, acc: 0.9964664578437805)
[2025-02-13 03:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:51][root][INFO] - Training Epoch: 1/2, step 6332/7134 completed (loss: 0.08077829331159592, acc: 0.9776951670646667)
[2025-02-13 03:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:51][root][INFO] - Training Epoch: 1/2, step 6333/7134 completed (loss: 0.04071708396077156, acc: 0.9857142567634583)
[2025-02-13 03:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:51][root][INFO] - Training Epoch: 1/2, step 6334/7134 completed (loss: 0.05331778526306152, acc: 0.9867424368858337)
[2025-02-13 03:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52][root][INFO] - Training Epoch: 1/2, step 6335/7134 completed (loss: 0.016389116644859314, acc: 0.9932885766029358)
[2025-02-13 03:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52][root][INFO] - Training Epoch: 1/2, step 6336/7134 completed (loss: 0.041149385273456573, acc: 0.9862385392189026)
[2025-02-13 03:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:53][root][INFO] - Training Epoch: 1/2, step 6337/7134 completed (loss: 0.011129623278975487, acc: 0.9963503479957581)
[2025-02-13 03:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:53][root][INFO] - Training Epoch: 1/2, step 6338/7134 completed (loss: 0.03478044643998146, acc: 0.9873617887496948)
[2025-02-13 03:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:54][root][INFO] - Training Epoch: 1/2, step 6339/7134 completed (loss: 0.04084542393684387, acc: 0.987889289855957)
[2025-02-13 03:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:54][root][INFO] - Training Epoch: 1/2, step 6340/7134 completed (loss: 0.011432524770498276, acc: 0.9956011772155762)
[2025-02-13 03:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:54][root][INFO] - Training Epoch: 1/2, step 6341/7134 completed (loss: 0.015292386524379253, acc: 0.9981024861335754)
[2025-02-13 03:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55][root][INFO] - Training Epoch: 1/2, step 6342/7134 completed (loss: 0.024513719603419304, acc: 0.9919246435165405)
[2025-02-13 03:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55][root][INFO] - Training Epoch: 1/2, step 6343/7134 completed (loss: 0.03595667704939842, acc: 0.9896729588508606)
[2025-02-13 03:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56][root][INFO] - Training Epoch: 1/2, step 6344/7134 completed (loss: 0.02608843892812729, acc: 0.9904240965843201)
[2025-02-13 03:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56][root][INFO] - Training Epoch: 1/2, step 6345/7134 completed (loss: 0.029506416991353035, acc: 0.9931694269180298)
[2025-02-13 03:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57][root][INFO] - Training Epoch: 1/2, step 6346/7134 completed (loss: 0.05604428052902222, acc: 0.9798761606216431)
[2025-02-13 03:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57][root][INFO] - Training Epoch: 1/2, step 6347/7134 completed (loss: 0.03865798935294151, acc: 0.9848484992980957)
[2025-02-13 03:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57][root][INFO] - Training Epoch: 1/2, step 6348/7134 completed (loss: 0.0632079541683197, acc: 0.9809027910232544)
[2025-02-13 03:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58][root][INFO] - Training Epoch: 1/2, step 6349/7134 completed (loss: 0.03326084092259407, acc: 0.9908925294876099)
[2025-02-13 03:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58][root][INFO] - Training Epoch: 1/2, step 6350/7134 completed (loss: 0.039949677884578705, acc: 0.9904761910438538)
[2025-02-13 03:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:59][root][INFO] - Training Epoch: 1/2, step 6351/7134 completed (loss: 0.059238500893116, acc: 0.9865671396255493)
[2025-02-13 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:59][root][INFO] - Training Epoch: 1/2, step 6352/7134 completed (loss: 0.11645092815160751, acc: 0.9762258529663086)
[2025-02-13 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00][root][INFO] - Training Epoch: 1/2, step 6353/7134 completed (loss: 0.030186565592885017, acc: 0.9882869720458984)
[2025-02-13 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00][root][INFO] - Training Epoch: 1/2, step 6354/7134 completed (loss: 0.02023988589644432, acc: 0.9944598078727722)
[2025-02-13 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00][root][INFO] - Training Epoch: 1/2, step 6355/7134 completed (loss: 0.01975409872829914, acc: 0.9951456189155579)
[2025-02-13 03:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01][root][INFO] - Training Epoch: 1/2, step 6356/7134 completed (loss: 0.023335576057434082, acc: 0.994020938873291)
[2025-02-13 03:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01][root][INFO] - Training Epoch: 1/2, step 6357/7134 completed (loss: 0.008540067821741104, acc: 0.9986110925674438)
[2025-02-13 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:02][root][INFO] - Training Epoch: 1/2, step 6358/7134 completed (loss: 0.02741312049329281, acc: 0.9929577708244324)
[2025-02-13 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:02][root][INFO] - Training Epoch: 1/2, step 6359/7134 completed (loss: 0.052285026758909225, acc: 0.9887955188751221)
[2025-02-13 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03][root][INFO] - Training Epoch: 1/2, step 6360/7134 completed (loss: 0.02551230974495411, acc: 0.9920381903648376)
[2025-02-13 03:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03][root][INFO] - Training Epoch: 1/2, step 6361/7134 completed (loss: 0.038077905774116516, acc: 0.989051103591919)
[2025-02-13 03:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04][root][INFO] - Training Epoch: 1/2, step 6362/7134 completed (loss: 0.02460549771785736, acc: 0.9908536672592163)
[2025-02-13 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04][root][INFO] - Training Epoch: 1/2, step 6363/7134 completed (loss: 0.033954523503780365, acc: 0.9882698059082031)
[2025-02-13 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04][root][INFO] - Training Epoch: 1/2, step 6364/7134 completed (loss: 0.01358172856271267, acc: 0.995708167552948)
[2025-02-13 03:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05][root][INFO] - Training Epoch: 1/2, step 6365/7134 completed (loss: 0.012817713432013988, acc: 0.9979674816131592)
[2025-02-13 03:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05][root][INFO] - Training Epoch: 1/2, step 6366/7134 completed (loss: 0.03244931250810623, acc: 0.9907529950141907)
[2025-02-13 03:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:06][root][INFO] - Training Epoch: 1/2, step 6367/7134 completed (loss: 0.03705914691090584, acc: 0.992438554763794)
[2025-02-13 03:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:06][root][INFO] - Training Epoch: 1/2, step 6368/7134 completed (loss: 0.06129053607583046, acc: 0.9901130199432373)
[2025-02-13 03:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07][root][INFO] - Training Epoch: 1/2, step 6369/7134 completed (loss: 0.043415457010269165, acc: 0.9894875288009644)
[2025-02-13 03:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07][root][INFO] - Training Epoch: 1/2, step 6370/7134 completed (loss: 0.1054120659828186, acc: 0.9683377146720886)
[2025-02-13 03:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07][root][INFO] - Training Epoch: 1/2, step 6371/7134 completed (loss: 0.04564885050058365, acc: 0.9889196753501892)
[2025-02-13 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:08][root][INFO] - Training Epoch: 1/2, step 6372/7134 completed (loss: 0.13717670738697052, acc: 0.9616122841835022)
[2025-02-13 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:08][root][INFO] - Training Epoch: 1/2, step 6373/7134 completed (loss: 0.07403989881277084, acc: 0.9799635410308838)
[2025-02-13 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09][root][INFO] - Training Epoch: 1/2, step 6374/7134 completed (loss: 0.019110670313239098, acc: 0.9937759041786194)
[2025-02-13 03:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09][root][INFO] - Training Epoch: 1/2, step 6375/7134 completed (loss: 0.06023566052317619, acc: 0.9807445406913757)
[2025-02-13 03:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10][root][INFO] - Training Epoch: 1/2, step 6376/7134 completed (loss: 0.019055107608437538, acc: 0.9926199316978455)
[2025-02-13 03:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10][root][INFO] - Training Epoch: 1/2, step 6377/7134 completed (loss: 0.0290007833391428, acc: 0.9908424615859985)
[2025-02-13 03:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10][root][INFO] - Training Epoch: 1/2, step 6378/7134 completed (loss: 0.04757923632860184, acc: 0.984994649887085)
[2025-02-13 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:11][root][INFO] - Training Epoch: 1/2, step 6379/7134 completed (loss: 0.03435136005282402, acc: 0.9885786771774292)
[2025-02-13 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:11][root][INFO] - Training Epoch: 1/2, step 6380/7134 completed (loss: 0.06219334527850151, acc: 0.992514967918396)
[2025-02-13 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12][root][INFO] - Training Epoch: 1/2, step 6381/7134 completed (loss: 0.02288001962006092, acc: 0.9938931465148926)
[2025-02-13 03:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12][root][INFO] - Training Epoch: 1/2, step 6382/7134 completed (loss: 0.008507399819791317, acc: 0.9960578083992004)
[2025-02-13 03:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:13][root][INFO] - Training Epoch: 1/2, step 6383/7134 completed (loss: 0.056618932634592056, acc: 0.9905020594596863)
[2025-02-13 03:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:13][root][INFO] - Training Epoch: 1/2, step 6384/7134 completed (loss: 0.04093226417899132, acc: 0.9910314083099365)
[2025-02-13 03:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14][root][INFO] - Training Epoch: 1/2, step 6385/7134 completed (loss: 0.06800082325935364, acc: 0.9868263602256775)
[2025-02-13 03:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14][root][INFO] - Training Epoch: 1/2, step 6386/7134 completed (loss: 0.018594786524772644, acc: 0.9951159954071045)
[2025-02-13 03:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14][root][INFO] - Training Epoch: 1/2, step 6387/7134 completed (loss: 0.062229882925748825, acc: 0.9822580814361572)
[2025-02-13 03:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15][root][INFO] - Training Epoch: 1/2, step 6388/7134 completed (loss: 0.054929219186306, acc: 0.9865269660949707)
[2025-02-13 03:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15][root][INFO] - Training Epoch: 1/2, step 6389/7134 completed (loss: 0.043675411492586136, acc: 0.990231990814209)
[2025-02-13 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16][root][INFO] - Training Epoch: 1/2, step 6390/7134 completed (loss: 0.04872250556945801, acc: 0.9898785352706909)
[2025-02-13 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16][root][INFO] - Training Epoch: 1/2, step 6391/7134 completed (loss: 0.05290376394987106, acc: 0.98975670337677)
[2025-02-13 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:17][root][INFO] - Training Epoch: 1/2, step 6392/7134 completed (loss: 0.0692673847079277, acc: 0.980512797832489)
[2025-02-13 03:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:17][root][INFO] - Training Epoch: 1/2, step 6393/7134 completed (loss: 0.02856675535440445, acc: 0.9886234402656555)
[2025-02-13 03:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18][root][INFO] - Training Epoch: 1/2, step 6394/7134 completed (loss: 0.060880567878484726, acc: 0.976356029510498)
[2025-02-13 03:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18][root][INFO] - Training Epoch: 1/2, step 6395/7134 completed (loss: 0.0369291827082634, acc: 0.9913899302482605)
[2025-02-13 03:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:19][root][INFO] - Training Epoch: 1/2, step 6396/7134 completed (loss: 0.050204798579216, acc: 0.9908257126808167)
[2025-02-13 03:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:19][root][INFO] - Training Epoch: 1/2, step 6397/7134 completed (loss: 0.01992223784327507, acc: 0.996277928352356)
[2025-02-13 03:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20][root][INFO] - Training Epoch: 1/2, step 6398/7134 completed (loss: 0.026277754455804825, acc: 0.9929676651954651)
[2025-02-13 03:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20][root][INFO] - Training Epoch: 1/2, step 6399/7134 completed (loss: 0.05156947672367096, acc: 0.9887217879295349)
[2025-02-13 03:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20][root][INFO] - Training Epoch: 1/2, step 6400/7134 completed (loss: 0.018923111259937286, acc: 0.9971510171890259)
[2025-02-13 03:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21][root][INFO] - Training Epoch: 1/2, step 6401/7134 completed (loss: 0.029468921944499016, acc: 0.9945710897445679)
[2025-02-13 03:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21][root][INFO] - Training Epoch: 1/2, step 6402/7134 completed (loss: 0.04520291090011597, acc: 0.9920544624328613)
[2025-02-13 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:22][root][INFO] - Training Epoch: 1/2, step 6403/7134 completed (loss: 0.03932740166783333, acc: 0.9885203838348389)
[2025-02-13 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:22][root][INFO] - Training Epoch: 1/2, step 6404/7134 completed (loss: 0.018230823799967766, acc: 0.9930459260940552)
[2025-02-13 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:23][root][INFO] - Training Epoch: 1/2, step 6405/7134 completed (loss: 0.016500500962138176, acc: 0.9956989288330078)
[2025-02-13 03:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:23][root][INFO] - Training Epoch: 1/2, step 6406/7134 completed (loss: 0.033568043261766434, acc: 0.9908779859542847)
[2025-02-13 03:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24][root][INFO] - Training Epoch: 1/2, step 6407/7134 completed (loss: 0.01570977084338665, acc: 0.9973718523979187)
[2025-02-13 03:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24][root][INFO] - Training Epoch: 1/2, step 6408/7134 completed (loss: 0.054904013872146606, acc: 0.9881831407546997)
[2025-02-13 03:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:25][root][INFO] - Training Epoch: 1/2, step 6409/7134 completed (loss: 0.02906031534075737, acc: 0.996052622795105)
[2025-02-13 03:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:25][root][INFO] - Training Epoch: 1/2, step 6410/7134 completed (loss: 0.026547515764832497, acc: 0.9944853186607361)
[2025-02-13 03:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:25][root][INFO] - Training Epoch: 1/2, step 6411/7134 completed (loss: 0.03314986079931259, acc: 0.9932998418807983)
[2025-02-13 03:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26][root][INFO] - Training Epoch: 1/2, step 6412/7134 completed (loss: 0.018838834017515182, acc: 0.9919999837875366)
[2025-02-13 03:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26][root][INFO] - Training Epoch: 1/2, step 6413/7134 completed (loss: 0.0448235422372818, acc: 0.9880136847496033)
[2025-02-13 03:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:27][root][INFO] - Training Epoch: 1/2, step 6414/7134 completed (loss: 0.012048293836414814, acc: 0.995468258857727)
[2025-02-13 03:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:27][root][INFO] - Training Epoch: 1/2, step 6415/7134 completed (loss: 0.023671984672546387, acc: 0.9910141229629517)
[2025-02-13 03:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28][root][INFO] - Training Epoch: 1/2, step 6416/7134 completed (loss: 0.04095049947500229, acc: 0.9867172837257385)
[2025-02-13 03:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28][root][INFO] - Training Epoch: 1/2, step 6417/7134 completed (loss: 0.08064896613359451, acc: 0.9859402179718018)
[2025-02-13 03:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28][root][INFO] - Training Epoch: 1/2, step 6418/7134 completed (loss: 0.12700249254703522, acc: 0.9705055952072144)
[2025-02-13 03:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:29][root][INFO] - Training Epoch: 1/2, step 6419/7134 completed (loss: 0.06613896042108536, acc: 0.9885807633399963)
[2025-02-13 03:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:29][root][INFO] - Training Epoch: 1/2, step 6420/7134 completed (loss: 0.04044226184487343, acc: 0.9870316982269287)
[2025-02-13 03:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30][root][INFO] - Training Epoch: 1/2, step 6421/7134 completed (loss: 0.029064323753118515, acc: 0.9910846948623657)
[2025-02-13 03:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30][root][INFO] - Training Epoch: 1/2, step 6422/7134 completed (loss: 0.04671686515212059, acc: 0.9863201379776001)
[2025-02-13 03:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31][root][INFO] - Training Epoch: 1/2, step 6423/7134 completed (loss: 0.06644179672002792, acc: 0.9809384346008301)
[2025-02-13 03:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31][root][INFO] - Training Epoch: 1/2, step 6424/7134 completed (loss: 0.048782337456941605, acc: 0.9861286282539368)
[2025-02-13 03:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32][root][INFO] - Training Epoch: 1/2, step 6425/7134 completed (loss: 0.04205746948719025, acc: 0.9895697236061096)
[2025-02-13 03:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32][root][INFO] - Training Epoch: 1/2, step 6426/7134 completed (loss: 0.046581439673900604, acc: 0.9854881167411804)
[2025-02-13 03:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32][root][INFO] - Training Epoch: 1/2, step 6427/7134 completed (loss: 0.04451838880777359, acc: 0.9914772510528564)
[2025-02-13 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33][root][INFO] - Training Epoch: 1/2, step 6428/7134 completed (loss: 0.02417512983083725, acc: 0.99301677942276)
[2025-02-13 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33][root][INFO] - Training Epoch: 1/2, step 6429/7134 completed (loss: 0.015397005714476109, acc: 0.9969696998596191)
[2025-02-13 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34][root][INFO] - Training Epoch: 1/2, step 6430/7134 completed (loss: 0.054213136434555054, acc: 0.9856114983558655)
[2025-02-13 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34][root][INFO] - Training Epoch: 1/2, step 6431/7134 completed (loss: 0.05964032560586929, acc: 0.9855263233184814)
[2025-02-13 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35][root][INFO] - Training Epoch: 1/2, step 6432/7134 completed (loss: 0.19485649466514587, acc: 0.9528619647026062)
[2025-02-13 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35][root][INFO] - Training Epoch: 1/2, step 6433/7134 completed (loss: 0.04425409436225891, acc: 0.9877049326896667)
[2025-02-13 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36][root][INFO] - Training Epoch: 1/2, step 6434/7134 completed (loss: 0.009098788723349571, acc: 0.9986684322357178)
[2025-02-13 03:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36][root][INFO] - Training Epoch: 1/2, step 6435/7134 completed (loss: 0.046510640531778336, acc: 0.9878683090209961)
[2025-02-13 03:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36][root][INFO] - Training Epoch: 1/2, step 6436/7134 completed (loss: 0.011619563214480877, acc: 1.0)
[2025-02-13 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:37][root][INFO] - Training Epoch: 1/2, step 6437/7134 completed (loss: 0.04267515242099762, acc: 0.9900000095367432)
[2025-02-13 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:37][root][INFO] - Training Epoch: 1/2, step 6438/7134 completed (loss: 0.036730144172906876, acc: 0.990111231803894)
[2025-02-13 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38][root][INFO] - Training Epoch: 1/2, step 6439/7134 completed (loss: 0.04337019473314285, acc: 0.98740154504776)
[2025-02-13 03:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38][root][INFO] - Training Epoch: 1/2, step 6440/7134 completed (loss: 0.03154522553086281, acc: 0.9892473220825195)
[2025-02-13 03:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:39][root][INFO] - Training Epoch: 1/2, step 6441/7134 completed (loss: 0.021742288023233414, acc: 0.9926144480705261)
[2025-02-13 03:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:39][root][INFO] - Training Epoch: 1/2, step 6442/7134 completed (loss: 0.03517039865255356, acc: 0.9931895732879639)
[2025-02-13 03:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:40][root][INFO] - Training Epoch: 1/2, step 6443/7134 completed (loss: 0.023078924044966698, acc: 0.9938119053840637)
[2025-02-13 03:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:40][root][INFO] - Training Epoch: 1/2, step 6444/7134 completed (loss: 0.0358877032995224, acc: 0.9860334992408752)
[2025-02-13 03:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:40][root][INFO] - Training Epoch: 1/2, step 6445/7134 completed (loss: 0.05304456502199173, acc: 0.9901356101036072)
[2025-02-13 03:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:41][root][INFO] - Training Epoch: 1/2, step 6446/7134 completed (loss: 0.023560350760817528, acc: 0.9940405488014221)
[2025-02-13 03:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:41][root][INFO] - Training Epoch: 1/2, step 6447/7134 completed (loss: 0.03071337193250656, acc: 0.9896432757377625)
[2025-02-13 03:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42][root][INFO] - Training Epoch: 1/2, step 6448/7134 completed (loss: 0.08218473941087723, acc: 0.9828660488128662)
[2025-02-13 03:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42][root][INFO] - Training Epoch: 1/2, step 6449/7134 completed (loss: 0.022936545312404633, acc: 0.9956521987915039)
[2025-02-13 03:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:43][root][INFO] - Training Epoch: 1/2, step 6450/7134 completed (loss: 0.021504845470190048, acc: 0.9940476417541504)
[2025-02-13 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:43][root][INFO] - Training Epoch: 1/2, step 6451/7134 completed (loss: 0.04519467055797577, acc: 0.9882467985153198)
[2025-02-13 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:44][root][INFO] - Training Epoch: 1/2, step 6452/7134 completed (loss: 0.012237015180289745, acc: 0.9966555237770081)
[2025-02-13 03:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:44][root][INFO] - Training Epoch: 1/2, step 6453/7134 completed (loss: 0.024989692494273186, acc: 0.9946236610412598)
[2025-02-13 03:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45][root][INFO] - Training Epoch: 1/2, step 6454/7134 completed (loss: 0.02835540473461151, acc: 0.9952885508537292)
[2025-02-13 03:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45][root][INFO] - Training Epoch: 1/2, step 6455/7134 completed (loss: 0.015623213723301888, acc: 0.9949066042900085)
[2025-02-13 03:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:46][root][INFO] - Training Epoch: 1/2, step 6456/7134 completed (loss: 0.022686418145895004, acc: 0.995398759841919)
[2025-02-13 03:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:46][root][INFO] - Training Epoch: 1/2, step 6457/7134 completed (loss: 0.014973152428865433, acc: 0.9977973699569702)
[2025-02-13 03:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:46][root][INFO] - Training Epoch: 1/2, step 6458/7134 completed (loss: 0.02870362251996994, acc: 0.9899159669876099)
[2025-02-13 03:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:47][root][INFO] - Training Epoch: 1/2, step 6459/7134 completed (loss: 0.0019993511959910393, acc: 1.0)
[2025-02-13 03:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:47][root][INFO] - Training Epoch: 1/2, step 6460/7134 completed (loss: 0.014723783358931541, acc: 0.995277464389801)
[2025-02-13 03:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:48][root][INFO] - Training Epoch: 1/2, step 6461/7134 completed (loss: 0.016536660492420197, acc: 0.9921156167984009)
[2025-02-13 03:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:48][root][INFO] - Training Epoch: 1/2, step 6462/7134 completed (loss: 0.015591626055538654, acc: 0.9913899302482605)
[2025-02-13 03:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49][root][INFO] - Training Epoch: 1/2, step 6463/7134 completed (loss: 0.011988692916929722, acc: 0.9943052530288696)
[2025-02-13 03:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49][root][INFO] - Training Epoch: 1/2, step 6464/7134 completed (loss: 0.020016707479953766, acc: 0.9934980273246765)
[2025-02-13 03:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:50][root][INFO] - Training Epoch: 1/2, step 6465/7134 completed (loss: 0.015866080299019814, acc: 0.9935732483863831)
[2025-02-13 03:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:50][root][INFO] - Training Epoch: 1/2, step 6466/7134 completed (loss: 0.009955283254384995, acc: 0.997357964515686)
[2025-02-13 03:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:50][root][INFO] - Training Epoch: 1/2, step 6467/7134 completed (loss: 0.03240837901830673, acc: 0.9938931465148926)
[2025-02-13 03:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51][root][INFO] - Training Epoch: 1/2, step 6468/7134 completed (loss: 0.023226849734783173, acc: 0.9924952983856201)
[2025-02-13 03:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51][root][INFO] - Training Epoch: 1/2, step 6469/7134 completed (loss: 0.03216223046183586, acc: 0.9902557730674744)
[2025-02-13 03:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:52][root][INFO] - Training Epoch: 1/2, step 6470/7134 completed (loss: 0.03873801231384277, acc: 0.9926650524139404)
[2025-02-13 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:52][root][INFO] - Training Epoch: 1/2, step 6471/7134 completed (loss: 0.006803876254707575, acc: 0.9983713626861572)
[2025-02-13 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53][root][INFO] - Training Epoch: 1/2, step 6472/7134 completed (loss: 0.05130336806178093, acc: 0.9905437231063843)
[2025-02-13 03:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53][root][INFO] - Training Epoch: 1/2, step 6473/7134 completed (loss: 0.0066286311484873295, acc: 0.9986594915390015)
[2025-02-13 03:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54][root][INFO] - Training Epoch: 1/2, step 6474/7134 completed (loss: 0.010932818055152893, acc: 0.9946164488792419)
[2025-02-13 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54][root][INFO] - Training Epoch: 1/2, step 6475/7134 completed (loss: 0.039739806205034256, acc: 0.9916666746139526)
[2025-02-13 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54][root][INFO] - Training Epoch: 1/2, step 6476/7134 completed (loss: 0.05931718274950981, acc: 0.9884488582611084)
[2025-02-13 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55][root][INFO] - Training Epoch: 1/2, step 6477/7134 completed (loss: 0.024758202955126762, acc: 0.9936440587043762)
[2025-02-13 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55][root][INFO] - Training Epoch: 1/2, step 6478/7134 completed (loss: 0.025454778224229813, acc: 0.9944649338722229)
[2025-02-13 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56][root][INFO] - Training Epoch: 1/2, step 6479/7134 completed (loss: 0.0207388773560524, acc: 0.9949748516082764)
[2025-02-13 03:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56][root][INFO] - Training Epoch: 1/2, step 6480/7134 completed (loss: 0.06617126613855362, acc: 0.9845361113548279)
[2025-02-13 03:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56][root][INFO] - Training Epoch: 1/2, step 6481/7134 completed (loss: 0.008070198819041252, acc: 0.9965986609458923)
[2025-02-13 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57][root][INFO] - Training Epoch: 1/2, step 6482/7134 completed (loss: 0.018711594864726067, acc: 0.9925000071525574)
[2025-02-13 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57][root][INFO] - Training Epoch: 1/2, step 6483/7134 completed (loss: 0.07684963941574097, acc: 0.9819494485855103)
[2025-02-13 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:58][root][INFO] - Training Epoch: 1/2, step 6484/7134 completed (loss: 0.034081749618053436, acc: 0.992668628692627)
[2025-02-13 03:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:58][root][INFO] - Training Epoch: 1/2, step 6485/7134 completed (loss: 0.017865227535367012, acc: 0.9940711259841919)
[2025-02-13 03:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59][root][INFO] - Training Epoch: 1/2, step 6486/7134 completed (loss: 0.09736540913581848, acc: 0.9727272987365723)
[2025-02-13 03:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59][root][INFO] - Training Epoch: 1/2, step 6487/7134 completed (loss: 0.1710619181394577, acc: 0.9595141410827637)
[2025-02-13 03:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59][root][INFO] - Training Epoch: 1/2, step 6488/7134 completed (loss: 0.08897572010755539, acc: 0.9657947421073914)
[2025-02-13 03:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:00][root][INFO] - Training Epoch: 1/2, step 6489/7134 completed (loss: 0.09441328048706055, acc: 0.9813432693481445)
[2025-02-13 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:00][root][INFO] - Training Epoch: 1/2, step 6490/7134 completed (loss: 0.04908463731408119, acc: 0.994854211807251)
[2025-02-13 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01][root][INFO] - Training Epoch: 1/2, step 6491/7134 completed (loss: 0.0891987755894661, acc: 0.9811066389083862)
[2025-02-13 03:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01][root][INFO] - Training Epoch: 1/2, step 6492/7134 completed (loss: 0.02888615056872368, acc: 0.9896507263183594)
[2025-02-13 03:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02][root][INFO] - Training Epoch: 1/2, step 6493/7134 completed (loss: 0.06009304150938988, acc: 0.9860050678253174)
[2025-02-13 03:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02][root][INFO] - Training Epoch: 1/2, step 6494/7134 completed (loss: 0.04468981921672821, acc: 0.9870848655700684)
[2025-02-13 03:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02][root][INFO] - Training Epoch: 1/2, step 6495/7134 completed (loss: 0.05110286548733711, acc: 0.9817671775817871)
[2025-02-13 03:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:03][root][INFO] - Training Epoch: 1/2, step 6496/7134 completed (loss: 0.04119309037923813, acc: 0.99370276927948)
[2025-02-13 03:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:03][root][INFO] - Training Epoch: 1/2, step 6497/7134 completed (loss: 0.08133220672607422, acc: 0.9797822833061218)
[2025-02-13 03:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04][root][INFO] - Training Epoch: 1/2, step 6498/7134 completed (loss: 0.019142430275678635, acc: 0.9939024448394775)
[2025-02-13 03:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04][root][INFO] - Training Epoch: 1/2, step 6499/7134 completed (loss: 0.04077470675110817, acc: 0.9887920022010803)
[2025-02-13 03:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:05][root][INFO] - Training Epoch: 1/2, step 6500/7134 completed (loss: 0.06535914540290833, acc: 0.981792688369751)
[2025-02-13 03:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:05][root][INFO] - Training Epoch: 1/2, step 6501/7134 completed (loss: 0.1361074447631836, acc: 0.9654731750488281)
[2025-02-13 03:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06][root][INFO] - Training Epoch: 1/2, step 6502/7134 completed (loss: 0.045942340046167374, acc: 0.9869358539581299)
[2025-02-13 03:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06][root][INFO] - Training Epoch: 1/2, step 6503/7134 completed (loss: 0.037059105932712555, acc: 0.9897360801696777)
[2025-02-13 03:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06][root][INFO] - Training Epoch: 1/2, step 6504/7134 completed (loss: 0.054991722106933594, acc: 0.9842022061347961)
[2025-02-13 03:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:07][root][INFO] - Training Epoch: 1/2, step 6505/7134 completed (loss: 0.08166977018117905, acc: 0.9826086759567261)
[2025-02-13 03:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:07][root][INFO] - Training Epoch: 1/2, step 6506/7134 completed (loss: 0.07955045253038406, acc: 0.9750271439552307)
[2025-02-13 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08][root][INFO] - Training Epoch: 1/2, step 6507/7134 completed (loss: 0.08064030855894089, acc: 0.9818456768989563)
[2025-02-13 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08][root][INFO] - Training Epoch: 1/2, step 6508/7134 completed (loss: 0.061987437307834625, acc: 0.9894366264343262)
[2025-02-13 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09][root][INFO] - Training Epoch: 1/2, step 6509/7134 completed (loss: 0.028149761259555817, acc: 0.9911406636238098)
[2025-02-13 03:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09][root][INFO] - Training Epoch: 1/2, step 6510/7134 completed (loss: 0.051753707230091095, acc: 0.985200822353363)
[2025-02-13 03:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:10][root][INFO] - Training Epoch: 1/2, step 6511/7134 completed (loss: 0.06471109390258789, acc: 0.9879699349403381)
[2025-02-13 03:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:10][root][INFO] - Training Epoch: 1/2, step 6512/7134 completed (loss: 0.028740355744957924, acc: 0.9923954606056213)
[2025-02-13 03:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11][root][INFO] - Training Epoch: 1/2, step 6513/7134 completed (loss: 0.037509940564632416, acc: 0.9907407164573669)
[2025-02-13 03:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11][root][INFO] - Training Epoch: 1/2, step 6514/7134 completed (loss: 0.049494147300720215, acc: 0.9886363744735718)
[2025-02-13 03:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11][root][INFO] - Training Epoch: 1/2, step 6515/7134 completed (loss: 0.04623081535100937, acc: 0.9847715497016907)
[2025-02-13 03:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:12][root][INFO] - Training Epoch: 1/2, step 6516/7134 completed (loss: 0.08270403742790222, acc: 0.9801061153411865)
[2025-02-13 03:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:12][root][INFO] - Training Epoch: 1/2, step 6517/7134 completed (loss: 0.0338030606508255, acc: 0.9868420958518982)
[2025-02-13 03:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13][root][INFO] - Training Epoch: 1/2, step 6518/7134 completed (loss: 0.0384163036942482, acc: 0.9917241334915161)
[2025-02-13 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13][root][INFO] - Training Epoch: 1/2, step 6519/7134 completed (loss: 0.07357395440340042, acc: 0.9893617033958435)
[2025-02-13 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14][root][INFO] - Training Epoch: 1/2, step 6520/7134 completed (loss: 0.022388609126210213, acc: 0.9922239780426025)
[2025-02-13 03:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14][root][INFO] - Training Epoch: 1/2, step 6521/7134 completed (loss: 0.01760001666843891, acc: 0.9921875)
[2025-02-13 03:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14][root][INFO] - Training Epoch: 1/2, step 6522/7134 completed (loss: 0.043886009603738785, acc: 0.9879931211471558)
[2025-02-13 03:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:15][root][INFO] - Training Epoch: 1/2, step 6523/7134 completed (loss: 0.08454349637031555, acc: 0.9782214164733887)
[2025-02-13 03:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:15][root][INFO] - Training Epoch: 1/2, step 6524/7134 completed (loss: 0.07906903326511383, acc: 0.9794344305992126)
[2025-02-13 03:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16][root][INFO] - Training Epoch: 1/2, step 6525/7134 completed (loss: 0.017652910202741623, acc: 0.9919678568840027)
[2025-02-13 03:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16][root][INFO] - Training Epoch: 1/2, step 6526/7134 completed (loss: 0.06202912703156471, acc: 0.9884763360023499)
[2025-02-13 03:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17][root][INFO] - Training Epoch: 1/2, step 6527/7134 completed (loss: 0.037767838686704636, acc: 0.9910979270935059)
[2025-02-13 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17][root][INFO] - Training Epoch: 1/2, step 6528/7134 completed (loss: 0.06420201808214188, acc: 0.9837545156478882)
[2025-02-13 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17][root][INFO] - Training Epoch: 1/2, step 6529/7134 completed (loss: 0.08342567086219788, acc: 0.981203019618988)
[2025-02-13 03:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:18][root][INFO] - Training Epoch: 1/2, step 6530/7134 completed (loss: 0.03207897022366524, acc: 0.9900142550468445)
[2025-02-13 03:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:18][root][INFO] - Training Epoch: 1/2, step 6531/7134 completed (loss: 0.02356668934226036, acc: 0.9942196607589722)
[2025-02-13 03:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19][root][INFO] - Training Epoch: 1/2, step 6532/7134 completed (loss: 0.04263397306203842, acc: 0.987484335899353)
[2025-02-13 03:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19][root][INFO] - Training Epoch: 1/2, step 6533/7134 completed (loss: 0.017010310664772987, acc: 0.9955223798751831)
[2025-02-13 03:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20][root][INFO] - Training Epoch: 1/2, step 6534/7134 completed (loss: 0.05196888744831085, acc: 0.9932975769042969)
[2025-02-13 03:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20][root][INFO] - Training Epoch: 1/2, step 6535/7134 completed (loss: 0.03637086600065231, acc: 0.9882903695106506)
[2025-02-13 03:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20][root][INFO] - Training Epoch: 1/2, step 6536/7134 completed (loss: 0.03832828626036644, acc: 0.9861111044883728)
[2025-02-13 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21][root][INFO] - Training Epoch: 1/2, step 6537/7134 completed (loss: 0.07695794105529785, acc: 0.9646017551422119)
[2025-02-13 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21][root][INFO] - Training Epoch: 1/2, step 6538/7134 completed (loss: 0.07412121444940567, acc: 0.9822379946708679)
[2025-02-13 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:22][root][INFO] - Training Epoch: 1/2, step 6539/7134 completed (loss: 0.014004137367010117, acc: 0.9969879388809204)
[2025-02-13 03:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:22][root][INFO] - Training Epoch: 1/2, step 6540/7134 completed (loss: 0.25948184728622437, acc: 0.9508196711540222)
[2025-02-13 03:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:22][root][INFO] - Training Epoch: 1/2, step 6541/7134 completed (loss: 0.06042656674981117, acc: 0.9803921580314636)
[2025-02-13 03:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23][root][INFO] - Training Epoch: 1/2, step 6542/7134 completed (loss: 0.05319729074835777, acc: 0.9847457408905029)
[2025-02-13 03:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23][root][INFO] - Training Epoch: 1/2, step 6543/7134 completed (loss: 0.012717656791210175, acc: 0.9958071112632751)
[2025-02-13 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:24][root][INFO] - Training Epoch: 1/2, step 6544/7134 completed (loss: 0.04729568958282471, acc: 0.9880715608596802)
[2025-02-13 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:24][root][INFO] - Training Epoch: 1/2, step 6545/7134 completed (loss: 0.028568971902132034, acc: 0.9944238066673279)
[2025-02-13 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25][root][INFO] - Training Epoch: 1/2, step 6546/7134 completed (loss: 0.036393214017152786, acc: 0.990227997303009)
[2025-02-13 03:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25][root][INFO] - Training Epoch: 1/2, step 6547/7134 completed (loss: 0.09409662336111069, acc: 0.9829642176628113)
[2025-02-13 03:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26][root][INFO] - Training Epoch: 1/2, step 6548/7134 completed (loss: 0.017574459314346313, acc: 0.9980158805847168)
[2025-02-13 03:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26][root][INFO] - Training Epoch: 1/2, step 6549/7134 completed (loss: 0.08371388167142868, acc: 0.9786407947540283)
[2025-02-13 03:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26][root][INFO] - Training Epoch: 1/2, step 6550/7134 completed (loss: 0.08721859008073807, acc: 0.9830769300460815)
[2025-02-13 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27][root][INFO] - Training Epoch: 1/2, step 6551/7134 completed (loss: 0.016526775434613228, acc: 0.9943052530288696)
[2025-02-13 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27][root][INFO] - Training Epoch: 1/2, step 6552/7134 completed (loss: 0.02447124756872654, acc: 0.9940239191055298)
[2025-02-13 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:28][root][INFO] - Training Epoch: 1/2, step 6553/7134 completed (loss: 0.03038826957345009, acc: 0.9872773289680481)
[2025-02-13 03:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:28][root][INFO] - Training Epoch: 1/2, step 6554/7134 completed (loss: 0.03722698986530304, acc: 0.9935135245323181)
[2025-02-13 03:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29][root][INFO] - Training Epoch: 1/2, step 6555/7134 completed (loss: 0.05214500427246094, acc: 0.9879275560379028)
[2025-02-13 03:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29][root][INFO] - Training Epoch: 1/2, step 6556/7134 completed (loss: 0.04146778956055641, acc: 0.9873684048652649)
[2025-02-13 03:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:30][root][INFO] - Training Epoch: 1/2, step 6557/7134 completed (loss: 0.034829381853342056, acc: 0.9858871102333069)
[2025-02-13 03:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:30][root][INFO] - Training Epoch: 1/2, step 6558/7134 completed (loss: 0.04825384169816971, acc: 0.9869565367698669)
[2025-02-13 03:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31][root][INFO] - Training Epoch: 1/2, step 6559/7134 completed (loss: 0.0360957570374012, acc: 0.9902912378311157)
[2025-02-13 03:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31][root][INFO] - Training Epoch: 1/2, step 6560/7134 completed (loss: 0.024394534528255463, acc: 0.9916765689849854)
[2025-02-13 03:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32][root][INFO] - Training Epoch: 1/2, step 6561/7134 completed (loss: 0.023961961269378662, acc: 0.9894737005233765)
[2025-02-13 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32][root][INFO] - Training Epoch: 1/2, step 6562/7134 completed (loss: 0.04334930330514908, acc: 0.9835575222969055)
[2025-02-13 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32][root][INFO] - Training Epoch: 1/2, step 6563/7134 completed (loss: 0.03508828580379486, acc: 0.9883585572242737)
[2025-02-13 03:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33][root][INFO] - Training Epoch: 1/2, step 6564/7134 completed (loss: 0.05380616709589958, acc: 0.9865047335624695)
[2025-02-13 03:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33][root][INFO] - Training Epoch: 1/2, step 6565/7134 completed (loss: 0.03545679152011871, acc: 0.9873149991035461)
[2025-02-13 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34][root][INFO] - Training Epoch: 1/2, step 6566/7134 completed (loss: 0.02396014891564846, acc: 0.9895536303520203)
[2025-02-13 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34][root][INFO] - Training Epoch: 1/2, step 6567/7134 completed (loss: 0.018816685304045677, acc: 0.9939209818840027)
[2025-02-13 03:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:35][root][INFO] - Training Epoch: 1/2, step 6568/7134 completed (loss: 0.020099608227610588, acc: 0.9936948418617249)
[2025-02-13 03:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:35][root][INFO] - Training Epoch: 1/2, step 6569/7134 completed (loss: 0.014778882265090942, acc: 0.9967032670974731)
[2025-02-13 03:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36][root][INFO] - Training Epoch: 1/2, step 6570/7134 completed (loss: 0.014467248693108559, acc: 0.996515691280365)
[2025-02-13 03:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36][root][INFO] - Training Epoch: 1/2, step 6571/7134 completed (loss: 0.018162840977311134, acc: 0.9947643876075745)
[2025-02-13 03:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37][root][INFO] - Training Epoch: 1/2, step 6572/7134 completed (loss: 0.04213763400912285, acc: 0.9878048896789551)
[2025-02-13 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37][root][INFO] - Training Epoch: 1/2, step 6573/7134 completed (loss: 0.02558610960841179, acc: 0.9920993447303772)
[2025-02-13 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38][root][INFO] - Training Epoch: 1/2, step 6574/7134 completed (loss: 0.01330221351236105, acc: 0.9948875308036804)
[2025-02-13 03:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38][root][INFO] - Training Epoch: 1/2, step 6575/7134 completed (loss: 0.03335503861308098, acc: 0.988950252532959)
[2025-02-13 03:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:39][root][INFO] - Training Epoch: 1/2, step 6576/7134 completed (loss: 0.01537787914276123, acc: 0.9978991746902466)
[2025-02-13 03:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:39][root][INFO] - Training Epoch: 1/2, step 6577/7134 completed (loss: 0.03665917366743088, acc: 0.9859943985939026)
[2025-02-13 03:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40][root][INFO] - Training Epoch: 1/2, step 6578/7134 completed (loss: 0.023602250963449478, acc: 0.9919928908348083)
[2025-02-13 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40][root][INFO] - Training Epoch: 1/2, step 6579/7134 completed (loss: 0.03495030477643013, acc: 0.9909256100654602)
[2025-02-13 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41][root][INFO] - Training Epoch: 1/2, step 6580/7134 completed (loss: 0.01666913740336895, acc: 0.9952940940856934)
[2025-02-13 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41][root][INFO] - Training Epoch: 1/2, step 6581/7134 completed (loss: 0.025995707139372826, acc: 0.9938499331474304)
[2025-02-13 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41][root][INFO] - Training Epoch: 1/2, step 6582/7134 completed (loss: 0.04224783927202225, acc: 0.9870503544807434)
[2025-02-13 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42][root][INFO] - Training Epoch: 1/2, step 6583/7134 completed (loss: 0.017914194613695145, acc: 0.9942792057991028)
[2025-02-13 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42][root][INFO] - Training Epoch: 1/2, step 6584/7134 completed (loss: 0.042629919946193695, acc: 0.9877601265907288)
[2025-02-13 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:43][root][INFO] - Training Epoch: 1/2, step 6585/7134 completed (loss: 0.03459800407290459, acc: 0.9884868264198303)
[2025-02-13 03:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:43][root][INFO] - Training Epoch: 1/2, step 6586/7134 completed (loss: 0.02723253332078457, acc: 0.9948186278343201)
[2025-02-13 03:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44][root][INFO] - Training Epoch: 1/2, step 6587/7134 completed (loss: 0.07850571721792221, acc: 0.9823113083839417)
[2025-02-13 03:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44][root][INFO] - Training Epoch: 1/2, step 6588/7134 completed (loss: 0.04030522331595421, acc: 0.9911949634552002)
[2025-02-13 03:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45][root][INFO] - Training Epoch: 1/2, step 6589/7134 completed (loss: 0.042191360145807266, acc: 0.9806362390518188)
[2025-02-13 03:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45][root][INFO] - Training Epoch: 1/2, step 6590/7134 completed (loss: 0.036140136420726776, acc: 0.9918509721755981)
[2025-02-13 03:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45][root][INFO] - Training Epoch: 1/2, step 6591/7134 completed (loss: 0.033635713160037994, acc: 0.9900867342948914)
[2025-02-13 03:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46][root][INFO] - Training Epoch: 1/2, step 6592/7134 completed (loss: 0.04989050328731537, acc: 0.9852440357208252)
[2025-02-13 03:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46][root][INFO] - Training Epoch: 1/2, step 6593/7134 completed (loss: 0.05178307369351387, acc: 0.9847792983055115)
[2025-02-13 03:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:47][root][INFO] - Training Epoch: 1/2, step 6594/7134 completed (loss: 0.033348824828863144, acc: 0.9882628917694092)
[2025-02-13 03:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:47][root][INFO] - Training Epoch: 1/2, step 6595/7134 completed (loss: 0.04974232614040375, acc: 0.9896142482757568)
[2025-02-13 03:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48][root][INFO] - Training Epoch: 1/2, step 6596/7134 completed (loss: 0.028797630220651627, acc: 0.9924127459526062)
[2025-02-13 03:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48][root][INFO] - Training Epoch: 1/2, step 6597/7134 completed (loss: 0.04175770655274391, acc: 0.9882075190544128)
[2025-02-13 03:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49][root][INFO] - Training Epoch: 1/2, step 6598/7134 completed (loss: 0.032247234135866165, acc: 0.990604043006897)
[2025-02-13 03:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49][root][INFO] - Training Epoch: 1/2, step 6599/7134 completed (loss: 0.058100625872612, acc: 0.9830917716026306)
[2025-02-13 03:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49][root][INFO] - Training Epoch: 1/2, step 6600/7134 completed (loss: 0.03317060321569443, acc: 0.9889065027236938)
[2025-02-13 03:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50][root][INFO] - Training Epoch: 1/2, step 6601/7134 completed (loss: 0.04576359689235687, acc: 0.9882628917694092)
[2025-02-13 03:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50][root][INFO] - Training Epoch: 1/2, step 6602/7134 completed (loss: 0.03539865463972092, acc: 0.9915764331817627)
[2025-02-13 03:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51][root][INFO] - Training Epoch: 1/2, step 6603/7134 completed (loss: 0.02425418235361576, acc: 0.9929873943328857)
[2025-02-13 03:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51][root][INFO] - Training Epoch: 1/2, step 6604/7134 completed (loss: 0.04445643350481987, acc: 0.9885057210922241)
[2025-02-13 03:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:52][root][INFO] - Training Epoch: 1/2, step 6605/7134 completed (loss: 0.08834044635295868, acc: 0.9751037359237671)
[2025-02-13 03:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:52][root][INFO] - Training Epoch: 1/2, step 6606/7134 completed (loss: 0.039444196969270706, acc: 0.9873737096786499)
[2025-02-13 03:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53][root][INFO] - Training Epoch: 1/2, step 6607/7134 completed (loss: 0.07288995385169983, acc: 0.9867021441459656)
[2025-02-13 03:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53][root][INFO] - Training Epoch: 1/2, step 6608/7134 completed (loss: 0.024258488789200783, acc: 0.9907038807868958)
[2025-02-13 03:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53][root][INFO] - Training Epoch: 1/2, step 6609/7134 completed (loss: 0.04035051912069321, acc: 0.9889975786209106)
[2025-02-13 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54][root][INFO] - Training Epoch: 1/2, step 6610/7134 completed (loss: 0.08084390312433243, acc: 0.9838274717330933)
[2025-02-13 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54][root][INFO] - Training Epoch: 1/2, step 6611/7134 completed (loss: 0.14054694771766663, acc: 0.9692307710647583)
[2025-02-13 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55][root][INFO] - Training Epoch: 1/2, step 6612/7134 completed (loss: 0.05842447280883789, acc: 0.9848307967185974)
[2025-02-13 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55][root][INFO] - Training Epoch: 1/2, step 6613/7134 completed (loss: 0.04198741912841797, acc: 0.988095223903656)
[2025-02-13 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56][root][INFO] - Training Epoch: 1/2, step 6614/7134 completed (loss: 0.0647488534450531, acc: 0.9845984578132629)
[2025-02-13 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56][root][INFO] - Training Epoch: 1/2, step 6615/7134 completed (loss: 0.045186202973127365, acc: 0.9830268621444702)
[2025-02-13 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57][root][INFO] - Training Epoch: 1/2, step 6616/7134 completed (loss: 0.03287249058485031, acc: 0.9906103014945984)
[2025-02-13 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57][root][INFO] - Training Epoch: 1/2, step 6617/7134 completed (loss: 0.036457255482673645, acc: 0.9884726405143738)
[2025-02-13 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57][root][INFO] - Training Epoch: 1/2, step 6618/7134 completed (loss: 0.029410962015390396, acc: 0.9940564632415771)
[2025-02-13 03:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:58][root][INFO] - Training Epoch: 1/2, step 6619/7134 completed (loss: 0.026575513184070587, acc: 0.9932735562324524)
[2025-02-13 03:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:58][root][INFO] - Training Epoch: 1/2, step 6620/7134 completed (loss: 0.03552445396780968, acc: 0.9917469024658203)
[2025-02-13 03:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59][root][INFO] - Training Epoch: 1/2, step 6621/7134 completed (loss: 0.04043351113796234, acc: 0.9838150143623352)
[2025-02-13 03:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59][root][INFO] - Training Epoch: 1/2, step 6622/7134 completed (loss: 0.041680045425891876, acc: 0.9918032884597778)
[2025-02-13 03:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00][root][INFO] - Training Epoch: 1/2, step 6623/7134 completed (loss: 0.023273691534996033, acc: 0.99245285987854)
[2025-02-13 03:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00][root][INFO] - Training Epoch: 1/2, step 6624/7134 completed (loss: 0.03754211589694023, acc: 0.9908854365348816)
[2025-02-13 03:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:01][root][INFO] - Training Epoch: 1/2, step 6625/7134 completed (loss: 0.023811258375644684, acc: 0.9944979548454285)
[2025-02-13 03:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:01][root][INFO] - Training Epoch: 1/2, step 6626/7134 completed (loss: 0.015664158388972282, acc: 0.9951515197753906)
[2025-02-13 03:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02][root][INFO] - Training Epoch: 1/2, step 6627/7134 completed (loss: 0.0245354063808918, acc: 0.9924324154853821)
[2025-02-13 03:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02][root][INFO] - Training Epoch: 1/2, step 6628/7134 completed (loss: 0.05525854602456093, acc: 0.9881656765937805)
[2025-02-13 03:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02][root][INFO] - Training Epoch: 1/2, step 6629/7134 completed (loss: 0.06167441979050636, acc: 0.9871630072593689)
[2025-02-13 03:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03][root][INFO] - Training Epoch: 1/2, step 6630/7134 completed (loss: 0.04488438367843628, acc: 0.9899665713310242)
[2025-02-13 03:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03][root][INFO] - Training Epoch: 1/2, step 6631/7134 completed (loss: 0.03143645077943802, acc: 0.9908257126808167)
[2025-02-13 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04][root][INFO] - Training Epoch: 1/2, step 6632/7134 completed (loss: 0.03121368959546089, acc: 0.993250846862793)
[2025-02-13 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04][root][INFO] - Training Epoch: 1/2, step 6633/7134 completed (loss: 0.04014015942811966, acc: 0.9914634227752686)
[2025-02-13 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:05][root][INFO] - Training Epoch: 1/2, step 6634/7134 completed (loss: 0.05361579358577728, acc: 0.9856887459754944)
[2025-02-13 03:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:05][root][INFO] - Training Epoch: 1/2, step 6635/7134 completed (loss: 0.09632588922977448, acc: 0.9769821166992188)
[2025-02-13 03:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06][root][INFO] - Training Epoch: 1/2, step 6636/7134 completed (loss: 0.040563445538282394, acc: 0.9885844588279724)
[2025-02-13 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06][root][INFO] - Training Epoch: 1/2, step 6637/7134 completed (loss: 0.028030559420585632, acc: 0.9956331849098206)
[2025-02-13 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06][root][INFO] - Training Epoch: 1/2, step 6638/7134 completed (loss: 0.023484714329242706, acc: 0.9949173927307129)
[2025-02-13 03:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07][root][INFO] - Training Epoch: 1/2, step 6639/7134 completed (loss: 0.03302375599741936, acc: 0.9895678162574768)
[2025-02-13 03:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07][root][INFO] - Training Epoch: 1/2, step 6640/7134 completed (loss: 0.019622767344117165, acc: 0.9938744306564331)
[2025-02-13 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08][root][INFO] - Training Epoch: 1/2, step 6641/7134 completed (loss: 0.07159746438264847, acc: 0.9800703525543213)
[2025-02-13 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08][root][INFO] - Training Epoch: 1/2, step 6642/7134 completed (loss: 0.026696572080254555, acc: 0.9924699068069458)
[2025-02-13 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09][root][INFO] - Training Epoch: 1/2, step 6643/7134 completed (loss: 0.022490331903100014, acc: 0.9933949708938599)
[2025-02-13 03:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09][root][INFO] - Training Epoch: 1/2, step 6644/7134 completed (loss: 0.05742362141609192, acc: 0.9877924919128418)
[2025-02-13 03:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10][root][INFO] - Training Epoch: 1/2, step 6645/7134 completed (loss: 0.023869415745139122, acc: 0.9901719689369202)
[2025-02-13 03:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10][root][INFO] - Training Epoch: 1/2, step 6646/7134 completed (loss: 0.03443216532468796, acc: 0.9889867901802063)
[2025-02-13 03:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:11][root][INFO] - Training Epoch: 1/2, step 6647/7134 completed (loss: 0.028290562331676483, acc: 0.991847813129425)
[2025-02-13 03:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:11][root][INFO] - Training Epoch: 1/2, step 6648/7134 completed (loss: 0.10397205501794815, acc: 0.9760000109672546)
[2025-02-13 03:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12][root][INFO] - Training Epoch: 1/2, step 6649/7134 completed (loss: 0.05797781050205231, acc: 0.9846516847610474)
[2025-02-13 03:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12][root][INFO] - Training Epoch: 1/2, step 6650/7134 completed (loss: 0.01753697358071804, acc: 0.9933035969734192)
[2025-02-13 03:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:13][root][INFO] - Training Epoch: 1/2, step 6651/7134 completed (loss: 0.028340380638837814, acc: 0.9959072470664978)
[2025-02-13 03:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:13][root][INFO] - Training Epoch: 1/2, step 6652/7134 completed (loss: 0.03338199481368065, acc: 0.989130437374115)
[2025-02-13 03:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:13][root][INFO] - Training Epoch: 1/2, step 6653/7134 completed (loss: 0.04589371755719185, acc: 0.9868565201759338)
[2025-02-13 03:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14][root][INFO] - Training Epoch: 1/2, step 6654/7134 completed (loss: 0.07900749146938324, acc: 0.9813953638076782)
[2025-02-13 03:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14][root][INFO] - Training Epoch: 1/2, step 6655/7134 completed (loss: 0.04682143032550812, acc: 0.985111653804779)
[2025-02-13 03:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15][root][INFO] - Training Epoch: 1/2, step 6656/7134 completed (loss: 0.040905844420194626, acc: 0.9885495901107788)
[2025-02-13 03:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15][root][INFO] - Training Epoch: 1/2, step 6657/7134 completed (loss: 0.04908550903201103, acc: 0.9850948452949524)
[2025-02-13 03:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16][root][INFO] - Training Epoch: 1/2, step 6658/7134 completed (loss: 0.02376730926334858, acc: 0.9935483932495117)
[2025-02-13 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16][root][INFO] - Training Epoch: 1/2, step 6659/7134 completed (loss: 0.0294738058000803, acc: 0.9934895634651184)
[2025-02-13 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17][root][INFO] - Training Epoch: 1/2, step 6660/7134 completed (loss: 0.02047659642994404, acc: 0.9940476417541504)
[2025-02-13 03:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17][root][INFO] - Training Epoch: 1/2, step 6661/7134 completed (loss: 0.031007394194602966, acc: 0.9913366436958313)
[2025-02-13 03:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:18][root][INFO] - Training Epoch: 1/2, step 6662/7134 completed (loss: 0.05824042111635208, acc: 0.9785407781600952)
[2025-02-13 03:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:18][root][INFO] - Training Epoch: 1/2, step 6663/7134 completed (loss: 0.036309417337179184, acc: 0.9923497438430786)
[2025-02-13 03:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19][root][INFO] - Training Epoch: 1/2, step 6664/7134 completed (loss: 0.0865945816040039, acc: 0.9667896628379822)
[2025-02-13 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19][root][INFO] - Training Epoch: 1/2, step 6665/7134 completed (loss: 0.08344852179288864, acc: 0.9659090638160706)
[2025-02-13 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19][root][INFO] - Training Epoch: 1/2, step 6666/7134 completed (loss: 0.027757076546549797, acc: 0.9923858046531677)
[2025-02-13 03:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20][root][INFO] - Training Epoch: 1/2, step 6667/7134 completed (loss: 0.047770071774721146, acc: 0.982332170009613)
[2025-02-13 03:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20][root][INFO] - Training Epoch: 1/2, step 6668/7134 completed (loss: 0.01735454425215721, acc: 0.9930675625801086)
[2025-02-13 03:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21][root][INFO] - Training Epoch: 1/2, step 6669/7134 completed (loss: 0.02841826342046261, acc: 0.9865771532058716)
[2025-02-13 03:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21][root][INFO] - Training Epoch: 1/2, step 6670/7134 completed (loss: 0.023892464116215706, acc: 0.9933110475540161)
[2025-02-13 03:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21][root][INFO] - Training Epoch: 1/2, step 6671/7134 completed (loss: 0.013508708216249943, acc: 0.9970674514770508)
[2025-02-13 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22][root][INFO] - Training Epoch: 1/2, step 6672/7134 completed (loss: 0.02256171405315399, acc: 0.9924952983856201)
[2025-02-13 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22][root][INFO] - Training Epoch: 1/2, step 6673/7134 completed (loss: 0.022361617535352707, acc: 0.9926578402519226)
[2025-02-13 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23][root][INFO] - Training Epoch: 1/2, step 6674/7134 completed (loss: 0.012309863232076168, acc: 1.0)
[2025-02-13 03:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23][root][INFO] - Training Epoch: 1/2, step 6675/7134 completed (loss: 0.01492774672806263, acc: 0.9950617551803589)
[2025-02-13 03:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24][root][INFO] - Training Epoch: 1/2, step 6676/7134 completed (loss: 0.02088717371225357, acc: 0.9952267408370972)
[2025-02-13 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24][root][INFO] - Training Epoch: 1/2, step 6677/7134 completed (loss: 0.06126033887267113, acc: 0.9866412281990051)
[2025-02-13 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24][root][INFO] - Training Epoch: 1/2, step 6678/7134 completed (loss: 0.015856413170695305, acc: 0.9964285492897034)
[2025-02-13 03:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25][root][INFO] - Training Epoch: 1/2, step 6679/7134 completed (loss: 0.03293641284108162, acc: 0.9912663698196411)
[2025-02-13 03:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25][root][INFO] - Training Epoch: 1/2, step 6680/7134 completed (loss: 0.006235087756067514, acc: 0.9972972869873047)
[2025-02-13 03:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26][root][INFO] - Training Epoch: 1/2, step 6681/7134 completed (loss: 0.014496191404759884, acc: 0.9958419799804688)
[2025-02-13 03:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26][root][INFO] - Training Epoch: 1/2, step 6682/7134 completed (loss: 0.04720024764537811, acc: 0.9929906725883484)
[2025-02-13 03:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26][root][INFO] - Training Epoch: 1/2, step 6683/7134 completed (loss: 0.03661463409662247, acc: 0.9835164546966553)
[2025-02-13 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27][root][INFO] - Training Epoch: 1/2, step 6684/7134 completed (loss: 0.038508664816617966, acc: 0.9928571581840515)
[2025-02-13 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27][root][INFO] - Training Epoch: 1/2, step 6685/7134 completed (loss: 0.034287210553884506, acc: 0.9923664331436157)
[2025-02-13 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28][root][INFO] - Training Epoch: 1/2, step 6686/7134 completed (loss: 0.06476476788520813, acc: 0.9876033067703247)
[2025-02-13 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28][root][INFO] - Training Epoch: 1/2, step 6687/7134 completed (loss: 0.09496452659368515, acc: 0.9722222089767456)
[2025-02-13 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29][root][INFO] - Training Epoch: 1/2, step 6688/7134 completed (loss: 0.10511591285467148, acc: 0.9742268323898315)
[2025-02-13 03:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29][root][INFO] - Training Epoch: 1/2, step 6689/7134 completed (loss: 0.05695955082774162, acc: 0.9821782112121582)
[2025-02-13 03:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29][root][INFO] - Training Epoch: 1/2, step 6690/7134 completed (loss: 0.14012110233306885, acc: 0.9612590670585632)
[2025-02-13 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30][root][INFO] - Training Epoch: 1/2, step 6691/7134 completed (loss: 0.03464203700423241, acc: 0.9918166995048523)
[2025-02-13 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30][root][INFO] - Training Epoch: 1/2, step 6692/7134 completed (loss: 0.04444028437137604, acc: 0.9872881174087524)
[2025-02-13 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31][root][INFO] - Training Epoch: 1/2, step 6693/7134 completed (loss: 0.03794294595718384, acc: 0.9851484894752502)
[2025-02-13 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31][root][INFO] - Training Epoch: 1/2, step 6694/7134 completed (loss: 0.04975653067231178, acc: 0.9847095012664795)
[2025-02-13 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31][root][INFO] - Training Epoch: 1/2, step 6695/7134 completed (loss: 0.04298485442996025, acc: 0.9876881241798401)
[2025-02-13 03:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32][root][INFO] - Training Epoch: 1/2, step 6696/7134 completed (loss: 0.0877113789319992, acc: 0.9814241528511047)
[2025-02-13 03:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32][root][INFO] - Training Epoch: 1/2, step 6697/7134 completed (loss: 0.07665754854679108, acc: 0.9754098653793335)
[2025-02-13 03:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33][root][INFO] - Training Epoch: 1/2, step 6698/7134 completed (loss: 0.023333247750997543, acc: 0.9910979270935059)
[2025-02-13 03:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33][root][INFO] - Training Epoch: 1/2, step 6699/7134 completed (loss: 0.037364646792411804, acc: 0.9908883571624756)
[2025-02-13 03:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34][root][INFO] - Training Epoch: 1/2, step 6700/7134 completed (loss: 0.0885360836982727, acc: 0.970588207244873)
[2025-02-13 03:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34][root][INFO] - Training Epoch: 1/2, step 6701/7134 completed (loss: 0.11420148611068726, acc: 0.9682539701461792)
[2025-02-13 03:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34][root][INFO] - Training Epoch: 1/2, step 6702/7134 completed (loss: 0.0478723868727684, acc: 0.9783890247344971)
[2025-02-13 03:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:35][root][INFO] - Training Epoch: 1/2, step 6703/7134 completed (loss: 0.08522927016019821, acc: 0.9736841917037964)
[2025-02-13 03:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:35][root][INFO] - Training Epoch: 1/2, step 6704/7134 completed (loss: 0.08743643760681152, acc: 0.9754204154014587)
[2025-02-13 03:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36][root][INFO] - Training Epoch: 1/2, step 6705/7134 completed (loss: 0.05642727017402649, acc: 0.9843953251838684)
[2025-02-13 03:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36][root][INFO] - Training Epoch: 1/2, step 6706/7134 completed (loss: 0.02600676752626896, acc: 0.9917355179786682)
[2025-02-13 03:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36][root][INFO] - Training Epoch: 1/2, step 6707/7134 completed (loss: 0.05360034853219986, acc: 0.9842209219932556)
[2025-02-13 03:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37][root][INFO] - Training Epoch: 1/2, step 6708/7134 completed (loss: 0.04552559554576874, acc: 0.9851484894752502)
[2025-02-13 03:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37][root][INFO] - Training Epoch: 1/2, step 6709/7134 completed (loss: 0.04237006604671478, acc: 0.9895209670066833)
[2025-02-13 03:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38][root][INFO] - Training Epoch: 1/2, step 6710/7134 completed (loss: 0.07151708751916885, acc: 0.9853747487068176)
[2025-02-13 03:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38][root][INFO] - Training Epoch: 1/2, step 6711/7134 completed (loss: 0.020141135901212692, acc: 0.9936507940292358)
[2025-02-13 03:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39][root][INFO] - Training Epoch: 1/2, step 6712/7134 completed (loss: 0.04950033873319626, acc: 0.9848713874816895)
[2025-02-13 03:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39][root][INFO] - Training Epoch: 1/2, step 6713/7134 completed (loss: 0.051821451634168625, acc: 0.9826338887214661)
[2025-02-13 03:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:40][root][INFO] - Training Epoch: 1/2, step 6714/7134 completed (loss: 0.04102214798331261, acc: 0.9866270422935486)
[2025-02-13 03:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:40][root][INFO] - Training Epoch: 1/2, step 6715/7134 completed (loss: 0.09182984381914139, acc: 0.9733123779296875)
[2025-02-13 03:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41][root][INFO] - Training Epoch: 1/2, step 6716/7134 completed (loss: 0.016211139038205147, acc: 0.9955089688301086)
[2025-02-13 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41][root][INFO] - Training Epoch: 1/2, step 6717/7134 completed (loss: 0.03244385868310928, acc: 0.990338146686554)
[2025-02-13 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41][root][INFO] - Training Epoch: 1/2, step 6718/7134 completed (loss: 0.062022384256124496, acc: 0.978787899017334)
[2025-02-13 03:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:42][root][INFO] - Training Epoch: 1/2, step 6719/7134 completed (loss: 0.0318702757358551, acc: 0.9869281053543091)
[2025-02-13 03:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:42][root][INFO] - Training Epoch: 1/2, step 6720/7134 completed (loss: 0.061030350625514984, acc: 0.9846368432044983)
[2025-02-13 03:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43][root][INFO] - Training Epoch: 1/2, step 6721/7134 completed (loss: 0.06425128877162933, acc: 0.9843993782997131)
[2025-02-13 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43][root][INFO] - Training Epoch: 1/2, step 6722/7134 completed (loss: 0.06973537802696228, acc: 0.9806678295135498)
[2025-02-13 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43][root][INFO] - Training Epoch: 1/2, step 6723/7134 completed (loss: 0.036932941526174545, acc: 0.987500011920929)
[2025-02-13 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44][root][INFO] - Training Epoch: 1/2, step 6724/7134 completed (loss: 0.03295731917023659, acc: 0.9926793575286865)
[2025-02-13 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44][root][INFO] - Training Epoch: 1/2, step 6725/7134 completed (loss: 0.035533156245946884, acc: 0.988054633140564)
[2025-02-13 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45][root][INFO] - Training Epoch: 1/2, step 6726/7134 completed (loss: 0.08065858483314514, acc: 0.9819276928901672)
[2025-02-13 03:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45][root][INFO] - Training Epoch: 1/2, step 6727/7134 completed (loss: 0.057686228305101395, acc: 0.9841549396514893)
[2025-02-13 03:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45][root][INFO] - Training Epoch: 1/2, step 6728/7134 completed (loss: 0.05590623989701271, acc: 0.9892473220825195)
[2025-02-13 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46][root][INFO] - Training Epoch: 1/2, step 6729/7134 completed (loss: 0.022161727771162987, acc: 0.9959239363670349)
[2025-02-13 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46][root][INFO] - Training Epoch: 1/2, step 6730/7134 completed (loss: 0.058897972106933594, acc: 0.9843013882637024)
[2025-02-13 03:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:47][root][INFO] - Training Epoch: 1/2, step 6731/7134 completed (loss: 0.037903714925050735, acc: 0.9864253401756287)
[2025-02-13 03:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:47][root][INFO] - Training Epoch: 1/2, step 6732/7134 completed (loss: 0.04081803560256958, acc: 0.9858490824699402)
[2025-02-13 03:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48][root][INFO] - Training Epoch: 1/2, step 6733/7134 completed (loss: 0.03433588147163391, acc: 0.9886147975921631)
[2025-02-13 03:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48][root][INFO] - Training Epoch: 1/2, step 6734/7134 completed (loss: 0.05070733278989792, acc: 0.9869158864021301)
[2025-02-13 03:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48][root][INFO] - Training Epoch: 1/2, step 6735/7134 completed (loss: 0.03740677982568741, acc: 0.9907833933830261)
[2025-02-13 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49][root][INFO] - Training Epoch: 1/2, step 6736/7134 completed (loss: 0.047841742634773254, acc: 0.9916434288024902)
[2025-02-13 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49][root][INFO] - Training Epoch: 1/2, step 6737/7134 completed (loss: 0.05777284875512123, acc: 0.9821717739105225)
[2025-02-13 03:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:50][root][INFO] - Training Epoch: 1/2, step 6738/7134 completed (loss: 0.05872936546802521, acc: 0.986940324306488)
[2025-02-13 03:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:50][root][INFO] - Training Epoch: 1/2, step 6739/7134 completed (loss: 0.035118017345666885, acc: 0.9908088445663452)
[2025-02-13 03:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51][root][INFO] - Training Epoch: 1/2, step 6740/7134 completed (loss: 0.054318398237228394, acc: 0.9898132681846619)
[2025-02-13 03:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51][root][INFO] - Training Epoch: 1/2, step 6741/7134 completed (loss: 0.02605440653860569, acc: 0.9898403286933899)
[2025-02-13 03:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51][root][INFO] - Training Epoch: 1/2, step 6742/7134 completed (loss: 0.027387283742427826, acc: 0.9884560108184814)
[2025-02-13 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52][root][INFO] - Training Epoch: 1/2, step 6743/7134 completed (loss: 0.018806535750627518, acc: 0.9959127902984619)
[2025-02-13 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52][root][INFO] - Training Epoch: 1/2, step 6744/7134 completed (loss: 0.025043055415153503, acc: 0.9903846383094788)
[2025-02-13 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53][root][INFO] - Training Epoch: 1/2, step 6745/7134 completed (loss: 0.006216987036168575, acc: 1.0)
[2025-02-13 03:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53][root][INFO] - Training Epoch: 1/2, step 6746/7134 completed (loss: 0.01869913935661316, acc: 0.9947090148925781)
[2025-02-13 03:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:54][root][INFO] - Training Epoch: 1/2, step 6747/7134 completed (loss: 0.02732953615486622, acc: 0.9950310587882996)
[2025-02-13 03:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:54][root][INFO] - Training Epoch: 1/2, step 6748/7134 completed (loss: 0.018711192533373833, acc: 0.9977452158927917)
[2025-02-13 03:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55][root][INFO] - Training Epoch: 1/2, step 6749/7134 completed (loss: 0.01987116038799286, acc: 0.9945533871650696)
[2025-02-13 03:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55][root][INFO] - Training Epoch: 1/2, step 6750/7134 completed (loss: 0.014929926954209805, acc: 0.9976931810379028)
[2025-02-13 03:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55][root][INFO] - Training Epoch: 1/2, step 6751/7134 completed (loss: 0.01784748025238514, acc: 0.9954338073730469)
[2025-02-13 03:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:56][root][INFO] - Training Epoch: 1/2, step 6752/7134 completed (loss: 0.010563784278929234, acc: 0.9970972537994385)
[2025-02-13 03:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:56][root][INFO] - Training Epoch: 1/2, step 6753/7134 completed (loss: 0.021623514592647552, acc: 0.9931192398071289)
[2025-02-13 03:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57][root][INFO] - Training Epoch: 1/2, step 6754/7134 completed (loss: 0.009171340614557266, acc: 0.9963855147361755)
[2025-02-13 03:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57][root][INFO] - Training Epoch: 1/2, step 6755/7134 completed (loss: 0.014756369404494762, acc: 0.9963054060935974)
[2025-02-13 03:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58][root][INFO] - Training Epoch: 1/2, step 6756/7134 completed (loss: 0.010829942300915718, acc: 0.9973190426826477)
[2025-02-13 03:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58][root][INFO] - Training Epoch: 1/2, step 6757/7134 completed (loss: 0.01652776263654232, acc: 0.9923780560493469)
[2025-02-13 03:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:59][root][INFO] - Training Epoch: 1/2, step 6758/7134 completed (loss: 0.011863838881254196, acc: 0.9961190223693848)
[2025-02-13 03:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:59][root][INFO] - Training Epoch: 1/2, step 6759/7134 completed (loss: 0.02899840846657753, acc: 0.9921700358390808)
[2025-02-13 03:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:59][root][INFO] - Training Epoch: 1/2, step 6760/7134 completed (loss: 0.02422887645661831, acc: 0.9934383034706116)
[2025-02-13 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:00][root][INFO] - Training Epoch: 1/2, step 6761/7134 completed (loss: 0.029757853597402573, acc: 0.9891566038131714)
[2025-02-13 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:00][root][INFO] - Training Epoch: 1/2, step 6762/7134 completed (loss: 0.03946898877620697, acc: 0.9914215803146362)
[2025-02-13 03:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01][root][INFO] - Training Epoch: 1/2, step 6763/7134 completed (loss: 0.012556120753288269, acc: 0.9962359070777893)
[2025-02-13 03:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01][root][INFO] - Training Epoch: 1/2, step 6764/7134 completed (loss: 0.019219743087887764, acc: 0.9899425506591797)
[2025-02-13 03:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02][root][INFO] - Training Epoch: 1/2, step 6765/7134 completed (loss: 0.01621990092098713, acc: 0.9933422207832336)
[2025-02-13 03:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02][root][INFO] - Training Epoch: 1/2, step 6766/7134 completed (loss: 0.017676319926977158, acc: 0.9944827556610107)
[2025-02-13 03:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03][root][INFO] - Training Epoch: 1/2, step 6767/7134 completed (loss: 0.05920117348432541, acc: 0.9932432174682617)
[2025-02-13 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03][root][INFO] - Training Epoch: 1/2, step 6768/7134 completed (loss: 0.012769553810358047, acc: 0.9961977005004883)
[2025-02-13 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03][root][INFO] - Training Epoch: 1/2, step 6769/7134 completed (loss: 0.01619895175099373, acc: 0.9938271641731262)
[2025-02-13 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04][root][INFO] - Training Epoch: 1/2, step 6770/7134 completed (loss: 0.051176898181438446, acc: 0.982550323009491)
[2025-02-13 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04][root][INFO] - Training Epoch: 1/2, step 6771/7134 completed (loss: 0.028935808688402176, acc: 0.9905362725257874)
[2025-02-13 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05][root][INFO] - Training Epoch: 1/2, step 6772/7134 completed (loss: 0.07166915386915207, acc: 0.9821746945381165)
[2025-02-13 03:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05][root][INFO] - Training Epoch: 1/2, step 6773/7134 completed (loss: 0.03510139510035515, acc: 0.9873417615890503)
[2025-02-13 03:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06][root][INFO] - Training Epoch: 1/2, step 6774/7134 completed (loss: 0.05586475878953934, acc: 0.9888535141944885)
[2025-02-13 03:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06][root][INFO] - Training Epoch: 1/2, step 6775/7134 completed (loss: 0.02641494944691658, acc: 0.992438554763794)
[2025-02-13 03:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06][root][INFO] - Training Epoch: 1/2, step 6776/7134 completed (loss: 0.03768026456236839, acc: 0.9851064085960388)
[2025-02-13 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07][root][INFO] - Training Epoch: 1/2, step 6777/7134 completed (loss: 0.02541395276784897, acc: 0.991946280002594)
[2025-02-13 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07][root][INFO] - Training Epoch: 1/2, step 6778/7134 completed (loss: 0.042388446629047394, acc: 0.98959881067276)
[2025-02-13 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:08][root][INFO] - Training Epoch: 1/2, step 6779/7134 completed (loss: 0.040424689650535583, acc: 0.9910447597503662)
[2025-02-13 03:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:08][root][INFO] - Training Epoch: 1/2, step 6780/7134 completed (loss: 0.03312059864401817, acc: 0.9925261735916138)
[2025-02-13 03:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09][root][INFO] - Training Epoch: 1/2, step 6781/7134 completed (loss: 0.01833181269466877, acc: 0.9940119981765747)
[2025-02-13 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09][root][INFO] - Training Epoch: 1/2, step 6782/7134 completed (loss: 0.03781985118985176, acc: 0.9930555820465088)
[2025-02-13 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09][root][INFO] - Training Epoch: 1/2, step 6783/7134 completed (loss: 0.03627793490886688, acc: 0.9904030561447144)
[2025-02-13 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:10][root][INFO] - Training Epoch: 1/2, step 6784/7134 completed (loss: 0.018810467794537544, acc: 0.9916805028915405)
[2025-02-13 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:10][root][INFO] - Training Epoch: 1/2, step 6785/7134 completed (loss: 0.01664448156952858, acc: 0.9921383857727051)
[2025-02-13 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11][root][INFO] - Training Epoch: 1/2, step 6786/7134 completed (loss: 0.014852157793939114, acc: 0.9956834316253662)
[2025-02-13 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11][root][INFO] - Training Epoch: 1/2, step 6787/7134 completed (loss: 0.027675369754433632, acc: 0.9881578683853149)
[2025-02-13 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12][root][INFO] - Training Epoch: 1/2, step 6788/7134 completed (loss: 0.017134545370936394, acc: 0.9946042895317078)
[2025-02-13 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12][root][INFO] - Training Epoch: 1/2, step 6789/7134 completed (loss: 0.01783854328095913, acc: 0.9961089491844177)
[2025-02-13 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12][root][INFO] - Training Epoch: 1/2, step 6790/7134 completed (loss: 0.04667210951447487, acc: 0.9899425506591797)
[2025-02-13 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13][root][INFO] - Training Epoch: 1/2, step 6791/7134 completed (loss: 0.022790027782320976, acc: 0.991482138633728)
[2025-02-13 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13][root][INFO] - Training Epoch: 1/2, step 6792/7134 completed (loss: 0.010327384807169437, acc: 0.9981651306152344)
[2025-02-13 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14][root][INFO] - Training Epoch: 1/2, step 6793/7134 completed (loss: 0.0074952091090381145, acc: 0.9957805871963501)
[2025-02-13 03:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14][root][INFO] - Training Epoch: 1/2, step 6794/7134 completed (loss: 0.01785770058631897, acc: 0.9967159032821655)
[2025-02-13 03:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15][root][INFO] - Training Epoch: 1/2, step 6795/7134 completed (loss: 0.00901028048247099, acc: 0.9974026083946228)
[2025-02-13 03:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15][root][INFO] - Training Epoch: 1/2, step 6796/7134 completed (loss: 0.011150470934808254, acc: 0.9970674514770508)
[2025-02-13 03:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15][root][INFO] - Training Epoch: 1/2, step 6797/7134 completed (loss: 0.04650105535984039, acc: 0.9883551597595215)
[2025-02-13 03:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:16][root][INFO] - Training Epoch: 1/2, step 6798/7134 completed (loss: 0.029156967997550964, acc: 0.9903581142425537)
[2025-02-13 03:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:16][root][INFO] - Training Epoch: 1/2, step 6799/7134 completed (loss: 0.0262862928211689, acc: 0.9931600689888)
[2025-02-13 03:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17][root][INFO] - Training Epoch: 1/2, step 6800/7134 completed (loss: 0.028257841244339943, acc: 0.9903314709663391)
[2025-02-13 03:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17][root][INFO] - Training Epoch: 1/2, step 6801/7134 completed (loss: 0.03898363932967186, acc: 0.9921259880065918)
[2025-02-13 03:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:18][root][INFO] - Training Epoch: 1/2, step 6802/7134 completed (loss: 0.017363587394356728, acc: 0.9970717430114746)
[2025-02-13 03:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:18][root][INFO] - Training Epoch: 1/2, step 6803/7134 completed (loss: 0.008977377787232399, acc: 0.9974842667579651)
[2025-02-13 03:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19][root][INFO] - Training Epoch: 1/2, step 6804/7134 completed (loss: 0.032622601836919785, acc: 0.996874988079071)
[2025-02-13 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19][root][INFO] - Training Epoch: 1/2, step 6805/7134 completed (loss: 0.04700421541929245, acc: 0.9903846383094788)
[2025-02-13 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19][root][INFO] - Training Epoch: 1/2, step 6806/7134 completed (loss: 0.05241004750132561, acc: 0.9936708807945251)
[2025-02-13 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:20][root][INFO] - Training Epoch: 1/2, step 6807/7134 completed (loss: 0.014678996056318283, acc: 0.9956896305084229)
[2025-02-13 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:20][root][INFO] - Training Epoch: 1/2, step 6808/7134 completed (loss: 0.03035830333828926, acc: 0.9920254945755005)
[2025-02-13 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21][root][INFO] - Training Epoch: 1/2, step 6809/7134 completed (loss: 0.04939677193760872, acc: 0.9905149340629578)
[2025-02-13 03:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21][root][INFO] - Training Epoch: 1/2, step 6810/7134 completed (loss: 0.010129687376320362, acc: 0.9973683953285217)
[2025-02-13 03:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22][root][INFO] - Training Epoch: 1/2, step 6811/7134 completed (loss: 0.05415225401520729, acc: 0.9908257126808167)
[2025-02-13 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22][root][INFO] - Training Epoch: 1/2, step 6812/7134 completed (loss: 0.02588171698153019, acc: 0.990275502204895)
[2025-02-13 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:23][root][INFO] - Training Epoch: 1/2, step 6813/7134 completed (loss: 0.009340879507362843, acc: 0.997183084487915)
[2025-02-13 03:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:23][root][INFO] - Training Epoch: 1/2, step 6814/7134 completed (loss: 0.021548045799136162, acc: 0.9946236610412598)
[2025-02-13 03:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:23][root][INFO] - Training Epoch: 1/2, step 6815/7134 completed (loss: 0.03253619745373726, acc: 0.9908015727996826)
[2025-02-13 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:24][root][INFO] - Training Epoch: 1/2, step 6816/7134 completed (loss: 0.030856557190418243, acc: 0.9916387796401978)
[2025-02-13 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:24][root][INFO] - Training Epoch: 1/2, step 6817/7134 completed (loss: 0.04330356791615486, acc: 0.9897435903549194)
[2025-02-13 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:25][root][INFO] - Training Epoch: 1/2, step 6818/7134 completed (loss: 0.014512550085783005, acc: 0.995502233505249)
[2025-02-13 03:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:25][root][INFO] - Training Epoch: 1/2, step 6819/7134 completed (loss: 0.038205552846193314, acc: 0.9910072088241577)
[2025-02-13 03:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:26][root][INFO] - Training Epoch: 1/2, step 6820/7134 completed (loss: 0.01725870929658413, acc: 0.9963964223861694)
[2025-02-13 03:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:26][root][INFO] - Training Epoch: 1/2, step 6821/7134 completed (loss: 0.020318541675806046, acc: 0.991919219493866)
[2025-02-13 03:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:26][root][INFO] - Training Epoch: 1/2, step 6822/7134 completed (loss: 0.05813371017575264, acc: 0.9808027744293213)
[2025-02-13 03:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27][root][INFO] - Training Epoch: 1/2, step 6823/7134 completed (loss: 0.03603941574692726, acc: 0.9885550737380981)
[2025-02-13 03:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27][root][INFO] - Training Epoch: 1/2, step 6824/7134 completed (loss: 0.0796835720539093, acc: 0.9737903475761414)
[2025-02-13 03:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28][root][INFO] - Training Epoch: 1/2, step 6825/7134 completed (loss: 0.01430914830416441, acc: 0.997560977935791)
[2025-02-13 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28][root][INFO] - Training Epoch: 1/2, step 6826/7134 completed (loss: 0.0488077774643898, acc: 0.9865319728851318)
[2025-02-13 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29][root][INFO] - Training Epoch: 1/2, step 6827/7134 completed (loss: 0.10963676124811172, acc: 0.9716494679450989)
[2025-02-13 03:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29][root][INFO] - Training Epoch: 1/2, step 6828/7134 completed (loss: 0.050707798451185226, acc: 0.9817517995834351)
[2025-02-13 03:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29][root][INFO] - Training Epoch: 1/2, step 6829/7134 completed (loss: 0.053215645253658295, acc: 0.9807121753692627)
[2025-02-13 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30][root][INFO] - Training Epoch: 1/2, step 6830/7134 completed (loss: 0.04031478986144066, acc: 0.9917627573013306)
[2025-02-13 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30][root][INFO] - Training Epoch: 1/2, step 6831/7134 completed (loss: 0.05560563877224922, acc: 0.9852150678634644)
[2025-02-13 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31][root][INFO] - Training Epoch: 1/2, step 6832/7134 completed (loss: 0.05577406659722328, acc: 0.9852125644683838)
[2025-02-13 03:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31][root][INFO] - Training Epoch: 1/2, step 6833/7134 completed (loss: 0.050936259329319, acc: 0.9855305552482605)
[2025-02-13 03:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32][root][INFO] - Training Epoch: 1/2, step 6834/7134 completed (loss: 0.03812538832426071, acc: 0.9863713979721069)
[2025-02-13 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32][root][INFO] - Training Epoch: 1/2, step 6835/7134 completed (loss: 0.01788008213043213, acc: 0.9935064911842346)
[2025-02-13 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32][root][INFO] - Training Epoch: 1/2, step 6836/7134 completed (loss: 0.03840756416320801, acc: 0.9874371886253357)
[2025-02-13 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33][root][INFO] - Training Epoch: 1/2, step 6837/7134 completed (loss: 0.03948213905096054, acc: 0.9901840686798096)
[2025-02-13 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33][root][INFO] - Training Epoch: 1/2, step 6838/7134 completed (loss: 0.0718475729227066, acc: 0.9800570011138916)
[2025-02-13 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34][root][INFO] - Training Epoch: 1/2, step 6839/7134 completed (loss: 0.1164979487657547, acc: 0.9669967293739319)
[2025-02-13 03:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34][root][INFO] - Training Epoch: 1/2, step 6840/7134 completed (loss: 0.08933965116739273, acc: 0.9756447076797485)
[2025-02-13 03:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35][root][INFO] - Training Epoch: 1/2, step 6841/7134 completed (loss: 0.042177699506282806, acc: 0.9841269850730896)
[2025-02-13 03:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35][root][INFO] - Training Epoch: 1/2, step 6842/7134 completed (loss: 0.028668152168393135, acc: 0.9912280440330505)
[2025-02-13 03:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35][root][INFO] - Training Epoch: 1/2, step 6843/7134 completed (loss: 0.05025528371334076, acc: 0.9827373623847961)
[2025-02-13 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36][root][INFO] - Training Epoch: 1/2, step 6844/7134 completed (loss: 0.030163338407874107, acc: 0.9896373152732849)
[2025-02-13 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36][root][INFO] - Training Epoch: 1/2, step 6845/7134 completed (loss: 0.02837350033223629, acc: 0.9914966225624084)
[2025-02-13 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37][root][INFO] - Training Epoch: 1/2, step 6846/7134 completed (loss: 0.030526963993906975, acc: 0.9928366541862488)
[2025-02-13 03:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37][root][INFO] - Training Epoch: 1/2, step 6847/7134 completed (loss: 0.03973929211497307, acc: 0.9878787994384766)
[2025-02-13 03:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38][root][INFO] - Training Epoch: 1/2, step 6848/7134 completed (loss: 0.025208178907632828, acc: 0.9864864945411682)
[2025-02-13 03:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38][root][INFO] - Training Epoch: 1/2, step 6849/7134 completed (loss: 0.032015930861234665, acc: 0.9965635538101196)
[2025-02-13 03:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38][root][INFO] - Training Epoch: 1/2, step 6850/7134 completed (loss: 0.0350513830780983, acc: 0.9839816689491272)
[2025-02-13 03:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39][root][INFO] - Training Epoch: 1/2, step 6851/7134 completed (loss: 0.050071313977241516, acc: 0.9830747246742249)
[2025-02-13 03:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39][root][INFO] - Training Epoch: 1/2, step 6852/7134 completed (loss: 0.017967691645026207, acc: 0.9912587404251099)
[2025-02-13 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40][root][INFO] - Training Epoch: 1/2, step 6853/7134 completed (loss: 0.04174739122390747, acc: 0.9889349937438965)
[2025-02-13 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40][root][INFO] - Training Epoch: 1/2, step 6854/7134 completed (loss: 0.03980454057455063, acc: 0.9913232326507568)
[2025-02-13 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41][root][INFO] - Training Epoch: 1/2, step 6855/7134 completed (loss: 0.012882059440016747, acc: 1.0)
[2025-02-13 03:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41][root][INFO] - Training Epoch: 1/2, step 6856/7134 completed (loss: 0.00614576181396842, acc: 0.995708167552948)
[2025-02-13 03:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42][root][INFO] - Training Epoch: 1/2, step 6857/7134 completed (loss: 0.0209658145904541, acc: 0.992668628692627)
[2025-02-13 03:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42][root][INFO] - Training Epoch: 1/2, step 6858/7134 completed (loss: 0.0022854539565742016, acc: 1.0)
[2025-02-13 03:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42][root][INFO] - Training Epoch: 1/2, step 6859/7134 completed (loss: 0.006850521080195904, acc: 0.9987195730209351)
[2025-02-13 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43][root][INFO] - Training Epoch: 1/2, step 6860/7134 completed (loss: 0.016589930281043053, acc: 0.9946808218955994)
[2025-02-13 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43][root][INFO] - Training Epoch: 1/2, step 6861/7134 completed (loss: 0.01345590315759182, acc: 0.9982699155807495)
[2025-02-13 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44][root][INFO] - Training Epoch: 1/2, step 6862/7134 completed (loss: 0.039855632930994034, acc: 0.9923809766769409)
[2025-02-13 03:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44][root][INFO] - Training Epoch: 1/2, step 6863/7134 completed (loss: 0.013407890684902668, acc: 0.994350254535675)
[2025-02-13 03:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45][root][INFO] - Training Epoch: 1/2, step 6864/7134 completed (loss: 0.010743727907538414, acc: 0.9950082898139954)
[2025-02-13 03:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45][root][INFO] - Training Epoch: 1/2, step 6865/7134 completed (loss: 0.030297737568616867, acc: 0.991752564907074)
[2025-02-13 03:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45][root][INFO] - Training Epoch: 1/2, step 6866/7134 completed (loss: 0.006117421668022871, acc: 0.9984802603721619)
[2025-02-13 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:46][root][INFO] - Training Epoch: 1/2, step 6867/7134 completed (loss: 0.0031065489165484905, acc: 1.0)
[2025-02-13 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:46][root][INFO] - Training Epoch: 1/2, step 6868/7134 completed (loss: 0.012350471690297127, acc: 0.9944598078727722)
[2025-02-13 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47][root][INFO] - Training Epoch: 1/2, step 6869/7134 completed (loss: 0.00529768830165267, acc: 0.9978586435317993)
[2025-02-13 03:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47][root][INFO] - Training Epoch: 1/2, step 6870/7134 completed (loss: 0.027776945382356644, acc: 0.9967426657676697)
[2025-02-13 03:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47][root][INFO] - Training Epoch: 1/2, step 6871/7134 completed (loss: 0.0032321817707270384, acc: 1.0)
[2025-02-13 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48][root][INFO] - Training Epoch: 1/2, step 6872/7134 completed (loss: 0.014457187615334988, acc: 0.9958449006080627)
[2025-02-13 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48][root][INFO] - Training Epoch: 1/2, step 6873/7134 completed (loss: 0.010636644437909126, acc: 0.9957982897758484)
[2025-02-13 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49][root][INFO] - Training Epoch: 1/2, step 6874/7134 completed (loss: 0.015445851720869541, acc: 0.9946808218955994)
[2025-02-13 03:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49][root][INFO] - Training Epoch: 1/2, step 6875/7134 completed (loss: 0.018278205767273903, acc: 0.9953632354736328)
[2025-02-13 03:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50][root][INFO] - Training Epoch: 1/2, step 6876/7134 completed (loss: 0.018288251012563705, acc: 0.9932318329811096)
[2025-02-13 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50][root][INFO] - Training Epoch: 1/2, step 6877/7134 completed (loss: 0.02006703056395054, acc: 0.9957864880561829)
[2025-02-13 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50][root][INFO] - Training Epoch: 1/2, step 6878/7134 completed (loss: 0.01801987737417221, acc: 0.995502233505249)
[2025-02-13 03:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51][root][INFO] - Training Epoch: 1/2, step 6879/7134 completed (loss: 0.010842823423445225, acc: 0.9937238693237305)
[2025-02-13 03:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51][root][INFO] - Training Epoch: 1/2, step 6880/7134 completed (loss: 0.011999580077826977, acc: 0.9953434467315674)
[2025-02-13 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52][root][INFO] - Training Epoch: 1/2, step 6881/7134 completed (loss: 0.043354470282793045, acc: 0.9927095770835876)
[2025-02-13 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52][root][INFO] - Training Epoch: 1/2, step 6882/7134 completed (loss: 0.03444177657365799, acc: 0.9906014800071716)
[2025-02-13 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:53][root][INFO] - Training Epoch: 1/2, step 6883/7134 completed (loss: 0.017756110057234764, acc: 0.9966996908187866)
[2025-02-13 03:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:53][root][INFO] - Training Epoch: 1/2, step 6884/7134 completed (loss: 0.05973079055547714, acc: 0.989924430847168)
[2025-02-13 03:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:53][root][INFO] - Training Epoch: 1/2, step 6885/7134 completed (loss: 0.05242238566279411, acc: 0.9888392686843872)
[2025-02-13 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54][root][INFO] - Training Epoch: 1/2, step 6886/7134 completed (loss: 0.02231338620185852, acc: 0.9936608672142029)
[2025-02-13 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54][root][INFO] - Training Epoch: 1/2, step 6887/7134 completed (loss: 0.03752545267343521, acc: 0.9897330403327942)
[2025-02-13 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55][root][INFO] - Training Epoch: 1/2, step 6888/7134 completed (loss: 0.07700546830892563, acc: 0.9817276000976562)
[2025-02-13 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55][root][INFO] - Training Epoch: 1/2, step 6889/7134 completed (loss: 0.01148175448179245, acc: 0.9969372153282166)
[2025-02-13 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56][root][INFO] - Training Epoch: 1/2, step 6890/7134 completed (loss: 0.027198683470487595, acc: 0.9907529950141907)
[2025-02-13 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56][root][INFO] - Training Epoch: 1/2, step 6891/7134 completed (loss: 0.025025106966495514, acc: 0.9896296262741089)
[2025-02-13 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57][root][INFO] - Training Epoch: 1/2, step 6892/7134 completed (loss: 0.01609988510608673, acc: 0.9947916865348816)
[2025-02-13 03:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57][root][INFO] - Training Epoch: 1/2, step 6893/7134 completed (loss: 0.04286443814635277, acc: 0.9813084006309509)
[2025-02-13 03:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57][root][INFO] - Training Epoch: 1/2, step 6894/7134 completed (loss: 0.05670876428484917, acc: 0.9855263233184814)
[2025-02-13 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58][root][INFO] - Training Epoch: 1/2, step 6895/7134 completed (loss: 0.036421820521354675, acc: 0.9856114983558655)
[2025-02-13 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58][root][INFO] - Training Epoch: 1/2, step 6896/7134 completed (loss: 0.029374655336141586, acc: 0.9884058237075806)
[2025-02-13 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59][root][INFO] - Training Epoch: 1/2, step 6897/7134 completed (loss: 0.027213983237743378, acc: 0.9901840686798096)
[2025-02-13 03:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59][root][INFO] - Training Epoch: 1/2, step 6898/7134 completed (loss: 0.012951118871569633, acc: 0.9945054650306702)
[2025-02-13 03:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00][root][INFO] - Training Epoch: 1/2, step 6899/7134 completed (loss: 0.035447049885988235, acc: 0.9912060499191284)
[2025-02-13 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00][root][INFO] - Training Epoch: 1/2, step 6900/7134 completed (loss: 0.03688300400972366, acc: 0.9844412803649902)
[2025-02-13 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:01][root][INFO] - Training Epoch: 1/2, step 6901/7134 completed (loss: 0.041703999042510986, acc: 0.9894319772720337)
[2025-02-13 03:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:01][root][INFO] - Training Epoch: 1/2, step 6902/7134 completed (loss: 0.025313600897789, acc: 0.9916782379150391)
[2025-02-13 03:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:01][root][INFO] - Training Epoch: 1/2, step 6903/7134 completed (loss: 0.020593727007508278, acc: 0.9949302673339844)
[2025-02-13 03:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02][root][INFO] - Training Epoch: 1/2, step 6904/7134 completed (loss: 0.016716254875063896, acc: 0.9950248599052429)
[2025-02-13 03:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02][root][INFO] - Training Epoch: 1/2, step 6905/7134 completed (loss: 0.02513740211725235, acc: 0.9923273921012878)
[2025-02-13 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03][root][INFO] - Training Epoch: 1/2, step 6906/7134 completed (loss: 0.032879844307899475, acc: 0.9952095746994019)
[2025-02-13 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03][root][INFO] - Training Epoch: 1/2, step 6907/7134 completed (loss: 0.02453233301639557, acc: 0.9937965273857117)
[2025-02-13 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04][root][INFO] - Training Epoch: 1/2, step 6908/7134 completed (loss: 0.04763820767402649, acc: 0.9870466589927673)
[2025-02-13 03:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04][root][INFO] - Training Epoch: 1/2, step 6909/7134 completed (loss: 0.023648912087082863, acc: 0.9934210777282715)
[2025-02-13 03:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05][root][INFO] - Training Epoch: 1/2, step 6910/7134 completed (loss: 0.013182057067751884, acc: 0.9961783289909363)
[2025-02-13 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05][root][INFO] - Training Epoch: 1/2, step 6911/7134 completed (loss: 0.04050610959529877, acc: 0.9898256063461304)
[2025-02-13 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06][root][INFO] - Training Epoch: 1/2, step 6912/7134 completed (loss: 0.027080757543444633, acc: 0.9962546825408936)
[2025-02-13 03:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06][root][INFO] - Training Epoch: 1/2, step 6913/7134 completed (loss: 0.02499251253902912, acc: 0.9926918148994446)
[2025-02-13 03:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06][root][INFO] - Training Epoch: 1/2, step 6914/7134 completed (loss: 0.06755156069993973, acc: 0.9882155060768127)
[2025-02-13 03:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:07][root][INFO] - Training Epoch: 1/2, step 6915/7134 completed (loss: 0.017030982300639153, acc: 0.9948186278343201)
[2025-02-13 03:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:07][root][INFO] - Training Epoch: 1/2, step 6916/7134 completed (loss: 0.027444126084446907, acc: 0.9926380515098572)
[2025-02-13 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08][root][INFO] - Training Epoch: 1/2, step 6917/7134 completed (loss: 0.05660777539014816, acc: 0.9861111044883728)
[2025-02-13 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08][root][INFO] - Training Epoch: 1/2, step 6918/7134 completed (loss: 0.02958027459681034, acc: 0.9941452145576477)
[2025-02-13 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09][root][INFO] - Training Epoch: 1/2, step 6919/7134 completed (loss: 0.03212396427989006, acc: 0.9919261932373047)
[2025-02-13 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09][root][INFO] - Training Epoch: 1/2, step 6920/7134 completed (loss: 0.012913390062749386, acc: 0.9971510171890259)
[2025-02-13 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10][root][INFO] - Training Epoch: 1/2, step 6921/7134 completed (loss: 0.07139584422111511, acc: 0.982679009437561)
[2025-02-13 03:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10][root][INFO] - Training Epoch: 1/2, step 6922/7134 completed (loss: 0.05908031016588211, acc: 0.9859594106674194)
[2025-02-13 03:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10][root][INFO] - Training Epoch: 1/2, step 6923/7134 completed (loss: 0.06185632571578026, acc: 0.9863813519477844)
[2025-02-13 03:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:11][root][INFO] - Training Epoch: 1/2, step 6924/7134 completed (loss: 0.02616579830646515, acc: 0.9904631972312927)
[2025-02-13 03:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:11][root][INFO] - Training Epoch: 1/2, step 6925/7134 completed (loss: 0.04179370403289795, acc: 0.9921875)
[2025-02-13 03:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12][root][INFO] - Training Epoch: 1/2, step 6926/7134 completed (loss: 0.04658413678407669, acc: 0.9891501069068909)
[2025-02-13 03:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12][root][INFO] - Training Epoch: 1/2, step 6927/7134 completed (loss: 0.050186317414045334, acc: 0.9892086386680603)
[2025-02-13 03:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:13][root][INFO] - Training Epoch: 1/2, step 6928/7134 completed (loss: 0.06633467972278595, acc: 0.9902152419090271)
[2025-02-13 03:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:13][root][INFO] - Training Epoch: 1/2, step 6929/7134 completed (loss: 0.04763178154826164, acc: 0.9901130199432373)
[2025-02-13 03:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14][root][INFO] - Training Epoch: 1/2, step 6930/7134 completed (loss: 0.0568644255399704, acc: 0.9826302528381348)
[2025-02-13 03:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14][root][INFO] - Training Epoch: 1/2, step 6931/7134 completed (loss: 0.039613787084817886, acc: 0.9914634227752686)
[2025-02-13 03:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15][root][INFO] - Training Epoch: 1/2, step 6932/7134 completed (loss: 0.04121224209666252, acc: 0.9897058606147766)
[2025-02-13 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15][root][INFO] - Training Epoch: 1/2, step 6933/7134 completed (loss: 0.07453738152980804, acc: 0.9805951118469238)
[2025-02-13 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15][root][INFO] - Training Epoch: 1/2, step 6934/7134 completed (loss: 0.06975821405649185, acc: 0.9839486479759216)
[2025-02-13 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16][root][INFO] - Training Epoch: 1/2, step 6935/7134 completed (loss: 0.03793729841709137, acc: 0.9921466112136841)
[2025-02-13 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16][root][INFO] - Training Epoch: 1/2, step 6936/7134 completed (loss: 0.015597798861563206, acc: 0.9933333396911621)
[2025-02-13 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17][root][INFO] - Training Epoch: 1/2, step 6937/7134 completed (loss: 0.11243665963411331, acc: 0.9737303256988525)
[2025-02-13 03:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17][root][INFO] - Training Epoch: 1/2, step 6938/7134 completed (loss: 0.09997447580099106, acc: 0.973437488079071)
[2025-02-13 03:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18][root][INFO] - Training Epoch: 1/2, step 6939/7134 completed (loss: 0.06472165882587433, acc: 0.9874826073646545)
[2025-02-13 03:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18][root][INFO] - Training Epoch: 1/2, step 6940/7134 completed (loss: 0.06687627732753754, acc: 0.982876718044281)
[2025-02-13 03:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19][root][INFO] - Training Epoch: 1/2, step 6941/7134 completed (loss: 0.027134699746966362, acc: 0.9947916865348816)
[2025-02-13 03:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19][root][INFO] - Training Epoch: 1/2, step 6942/7134 completed (loss: 0.054977819323539734, acc: 0.9849315285682678)
[2025-02-13 03:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19][root][INFO] - Training Epoch: 1/2, step 6943/7134 completed (loss: 0.04113372415304184, acc: 0.9895397424697876)
[2025-02-13 03:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20][root][INFO] - Training Epoch: 1/2, step 6944/7134 completed (loss: 0.03698063641786575, acc: 0.9888535141944885)
[2025-02-13 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20][root][INFO] - Training Epoch: 1/2, step 6945/7134 completed (loss: 0.06743994355201721, acc: 0.9825479984283447)
[2025-02-13 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:21][root][INFO] - Training Epoch: 1/2, step 6946/7134 completed (loss: 0.03460319712758064, acc: 0.9885386824607849)
[2025-02-13 03:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:21][root][INFO] - Training Epoch: 1/2, step 6947/7134 completed (loss: 0.0282557625323534, acc: 0.9956521987915039)
[2025-02-13 03:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:21][root][INFO] - Training Epoch: 1/2, step 6948/7134 completed (loss: 0.045177068561315536, acc: 0.9899497628211975)
[2025-02-13 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22][root][INFO] - Training Epoch: 1/2, step 6949/7134 completed (loss: 0.04990602657198906, acc: 0.9871794581413269)
[2025-02-13 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22][root][INFO] - Training Epoch: 1/2, step 6950/7134 completed (loss: 0.0260368213057518, acc: 0.9948979616165161)
[2025-02-13 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23][root][INFO] - Training Epoch: 1/2, step 6951/7134 completed (loss: 0.023301569744944572, acc: 0.991525411605835)
[2025-02-13 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23][root][INFO] - Training Epoch: 1/2, step 6952/7134 completed (loss: 0.01752353645861149, acc: 0.9929906725883484)
[2025-02-13 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23][root][INFO] - Training Epoch: 1/2, step 6953/7134 completed (loss: 0.02123558707535267, acc: 0.9934425950050354)
[2025-02-13 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24][root][INFO] - Training Epoch: 1/2, step 6954/7134 completed (loss: 0.07209058105945587, acc: 0.9850746393203735)
[2025-02-13 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24][root][INFO] - Training Epoch: 1/2, step 6955/7134 completed (loss: 0.026117490604519844, acc: 0.9908536672592163)
[2025-02-13 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25][root][INFO] - Training Epoch: 1/2, step 6956/7134 completed (loss: 0.03676178678870201, acc: 0.9893428087234497)
[2025-02-13 03:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25][root][INFO] - Training Epoch: 1/2, step 6957/7134 completed (loss: 0.022486794739961624, acc: 0.9960861206054688)
[2025-02-13 03:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26][root][INFO] - Training Epoch: 1/2, step 6958/7134 completed (loss: 0.057631563395261765, acc: 0.9805068373680115)
[2025-02-13 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26][root][INFO] - Training Epoch: 1/2, step 6959/7134 completed (loss: 0.03667706251144409, acc: 0.9866962432861328)
[2025-02-13 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26][root][INFO] - Training Epoch: 1/2, step 6960/7134 completed (loss: 0.03692896291613579, acc: 0.9902439117431641)
[2025-02-13 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27][root][INFO] - Training Epoch: 1/2, step 6961/7134 completed (loss: 0.0391126349568367, acc: 0.9898989796638489)
[2025-02-13 03:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27][root][INFO] - Training Epoch: 1/2, step 6962/7134 completed (loss: 0.04323897138237953, acc: 0.991304337978363)
[2025-02-13 03:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28][root][INFO] - Training Epoch: 1/2, step 6963/7134 completed (loss: 0.020212918519973755, acc: 0.9949495196342468)
[2025-02-13 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28][root][INFO] - Training Epoch: 1/2, step 6964/7134 completed (loss: 0.020089637488126755, acc: 0.9981949329376221)
[2025-02-13 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28][root][INFO] - Training Epoch: 1/2, step 6965/7134 completed (loss: 0.021799197420477867, acc: 0.9945054650306702)
[2025-02-13 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29][root][INFO] - Training Epoch: 1/2, step 6966/7134 completed (loss: 0.048631563782691956, acc: 0.9850373864173889)
[2025-02-13 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29][root][INFO] - Training Epoch: 1/2, step 6967/7134 completed (loss: 0.013983181677758694, acc: 0.993966817855835)
[2025-02-13 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30][root][INFO] - Training Epoch: 1/2, step 6968/7134 completed (loss: 0.026302717626094818, acc: 0.9913793206214905)
[2025-02-13 03:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30][root][INFO] - Training Epoch: 1/2, step 6969/7134 completed (loss: 0.07403388619422913, acc: 0.9833679795265198)
[2025-02-13 03:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30][root][INFO] - Training Epoch: 1/2, step 6970/7134 completed (loss: 0.07157489657402039, acc: 0.9789473414421082)
[2025-02-13 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31][root][INFO] - Training Epoch: 1/2, step 6971/7134 completed (loss: 0.053411055356264114, acc: 0.9861830472946167)
[2025-02-13 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31][root][INFO] - Training Epoch: 1/2, step 6972/7134 completed (loss: 0.02342718280851841, acc: 0.9893048405647278)
[2025-02-13 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32][root][INFO] - Training Epoch: 1/2, step 6973/7134 completed (loss: 0.04352319613099098, acc: 0.9911894202232361)
[2025-02-13 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32][root][INFO] - Training Epoch: 1/2, step 6974/7134 completed (loss: 0.059122633188962936, acc: 0.9862068891525269)
[2025-02-13 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33][root][INFO] - Training Epoch: 1/2, step 6975/7134 completed (loss: 0.044541992247104645, acc: 0.9934924244880676)
[2025-02-13 03:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33][root][INFO] - Training Epoch: 1/2, step 6976/7134 completed (loss: 0.048143170773983, acc: 0.9861351847648621)
[2025-02-13 03:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33][root][INFO] - Training Epoch: 1/2, step 6977/7134 completed (loss: 0.033341869711875916, acc: 0.9830827116966248)
[2025-02-13 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34][root][INFO] - Training Epoch: 1/2, step 6978/7134 completed (loss: 0.025107335299253464, acc: 0.9934959411621094)
[2025-02-13 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34][root][INFO] - Training Epoch: 1/2, step 6979/7134 completed (loss: 0.03991615027189255, acc: 0.9862805008888245)
[2025-02-13 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:35][root][INFO] - Training Epoch: 1/2, step 6980/7134 completed (loss: 0.11848025768995285, acc: 0.9672955870628357)
[2025-02-13 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:35][root][INFO] - Training Epoch: 1/2, step 6981/7134 completed (loss: 0.09908629208803177, acc: 0.977011501789093)
[2025-02-13 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36][root][INFO] - Training Epoch: 1/2, step 6982/7134 completed (loss: 0.08573988080024719, acc: 0.9805115461349487)
[2025-02-13 03:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36][root][INFO] - Training Epoch: 1/2, step 6983/7134 completed (loss: 0.10401441901922226, acc: 0.9747774600982666)
[2025-02-13 03:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37][root][INFO] - Training Epoch: 1/2, step 6984/7134 completed (loss: 0.06112198159098625, acc: 0.9871382713317871)
[2025-02-13 03:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37][root][INFO] - Training Epoch: 1/2, step 6985/7134 completed (loss: 0.11052849143743515, acc: 0.9753845930099487)
[2025-02-13 03:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38][root][INFO] - Training Epoch: 1/2, step 6986/7134 completed (loss: 0.03399007394909859, acc: 0.9866270422935486)
[2025-02-13 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38][root][INFO] - Training Epoch: 1/2, step 6987/7134 completed (loss: 0.0503753200173378, acc: 0.9848484992980957)
[2025-02-13 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38][root][INFO] - Training Epoch: 1/2, step 6988/7134 completed (loss: 0.02551373653113842, acc: 0.9940357804298401)
[2025-02-13 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:39][root][INFO] - Training Epoch: 1/2, step 6989/7134 completed (loss: 0.08772460371255875, acc: 0.9672726988792419)
[2025-02-13 03:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:39][root][INFO] - Training Epoch: 1/2, step 6990/7134 completed (loss: 0.023078691214323044, acc: 0.9955357313156128)
[2025-02-13 03:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40][root][INFO] - Training Epoch: 1/2, step 6991/7134 completed (loss: 0.059985022991895676, acc: 0.9849624037742615)
[2025-02-13 03:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40][root][INFO] - Training Epoch: 1/2, step 6992/7134 completed (loss: 0.05983377620577812, acc: 0.9873684048652649)
[2025-02-13 03:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40][root][INFO] - Training Epoch: 1/2, step 6993/7134 completed (loss: 0.10133564472198486, acc: 0.9728506803512573)
[2025-02-13 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:41][root][INFO] - Training Epoch: 1/2, step 6994/7134 completed (loss: 0.13215667009353638, acc: 0.9678972959518433)
[2025-02-13 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:41][root][INFO] - Training Epoch: 1/2, step 6995/7134 completed (loss: 0.03605964779853821, acc: 0.9921466112136841)
[2025-02-13 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42][root][INFO] - Training Epoch: 1/2, step 6996/7134 completed (loss: 0.03542819619178772, acc: 0.9918830990791321)
[2025-02-13 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42][root][INFO] - Training Epoch: 1/2, step 6997/7134 completed (loss: 0.03593495488166809, acc: 0.9922118186950684)
[2025-02-13 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43][root][INFO] - Training Epoch: 1/2, step 6998/7134 completed (loss: 0.10010384768247604, acc: 0.970588207244873)
[2025-02-13 03:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43][root][INFO] - Training Epoch: 1/2, step 6999/7134 completed (loss: 0.02273874171078205, acc: 0.9921568632125854)
[2025-02-13 03:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43][root][INFO] - Training Epoch: 1/2, step 7000/7134 completed (loss: 0.024229170754551888, acc: 0.9936440587043762)
[2025-02-13 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44][root][INFO] - Training Epoch: 1/2, step 7001/7134 completed (loss: 0.09070193767547607, acc: 0.9819276928901672)
[2025-02-13 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44][root][INFO] - Training Epoch: 1/2, step 7002/7134 completed (loss: 0.017491735517978668, acc: 0.993914783000946)
[2025-02-13 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45][root][INFO] - Training Epoch: 1/2, step 7003/7134 completed (loss: 0.019064925611019135, acc: 0.9901424050331116)
[2025-02-13 03:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45][root][INFO] - Training Epoch: 1/2, step 7004/7134 completed (loss: 0.08027106523513794, acc: 0.9763779640197754)
[2025-02-13 03:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:46][root][INFO] - Training Epoch: 1/2, step 7005/7134 completed (loss: 0.03653287515044212, acc: 0.9878542423248291)
[2025-02-13 03:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:46][root][INFO] - Training Epoch: 1/2, step 7006/7134 completed (loss: 0.03828500583767891, acc: 0.9890561103820801)
[2025-02-13 03:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47][root][INFO] - Training Epoch: 1/2, step 7007/7134 completed (loss: 0.03348330780863762, acc: 0.9910179376602173)
[2025-02-13 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47][root][INFO] - Training Epoch: 1/2, step 7008/7134 completed (loss: 0.027011698111891747, acc: 0.992601752281189)
[2025-02-13 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47][root][INFO] - Training Epoch: 1/2, step 7009/7134 completed (loss: 0.032554082572460175, acc: 0.9884259104728699)
[2025-02-13 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48][root][INFO] - Training Epoch: 1/2, step 7010/7134 completed (loss: 0.011472796089947224, acc: 0.9984591603279114)
[2025-02-13 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48][root][INFO] - Training Epoch: 1/2, step 7011/7134 completed (loss: 0.03220866620540619, acc: 0.9879807829856873)
[2025-02-13 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49][root][INFO] - Training Epoch: 1/2, step 7012/7134 completed (loss: 0.06259781122207642, acc: 0.9854227304458618)
[2025-02-13 03:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49][root][INFO] - Training Epoch: 1/2, step 7013/7134 completed (loss: 0.019411174580454826, acc: 0.9956772327423096)
[2025-02-13 03:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50][root][INFO] - Training Epoch: 1/2, step 7014/7134 completed (loss: 0.027649536728858948, acc: 0.9935587644577026)
[2025-02-13 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50][root][INFO] - Training Epoch: 1/2, step 7015/7134 completed (loss: 0.04294845089316368, acc: 0.980322003364563)
[2025-02-13 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51][root][INFO] - Training Epoch: 1/2, step 7016/7134 completed (loss: 0.044462043792009354, acc: 0.9884393215179443)
[2025-02-13 03:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51][root][INFO] - Training Epoch: 1/2, step 7017/7134 completed (loss: 0.05892118439078331, acc: 0.9875518679618835)
[2025-02-13 03:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51][root][INFO] - Training Epoch: 1/2, step 7018/7134 completed (loss: 0.04879416152834892, acc: 0.9922480583190918)
[2025-02-13 03:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52][root][INFO] - Training Epoch: 1/2, step 7019/7134 completed (loss: 0.1190359890460968, acc: 0.9678362607955933)
[2025-02-13 03:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52][root][INFO] - Training Epoch: 1/2, step 7020/7134 completed (loss: 0.026195170357823372, acc: 0.9901269674301147)
[2025-02-13 03:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53][root][INFO] - Training Epoch: 1/2, step 7021/7134 completed (loss: 0.024030672386288643, acc: 0.9948892593383789)
[2025-02-13 03:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53][root][INFO] - Training Epoch: 1/2, step 7022/7134 completed (loss: 0.08419398218393326, acc: 0.9845678806304932)
[2025-02-13 03:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54][root][INFO] - Training Epoch: 1/2, step 7023/7134 completed (loss: 0.094197578728199, acc: 0.9713603854179382)
[2025-02-13 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54][root][INFO] - Training Epoch: 1/2, step 7024/7134 completed (loss: 0.023727023974061012, acc: 0.9904458522796631)
[2025-02-13 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54][root][INFO] - Training Epoch: 1/2, step 7025/7134 completed (loss: 0.09895362704992294, acc: 0.9632183909416199)
[2025-02-13 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55][root][INFO] - Training Epoch: 1/2, step 7026/7134 completed (loss: 0.10039570182561874, acc: 0.9729729890823364)
[2025-02-13 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55][root][INFO] - Training Epoch: 1/2, step 7027/7134 completed (loss: 0.04173578694462776, acc: 0.9878048896789551)
[2025-02-13 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:56][root][INFO] - Training Epoch: 1/2, step 7028/7134 completed (loss: 0.021635008975863457, acc: 0.9927361011505127)
[2025-02-13 03:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:56][root][INFO] - Training Epoch: 1/2, step 7029/7134 completed (loss: 0.03162842616438866, acc: 0.9860788583755493)
[2025-02-13 03:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:56][root][INFO] - Training Epoch: 1/2, step 7030/7134 completed (loss: 0.11639805883169174, acc: 0.971563994884491)
[2025-02-13 03:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:57][root][INFO] - Training Epoch: 1/2, step 7031/7134 completed (loss: 0.07241211831569672, acc: 0.9746192693710327)
[2025-02-13 03:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:57][root][INFO] - Training Epoch: 1/2, step 7032/7134 completed (loss: 0.07733724266290665, acc: 0.9756097793579102)
[2025-02-13 03:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:58][root][INFO] - Training Epoch: 1/2, step 7033/7134 completed (loss: 0.055229976773262024, acc: 0.9706745147705078)
[2025-02-13 03:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:58][root][INFO] - Training Epoch: 1/2, step 7034/7134 completed (loss: 0.09823572635650635, acc: 0.9752650260925293)
[2025-02-13 03:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:58][root][INFO] - Training Epoch: 1/2, step 7035/7134 completed (loss: 0.11273857951164246, acc: 0.9722222089767456)
[2025-02-13 03:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59][root][INFO] - Training Epoch: 1/2, step 7036/7134 completed (loss: 0.06107267364859581, acc: 0.9753086566925049)
[2025-02-13 03:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59][root][INFO] - Training Epoch: 1/2, step 7037/7134 completed (loss: 0.05989152193069458, acc: 0.97265625)
[2025-02-13 03:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59][root][INFO] - Training Epoch: 1/2, step 7038/7134 completed (loss: 0.13785265386104584, acc: 0.9630606770515442)
[2025-02-13 03:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:00][root][INFO] - Training Epoch: 1/2, step 7039/7134 completed (loss: 0.03184472396969795, acc: 0.990338146686554)
[2025-02-13 03:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:00][root][INFO] - Training Epoch: 1/2, step 7040/7134 completed (loss: 0.13536596298217773, acc: 0.9568965435028076)
[2025-02-13 03:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01][root][INFO] - Training Epoch: 1/2, step 7041/7134 completed (loss: 0.08733128011226654, acc: 0.9779874086380005)
[2025-02-13 03:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01][root][INFO] - Training Epoch: 1/2, step 7042/7134 completed (loss: 0.06850310415029526, acc: 0.9692307710647583)
[2025-02-13 03:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01][root][INFO] - Training Epoch: 1/2, step 7043/7134 completed (loss: 0.07615438103675842, acc: 0.9733333587646484)
[2025-02-13 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02][root][INFO] - Training Epoch: 1/2, step 7044/7134 completed (loss: 0.1104707196354866, acc: 0.9724770784378052)
[2025-02-13 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02][root][INFO] - Training Epoch: 1/2, step 7045/7134 completed (loss: 0.037284135818481445, acc: 0.9897959232330322)
[2025-02-13 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03][root][INFO] - Training Epoch: 1/2, step 7046/7134 completed (loss: 0.08502165228128433, acc: 0.9795082211494446)
[2025-02-13 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03][root][INFO] - Training Epoch: 1/2, step 7047/7134 completed (loss: 0.08231896162033081, acc: 0.9785714149475098)
[2025-02-13 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04][root][INFO] - Training Epoch: 1/2, step 7048/7134 completed (loss: 0.0035060427617281675, acc: 1.0)
[2025-02-13 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04][root][INFO] - Training Epoch: 1/2, step 7049/7134 completed (loss: 0.025564726442098618, acc: 0.990641713142395)
[2025-02-13 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04][root][INFO] - Training Epoch: 1/2, step 7050/7134 completed (loss: 0.022687669843435287, acc: 0.9948186278343201)
[2025-02-13 03:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:05][root][INFO] - Training Epoch: 1/2, step 7051/7134 completed (loss: 0.04643825814127922, acc: 0.9886220097541809)
[2025-02-13 03:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:05][root][INFO] - Training Epoch: 1/2, step 7052/7134 completed (loss: 0.04013005644083023, acc: 0.9866844415664673)
[2025-02-13 03:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06][root][INFO] - Training Epoch: 1/2, step 7053/7134 completed (loss: 0.019377442076802254, acc: 0.9914737939834595)
[2025-02-13 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06][root][INFO] - Training Epoch: 1/2, step 7054/7134 completed (loss: 0.05032308027148247, acc: 0.982594907283783)
[2025-02-13 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:07][root][INFO] - Training Epoch: 1/2, step 7055/7134 completed (loss: 0.04273935779929161, acc: 0.9885203838348389)
[2025-02-13 03:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:07][root][INFO] - Training Epoch: 1/2, step 7056/7134 completed (loss: 0.056510210037231445, acc: 0.9843937754631042)
[2025-02-13 03:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08][root][INFO] - Training Epoch: 1/2, step 7057/7134 completed (loss: 0.10287821292877197, acc: 0.9780821800231934)
[2025-02-13 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08][root][INFO] - Training Epoch: 1/2, step 7058/7134 completed (loss: 0.05881800130009651, acc: 0.9862204790115356)
[2025-02-13 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08][root][INFO] - Training Epoch: 1/2, step 7059/7134 completed (loss: 0.017705416306853294, acc: 0.9916201233863831)
[2025-02-13 03:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09][root][INFO] - Training Epoch: 1/2, step 7060/7134 completed (loss: 0.06634467840194702, acc: 0.9854838848114014)
[2025-02-13 03:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09][root][INFO] - Training Epoch: 1/2, step 7061/7134 completed (loss: 0.03888331726193428, acc: 0.9934895634651184)
[2025-02-13 03:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:10][root][INFO] - Training Epoch: 1/2, step 7062/7134 completed (loss: 0.09758016467094421, acc: 0.9733333587646484)
[2025-02-13 03:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:10][root][INFO] - Training Epoch: 1/2, step 7063/7134 completed (loss: 0.067693330347538, acc: 0.9908496737480164)
[2025-02-13 03:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11][root][INFO] - Training Epoch: 1/2, step 7064/7134 completed (loss: 0.028361894190311432, acc: 0.9910979270935059)
[2025-02-13 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11][root][INFO] - Training Epoch: 1/2, step 7065/7134 completed (loss: 0.02040792815387249, acc: 0.9953650236129761)
[2025-02-13 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12][root][INFO] - Training Epoch: 1/2, step 7066/7134 completed (loss: 0.024042408913373947, acc: 0.9940546751022339)
[2025-02-13 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12][root][INFO] - Training Epoch: 1/2, step 7067/7134 completed (loss: 0.02265111356973648, acc: 0.9898089170455933)
[2025-02-13 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13][root][INFO] - Training Epoch: 1/2, step 7068/7134 completed (loss: 0.018726516515016556, acc: 0.993968665599823)
[2025-02-13 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13][root][INFO] - Training Epoch: 1/2, step 7069/7134 completed (loss: 0.0250372514128685, acc: 0.993096649646759)
[2025-02-13 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13][root][INFO] - Training Epoch: 1/2, step 7070/7134 completed (loss: 0.023574577644467354, acc: 0.9945130348205566)
[2025-02-13 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:14][root][INFO] - Training Epoch: 1/2, step 7071/7134 completed (loss: 0.0513027124106884, acc: 0.9924812316894531)
[2025-02-13 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:14][root][INFO] - Training Epoch: 1/2, step 7072/7134 completed (loss: 0.024996571242809296, acc: 0.9939393997192383)
[2025-02-13 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15][root][INFO] - Training Epoch: 1/2, step 7073/7134 completed (loss: 0.03059002198278904, acc: 0.9874857664108276)
[2025-02-13 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15][root][INFO] - Training Epoch: 1/2, step 7074/7134 completed (loss: 0.056536536663770676, acc: 0.980879545211792)
[2025-02-13 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16][root][INFO] - Training Epoch: 1/2, step 7075/7134 completed (loss: 0.02467893809080124, acc: 0.9910314083099365)
[2025-02-13 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16][root][INFO] - Training Epoch: 1/2, step 7076/7134 completed (loss: 0.03696436062455177, acc: 0.9900285005569458)
[2025-02-13 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16][root][INFO] - Training Epoch: 1/2, step 7077/7134 completed (loss: 0.048966094851493835, acc: 0.9819672107696533)
[2025-02-13 03:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:17][root][INFO] - Training Epoch: 1/2, step 7078/7134 completed (loss: 0.025809817016124725, acc: 0.9887820482254028)
[2025-02-13 03:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:17][root][INFO] - Training Epoch: 1/2, step 7079/7134 completed (loss: 0.03127488121390343, acc: 0.9901477694511414)
[2025-02-13 03:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:18][root][INFO] - Training Epoch: 1/2, step 7080/7134 completed (loss: 0.024388551712036133, acc: 0.9916943311691284)
[2025-02-13 03:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:18][root][INFO] - Training Epoch: 1/2, step 7081/7134 completed (loss: 0.03970145806670189, acc: 0.9845722317695618)
[2025-02-13 03:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:19][root][INFO] - Training Epoch: 1/2, step 7082/7134 completed (loss: 0.046050239354372025, acc: 0.9886914491653442)
[2025-02-13 03:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:19][root][INFO] - Training Epoch: 1/2, step 7083/7134 completed (loss: 0.021456245332956314, acc: 0.9933110475540161)
[2025-02-13 03:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:19][root][INFO] - Training Epoch: 1/2, step 7084/7134 completed (loss: 0.03779415413737297, acc: 0.9912917017936707)
[2025-02-13 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:20][root][INFO] - Training Epoch: 1/2, step 7085/7134 completed (loss: 0.047162242233753204, acc: 0.9907407164573669)
[2025-02-13 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:20][root][INFO] - Training Epoch: 1/2, step 7086/7134 completed (loss: 0.015323709696531296, acc: 0.99589604139328)
[2025-02-13 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21][root][INFO] - Training Epoch: 1/2, step 7087/7134 completed (loss: 0.030452080070972443, acc: 0.9926062822341919)
[2025-02-13 03:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21][root][INFO] - Training Epoch: 1/2, step 7088/7134 completed (loss: 0.041207585483789444, acc: 0.9884105920791626)
[2025-02-13 03:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:22][root][INFO] - Training Epoch: 1/2, step 7089/7134 completed (loss: 0.1160687729716301, acc: 0.9808917045593262)
[2025-02-13 03:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:22][root][INFO] - Training Epoch: 1/2, step 7090/7134 completed (loss: 0.03427717834711075, acc: 0.9905149340629578)
[2025-02-13 03:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:22][root][INFO] - Training Epoch: 1/2, step 7091/7134 completed (loss: 0.007108661811798811, acc: 0.9972144961357117)
[2025-02-13 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:23][root][INFO] - Training Epoch: 1/2, step 7092/7134 completed (loss: 0.02057315781712532, acc: 0.9914236664772034)
[2025-02-13 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:23][root][INFO] - Training Epoch: 1/2, step 7093/7134 completed (loss: 0.04272348806262016, acc: 0.9861111044883728)
[2025-02-13 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24][root][INFO] - Training Epoch: 1/2, step 7094/7134 completed (loss: 0.02091437391936779, acc: 0.9930875301361084)
[2025-02-13 03:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24][root][INFO] - Training Epoch: 1/2, step 7095/7134 completed (loss: 0.04230118915438652, acc: 0.9938016533851624)
[2025-02-13 03:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:25][root][INFO] - Training Epoch: 1/2, step 7096/7134 completed (loss: 0.06090041622519493, acc: 0.9798319339752197)
[2025-02-13 03:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:25][root][INFO] - Training Epoch: 1/2, step 7097/7134 completed (loss: 0.04519462212920189, acc: 0.9870316982269287)
[2025-02-13 03:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:25][root][INFO] - Training Epoch: 1/2, step 7098/7134 completed (loss: 0.025862786918878555, acc: 0.9919742941856384)
[2025-02-13 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:26][root][INFO] - Training Epoch: 1/2, step 7099/7134 completed (loss: 0.04191528260707855, acc: 0.988034188747406)
[2025-02-13 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:26][root][INFO] - Training Epoch: 1/2, step 7100/7134 completed (loss: 0.060813479125499725, acc: 0.9813953638076782)
[2025-02-13 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27][root][INFO] - Training Epoch: 1/2, step 7101/7134 completed (loss: 0.09976474940776825, acc: 0.9785330891609192)
[2025-02-13 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27][root][INFO] - Training Epoch: 1/2, step 7102/7134 completed (loss: 0.034874822944402695, acc: 0.9941860437393188)
[2025-02-13 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27][root][INFO] - Training Epoch: 1/2, step 7103/7134 completed (loss: 0.031544510275125504, acc: 0.9919354915618896)
[2025-02-13 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:28][root][INFO] - Training Epoch: 1/2, step 7104/7134 completed (loss: 0.02417043037712574, acc: 0.991769552230835)
[2025-02-13 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:28][root][INFO] - Training Epoch: 1/2, step 7105/7134 completed (loss: 0.039190687239170074, acc: 0.9900000095367432)
[2025-02-13 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29][root][INFO] - Training Epoch: 1/2, step 7106/7134 completed (loss: 0.01227570977061987, acc: 0.9966611266136169)
[2025-02-13 03:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29][root][INFO] - Training Epoch: 1/2, step 7107/7134 completed (loss: 0.027691634371876717, acc: 0.989130437374115)
[2025-02-13 03:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30][root][INFO] - Training Epoch: 1/2, step 7108/7134 completed (loss: 0.030559880658984184, acc: 0.9916666746139526)
[2025-02-13 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30][root][INFO] - Training Epoch: 1/2, step 7109/7134 completed (loss: 0.0560464933514595, acc: 0.9793621301651001)
[2025-02-13 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30][root][INFO] - Training Epoch: 1/2, step 7110/7134 completed (loss: 0.030086316168308258, acc: 0.991525411605835)
[2025-02-13 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:31][root][INFO] - Training Epoch: 1/2, step 7111/7134 completed (loss: 0.06922062486410141, acc: 0.9778270721435547)
[2025-02-13 03:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:31][root][INFO] - Training Epoch: 1/2, step 7112/7134 completed (loss: 0.03201863914728165, acc: 0.9876543283462524)
[2025-02-13 03:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32][root][INFO] - Training Epoch: 1/2, step 7113/7134 completed (loss: 0.08158976584672928, acc: 0.9731861352920532)
[2025-02-13 03:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32][root][INFO] - Training Epoch: 1/2, step 7114/7134 completed (loss: 0.04274772107601166, acc: 0.990439772605896)
[2025-02-13 03:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32][root][INFO] - Training Epoch: 1/2, step 7115/7134 completed (loss: 0.025115493685007095, acc: 0.9894179701805115)
[2025-02-13 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33][root][INFO] - Training Epoch: 1/2, step 7116/7134 completed (loss: 0.045421913266181946, acc: 0.9894551634788513)
[2025-02-13 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33][root][INFO] - Training Epoch: 1/2, step 7117/7134 completed (loss: 0.017928535118699074, acc: 0.9950000047683716)
[2025-02-13 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34][root][INFO] - Training Epoch: 1/2, step 7118/7134 completed (loss: 0.04318242147564888, acc: 0.9854651093482971)
[2025-02-13 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34][root][INFO] - Training Epoch: 1/2, step 7119/7134 completed (loss: 0.02683700993657112, acc: 0.9934533834457397)
[2025-02-13 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34][root][INFO] - Training Epoch: 1/2, step 7120/7134 completed (loss: 0.04327511414885521, acc: 0.9883177280426025)
[2025-02-13 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:35][root][INFO] - Training Epoch: 1/2, step 7121/7134 completed (loss: 0.02173425629734993, acc: 0.9911971688270569)
[2025-02-13 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:35][root][INFO] - Training Epoch: 1/2, step 7122/7134 completed (loss: 0.028873208910226822, acc: 0.9866310358047485)
[2025-02-13 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:36][root][INFO] - Training Epoch: 1/2, step 7123/7134 completed (loss: 0.030738376080989838, acc: 0.9853658676147461)
[2025-02-13 03:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:36][root][INFO] - Training Epoch: 1/2, step 7124/7134 completed (loss: 0.016070252284407616, acc: 0.998123824596405)
[2025-02-13 03:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37][root][INFO] - Training Epoch: 1/2, step 7125/7134 completed (loss: 0.025508280843496323, acc: 0.9900497794151306)
[2025-02-13 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37][root][INFO] - Training Epoch: 1/2, step 7126/7134 completed (loss: 0.06994153559207916, acc: 0.9780775904655457)
[2025-02-13 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37][root][INFO] - Training Epoch: 1/2, step 7127/7134 completed (loss: 0.02489665523171425, acc: 0.993630588054657)
[2025-02-13 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:38][root][INFO] - Training Epoch: 1/2, step 7128/7134 completed (loss: 0.04190969094634056, acc: 0.9853658676147461)
[2025-02-13 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:38][root][INFO] - Training Epoch: 1/2, step 7129/7134 completed (loss: 0.07167702913284302, acc: 0.9753915071487427)
[2025-02-13 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:39][root][INFO] - Training Epoch: 1/2, step 7130/7134 completed (loss: 0.10375741124153137, acc: 0.9678571224212646)
[2025-02-13 03:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:39][root][INFO] - Training Epoch: 1/2, step 7131/7134 completed (loss: 0.015331381000578403, acc: 0.9953595995903015)
[2025-02-13 03:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:04][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0481, device='cuda:0') eval_epoch_loss=tensor(0.0470, device='cuda:0') eval_epoch_acc=tensor(0.9870, device='cuda:0')
[2025-02-13 03:35:04][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:35:04][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:35:04][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_7132_loss_0.046969179064035416/model.pt
[2025-02-13 03:35:04][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:35:04][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.046969179064035416
[2025-02-13 03:35:04][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.987034797668457
[2025-02-13 03:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:04][root][INFO] - Training Epoch: 1/2, step 7132/7134 completed (loss: 0.0210744459182024, acc: 0.9932659864425659)
[2025-02-13 03:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:05][root][INFO] - Training Epoch: 1/2, step 7133/7134 completed (loss: 0.010129015892744064, acc: 0.996303141117096)
[2025-02-13 03:35:05][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=1.0758, train_epoch_loss=0.0730, epoch time 4218.343115992844s
[2025-02-13 03:35:05][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2025-02-13 03:35:05][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 30 GB
[2025-02-13 03:35:05][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2025-02-13 03:35:05][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-02-13 03:35:05][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-13 03:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:07][root][INFO] - Training Epoch: 2/2, step 0/7134 completed (loss: 0.033466238528490067, acc: 0.9910846948623657)
[2025-02-13 03:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:07][root][INFO] - Training Epoch: 2/2, step 1/7134 completed (loss: 0.03819018974900246, acc: 0.9888424277305603)
[2025-02-13 03:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:07][root][INFO] - Training Epoch: 2/2, step 2/7134 completed (loss: 0.0244086142629385, acc: 0.9946452379226685)
[2025-02-13 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:08][root][INFO] - Training Epoch: 2/2, step 3/7134 completed (loss: 0.007809262722730637, acc: 0.9987195730209351)
[2025-02-13 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:08][root][INFO] - Training Epoch: 2/2, step 4/7134 completed (loss: 0.03306765481829643, acc: 0.9904502034187317)
[2025-02-13 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:09][root][INFO] - Training Epoch: 2/2, step 5/7134 completed (loss: 0.012996350415050983, acc: 0.9974619150161743)
[2025-02-13 03:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:09][root][INFO] - Training Epoch: 2/2, step 6/7134 completed (loss: 0.008944561704993248, acc: 0.9983948469161987)
[2025-02-13 03:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:10][root][INFO] - Training Epoch: 2/2, step 7/7134 completed (loss: 0.03897834196686745, acc: 0.9887920022010803)
[2025-02-13 03:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:10][root][INFO] - Training Epoch: 2/2, step 8/7134 completed (loss: 0.0630541741847992, acc: 0.9807427525520325)
[2025-02-13 03:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:11][root][INFO] - Training Epoch: 2/2, step 9/7134 completed (loss: 0.01521205436438322, acc: 0.9953488111495972)
[2025-02-13 03:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:11][root][INFO] - Training Epoch: 2/2, step 10/7134 completed (loss: 0.035332899540662766, acc: 0.9886202216148376)
[2025-02-13 03:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12][root][INFO] - Training Epoch: 2/2, step 11/7134 completed (loss: 0.013660765253007412, acc: 0.998123824596405)
[2025-02-13 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12][root][INFO] - Training Epoch: 2/2, step 12/7134 completed (loss: 0.013813650235533714, acc: 0.9946164488792419)
[2025-02-13 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12][root][INFO] - Training Epoch: 2/2, step 13/7134 completed (loss: 0.033501651138067245, acc: 0.9917469024658203)
[2025-02-13 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:13][root][INFO] - Training Epoch: 2/2, step 14/7134 completed (loss: 0.010047024115920067, acc: 0.9983193278312683)
[2025-02-13 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:13][root][INFO] - Training Epoch: 2/2, step 15/7134 completed (loss: 0.026562882587313652, acc: 0.9965576529502869)
[2025-02-13 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:14][root][INFO] - Training Epoch: 2/2, step 16/7134 completed (loss: 0.0017527614254504442, acc: 1.0)
[2025-02-13 03:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:14][root][INFO] - Training Epoch: 2/2, step 17/7134 completed (loss: 0.07147706300020218, acc: 0.9810810685157776)
[2025-02-13 03:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:15][root][INFO] - Training Epoch: 2/2, step 18/7134 completed (loss: 0.02456207200884819, acc: 0.9943181872367859)
[2025-02-13 03:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:15][root][INFO] - Training Epoch: 2/2, step 19/7134 completed (loss: 0.030734103173017502, acc: 0.9917241334915161)
[2025-02-13 03:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:15][root][INFO] - Training Epoch: 2/2, step 20/7134 completed (loss: 0.008554327301681042, acc: 0.9947368502616882)
[2025-02-13 03:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16][root][INFO] - Training Epoch: 2/2, step 21/7134 completed (loss: 0.006914292462170124, acc: 0.998711347579956)
[2025-02-13 03:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16][root][INFO] - Training Epoch: 2/2, step 22/7134 completed (loss: 0.0029765369836241007, acc: 0.9985590577125549)
[2025-02-13 03:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:17][root][INFO] - Training Epoch: 2/2, step 23/7134 completed (loss: 0.007804796565324068, acc: 0.998641312122345)
[2025-02-13 03:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:17][root][INFO] - Training Epoch: 2/2, step 24/7134 completed (loss: 0.012738333083689213, acc: 0.9944289922714233)
[2025-02-13 03:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:18][root][INFO] - Training Epoch: 2/2, step 25/7134 completed (loss: 0.017496107146143913, acc: 0.9957386255264282)
[2025-02-13 03:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:18][root][INFO] - Training Epoch: 2/2, step 26/7134 completed (loss: 0.031967129558324814, acc: 0.9867256879806519)
[2025-02-13 03:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19][root][INFO] - Training Epoch: 2/2, step 27/7134 completed (loss: 0.008275463245809078, acc: 0.9985228776931763)
[2025-02-13 03:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19][root][INFO] - Training Epoch: 2/2, step 28/7134 completed (loss: 0.04756227508187294, acc: 0.9879518151283264)
[2025-02-13 03:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:20][root][INFO] - Training Epoch: 2/2, step 29/7134 completed (loss: 0.035550281405448914, acc: 0.9886685609817505)
[2025-02-13 03:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:20][root][INFO] - Training Epoch: 2/2, step 30/7134 completed (loss: 0.01811295934021473, acc: 0.9954802393913269)
[2025-02-13 03:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:21][root][INFO] - Training Epoch: 2/2, step 31/7134 completed (loss: 0.03292977064847946, acc: 0.9883474707603455)
[2025-02-13 03:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:21][root][INFO] - Training Epoch: 2/2, step 32/7134 completed (loss: 0.05225909873843193, acc: 0.9882965087890625)
[2025-02-13 03:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:21][root][INFO] - Training Epoch: 2/2, step 33/7134 completed (loss: 0.09989475458860397, acc: 0.9820716977119446)
[2025-02-13 03:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22][root][INFO] - Training Epoch: 2/2, step 34/7134 completed (loss: 0.03563203290104866, acc: 0.9894490242004395)
[2025-02-13 03:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22][root][INFO] - Training Epoch: 2/2, step 35/7134 completed (loss: 0.04276232421398163, acc: 0.9868049025535583)
[2025-02-13 03:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:23][root][INFO] - Training Epoch: 2/2, step 36/7134 completed (loss: 0.03736129403114319, acc: 0.9913138151168823)
[2025-02-13 03:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:23][root][INFO] - Training Epoch: 2/2, step 37/7134 completed (loss: 0.036954525858163834, acc: 0.991239070892334)
[2025-02-13 03:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:24][root][INFO] - Training Epoch: 2/2, step 38/7134 completed (loss: 0.02291271835565567, acc: 0.9940476417541504)
[2025-02-13 03:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:24][root][INFO] - Training Epoch: 2/2, step 39/7134 completed (loss: 0.011372371576726437, acc: 0.9972936511039734)
[2025-02-13 03:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:25][root][INFO] - Training Epoch: 2/2, step 40/7134 completed (loss: 0.014934365637600422, acc: 0.9955157041549683)
[2025-02-13 03:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:25][root][INFO] - Training Epoch: 2/2, step 41/7134 completed (loss: 0.019888190552592278, acc: 0.9930651783943176)
[2025-02-13 03:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26][root][INFO] - Training Epoch: 2/2, step 42/7134 completed (loss: 0.058978382498025894, acc: 0.9901823401451111)
[2025-02-13 03:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26][root][INFO] - Training Epoch: 2/2, step 43/7134 completed (loss: 0.00804140418767929, acc: 0.996129035949707)
[2025-02-13 03:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26][root][INFO] - Training Epoch: 2/2, step 44/7134 completed (loss: 0.038610585033893585, acc: 0.993630588054657)
[2025-02-13 03:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:27][root][INFO] - Training Epoch: 2/2, step 45/7134 completed (loss: 0.03419400751590729, acc: 0.9945828914642334)
[2025-02-13 03:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:27][root][INFO] - Training Epoch: 2/2, step 46/7134 completed (loss: 0.03247404098510742, acc: 0.9919999837875366)
[2025-02-13 03:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:28][root][INFO] - Training Epoch: 2/2, step 47/7134 completed (loss: 0.03078649751842022, acc: 0.9898989796638489)
[2025-02-13 03:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:28][root][INFO] - Training Epoch: 2/2, step 48/7134 completed (loss: 0.011622893624007702, acc: 0.9966292381286621)
[2025-02-13 03:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29][root][INFO] - Training Epoch: 2/2, step 49/7134 completed (loss: 0.015102062374353409, acc: 0.9972565174102783)
[2025-02-13 03:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29][root][INFO] - Training Epoch: 2/2, step 50/7134 completed (loss: 0.008169794455170631, acc: 0.9965397715568542)
[2025-02-13 03:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:30][root][INFO] - Training Epoch: 2/2, step 51/7134 completed (loss: 0.008384490385651588, acc: 0.9974619150161743)
[2025-02-13 03:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:30][root][INFO] - Training Epoch: 2/2, step 52/7134 completed (loss: 0.0037093786522746086, acc: 1.0)
[2025-02-13 03:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31][root][INFO] - Training Epoch: 2/2, step 53/7134 completed (loss: 0.012437701225280762, acc: 0.9934383034706116)
[2025-02-13 03:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31][root][INFO] - Training Epoch: 2/2, step 54/7134 completed (loss: 0.005986445117741823, acc: 0.9988492727279663)
[2025-02-13 03:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31][root][INFO] - Training Epoch: 2/2, step 55/7134 completed (loss: 0.014334937557578087, acc: 0.9961928725242615)
[2025-02-13 03:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:32][root][INFO] - Training Epoch: 2/2, step 56/7134 completed (loss: 0.016890866681933403, acc: 0.9946428537368774)
[2025-02-13 03:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:32][root][INFO] - Training Epoch: 2/2, step 57/7134 completed (loss: 0.03970882669091225, acc: 0.9875776171684265)
[2025-02-13 03:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:33][root][INFO] - Training Epoch: 2/2, step 58/7134 completed (loss: 0.031554657965898514, acc: 0.9923175573348999)
[2025-02-13 03:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:33][root][INFO] - Training Epoch: 2/2, step 59/7134 completed (loss: 0.027266763150691986, acc: 0.9940898418426514)
[2025-02-13 03:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34][root][INFO] - Training Epoch: 2/2, step 60/7134 completed (loss: 0.02705197222530842, acc: 0.9900568127632141)
[2025-02-13 03:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34][root][INFO] - Training Epoch: 2/2, step 61/7134 completed (loss: 0.030264610424637794, acc: 0.9914320707321167)
[2025-02-13 03:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:35][root][INFO] - Training Epoch: 2/2, step 62/7134 completed (loss: 0.02490847371518612, acc: 0.9885641932487488)
[2025-02-13 03:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:35][root][INFO] - Training Epoch: 2/2, step 63/7134 completed (loss: 0.062487535178661346, acc: 0.9875466823577881)
[2025-02-13 03:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36][root][INFO] - Training Epoch: 2/2, step 64/7134 completed (loss: 0.019915373995900154, acc: 0.9919354915618896)
[2025-02-13 03:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36][root][INFO] - Training Epoch: 2/2, step 65/7134 completed (loss: 0.05302240699529648, acc: 0.9791154861450195)
[2025-02-13 03:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:37][root][INFO] - Training Epoch: 2/2, step 66/7134 completed (loss: 0.05616964399814606, acc: 0.9856630563735962)
[2025-02-13 03:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:37][root][INFO] - Training Epoch: 2/2, step 67/7134 completed (loss: 0.03431963920593262, acc: 0.98562091588974)
[2025-02-13 03:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:38][root][INFO] - Training Epoch: 2/2, step 68/7134 completed (loss: 0.045914411544799805, acc: 0.9870967864990234)
[2025-02-13 03:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:38][root][INFO] - Training Epoch: 2/2, step 69/7134 completed (loss: 0.06603348255157471, acc: 0.9775561094284058)
[2025-02-13 03:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:38][root][INFO] - Training Epoch: 2/2, step 70/7134 completed (loss: 0.054457515478134155, acc: 0.9789473414421082)
[2025-02-13 03:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39][root][INFO] - Training Epoch: 2/2, step 71/7134 completed (loss: 0.023428531363606453, acc: 0.994397759437561)
[2025-02-13 03:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39][root][INFO] - Training Epoch: 2/2, step 72/7134 completed (loss: 0.029731884598731995, acc: 0.9910485744476318)
[2025-02-13 03:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:40][root][INFO] - Training Epoch: 2/2, step 73/7134 completed (loss: 0.0221769530326128, acc: 0.9959239363670349)
[2025-02-13 03:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:40][root][INFO] - Training Epoch: 2/2, step 74/7134 completed (loss: 0.04061543196439743, acc: 0.9888579249382019)
[2025-02-13 03:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41][root][INFO] - Training Epoch: 2/2, step 75/7134 completed (loss: 0.06902268528938293, acc: 0.9835575222969055)
[2025-02-13 03:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41][root][INFO] - Training Epoch: 2/2, step 76/7134 completed (loss: 0.012032276019454002, acc: 0.997481107711792)
[2025-02-13 03:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:42][root][INFO] - Training Epoch: 2/2, step 77/7134 completed (loss: 0.021034356206655502, acc: 0.9949874877929688)
[2025-02-13 03:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:42][root][INFO] - Training Epoch: 2/2, step 78/7134 completed (loss: 0.03936168551445007, acc: 0.9858490824699402)
[2025-02-13 03:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:43][root][INFO] - Training Epoch: 2/2, step 79/7134 completed (loss: 0.04265633597970009, acc: 0.9830247163772583)
[2025-02-13 03:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:43][root][INFO] - Training Epoch: 2/2, step 80/7134 completed (loss: 0.031302522867918015, acc: 0.99609375)
[2025-02-13 03:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44][root][INFO] - Training Epoch: 2/2, step 81/7134 completed (loss: 0.029709171503782272, acc: 0.9934554696083069)
[2025-02-13 03:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44][root][INFO] - Training Epoch: 2/2, step 82/7134 completed (loss: 0.023598849773406982, acc: 0.9929278492927551)
[2025-02-13 03:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44][root][INFO] - Training Epoch: 2/2, step 83/7134 completed (loss: 0.021316707134246826, acc: 0.9922879338264465)
[2025-02-13 03:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:45][root][INFO] - Training Epoch: 2/2, step 84/7134 completed (loss: 0.01934410072863102, acc: 0.9930915236473083)
[2025-02-13 03:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:45][root][INFO] - Training Epoch: 2/2, step 85/7134 completed (loss: 0.05274350568652153, acc: 0.9880668520927429)
[2025-02-13 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46][root][INFO] - Training Epoch: 2/2, step 86/7134 completed (loss: 0.018948329612612724, acc: 0.9954128265380859)
[2025-02-13 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46][root][INFO] - Training Epoch: 2/2, step 87/7134 completed (loss: 0.09755459427833557, acc: 0.9794420003890991)
[2025-02-13 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:47][root][INFO] - Training Epoch: 2/2, step 88/7134 completed (loss: 0.06796379387378693, acc: 0.9820193648338318)
[2025-02-13 03:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:47][root][INFO] - Training Epoch: 2/2, step 89/7134 completed (loss: 0.023296276107430458, acc: 0.9910141229629517)
[2025-02-13 03:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48][root][INFO] - Training Epoch: 2/2, step 90/7134 completed (loss: 0.04132523760199547, acc: 0.9877622127532959)
[2025-02-13 03:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48][root][INFO] - Training Epoch: 2/2, step 91/7134 completed (loss: 0.039961133152246475, acc: 0.9857142567634583)
[2025-02-13 03:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:49][root][INFO] - Training Epoch: 2/2, step 92/7134 completed (loss: 0.04016571119427681, acc: 0.9873772859573364)
[2025-02-13 03:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:49][root][INFO] - Training Epoch: 2/2, step 93/7134 completed (loss: 0.014966952614486217, acc: 0.994490385055542)
[2025-02-13 03:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:49][root][INFO] - Training Epoch: 2/2, step 94/7134 completed (loss: 0.08010903745889664, acc: 0.9860788583755493)
[2025-02-13 03:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:50][root][INFO] - Training Epoch: 2/2, step 95/7134 completed (loss: 0.03279362991452217, acc: 0.9874371886253357)
[2025-02-13 03:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:50][root][INFO] - Training Epoch: 2/2, step 96/7134 completed (loss: 0.02615477330982685, acc: 0.9906890392303467)
[2025-02-13 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51][root][INFO] - Training Epoch: 2/2, step 97/7134 completed (loss: 0.16098415851593018, acc: 0.9518072009086609)
[2025-02-13 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51][root][INFO] - Training Epoch: 2/2, step 98/7134 completed (loss: 0.06637927144765854, acc: 0.9822006225585938)
[2025-02-13 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:52][root][INFO] - Training Epoch: 2/2, step 99/7134 completed (loss: 0.08248042315244675, acc: 0.9798164963722229)
[2025-02-13 03:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:52][root][INFO] - Training Epoch: 2/2, step 100/7134 completed (loss: 0.018927842378616333, acc: 0.9935897588729858)
[2025-02-13 03:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:53][root][INFO] - Training Epoch: 2/2, step 101/7134 completed (loss: 0.03968459367752075, acc: 0.9868420958518982)
[2025-02-13 03:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:53][root][INFO] - Training Epoch: 2/2, step 102/7134 completed (loss: 0.024379020556807518, acc: 0.9951159954071045)
[2025-02-13 03:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54][root][INFO] - Training Epoch: 2/2, step 103/7134 completed (loss: 0.026294885203242302, acc: 0.9900000095367432)
[2025-02-13 03:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54][root][INFO] - Training Epoch: 2/2, step 104/7134 completed (loss: 0.02318795770406723, acc: 0.9924471378326416)
[2025-02-13 03:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54][root][INFO] - Training Epoch: 2/2, step 105/7134 completed (loss: 0.01552833802998066, acc: 0.995106041431427)
[2025-02-13 03:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:55][root][INFO] - Training Epoch: 2/2, step 106/7134 completed (loss: 0.051662594079971313, acc: 0.9849435091018677)
[2025-02-13 03:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:55][root][INFO] - Training Epoch: 2/2, step 107/7134 completed (loss: 0.10486605018377304, acc: 0.9684813618659973)
[2025-02-13 03:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:56][root][INFO] - Training Epoch: 2/2, step 108/7134 completed (loss: 0.04772970825433731, acc: 0.9856114983558655)
[2025-02-13 03:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:56][root][INFO] - Training Epoch: 2/2, step 109/7134 completed (loss: 0.026052450761198997, acc: 0.9908088445663452)
[2025-02-13 03:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57][root][INFO] - Training Epoch: 2/2, step 110/7134 completed (loss: 0.044563088566064835, acc: 0.9847238659858704)
[2025-02-13 03:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57][root][INFO] - Training Epoch: 2/2, step 111/7134 completed (loss: 0.0343506895005703, acc: 0.99303138256073)
[2025-02-13 03:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:58][root][INFO] - Training Epoch: 2/2, step 112/7134 completed (loss: 0.03125867247581482, acc: 0.9892141819000244)
[2025-02-13 03:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:58][root][INFO] - Training Epoch: 2/2, step 113/7134 completed (loss: 0.016934063285589218, acc: 0.9962025284767151)
[2025-02-13 03:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:58][root][INFO] - Training Epoch: 2/2, step 114/7134 completed (loss: 0.026377776637673378, acc: 0.988063633441925)
[2025-02-13 03:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59][root][INFO] - Training Epoch: 2/2, step 115/7134 completed (loss: 0.06712301075458527, acc: 0.9711285829544067)
[2025-02-13 03:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59][root][INFO] - Training Epoch: 2/2, step 116/7134 completed (loss: 0.03549874201416969, acc: 0.9882214665412903)
[2025-02-13 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:00][root][INFO] - Training Epoch: 2/2, step 117/7134 completed (loss: 0.030827458947896957, acc: 0.9889196753501892)
[2025-02-13 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:00][root][INFO] - Training Epoch: 2/2, step 118/7134 completed (loss: 0.028884651139378548, acc: 0.9885203838348389)
[2025-02-13 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01][root][INFO] - Training Epoch: 2/2, step 119/7134 completed (loss: 0.036648787558078766, acc: 0.9842615127563477)
[2025-02-13 03:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01][root][INFO] - Training Epoch: 2/2, step 120/7134 completed (loss: 0.019169555976986885, acc: 0.993446946144104)
[2025-02-13 03:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:02][root][INFO] - Training Epoch: 2/2, step 121/7134 completed (loss: 0.021892055869102478, acc: 0.9933949708938599)
[2025-02-13 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:02][root][INFO] - Training Epoch: 2/2, step 122/7134 completed (loss: 0.02043250948190689, acc: 0.9933775067329407)
[2025-02-13 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03][root][INFO] - Training Epoch: 2/2, step 123/7134 completed (loss: 0.011068008840084076, acc: 0.9924924969673157)
[2025-02-13 03:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03][root][INFO] - Training Epoch: 2/2, step 124/7134 completed (loss: 0.019240310415625572, acc: 0.9948275685310364)
[2025-02-13 03:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03][root][INFO] - Training Epoch: 2/2, step 125/7134 completed (loss: 0.036944061517715454, acc: 0.9922566413879395)
[2025-02-13 03:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:04][root][INFO] - Training Epoch: 2/2, step 126/7134 completed (loss: 0.012126284651458263, acc: 0.9959999918937683)
[2025-02-13 03:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:04][root][INFO] - Training Epoch: 2/2, step 127/7134 completed (loss: 0.018751583993434906, acc: 0.9925925731658936)
[2025-02-13 03:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05][root][INFO] - Training Epoch: 2/2, step 128/7134 completed (loss: 0.019485732540488243, acc: 0.9953863620758057)
[2025-02-13 03:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05][root][INFO] - Training Epoch: 2/2, step 129/7134 completed (loss: 0.04658312723040581, acc: 0.9901960492134094)
[2025-02-13 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:06][root][INFO] - Training Epoch: 2/2, step 130/7134 completed (loss: 0.018393797799944878, acc: 0.9934210777282715)
[2025-02-13 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:06][root][INFO] - Training Epoch: 2/2, step 131/7134 completed (loss: 0.011576030403375626, acc: 0.9973683953285217)
[2025-02-13 03:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:07][root][INFO] - Training Epoch: 2/2, step 132/7134 completed (loss: 0.027013448998332024, acc: 0.9883889555931091)
[2025-02-13 03:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:07][root][INFO] - Training Epoch: 2/2, step 133/7134 completed (loss: 0.011259151622653008, acc: 0.9983360767364502)
[2025-02-13 03:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08][root][INFO] - Training Epoch: 2/2, step 134/7134 completed (loss: 0.025726420804858208, acc: 0.9946902394294739)
[2025-02-13 03:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08][root][INFO] - Training Epoch: 2/2, step 135/7134 completed (loss: 0.0218961164355278, acc: 0.9944444298744202)
[2025-02-13 03:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:09][root][INFO] - Training Epoch: 2/2, step 136/7134 completed (loss: 0.018999632447957993, acc: 0.9964157938957214)
[2025-02-13 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:09][root][INFO] - Training Epoch: 2/2, step 137/7134 completed (loss: 0.05197446793317795, acc: 0.9889298677444458)
[2025-02-13 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:10][root][INFO] - Training Epoch: 2/2, step 138/7134 completed (loss: 0.02334612049162388, acc: 0.993678867816925)
[2025-02-13 03:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:10][root][INFO] - Training Epoch: 2/2, step 139/7134 completed (loss: 0.04215415567159653, acc: 0.9910485744476318)
[2025-02-13 03:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11][root][INFO] - Training Epoch: 2/2, step 140/7134 completed (loss: 0.014543614350259304, acc: 0.9955223798751831)
[2025-02-13 03:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11][root][INFO] - Training Epoch: 2/2, step 141/7134 completed (loss: 0.004776467103511095, acc: 1.0)
[2025-02-13 03:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11][root][INFO] - Training Epoch: 2/2, step 142/7134 completed (loss: 0.06145530194044113, acc: 0.9902642369270325)
[2025-02-13 03:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:12][root][INFO] - Training Epoch: 2/2, step 143/7134 completed (loss: 0.01003301702439785, acc: 0.9960886836051941)
[2025-02-13 03:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:12][root][INFO] - Training Epoch: 2/2, step 144/7134 completed (loss: 0.06935393810272217, acc: 0.9816360473632812)
[2025-02-13 03:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:13][root][INFO] - Training Epoch: 2/2, step 145/7134 completed (loss: 0.1119793951511383, acc: 0.9704225063323975)
[2025-02-13 03:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:13][root][INFO] - Training Epoch: 2/2, step 146/7134 completed (loss: 0.04225366190075874, acc: 0.9871794581413269)
[2025-02-13 03:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:14][root][INFO] - Training Epoch: 2/2, step 147/7134 completed (loss: 0.05903313308954239, acc: 0.9861351847648621)
[2025-02-13 03:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:14][root][INFO] - Training Epoch: 2/2, step 148/7134 completed (loss: 0.15997250378131866, acc: 0.953596293926239)
[2025-02-13 03:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15][root][INFO] - Training Epoch: 2/2, step 149/7134 completed (loss: 0.11386822909116745, acc: 0.9754838943481445)
[2025-02-13 03:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15][root][INFO] - Training Epoch: 2/2, step 150/7134 completed (loss: 0.055822353810071945, acc: 0.98128342628479)
[2025-02-13 03:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:16][root][INFO] - Training Epoch: 2/2, step 151/7134 completed (loss: 0.048326633870601654, acc: 0.9856801629066467)
[2025-02-13 03:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:16][root][INFO] - Training Epoch: 2/2, step 152/7134 completed (loss: 0.042017627507448196, acc: 0.9897494316101074)
[2025-02-13 03:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:16][root][INFO] - Training Epoch: 2/2, step 153/7134 completed (loss: 0.08120320737361908, acc: 0.9818435907363892)
[2025-02-13 03:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:17][root][INFO] - Training Epoch: 2/2, step 154/7134 completed (loss: 0.04886693134903908, acc: 0.9908814430236816)
[2025-02-13 03:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:17][root][INFO] - Training Epoch: 2/2, step 155/7134 completed (loss: 0.03607496619224548, acc: 0.9870129823684692)
[2025-02-13 03:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18][root][INFO] - Training Epoch: 2/2, step 156/7134 completed (loss: 0.030415890738368034, acc: 0.9906651377677917)
[2025-02-13 03:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18][root][INFO] - Training Epoch: 2/2, step 157/7134 completed (loss: 0.06960486620664597, acc: 0.9915662407875061)
[2025-02-13 03:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:19][root][INFO] - Training Epoch: 2/2, step 158/7134 completed (loss: 0.021653134375810623, acc: 0.9933949708938599)
[2025-02-13 03:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:19][root][INFO] - Training Epoch: 2/2, step 159/7134 completed (loss: 0.048917900770902634, acc: 0.9855700135231018)
[2025-02-13 03:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:20][root][INFO] - Training Epoch: 2/2, step 160/7134 completed (loss: 0.04502284526824951, acc: 0.9858299493789673)
[2025-02-13 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:20][root][INFO] - Training Epoch: 2/2, step 161/7134 completed (loss: 0.021820731461048126, acc: 0.9883720874786377)
[2025-02-13 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:21][root][INFO] - Training Epoch: 2/2, step 162/7134 completed (loss: 0.05069827660918236, acc: 0.9769821166992188)
[2025-02-13 03:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:21][root][INFO] - Training Epoch: 2/2, step 163/7134 completed (loss: 0.047176964581012726, acc: 0.9881831407546997)
[2025-02-13 03:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:21][root][INFO] - Training Epoch: 2/2, step 164/7134 completed (loss: 0.03199787065386772, acc: 0.9914893507957458)
[2025-02-13 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22][root][INFO] - Training Epoch: 2/2, step 165/7134 completed (loss: 0.07369954138994217, acc: 0.980861246585846)
[2025-02-13 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22][root][INFO] - Training Epoch: 2/2, step 166/7134 completed (loss: 0.039954353123903275, acc: 0.9940298795700073)
[2025-02-13 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:23][root][INFO] - Training Epoch: 2/2, step 167/7134 completed (loss: 0.45471516251564026, acc: 0.9180035591125488)
[2025-02-13 03:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:23][root][INFO] - Training Epoch: 2/2, step 168/7134 completed (loss: 1.339847207069397, acc: 0.7372549176216125)
[2025-02-13 03:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:24][root][INFO] - Training Epoch: 2/2, step 169/7134 completed (loss: 0.9572385549545288, acc: 0.7899159789085388)
[2025-02-13 03:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:24][root][INFO] - Training Epoch: 2/2, step 170/7134 completed (loss: 0.37718406319618225, acc: 0.8918406367301941)
[2025-02-13 03:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:24][root][INFO] - Training Epoch: 2/2, step 171/7134 completed (loss: 0.43648281693458557, acc: 0.899350643157959)
[2025-02-13 03:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:25][root][INFO] - Training Epoch: 2/2, step 172/7134 completed (loss: 0.16832737624645233, acc: 0.9633252024650574)
[2025-02-13 03:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:25][root][INFO] - Training Epoch: 2/2, step 173/7134 completed (loss: 0.14825160801410675, acc: 0.9535398483276367)
[2025-02-13 03:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26][root][INFO] - Training Epoch: 2/2, step 174/7134 completed (loss: 0.17156465351581573, acc: 0.9507042169570923)
[2025-02-13 03:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26][root][INFO] - Training Epoch: 2/2, step 175/7134 completed (loss: 0.16684070229530334, acc: 0.9563636183738708)
[2025-02-13 03:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27][root][INFO] - Training Epoch: 2/2, step 176/7134 completed (loss: 0.15903300046920776, acc: 0.9521738886833191)
[2025-02-13 03:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27][root][INFO] - Training Epoch: 2/2, step 177/7134 completed (loss: 0.16233159601688385, acc: 0.9574074149131775)
[2025-02-13 03:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27][root][INFO] - Training Epoch: 2/2, step 178/7134 completed (loss: 0.10989681631326675, acc: 0.9689922332763672)
[2025-02-13 03:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:28][root][INFO] - Training Epoch: 2/2, step 179/7134 completed (loss: 0.06516752392053604, acc: 0.9767891764640808)
[2025-02-13 03:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:28][root][INFO] - Training Epoch: 2/2, step 180/7134 completed (loss: 0.05864839628338814, acc: 0.9845758080482483)
[2025-02-13 03:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:29][root][INFO] - Training Epoch: 2/2, step 181/7134 completed (loss: 0.0332745760679245, acc: 0.9881423115730286)
[2025-02-13 03:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:29][root][INFO] - Training Epoch: 2/2, step 182/7134 completed (loss: 0.026082435622811317, acc: 0.9938949942588806)
[2025-02-13 03:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30][root][INFO] - Training Epoch: 2/2, step 183/7134 completed (loss: 0.03414412960410118, acc: 0.9931880235671997)
[2025-02-13 03:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30][root][INFO] - Training Epoch: 2/2, step 184/7134 completed (loss: 0.039287108927965164, acc: 0.9877111911773682)
[2025-02-13 03:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:31][root][INFO] - Training Epoch: 2/2, step 185/7134 completed (loss: 0.017780965194106102, acc: 0.9928469061851501)
[2025-02-13 03:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:31][root][INFO] - Training Epoch: 2/2, step 186/7134 completed (loss: 0.04911581799387932, acc: 0.9887820482254028)
[2025-02-13 03:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:32][root][INFO] - Training Epoch: 2/2, step 187/7134 completed (loss: 0.041701655834913254, acc: 0.9860334992408752)
[2025-02-13 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:32][root][INFO] - Training Epoch: 2/2, step 188/7134 completed (loss: 0.07478058338165283, acc: 0.9791666865348816)
[2025-02-13 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:32][root][INFO] - Training Epoch: 2/2, step 189/7134 completed (loss: 0.053053759038448334, acc: 0.9865384697914124)
[2025-02-13 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:33][root][INFO] - Training Epoch: 2/2, step 190/7134 completed (loss: 0.11452033370733261, acc: 0.9765990376472473)
[2025-02-13 03:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:33][root][INFO] - Training Epoch: 2/2, step 191/7134 completed (loss: 0.042195942252874374, acc: 0.9935897588729858)
[2025-02-13 03:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34][root][INFO] - Training Epoch: 2/2, step 192/7134 completed (loss: 0.008069214411079884, acc: 0.9977090358734131)
[2025-02-13 03:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34][root][INFO] - Training Epoch: 2/2, step 193/7134 completed (loss: 0.039066288620233536, acc: 0.9892617464065552)
[2025-02-13 03:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:35][root][INFO] - Training Epoch: 2/2, step 194/7134 completed (loss: 0.0526394248008728, acc: 0.9799138903617859)
[2025-02-13 03:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:35][root][INFO] - Training Epoch: 2/2, step 195/7134 completed (loss: 0.0465584322810173, acc: 0.9798319339752197)
[2025-02-13 03:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:35][root][INFO] - Training Epoch: 2/2, step 196/7134 completed (loss: 0.06971923261880875, acc: 0.9826202988624573)
[2025-02-13 03:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:36][root][INFO] - Training Epoch: 2/2, step 197/7134 completed (loss: 0.02592025138437748, acc: 0.9971510171890259)
[2025-02-13 03:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:36][root][INFO] - Training Epoch: 2/2, step 198/7134 completed (loss: 0.023637646809220314, acc: 0.991037130355835)
[2025-02-13 03:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37][root][INFO] - Training Epoch: 2/2, step 199/7134 completed (loss: 0.03568471223115921, acc: 0.9885057210922241)
[2025-02-13 03:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37][root][INFO] - Training Epoch: 2/2, step 200/7134 completed (loss: 0.015122462064027786, acc: 0.9973154067993164)
[2025-02-13 03:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:38][root][INFO] - Training Epoch: 2/2, step 201/7134 completed (loss: 0.02847166359424591, acc: 0.9915730357170105)
[2025-02-13 03:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:38][root][INFO] - Training Epoch: 2/2, step 202/7134 completed (loss: 0.042482271790504456, acc: 0.9889655113220215)
[2025-02-13 03:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39][root][INFO] - Training Epoch: 2/2, step 203/7134 completed (loss: 0.07139518857002258, acc: 0.9779614210128784)
[2025-02-13 03:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39][root][INFO] - Training Epoch: 2/2, step 204/7134 completed (loss: 0.05454087257385254, acc: 0.9772036671638489)
[2025-02-13 03:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:40][root][INFO] - Training Epoch: 2/2, step 205/7134 completed (loss: 0.06340976804494858, acc: 0.983182430267334)
[2025-02-13 03:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:40][root][INFO] - Training Epoch: 2/2, step 206/7134 completed (loss: 0.08056935667991638, acc: 0.9823434948921204)
[2025-02-13 03:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:40][root][INFO] - Training Epoch: 2/2, step 207/7134 completed (loss: 0.05453333258628845, acc: 0.9829351305961609)
[2025-02-13 03:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:41][root][INFO] - Training Epoch: 2/2, step 208/7134 completed (loss: 0.09590977430343628, acc: 0.9764705896377563)
[2025-02-13 03:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:41][root][INFO] - Training Epoch: 2/2, step 209/7134 completed (loss: 0.10542667657136917, acc: 0.977979302406311)
[2025-02-13 03:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42][root][INFO] - Training Epoch: 2/2, step 210/7134 completed (loss: 0.0869479700922966, acc: 0.9759904146194458)
[2025-02-13 03:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42][root][INFO] - Training Epoch: 2/2, step 211/7134 completed (loss: 0.1009017750620842, acc: 0.9673202633857727)
[2025-02-13 03:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:43][root][INFO] - Training Epoch: 2/2, step 212/7134 completed (loss: 0.1462077647447586, acc: 0.9793388247489929)
[2025-02-13 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:43][root][INFO] - Training Epoch: 2/2, step 213/7134 completed (loss: 0.06991662085056305, acc: 0.9822006225585938)
[2025-02-13 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:43][root][INFO] - Training Epoch: 2/2, step 214/7134 completed (loss: 0.06263189762830734, acc: 0.9807692170143127)
[2025-02-13 03:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44][root][INFO] - Training Epoch: 2/2, step 215/7134 completed (loss: 0.03563494607806206, acc: 0.9878472089767456)
[2025-02-13 03:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44][root][INFO] - Training Epoch: 2/2, step 216/7134 completed (loss: 0.02677759900689125, acc: 0.9930555820465088)
[2025-02-13 03:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:45][root][INFO] - Training Epoch: 2/2, step 217/7134 completed (loss: 0.036602821201086044, acc: 0.9903030395507812)
[2025-02-13 03:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:45][root][INFO] - Training Epoch: 2/2, step 218/7134 completed (loss: 0.017868027091026306, acc: 0.9949109554290771)
[2025-02-13 03:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:46][root][INFO] - Training Epoch: 2/2, step 219/7134 completed (loss: 0.045212164521217346, acc: 0.9902533888816833)
[2025-02-13 03:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:46][root][INFO] - Training Epoch: 2/2, step 220/7134 completed (loss: 0.03981636092066765, acc: 0.9937238693237305)
[2025-02-13 03:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47][root][INFO] - Training Epoch: 2/2, step 221/7134 completed (loss: 0.0302642360329628, acc: 0.9880596995353699)
[2025-02-13 03:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47][root][INFO] - Training Epoch: 2/2, step 222/7134 completed (loss: 0.043287839740514755, acc: 0.985602080821991)
[2025-02-13 03:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:48][root][INFO] - Training Epoch: 2/2, step 223/7134 completed (loss: 0.06825138628482819, acc: 0.9794608354568481)
[2025-02-13 03:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:48][root][INFO] - Training Epoch: 2/2, step 224/7134 completed (loss: 0.09581897407770157, acc: 0.9766536951065063)
[2025-02-13 03:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:49][root][INFO] - Training Epoch: 2/2, step 225/7134 completed (loss: 0.09942129254341125, acc: 0.9756097793579102)
[2025-02-13 03:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:49][root][INFO] - Training Epoch: 2/2, step 226/7134 completed (loss: 0.028471361845731735, acc: 0.995110034942627)
[2025-02-13 03:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:49][root][INFO] - Training Epoch: 2/2, step 227/7134 completed (loss: 0.11809662729501724, acc: 0.9714640378952026)
[2025-02-13 03:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:50][root][INFO] - Training Epoch: 2/2, step 228/7134 completed (loss: 0.06668508052825928, acc: 0.9840810298919678)
[2025-02-13 03:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:50][root][INFO] - Training Epoch: 2/2, step 229/7134 completed (loss: 0.09990555793046951, acc: 0.9734176993370056)
[2025-02-13 03:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:51][root][INFO] - Training Epoch: 2/2, step 230/7134 completed (loss: 0.037264905869960785, acc: 0.9877150058746338)
[2025-02-13 03:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:51][root][INFO] - Training Epoch: 2/2, step 231/7134 completed (loss: 0.04463999345898628, acc: 0.9887005686759949)
[2025-02-13 03:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52][root][INFO] - Training Epoch: 2/2, step 232/7134 completed (loss: 0.04394185170531273, acc: 0.9917627573013306)
[2025-02-13 03:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52][root][INFO] - Training Epoch: 2/2, step 233/7134 completed (loss: 0.0511663593351841, acc: 0.9857369065284729)
[2025-02-13 03:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53][root][INFO] - Training Epoch: 2/2, step 234/7134 completed (loss: 0.06044498831033707, acc: 0.9831081032752991)
[2025-02-13 03:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53][root][INFO] - Training Epoch: 2/2, step 235/7134 completed (loss: 0.10052371025085449, acc: 0.9757834672927856)
[2025-02-13 03:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53][root][INFO] - Training Epoch: 2/2, step 236/7134 completed (loss: 0.018989505246281624, acc: 0.996874988079071)
[2025-02-13 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54][root][INFO] - Training Epoch: 2/2, step 237/7134 completed (loss: 0.055039726197719574, acc: 0.9865871667861938)
[2025-02-13 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54][root][INFO] - Training Epoch: 2/2, step 238/7134 completed (loss: 0.07182025909423828, acc: 0.9906716346740723)
[2025-02-13 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:55][root][INFO] - Training Epoch: 2/2, step 239/7134 completed (loss: 0.015354912728071213, acc: 0.993697464466095)
[2025-02-13 03:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:55][root][INFO] - Training Epoch: 2/2, step 240/7134 completed (loss: 0.06279376149177551, acc: 0.9830508232116699)
[2025-02-13 03:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:56][root][INFO] - Training Epoch: 2/2, step 241/7134 completed (loss: 0.09046546369791031, acc: 0.9806763529777527)
[2025-02-13 03:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:56][root][INFO] - Training Epoch: 2/2, step 242/7134 completed (loss: 0.06840316951274872, acc: 0.9774696826934814)
[2025-02-13 03:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:56][root][INFO] - Training Epoch: 2/2, step 243/7134 completed (loss: 0.09727750718593597, acc: 0.9744898080825806)
[2025-02-13 03:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:57][root][INFO] - Training Epoch: 2/2, step 244/7134 completed (loss: 0.017383992671966553, acc: 0.993966817855835)
[2025-02-13 03:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:57][root][INFO] - Training Epoch: 2/2, step 245/7134 completed (loss: 0.07767727971076965, acc: 0.9807355403900146)
[2025-02-13 03:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58][root][INFO] - Training Epoch: 2/2, step 246/7134 completed (loss: 0.0380576029419899, acc: 0.9873816967010498)
[2025-02-13 03:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58][root][INFO] - Training Epoch: 2/2, step 247/7134 completed (loss: 0.07582709193229675, acc: 0.9782214164733887)
[2025-02-13 03:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:59][root][INFO] - Training Epoch: 2/2, step 248/7134 completed (loss: 0.032705921679735184, acc: 0.9864341020584106)
[2025-02-13 03:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:59][root][INFO] - Training Epoch: 2/2, step 249/7134 completed (loss: 0.03848019242286682, acc: 0.9865951538085938)
[2025-02-13 03:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:59][root][INFO] - Training Epoch: 2/2, step 250/7134 completed (loss: 0.05036335065960884, acc: 0.9794303774833679)
[2025-02-13 03:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00][root][INFO] - Training Epoch: 2/2, step 251/7134 completed (loss: 0.03961736708879471, acc: 0.9863247871398926)
[2025-02-13 03:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00][root][INFO] - Training Epoch: 2/2, step 252/7134 completed (loss: 0.026819095015525818, acc: 0.9918032884597778)
[2025-02-13 03:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:01][root][INFO] - Training Epoch: 2/2, step 253/7134 completed (loss: 0.026898669078946114, acc: 0.987522304058075)
[2025-02-13 03:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:01][root][INFO] - Training Epoch: 2/2, step 254/7134 completed (loss: 0.023850159719586372, acc: 0.9928571581840515)
[2025-02-13 03:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:02][root][INFO] - Training Epoch: 2/2, step 255/7134 completed (loss: 0.03472519665956497, acc: 0.9912280440330505)
[2025-02-13 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:02][root][INFO] - Training Epoch: 2/2, step 256/7134 completed (loss: 0.05264635384082794, acc: 0.9816176295280457)
[2025-02-13 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03][root][INFO] - Training Epoch: 2/2, step 257/7134 completed (loss: 0.034964028745889664, acc: 0.987730085849762)
[2025-02-13 03:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03][root][INFO] - Training Epoch: 2/2, step 258/7134 completed (loss: 0.03331352770328522, acc: 0.9917355179786682)
[2025-02-13 03:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03][root][INFO] - Training Epoch: 2/2, step 259/7134 completed (loss: 0.05122523382306099, acc: 0.9854133129119873)
[2025-02-13 03:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:04][root][INFO] - Training Epoch: 2/2, step 260/7134 completed (loss: 0.03900272399187088, acc: 0.9915397763252258)
[2025-02-13 03:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:04][root][INFO] - Training Epoch: 2/2, step 261/7134 completed (loss: 0.026674602180719376, acc: 0.9959100484848022)
[2025-02-13 03:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:05][root][INFO] - Training Epoch: 2/2, step 262/7134 completed (loss: 0.01282492931932211, acc: 0.9966722130775452)
[2025-02-13 03:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:05][root][INFO] - Training Epoch: 2/2, step 263/7134 completed (loss: 0.03281334787607193, acc: 0.9897828698158264)
[2025-02-13 03:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06][root][INFO] - Training Epoch: 2/2, step 264/7134 completed (loss: 0.015851760283112526, acc: 0.9964953064918518)
[2025-02-13 03:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06][root][INFO] - Training Epoch: 2/2, step 265/7134 completed (loss: 0.04938890039920807, acc: 0.9871134161949158)
[2025-02-13 03:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:07][root][INFO] - Training Epoch: 2/2, step 266/7134 completed (loss: 0.023181088268756866, acc: 0.9931192398071289)
[2025-02-13 03:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:07][root][INFO] - Training Epoch: 2/2, step 267/7134 completed (loss: 0.022877957671880722, acc: 0.9942528605461121)
[2025-02-13 03:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08][root][INFO] - Training Epoch: 2/2, step 268/7134 completed (loss: 0.046109139919281006, acc: 0.9870298504829407)
[2025-02-13 03:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08][root][INFO] - Training Epoch: 2/2, step 269/7134 completed (loss: 0.03304092213511467, acc: 0.9902642369270325)
[2025-02-13 03:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08][root][INFO] - Training Epoch: 2/2, step 270/7134 completed (loss: 0.03368764743208885, acc: 0.9925373196601868)
[2025-02-13 03:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:09][root][INFO] - Training Epoch: 2/2, step 271/7134 completed (loss: 0.032418593764305115, acc: 0.9911727905273438)
[2025-02-13 03:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:09][root][INFO] - Training Epoch: 2/2, step 272/7134 completed (loss: 0.025053367018699646, acc: 0.9923664331436157)
[2025-02-13 03:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:10][root][INFO] - Training Epoch: 2/2, step 273/7134 completed (loss: 0.015210183337330818, acc: 0.9940119981765747)
[2025-02-13 03:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:10][root][INFO] - Training Epoch: 2/2, step 274/7134 completed (loss: 0.03161012753844261, acc: 0.993122398853302)
[2025-02-13 03:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:11][root][INFO] - Training Epoch: 2/2, step 275/7134 completed (loss: 0.020224744454026222, acc: 0.9930394291877747)
[2025-02-13 03:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:11][root][INFO] - Training Epoch: 2/2, step 276/7134 completed (loss: 0.02565588988363743, acc: 0.9890377521514893)
[2025-02-13 03:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12][root][INFO] - Training Epoch: 2/2, step 277/7134 completed (loss: 0.04062996059656143, acc: 0.9897040128707886)
[2025-02-13 03:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12][root][INFO] - Training Epoch: 2/2, step 278/7134 completed (loss: 0.020193755626678467, acc: 0.992337167263031)
[2025-02-13 03:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:13][root][INFO] - Training Epoch: 2/2, step 279/7134 completed (loss: 0.028888797387480736, acc: 0.9895012974739075)
[2025-02-13 03:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:13][root][INFO] - Training Epoch: 2/2, step 280/7134 completed (loss: 0.037496548146009445, acc: 0.9856630563735962)
[2025-02-13 03:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:13][root][INFO] - Training Epoch: 2/2, step 281/7134 completed (loss: 0.04608048498630524, acc: 0.9879999756813049)
[2025-02-13 03:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:14][root][INFO] - Training Epoch: 2/2, step 282/7134 completed (loss: 0.06697908043861389, acc: 0.9786259531974792)
[2025-02-13 03:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:14][root][INFO] - Training Epoch: 2/2, step 283/7134 completed (loss: 0.036673370748758316, acc: 0.9915110468864441)
[2025-02-13 03:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15][root][INFO] - Training Epoch: 2/2, step 284/7134 completed (loss: 0.013792697340250015, acc: 0.9932885766029358)
[2025-02-13 03:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15][root][INFO] - Training Epoch: 2/2, step 285/7134 completed (loss: 0.025434009730815887, acc: 0.9876543283462524)
[2025-02-13 03:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:16][root][INFO] - Training Epoch: 2/2, step 286/7134 completed (loss: 0.0479615218937397, acc: 0.9929203391075134)
[2025-02-13 03:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:16][root][INFO] - Training Epoch: 2/2, step 287/7134 completed (loss: 0.02736489661037922, acc: 0.9935794472694397)
[2025-02-13 03:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:16][root][INFO] - Training Epoch: 2/2, step 288/7134 completed (loss: 0.04256059229373932, acc: 0.9871086478233337)
[2025-02-13 03:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:17][root][INFO] - Training Epoch: 2/2, step 289/7134 completed (loss: 0.03130505234003067, acc: 0.9873417615890503)
[2025-02-13 03:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:17][root][INFO] - Training Epoch: 2/2, step 290/7134 completed (loss: 0.025107858702540398, acc: 0.9924012422561646)
[2025-02-13 03:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18][root][INFO] - Training Epoch: 2/2, step 291/7134 completed (loss: 0.06486139446496964, acc: 0.9802631735801697)
[2025-02-13 03:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18][root][INFO] - Training Epoch: 2/2, step 292/7134 completed (loss: 0.06001167744398117, acc: 0.9815950989723206)
[2025-02-13 03:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:19][root][INFO] - Training Epoch: 2/2, step 293/7134 completed (loss: 0.027371490374207497, acc: 0.9879518151283264)
[2025-02-13 03:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:19][root][INFO] - Training Epoch: 2/2, step 294/7134 completed (loss: 0.040842145681381226, acc: 0.9844054579734802)
[2025-02-13 03:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:20][root][INFO] - Training Epoch: 2/2, step 295/7134 completed (loss: 0.05994535610079765, acc: 0.9778156876564026)
[2025-02-13 03:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:20][root][INFO] - Training Epoch: 2/2, step 296/7134 completed (loss: 0.025347726419568062, acc: 0.9906976819038391)
[2025-02-13 03:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21][root][INFO] - Training Epoch: 2/2, step 297/7134 completed (loss: 0.028846202418208122, acc: 0.9928951859474182)
[2025-02-13 03:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21][root][INFO] - Training Epoch: 2/2, step 298/7134 completed (loss: 0.022624479606747627, acc: 0.995199978351593)
[2025-02-13 03:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21][root][INFO] - Training Epoch: 2/2, step 299/7134 completed (loss: 0.037519752979278564, acc: 0.9864661693572998)
[2025-02-13 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:22][root][INFO] - Training Epoch: 2/2, step 300/7134 completed (loss: 0.024355517700314522, acc: 0.9886731505393982)
[2025-02-13 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:22][root][INFO] - Training Epoch: 2/2, step 301/7134 completed (loss: 0.02356034703552723, acc: 0.9900990128517151)
[2025-02-13 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:23][root][INFO] - Training Epoch: 2/2, step 302/7134 completed (loss: 0.042780887335538864, acc: 0.9902533888816833)
[2025-02-13 03:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:23][root][INFO] - Training Epoch: 2/2, step 303/7134 completed (loss: 0.03514775261282921, acc: 0.9902912378311157)
[2025-02-13 03:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24][root][INFO] - Training Epoch: 2/2, step 304/7134 completed (loss: 0.017660869285464287, acc: 0.9925925731658936)
[2025-02-13 03:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24][root][INFO] - Training Epoch: 2/2, step 305/7134 completed (loss: 0.019811293110251427, acc: 0.9920508861541748)
[2025-02-13 03:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24][root][INFO] - Training Epoch: 2/2, step 306/7134 completed (loss: 0.0037575324531644583, acc: 1.0)
[2025-02-13 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:25][root][INFO] - Training Epoch: 2/2, step 307/7134 completed (loss: 0.060112230479717255, acc: 0.982206404209137)
[2025-02-13 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:25][root][INFO] - Training Epoch: 2/2, step 308/7134 completed (loss: 0.014611070044338703, acc: 0.9967105388641357)
[2025-02-13 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:26][root][INFO] - Training Epoch: 2/2, step 309/7134 completed (loss: 0.02460254728794098, acc: 0.9896551966667175)
[2025-02-13 03:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:26][root][INFO] - Training Epoch: 2/2, step 310/7134 completed (loss: 0.013294312171638012, acc: 0.9985380172729492)
[2025-02-13 03:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:27][root][INFO] - Training Epoch: 2/2, step 311/7134 completed (loss: 0.015287341549992561, acc: 0.996889591217041)
[2025-02-13 03:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:27][root][INFO] - Training Epoch: 2/2, step 312/7134 completed (loss: 0.017045052722096443, acc: 0.9965338110923767)
[2025-02-13 03:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28][root][INFO] - Training Epoch: 2/2, step 313/7134 completed (loss: 0.0176781564950943, acc: 0.9947826266288757)
[2025-02-13 03:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28][root][INFO] - Training Epoch: 2/2, step 314/7134 completed (loss: 0.022049279883503914, acc: 0.9937499761581421)
[2025-02-13 03:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28][root][INFO] - Training Epoch: 2/2, step 315/7134 completed (loss: 0.026283761486411095, acc: 0.9950819611549377)
[2025-02-13 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:29][root][INFO] - Training Epoch: 2/2, step 316/7134 completed (loss: 0.0267565306276083, acc: 0.9927797913551331)
[2025-02-13 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:29][root][INFO] - Training Epoch: 2/2, step 317/7134 completed (loss: 0.002804787829518318, acc: 1.0)
[2025-02-13 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30][root][INFO] - Training Epoch: 2/2, step 318/7134 completed (loss: 0.05221797898411751, acc: 0.9877899885177612)
[2025-02-13 03:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30][root][INFO] - Training Epoch: 2/2, step 319/7134 completed (loss: 0.0467936247587204, acc: 0.9887797832489014)
[2025-02-13 03:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:31][root][INFO] - Training Epoch: 2/2, step 320/7134 completed (loss: 0.04541546851396561, acc: 0.9914407730102539)
[2025-02-13 03:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:31][root][INFO] - Training Epoch: 2/2, step 321/7134 completed (loss: 0.07156930863857269, acc: 0.9782886505126953)
[2025-02-13 03:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:32][root][INFO] - Training Epoch: 2/2, step 322/7134 completed (loss: 0.04249250143766403, acc: 0.9849849939346313)
[2025-02-13 03:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:32][root][INFO] - Training Epoch: 2/2, step 323/7134 completed (loss: 0.050775811076164246, acc: 0.9897304177284241)
[2025-02-13 03:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33][root][INFO] - Training Epoch: 2/2, step 324/7134 completed (loss: 0.05878904461860657, acc: 0.9838895201683044)
[2025-02-13 03:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33][root][INFO] - Training Epoch: 2/2, step 325/7134 completed (loss: 0.021353455260396004, acc: 0.9922879338264465)
[2025-02-13 03:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33][root][INFO] - Training Epoch: 2/2, step 326/7134 completed (loss: 0.019080016762018204, acc: 0.9951279163360596)
[2025-02-13 03:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:34][root][INFO] - Training Epoch: 2/2, step 327/7134 completed (loss: 0.05403507500886917, acc: 0.9853603839874268)
[2025-02-13 03:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:34][root][INFO] - Training Epoch: 2/2, step 328/7134 completed (loss: 0.025920456275343895, acc: 0.9940740466117859)
[2025-02-13 03:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:35][root][INFO] - Training Epoch: 2/2, step 329/7134 completed (loss: 0.01007162407040596, acc: 0.9964747428894043)
[2025-02-13 03:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:35][root][INFO] - Training Epoch: 2/2, step 330/7134 completed (loss: 0.021363316103816032, acc: 0.9919435977935791)
[2025-02-13 03:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36][root][INFO] - Training Epoch: 2/2, step 331/7134 completed (loss: 0.02088303677737713, acc: 0.9920182228088379)
[2025-02-13 03:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36][root][INFO] - Training Epoch: 2/2, step 332/7134 completed (loss: 0.009352528490126133, acc: 0.9976387023925781)
[2025-02-13 03:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:37][root][INFO] - Training Epoch: 2/2, step 333/7134 completed (loss: 0.02847049944102764, acc: 0.9902676343917847)
[2025-02-13 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:37][root][INFO] - Training Epoch: 2/2, step 334/7134 completed (loss: 0.016530204564332962, acc: 0.9949302673339844)
[2025-02-13 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:38][root][INFO] - Training Epoch: 2/2, step 335/7134 completed (loss: 0.01749633438885212, acc: 0.9970887899398804)
[2025-02-13 03:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:38][root][INFO] - Training Epoch: 2/2, step 336/7134 completed (loss: 0.03899654000997543, acc: 0.9904191493988037)
[2025-02-13 03:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39][root][INFO] - Training Epoch: 2/2, step 337/7134 completed (loss: 0.011572225019335747, acc: 0.9984662532806396)
[2025-02-13 03:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39][root][INFO] - Training Epoch: 2/2, step 338/7134 completed (loss: 0.06045622378587723, acc: 0.989924430847168)
[2025-02-13 03:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39][root][INFO] - Training Epoch: 2/2, step 339/7134 completed (loss: 0.013538156636059284, acc: 0.9937499761581421)
[2025-02-13 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:40][root][INFO] - Training Epoch: 2/2, step 340/7134 completed (loss: 0.033222585916519165, acc: 0.9872773289680481)
[2025-02-13 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:40][root][INFO] - Training Epoch: 2/2, step 341/7134 completed (loss: 0.015810726210474968, acc: 0.9936948418617249)
[2025-02-13 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41][root][INFO] - Training Epoch: 2/2, step 342/7134 completed (loss: 0.012671349570155144, acc: 0.9954545497894287)
[2025-02-13 03:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41][root][INFO] - Training Epoch: 2/2, step 343/7134 completed (loss: 0.03257429227232933, acc: 0.9905481934547424)
[2025-02-13 03:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:42][root][INFO] - Training Epoch: 2/2, step 344/7134 completed (loss: 0.013871635310351849, acc: 0.9963503479957581)
[2025-02-13 03:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:42][root][INFO] - Training Epoch: 2/2, step 345/7134 completed (loss: 0.02512350305914879, acc: 0.9946428537368774)
[2025-02-13 03:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:43][root][INFO] - Training Epoch: 2/2, step 346/7134 completed (loss: 0.023616502061486244, acc: 0.9920508861541748)
[2025-02-13 03:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:43][root][INFO] - Training Epoch: 2/2, step 347/7134 completed (loss: 0.03045072965323925, acc: 0.9901823401451111)
[2025-02-13 03:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:43][root][INFO] - Training Epoch: 2/2, step 348/7134 completed (loss: 0.007162146735936403, acc: 0.998643159866333)
[2025-02-13 03:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44][root][INFO] - Training Epoch: 2/2, step 349/7134 completed (loss: 0.022255893796682358, acc: 0.991946280002594)
[2025-02-13 03:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44][root][INFO] - Training Epoch: 2/2, step 350/7134 completed (loss: 0.029914550483226776, acc: 0.9915764331817627)
[2025-02-13 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:45][root][INFO] - Training Epoch: 2/2, step 351/7134 completed (loss: 0.05004182830452919, acc: 0.9872773289680481)
[2025-02-13 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:45][root][INFO] - Training Epoch: 2/2, step 352/7134 completed (loss: 0.013071258552372456, acc: 0.9972826242446899)
[2025-02-13 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:46][root][INFO] - Training Epoch: 2/2, step 353/7134 completed (loss: 0.019552752375602722, acc: 0.9933510422706604)
[2025-02-13 03:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:46][root][INFO] - Training Epoch: 2/2, step 354/7134 completed (loss: 0.01309007778763771, acc: 0.9976717233657837)
[2025-02-13 03:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47][root][INFO] - Training Epoch: 2/2, step 355/7134 completed (loss: 0.052234090864658356, acc: 0.9916201233863831)
[2025-02-13 03:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47][root][INFO] - Training Epoch: 2/2, step 356/7134 completed (loss: 0.018303658813238144, acc: 0.9960317611694336)
[2025-02-13 03:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:48][root][INFO] - Training Epoch: 2/2, step 357/7134 completed (loss: 0.017990903928875923, acc: 0.9949109554290771)
[2025-02-13 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:48][root][INFO] - Training Epoch: 2/2, step 358/7134 completed (loss: 0.019106131047010422, acc: 0.9918144345283508)
[2025-02-13 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:49][root][INFO] - Training Epoch: 2/2, step 359/7134 completed (loss: 0.013397176750004292, acc: 0.9945945739746094)
[2025-02-13 03:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:49][root][INFO] - Training Epoch: 2/2, step 360/7134 completed (loss: 0.017042504623532295, acc: 0.9939758777618408)
[2025-02-13 03:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:49][root][INFO] - Training Epoch: 2/2, step 361/7134 completed (loss: 0.019061332568526268, acc: 0.9955947399139404)
[2025-02-13 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:50][root][INFO] - Training Epoch: 2/2, step 362/7134 completed (loss: 0.03367051109671593, acc: 0.992514967918396)
[2025-02-13 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:50][root][INFO] - Training Epoch: 2/2, step 363/7134 completed (loss: 0.03822656720876694, acc: 0.9895833134651184)
[2025-02-13 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51][root][INFO] - Training Epoch: 2/2, step 364/7134 completed (loss: 0.014797812327742577, acc: 0.9929906725883484)
[2025-02-13 03:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51][root][INFO] - Training Epoch: 2/2, step 365/7134 completed (loss: 0.017335012555122375, acc: 0.9986225962638855)
[2025-02-13 03:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:52][root][INFO] - Training Epoch: 2/2, step 366/7134 completed (loss: 0.017976826056838036, acc: 0.9937304258346558)
[2025-02-13 03:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:52][root][INFO] - Training Epoch: 2/2, step 367/7134 completed (loss: 0.016725024208426476, acc: 0.9937499761581421)
[2025-02-13 03:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53][root][INFO] - Training Epoch: 2/2, step 368/7134 completed (loss: 0.013220340944826603, acc: 0.9955423474311829)
[2025-02-13 03:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53][root][INFO] - Training Epoch: 2/2, step 369/7134 completed (loss: 0.023093346506357193, acc: 0.992343008518219)
[2025-02-13 03:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53][root][INFO] - Training Epoch: 2/2, step 370/7134 completed (loss: 0.03511976823210716, acc: 0.9883551597595215)
[2025-02-13 03:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:54][root][INFO] - Training Epoch: 2/2, step 371/7134 completed (loss: 0.0192570760846138, acc: 0.9924812316894531)
[2025-02-13 03:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:54][root][INFO] - Training Epoch: 2/2, step 372/7134 completed (loss: 0.03293680027127266, acc: 0.9881129264831543)
[2025-02-13 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:55][root][INFO] - Training Epoch: 2/2, step 373/7134 completed (loss: 0.011026936583220959, acc: 0.9955157041549683)
[2025-02-13 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:55][root][INFO] - Training Epoch: 2/2, step 374/7134 completed (loss: 0.02465318702161312, acc: 0.9935691356658936)
[2025-02-13 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:56][root][INFO] - Training Epoch: 2/2, step 375/7134 completed (loss: 0.029440773651003838, acc: 0.9919678568840027)
[2025-02-13 03:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:56][root][INFO] - Training Epoch: 2/2, step 376/7134 completed (loss: 0.03166034445166588, acc: 0.9903448224067688)
[2025-02-13 03:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57][root][INFO] - Training Epoch: 2/2, step 377/7134 completed (loss: 0.020839737728238106, acc: 0.993122398853302)
[2025-02-13 03:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57][root][INFO] - Training Epoch: 2/2, step 378/7134 completed (loss: 0.010976266115903854, acc: 0.9972752332687378)
[2025-02-13 03:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:58][root][INFO] - Training Epoch: 2/2, step 379/7134 completed (loss: 0.015596802346408367, acc: 0.9949748516082764)
[2025-02-13 03:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:58][root][INFO] - Training Epoch: 2/2, step 380/7134 completed (loss: 0.051006413996219635, acc: 0.9789750576019287)
[2025-02-13 03:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:58][root][INFO] - Training Epoch: 2/2, step 381/7134 completed (loss: 0.05527384579181671, acc: 0.9870800971984863)
[2025-02-13 03:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:59][root][INFO] - Training Epoch: 2/2, step 382/7134 completed (loss: 0.0217630323022604, acc: 0.9922978281974792)
[2025-02-13 03:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:59][root][INFO] - Training Epoch: 2/2, step 383/7134 completed (loss: 0.02272900938987732, acc: 0.992438554763794)
[2025-02-13 03:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00][root][INFO] - Training Epoch: 2/2, step 384/7134 completed (loss: 0.030223021283745766, acc: 0.9919893145561218)
[2025-02-13 03:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00][root][INFO] - Training Epoch: 2/2, step 385/7134 completed (loss: 0.01877679117023945, acc: 0.9952903985977173)
[2025-02-13 03:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:01][root][INFO] - Training Epoch: 2/2, step 386/7134 completed (loss: 0.017504241317510605, acc: 0.9938650131225586)
[2025-02-13 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:01][root][INFO] - Training Epoch: 2/2, step 387/7134 completed (loss: 0.03962370380759239, acc: 0.9868804812431335)
[2025-02-13 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:01][root][INFO] - Training Epoch: 2/2, step 388/7134 completed (loss: 0.010126720182597637, acc: 0.9964788556098938)
[2025-02-13 03:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:02][root][INFO] - Training Epoch: 2/2, step 389/7134 completed (loss: 0.039577629417181015, acc: 0.9862259030342102)
[2025-02-13 03:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:02][root][INFO] - Training Epoch: 2/2, step 390/7134 completed (loss: 0.02363457717001438, acc: 0.9935275316238403)
[2025-02-13 03:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03][root][INFO] - Training Epoch: 2/2, step 391/7134 completed (loss: 0.04191206395626068, acc: 0.9863945841789246)
[2025-02-13 03:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03][root][INFO] - Training Epoch: 2/2, step 392/7134 completed (loss: 0.024106603115797043, acc: 0.9896296262741089)
[2025-02-13 03:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:04][root][INFO] - Training Epoch: 2/2, step 393/7134 completed (loss: 0.025386836379766464, acc: 0.9888888597488403)
[2025-02-13 03:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:04][root][INFO] - Training Epoch: 2/2, step 394/7134 completed (loss: 0.024548187851905823, acc: 0.9902724027633667)
[2025-02-13 03:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:05][root][INFO] - Training Epoch: 2/2, step 395/7134 completed (loss: 0.06509323418140411, acc: 0.9876543283462524)
[2025-02-13 03:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:05][root][INFO] - Training Epoch: 2/2, step 396/7134 completed (loss: 0.01764211617410183, acc: 0.9935483932495117)
[2025-02-13 03:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:05][root][INFO] - Training Epoch: 2/2, step 397/7134 completed (loss: 0.050864771008491516, acc: 0.988095223903656)
[2025-02-13 03:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:06][root][INFO] - Training Epoch: 2/2, step 398/7134 completed (loss: 0.021391045302152634, acc: 0.9948453903198242)
[2025-02-13 03:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:06][root][INFO] - Training Epoch: 2/2, step 399/7134 completed (loss: 0.023352229967713356, acc: 0.995245635509491)
[2025-02-13 03:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07][root][INFO] - Training Epoch: 2/2, step 400/7134 completed (loss: 0.07341848313808441, acc: 0.9776119589805603)
[2025-02-13 03:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07][root][INFO] - Training Epoch: 2/2, step 401/7134 completed (loss: 0.01993916742503643, acc: 0.9963235259056091)
[2025-02-13 03:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:08][root][INFO] - Training Epoch: 2/2, step 402/7134 completed (loss: 0.019759435206651688, acc: 0.9949302673339844)
[2025-02-13 03:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:08][root][INFO] - Training Epoch: 2/2, step 403/7134 completed (loss: 0.02037160098552704, acc: 0.9946004152297974)
[2025-02-13 03:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:09][root][INFO] - Training Epoch: 2/2, step 404/7134 completed (loss: 0.023215608671307564, acc: 0.9923076629638672)
[2025-02-13 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:09][root][INFO] - Training Epoch: 2/2, step 405/7134 completed (loss: 0.0680081769824028, acc: 0.9828660488128662)
[2025-02-13 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10][root][INFO] - Training Epoch: 2/2, step 406/7134 completed (loss: 0.04429139196872711, acc: 0.9898989796638489)
[2025-02-13 03:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10][root][INFO] - Training Epoch: 2/2, step 407/7134 completed (loss: 0.09905251860618591, acc: 0.9796609878540039)
[2025-02-13 03:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:11][root][INFO] - Training Epoch: 2/2, step 408/7134 completed (loss: 0.07457708567380905, acc: 0.9780521392822266)
[2025-02-13 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:11][root][INFO] - Training Epoch: 2/2, step 409/7134 completed (loss: 0.028669018298387527, acc: 0.9886792302131653)
[2025-02-13 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:12][root][INFO] - Training Epoch: 2/2, step 410/7134 completed (loss: 0.035463597625494, acc: 0.9866468906402588)
[2025-02-13 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:12][root][INFO] - Training Epoch: 2/2, step 411/7134 completed (loss: 0.03792263939976692, acc: 0.9919447898864746)
[2025-02-13 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:12][root][INFO] - Training Epoch: 2/2, step 412/7134 completed (loss: 0.020079925656318665, acc: 0.9931350350379944)
[2025-02-13 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:13][root][INFO] - Training Epoch: 2/2, step 413/7134 completed (loss: 0.021900871768593788, acc: 0.9919893145561218)
[2025-02-13 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:13][root][INFO] - Training Epoch: 2/2, step 414/7134 completed (loss: 0.023051749914884567, acc: 0.9910979270935059)
[2025-02-13 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14][root][INFO] - Training Epoch: 2/2, step 415/7134 completed (loss: 0.04157806187868118, acc: 0.9918981194496155)
[2025-02-13 03:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14][root][INFO] - Training Epoch: 2/2, step 416/7134 completed (loss: 0.022498616948723793, acc: 0.9917355179786682)
[2025-02-13 03:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:15][root][INFO] - Training Epoch: 2/2, step 417/7134 completed (loss: 0.05880697816610336, acc: 0.9850543737411499)
[2025-02-13 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:15][root][INFO] - Training Epoch: 2/2, step 418/7134 completed (loss: 0.02455534227192402, acc: 0.9908151626586914)
[2025-02-13 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:16][root][INFO] - Training Epoch: 2/2, step 419/7134 completed (loss: 0.02564983442425728, acc: 0.9925558567047119)
[2025-02-13 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:16][root][INFO] - Training Epoch: 2/2, step 420/7134 completed (loss: 0.026118135079741478, acc: 0.9915397763252258)
[2025-02-13 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:17][root][INFO] - Training Epoch: 2/2, step 421/7134 completed (loss: 0.015902142971754074, acc: 0.9920381903648376)
[2025-02-13 03:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:17][root][INFO] - Training Epoch: 2/2, step 422/7134 completed (loss: 0.024335412308573723, acc: 0.9897058606147766)
[2025-02-13 03:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:17][root][INFO] - Training Epoch: 2/2, step 423/7134 completed (loss: 0.018140779808163643, acc: 0.9928571581840515)
[2025-02-13 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18][root][INFO] - Training Epoch: 2/2, step 424/7134 completed (loss: 0.01947995275259018, acc: 0.9936708807945251)
[2025-02-13 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18][root][INFO] - Training Epoch: 2/2, step 425/7134 completed (loss: 0.005353708751499653, acc: 0.9956395626068115)
[2025-02-13 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:19][root][INFO] - Training Epoch: 2/2, step 426/7134 completed (loss: 0.004053604789078236, acc: 1.0)
[2025-02-13 03:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:19][root][INFO] - Training Epoch: 2/2, step 427/7134 completed (loss: 0.015092599205672741, acc: 0.9969651103019714)
[2025-02-13 03:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20][root][INFO] - Training Epoch: 2/2, step 428/7134 completed (loss: 0.026822233572602272, acc: 0.990963876247406)
[2025-02-13 03:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20][root][INFO] - Training Epoch: 2/2, step 429/7134 completed (loss: 0.033565323799848557, acc: 0.9841017723083496)
[2025-02-13 03:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20][root][INFO] - Training Epoch: 2/2, step 430/7134 completed (loss: 0.017130667343735695, acc: 0.9928264021873474)
[2025-02-13 03:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:21][root][INFO] - Training Epoch: 2/2, step 431/7134 completed (loss: 0.03354277461767197, acc: 0.9942362904548645)
[2025-02-13 03:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:21][root][INFO] - Training Epoch: 2/2, step 432/7134 completed (loss: 0.012069066055119038, acc: 0.998171865940094)
[2025-02-13 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22][root][INFO] - Training Epoch: 2/2, step 433/7134 completed (loss: 0.017350517213344574, acc: 0.9959239363670349)
[2025-02-13 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22][root][INFO] - Training Epoch: 2/2, step 434/7134 completed (loss: 0.01729317009449005, acc: 0.9956076145172119)
[2025-02-13 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:23][root][INFO] - Training Epoch: 2/2, step 435/7134 completed (loss: 0.01809585466980934, acc: 0.9943262338638306)
[2025-02-13 03:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:23][root][INFO] - Training Epoch: 2/2, step 436/7134 completed (loss: 0.04333175718784332, acc: 0.9921259880065918)
[2025-02-13 03:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:24][root][INFO] - Training Epoch: 2/2, step 437/7134 completed (loss: 0.036822609603405, acc: 0.9911660552024841)
[2025-02-13 03:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:24][root][INFO] - Training Epoch: 2/2, step 438/7134 completed (loss: 0.03143039718270302, acc: 0.9942307472229004)
[2025-02-13 03:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25][root][INFO] - Training Epoch: 2/2, step 439/7134 completed (loss: 0.02486538700759411, acc: 0.9921466112136841)
[2025-02-13 03:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25][root][INFO] - Training Epoch: 2/2, step 440/7134 completed (loss: 0.018154378980398178, acc: 0.9913344979286194)
[2025-02-13 03:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25][root][INFO] - Training Epoch: 2/2, step 441/7134 completed (loss: 0.008025145158171654, acc: 0.9970015287399292)
[2025-02-13 03:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:26][root][INFO] - Training Epoch: 2/2, step 442/7134 completed (loss: 0.004159256815910339, acc: 1.0)
[2025-02-13 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:26][root][INFO] - Training Epoch: 2/2, step 443/7134 completed (loss: 0.003206820460036397, acc: 1.0)
[2025-02-13 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:27][root][INFO] - Training Epoch: 2/2, step 444/7134 completed (loss: 0.004556180443614721, acc: 1.0)
[2025-02-13 03:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:27][root][INFO] - Training Epoch: 2/2, step 445/7134 completed (loss: 0.021711966022849083, acc: 0.9941691160202026)
[2025-02-13 03:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28][root][INFO] - Training Epoch: 2/2, step 446/7134 completed (loss: 0.008153671398758888, acc: 0.9972144961357117)
[2025-02-13 03:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28][root][INFO] - Training Epoch: 2/2, step 447/7134 completed (loss: 0.009993568062782288, acc: 0.9958391189575195)
[2025-02-13 03:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28][root][INFO] - Training Epoch: 2/2, step 448/7134 completed (loss: 0.02807910554111004, acc: 0.9923273921012878)
[2025-02-13 03:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:29][root][INFO] - Training Epoch: 2/2, step 449/7134 completed (loss: 0.09461522847414017, acc: 0.9803664684295654)
[2025-02-13 03:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:29][root][INFO] - Training Epoch: 2/2, step 450/7134 completed (loss: 0.03413831442594528, acc: 0.9915074110031128)
[2025-02-13 03:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:30][root][INFO] - Training Epoch: 2/2, step 451/7134 completed (loss: 0.06909287720918655, acc: 0.9785969257354736)
[2025-02-13 03:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:30][root][INFO] - Training Epoch: 2/2, step 452/7134 completed (loss: 0.05479162931442261, acc: 0.982191801071167)
[2025-02-13 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31][root][INFO] - Training Epoch: 2/2, step 453/7134 completed (loss: 0.0398801788687706, acc: 0.988990843296051)
[2025-02-13 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31][root][INFO] - Training Epoch: 2/2, step 454/7134 completed (loss: 0.0602775402367115, acc: 0.979547917842865)
[2025-02-13 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:32][root][INFO] - Training Epoch: 2/2, step 455/7134 completed (loss: 0.07115345448255539, acc: 0.9843137264251709)
[2025-02-13 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:32][root][INFO] - Training Epoch: 2/2, step 456/7134 completed (loss: 0.019473545253276825, acc: 0.9915013909339905)
[2025-02-13 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:33][root][INFO] - Training Epoch: 2/2, step 457/7134 completed (loss: 0.02521434612572193, acc: 0.994966447353363)
[2025-02-13 03:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:33][root][INFO] - Training Epoch: 2/2, step 458/7134 completed (loss: 0.028077874332666397, acc: 0.9920477271080017)
[2025-02-13 03:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34][root][INFO] - Training Epoch: 2/2, step 459/7134 completed (loss: 0.056623246520757675, acc: 0.9807407259941101)
[2025-02-13 03:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34][root][INFO] - Training Epoch: 2/2, step 460/7134 completed (loss: 0.05519653856754303, acc: 0.98740154504776)
[2025-02-13 03:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34][root][INFO] - Training Epoch: 2/2, step 461/7134 completed (loss: 0.027265701442956924, acc: 0.9887797832489014)
[2025-02-13 03:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:35][root][INFO] - Training Epoch: 2/2, step 462/7134 completed (loss: 0.0541994646191597, acc: 0.9836448431015015)
[2025-02-13 03:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:35][root][INFO] - Training Epoch: 2/2, step 463/7134 completed (loss: 0.047702986747026443, acc: 0.9849435091018677)
[2025-02-13 03:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:36][root][INFO] - Training Epoch: 2/2, step 464/7134 completed (loss: 0.03584054484963417, acc: 0.9872521162033081)
[2025-02-13 03:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:36][root][INFO] - Training Epoch: 2/2, step 465/7134 completed (loss: 0.061158470809459686, acc: 0.9882506728172302)
[2025-02-13 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37][root][INFO] - Training Epoch: 2/2, step 466/7134 completed (loss: 0.0759730264544487, acc: 0.9742599725723267)
[2025-02-13 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37][root][INFO] - Training Epoch: 2/2, step 467/7134 completed (loss: 0.08224119246006012, acc: 0.9750849604606628)
[2025-02-13 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:38][root][INFO] - Training Epoch: 2/2, step 468/7134 completed (loss: 0.15086133778095245, acc: 0.9628610610961914)
[2025-02-13 03:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:38][root][INFO] - Training Epoch: 2/2, step 469/7134 completed (loss: 0.07047460973262787, acc: 0.9812679886817932)
[2025-02-13 03:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39][root][INFO] - Training Epoch: 2/2, step 470/7134 completed (loss: 0.05887502059340477, acc: 0.9823129177093506)
[2025-02-13 03:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39][root][INFO] - Training Epoch: 2/2, step 471/7134 completed (loss: 0.05965198203921318, acc: 0.9834319353103638)
[2025-02-13 03:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39][root][INFO] - Training Epoch: 2/2, step 472/7134 completed (loss: 0.08185791969299316, acc: 0.9780058860778809)
[2025-02-13 03:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:40][root][INFO] - Training Epoch: 2/2, step 473/7134 completed (loss: 0.041150324046611786, acc: 0.9814814925193787)
[2025-02-13 03:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:40][root][INFO] - Training Epoch: 2/2, step 474/7134 completed (loss: 0.08731865882873535, acc: 0.9815546870231628)
[2025-02-13 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:41][root][INFO] - Training Epoch: 2/2, step 475/7134 completed (loss: 0.09540487080812454, acc: 0.9735293984413147)
[2025-02-13 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:41][root][INFO] - Training Epoch: 2/2, step 476/7134 completed (loss: 0.07522464543581009, acc: 0.9813753366470337)
[2025-02-13 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:42][root][INFO] - Training Epoch: 2/2, step 477/7134 completed (loss: 0.0560726672410965, acc: 0.9865525960922241)
[2025-02-13 03:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:42][root][INFO] - Training Epoch: 2/2, step 478/7134 completed (loss: 0.11121325939893723, acc: 0.9704017043113708)
[2025-02-13 03:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43][root][INFO] - Training Epoch: 2/2, step 479/7134 completed (loss: 0.02440505661070347, acc: 0.991391658782959)
[2025-02-13 03:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43][root][INFO] - Training Epoch: 2/2, step 480/7134 completed (loss: 0.022952528670430183, acc: 0.9956204295158386)
[2025-02-13 03:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:44][root][INFO] - Training Epoch: 2/2, step 481/7134 completed (loss: 0.029647834599018097, acc: 0.9908854365348816)
[2025-02-13 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:44][root][INFO] - Training Epoch: 2/2, step 482/7134 completed (loss: 0.011251033283770084, acc: 0.9971428513526917)
[2025-02-13 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:44][root][INFO] - Training Epoch: 2/2, step 483/7134 completed (loss: 0.008939041756093502, acc: 0.9987714886665344)
[2025-02-13 03:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45][root][INFO] - Training Epoch: 2/2, step 484/7134 completed (loss: 0.041199635714292526, acc: 0.9893048405647278)
[2025-02-13 03:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45][root][INFO] - Training Epoch: 2/2, step 485/7134 completed (loss: 0.08613796532154083, acc: 0.9803328514099121)
[2025-02-13 03:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:46][root][INFO] - Training Epoch: 2/2, step 486/7134 completed (loss: 0.03917793929576874, acc: 0.9901315569877625)
[2025-02-13 03:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:46][root][INFO] - Training Epoch: 2/2, step 487/7134 completed (loss: 0.02871127985417843, acc: 0.9926578402519226)
[2025-02-13 03:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47][root][INFO] - Training Epoch: 2/2, step 488/7134 completed (loss: 0.08947689086198807, acc: 0.9746376872062683)
[2025-02-13 03:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47][root][INFO] - Training Epoch: 2/2, step 489/7134 completed (loss: 0.05424036458134651, acc: 0.9865546226501465)
[2025-02-13 03:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:48][root][INFO] - Training Epoch: 2/2, step 490/7134 completed (loss: 0.03770788386464119, acc: 0.994557797908783)
[2025-02-13 03:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:48][root][INFO] - Training Epoch: 2/2, step 491/7134 completed (loss: 0.02922242321074009, acc: 0.9917762875556946)
[2025-02-13 03:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:48][root][INFO] - Training Epoch: 2/2, step 492/7134 completed (loss: 0.008810787461698055, acc: 0.998487114906311)
[2025-02-13 03:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:49][root][INFO] - Training Epoch: 2/2, step 493/7134 completed (loss: 0.04567170515656471, acc: 0.984240710735321)
[2025-02-13 03:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:49][root][INFO] - Training Epoch: 2/2, step 494/7134 completed (loss: 0.01643000915646553, acc: 0.9937499761581421)
[2025-02-13 03:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50][root][INFO] - Training Epoch: 2/2, step 495/7134 completed (loss: 0.02530253864824772, acc: 0.9921568632125854)
[2025-02-13 03:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50][root][INFO] - Training Epoch: 2/2, step 496/7134 completed (loss: 0.0357782356441021, acc: 0.9918699264526367)
[2025-02-13 03:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:51][root][INFO] - Training Epoch: 2/2, step 497/7134 completed (loss: 0.05819236487150192, acc: 0.9864341020584106)
[2025-02-13 03:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:51][root][INFO] - Training Epoch: 2/2, step 498/7134 completed (loss: 0.025166384875774384, acc: 0.995192289352417)
[2025-02-13 03:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:52][root][INFO] - Training Epoch: 2/2, step 499/7134 completed (loss: 0.013068126514554024, acc: 0.9972183704376221)
[2025-02-13 03:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:52][root][INFO] - Training Epoch: 2/2, step 500/7134 completed (loss: 0.03138626739382744, acc: 0.9919999837875366)
[2025-02-13 03:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:52][root][INFO] - Training Epoch: 2/2, step 501/7134 completed (loss: 0.01487770490348339, acc: 0.9958449006080627)
[2025-02-13 03:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:53][root][INFO] - Training Epoch: 2/2, step 502/7134 completed (loss: 0.009314495138823986, acc: 0.9969419240951538)
[2025-02-13 03:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:53][root][INFO] - Training Epoch: 2/2, step 503/7134 completed (loss: 0.030299974605441093, acc: 0.9892617464065552)
[2025-02-13 03:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54][root][INFO] - Training Epoch: 2/2, step 504/7134 completed (loss: 0.0158301442861557, acc: 0.9958100318908691)
[2025-02-13 03:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54][root][INFO] - Training Epoch: 2/2, step 505/7134 completed (loss: 0.01935962773859501, acc: 0.9947643876075745)
[2025-02-13 03:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:55][root][INFO] - Training Epoch: 2/2, step 506/7134 completed (loss: 0.013191726058721542, acc: 0.996268630027771)
[2025-02-13 03:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:55][root][INFO] - Training Epoch: 2/2, step 507/7134 completed (loss: 0.02609138935804367, acc: 0.99210524559021)
[2025-02-13 03:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:56][root][INFO] - Training Epoch: 2/2, step 508/7134 completed (loss: 0.024281097576022148, acc: 0.9916201233863831)
[2025-02-13 03:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:56][root][INFO] - Training Epoch: 2/2, step 509/7134 completed (loss: 0.028925666585564613, acc: 0.9907264113426208)
[2025-02-13 03:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:56][root][INFO] - Training Epoch: 2/2, step 510/7134 completed (loss: 0.08194448053836823, acc: 0.9812889695167542)
[2025-02-13 03:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57][root][INFO] - Training Epoch: 2/2, step 511/7134 completed (loss: 0.03728111460804939, acc: 0.995110034942627)
[2025-02-13 03:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57][root][INFO] - Training Epoch: 2/2, step 512/7134 completed (loss: 0.06566079705953598, acc: 0.9839679598808289)
[2025-02-13 03:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:58][root][INFO] - Training Epoch: 2/2, step 513/7134 completed (loss: 0.0641249418258667, acc: 0.9845094680786133)
[2025-02-13 03:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:58][root][INFO] - Training Epoch: 2/2, step 514/7134 completed (loss: 0.0569189190864563, acc: 0.9824120402336121)
[2025-02-13 03:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:59][root][INFO] - Training Epoch: 2/2, step 515/7134 completed (loss: 0.022508500143885612, acc: 0.9957567453384399)
[2025-02-13 03:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:59][root][INFO] - Training Epoch: 2/2, step 516/7134 completed (loss: 0.019147783517837524, acc: 0.9977116584777832)
[2025-02-13 03:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:59][root][INFO] - Training Epoch: 2/2, step 517/7134 completed (loss: 0.061253223568201065, acc: 0.9892473220825195)
[2025-02-13 03:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00][root][INFO] - Training Epoch: 2/2, step 518/7134 completed (loss: 0.025762371718883514, acc: 0.9925037622451782)
[2025-02-13 03:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00][root][INFO] - Training Epoch: 2/2, step 519/7134 completed (loss: 0.0276067815721035, acc: 0.9938144087791443)
[2025-02-13 03:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:01][root][INFO] - Training Epoch: 2/2, step 520/7134 completed (loss: 0.013867976143956184, acc: 0.996835470199585)
[2025-02-13 03:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:01][root][INFO] - Training Epoch: 2/2, step 521/7134 completed (loss: 0.03790222480893135, acc: 0.9879840016365051)
[2025-02-13 03:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:02][root][INFO] - Training Epoch: 2/2, step 522/7134 completed (loss: 0.023106539621949196, acc: 0.9932975769042969)
[2025-02-13 03:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:02][root][INFO] - Training Epoch: 2/2, step 523/7134 completed (loss: 0.016658052802085876, acc: 0.9956140518188477)
[2025-02-13 03:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:03][root][INFO] - Training Epoch: 2/2, step 524/7134 completed (loss: 0.02795199491083622, acc: 0.9946236610412598)
[2025-02-13 03:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:03][root][INFO] - Training Epoch: 2/2, step 525/7134 completed (loss: 0.06413212418556213, acc: 0.984544038772583)
[2025-02-13 03:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04][root][INFO] - Training Epoch: 2/2, step 526/7134 completed (loss: 0.02412923239171505, acc: 0.9931507110595703)
[2025-02-13 03:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04][root][INFO] - Training Epoch: 2/2, step 527/7134 completed (loss: 0.03591781109571457, acc: 0.9944030046463013)
[2025-02-13 03:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04][root][INFO] - Training Epoch: 2/2, step 528/7134 completed (loss: 0.002387973479926586, acc: 1.0)
[2025-02-13 03:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:05][root][INFO] - Training Epoch: 2/2, step 529/7134 completed (loss: 0.039846859872341156, acc: 0.9873015880584717)
[2025-02-13 03:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:05][root][INFO] - Training Epoch: 2/2, step 530/7134 completed (loss: 0.03018784523010254, acc: 0.9914236664772034)
[2025-02-13 03:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:06][root][INFO] - Training Epoch: 2/2, step 531/7134 completed (loss: 0.08003822714090347, acc: 0.9863247871398926)
[2025-02-13 03:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:06][root][INFO] - Training Epoch: 2/2, step 532/7134 completed (loss: 0.01300745364278555, acc: 0.9979209899902344)
[2025-02-13 03:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:07][root][INFO] - Training Epoch: 2/2, step 533/7134 completed (loss: 0.06359781324863434, acc: 0.9828178882598877)
[2025-02-13 03:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:07][root][INFO] - Training Epoch: 2/2, step 534/7134 completed (loss: 0.01902616210281849, acc: 0.9931318759918213)
[2025-02-13 03:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08][root][INFO] - Training Epoch: 2/2, step 535/7134 completed (loss: 0.03632872551679611, acc: 0.9876977205276489)
[2025-02-13 03:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08][root][INFO] - Training Epoch: 2/2, step 536/7134 completed (loss: 0.02060449682176113, acc: 0.9941605925559998)
[2025-02-13 03:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08][root][INFO] - Training Epoch: 2/2, step 537/7134 completed (loss: 0.04142451658844948, acc: 0.9852724671363831)
[2025-02-13 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:09][root][INFO] - Training Epoch: 2/2, step 538/7134 completed (loss: 0.023917503654956818, acc: 0.9896142482757568)
[2025-02-13 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:09][root][INFO] - Training Epoch: 2/2, step 539/7134 completed (loss: 0.03062448836863041, acc: 0.9896193742752075)
[2025-02-13 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10][root][INFO] - Training Epoch: 2/2, step 540/7134 completed (loss: 0.025099022313952446, acc: 0.9939393997192383)
[2025-02-13 03:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10][root][INFO] - Training Epoch: 2/2, step 541/7134 completed (loss: 0.04836566001176834, acc: 0.9880319237709045)
[2025-02-13 03:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:11][root][INFO] - Training Epoch: 2/2, step 542/7134 completed (loss: 0.06775429844856262, acc: 0.9812206625938416)
[2025-02-13 03:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:11][root][INFO] - Training Epoch: 2/2, step 543/7134 completed (loss: 0.05412764847278595, acc: 0.978300154209137)
[2025-02-13 03:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12][root][INFO] - Training Epoch: 2/2, step 544/7134 completed (loss: 0.044699713587760925, acc: 0.9858044385910034)
[2025-02-13 03:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12][root][INFO] - Training Epoch: 2/2, step 545/7134 completed (loss: 0.04536241292953491, acc: 0.9822866320610046)
[2025-02-13 03:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12][root][INFO] - Training Epoch: 2/2, step 546/7134 completed (loss: 0.10544164478778839, acc: 0.9716312289237976)
[2025-02-13 03:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:13][root][INFO] - Training Epoch: 2/2, step 547/7134 completed (loss: 0.055434372276067734, acc: 0.9854862093925476)
[2025-02-13 03:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:13][root][INFO] - Training Epoch: 2/2, step 548/7134 completed (loss: 0.03249885514378548, acc: 0.9895697236061096)
[2025-02-13 03:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14][root][INFO] - Training Epoch: 2/2, step 549/7134 completed (loss: 0.06790118664503098, acc: 0.9829192757606506)
[2025-02-13 03:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14][root][INFO] - Training Epoch: 2/2, step 550/7134 completed (loss: 0.049814242869615555, acc: 0.988054633140564)
[2025-02-13 03:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:15][root][INFO] - Training Epoch: 2/2, step 551/7134 completed (loss: 0.0448775440454483, acc: 0.9815837740898132)
[2025-02-13 03:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:15][root][INFO] - Training Epoch: 2/2, step 552/7134 completed (loss: 0.028972316533327103, acc: 0.9900285005569458)
[2025-02-13 03:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:15][root][INFO] - Training Epoch: 2/2, step 553/7134 completed (loss: 0.03238358721137047, acc: 0.9868420958518982)
[2025-02-13 03:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:16][root][INFO] - Training Epoch: 2/2, step 554/7134 completed (loss: 0.046079061925411224, acc: 0.9885877370834351)
[2025-02-13 03:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:16][root][INFO] - Training Epoch: 2/2, step 555/7134 completed (loss: 0.04769944027066231, acc: 0.9857819676399231)
[2025-02-13 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17][root][INFO] - Training Epoch: 2/2, step 556/7134 completed (loss: 0.031357016414403915, acc: 0.9915134310722351)
[2025-02-13 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17][root][INFO] - Training Epoch: 2/2, step 557/7134 completed (loss: 0.04745342209935188, acc: 0.9830795526504517)
[2025-02-13 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:18][root][INFO] - Training Epoch: 2/2, step 558/7134 completed (loss: 0.020971260964870453, acc: 0.9957447052001953)
[2025-02-13 03:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:18][root][INFO] - Training Epoch: 2/2, step 559/7134 completed (loss: 0.07595401257276535, acc: 0.9772079586982727)
[2025-02-13 03:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:19][root][INFO] - Training Epoch: 2/2, step 560/7134 completed (loss: 0.028457315638661385, acc: 0.9894551634788513)
[2025-02-13 03:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:19][root][INFO] - Training Epoch: 2/2, step 561/7134 completed (loss: 0.07025427371263504, acc: 0.9782886505126953)
[2025-02-13 03:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20][root][INFO] - Training Epoch: 2/2, step 562/7134 completed (loss: 0.02848627232015133, acc: 0.9894039630889893)
[2025-02-13 03:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20][root][INFO] - Training Epoch: 2/2, step 563/7134 completed (loss: 0.07856824994087219, acc: 0.9841772317886353)
[2025-02-13 03:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20][root][INFO] - Training Epoch: 2/2, step 564/7134 completed (loss: 0.04369744285941124, acc: 0.9876390695571899)
[2025-02-13 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:21][root][INFO] - Training Epoch: 2/2, step 565/7134 completed (loss: 0.039419736713171005, acc: 0.9849246144294739)
[2025-02-13 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:21][root][INFO] - Training Epoch: 2/2, step 566/7134 completed (loss: 0.07399541139602661, acc: 0.9791183471679688)
[2025-02-13 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22][root][INFO] - Training Epoch: 2/2, step 567/7134 completed (loss: 0.044226180762052536, acc: 0.9859747290611267)
[2025-02-13 03:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22][root][INFO] - Training Epoch: 2/2, step 568/7134 completed (loss: 0.04042372107505798, acc: 0.9882352948188782)
[2025-02-13 03:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:23][root][INFO] - Training Epoch: 2/2, step 569/7134 completed (loss: 0.013527206145226955, acc: 0.9962962865829468)
[2025-02-13 03:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:23][root][INFO] - Training Epoch: 2/2, step 570/7134 completed (loss: 0.024283789098262787, acc: 0.9905992746353149)
[2025-02-13 03:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:24][root][INFO] - Training Epoch: 2/2, step 571/7134 completed (loss: 0.020945338532328606, acc: 0.994962215423584)
[2025-02-13 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:24][root][INFO] - Training Epoch: 2/2, step 572/7134 completed (loss: 0.019098585471510887, acc: 0.9961389899253845)
[2025-02-13 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:24][root][INFO] - Training Epoch: 2/2, step 573/7134 completed (loss: 0.010044981725513935, acc: 0.9948849081993103)
[2025-02-13 03:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:25][root][INFO] - Training Epoch: 2/2, step 574/7134 completed (loss: 0.015973351895809174, acc: 0.9960474371910095)
[2025-02-13 03:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:25][root][INFO] - Training Epoch: 2/2, step 575/7134 completed (loss: 0.019971955567598343, acc: 0.9935732483863831)
[2025-02-13 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26][root][INFO] - Training Epoch: 2/2, step 576/7134 completed (loss: 0.01402612030506134, acc: 0.9950494766235352)
[2025-02-13 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26][root][INFO] - Training Epoch: 2/2, step 577/7134 completed (loss: 0.01683356985449791, acc: 0.9934123754501343)
[2025-02-13 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:27][root][INFO] - Training Epoch: 2/2, step 578/7134 completed (loss: 0.023437896743416786, acc: 0.9930394291877747)
[2025-02-13 03:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:27][root][INFO] - Training Epoch: 2/2, step 579/7134 completed (loss: 0.045014455914497375, acc: 0.9833119511604309)
[2025-02-13 03:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28][root][INFO] - Training Epoch: 2/2, step 580/7134 completed (loss: 0.047272756695747375, acc: 0.981794536113739)
[2025-02-13 03:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28][root][INFO] - Training Epoch: 2/2, step 581/7134 completed (loss: 0.023331278935074806, acc: 0.9930264949798584)
[2025-02-13 03:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:29][root][INFO] - Training Epoch: 2/2, step 582/7134 completed (loss: 0.03084808588027954, acc: 0.9916167855262756)
[2025-02-13 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:29][root][INFO] - Training Epoch: 2/2, step 583/7134 completed (loss: 0.0240337997674942, acc: 0.9935064911842346)
[2025-02-13 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:30][root][INFO] - Training Epoch: 2/2, step 584/7134 completed (loss: 0.007149894721806049, acc: 0.9988038539886475)
[2025-02-13 03:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:30][root][INFO] - Training Epoch: 2/2, step 585/7134 completed (loss: 0.021046223118901253, acc: 0.9951691031455994)
[2025-02-13 03:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:30][root][INFO] - Training Epoch: 2/2, step 586/7134 completed (loss: 0.01947733759880066, acc: 0.9960886836051941)
[2025-02-13 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31][root][INFO] - Training Epoch: 2/2, step 587/7134 completed (loss: 0.03856273740530014, acc: 0.9911054372787476)
[2025-02-13 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31][root][INFO] - Training Epoch: 2/2, step 588/7134 completed (loss: 0.021108325570821762, acc: 0.9968602657318115)
[2025-02-13 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:32][root][INFO] - Training Epoch: 2/2, step 589/7134 completed (loss: 0.0266213808208704, acc: 0.988252580165863)
[2025-02-13 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:32][root][INFO] - Training Epoch: 2/2, step 590/7134 completed (loss: 0.019617581740021706, acc: 0.995488703250885)
[2025-02-13 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:33][root][INFO] - Training Epoch: 2/2, step 591/7134 completed (loss: 0.03439630568027496, acc: 0.9939467310905457)
[2025-02-13 03:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:33][root][INFO] - Training Epoch: 2/2, step 592/7134 completed (loss: 0.02236347459256649, acc: 0.9936548471450806)
[2025-02-13 03:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34][root][INFO] - Training Epoch: 2/2, step 593/7134 completed (loss: 0.033222079277038574, acc: 0.9881889820098877)
[2025-02-13 03:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34][root][INFO] - Training Epoch: 2/2, step 594/7134 completed (loss: 0.04344068467617035, acc: 0.9899371266365051)
[2025-02-13 03:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:35][root][INFO] - Training Epoch: 2/2, step 595/7134 completed (loss: 0.02728879079222679, acc: 0.993678867816925)
[2025-02-13 03:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:35][root][INFO] - Training Epoch: 2/2, step 596/7134 completed (loss: 0.01326826959848404, acc: 0.9935566782951355)
[2025-02-13 03:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:36][root][INFO] - Training Epoch: 2/2, step 597/7134 completed (loss: 0.09788163006305695, acc: 0.9749670624732971)
[2025-02-13 03:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:36][root][INFO] - Training Epoch: 2/2, step 598/7134 completed (loss: 0.03792720288038254, acc: 0.9878214001655579)
[2025-02-13 03:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:36][root][INFO] - Training Epoch: 2/2, step 599/7134 completed (loss: 0.023129697889089584, acc: 0.9924812316894531)
[2025-02-13 03:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37][root][INFO] - Training Epoch: 2/2, step 600/7134 completed (loss: 0.03994593024253845, acc: 0.9927797913551331)
[2025-02-13 03:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37][root][INFO] - Training Epoch: 2/2, step 601/7134 completed (loss: 0.030964108183979988, acc: 0.9861687421798706)
[2025-02-13 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:38][root][INFO] - Training Epoch: 2/2, step 602/7134 completed (loss: 0.03164801746606827, acc: 0.9930394291877747)
[2025-02-13 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:38][root][INFO] - Training Epoch: 2/2, step 603/7134 completed (loss: 0.02052326686680317, acc: 0.9948586225509644)
[2025-02-13 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:39][root][INFO] - Training Epoch: 2/2, step 604/7134 completed (loss: 0.026161525398492813, acc: 0.9868228435516357)
[2025-02-13 03:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:39][root][INFO] - Training Epoch: 2/2, step 605/7134 completed (loss: 0.019147226586937904, acc: 0.9922279715538025)
[2025-02-13 03:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40][root][INFO] - Training Epoch: 2/2, step 606/7134 completed (loss: 0.060179028660058975, acc: 0.9802225232124329)
[2025-02-13 03:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40][root][INFO] - Training Epoch: 2/2, step 607/7134 completed (loss: 0.03049328736960888, acc: 0.9919893145561218)
[2025-02-13 03:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:41][root][INFO] - Training Epoch: 2/2, step 608/7134 completed (loss: 0.023912601172924042, acc: 0.9935232996940613)
[2025-02-13 03:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:41][root][INFO] - Training Epoch: 2/2, step 609/7134 completed (loss: 0.02675759233534336, acc: 0.9918032884597778)
[2025-02-13 03:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:42][root][INFO] - Training Epoch: 2/2, step 610/7134 completed (loss: 0.036345843225717545, acc: 0.9886220097541809)
[2025-02-13 03:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:42][root][INFO] - Training Epoch: 2/2, step 611/7134 completed (loss: 0.017704134806990623, acc: 0.9939024448394775)
[2025-02-13 03:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:42][root][INFO] - Training Epoch: 2/2, step 612/7134 completed (loss: 0.024026542901992798, acc: 0.9920844435691833)
[2025-02-13 03:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:43][root][INFO] - Training Epoch: 2/2, step 613/7134 completed (loss: 0.026209751144051552, acc: 0.9906213283538818)
[2025-02-13 03:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:43][root][INFO] - Training Epoch: 2/2, step 614/7134 completed (loss: 0.01757066324353218, acc: 0.9920724630355835)
[2025-02-13 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:44][root][INFO] - Training Epoch: 2/2, step 615/7134 completed (loss: 0.007133679464459419, acc: 1.0)
[2025-02-13 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:44][root][INFO] - Training Epoch: 2/2, step 616/7134 completed (loss: 0.02083384059369564, acc: 0.9931818246841431)
[2025-02-13 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45][root][INFO] - Training Epoch: 2/2, step 617/7134 completed (loss: 0.024010485038161278, acc: 0.9931662678718567)
[2025-02-13 03:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45][root][INFO] - Training Epoch: 2/2, step 618/7134 completed (loss: 0.015544019639492035, acc: 0.9959893226623535)
[2025-02-13 03:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:46][root][INFO] - Training Epoch: 2/2, step 619/7134 completed (loss: 0.031260207295417786, acc: 0.9939271211624146)
[2025-02-13 03:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:46][root][INFO] - Training Epoch: 2/2, step 620/7134 completed (loss: 0.049176864326000214, acc: 0.9888535141944885)
[2025-02-13 03:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47][root][INFO] - Training Epoch: 2/2, step 621/7134 completed (loss: 0.04547501727938652, acc: 0.9923896789550781)
[2025-02-13 03:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47][root][INFO] - Training Epoch: 2/2, step 622/7134 completed (loss: 0.02523251436650753, acc: 0.9899569749832153)
[2025-02-13 03:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47][root][INFO] - Training Epoch: 2/2, step 623/7134 completed (loss: 0.06446530669927597, acc: 0.9897959232330322)
[2025-02-13 03:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:48][root][INFO] - Training Epoch: 2/2, step 624/7134 completed (loss: 0.013020791113376617, acc: 0.995945930480957)
[2025-02-13 03:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:48][root][INFO] - Training Epoch: 2/2, step 625/7134 completed (loss: 0.037567924708127975, acc: 0.9894459247589111)
[2025-02-13 03:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49][root][INFO] - Training Epoch: 2/2, step 626/7134 completed (loss: 0.07505331933498383, acc: 0.9862542748451233)
[2025-02-13 03:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49][root][INFO] - Training Epoch: 2/2, step 627/7134 completed (loss: 0.025488771498203278, acc: 0.99262535572052)
[2025-02-13 03:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:50][root][INFO] - Training Epoch: 2/2, step 628/7134 completed (loss: 0.022174816578626633, acc: 0.9958391189575195)
[2025-02-13 03:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:50][root][INFO] - Training Epoch: 2/2, step 629/7134 completed (loss: 0.04681047797203064, acc: 0.9899857044219971)
[2025-02-13 03:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51][root][INFO] - Training Epoch: 2/2, step 630/7134 completed (loss: 0.02332943119108677, acc: 0.9896373152732849)
[2025-02-13 03:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51][root][INFO] - Training Epoch: 2/2, step 631/7134 completed (loss: 0.023426450788974762, acc: 0.9907894730567932)
[2025-02-13 03:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51][root][INFO] - Training Epoch: 2/2, step 632/7134 completed (loss: 0.017830975353717804, acc: 0.9951456189155579)
[2025-02-13 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52][root][INFO] - Training Epoch: 2/2, step 633/7134 completed (loss: 0.029279280453920364, acc: 0.9927431344985962)
[2025-02-13 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52][root][INFO] - Training Epoch: 2/2, step 634/7134 completed (loss: 0.011270011775195599, acc: 0.9984177350997925)
[2025-02-13 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:53][root][INFO] - Training Epoch: 2/2, step 635/7134 completed (loss: 0.029971906915307045, acc: 0.9908397197723389)
[2025-02-13 03:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:53][root][INFO] - Training Epoch: 2/2, step 636/7134 completed (loss: 0.06455143541097641, acc: 0.9889655113220215)
[2025-02-13 03:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:54][root][INFO] - Training Epoch: 2/2, step 637/7134 completed (loss: 0.022520916536450386, acc: 0.9925512075424194)
[2025-02-13 03:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:54][root][INFO] - Training Epoch: 2/2, step 638/7134 completed (loss: 0.040125831961631775, acc: 0.9921996593475342)
[2025-02-13 03:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:54][root][INFO] - Training Epoch: 2/2, step 639/7134 completed (loss: 0.015197236090898514, acc: 0.9933333396911621)
[2025-02-13 03:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55][root][INFO] - Training Epoch: 2/2, step 640/7134 completed (loss: 0.041910938918590546, acc: 0.991304337978363)
[2025-02-13 03:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55][root][INFO] - Training Epoch: 2/2, step 641/7134 completed (loss: 0.03060547262430191, acc: 0.9901800155639648)
[2025-02-13 03:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:56][root][INFO] - Training Epoch: 2/2, step 642/7134 completed (loss: 0.012156277894973755, acc: 0.9965986609458923)
[2025-02-13 03:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:56][root][INFO] - Training Epoch: 2/2, step 643/7134 completed (loss: 0.015317444689571857, acc: 0.9935815334320068)
[2025-02-13 03:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57][root][INFO] - Training Epoch: 2/2, step 644/7134 completed (loss: 0.013627185486257076, acc: 0.9961389899253845)
[2025-02-13 03:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57][root][INFO] - Training Epoch: 2/2, step 645/7134 completed (loss: 0.02702178619801998, acc: 0.9949495196342468)
[2025-02-13 03:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:58][root][INFO] - Training Epoch: 2/2, step 646/7134 completed (loss: 0.03383425995707512, acc: 0.9938837885856628)
[2025-02-13 03:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:58][root][INFO] - Training Epoch: 2/2, step 647/7134 completed (loss: 0.006386282853782177, acc: 0.9983079433441162)
[2025-02-13 03:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:58][root][INFO] - Training Epoch: 2/2, step 648/7134 completed (loss: 0.05568595230579376, acc: 0.9860383868217468)
[2025-02-13 03:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59][root][INFO] - Training Epoch: 2/2, step 649/7134 completed (loss: 0.028714366257190704, acc: 0.9923076629638672)
[2025-02-13 03:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59][root][INFO] - Training Epoch: 2/2, step 650/7134 completed (loss: 0.039445068687200546, acc: 0.9890109896659851)
[2025-02-13 03:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:00][root][INFO] - Training Epoch: 2/2, step 651/7134 completed (loss: 0.06141398847103119, acc: 0.9862385392189026)
[2025-02-13 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:00][root][INFO] - Training Epoch: 2/2, step 652/7134 completed (loss: 0.023807287216186523, acc: 0.993127167224884)
[2025-02-13 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01][root][INFO] - Training Epoch: 2/2, step 653/7134 completed (loss: 0.02508927509188652, acc: 0.9929078221321106)
[2025-02-13 03:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01][root][INFO] - Training Epoch: 2/2, step 654/7134 completed (loss: 0.10452547669410706, acc: 0.9697624444961548)
[2025-02-13 03:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01][root][INFO] - Training Epoch: 2/2, step 655/7134 completed (loss: 0.037826646119356155, acc: 0.9858871102333069)
[2025-02-13 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:02][root][INFO] - Training Epoch: 2/2, step 656/7134 completed (loss: 0.05528245121240616, acc: 0.9736147522926331)
[2025-02-13 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:02][root][INFO] - Training Epoch: 2/2, step 657/7134 completed (loss: 0.025613490492105484, acc: 0.9919742941856384)
[2025-02-13 03:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:03][root][INFO] - Training Epoch: 2/2, step 658/7134 completed (loss: 0.0296799149364233, acc: 0.990777313709259)
[2025-02-13 03:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:03][root][INFO] - Training Epoch: 2/2, step 659/7134 completed (loss: 0.01696494035422802, acc: 0.9956011772155762)
[2025-02-13 03:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:04][root][INFO] - Training Epoch: 2/2, step 660/7134 completed (loss: 0.05184042453765869, acc: 0.9926900863647461)
[2025-02-13 03:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:04][root][INFO] - Training Epoch: 2/2, step 661/7134 completed (loss: 0.09003221243619919, acc: 0.98221755027771)
[2025-02-13 03:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05][root][INFO] - Training Epoch: 2/2, step 662/7134 completed (loss: 0.025512928143143654, acc: 0.9919571280479431)
[2025-02-13 03:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05][root][INFO] - Training Epoch: 2/2, step 663/7134 completed (loss: 0.029328450560569763, acc: 0.9910213351249695)
[2025-02-13 03:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:06][root][INFO] - Training Epoch: 2/2, step 664/7134 completed (loss: 0.03780924528837204, acc: 0.9950000047683716)
[2025-02-13 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:06][root][INFO] - Training Epoch: 2/2, step 665/7134 completed (loss: 0.036101266741752625, acc: 0.9919246435165405)
[2025-02-13 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:07][root][INFO] - Training Epoch: 2/2, step 666/7134 completed (loss: 0.046768706291913986, acc: 0.9862637519836426)
[2025-02-13 03:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:07][root][INFO] - Training Epoch: 2/2, step 667/7134 completed (loss: 0.039630405604839325, acc: 0.9867452383041382)
[2025-02-13 03:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08][root][INFO] - Training Epoch: 2/2, step 668/7134 completed (loss: 0.02804236114025116, acc: 0.9936628937721252)
[2025-02-13 03:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08][root][INFO] - Training Epoch: 2/2, step 669/7134 completed (loss: 0.02037212997674942, acc: 0.9959072470664978)
[2025-02-13 03:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:09][root][INFO] - Training Epoch: 2/2, step 670/7134 completed (loss: 0.041023921221494675, acc: 0.987679660320282)
[2025-02-13 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:09][root][INFO] - Training Epoch: 2/2, step 671/7134 completed (loss: 0.034951101988554, acc: 0.984994649887085)
[2025-02-13 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:10][root][INFO] - Training Epoch: 2/2, step 672/7134 completed (loss: 0.0533299557864666, acc: 0.9880775213241577)
[2025-02-13 03:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:10][root][INFO] - Training Epoch: 2/2, step 673/7134 completed (loss: 0.03417554125189781, acc: 0.9901719689369202)
[2025-02-13 03:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:10][root][INFO] - Training Epoch: 2/2, step 674/7134 completed (loss: 0.033974070101976395, acc: 0.9894419312477112)
[2025-02-13 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11][root][INFO] - Training Epoch: 2/2, step 675/7134 completed (loss: 0.06569306552410126, acc: 0.9869621992111206)
[2025-02-13 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11][root][INFO] - Training Epoch: 2/2, step 676/7134 completed (loss: 0.0411110520362854, acc: 0.9940828680992126)
[2025-02-13 03:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:12][root][INFO] - Training Epoch: 2/2, step 677/7134 completed (loss: 0.011762475594878197, acc: 0.9963008761405945)
[2025-02-13 03:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:12][root][INFO] - Training Epoch: 2/2, step 678/7134 completed (loss: 0.013276499696075916, acc: 0.9973081946372986)
[2025-02-13 03:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13][root][INFO] - Training Epoch: 2/2, step 679/7134 completed (loss: 0.01339375413954258, acc: 0.9943289160728455)
[2025-02-13 03:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13][root][INFO] - Training Epoch: 2/2, step 680/7134 completed (loss: 0.05153373256325722, acc: 0.9851751923561096)
[2025-02-13 03:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:14][root][INFO] - Training Epoch: 2/2, step 681/7134 completed (loss: 0.0309724323451519, acc: 0.9938650131225586)
[2025-02-13 03:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:14][root][INFO] - Training Epoch: 2/2, step 682/7134 completed (loss: 0.022486479952931404, acc: 0.9927954077720642)
[2025-02-13 03:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15][root][INFO] - Training Epoch: 2/2, step 683/7134 completed (loss: 0.04842415079474449, acc: 0.9849108457565308)
[2025-02-13 03:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15][root][INFO] - Training Epoch: 2/2, step 684/7134 completed (loss: 0.02395748160779476, acc: 0.9889196753501892)
[2025-02-13 03:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:16][root][INFO] - Training Epoch: 2/2, step 685/7134 completed (loss: 0.02213810570538044, acc: 0.9925187230110168)
[2025-02-13 03:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:16][root][INFO] - Training Epoch: 2/2, step 686/7134 completed (loss: 0.005333222448825836, acc: 0.998678982257843)
[2025-02-13 03:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:17][root][INFO] - Training Epoch: 2/2, step 687/7134 completed (loss: 0.015389259904623032, acc: 0.9961315393447876)
[2025-02-13 03:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:17][root][INFO] - Training Epoch: 2/2, step 688/7134 completed (loss: 0.022505197674036026, acc: 0.9955621361732483)
[2025-02-13 03:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:17][root][INFO] - Training Epoch: 2/2, step 689/7134 completed (loss: 0.013576719909906387, acc: 0.9948717951774597)
[2025-02-13 03:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18][root][INFO] - Training Epoch: 2/2, step 690/7134 completed (loss: 0.012782524339854717, acc: 0.996820330619812)
[2025-02-13 03:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18][root][INFO] - Training Epoch: 2/2, step 691/7134 completed (loss: 0.03609297797083855, acc: 0.9928160905838013)
[2025-02-13 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:19][root][INFO] - Training Epoch: 2/2, step 692/7134 completed (loss: 0.04580945149064064, acc: 0.984000027179718)
[2025-02-13 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:19][root][INFO] - Training Epoch: 2/2, step 693/7134 completed (loss: 0.030649816617369652, acc: 0.9854604005813599)
[2025-02-13 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20][root][INFO] - Training Epoch: 2/2, step 694/7134 completed (loss: 0.04875637963414192, acc: 0.9839416146278381)
[2025-02-13 03:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20][root][INFO] - Training Epoch: 2/2, step 695/7134 completed (loss: 0.05781405419111252, acc: 0.9862306118011475)
[2025-02-13 03:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:21][root][INFO] - Training Epoch: 2/2, step 696/7134 completed (loss: 0.0694945901632309, acc: 0.983818769454956)
[2025-02-13 03:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:21][root][INFO] - Training Epoch: 2/2, step 697/7134 completed (loss: 0.023183049634099007, acc: 0.9975185990333557)
[2025-02-13 03:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:21][root][INFO] - Training Epoch: 2/2, step 698/7134 completed (loss: 0.013117937371134758, acc: 0.9967426657676697)
[2025-02-13 03:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:22][root][INFO] - Training Epoch: 2/2, step 699/7134 completed (loss: 0.01867171749472618, acc: 0.9968847632408142)
[2025-02-13 03:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:22][root][INFO] - Training Epoch: 2/2, step 700/7134 completed (loss: 0.04670918732881546, acc: 0.9888059496879578)
[2025-02-13 03:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23][root][INFO] - Training Epoch: 2/2, step 701/7134 completed (loss: 0.029248196631669998, acc: 0.9893617033958435)
[2025-02-13 03:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23][root][INFO] - Training Epoch: 2/2, step 702/7134 completed (loss: 0.02307484671473503, acc: 0.9935064911842346)
[2025-02-13 03:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:24][root][INFO] - Training Epoch: 2/2, step 703/7134 completed (loss: 0.024982986971735954, acc: 0.9919224381446838)
[2025-02-13 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:24][root][INFO] - Training Epoch: 2/2, step 704/7134 completed (loss: 0.02596113085746765, acc: 0.9908814430236816)
[2025-02-13 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:25][root][INFO] - Training Epoch: 2/2, step 705/7134 completed (loss: 0.04494168236851692, acc: 0.9843575358390808)
[2025-02-13 03:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:25][root][INFO] - Training Epoch: 2/2, step 706/7134 completed (loss: 0.014985610730946064, acc: 0.9956236481666565)
[2025-02-13 03:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:26][root][INFO] - Training Epoch: 2/2, step 707/7134 completed (loss: 0.01878076232969761, acc: 0.9928057789802551)
[2025-02-13 03:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:26][root][INFO] - Training Epoch: 2/2, step 708/7134 completed (loss: 0.018486540764570236, acc: 0.9938042163848877)
[2025-02-13 03:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27][root][INFO] - Training Epoch: 2/2, step 709/7134 completed (loss: 0.020565131679177284, acc: 0.9952267408370972)
[2025-02-13 03:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27][root][INFO] - Training Epoch: 2/2, step 710/7134 completed (loss: 0.014665250666439533, acc: 0.9951573610305786)
[2025-02-13 03:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27][root][INFO] - Training Epoch: 2/2, step 711/7134 completed (loss: 0.022298933938145638, acc: 0.9917355179786682)
[2025-02-13 03:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:28][root][INFO] - Training Epoch: 2/2, step 712/7134 completed (loss: 0.030739987269043922, acc: 0.9938949942588806)
[2025-02-13 03:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:28][root][INFO] - Training Epoch: 2/2, step 713/7134 completed (loss: 0.021122053265571594, acc: 0.9942330121994019)
[2025-02-13 03:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:29][root][INFO] - Training Epoch: 2/2, step 714/7134 completed (loss: 0.026136012747883797, acc: 0.9951691031455994)
[2025-02-13 03:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:29][root][INFO] - Training Epoch: 2/2, step 715/7134 completed (loss: 0.005800000857561827, acc: 0.9985795617103577)
[2025-02-13 03:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30][root][INFO] - Training Epoch: 2/2, step 716/7134 completed (loss: 0.012683105655014515, acc: 0.997724711894989)
[2025-02-13 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30][root][INFO] - Training Epoch: 2/2, step 717/7134 completed (loss: 0.04186920449137688, acc: 0.991150438785553)
[2025-02-13 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:31][root][INFO] - Training Epoch: 2/2, step 718/7134 completed (loss: 0.011188995093107224, acc: 0.9962406158447266)
[2025-02-13 03:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:31][root][INFO] - Training Epoch: 2/2, step 719/7134 completed (loss: 0.009091667830944061, acc: 0.998275876045227)
[2025-02-13 03:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:32][root][INFO] - Training Epoch: 2/2, step 720/7134 completed (loss: 0.022127844393253326, acc: 0.9924812316894531)
[2025-02-13 03:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:32][root][INFO] - Training Epoch: 2/2, step 721/7134 completed (loss: 0.022431062534451485, acc: 0.9935979247093201)
[2025-02-13 03:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33][root][INFO] - Training Epoch: 2/2, step 722/7134 completed (loss: 0.015354504808783531, acc: 0.9953325390815735)
[2025-02-13 03:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33][root][INFO] - Training Epoch: 2/2, step 723/7134 completed (loss: 0.009630699642002583, acc: 0.9963680505752563)
[2025-02-13 03:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33][root][INFO] - Training Epoch: 2/2, step 724/7134 completed (loss: 0.017616840079426765, acc: 0.9942775368690491)
[2025-02-13 03:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:34][root][INFO] - Training Epoch: 2/2, step 725/7134 completed (loss: 0.012568507343530655, acc: 0.9971056580543518)
[2025-02-13 03:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:34][root][INFO] - Training Epoch: 2/2, step 726/7134 completed (loss: 0.009697770699858665, acc: 0.9958158731460571)
[2025-02-13 03:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:35][root][INFO] - Training Epoch: 2/2, step 727/7134 completed (loss: 0.005466075148433447, acc: 0.9971140027046204)
[2025-02-13 03:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:35][root][INFO] - Training Epoch: 2/2, step 728/7134 completed (loss: 0.008075051940977573, acc: 0.9958677887916565)
[2025-02-13 03:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36][root][INFO] - Training Epoch: 2/2, step 729/7134 completed (loss: 0.013638672418892384, acc: 0.9958275556564331)
[2025-02-13 03:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36][root][INFO] - Training Epoch: 2/2, step 730/7134 completed (loss: 0.01475489605218172, acc: 0.9948119521141052)
[2025-02-13 03:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:37][root][INFO] - Training Epoch: 2/2, step 731/7134 completed (loss: 0.012802325189113617, acc: 0.9973333477973938)
[2025-02-13 03:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:37][root][INFO] - Training Epoch: 2/2, step 732/7134 completed (loss: 0.03293187543749809, acc: 0.9886792302131653)
[2025-02-13 03:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:38][root][INFO] - Training Epoch: 2/2, step 733/7134 completed (loss: 0.036843955516815186, acc: 0.9857904314994812)
[2025-02-13 03:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:38][root][INFO] - Training Epoch: 2/2, step 734/7134 completed (loss: 0.007837935350835323, acc: 0.996927797794342)
[2025-02-13 03:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39][root][INFO] - Training Epoch: 2/2, step 735/7134 completed (loss: 0.019545556977391243, acc: 0.9935691356658936)
[2025-02-13 03:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39][root][INFO] - Training Epoch: 2/2, step 736/7134 completed (loss: 0.021542182192206383, acc: 0.9890109896659851)
[2025-02-13 03:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39][root][INFO] - Training Epoch: 2/2, step 737/7134 completed (loss: 0.06633878499269485, acc: 0.9858012199401855)
[2025-02-13 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:40][root][INFO] - Training Epoch: 2/2, step 738/7134 completed (loss: 0.008381322957575321, acc: 0.9942307472229004)
[2025-02-13 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:40][root][INFO] - Training Epoch: 2/2, step 739/7134 completed (loss: 0.017360325902700424, acc: 0.9926605224609375)
[2025-02-13 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:41][root][INFO] - Training Epoch: 2/2, step 740/7134 completed (loss: 0.0029069241136312485, acc: 1.0)
[2025-02-13 03:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:41][root][INFO] - Training Epoch: 2/2, step 741/7134 completed (loss: 0.016783075407147408, acc: 0.9952681660652161)
[2025-02-13 03:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42][root][INFO] - Training Epoch: 2/2, step 742/7134 completed (loss: 0.016469834372401237, acc: 0.9937106966972351)
[2025-02-13 03:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42][root][INFO] - Training Epoch: 2/2, step 743/7134 completed (loss: 0.0395321398973465, acc: 0.990963876247406)
[2025-02-13 03:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42][root][INFO] - Training Epoch: 2/2, step 744/7134 completed (loss: 0.01618640497326851, acc: 0.9940476417541504)
[2025-02-13 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:43][root][INFO] - Training Epoch: 2/2, step 745/7134 completed (loss: 0.0207811389118433, acc: 0.995468258857727)
[2025-02-13 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:43][root][INFO] - Training Epoch: 2/2, step 746/7134 completed (loss: 0.03463934361934662, acc: 0.9901477694511414)
[2025-02-13 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44][root][INFO] - Training Epoch: 2/2, step 747/7134 completed (loss: 0.0848364531993866, acc: 0.9800664186477661)
[2025-02-13 03:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44][root][INFO] - Training Epoch: 2/2, step 748/7134 completed (loss: 0.01677819713950157, acc: 0.995488703250885)
[2025-02-13 03:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:45][root][INFO] - Training Epoch: 2/2, step 749/7134 completed (loss: 0.014929037541151047, acc: 0.9956772327423096)
[2025-02-13 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:45][root][INFO] - Training Epoch: 2/2, step 750/7134 completed (loss: 0.015542595647275448, acc: 0.9945945739746094)
[2025-02-13 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:46][root][INFO] - Training Epoch: 2/2, step 751/7134 completed (loss: 0.07228437811136246, acc: 0.9829642176628113)
[2025-02-13 03:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:46][root][INFO] - Training Epoch: 2/2, step 752/7134 completed (loss: 0.06171707808971405, acc: 0.9879879951477051)
[2025-02-13 03:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:46][root][INFO] - Training Epoch: 2/2, step 753/7134 completed (loss: 0.03215787559747696, acc: 0.9915540814399719)
[2025-02-13 03:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47][root][INFO] - Training Epoch: 2/2, step 754/7134 completed (loss: 0.017435070127248764, acc: 0.9954751133918762)
[2025-02-13 03:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47][root][INFO] - Training Epoch: 2/2, step 755/7134 completed (loss: 0.06261531263589859, acc: 0.9867987036705017)
[2025-02-13 03:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:48][root][INFO] - Training Epoch: 2/2, step 756/7134 completed (loss: 0.027874881401658058, acc: 0.9925233721733093)
[2025-02-13 03:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:48][root][INFO] - Training Epoch: 2/2, step 757/7134 completed (loss: 0.016114847734570503, acc: 0.9942528605461121)
[2025-02-13 03:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:49][root][INFO] - Training Epoch: 2/2, step 758/7134 completed (loss: 0.01848992519080639, acc: 0.9931153059005737)
[2025-02-13 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:49][root][INFO] - Training Epoch: 2/2, step 759/7134 completed (loss: 0.027801023796200752, acc: 0.9883720874786377)
[2025-02-13 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50][root][INFO] - Training Epoch: 2/2, step 760/7134 completed (loss: 0.006405603140592575, acc: 1.0)
[2025-02-13 03:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50][root][INFO] - Training Epoch: 2/2, step 761/7134 completed (loss: 0.02011496014893055, acc: 0.9937402009963989)
[2025-02-13 03:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50][root][INFO] - Training Epoch: 2/2, step 762/7134 completed (loss: 0.0161786787211895, acc: 0.9946236610412598)
[2025-02-13 03:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:51][root][INFO] - Training Epoch: 2/2, step 763/7134 completed (loss: 0.04188202694058418, acc: 0.9853420257568359)
[2025-02-13 03:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:51][root][INFO] - Training Epoch: 2/2, step 764/7134 completed (loss: 0.014805072918534279, acc: 0.9942611455917358)
[2025-02-13 03:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:52][root][INFO] - Training Epoch: 2/2, step 765/7134 completed (loss: 0.010336418636143208, acc: 0.9970717430114746)
[2025-02-13 03:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:52][root][INFO] - Training Epoch: 2/2, step 766/7134 completed (loss: 0.01236867718398571, acc: 0.997032642364502)
[2025-02-13 03:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53][root][INFO] - Training Epoch: 2/2, step 767/7134 completed (loss: 0.020562997087836266, acc: 0.9956076145172119)
[2025-02-13 03:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53][root][INFO] - Training Epoch: 2/2, step 768/7134 completed (loss: 0.03289765864610672, acc: 0.9900850057601929)
[2025-02-13 03:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:54][root][INFO] - Training Epoch: 2/2, step 769/7134 completed (loss: 0.009568510577082634, acc: 0.9971346855163574)
[2025-02-13 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:54][root][INFO] - Training Epoch: 2/2, step 770/7134 completed (loss: 0.023243669420480728, acc: 0.9954057931900024)
[2025-02-13 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:54][root][INFO] - Training Epoch: 2/2, step 771/7134 completed (loss: 0.025388697162270546, acc: 0.9900990128517151)
[2025-02-13 03:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:55][root][INFO] - Training Epoch: 2/2, step 772/7134 completed (loss: 0.0200644601136446, acc: 0.9932998418807983)
[2025-02-13 03:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:55][root][INFO] - Training Epoch: 2/2, step 773/7134 completed (loss: 0.019871164113283157, acc: 0.9935732483863831)
[2025-02-13 03:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56][root][INFO] - Training Epoch: 2/2, step 774/7134 completed (loss: 0.010741075500845909, acc: 0.9966777563095093)
[2025-02-13 03:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56][root][INFO] - Training Epoch: 2/2, step 775/7134 completed (loss: 0.022867796942591667, acc: 0.9960052967071533)
[2025-02-13 03:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:57][root][INFO] - Training Epoch: 2/2, step 776/7134 completed (loss: 0.005502472631633282, acc: 0.9985207319259644)
[2025-02-13 03:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:57][root][INFO] - Training Epoch: 2/2, step 777/7134 completed (loss: 0.018366292119026184, acc: 0.9967897534370422)
[2025-02-13 03:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:58][root][INFO] - Training Epoch: 2/2, step 778/7134 completed (loss: 0.039139945060014725, acc: 0.9919871687889099)
[2025-02-13 03:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:58][root][INFO] - Training Epoch: 2/2, step 779/7134 completed (loss: 0.025073764845728874, acc: 0.9956204295158386)
[2025-02-13 03:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:59][root][INFO] - Training Epoch: 2/2, step 780/7134 completed (loss: 0.017276762053370476, acc: 0.9923954606056213)
[2025-02-13 03:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:59][root][INFO] - Training Epoch: 2/2, step 781/7134 completed (loss: 0.026622086763381958, acc: 0.9914529919624329)
[2025-02-13 03:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:59][root][INFO] - Training Epoch: 2/2, step 782/7134 completed (loss: 0.020089328289031982, acc: 0.9936708807945251)
[2025-02-13 03:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00][root][INFO] - Training Epoch: 2/2, step 783/7134 completed (loss: 0.028023749589920044, acc: 0.9923567175865173)
[2025-02-13 03:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00][root][INFO] - Training Epoch: 2/2, step 784/7134 completed (loss: 0.028451373800635338, acc: 0.986601710319519)
[2025-02-13 03:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:01][root][INFO] - Training Epoch: 2/2, step 785/7134 completed (loss: 0.012468121014535427, acc: 1.0)
[2025-02-13 03:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:01][root][INFO] - Training Epoch: 2/2, step 786/7134 completed (loss: 0.03317861258983612, acc: 0.9944238066673279)
[2025-02-13 03:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:02][root][INFO] - Training Epoch: 2/2, step 787/7134 completed (loss: 0.00519730057567358, acc: 1.0)
[2025-02-13 03:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:02][root][INFO] - Training Epoch: 2/2, step 788/7134 completed (loss: 0.045103415846824646, acc: 0.9867841601371765)
[2025-02-13 03:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:03][root][INFO] - Training Epoch: 2/2, step 789/7134 completed (loss: 0.021382354199886322, acc: 0.9938931465148926)
[2025-02-13 03:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:03][root][INFO] - Training Epoch: 2/2, step 790/7134 completed (loss: 0.06071211025118828, acc: 0.9881656765937805)
[2025-02-13 03:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04][root][INFO] - Training Epoch: 2/2, step 791/7134 completed (loss: 0.049222804605960846, acc: 0.9881423115730286)
[2025-02-13 03:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04][root][INFO] - Training Epoch: 2/2, step 792/7134 completed (loss: 0.03269205987453461, acc: 0.9907264113426208)
[2025-02-13 03:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04][root][INFO] - Training Epoch: 2/2, step 793/7134 completed (loss: 0.009477328509092331, acc: 0.9955621361732483)
[2025-02-13 03:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:05][root][INFO] - Training Epoch: 2/2, step 794/7134 completed (loss: 0.028808385133743286, acc: 0.9906759858131409)
[2025-02-13 03:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:05][root][INFO] - Training Epoch: 2/2, step 795/7134 completed (loss: 0.052953559905290604, acc: 0.9844236969947815)
[2025-02-13 03:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06][root][INFO] - Training Epoch: 2/2, step 796/7134 completed (loss: 0.04859141632914543, acc: 0.9797979593276978)
[2025-02-13 03:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06][root][INFO] - Training Epoch: 2/2, step 797/7134 completed (loss: 0.037786975502967834, acc: 0.9899598360061646)
[2025-02-13 03:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06][root][INFO] - Training Epoch: 2/2, step 798/7134 completed (loss: 0.01679125986993313, acc: 0.9920634627342224)
[2025-02-13 03:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07][root][INFO] - Training Epoch: 2/2, step 799/7134 completed (loss: 0.02708994410932064, acc: 0.9892473220825195)
[2025-02-13 03:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07][root][INFO] - Training Epoch: 2/2, step 800/7134 completed (loss: 0.039070215076208115, acc: 0.9906890392303467)
[2025-02-13 03:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:08][root][INFO] - Training Epoch: 2/2, step 801/7134 completed (loss: 0.047476381063461304, acc: 0.9855999946594238)
[2025-02-13 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:08][root][INFO] - Training Epoch: 2/2, step 802/7134 completed (loss: 0.03348735719919205, acc: 0.988727867603302)
[2025-02-13 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:09][root][INFO] - Training Epoch: 2/2, step 803/7134 completed (loss: 0.011043798178434372, acc: 1.0)
[2025-02-13 03:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:09][root][INFO] - Training Epoch: 2/2, step 804/7134 completed (loss: 0.01746656559407711, acc: 1.0)
[2025-02-13 03:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:09][root][INFO] - Training Epoch: 2/2, step 805/7134 completed (loss: 0.036096468567848206, acc: 0.9850187301635742)
[2025-02-13 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:10][root][INFO] - Training Epoch: 2/2, step 806/7134 completed (loss: 0.014568635262548923, acc: 0.9972066879272461)
[2025-02-13 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:10][root][INFO] - Training Epoch: 2/2, step 807/7134 completed (loss: 0.034742098301649094, acc: 0.989051103591919)
[2025-02-13 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11][root][INFO] - Training Epoch: 2/2, step 808/7134 completed (loss: 0.015409602783620358, acc: 0.9964072108268738)
[2025-02-13 03:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11][root][INFO] - Training Epoch: 2/2, step 809/7134 completed (loss: 0.0077154384925961494, acc: 0.9985141158103943)
[2025-02-13 03:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12][root][INFO] - Training Epoch: 2/2, step 810/7134 completed (loss: 0.03177723288536072, acc: 0.9905533194541931)
[2025-02-13 03:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12][root][INFO] - Training Epoch: 2/2, step 811/7134 completed (loss: 0.08306992799043655, acc: 0.9806678295135498)
[2025-02-13 03:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13][root][INFO] - Training Epoch: 2/2, step 812/7134 completed (loss: 0.020411618053913116, acc: 0.9936948418617249)
[2025-02-13 03:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13][root][INFO] - Training Epoch: 2/2, step 813/7134 completed (loss: 0.024405671283602715, acc: 0.9883889555931091)
[2025-02-13 03:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13][root][INFO] - Training Epoch: 2/2, step 814/7134 completed (loss: 0.02239094488322735, acc: 0.9935483932495117)
[2025-02-13 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:14][root][INFO] - Training Epoch: 2/2, step 815/7134 completed (loss: 0.03670334070920944, acc: 0.989180862903595)
[2025-02-13 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:14][root][INFO] - Training Epoch: 2/2, step 816/7134 completed (loss: 0.04305616021156311, acc: 0.989830493927002)
[2025-02-13 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15][root][INFO] - Training Epoch: 2/2, step 817/7134 completed (loss: 0.059414342045784, acc: 0.9890310764312744)
[2025-02-13 03:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15][root][INFO] - Training Epoch: 2/2, step 818/7134 completed (loss: 0.04728880152106285, acc: 0.9810996651649475)
[2025-02-13 03:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:16][root][INFO] - Training Epoch: 2/2, step 819/7134 completed (loss: 0.04252350330352783, acc: 0.9919999837875366)
[2025-02-13 03:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:16][root][INFO] - Training Epoch: 2/2, step 820/7134 completed (loss: 0.03504658862948418, acc: 0.9938176274299622)
[2025-02-13 03:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:17][root][INFO] - Training Epoch: 2/2, step 821/7134 completed (loss: 0.0290437750518322, acc: 0.9946380853652954)
[2025-02-13 03:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:17][root][INFO] - Training Epoch: 2/2, step 822/7134 completed (loss: 0.018229249864816666, acc: 0.9921507239341736)
[2025-02-13 03:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:17][root][INFO] - Training Epoch: 2/2, step 823/7134 completed (loss: 0.00821433961391449, acc: 0.9962476491928101)
[2025-02-13 03:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:18][root][INFO] - Training Epoch: 2/2, step 824/7134 completed (loss: 0.035494741052389145, acc: 0.9876325130462646)
[2025-02-13 03:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:18][root][INFO] - Training Epoch: 2/2, step 825/7134 completed (loss: 0.026891708374023438, acc: 0.992094874382019)
[2025-02-13 03:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:19][root][INFO] - Training Epoch: 2/2, step 826/7134 completed (loss: 0.014662203378975391, acc: 0.9972752332687378)
[2025-02-13 03:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:19][root][INFO] - Training Epoch: 2/2, step 827/7134 completed (loss: 0.02963585965335369, acc: 0.9895012974739075)
[2025-02-13 03:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20][root][INFO] - Training Epoch: 2/2, step 828/7134 completed (loss: 0.036113761365413666, acc: 0.9922239780426025)
[2025-02-13 03:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20][root][INFO] - Training Epoch: 2/2, step 829/7134 completed (loss: 0.02360301837325096, acc: 0.9975728392601013)
[2025-02-13 03:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:21][root][INFO] - Training Epoch: 2/2, step 830/7134 completed (loss: 0.0288984552025795, acc: 0.9900332093238831)
[2025-02-13 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:21][root][INFO] - Training Epoch: 2/2, step 831/7134 completed (loss: 0.007851291447877884, acc: 0.998630166053772)
[2025-02-13 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:21][root][INFO] - Training Epoch: 2/2, step 832/7134 completed (loss: 0.06858516484498978, acc: 0.9829351305961609)
[2025-02-13 03:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:22][root][INFO] - Training Epoch: 2/2, step 833/7134 completed (loss: 0.014632091857492924, acc: 0.9957982897758484)
[2025-02-13 03:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:22][root][INFO] - Training Epoch: 2/2, step 834/7134 completed (loss: 0.01176638063043356, acc: 0.9958041906356812)
[2025-02-13 03:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:23][root][INFO] - Training Epoch: 2/2, step 835/7134 completed (loss: 0.03824663907289505, acc: 0.988811194896698)
[2025-02-13 03:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:23][root][INFO] - Training Epoch: 2/2, step 836/7134 completed (loss: 0.033313293009996414, acc: 0.9941995143890381)
[2025-02-13 03:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24][root][INFO] - Training Epoch: 2/2, step 837/7134 completed (loss: 0.0314360111951828, acc: 0.9931880235671997)
[2025-02-13 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24][root][INFO] - Training Epoch: 2/2, step 838/7134 completed (loss: 0.011727171950042248, acc: 0.998410165309906)
[2025-02-13 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:25][root][INFO] - Training Epoch: 2/2, step 839/7134 completed (loss: 0.005942970979958773, acc: 1.0)
[2025-02-13 03:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:25][root][INFO] - Training Epoch: 2/2, step 840/7134 completed (loss: 0.014995133504271507, acc: 0.9944211840629578)
[2025-02-13 03:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:26][root][INFO] - Training Epoch: 2/2, step 841/7134 completed (loss: 0.020211681723594666, acc: 0.9930843710899353)
[2025-02-13 03:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:26][root][INFO] - Training Epoch: 2/2, step 842/7134 completed (loss: 0.015112664550542831, acc: 0.9946091771125793)
[2025-02-13 03:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:27][root][INFO] - Training Epoch: 2/2, step 843/7134 completed (loss: 0.020453082397580147, acc: 0.9944367408752441)
[2025-02-13 03:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:27][root][INFO] - Training Epoch: 2/2, step 844/7134 completed (loss: 0.02326267585158348, acc: 0.9889415502548218)
[2025-02-13 03:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:27][root][INFO] - Training Epoch: 2/2, step 845/7134 completed (loss: 0.02542526088654995, acc: 0.991150438785553)
[2025-02-13 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:28][root][INFO] - Training Epoch: 2/2, step 846/7134 completed (loss: 0.015224941074848175, acc: 0.997032642364502)
[2025-02-13 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:28][root][INFO] - Training Epoch: 2/2, step 847/7134 completed (loss: 0.015081910416483879, acc: 0.99615877866745)
[2025-02-13 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29][root][INFO] - Training Epoch: 2/2, step 848/7134 completed (loss: 0.029574906453490257, acc: 0.988875150680542)
[2025-02-13 03:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29][root][INFO] - Training Epoch: 2/2, step 849/7134 completed (loss: 0.015764834359288216, acc: 0.993954062461853)
[2025-02-13 03:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:30][root][INFO] - Training Epoch: 2/2, step 850/7134 completed (loss: 0.009090987034142017, acc: 0.9959731698036194)
[2025-02-13 03:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:30][root][INFO] - Training Epoch: 2/2, step 851/7134 completed (loss: 0.034614093601703644, acc: 0.9868593811988831)
[2025-02-13 03:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:31][root][INFO] - Training Epoch: 2/2, step 852/7134 completed (loss: 0.006560177076607943, acc: 0.9987789988517761)
[2025-02-13 03:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:31][root][INFO] - Training Epoch: 2/2, step 853/7134 completed (loss: 0.010924908332526684, acc: 0.9955947399139404)
[2025-02-13 03:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32][root][INFO] - Training Epoch: 2/2, step 854/7134 completed (loss: 0.01982005499303341, acc: 0.9975124597549438)
[2025-02-13 03:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32][root][INFO] - Training Epoch: 2/2, step 855/7134 completed (loss: 0.02427443489432335, acc: 0.9906322956085205)
[2025-02-13 03:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33][root][INFO] - Training Epoch: 2/2, step 856/7134 completed (loss: 0.010868449695408344, acc: 0.9975786805152893)
[2025-02-13 03:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33][root][INFO] - Training Epoch: 2/2, step 857/7134 completed (loss: 0.025824666023254395, acc: 0.9941262602806091)
[2025-02-13 03:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33][root][INFO] - Training Epoch: 2/2, step 858/7134 completed (loss: 0.038381677120923996, acc: 0.9850746393203735)
[2025-02-13 03:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:34][root][INFO] - Training Epoch: 2/2, step 859/7134 completed (loss: 0.034861717373132706, acc: 0.9928910136222839)
[2025-02-13 03:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:34][root][INFO] - Training Epoch: 2/2, step 860/7134 completed (loss: 0.029747016727924347, acc: 0.9886220097541809)
[2025-02-13 03:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:35][root][INFO] - Training Epoch: 2/2, step 861/7134 completed (loss: 0.04563811793923378, acc: 0.9848320484161377)
[2025-02-13 03:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:35][root][INFO] - Training Epoch: 2/2, step 862/7134 completed (loss: 0.02553548477590084, acc: 0.9915764331817627)
[2025-02-13 03:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:36][root][INFO] - Training Epoch: 2/2, step 863/7134 completed (loss: 0.021193327382206917, acc: 0.9941792488098145)
[2025-02-13 03:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:36][root][INFO] - Training Epoch: 2/2, step 864/7134 completed (loss: 0.04458429291844368, acc: 0.9840707778930664)
[2025-02-13 03:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:37][root][INFO] - Training Epoch: 2/2, step 865/7134 completed (loss: 0.044203177094459534, acc: 0.9817517995834351)
[2025-02-13 03:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:37][root][INFO] - Training Epoch: 2/2, step 866/7134 completed (loss: 0.04827719181776047, acc: 0.9851852059364319)
[2025-02-13 03:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38][root][INFO] - Training Epoch: 2/2, step 867/7134 completed (loss: 0.04318162426352501, acc: 0.9887359142303467)
[2025-02-13 03:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38][root][INFO] - Training Epoch: 2/2, step 868/7134 completed (loss: 0.0518801212310791, acc: 0.9895287752151489)
[2025-02-13 03:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38][root][INFO] - Training Epoch: 2/2, step 869/7134 completed (loss: 0.04335227981209755, acc: 0.987261176109314)
[2025-02-13 03:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:39][root][INFO] - Training Epoch: 2/2, step 870/7134 completed (loss: 0.017590513452887535, acc: 0.9946091771125793)
[2025-02-13 03:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:39][root][INFO] - Training Epoch: 2/2, step 871/7134 completed (loss: 0.05580355226993561, acc: 0.9885993599891663)
[2025-02-13 03:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:40][root][INFO] - Training Epoch: 2/2, step 872/7134 completed (loss: 0.03685063496232033, acc: 0.9905808568000793)
[2025-02-13 03:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:40][root][INFO] - Training Epoch: 2/2, step 873/7134 completed (loss: 0.05112093687057495, acc: 0.9873239398002625)
[2025-02-13 03:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41][root][INFO] - Training Epoch: 2/2, step 874/7134 completed (loss: 0.02114635892212391, acc: 0.9938367009162903)
[2025-02-13 03:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41][root][INFO] - Training Epoch: 2/2, step 875/7134 completed (loss: 0.019680721685290337, acc: 0.9926793575286865)
[2025-02-13 03:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:42][root][INFO] - Training Epoch: 2/2, step 876/7134 completed (loss: 0.01626277156174183, acc: 0.9970760345458984)
[2025-02-13 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:42][root][INFO] - Training Epoch: 2/2, step 877/7134 completed (loss: 0.057594869285821915, acc: 0.9827814698219299)
[2025-02-13 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:43][root][INFO] - Training Epoch: 2/2, step 878/7134 completed (loss: 0.02673276886343956, acc: 0.9907161593437195)
[2025-02-13 03:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:43][root][INFO] - Training Epoch: 2/2, step 879/7134 completed (loss: 0.06049869954586029, acc: 0.9854439496994019)
[2025-02-13 03:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:43][root][INFO] - Training Epoch: 2/2, step 880/7134 completed (loss: 0.0075967516750097275, acc: 0.9970887899398804)
[2025-02-13 03:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:44][root][INFO] - Training Epoch: 2/2, step 881/7134 completed (loss: 0.012756573036313057, acc: 0.9979959726333618)
[2025-02-13 03:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:44][root][INFO] - Training Epoch: 2/2, step 882/7134 completed (loss: 0.027617277577519417, acc: 0.9946042895317078)
[2025-02-13 03:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45][root][INFO] - Training Epoch: 2/2, step 883/7134 completed (loss: 0.050030868500471115, acc: 0.9831804037094116)
[2025-02-13 03:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45][root][INFO] - Training Epoch: 2/2, step 884/7134 completed (loss: 0.01910625398159027, acc: 0.9932975769042969)
[2025-02-13 03:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:46][root][INFO] - Training Epoch: 2/2, step 885/7134 completed (loss: 0.003064413322135806, acc: 1.0)
[2025-02-13 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:46][root][INFO] - Training Epoch: 2/2, step 886/7134 completed (loss: 0.03451334685087204, acc: 0.9941520690917969)
[2025-02-13 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:47][root][INFO] - Training Epoch: 2/2, step 887/7134 completed (loss: 0.0219480711966753, acc: 0.9982300996780396)
[2025-02-13 03:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:47][root][INFO] - Training Epoch: 2/2, step 888/7134 completed (loss: 0.043093591928482056, acc: 0.987860381603241)
[2025-02-13 03:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:47][root][INFO] - Training Epoch: 2/2, step 889/7134 completed (loss: 0.0062430487014353275, acc: 1.0)
[2025-02-13 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48][root][INFO] - Training Epoch: 2/2, step 890/7134 completed (loss: 0.04649801924824715, acc: 0.9915110468864441)
[2025-02-13 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48][root][INFO] - Training Epoch: 2/2, step 891/7134 completed (loss: 0.04832153022289276, acc: 0.9888734221458435)
[2025-02-13 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:49][root][INFO] - Training Epoch: 2/2, step 892/7134 completed (loss: 0.043206099420785904, acc: 0.9883720874786377)
[2025-02-13 03:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:49][root][INFO] - Training Epoch: 2/2, step 893/7134 completed (loss: 0.04020574316382408, acc: 0.99071204662323)
[2025-02-13 03:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:50][root][INFO] - Training Epoch: 2/2, step 894/7134 completed (loss: 0.03281127288937569, acc: 0.9902234673500061)
[2025-02-13 03:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:50][root][INFO] - Training Epoch: 2/2, step 895/7134 completed (loss: 0.015884846448898315, acc: 0.9965811967849731)
[2025-02-13 03:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:50][root][INFO] - Training Epoch: 2/2, step 896/7134 completed (loss: 0.023162899538874626, acc: 0.9940000176429749)
[2025-02-13 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51][root][INFO] - Training Epoch: 2/2, step 897/7134 completed (loss: 0.013910654932260513, acc: 0.9944289922714233)
[2025-02-13 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51][root][INFO] - Training Epoch: 2/2, step 898/7134 completed (loss: 0.008846542797982693, acc: 0.9962639808654785)
[2025-02-13 03:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:52][root][INFO] - Training Epoch: 2/2, step 899/7134 completed (loss: 0.01773807965219021, acc: 0.9937810897827148)
[2025-02-13 03:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:52][root][INFO] - Training Epoch: 2/2, step 900/7134 completed (loss: 0.014411259442567825, acc: 0.9943740963935852)
[2025-02-13 03:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53][root][INFO] - Training Epoch: 2/2, step 901/7134 completed (loss: 0.01574014499783516, acc: 0.9933333396911621)
[2025-02-13 03:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53][root][INFO] - Training Epoch: 2/2, step 902/7134 completed (loss: 0.01276728231459856, acc: 0.9985652565956116)
[2025-02-13 03:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:54][root][INFO] - Training Epoch: 2/2, step 903/7134 completed (loss: 0.004080943763256073, acc: 1.0)
[2025-02-13 03:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:54][root][INFO] - Training Epoch: 2/2, step 904/7134 completed (loss: 0.009126431308686733, acc: 0.9986979365348816)
[2025-02-13 03:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:55][root][INFO] - Training Epoch: 2/2, step 905/7134 completed (loss: 0.010004465468227863, acc: 0.9986522793769836)
[2025-02-13 03:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:55][root][INFO] - Training Epoch: 2/2, step 906/7134 completed (loss: 0.007306884508579969, acc: 0.9986594915390015)
[2025-02-13 03:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:56][root][INFO] - Training Epoch: 2/2, step 907/7134 completed (loss: 0.007698102854192257, acc: 0.9986842274665833)
[2025-02-13 03:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:56][root][INFO] - Training Epoch: 2/2, step 908/7134 completed (loss: 0.014114278368651867, acc: 0.9934980273246765)
[2025-02-13 03:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:56][root][INFO] - Training Epoch: 2/2, step 909/7134 completed (loss: 0.032030172646045685, acc: 0.9939939975738525)
[2025-02-13 03:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57][root][INFO] - Training Epoch: 2/2, step 910/7134 completed (loss: 0.030162140727043152, acc: 0.9893190860748291)
[2025-02-13 03:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57][root][INFO] - Training Epoch: 2/2, step 911/7134 completed (loss: 0.008004209958016872, acc: 0.9972028136253357)
[2025-02-13 03:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:58][root][INFO] - Training Epoch: 2/2, step 912/7134 completed (loss: 0.0361732579767704, acc: 0.9924337863922119)
[2025-02-13 03:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:58][root][INFO] - Training Epoch: 2/2, step 913/7134 completed (loss: 0.013500149361789227, acc: 0.9949748516082764)
[2025-02-13 03:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:59][root][INFO] - Training Epoch: 2/2, step 914/7134 completed (loss: 0.016948118805885315, acc: 0.9936224222183228)
[2025-02-13 03:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:59][root][INFO] - Training Epoch: 2/2, step 915/7134 completed (loss: 0.06688475608825684, acc: 0.9870550036430359)
[2025-02-13 03:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00][root][INFO] - Training Epoch: 2/2, step 916/7134 completed (loss: 0.011301775462925434, acc: 0.9965096116065979)
[2025-02-13 03:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00][root][INFO] - Training Epoch: 2/2, step 917/7134 completed (loss: 0.03593236207962036, acc: 0.9879336357116699)
[2025-02-13 03:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00][root][INFO] - Training Epoch: 2/2, step 918/7134 completed (loss: 0.008283755742013454, acc: 0.9969372153282166)
[2025-02-13 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:01][root][INFO] - Training Epoch: 2/2, step 919/7134 completed (loss: 0.009787137620151043, acc: 0.9956709742546082)
[2025-02-13 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:01][root][INFO] - Training Epoch: 2/2, step 920/7134 completed (loss: 0.023091038689017296, acc: 0.994397759437561)
[2025-02-13 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02][root][INFO] - Training Epoch: 2/2, step 921/7134 completed (loss: 0.01012010034173727, acc: 0.9959349632263184)
[2025-02-13 03:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02][root][INFO] - Training Epoch: 2/2, step 922/7134 completed (loss: 0.01971866935491562, acc: 0.9918032884597778)
[2025-02-13 03:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:03][root][INFO] - Training Epoch: 2/2, step 923/7134 completed (loss: 0.03355589509010315, acc: 0.9894179701805115)
[2025-02-13 03:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:03][root][INFO] - Training Epoch: 2/2, step 924/7134 completed (loss: 0.0033501789439469576, acc: 0.998792290687561)
[2025-02-13 03:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:04][root][INFO] - Training Epoch: 2/2, step 925/7134 completed (loss: 0.025530384853482246, acc: 0.9926578402519226)
[2025-02-13 03:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:04][root][INFO] - Training Epoch: 2/2, step 926/7134 completed (loss: 0.04009194299578667, acc: 0.9912609457969666)
[2025-02-13 03:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05][root][INFO] - Training Epoch: 2/2, step 927/7134 completed (loss: 0.05268537625670433, acc: 0.9934640526771545)
[2025-02-13 03:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05][root][INFO] - Training Epoch: 2/2, step 928/7134 completed (loss: 0.02002267725765705, acc: 0.9921104311943054)
[2025-02-13 03:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05][root][INFO] - Training Epoch: 2/2, step 929/7134 completed (loss: 0.016555096954107285, acc: 0.9952606558799744)
[2025-02-13 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:06][root][INFO] - Training Epoch: 2/2, step 930/7134 completed (loss: 0.0316263847053051, acc: 0.9919354915618896)
[2025-02-13 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:06][root][INFO] - Training Epoch: 2/2, step 931/7134 completed (loss: 0.03718443959951401, acc: 0.9894179701805115)
[2025-02-13 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:07][root][INFO] - Training Epoch: 2/2, step 932/7134 completed (loss: 0.023715347051620483, acc: 0.9899713397026062)
[2025-02-13 03:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:07][root][INFO] - Training Epoch: 2/2, step 933/7134 completed (loss: 0.029731879010796547, acc: 0.995768666267395)
[2025-02-13 03:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:08][root][INFO] - Training Epoch: 2/2, step 934/7134 completed (loss: 0.005904120858758688, acc: 0.9986910820007324)
[2025-02-13 03:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:08][root][INFO] - Training Epoch: 2/2, step 935/7134 completed (loss: 0.017826318740844727, acc: 0.9933221936225891)
[2025-02-13 03:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:09][root][INFO] - Training Epoch: 2/2, step 936/7134 completed (loss: 0.02295026369392872, acc: 0.9964850544929504)
[2025-02-13 03:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:09][root][INFO] - Training Epoch: 2/2, step 937/7134 completed (loss: 0.018534038215875626, acc: 0.9982394576072693)
[2025-02-13 03:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:09][root][INFO] - Training Epoch: 2/2, step 938/7134 completed (loss: 0.010524138808250427, acc: 0.9942693114280701)
[2025-02-13 03:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10][root][INFO] - Training Epoch: 2/2, step 939/7134 completed (loss: 0.027248967438936234, acc: 0.9890965819358826)
[2025-02-13 03:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10][root][INFO] - Training Epoch: 2/2, step 940/7134 completed (loss: 0.029461681842803955, acc: 0.9928469061851501)
[2025-02-13 03:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:11][root][INFO] - Training Epoch: 2/2, step 941/7134 completed (loss: 0.01835663616657257, acc: 0.9931034445762634)
[2025-02-13 03:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:11][root][INFO] - Training Epoch: 2/2, step 942/7134 completed (loss: 0.02765452302992344, acc: 0.994194507598877)
[2025-02-13 03:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:12][root][INFO] - Training Epoch: 2/2, step 943/7134 completed (loss: 0.015715694054961205, acc: 0.9969558715820312)
[2025-02-13 03:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:12][root][INFO] - Training Epoch: 2/2, step 944/7134 completed (loss: 0.022557897493243217, acc: 0.9940029978752136)
[2025-02-13 03:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:13][root][INFO] - Training Epoch: 2/2, step 945/7134 completed (loss: 0.029582908377051353, acc: 0.9958041906356812)
[2025-02-13 03:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:13][root][INFO] - Training Epoch: 2/2, step 946/7134 completed (loss: 0.005796373821794987, acc: 1.0)
[2025-02-13 03:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:13][root][INFO] - Training Epoch: 2/2, step 947/7134 completed (loss: 0.028872938826680183, acc: 0.9948365092277527)
[2025-02-13 03:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:14][root][INFO] - Training Epoch: 2/2, step 948/7134 completed (loss: 0.023137791082262993, acc: 0.9944367408752441)
[2025-02-13 03:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:14][root][INFO] - Training Epoch: 2/2, step 949/7134 completed (loss: 0.015422489494085312, acc: 0.9957924485206604)
[2025-02-13 03:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:15][root][INFO] - Training Epoch: 2/2, step 950/7134 completed (loss: 0.012027639895677567, acc: 0.9938271641731262)
[2025-02-13 03:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:15][root][INFO] - Training Epoch: 2/2, step 951/7134 completed (loss: 0.017191793769598007, acc: 0.9947183132171631)
[2025-02-13 03:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:16][root][INFO] - Training Epoch: 2/2, step 952/7134 completed (loss: 0.028339318931102753, acc: 0.9883527159690857)
[2025-02-13 03:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:16][root][INFO] - Training Epoch: 2/2, step 953/7134 completed (loss: 0.04660521447658539, acc: 0.9865030646324158)
[2025-02-13 03:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:17][root][INFO] - Training Epoch: 2/2, step 954/7134 completed (loss: 0.05086999014019966, acc: 0.9897210001945496)
[2025-02-13 03:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:17][root][INFO] - Training Epoch: 2/2, step 955/7134 completed (loss: 0.02116084098815918, acc: 0.993306577205658)
[2025-02-13 03:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18][root][INFO] - Training Epoch: 2/2, step 956/7134 completed (loss: 0.01433472242206335, acc: 0.995121955871582)
[2025-02-13 03:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18][root][INFO] - Training Epoch: 2/2, step 957/7134 completed (loss: 0.02074052393436432, acc: 0.9931740760803223)
[2025-02-13 03:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18][root][INFO] - Training Epoch: 2/2, step 958/7134 completed (loss: 0.017651649191975594, acc: 0.9944853186607361)
[2025-02-13 03:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:19][root][INFO] - Training Epoch: 2/2, step 959/7134 completed (loss: 0.05612911283969879, acc: 0.9799465537071228)
[2025-02-13 03:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:19][root][INFO] - Training Epoch: 2/2, step 960/7134 completed (loss: 0.03527532517910004, acc: 0.9886578321456909)
[2025-02-13 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:20][root][INFO] - Training Epoch: 2/2, step 961/7134 completed (loss: 0.030705301091074944, acc: 0.9946091771125793)
[2025-02-13 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:20][root][INFO] - Training Epoch: 2/2, step 962/7134 completed (loss: 0.07635466754436493, acc: 0.9848275780677795)
[2025-02-13 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21][root][INFO] - Training Epoch: 2/2, step 963/7134 completed (loss: 0.04276714846491814, acc: 0.980182945728302)
[2025-02-13 03:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21][root][INFO] - Training Epoch: 2/2, step 964/7134 completed (loss: 0.10065358132123947, acc: 0.9790105223655701)
[2025-02-13 03:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22][root][INFO] - Training Epoch: 2/2, step 965/7134 completed (loss: 0.053027547895908356, acc: 0.9856528043746948)
[2025-02-13 03:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22][root][INFO] - Training Epoch: 2/2, step 966/7134 completed (loss: 0.03329949453473091, acc: 0.9950819611549377)
[2025-02-13 03:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22][root][INFO] - Training Epoch: 2/2, step 967/7134 completed (loss: 0.030290640890598297, acc: 0.9904502034187317)
[2025-02-13 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:23][root][INFO] - Training Epoch: 2/2, step 968/7134 completed (loss: 0.06820312142372131, acc: 0.9801192879676819)
[2025-02-13 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:23][root][INFO] - Training Epoch: 2/2, step 969/7134 completed (loss: 0.03451652452349663, acc: 0.9849170446395874)
[2025-02-13 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:24][root][INFO] - Training Epoch: 2/2, step 970/7134 completed (loss: 0.042785365134477615, acc: 0.982824444770813)
[2025-02-13 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:24][root][INFO] - Training Epoch: 2/2, step 971/7134 completed (loss: 0.02907809242606163, acc: 0.9901840686798096)
[2025-02-13 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25][root][INFO] - Training Epoch: 2/2, step 972/7134 completed (loss: 0.03809874504804611, acc: 0.9887920022010803)
[2025-02-13 03:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25][root][INFO] - Training Epoch: 2/2, step 973/7134 completed (loss: 0.1050981730222702, acc: 0.9730769395828247)
[2025-02-13 03:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:26][root][INFO] - Training Epoch: 2/2, step 974/7134 completed (loss: 0.04543301835656166, acc: 0.986316978931427)
[2025-02-13 03:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:26][root][INFO] - Training Epoch: 2/2, step 975/7134 completed (loss: 0.05600665882229805, acc: 0.9868420958518982)
[2025-02-13 03:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27][root][INFO] - Training Epoch: 2/2, step 976/7134 completed (loss: 0.049404580146074295, acc: 0.9875389337539673)
[2025-02-13 03:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27][root][INFO] - Training Epoch: 2/2, step 977/7134 completed (loss: 0.047716353088617325, acc: 0.9886363744735718)
[2025-02-13 03:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27][root][INFO] - Training Epoch: 2/2, step 978/7134 completed (loss: 0.021671216934919357, acc: 0.9951865077018738)
[2025-02-13 03:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28][root][INFO] - Training Epoch: 2/2, step 979/7134 completed (loss: 0.023224981501698494, acc: 0.9933155179023743)
[2025-02-13 03:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28][root][INFO] - Training Epoch: 2/2, step 980/7134 completed (loss: 0.049522921442985535, acc: 0.9900000095367432)
[2025-02-13 03:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:29][root][INFO] - Training Epoch: 2/2, step 981/7134 completed (loss: 0.10232365131378174, acc: 0.9807074069976807)
[2025-02-13 03:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:29][root][INFO] - Training Epoch: 2/2, step 982/7134 completed (loss: 0.014442076906561852, acc: 0.9971910119056702)
[2025-02-13 03:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30][root][INFO] - Training Epoch: 2/2, step 983/7134 completed (loss: 0.040936365723609924, acc: 0.9880136847496033)
[2025-02-13 03:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30][root][INFO] - Training Epoch: 2/2, step 984/7134 completed (loss: 0.03909481316804886, acc: 0.9944853186607361)
[2025-02-13 03:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:31][root][INFO] - Training Epoch: 2/2, step 985/7134 completed (loss: 0.03143572434782982, acc: 0.9910714030265808)
[2025-02-13 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:31][root][INFO] - Training Epoch: 2/2, step 986/7134 completed (loss: 0.01859729178249836, acc: 0.9929178357124329)
[2025-02-13 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:32][root][INFO] - Training Epoch: 2/2, step 987/7134 completed (loss: 0.02615414373576641, acc: 0.9967845678329468)
[2025-02-13 03:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:32][root][INFO] - Training Epoch: 2/2, step 988/7134 completed (loss: 0.07441286742687225, acc: 0.9805825352668762)
[2025-02-13 03:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:32][root][INFO] - Training Epoch: 2/2, step 989/7134 completed (loss: 0.03360036015510559, acc: 0.9902642369270325)
[2025-02-13 03:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33][root][INFO] - Training Epoch: 2/2, step 990/7134 completed (loss: 0.04109599068760872, acc: 0.9873577952384949)
[2025-02-13 03:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33][root][INFO] - Training Epoch: 2/2, step 991/7134 completed (loss: 0.057654816657304764, acc: 0.9796556830406189)
[2025-02-13 03:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:34][root][INFO] - Training Epoch: 2/2, step 992/7134 completed (loss: 0.03685774281620979, acc: 0.9907894730567932)
[2025-02-13 03:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:34][root][INFO] - Training Epoch: 2/2, step 993/7134 completed (loss: 0.03478122875094414, acc: 0.9852700233459473)
[2025-02-13 03:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35][root][INFO] - Training Epoch: 2/2, step 994/7134 completed (loss: 0.039190623909235, acc: 0.9909909963607788)
[2025-02-13 03:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35][root][INFO] - Training Epoch: 2/2, step 995/7134 completed (loss: 0.031988561153411865, acc: 0.9891975522041321)
[2025-02-13 03:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:36][root][INFO] - Training Epoch: 2/2, step 996/7134 completed (loss: 0.030237121507525444, acc: 0.9902234673500061)
[2025-02-13 03:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:36][root][INFO] - Training Epoch: 2/2, step 997/7134 completed (loss: 0.02039649523794651, acc: 0.9942693114280701)
[2025-02-13 03:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:37][root][INFO] - Training Epoch: 2/2, step 998/7134 completed (loss: 0.025619562715291977, acc: 0.9957746267318726)
[2025-02-13 03:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:37][root][INFO] - Training Epoch: 2/2, step 999/7134 completed (loss: 0.013938295654952526, acc: 0.9946236610412598)
[2025-02-13 03:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:37][root][INFO] - Training Epoch: 2/2, step 1000/7134 completed (loss: 0.014206407591700554, acc: 0.9965753555297852)
[2025-02-13 03:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38][root][INFO] - Training Epoch: 2/2, step 1001/7134 completed (loss: 0.0286449883133173, acc: 0.9939393997192383)
[2025-02-13 03:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38][root][INFO] - Training Epoch: 2/2, step 1002/7134 completed (loss: 0.02366008050739765, acc: 0.9928673505783081)
[2025-02-13 03:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:39][root][INFO] - Training Epoch: 2/2, step 1003/7134 completed (loss: 0.013425825163722038, acc: 0.9955423474311829)
[2025-02-13 03:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:39][root][INFO] - Training Epoch: 2/2, step 1004/7134 completed (loss: 0.03339054435491562, acc: 0.991752564907074)
[2025-02-13 03:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40][root][INFO] - Training Epoch: 2/2, step 1005/7134 completed (loss: 0.06101726368069649, acc: 0.9817880988121033)
[2025-02-13 03:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40][root][INFO] - Training Epoch: 2/2, step 1006/7134 completed (loss: 0.027768174186348915, acc: 0.9944289922714233)
[2025-02-13 03:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40][root][INFO] - Training Epoch: 2/2, step 1007/7134 completed (loss: 0.059924833476543427, acc: 0.9874411225318909)
[2025-02-13 03:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:41][root][INFO] - Training Epoch: 2/2, step 1008/7134 completed (loss: 0.029768941923975945, acc: 0.9914893507957458)
[2025-02-13 03:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:41][root][INFO] - Training Epoch: 2/2, step 1009/7134 completed (loss: 0.05026448518037796, acc: 0.9928951859474182)
[2025-02-13 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42][root][INFO] - Training Epoch: 2/2, step 1010/7134 completed (loss: 0.032009556889534, acc: 0.9901185631752014)
[2025-02-13 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42][root][INFO] - Training Epoch: 2/2, step 1011/7134 completed (loss: 0.023845987394452095, acc: 0.9917012453079224)
[2025-02-13 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43][root][INFO] - Training Epoch: 2/2, step 1012/7134 completed (loss: 0.03501354530453682, acc: 0.9929178357124329)
[2025-02-13 03:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43][root][INFO] - Training Epoch: 2/2, step 1013/7134 completed (loss: 0.05238110199570656, acc: 0.984240710735321)
[2025-02-13 03:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:44][root][INFO] - Training Epoch: 2/2, step 1014/7134 completed (loss: 0.019747627899050713, acc: 0.9950739145278931)
[2025-02-13 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:44][root][INFO] - Training Epoch: 2/2, step 1015/7134 completed (loss: 0.03853224217891693, acc: 0.9894578456878662)
[2025-02-13 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:44][root][INFO] - Training Epoch: 2/2, step 1016/7134 completed (loss: 0.025677302852272987, acc: 0.990439772605896)
[2025-02-13 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45][root][INFO] - Training Epoch: 2/2, step 1017/7134 completed (loss: 0.0056753274984657764, acc: 0.9986577033996582)
[2025-02-13 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45][root][INFO] - Training Epoch: 2/2, step 1018/7134 completed (loss: 0.03604356199502945, acc: 0.9926793575286865)
[2025-02-13 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:46][root][INFO] - Training Epoch: 2/2, step 1019/7134 completed (loss: 0.012777822092175484, acc: 0.995192289352417)
[2025-02-13 03:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:46][root][INFO] - Training Epoch: 2/2, step 1020/7134 completed (loss: 0.00863261241465807, acc: 0.9985074400901794)
[2025-02-13 03:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:47][root][INFO] - Training Epoch: 2/2, step 1021/7134 completed (loss: 0.021656468510627747, acc: 0.9968152642250061)
[2025-02-13 03:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:47][root][INFO] - Training Epoch: 2/2, step 1022/7134 completed (loss: 0.015213662758469582, acc: 0.9947299361228943)
[2025-02-13 03:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48][root][INFO] - Training Epoch: 2/2, step 1023/7134 completed (loss: 0.03353190794587135, acc: 0.992277979850769)
[2025-02-13 03:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48][root][INFO] - Training Epoch: 2/2, step 1024/7134 completed (loss: 0.031308364123106, acc: 0.9928186535835266)
[2025-02-13 03:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48][root][INFO] - Training Epoch: 2/2, step 1025/7134 completed (loss: 0.011003185994923115, acc: 0.9964413046836853)
[2025-02-13 03:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:49][root][INFO] - Training Epoch: 2/2, step 1026/7134 completed (loss: 0.029976285994052887, acc: 0.9936842322349548)
[2025-02-13 03:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:49][root][INFO] - Training Epoch: 2/2, step 1027/7134 completed (loss: 0.009049843065440655, acc: 0.9986467957496643)
[2025-02-13 03:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:50][root][INFO] - Training Epoch: 2/2, step 1028/7134 completed (loss: 0.015915952622890472, acc: 0.9955157041549683)
[2025-02-13 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:50][root][INFO] - Training Epoch: 2/2, step 1029/7134 completed (loss: 0.005162353627383709, acc: 0.9985141158103943)
[2025-02-13 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:51][root][INFO] - Training Epoch: 2/2, step 1030/7134 completed (loss: 0.009312089532613754, acc: 0.9973045587539673)
[2025-02-13 03:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:51][root][INFO] - Training Epoch: 2/2, step 1031/7134 completed (loss: 0.026505429297685623, acc: 0.9923780560493469)
[2025-02-13 03:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52][root][INFO] - Training Epoch: 2/2, step 1032/7134 completed (loss: 0.01740599423646927, acc: 0.9965437650680542)
[2025-02-13 03:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52][root][INFO] - Training Epoch: 2/2, step 1033/7134 completed (loss: 0.02605896070599556, acc: 0.9899569749832153)
[2025-02-13 03:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52][root][INFO] - Training Epoch: 2/2, step 1034/7134 completed (loss: 0.03590447083115578, acc: 0.9886547923088074)
[2025-02-13 03:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53][root][INFO] - Training Epoch: 2/2, step 1035/7134 completed (loss: 0.12563185393810272, acc: 0.9666666388511658)
[2025-02-13 03:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53][root][INFO] - Training Epoch: 2/2, step 1036/7134 completed (loss: 0.13170616328716278, acc: 0.9688888788223267)
[2025-02-13 03:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:54][root][INFO] - Training Epoch: 2/2, step 1037/7134 completed (loss: 0.012633374892175198, acc: 0.9988095164299011)
[2025-02-13 03:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:54][root][INFO] - Training Epoch: 2/2, step 1038/7134 completed (loss: 0.007681564893573523, acc: 0.9983221292495728)
[2025-02-13 03:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:55][root][INFO] - Training Epoch: 2/2, step 1039/7134 completed (loss: 0.0181816928088665, acc: 0.9958592057228088)
[2025-02-13 03:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:55][root][INFO] - Training Epoch: 2/2, step 1040/7134 completed (loss: 0.010754786431789398, acc: 0.9982876777648926)
[2025-02-13 03:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:55][root][INFO] - Training Epoch: 2/2, step 1041/7134 completed (loss: 0.015398397110402584, acc: 0.9937694668769836)
[2025-02-13 03:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:56][root][INFO] - Training Epoch: 2/2, step 1042/7134 completed (loss: 0.02184908278286457, acc: 0.9948717951774597)
[2025-02-13 03:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:56][root][INFO] - Training Epoch: 2/2, step 1043/7134 completed (loss: 0.014010900631546974, acc: 0.9951865077018738)
[2025-02-13 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57][root][INFO] - Training Epoch: 2/2, step 1044/7134 completed (loss: 0.030888719484210014, acc: 0.9945205450057983)
[2025-02-13 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57][root][INFO] - Training Epoch: 2/2, step 1045/7134 completed (loss: 0.07338270545005798, acc: 0.9773828983306885)
[2025-02-13 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:58][root][INFO] - Training Epoch: 2/2, step 1046/7134 completed (loss: 0.025802405551075935, acc: 0.9942085146903992)
[2025-02-13 03:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:58][root][INFO] - Training Epoch: 2/2, step 1047/7134 completed (loss: 0.04846832901239395, acc: 0.9807407259941101)
[2025-02-13 03:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59][root][INFO] - Training Epoch: 2/2, step 1048/7134 completed (loss: 0.011860792525112629, acc: 0.9956011772155762)
[2025-02-13 03:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59][root][INFO] - Training Epoch: 2/2, step 1049/7134 completed (loss: 0.036735646426677704, acc: 0.9868228435516357)
[2025-02-13 03:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:00][root][INFO] - Training Epoch: 2/2, step 1050/7134 completed (loss: 0.025153160095214844, acc: 0.9877049326896667)
[2025-02-13 03:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:00][root][INFO] - Training Epoch: 2/2, step 1051/7134 completed (loss: 0.019832337275147438, acc: 0.9931507110595703)
[2025-02-13 03:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:01][root][INFO] - Training Epoch: 2/2, step 1052/7134 completed (loss: 0.04098837077617645, acc: 0.9854604005813599)
[2025-02-13 03:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:01][root][INFO] - Training Epoch: 2/2, step 1053/7134 completed (loss: 0.013881589286029339, acc: 0.9943661689758301)
[2025-02-13 03:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02][root][INFO] - Training Epoch: 2/2, step 1054/7134 completed (loss: 0.01930925063788891, acc: 0.9947019815444946)
[2025-02-13 03:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02][root][INFO] - Training Epoch: 2/2, step 1055/7134 completed (loss: 0.05338403210043907, acc: 0.9756097793579102)
[2025-02-13 03:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02][root][INFO] - Training Epoch: 2/2, step 1056/7134 completed (loss: 0.049521490931510925, acc: 0.9882746934890747)
[2025-02-13 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:03][root][INFO] - Training Epoch: 2/2, step 1057/7134 completed (loss: 0.029738007113337517, acc: 0.9898843765258789)
[2025-02-13 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:03][root][INFO] - Training Epoch: 2/2, step 1058/7134 completed (loss: 0.021410927176475525, acc: 0.9956521987915039)
[2025-02-13 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04][root][INFO] - Training Epoch: 2/2, step 1059/7134 completed (loss: 0.026118841022253036, acc: 0.9922580718994141)
[2025-02-13 03:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04][root][INFO] - Training Epoch: 2/2, step 1060/7134 completed (loss: 0.022791527211666107, acc: 0.9952681660652161)
[2025-02-13 03:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:05][root][INFO] - Training Epoch: 2/2, step 1061/7134 completed (loss: 0.020623328164219856, acc: 0.9956395626068115)
[2025-02-13 03:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:05][root][INFO] - Training Epoch: 2/2, step 1062/7134 completed (loss: 0.030449621379375458, acc: 0.9864406585693359)
[2025-02-13 03:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:06][root][INFO] - Training Epoch: 2/2, step 1063/7134 completed (loss: 0.0084517952054739, acc: 0.9983792304992676)
[2025-02-13 03:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:06][root][INFO] - Training Epoch: 2/2, step 1064/7134 completed (loss: 0.026668574661016464, acc: 0.9938271641731262)
[2025-02-13 03:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:06][root][INFO] - Training Epoch: 2/2, step 1065/7134 completed (loss: 0.014487412758171558, acc: 0.9959349632263184)
[2025-02-13 03:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07][root][INFO] - Training Epoch: 2/2, step 1066/7134 completed (loss: 0.04467720910906792, acc: 0.9815436005592346)
[2025-02-13 03:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07][root][INFO] - Training Epoch: 2/2, step 1067/7134 completed (loss: 0.010402107611298561, acc: 1.0)
[2025-02-13 03:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08][root][INFO] - Training Epoch: 2/2, step 1068/7134 completed (loss: 0.02291978895664215, acc: 0.9924952983856201)
[2025-02-13 03:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08][root][INFO] - Training Epoch: 2/2, step 1069/7134 completed (loss: 0.01964636519551277, acc: 0.9933221936225891)
[2025-02-13 03:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:09][root][INFO] - Training Epoch: 2/2, step 1070/7134 completed (loss: 0.025754183530807495, acc: 0.9934318661689758)
[2025-02-13 03:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:09][root][INFO] - Training Epoch: 2/2, step 1071/7134 completed (loss: 0.008389274589717388, acc: 0.9973226189613342)
[2025-02-13 03:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10][root][INFO] - Training Epoch: 2/2, step 1072/7134 completed (loss: 0.0012462014565244317, acc: 1.0)
[2025-02-13 03:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10][root][INFO] - Training Epoch: 2/2, step 1073/7134 completed (loss: 0.0047565363347530365, acc: 0.9985590577125549)
[2025-02-13 03:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10][root][INFO] - Training Epoch: 2/2, step 1074/7134 completed (loss: 0.03602450713515282, acc: 0.9861830472946167)
[2025-02-13 03:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11][root][INFO] - Training Epoch: 2/2, step 1075/7134 completed (loss: 0.06306686252355576, acc: 0.9834254384040833)
[2025-02-13 03:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11][root][INFO] - Training Epoch: 2/2, step 1076/7134 completed (loss: 0.05682312324643135, acc: 0.9813829660415649)
[2025-02-13 03:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12][root][INFO] - Training Epoch: 2/2, step 1077/7134 completed (loss: 0.036751504987478256, acc: 0.9842342138290405)
[2025-02-13 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12][root][INFO] - Training Epoch: 2/2, step 1078/7134 completed (loss: 0.07633157819509506, acc: 0.9838449358940125)
[2025-02-13 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:13][root][INFO] - Training Epoch: 2/2, step 1079/7134 completed (loss: 0.02196107991039753, acc: 0.9919261932373047)
[2025-02-13 03:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:13][root][INFO] - Training Epoch: 2/2, step 1080/7134 completed (loss: 0.038781359791755676, acc: 0.9943609237670898)
[2025-02-13 03:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14][root][INFO] - Training Epoch: 2/2, step 1081/7134 completed (loss: 0.00782851129770279, acc: 0.9971949458122253)
[2025-02-13 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14][root][INFO] - Training Epoch: 2/2, step 1082/7134 completed (loss: 0.01267333421856165, acc: 0.9969167709350586)
[2025-02-13 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15][root][INFO] - Training Epoch: 2/2, step 1083/7134 completed (loss: 0.009448069147765636, acc: 0.9977400898933411)
[2025-02-13 03:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15][root][INFO] - Training Epoch: 2/2, step 1084/7134 completed (loss: 0.016965657472610474, acc: 0.9938042163848877)
[2025-02-13 03:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15][root][INFO] - Training Epoch: 2/2, step 1085/7134 completed (loss: 0.02276601456105709, acc: 0.9919678568840027)
[2025-02-13 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16][root][INFO] - Training Epoch: 2/2, step 1086/7134 completed (loss: 0.048654068261384964, acc: 0.9829867482185364)
[2025-02-13 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16][root][INFO] - Training Epoch: 2/2, step 1087/7134 completed (loss: 0.02861945703625679, acc: 0.9914634227752686)
[2025-02-13 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17][root][INFO] - Training Epoch: 2/2, step 1088/7134 completed (loss: 0.015996884554624557, acc: 0.9958100318908691)
[2025-02-13 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17][root][INFO] - Training Epoch: 2/2, step 1089/7134 completed (loss: 0.03623553365468979, acc: 0.9910581111907959)
[2025-02-13 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18][root][INFO] - Training Epoch: 2/2, step 1090/7134 completed (loss: 0.027447868138551712, acc: 0.9921996593475342)
[2025-02-13 03:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18][root][INFO] - Training Epoch: 2/2, step 1091/7134 completed (loss: 0.011682669632136822, acc: 0.9984779357910156)
[2025-02-13 03:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19][root][INFO] - Training Epoch: 2/2, step 1092/7134 completed (loss: 0.009872573427855968, acc: 0.9985548853874207)
[2025-02-13 03:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19][root][INFO] - Training Epoch: 2/2, step 1093/7134 completed (loss: 0.05855296552181244, acc: 0.982758641242981)
[2025-02-13 03:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19][root][INFO] - Training Epoch: 2/2, step 1094/7134 completed (loss: 0.041204676032066345, acc: 0.9851301312446594)
[2025-02-13 03:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20][root][INFO] - Training Epoch: 2/2, step 1095/7134 completed (loss: 0.0244916845113039, acc: 0.9926918148994446)
[2025-02-13 03:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20][root][INFO] - Training Epoch: 2/2, step 1096/7134 completed (loss: 0.030559061095118523, acc: 0.9911392331123352)
[2025-02-13 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21][root][INFO] - Training Epoch: 2/2, step 1097/7134 completed (loss: 0.04787759482860565, acc: 0.9845505356788635)
[2025-02-13 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21][root][INFO] - Training Epoch: 2/2, step 1098/7134 completed (loss: 0.011834898963570595, acc: 0.995055615901947)
[2025-02-13 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:22][root][INFO] - Training Epoch: 2/2, step 1099/7134 completed (loss: 0.02224045991897583, acc: 0.9948717951774597)
[2025-02-13 03:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:22][root][INFO] - Training Epoch: 2/2, step 1100/7134 completed (loss: 0.02418968640267849, acc: 0.9932065010070801)
[2025-02-13 03:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23][root][INFO] - Training Epoch: 2/2, step 1101/7134 completed (loss: 0.014039816334843636, acc: 0.9948052167892456)
[2025-02-13 03:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23][root][INFO] - Training Epoch: 2/2, step 1102/7134 completed (loss: 0.03457671403884888, acc: 0.9902152419090271)
[2025-02-13 03:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24][root][INFO] - Training Epoch: 2/2, step 1103/7134 completed (loss: 0.04915078729391098, acc: 0.9879999756813049)
[2025-02-13 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24][root][INFO] - Training Epoch: 2/2, step 1104/7134 completed (loss: 0.030027857050299644, acc: 0.9927954077720642)
[2025-02-13 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25][root][INFO] - Training Epoch: 2/2, step 1105/7134 completed (loss: 0.03077969141304493, acc: 0.9905660152435303)
[2025-02-13 03:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25][root][INFO] - Training Epoch: 2/2, step 1106/7134 completed (loss: 0.024063576012849808, acc: 0.9931129217147827)
[2025-02-13 03:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25][root][INFO] - Training Epoch: 2/2, step 1107/7134 completed (loss: 0.02087564207613468, acc: 0.991793692111969)
[2025-02-13 03:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26][root][INFO] - Training Epoch: 2/2, step 1108/7134 completed (loss: 0.0158680509775877, acc: 0.9961340427398682)
[2025-02-13 03:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26][root][INFO] - Training Epoch: 2/2, step 1109/7134 completed (loss: 0.024373171851038933, acc: 0.9888268113136292)
[2025-02-13 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27][root][INFO] - Training Epoch: 2/2, step 1110/7134 completed (loss: 0.024366434663534164, acc: 0.9921362996101379)
[2025-02-13 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27][root][INFO] - Training Epoch: 2/2, step 1111/7134 completed (loss: 0.03344344347715378, acc: 0.9917126893997192)
[2025-02-13 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28][root][INFO] - Training Epoch: 2/2, step 1112/7134 completed (loss: 0.02649940364062786, acc: 0.9955882430076599)
[2025-02-13 03:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28][root][INFO] - Training Epoch: 2/2, step 1113/7134 completed (loss: 0.01434426661580801, acc: 0.9948520064353943)
[2025-02-13 03:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29][root][INFO] - Training Epoch: 2/2, step 1114/7134 completed (loss: 0.033226653933525085, acc: 0.9902794361114502)
[2025-02-13 03:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29][root][INFO] - Training Epoch: 2/2, step 1115/7134 completed (loss: 0.022257348522543907, acc: 0.9952830076217651)
[2025-02-13 03:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30][root][INFO] - Training Epoch: 2/2, step 1116/7134 completed (loss: 0.007885082624852657, acc: 0.9981343150138855)
[2025-02-13 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30][root][INFO] - Training Epoch: 2/2, step 1117/7134 completed (loss: 0.010887965559959412, acc: 0.9969325065612793)
[2025-02-13 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30][root][INFO] - Training Epoch: 2/2, step 1118/7134 completed (loss: 0.002597966929897666, acc: 1.0)
[2025-02-13 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31][root][INFO] - Training Epoch: 2/2, step 1119/7134 completed (loss: 0.018591970205307007, acc: 0.9936808943748474)
[2025-02-13 03:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31][root][INFO] - Training Epoch: 2/2, step 1120/7134 completed (loss: 0.025960898026823997, acc: 0.9905660152435303)
[2025-02-13 03:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32][root][INFO] - Training Epoch: 2/2, step 1121/7134 completed (loss: 0.021226869896054268, acc: 0.9926144480705261)
[2025-02-13 03:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32][root][INFO] - Training Epoch: 2/2, step 1122/7134 completed (loss: 0.031178539618849754, acc: 0.9902912378311157)
[2025-02-13 03:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32][root][INFO] - Training Epoch: 2/2, step 1123/7134 completed (loss: 0.014641229063272476, acc: 0.995184600353241)
[2025-02-13 03:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33][root][INFO] - Training Epoch: 2/2, step 1124/7134 completed (loss: 0.01734096184372902, acc: 0.9922600388526917)
[2025-02-13 03:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33][root][INFO] - Training Epoch: 2/2, step 1125/7134 completed (loss: 0.011269401758909225, acc: 0.9945873022079468)
[2025-02-13 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34][root][INFO] - Training Epoch: 2/2, step 1126/7134 completed (loss: 0.02615317516028881, acc: 0.9930939078330994)
[2025-02-13 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34][root][INFO] - Training Epoch: 2/2, step 1127/7134 completed (loss: 0.0186750665307045, acc: 0.995199978351593)
[2025-02-13 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35][root][INFO] - Training Epoch: 2/2, step 1128/7134 completed (loss: 0.02091021090745926, acc: 0.9947916865348816)
[2025-02-13 03:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35][root][INFO] - Training Epoch: 2/2, step 1129/7134 completed (loss: 0.028570357710123062, acc: 0.9910314083099365)
[2025-02-13 03:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36][root][INFO] - Training Epoch: 2/2, step 1130/7134 completed (loss: 0.004727206192910671, acc: 0.9983844757080078)
[2025-02-13 03:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36][root][INFO] - Training Epoch: 2/2, step 1131/7134 completed (loss: 0.03832145035266876, acc: 0.9917452931404114)
[2025-02-13 03:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36][root][INFO] - Training Epoch: 2/2, step 1132/7134 completed (loss: 0.06856069713830948, acc: 0.9831546545028687)
[2025-02-13 03:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37][root][INFO] - Training Epoch: 2/2, step 1133/7134 completed (loss: 0.07840568572282791, acc: 0.980028510093689)
[2025-02-13 03:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37][root][INFO] - Training Epoch: 2/2, step 1134/7134 completed (loss: 0.04882495850324631, acc: 0.9850373864173889)
[2025-02-13 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38][root][INFO] - Training Epoch: 2/2, step 1135/7134 completed (loss: 0.04562044516205788, acc: 0.9824000000953674)
[2025-02-13 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38][root][INFO] - Training Epoch: 2/2, step 1136/7134 completed (loss: 0.04337393864989281, acc: 0.9877551198005676)
[2025-02-13 03:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39][root][INFO] - Training Epoch: 2/2, step 1137/7134 completed (loss: 0.0297929048538208, acc: 0.9931818246841431)
[2025-02-13 03:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39][root][INFO] - Training Epoch: 2/2, step 1138/7134 completed (loss: 0.0277108084410429, acc: 0.9931694269180298)
[2025-02-13 03:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40][root][INFO] - Training Epoch: 2/2, step 1139/7134 completed (loss: 0.07148602604866028, acc: 0.9763912558555603)
[2025-02-13 03:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40][root][INFO] - Training Epoch: 2/2, step 1140/7134 completed (loss: 0.021557359024882317, acc: 0.9974937438964844)
[2025-02-13 03:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41][root][INFO] - Training Epoch: 2/2, step 1141/7134 completed (loss: 0.011150240898132324, acc: 0.9976580739021301)
[2025-02-13 03:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41][root][INFO] - Training Epoch: 2/2, step 1142/7134 completed (loss: 0.0429677776992321, acc: 0.9921875)
[2025-02-13 03:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41][root][INFO] - Training Epoch: 2/2, step 1143/7134 completed (loss: 0.06370147317647934, acc: 0.977746844291687)
[2025-02-13 03:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42][root][INFO] - Training Epoch: 2/2, step 1144/7134 completed (loss: 0.05162220448255539, acc: 0.9862499833106995)
[2025-02-13 03:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42][root][INFO] - Training Epoch: 2/2, step 1145/7134 completed (loss: 0.020956721156835556, acc: 0.9976019263267517)
[2025-02-13 03:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43][root][INFO] - Training Epoch: 2/2, step 1146/7134 completed (loss: 0.01785207912325859, acc: 0.9945651888847351)
[2025-02-13 03:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43][root][INFO] - Training Epoch: 2/2, step 1147/7134 completed (loss: 0.013389105908572674, acc: 0.996259331703186)
[2025-02-13 03:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:44][root][INFO] - Training Epoch: 2/2, step 1148/7134 completed (loss: 0.038386255502700806, acc: 0.9897611141204834)
[2025-02-13 03:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:44][root][INFO] - Training Epoch: 2/2, step 1149/7134 completed (loss: 0.0495733879506588, acc: 0.9824175834655762)
[2025-02-13 03:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45][root][INFO] - Training Epoch: 2/2, step 1150/7134 completed (loss: 0.019573470577597618, acc: 0.9968992471694946)
[2025-02-13 03:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45][root][INFO] - Training Epoch: 2/2, step 1151/7134 completed (loss: 0.008085368201136589, acc: 0.9979715943336487)
[2025-02-13 03:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46][root][INFO] - Training Epoch: 2/2, step 1152/7134 completed (loss: 0.02255888655781746, acc: 0.988034188747406)
[2025-02-13 03:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46][root][INFO] - Training Epoch: 2/2, step 1153/7134 completed (loss: 0.05312272533774376, acc: 0.9891008138656616)
[2025-02-13 03:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46][root][INFO] - Training Epoch: 2/2, step 1154/7134 completed (loss: 0.02936355397105217, acc: 0.991150438785553)
[2025-02-13 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47][root][INFO] - Training Epoch: 2/2, step 1155/7134 completed (loss: 0.042680591344833374, acc: 0.9780701994895935)
[2025-02-13 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47][root][INFO] - Training Epoch: 2/2, step 1156/7134 completed (loss: 0.006657574791461229, acc: 0.9984639286994934)
[2025-02-13 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:48][root][INFO] - Training Epoch: 2/2, step 1157/7134 completed (loss: 0.04106724634766579, acc: 0.9915110468864441)
[2025-02-13 03:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:48][root][INFO] - Training Epoch: 2/2, step 1158/7134 completed (loss: 0.037032365798950195, acc: 0.9822485446929932)
[2025-02-13 03:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49][root][INFO] - Training Epoch: 2/2, step 1159/7134 completed (loss: 0.0418224036693573, acc: 0.9858155846595764)
[2025-02-13 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49][root][INFO] - Training Epoch: 2/2, step 1160/7134 completed (loss: 0.01513970922678709, acc: 0.9955357313156128)
[2025-02-13 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50][root][INFO] - Training Epoch: 2/2, step 1161/7134 completed (loss: 0.06858190149068832, acc: 0.985318124294281)
[2025-02-13 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50][root][INFO] - Training Epoch: 2/2, step 1162/7134 completed (loss: 0.027906494215130806, acc: 0.9867768883705139)
[2025-02-13 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50][root][INFO] - Training Epoch: 2/2, step 1163/7134 completed (loss: 0.03462368622422218, acc: 0.9914675951004028)
[2025-02-13 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:51][root][INFO] - Training Epoch: 2/2, step 1164/7134 completed (loss: 0.039332594722509384, acc: 0.9930459260940552)
[2025-02-13 03:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:51][root][INFO] - Training Epoch: 2/2, step 1165/7134 completed (loss: 0.014174646697938442, acc: 0.9945725798606873)
[2025-02-13 03:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52][root][INFO] - Training Epoch: 2/2, step 1166/7134 completed (loss: 0.015866057947278023, acc: 0.9958847761154175)
[2025-02-13 03:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52][root][INFO] - Training Epoch: 2/2, step 1167/7134 completed (loss: 0.010679596103727818, acc: 0.9960707426071167)
[2025-02-13 03:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53][root][INFO] - Training Epoch: 2/2, step 1168/7134 completed (loss: 0.009548837319016457, acc: 0.9986842274665833)
[2025-02-13 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53][root][INFO] - Training Epoch: 2/2, step 1169/7134 completed (loss: 0.030660439282655716, acc: 0.9864864945411682)
[2025-02-13 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54][root][INFO] - Training Epoch: 2/2, step 1170/7134 completed (loss: 0.023992447182536125, acc: 0.9957864880561829)
[2025-02-13 03:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54][root][INFO] - Training Epoch: 2/2, step 1171/7134 completed (loss: 0.02196279726922512, acc: 0.9933221936225891)
[2025-02-13 03:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54][root][INFO] - Training Epoch: 2/2, step 1172/7134 completed (loss: 0.02541707456111908, acc: 0.9894099831581116)
[2025-02-13 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55][root][INFO] - Training Epoch: 2/2, step 1173/7134 completed (loss: 0.03104560077190399, acc: 0.9927536249160767)
[2025-02-13 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55][root][INFO] - Training Epoch: 2/2, step 1174/7134 completed (loss: 0.014563610777258873, acc: 0.9969969987869263)
[2025-02-13 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:56][root][INFO] - Training Epoch: 2/2, step 1175/7134 completed (loss: 0.014926421456038952, acc: 0.9956521987915039)
[2025-02-13 03:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:56][root][INFO] - Training Epoch: 2/2, step 1176/7134 completed (loss: 0.013086470775306225, acc: 0.9965753555297852)
[2025-02-13 03:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57][root][INFO] - Training Epoch: 2/2, step 1177/7134 completed (loss: 0.015579805709421635, acc: 0.9941176176071167)
[2025-02-13 03:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57][root][INFO] - Training Epoch: 2/2, step 1178/7134 completed (loss: 0.0180177241563797, acc: 0.9953271150588989)
[2025-02-13 03:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57][root][INFO] - Training Epoch: 2/2, step 1179/7134 completed (loss: 0.010901770554482937, acc: 0.9961758852005005)
[2025-02-13 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58][root][INFO] - Training Epoch: 2/2, step 1180/7134 completed (loss: 0.00814042054116726, acc: 0.9963503479957581)
[2025-02-13 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58][root][INFO] - Training Epoch: 2/2, step 1181/7134 completed (loss: 0.015825826674699783, acc: 0.9942445755004883)
[2025-02-13 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59][root][INFO] - Training Epoch: 2/2, step 1182/7134 completed (loss: 0.004528380464762449, acc: 1.0)
[2025-02-13 03:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59][root][INFO] - Training Epoch: 2/2, step 1183/7134 completed (loss: 0.005876235663890839, acc: 0.9985315799713135)
[2025-02-13 03:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00][root][INFO] - Training Epoch: 2/2, step 1184/7134 completed (loss: 0.043835967779159546, acc: 0.9928469061851501)
[2025-02-13 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00][root][INFO] - Training Epoch: 2/2, step 1185/7134 completed (loss: 0.005489588249474764, acc: 0.9984962344169617)
[2025-02-13 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01][root][INFO] - Training Epoch: 2/2, step 1186/7134 completed (loss: 0.016853787004947662, acc: 0.9987030029296875)
[2025-02-13 03:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01][root][INFO] - Training Epoch: 2/2, step 1187/7134 completed (loss: 0.010850337333977222, acc: 0.9944598078727722)
[2025-02-13 03:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01][root][INFO] - Training Epoch: 2/2, step 1188/7134 completed (loss: 0.003335893852636218, acc: 1.0)
[2025-02-13 03:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02][root][INFO] - Training Epoch: 2/2, step 1189/7134 completed (loss: 0.014074871316552162, acc: 0.9971098303794861)
[2025-02-13 03:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02][root][INFO] - Training Epoch: 2/2, step 1190/7134 completed (loss: 0.009133664891123772, acc: 0.9974554777145386)
[2025-02-13 03:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03][root][INFO] - Training Epoch: 2/2, step 1191/7134 completed (loss: 0.02982509694993496, acc: 0.9940915703773499)
[2025-02-13 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03][root][INFO] - Training Epoch: 2/2, step 1192/7134 completed (loss: 0.005599703639745712, acc: 1.0)
[2025-02-13 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04][root][INFO] - Training Epoch: 2/2, step 1193/7134 completed (loss: 0.027083715423941612, acc: 0.9952152967453003)
[2025-02-13 03:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04][root][INFO] - Training Epoch: 2/2, step 1194/7134 completed (loss: 0.03830728307366371, acc: 0.9917582273483276)
[2025-02-13 03:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05][root][INFO] - Training Epoch: 2/2, step 1195/7134 completed (loss: 0.035541221499443054, acc: 0.9885621070861816)
[2025-02-13 03:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05][root][INFO] - Training Epoch: 2/2, step 1196/7134 completed (loss: 0.043220993131399155, acc: 0.9901639223098755)
[2025-02-13 03:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05][root][INFO] - Training Epoch: 2/2, step 1197/7134 completed (loss: 0.028106605634093285, acc: 0.9882698059082031)
[2025-02-13 03:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:06][root][INFO] - Training Epoch: 2/2, step 1198/7134 completed (loss: 0.02046787738800049, acc: 0.9928057789802551)
[2025-02-13 03:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:06][root][INFO] - Training Epoch: 2/2, step 1199/7134 completed (loss: 0.01873989775776863, acc: 0.9955817461013794)
[2025-02-13 03:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07][root][INFO] - Training Epoch: 2/2, step 1200/7134 completed (loss: 0.0065491292625665665, acc: 0.9958932399749756)
[2025-02-13 03:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07][root][INFO] - Training Epoch: 2/2, step 1201/7134 completed (loss: 0.01956966333091259, acc: 0.9911634922027588)
[2025-02-13 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08][root][INFO] - Training Epoch: 2/2, step 1202/7134 completed (loss: 0.028135837987065315, acc: 0.9905787110328674)
[2025-02-13 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08][root][INFO] - Training Epoch: 2/2, step 1203/7134 completed (loss: 0.005064144264906645, acc: 0.9984825253486633)
[2025-02-13 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09][root][INFO] - Training Epoch: 2/2, step 1204/7134 completed (loss: 0.028703782707452774, acc: 0.9888059496879578)
[2025-02-13 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09][root][INFO] - Training Epoch: 2/2, step 1205/7134 completed (loss: 0.07796993851661682, acc: 0.9816901683807373)
[2025-02-13 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10][root][INFO] - Training Epoch: 2/2, step 1206/7134 completed (loss: 0.023894933983683586, acc: 0.9888337254524231)
[2025-02-13 03:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10][root][INFO] - Training Epoch: 2/2, step 1207/7134 completed (loss: 0.026555538177490234, acc: 0.9924623370170593)
[2025-02-13 03:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11][root][INFO] - Training Epoch: 2/2, step 1208/7134 completed (loss: 0.03801850974559784, acc: 0.986940324306488)
[2025-02-13 03:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11][root][INFO] - Training Epoch: 2/2, step 1209/7134 completed (loss: 0.08210254460573196, acc: 0.9780621528625488)
[2025-02-13 03:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11][root][INFO] - Training Epoch: 2/2, step 1210/7134 completed (loss: 0.05471602827310562, acc: 0.980861246585846)
[2025-02-13 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12][root][INFO] - Training Epoch: 2/2, step 1211/7134 completed (loss: 0.07860765606164932, acc: 0.978805422782898)
[2025-02-13 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12][root][INFO] - Training Epoch: 2/2, step 1212/7134 completed (loss: 0.08075512200593948, acc: 0.9786585569381714)
[2025-02-13 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13][root][INFO] - Training Epoch: 2/2, step 1213/7134 completed (loss: 0.026185620576143265, acc: 0.9915134310722351)
[2025-02-13 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13][root][INFO] - Training Epoch: 2/2, step 1214/7134 completed (loss: 0.0507015623152256, acc: 0.9861538410186768)
[2025-02-13 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14][root][INFO] - Training Epoch: 2/2, step 1215/7134 completed (loss: 0.04985789209604263, acc: 0.9834482669830322)
[2025-02-13 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14][root][INFO] - Training Epoch: 2/2, step 1216/7134 completed (loss: 0.018572762608528137, acc: 0.9985401630401611)
[2025-02-13 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15][root][INFO] - Training Epoch: 2/2, step 1217/7134 completed (loss: 0.03593997284770012, acc: 0.9902597665786743)
[2025-02-13 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15][root][INFO] - Training Epoch: 2/2, step 1218/7134 completed (loss: 0.07867010682821274, acc: 0.9820895791053772)
[2025-02-13 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16][root][INFO] - Training Epoch: 2/2, step 1219/7134 completed (loss: 0.06353261321783066, acc: 0.9859402179718018)
[2025-02-13 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16][root][INFO] - Training Epoch: 2/2, step 1220/7134 completed (loss: 0.06657350808382034, acc: 0.9873096346855164)
[2025-02-13 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17][root][INFO] - Training Epoch: 2/2, step 1221/7134 completed (loss: 0.06533968448638916, acc: 0.9835526347160339)
[2025-02-13 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17][root][INFO] - Training Epoch: 2/2, step 1222/7134 completed (loss: 0.04994684457778931, acc: 0.9868891835212708)
[2025-02-13 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17][root][INFO] - Training Epoch: 2/2, step 1223/7134 completed (loss: 0.12564291059970856, acc: 0.973936915397644)
[2025-02-13 03:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18][root][INFO] - Training Epoch: 2/2, step 1224/7134 completed (loss: 0.04682748392224312, acc: 0.9911699891090393)
[2025-02-13 03:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18][root][INFO] - Training Epoch: 2/2, step 1225/7134 completed (loss: 0.04599612578749657, acc: 0.9886492490768433)
[2025-02-13 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19][root][INFO] - Training Epoch: 2/2, step 1226/7134 completed (loss: 0.08382496237754822, acc: 0.9798792600631714)
[2025-02-13 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19][root][INFO] - Training Epoch: 2/2, step 1227/7134 completed (loss: 0.06693366169929504, acc: 0.9785522818565369)
[2025-02-13 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20][root][INFO] - Training Epoch: 2/2, step 1228/7134 completed (loss: 0.07312019914388657, acc: 0.9842632412910461)
[2025-02-13 03:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20][root][INFO] - Training Epoch: 2/2, step 1229/7134 completed (loss: 0.031615059822797775, acc: 0.9927971363067627)
[2025-02-13 03:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21][root][INFO] - Training Epoch: 2/2, step 1230/7134 completed (loss: 0.05890775844454765, acc: 0.9807909727096558)
[2025-02-13 03:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21][root][INFO] - Training Epoch: 2/2, step 1231/7134 completed (loss: 0.041346997022628784, acc: 0.9927611351013184)
[2025-02-13 03:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22][root][INFO] - Training Epoch: 2/2, step 1232/7134 completed (loss: 0.04731319472193718, acc: 0.9850904941558838)
[2025-02-13 03:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22][root][INFO] - Training Epoch: 2/2, step 1233/7134 completed (loss: 0.09432204067707062, acc: 0.9722543358802795)
[2025-02-13 03:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23][root][INFO] - Training Epoch: 2/2, step 1234/7134 completed (loss: 0.02918468415737152, acc: 0.9906832575798035)
[2025-02-13 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23][root][INFO] - Training Epoch: 2/2, step 1235/7134 completed (loss: 0.042642056941986084, acc: 0.9916067123413086)
[2025-02-13 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24][root][INFO] - Training Epoch: 2/2, step 1236/7134 completed (loss: 0.06286102533340454, acc: 0.9824304580688477)
[2025-02-13 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24][root][INFO] - Training Epoch: 2/2, step 1237/7134 completed (loss: 0.04865243658423424, acc: 0.9875776171684265)
[2025-02-13 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25][root][INFO] - Training Epoch: 2/2, step 1238/7134 completed (loss: 0.09801899641752243, acc: 0.9679766893386841)
[2025-02-13 03:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25][root][INFO] - Training Epoch: 2/2, step 1239/7134 completed (loss: 0.06279554963111877, acc: 0.9842632412910461)
[2025-02-13 03:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25][root][INFO] - Training Epoch: 2/2, step 1240/7134 completed (loss: 0.053656019270420074, acc: 0.9841269850730896)
[2025-02-13 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:26][root][INFO] - Training Epoch: 2/2, step 1241/7134 completed (loss: 0.04322337731719017, acc: 0.9879518151283264)
[2025-02-13 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:26][root][INFO] - Training Epoch: 2/2, step 1242/7134 completed (loss: 0.04225948080420494, acc: 0.9915865659713745)
[2025-02-13 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27][root][INFO] - Training Epoch: 2/2, step 1243/7134 completed (loss: 0.033717308193445206, acc: 0.991631805896759)
[2025-02-13 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27][root][INFO] - Training Epoch: 2/2, step 1244/7134 completed (loss: 0.06771531701087952, acc: 0.9867549538612366)
[2025-02-13 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28][root][INFO] - Training Epoch: 2/2, step 1245/7134 completed (loss: 0.05165807902812958, acc: 0.986146092414856)
[2025-02-13 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28][root][INFO] - Training Epoch: 2/2, step 1246/7134 completed (loss: 0.08398088812828064, acc: 0.9820282459259033)
[2025-02-13 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29][root][INFO] - Training Epoch: 2/2, step 1247/7134 completed (loss: 0.029021048918366432, acc: 0.9913169145584106)
[2025-02-13 03:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29][root][INFO] - Training Epoch: 2/2, step 1248/7134 completed (loss: 0.05303095653653145, acc: 0.9839816689491272)
[2025-02-13 03:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30][root][INFO] - Training Epoch: 2/2, step 1249/7134 completed (loss: 0.019666744396090508, acc: 0.9928825497627258)
[2025-02-13 03:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30][root][INFO] - Training Epoch: 2/2, step 1250/7134 completed (loss: 0.019792938604950905, acc: 0.9943342804908752)
[2025-02-13 03:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30][root][INFO] - Training Epoch: 2/2, step 1251/7134 completed (loss: 0.01938256435096264, acc: 0.9945130348205566)
[2025-02-13 03:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:31][root][INFO] - Training Epoch: 2/2, step 1252/7134 completed (loss: 0.0191102996468544, acc: 0.9969135522842407)
[2025-02-13 03:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:31][root][INFO] - Training Epoch: 2/2, step 1253/7134 completed (loss: 0.04871927201747894, acc: 0.9826254844665527)
[2025-02-13 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32][root][INFO] - Training Epoch: 2/2, step 1254/7134 completed (loss: 0.04765444993972778, acc: 0.9803030490875244)
[2025-02-13 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32][root][INFO] - Training Epoch: 2/2, step 1255/7134 completed (loss: 0.04104834049940109, acc: 0.991253674030304)
[2025-02-13 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33][root][INFO] - Training Epoch: 2/2, step 1256/7134 completed (loss: 0.027629388496279716, acc: 0.9903537034988403)
[2025-02-13 03:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33][root][INFO] - Training Epoch: 2/2, step 1257/7134 completed (loss: 0.011532793752849102, acc: 0.9983844757080078)
[2025-02-13 03:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34][root][INFO] - Training Epoch: 2/2, step 1258/7134 completed (loss: 0.047280535101890564, acc: 0.9838945865631104)
[2025-02-13 03:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34][root][INFO] - Training Epoch: 2/2, step 1259/7134 completed (loss: 0.06076589599251747, acc: 0.9871244430541992)
[2025-02-13 03:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35][root][INFO] - Training Epoch: 2/2, step 1260/7134 completed (loss: 0.012806088663637638, acc: 0.9971098303794861)
[2025-02-13 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35][root][INFO] - Training Epoch: 2/2, step 1261/7134 completed (loss: 0.026891227811574936, acc: 0.9910979270935059)
[2025-02-13 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35][root][INFO] - Training Epoch: 2/2, step 1262/7134 completed (loss: 0.00774904852733016, acc: 0.9981684684753418)
[2025-02-13 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36][root][INFO] - Training Epoch: 2/2, step 1263/7134 completed (loss: 0.011959089897572994, acc: 0.9942857027053833)
[2025-02-13 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36][root][INFO] - Training Epoch: 2/2, step 1264/7134 completed (loss: 0.01858312264084816, acc: 0.9953271150588989)
[2025-02-13 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:37][root][INFO] - Training Epoch: 2/2, step 1265/7134 completed (loss: 0.013968894258141518, acc: 0.9962406158447266)
[2025-02-13 03:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:37][root][INFO] - Training Epoch: 2/2, step 1266/7134 completed (loss: 0.014391640201210976, acc: 0.9965576529502869)
[2025-02-13 03:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38][root][INFO] - Training Epoch: 2/2, step 1267/7134 completed (loss: 0.019114550203084946, acc: 0.9958620667457581)
[2025-02-13 03:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38][root][INFO] - Training Epoch: 2/2, step 1268/7134 completed (loss: 0.022562745958566666, acc: 0.9896602630615234)
[2025-02-13 03:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39][root][INFO] - Training Epoch: 2/2, step 1269/7134 completed (loss: 0.07180005311965942, acc: 0.9758620858192444)
[2025-02-13 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39][root][INFO] - Training Epoch: 2/2, step 1270/7134 completed (loss: 0.015227895230054855, acc: 0.9944211840629578)
[2025-02-13 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39][root][INFO] - Training Epoch: 2/2, step 1271/7134 completed (loss: 0.05537349730730057, acc: 0.9888888597488403)
[2025-02-13 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40][root][INFO] - Training Epoch: 2/2, step 1272/7134 completed (loss: 0.016805997118353844, acc: 0.9933949708938599)
[2025-02-13 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40][root][INFO] - Training Epoch: 2/2, step 1273/7134 completed (loss: 0.015774430707097054, acc: 0.9951612949371338)
[2025-02-13 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:41][root][INFO] - Training Epoch: 2/2, step 1274/7134 completed (loss: 0.010478724725544453, acc: 0.9966216087341309)
[2025-02-13 03:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:41][root][INFO] - Training Epoch: 2/2, step 1275/7134 completed (loss: 0.017541874200105667, acc: 0.9928186535835266)
[2025-02-13 03:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42][root][INFO] - Training Epoch: 2/2, step 1276/7134 completed (loss: 0.0172348003834486, acc: 0.9903225898742676)
[2025-02-13 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42][root][INFO] - Training Epoch: 2/2, step 1277/7134 completed (loss: 0.018745766952633858, acc: 0.993537962436676)
[2025-02-13 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42][root][INFO] - Training Epoch: 2/2, step 1278/7134 completed (loss: 0.030903423205018044, acc: 0.9922239780426025)
[2025-02-13 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43][root][INFO] - Training Epoch: 2/2, step 1279/7134 completed (loss: 0.03161247447133064, acc: 0.9913151264190674)
[2025-02-13 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43][root][INFO] - Training Epoch: 2/2, step 1280/7134 completed (loss: 0.052882660180330276, acc: 0.9908015727996826)
[2025-02-13 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:44][root][INFO] - Training Epoch: 2/2, step 1281/7134 completed (loss: 0.029296163469552994, acc: 0.9931153059005737)
[2025-02-13 03:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:44][root][INFO] - Training Epoch: 2/2, step 1282/7134 completed (loss: 0.09976352006196976, acc: 0.9759358167648315)
[2025-02-13 03:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45][root][INFO] - Training Epoch: 2/2, step 1283/7134 completed (loss: 0.05061975121498108, acc: 0.9876760840415955)
[2025-02-13 03:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45][root][INFO] - Training Epoch: 2/2, step 1284/7134 completed (loss: 0.051332104951143265, acc: 0.9863760471343994)
[2025-02-13 03:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:46][root][INFO] - Training Epoch: 2/2, step 1285/7134 completed (loss: 0.07609338313341141, acc: 0.9841269850730896)
[2025-02-13 03:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:46][root][INFO] - Training Epoch: 2/2, step 1286/7134 completed (loss: 0.03619987145066261, acc: 0.9942775368690491)
[2025-02-13 03:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47][root][INFO] - Training Epoch: 2/2, step 1287/7134 completed (loss: 0.04705366492271423, acc: 0.9883720874786377)
[2025-02-13 03:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47][root][INFO] - Training Epoch: 2/2, step 1288/7134 completed (loss: 0.039580509066581726, acc: 0.9885786771774292)
[2025-02-13 03:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47][root][INFO] - Training Epoch: 2/2, step 1289/7134 completed (loss: 0.03143312782049179, acc: 0.9882746934890747)
[2025-02-13 03:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:48][root][INFO] - Training Epoch: 2/2, step 1290/7134 completed (loss: 0.021616961807012558, acc: 0.9957746267318726)
[2025-02-13 03:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:48][root][INFO] - Training Epoch: 2/2, step 1291/7134 completed (loss: 0.026577413082122803, acc: 0.9958158731460571)
[2025-02-13 03:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49][root][INFO] - Training Epoch: 2/2, step 1292/7134 completed (loss: 0.026790650561451912, acc: 0.9948052167892456)
[2025-02-13 03:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49][root][INFO] - Training Epoch: 2/2, step 1293/7134 completed (loss: 0.02225375734269619, acc: 0.9941792488098145)
[2025-02-13 03:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50][root][INFO] - Training Epoch: 2/2, step 1294/7134 completed (loss: 0.031294867396354675, acc: 0.9908972978591919)
[2025-02-13 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50][root][INFO] - Training Epoch: 2/2, step 1295/7134 completed (loss: 0.029684728011488914, acc: 0.9915110468864441)
[2025-02-13 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51][root][INFO] - Training Epoch: 2/2, step 1296/7134 completed (loss: 0.005120755638927221, acc: 1.0)
[2025-02-13 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51][root][INFO] - Training Epoch: 2/2, step 1297/7134 completed (loss: 0.014682246372103691, acc: 0.9958419799804688)
[2025-02-13 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51][root][INFO] - Training Epoch: 2/2, step 1298/7134 completed (loss: 0.05742371082305908, acc: 0.9838129281997681)
[2025-02-13 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52][root][INFO] - Training Epoch: 2/2, step 1299/7134 completed (loss: 0.014062858186662197, acc: 0.9947916865348816)
[2025-02-13 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52][root][INFO] - Training Epoch: 2/2, step 1300/7134 completed (loss: 0.02808326669037342, acc: 0.9902557730674744)
[2025-02-13 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53][root][INFO] - Training Epoch: 2/2, step 1301/7134 completed (loss: 0.012453204952180386, acc: 0.997032642364502)
[2025-02-13 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53][root][INFO] - Training Epoch: 2/2, step 1302/7134 completed (loss: 0.03855682164430618, acc: 0.9921875)
[2025-02-13 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54][root][INFO] - Training Epoch: 2/2, step 1303/7134 completed (loss: 0.03382261469960213, acc: 0.9903846383094788)
[2025-02-13 03:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54][root][INFO] - Training Epoch: 2/2, step 1304/7134 completed (loss: 0.05585725978016853, acc: 0.9855072498321533)
[2025-02-13 03:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55][root][INFO] - Training Epoch: 2/2, step 1305/7134 completed (loss: 0.0425301231443882, acc: 0.989847719669342)
[2025-02-13 03:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55][root][INFO] - Training Epoch: 2/2, step 1306/7134 completed (loss: 0.042479466646909714, acc: 0.987860381603241)
[2025-02-13 03:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56][root][INFO] - Training Epoch: 2/2, step 1307/7134 completed (loss: 0.01861904375255108, acc: 0.9952606558799744)
[2025-02-13 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56][root][INFO] - Training Epoch: 2/2, step 1308/7134 completed (loss: 0.018159018829464912, acc: 0.995488703250885)
[2025-02-13 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56][root][INFO] - Training Epoch: 2/2, step 1309/7134 completed (loss: 0.03854266181588173, acc: 0.989234447479248)
[2025-02-13 03:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:57][root][INFO] - Training Epoch: 2/2, step 1310/7134 completed (loss: 0.025561047717928886, acc: 0.9904761910438538)
[2025-02-13 03:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:57][root][INFO] - Training Epoch: 2/2, step 1311/7134 completed (loss: 0.022351985797286034, acc: 0.9909502267837524)
[2025-02-13 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58][root][INFO] - Training Epoch: 2/2, step 1312/7134 completed (loss: 0.07399185001850128, acc: 0.9838274717330933)
[2025-02-13 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58][root][INFO] - Training Epoch: 2/2, step 1313/7134 completed (loss: 0.055627282708883286, acc: 0.9815863966941833)
[2025-02-13 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59][root][INFO] - Training Epoch: 2/2, step 1314/7134 completed (loss: 0.03738340735435486, acc: 0.9929824471473694)
[2025-02-13 03:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59][root][INFO] - Training Epoch: 2/2, step 1315/7134 completed (loss: 0.10129062086343765, acc: 0.9760000109672546)
[2025-02-13 03:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:00][root][INFO] - Training Epoch: 2/2, step 1316/7134 completed (loss: 0.013735049404203892, acc: 0.9981167316436768)
[2025-02-13 03:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:00][root][INFO] - Training Epoch: 2/2, step 1317/7134 completed (loss: 0.10329288244247437, acc: 0.9674796462059021)
[2025-02-13 03:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01][root][INFO] - Training Epoch: 2/2, step 1318/7134 completed (loss: 0.018244370818138123, acc: 0.9945504069328308)
[2025-02-13 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01][root][INFO] - Training Epoch: 2/2, step 1319/7134 completed (loss: 0.0416221059858799, acc: 0.9954233169555664)
[2025-02-13 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01][root][INFO] - Training Epoch: 2/2, step 1320/7134 completed (loss: 0.07108473032712936, acc: 0.9820846915245056)
[2025-02-13 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:02][root][INFO] - Training Epoch: 2/2, step 1321/7134 completed (loss: 0.019617052748799324, acc: 0.9926470518112183)
[2025-02-13 03:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:02][root][INFO] - Training Epoch: 2/2, step 1322/7134 completed (loss: 0.044922180473804474, acc: 0.9901800155639648)
[2025-02-13 03:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03][root][INFO] - Training Epoch: 2/2, step 1323/7134 completed (loss: 0.03916660696268082, acc: 0.987500011920929)
[2025-02-13 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03][root][INFO] - Training Epoch: 2/2, step 1324/7134 completed (loss: 0.02161286398768425, acc: 0.9934210777282715)
[2025-02-13 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:04][root][INFO] - Training Epoch: 2/2, step 1325/7134 completed (loss: 0.01289944164454937, acc: 0.997732400894165)
[2025-02-13 03:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:04][root][INFO] - Training Epoch: 2/2, step 1326/7134 completed (loss: 0.042003046721220016, acc: 0.9923161268234253)
[2025-02-13 03:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05][root][INFO] - Training Epoch: 2/2, step 1327/7134 completed (loss: 0.05412804335355759, acc: 0.9850075244903564)
[2025-02-13 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05][root][INFO] - Training Epoch: 2/2, step 1328/7134 completed (loss: 0.04948921129107475, acc: 0.9906976819038391)
[2025-02-13 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05][root][INFO] - Training Epoch: 2/2, step 1329/7134 completed (loss: 0.04623296484351158, acc: 0.9834254384040833)
[2025-02-13 03:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:06][root][INFO] - Training Epoch: 2/2, step 1330/7134 completed (loss: 0.018033340573310852, acc: 0.994854211807251)
[2025-02-13 03:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:06][root][INFO] - Training Epoch: 2/2, step 1331/7134 completed (loss: 0.006616351660341024, acc: 0.9983713626861572)
[2025-02-13 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07][root][INFO] - Training Epoch: 2/2, step 1332/7134 completed (loss: 0.012111661955714226, acc: 0.9956331849098206)
[2025-02-13 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07][root][INFO] - Training Epoch: 2/2, step 1333/7134 completed (loss: 0.029767321422696114, acc: 0.9897959232330322)
[2025-02-13 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08][root][INFO] - Training Epoch: 2/2, step 1334/7134 completed (loss: 0.0705212950706482, acc: 0.97826087474823)
[2025-02-13 03:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08][root][INFO] - Training Epoch: 2/2, step 1335/7134 completed (loss: 0.09965039044618607, acc: 0.9813581705093384)
[2025-02-13 03:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09][root][INFO] - Training Epoch: 2/2, step 1336/7134 completed (loss: 0.02229209989309311, acc: 0.9918808937072754)
[2025-02-13 03:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09][root][INFO] - Training Epoch: 2/2, step 1337/7134 completed (loss: 0.01005032379180193, acc: 0.995398759841919)
[2025-02-13 03:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09][root][INFO] - Training Epoch: 2/2, step 1338/7134 completed (loss: 0.04751687869429588, acc: 0.9839357137680054)
[2025-02-13 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10][root][INFO] - Training Epoch: 2/2, step 1339/7134 completed (loss: 0.044980112463235855, acc: 0.983146071434021)
[2025-02-13 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10][root][INFO] - Training Epoch: 2/2, step 1340/7134 completed (loss: 0.03833790495991707, acc: 0.9850968718528748)
[2025-02-13 03:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11][root][INFO] - Training Epoch: 2/2, step 1341/7134 completed (loss: 0.04165985807776451, acc: 0.9887005686759949)
[2025-02-13 03:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11][root][INFO] - Training Epoch: 2/2, step 1342/7134 completed (loss: 0.004177757538855076, acc: 1.0)
[2025-02-13 03:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:12][root][INFO] - Training Epoch: 2/2, step 1343/7134 completed (loss: 0.06520441174507141, acc: 0.989266574382782)
[2025-02-13 03:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:12][root][INFO] - Training Epoch: 2/2, step 1344/7134 completed (loss: 0.016018947586417198, acc: 0.9954751133918762)
[2025-02-13 03:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:13][root][INFO] - Training Epoch: 2/2, step 1345/7134 completed (loss: 0.020082591101527214, acc: 0.9893898963928223)
[2025-02-13 03:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:13][root][INFO] - Training Epoch: 2/2, step 1346/7134 completed (loss: 0.04403024539351463, acc: 0.9893190860748291)
[2025-02-13 03:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:14][root][INFO] - Training Epoch: 2/2, step 1347/7134 completed (loss: 0.04696870967745781, acc: 0.9878048896789551)
[2025-02-13 03:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:14][root][INFO] - Training Epoch: 2/2, step 1348/7134 completed (loss: 0.014822863973677158, acc: 0.9960681796073914)
[2025-02-13 03:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:14][root][INFO] - Training Epoch: 2/2, step 1349/7134 completed (loss: 0.01961291767656803, acc: 0.9965811967849731)
[2025-02-13 03:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:15][root][INFO] - Training Epoch: 2/2, step 1350/7134 completed (loss: 0.023809261620044708, acc: 0.9956584572792053)
[2025-02-13 03:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:15][root][INFO] - Training Epoch: 2/2, step 1351/7134 completed (loss: 0.014965119771659374, acc: 0.9960707426071167)
[2025-02-13 03:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16][root][INFO] - Training Epoch: 2/2, step 1352/7134 completed (loss: 0.027498891577124596, acc: 0.992559552192688)
[2025-02-13 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16][root][INFO] - Training Epoch: 2/2, step 1353/7134 completed (loss: 0.04111531749367714, acc: 0.99042147397995)
[2025-02-13 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17][root][INFO] - Training Epoch: 2/2, step 1354/7134 completed (loss: 0.015613671392202377, acc: 0.9967585206031799)
[2025-02-13 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17][root][INFO] - Training Epoch: 2/2, step 1355/7134 completed (loss: 0.008802388794720173, acc: 0.9953271150588989)
[2025-02-13 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18][root][INFO] - Training Epoch: 2/2, step 1356/7134 completed (loss: 0.005292266141623259, acc: 1.0)
[2025-02-13 03:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18][root][INFO] - Training Epoch: 2/2, step 1357/7134 completed (loss: 0.0071692378260195255, acc: 0.9987389445304871)
[2025-02-13 03:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18][root][INFO] - Training Epoch: 2/2, step 1358/7134 completed (loss: 0.026306791231036186, acc: 0.9948052167892456)
[2025-02-13 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19][root][INFO] - Training Epoch: 2/2, step 1359/7134 completed (loss: 0.006456049624830484, acc: 0.9979757070541382)
[2025-02-13 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19][root][INFO] - Training Epoch: 2/2, step 1360/7134 completed (loss: 0.003952891565859318, acc: 0.9986559152603149)
[2025-02-13 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20][root][INFO] - Training Epoch: 2/2, step 1361/7134 completed (loss: 0.020893005654215813, acc: 0.9941605925559998)
[2025-02-13 03:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20][root][INFO] - Training Epoch: 2/2, step 1362/7134 completed (loss: 0.009968496859073639, acc: 0.9959016442298889)
[2025-02-13 03:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21][root][INFO] - Training Epoch: 2/2, step 1363/7134 completed (loss: 0.013559664599597454, acc: 0.9959785342216492)
[2025-02-13 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21][root][INFO] - Training Epoch: 2/2, step 1364/7134 completed (loss: 0.004801343660801649, acc: 0.9971428513526917)
[2025-02-13 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22][root][INFO] - Training Epoch: 2/2, step 1365/7134 completed (loss: 0.004636975005269051, acc: 0.9981684684753418)
[2025-02-13 03:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22][root][INFO] - Training Epoch: 2/2, step 1366/7134 completed (loss: 0.004516578279435635, acc: 0.9982143044471741)
[2025-02-13 03:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23][root][INFO] - Training Epoch: 2/2, step 1367/7134 completed (loss: 0.0011200300650671124, acc: 1.0)
[2025-02-13 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23][root][INFO] - Training Epoch: 2/2, step 1368/7134 completed (loss: 0.02255093678832054, acc: 0.9908536672592163)
[2025-02-13 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23][root][INFO] - Training Epoch: 2/2, step 1369/7134 completed (loss: 0.016400324180722237, acc: 0.9929947257041931)
[2025-02-13 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24][root][INFO] - Training Epoch: 2/2, step 1370/7134 completed (loss: 0.02952962927520275, acc: 0.992414653301239)
[2025-02-13 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24][root][INFO] - Training Epoch: 2/2, step 1371/7134 completed (loss: 0.019500359892845154, acc: 0.9956331849098206)
[2025-02-13 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25][root][INFO] - Training Epoch: 2/2, step 1372/7134 completed (loss: 0.03731599822640419, acc: 0.9906367063522339)
[2025-02-13 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25][root][INFO] - Training Epoch: 2/2, step 1373/7134 completed (loss: 0.024856168776750565, acc: 0.9901574850082397)
[2025-02-13 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26][root][INFO] - Training Epoch: 2/2, step 1374/7134 completed (loss: 0.01936786063015461, acc: 0.9920791983604431)
[2025-02-13 03:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26][root][INFO] - Training Epoch: 2/2, step 1375/7134 completed (loss: 0.02446460723876953, acc: 0.9903846383094788)
[2025-02-13 03:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27][root][INFO] - Training Epoch: 2/2, step 1376/7134 completed (loss: 0.05375337228178978, acc: 0.9812108278274536)
[2025-02-13 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27][root][INFO] - Training Epoch: 2/2, step 1377/7134 completed (loss: 0.022522451356053352, acc: 0.9905882477760315)
[2025-02-13 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27][root][INFO] - Training Epoch: 2/2, step 1378/7134 completed (loss: 0.04760964214801788, acc: 0.9863547682762146)
[2025-02-13 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28][root][INFO] - Training Epoch: 2/2, step 1379/7134 completed (loss: 0.03876924142241478, acc: 0.9870967864990234)
[2025-02-13 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28][root][INFO] - Training Epoch: 2/2, step 1380/7134 completed (loss: 0.025860553607344627, acc: 0.9925233721733093)
[2025-02-13 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29][root][INFO] - Training Epoch: 2/2, step 1381/7134 completed (loss: 0.02885894477367401, acc: 0.9883913993835449)
[2025-02-13 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29][root][INFO] - Training Epoch: 2/2, step 1382/7134 completed (loss: 0.036821044981479645, acc: 0.9873015880584717)
[2025-02-13 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30][root][INFO] - Training Epoch: 2/2, step 1383/7134 completed (loss: 0.017741331830620766, acc: 0.9948365092277527)
[2025-02-13 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30][root][INFO] - Training Epoch: 2/2, step 1384/7134 completed (loss: 0.05726855248212814, acc: 0.9783693552017212)
[2025-02-13 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30][root][INFO] - Training Epoch: 2/2, step 1385/7134 completed (loss: 0.039549410343170166, acc: 0.9871559739112854)
[2025-02-13 03:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31][root][INFO] - Training Epoch: 2/2, step 1386/7134 completed (loss: 0.026871200650930405, acc: 0.99262535572052)
[2025-02-13 03:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31][root][INFO] - Training Epoch: 2/2, step 1387/7134 completed (loss: 0.03272317349910736, acc: 0.9885386824607849)
[2025-02-13 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32][root][INFO] - Training Epoch: 2/2, step 1388/7134 completed (loss: 0.04412289708852768, acc: 0.9897785186767578)
[2025-02-13 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32][root][INFO] - Training Epoch: 2/2, step 1389/7134 completed (loss: 0.021604478359222412, acc: 0.9951377511024475)
[2025-02-13 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:33][root][INFO] - Training Epoch: 2/2, step 1390/7134 completed (loss: 0.020346276462078094, acc: 0.9875195026397705)
[2025-02-13 03:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:33][root][INFO] - Training Epoch: 2/2, step 1391/7134 completed (loss: 0.05214807018637657, acc: 0.9806678295135498)
[2025-02-13 03:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34][root][INFO] - Training Epoch: 2/2, step 1392/7134 completed (loss: 0.02734784223139286, acc: 0.991349458694458)
[2025-02-13 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34][root][INFO] - Training Epoch: 2/2, step 1393/7134 completed (loss: 0.023452404886484146, acc: 0.9907833933830261)
[2025-02-13 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34][root][INFO] - Training Epoch: 2/2, step 1394/7134 completed (loss: 0.01833825185894966, acc: 0.988095223903656)
[2025-02-13 03:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:35][root][INFO] - Training Epoch: 2/2, step 1395/7134 completed (loss: 0.03708883374929428, acc: 0.9881656765937805)
[2025-02-13 03:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:35][root][INFO] - Training Epoch: 2/2, step 1396/7134 completed (loss: 0.03301467373967171, acc: 0.9881756901741028)
[2025-02-13 03:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36][root][INFO] - Training Epoch: 2/2, step 1397/7134 completed (loss: 0.05568226799368858, acc: 0.9881955981254578)
[2025-02-13 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36][root][INFO] - Training Epoch: 2/2, step 1398/7134 completed (loss: 0.012914536520838737, acc: 0.993914783000946)
[2025-02-13 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37][root][INFO] - Training Epoch: 2/2, step 1399/7134 completed (loss: 0.051559604704380035, acc: 0.9855072498321533)
[2025-02-13 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37][root][INFO] - Training Epoch: 2/2, step 1400/7134 completed (loss: 0.027028778567910194, acc: 0.9930192232131958)
[2025-02-13 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37][root][INFO] - Training Epoch: 2/2, step 1401/7134 completed (loss: 0.010840082541108131, acc: 0.9983525276184082)
[2025-02-13 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38][root][INFO] - Training Epoch: 2/2, step 1402/7134 completed (loss: 0.01745353639125824, acc: 0.9930915236473083)
[2025-02-13 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38][root][INFO] - Training Epoch: 2/2, step 1403/7134 completed (loss: 0.01802827976644039, acc: 0.9919678568840027)
[2025-02-13 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39][root][INFO] - Training Epoch: 2/2, step 1404/7134 completed (loss: 0.014186467044055462, acc: 0.9986737370491028)
[2025-02-13 03:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39][root][INFO] - Training Epoch: 2/2, step 1405/7134 completed (loss: 0.01829945482313633, acc: 0.994301974773407)
[2025-02-13 03:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40][root][INFO] - Training Epoch: 2/2, step 1406/7134 completed (loss: 0.0030759272631257772, acc: 0.9985632300376892)
[2025-02-13 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40][root][INFO] - Training Epoch: 2/2, step 1407/7134 completed (loss: 0.006496111862361431, acc: 0.9973924160003662)
[2025-02-13 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41][root][INFO] - Training Epoch: 2/2, step 1408/7134 completed (loss: 0.011212002485990524, acc: 0.9967948794364929)
[2025-02-13 03:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41][root][INFO] - Training Epoch: 2/2, step 1409/7134 completed (loss: 0.004795428365468979, acc: 0.9985097050666809)
[2025-02-13 03:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41][root][INFO] - Training Epoch: 2/2, step 1410/7134 completed (loss: 0.015324532985687256, acc: 0.9973958134651184)
[2025-02-13 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42][root][INFO] - Training Epoch: 2/2, step 1411/7134 completed (loss: 0.005901270546019077, acc: 0.998670220375061)
[2025-02-13 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42][root][INFO] - Training Epoch: 2/2, step 1412/7134 completed (loss: 0.006861853878945112, acc: 0.9988123774528503)
[2025-02-13 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43][root][INFO] - Training Epoch: 2/2, step 1413/7134 completed (loss: 0.012170189060270786, acc: 0.9971222877502441)
[2025-02-13 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43][root][INFO] - Training Epoch: 2/2, step 1414/7134 completed (loss: 0.007822087034583092, acc: 0.9984423518180847)
[2025-02-13 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44][root][INFO] - Training Epoch: 2/2, step 1415/7134 completed (loss: 0.02387428842484951, acc: 0.9917218685150146)
[2025-02-13 03:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44][root][INFO] - Training Epoch: 2/2, step 1416/7134 completed (loss: 0.009424610994756222, acc: 0.9971910119056702)
[2025-02-13 03:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45][root][INFO] - Training Epoch: 2/2, step 1417/7134 completed (loss: 0.010159186087548733, acc: 0.9965986609458923)
[2025-02-13 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45][root][INFO] - Training Epoch: 2/2, step 1418/7134 completed (loss: 0.004986414685845375, acc: 0.9980915784835815)
[2025-02-13 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46][root][INFO] - Training Epoch: 2/2, step 1419/7134 completed (loss: 0.015688586980104446, acc: 0.997300922870636)
[2025-02-13 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46][root][INFO] - Training Epoch: 2/2, step 1420/7134 completed (loss: 0.01308401208370924, acc: 0.9959568977355957)
[2025-02-13 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46][root][INFO] - Training Epoch: 2/2, step 1421/7134 completed (loss: 0.010524597950279713, acc: 0.9967948794364929)
[2025-02-13 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47][root][INFO] - Training Epoch: 2/2, step 1422/7134 completed (loss: 0.03385528549551964, acc: 0.9871794581413269)
[2025-02-13 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47][root][INFO] - Training Epoch: 2/2, step 1423/7134 completed (loss: 0.03290756046772003, acc: 0.9897959232330322)
[2025-02-13 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48][root][INFO] - Training Epoch: 2/2, step 1424/7134 completed (loss: 0.045121170580387115, acc: 0.9847908616065979)
[2025-02-13 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48][root][INFO] - Training Epoch: 2/2, step 1425/7134 completed (loss: 0.02709757350385189, acc: 0.9850968718528748)
[2025-02-13 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49][root][INFO] - Training Epoch: 2/2, step 1426/7134 completed (loss: 0.02489653043448925, acc: 0.9884318709373474)
[2025-02-13 03:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49][root][INFO] - Training Epoch: 2/2, step 1427/7134 completed (loss: 0.02966754138469696, acc: 0.9859514832496643)
[2025-02-13 03:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50][root][INFO] - Training Epoch: 2/2, step 1428/7134 completed (loss: 0.15444570779800415, acc: 0.9516616463661194)
[2025-02-13 03:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50][root][INFO] - Training Epoch: 2/2, step 1429/7134 completed (loss: 0.02440672181546688, acc: 0.9916840195655823)
[2025-02-13 03:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51][root][INFO] - Training Epoch: 2/2, step 1430/7134 completed (loss: 0.04895938187837601, acc: 0.9870848655700684)
[2025-02-13 03:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51][root][INFO] - Training Epoch: 2/2, step 1431/7134 completed (loss: 0.013367444276809692, acc: 0.997286319732666)
[2025-02-13 03:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51][root][INFO] - Training Epoch: 2/2, step 1432/7134 completed (loss: 0.015727827325463295, acc: 0.992277979850769)
[2025-02-13 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:52][root][INFO] - Training Epoch: 2/2, step 1433/7134 completed (loss: 0.009887788444757462, acc: 0.9973822236061096)
[2025-02-13 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:52][root][INFO] - Training Epoch: 2/2, step 1434/7134 completed (loss: 0.03804751858115196, acc: 0.9879518151283264)
[2025-02-13 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53][root][INFO] - Training Epoch: 2/2, step 1435/7134 completed (loss: 0.030421758070588112, acc: 0.9912023544311523)
[2025-02-13 03:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53][root][INFO] - Training Epoch: 2/2, step 1436/7134 completed (loss: 0.0516219437122345, acc: 0.9809644818305969)
[2025-02-13 03:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54][root][INFO] - Training Epoch: 2/2, step 1437/7134 completed (loss: 0.02310396358370781, acc: 0.9930362105369568)
[2025-02-13 03:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54][root][INFO] - Training Epoch: 2/2, step 1438/7134 completed (loss: 0.029471395537257195, acc: 0.9941605925559998)
[2025-02-13 03:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:55][root][INFO] - Training Epoch: 2/2, step 1439/7134 completed (loss: 0.021114680916070938, acc: 0.9907235503196716)
[2025-02-13 03:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:55][root][INFO] - Training Epoch: 2/2, step 1440/7134 completed (loss: 0.047841817140579224, acc: 0.979345977306366)
[2025-02-13 03:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56][root][INFO] - Training Epoch: 2/2, step 1441/7134 completed (loss: 0.02605275809764862, acc: 0.9941349029541016)
[2025-02-13 03:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56][root][INFO] - Training Epoch: 2/2, step 1442/7134 completed (loss: 0.023121559992432594, acc: 0.9952152967453003)
[2025-02-13 03:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56][root][INFO] - Training Epoch: 2/2, step 1443/7134 completed (loss: 0.07291726022958755, acc: 0.9841827750205994)
[2025-02-13 03:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57][root][INFO] - Training Epoch: 2/2, step 1444/7134 completed (loss: 0.03030892089009285, acc: 0.9922178983688354)
[2025-02-13 03:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57][root][INFO] - Training Epoch: 2/2, step 1445/7134 completed (loss: 0.047677792608737946, acc: 0.9850249290466309)
[2025-02-13 03:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58][root][INFO] - Training Epoch: 2/2, step 1446/7134 completed (loss: 0.0153053505346179, acc: 0.9957325458526611)
[2025-02-13 03:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58][root][INFO] - Training Epoch: 2/2, step 1447/7134 completed (loss: 0.03471190854907036, acc: 0.9886524677276611)
[2025-02-13 03:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59][root][INFO] - Training Epoch: 2/2, step 1448/7134 completed (loss: 0.0401606410741806, acc: 0.9872881174087524)
[2025-02-13 03:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59][root][INFO] - Training Epoch: 2/2, step 1449/7134 completed (loss: 0.05710987001657486, acc: 0.9845505356788635)
[2025-02-13 03:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59][root][INFO] - Training Epoch: 2/2, step 1450/7134 completed (loss: 0.013924125581979752, acc: 0.994194507598877)
[2025-02-13 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00][root][INFO] - Training Epoch: 2/2, step 1451/7134 completed (loss: 0.01526847667992115, acc: 0.9956076145172119)
[2025-02-13 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00][root][INFO] - Training Epoch: 2/2, step 1452/7134 completed (loss: 0.06843309849500656, acc: 0.9861111044883728)
[2025-02-13 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01][root][INFO] - Training Epoch: 2/2, step 1453/7134 completed (loss: 0.052972275763750076, acc: 0.9873217344284058)
[2025-02-13 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01][root][INFO] - Training Epoch: 2/2, step 1454/7134 completed (loss: 0.054726146161556244, acc: 0.982503354549408)
[2025-02-13 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:02][root][INFO] - Training Epoch: 2/2, step 1455/7134 completed (loss: 0.05962761864066124, acc: 0.9871559739112854)
[2025-02-13 03:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:02][root][INFO] - Training Epoch: 2/2, step 1456/7134 completed (loss: 0.01292317733168602, acc: 0.9978166222572327)
[2025-02-13 03:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:03][root][INFO] - Training Epoch: 2/2, step 1457/7134 completed (loss: 0.03207041695713997, acc: 0.9937984347343445)
[2025-02-13 03:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:03][root][INFO] - Training Epoch: 2/2, step 1458/7134 completed (loss: 0.009520001709461212, acc: 1.0)
[2025-02-13 03:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:03][root][INFO] - Training Epoch: 2/2, step 1459/7134 completed (loss: 0.016076108440756798, acc: 0.9964476227760315)
[2025-02-13 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04][root][INFO] - Training Epoch: 2/2, step 1460/7134 completed (loss: 0.0463397279381752, acc: 0.9842105507850647)
[2025-02-13 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04][root][INFO] - Training Epoch: 2/2, step 1461/7134 completed (loss: 0.020291801542043686, acc: 0.9888357520103455)
[2025-02-13 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05][root][INFO] - Training Epoch: 2/2, step 1462/7134 completed (loss: 0.052545882761478424, acc: 0.9841269850730896)
[2025-02-13 03:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05][root][INFO] - Training Epoch: 2/2, step 1463/7134 completed (loss: 0.05255820229649544, acc: 0.9832636117935181)
[2025-02-13 03:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06][root][INFO] - Training Epoch: 2/2, step 1464/7134 completed (loss: 0.07825161516666412, acc: 0.9849849939346313)
[2025-02-13 03:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06][root][INFO] - Training Epoch: 2/2, step 1465/7134 completed (loss: 0.026451442390680313, acc: 0.9937304258346558)
[2025-02-13 03:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07][root][INFO] - Training Epoch: 2/2, step 1466/7134 completed (loss: 0.011341828852891922, acc: 0.9957507252693176)
[2025-02-13 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07][root][INFO] - Training Epoch: 2/2, step 1467/7134 completed (loss: 0.00794360414147377, acc: 0.9971181750297546)
[2025-02-13 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07][root][INFO] - Training Epoch: 2/2, step 1468/7134 completed (loss: 0.012842591851949692, acc: 0.9947159886360168)
[2025-02-13 03:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:08][root][INFO] - Training Epoch: 2/2, step 1469/7134 completed (loss: 0.01760125905275345, acc: 0.9932705163955688)
[2025-02-13 03:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:08][root][INFO] - Training Epoch: 2/2, step 1470/7134 completed (loss: 0.022327017039060593, acc: 0.9927745461463928)
[2025-02-13 03:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09][root][INFO] - Training Epoch: 2/2, step 1471/7134 completed (loss: 0.025154463946819305, acc: 0.9928951859474182)
[2025-02-13 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09][root][INFO] - Training Epoch: 2/2, step 1472/7134 completed (loss: 0.027202537283301353, acc: 0.99245285987854)
[2025-02-13 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:10][root][INFO] - Training Epoch: 2/2, step 1473/7134 completed (loss: 0.02763279341161251, acc: 0.9896142482757568)
[2025-02-13 03:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:10][root][INFO] - Training Epoch: 2/2, step 1474/7134 completed (loss: 0.038095295429229736, acc: 0.9832214713096619)
[2025-02-13 03:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11][root][INFO] - Training Epoch: 2/2, step 1475/7134 completed (loss: 0.03490104898810387, acc: 0.9920844435691833)
[2025-02-13 03:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11][root][INFO] - Training Epoch: 2/2, step 1476/7134 completed (loss: 0.020390400663018227, acc: 0.9922360181808472)
[2025-02-13 03:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11][root][INFO] - Training Epoch: 2/2, step 1477/7134 completed (loss: 0.03715604171156883, acc: 0.9912739992141724)
[2025-02-13 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12][root][INFO] - Training Epoch: 2/2, step 1478/7134 completed (loss: 0.013263452798128128, acc: 0.9956204295158386)
[2025-02-13 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12][root][INFO] - Training Epoch: 2/2, step 1479/7134 completed (loss: 0.018326129764318466, acc: 0.9942528605461121)
[2025-02-13 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13][root][INFO] - Training Epoch: 2/2, step 1480/7134 completed (loss: 0.004606257192790508, acc: 0.998701274394989)
[2025-02-13 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13][root][INFO] - Training Epoch: 2/2, step 1481/7134 completed (loss: 0.030239243060350418, acc: 0.9973683953285217)
[2025-02-13 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14][root][INFO] - Training Epoch: 2/2, step 1482/7134 completed (loss: 0.022849757224321365, acc: 0.9957386255264282)
[2025-02-13 03:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14][root][INFO] - Training Epoch: 2/2, step 1483/7134 completed (loss: 0.009486613795161247, acc: 0.9968404173851013)
[2025-02-13 03:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15][root][INFO] - Training Epoch: 2/2, step 1484/7134 completed (loss: 0.03953464701771736, acc: 0.9898843765258789)
[2025-02-13 03:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15][root][INFO] - Training Epoch: 2/2, step 1485/7134 completed (loss: 0.017094094306230545, acc: 0.993678867816925)
[2025-02-13 03:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16][root][INFO] - Training Epoch: 2/2, step 1486/7134 completed (loss: 0.03405913710594177, acc: 0.9945873022079468)
[2025-02-13 03:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16][root][INFO] - Training Epoch: 2/2, step 1487/7134 completed (loss: 0.005946669727563858, acc: 0.9986928105354309)
[2025-02-13 03:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16][root][INFO] - Training Epoch: 2/2, step 1488/7134 completed (loss: 0.019525762647390366, acc: 0.9957219362258911)
[2025-02-13 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17][root][INFO] - Training Epoch: 2/2, step 1489/7134 completed (loss: 0.017130764201283455, acc: 0.9964953064918518)
[2025-02-13 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17][root][INFO] - Training Epoch: 2/2, step 1490/7134 completed (loss: 0.007169995456933975, acc: 1.0)
[2025-02-13 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18][root][INFO] - Training Epoch: 2/2, step 1491/7134 completed (loss: 0.029057351872324944, acc: 0.9942528605461121)
[2025-02-13 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18][root][INFO] - Training Epoch: 2/2, step 1492/7134 completed (loss: 0.04694293439388275, acc: 0.988399088382721)
[2025-02-13 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:19][root][INFO] - Training Epoch: 2/2, step 1493/7134 completed (loss: 0.02261609025299549, acc: 0.9965437650680542)
[2025-02-13 03:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:19][root][INFO] - Training Epoch: 2/2, step 1494/7134 completed (loss: 0.012817228212952614, acc: 0.9951338171958923)
[2025-02-13 03:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20][root][INFO] - Training Epoch: 2/2, step 1495/7134 completed (loss: 0.0060341572389006615, acc: 0.9988024234771729)
[2025-02-13 03:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20][root][INFO] - Training Epoch: 2/2, step 1496/7134 completed (loss: 0.04515410587191582, acc: 0.9900285005569458)
[2025-02-13 03:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21][root][INFO] - Training Epoch: 2/2, step 1497/7134 completed (loss: 0.025371888652443886, acc: 0.9920814633369446)
[2025-02-13 03:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21][root][INFO] - Training Epoch: 2/2, step 1498/7134 completed (loss: 0.004839256871491671, acc: 1.0)
[2025-02-13 03:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22][root][INFO] - Training Epoch: 2/2, step 1499/7134 completed (loss: 0.009680475108325481, acc: 0.9964413046836853)
[2025-02-13 03:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22][root][INFO] - Training Epoch: 2/2, step 1500/7134 completed (loss: 0.010272115468978882, acc: 0.998344361782074)
[2025-02-13 03:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22][root][INFO] - Training Epoch: 2/2, step 1501/7134 completed (loss: 0.02418610081076622, acc: 0.99262535572052)
[2025-02-13 03:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:23][root][INFO] - Training Epoch: 2/2, step 1502/7134 completed (loss: 0.02764192409813404, acc: 0.990980863571167)
[2025-02-13 03:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:23][root][INFO] - Training Epoch: 2/2, step 1503/7134 completed (loss: 0.042663924396038055, acc: 0.9801136255264282)
[2025-02-13 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24][root][INFO] - Training Epoch: 2/2, step 1504/7134 completed (loss: 0.00954058300703764, acc: 0.9974842667579651)
[2025-02-13 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24][root][INFO] - Training Epoch: 2/2, step 1505/7134 completed (loss: 0.029406337067484856, acc: 0.9937694668769836)
[2025-02-13 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25][root][INFO] - Training Epoch: 2/2, step 1506/7134 completed (loss: 0.02798563428223133, acc: 0.9908814430236816)
[2025-02-13 03:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25][root][INFO] - Training Epoch: 2/2, step 1507/7134 completed (loss: 0.01812116429209709, acc: 0.9934554696083069)
[2025-02-13 03:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26][root][INFO] - Training Epoch: 2/2, step 1508/7134 completed (loss: 0.011056968942284584, acc: 0.9967177510261536)
[2025-02-13 03:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26][root][INFO] - Training Epoch: 2/2, step 1509/7134 completed (loss: 0.008068426512181759, acc: 0.9985994100570679)
[2025-02-13 03:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27][root][INFO] - Training Epoch: 2/2, step 1510/7134 completed (loss: 0.03145136311650276, acc: 0.9900000095367432)
[2025-02-13 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27][root][INFO] - Training Epoch: 2/2, step 1511/7134 completed (loss: 0.04216403886675835, acc: 0.9872340559959412)
[2025-02-13 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27][root][INFO] - Training Epoch: 2/2, step 1512/7134 completed (loss: 0.009371851570904255, acc: 0.9984779357910156)
[2025-02-13 03:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28][root][INFO] - Training Epoch: 2/2, step 1513/7134 completed (loss: 0.00791556853801012, acc: 0.9960317611694336)
[2025-02-13 03:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28][root][INFO] - Training Epoch: 2/2, step 1514/7134 completed (loss: 0.03024917095899582, acc: 0.9962732791900635)
[2025-02-13 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29][root][INFO] - Training Epoch: 2/2, step 1515/7134 completed (loss: 0.016624050214886665, acc: 0.9953271150588989)
[2025-02-13 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29][root][INFO] - Training Epoch: 2/2, step 1516/7134 completed (loss: 0.028017356991767883, acc: 0.994301974773407)
[2025-02-13 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:30][root][INFO] - Training Epoch: 2/2, step 1517/7134 completed (loss: 0.02042906917631626, acc: 0.9894958138465881)
[2025-02-13 03:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:30][root][INFO] - Training Epoch: 2/2, step 1518/7134 completed (loss: 0.00818695779889822, acc: 0.9975903630256653)
[2025-02-13 03:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:31][root][INFO] - Training Epoch: 2/2, step 1519/7134 completed (loss: 0.00781079800799489, acc: 0.9976958632469177)
[2025-02-13 03:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:31][root][INFO] - Training Epoch: 2/2, step 1520/7134 completed (loss: 0.011381994001567364, acc: 0.9968798756599426)
[2025-02-13 03:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32][root][INFO] - Training Epoch: 2/2, step 1521/7134 completed (loss: 0.06873203068971634, acc: 0.9776536226272583)
[2025-02-13 03:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32][root][INFO] - Training Epoch: 2/2, step 1522/7134 completed (loss: 0.06922465562820435, acc: 0.9749518036842346)
[2025-02-13 03:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33][root][INFO] - Training Epoch: 2/2, step 1523/7134 completed (loss: 0.08786363899707794, acc: 0.9760119915008545)
[2025-02-13 03:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33][root][INFO] - Training Epoch: 2/2, step 1524/7134 completed (loss: 0.022995317354798317, acc: 0.9966443181037903)
[2025-02-13 03:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33][root][INFO] - Training Epoch: 2/2, step 1525/7134 completed (loss: 0.021388454362750053, acc: 0.9909502267837524)
[2025-02-13 03:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:34][root][INFO] - Training Epoch: 2/2, step 1526/7134 completed (loss: 0.05171417444944382, acc: 0.9869375824928284)
[2025-02-13 03:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:34][root][INFO] - Training Epoch: 2/2, step 1527/7134 completed (loss: 0.09869686514139175, acc: 0.9749582409858704)
[2025-02-13 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35][root][INFO] - Training Epoch: 2/2, step 1528/7134 completed (loss: 0.06030603125691414, acc: 0.9806763529777527)
[2025-02-13 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35][root][INFO] - Training Epoch: 2/2, step 1529/7134 completed (loss: 0.010889043100178242, acc: 0.9964328408241272)
[2025-02-13 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36][root][INFO] - Training Epoch: 2/2, step 1530/7134 completed (loss: 0.04811711609363556, acc: 0.9885277152061462)
[2025-02-13 03:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36][root][INFO] - Training Epoch: 2/2, step 1531/7134 completed (loss: 0.115913987159729, acc: 0.9716312289237976)
[2025-02-13 03:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:37][root][INFO] - Training Epoch: 2/2, step 1532/7134 completed (loss: 0.018720226362347603, acc: 0.9953756928443909)
[2025-02-13 03:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:37][root][INFO] - Training Epoch: 2/2, step 1533/7134 completed (loss: 0.024057425558567047, acc: 0.9889349937438965)
[2025-02-13 03:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38][root][INFO] - Training Epoch: 2/2, step 1534/7134 completed (loss: 0.04284163936972618, acc: 0.9875346422195435)
[2025-02-13 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38][root][INFO] - Training Epoch: 2/2, step 1535/7134 completed (loss: 0.04425542801618576, acc: 0.9892215728759766)
[2025-02-13 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39][root][INFO] - Training Epoch: 2/2, step 1536/7134 completed (loss: 0.022816769778728485, acc: 0.993678867816925)
[2025-02-13 03:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39][root][INFO] - Training Epoch: 2/2, step 1537/7134 completed (loss: 0.04157900810241699, acc: 0.9869109988212585)
[2025-02-13 03:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39][root][INFO] - Training Epoch: 2/2, step 1538/7134 completed (loss: 0.046938199549913406, acc: 0.98975670337677)
[2025-02-13 03:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:40][root][INFO] - Training Epoch: 2/2, step 1539/7134 completed (loss: 0.02495085634291172, acc: 0.9934123754501343)
[2025-02-13 03:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:40][root][INFO] - Training Epoch: 2/2, step 1540/7134 completed (loss: 0.020543089136481285, acc: 0.9919571280479431)
[2025-02-13 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41][root][INFO] - Training Epoch: 2/2, step 1541/7134 completed (loss: 0.034546706825494766, acc: 0.9886792302131653)
[2025-02-13 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41][root][INFO] - Training Epoch: 2/2, step 1542/7134 completed (loss: 0.01780964434146881, acc: 0.9959568977355957)
[2025-02-13 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42][root][INFO] - Training Epoch: 2/2, step 1543/7134 completed (loss: 0.020560400560498238, acc: 0.9923497438430786)
[2025-02-13 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42][root][INFO] - Training Epoch: 2/2, step 1544/7134 completed (loss: 0.02876891940832138, acc: 0.9915048480033875)
[2025-02-13 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43][root][INFO] - Training Epoch: 2/2, step 1545/7134 completed (loss: 0.02068786881864071, acc: 0.9942611455917358)
[2025-02-13 03:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43][root][INFO] - Training Epoch: 2/2, step 1546/7134 completed (loss: 0.06448184698820114, acc: 0.9831697344779968)
[2025-02-13 03:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44][root][INFO] - Training Epoch: 2/2, step 1547/7134 completed (loss: 0.02670818381011486, acc: 0.9895522594451904)
[2025-02-13 03:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44][root][INFO] - Training Epoch: 2/2, step 1548/7134 completed (loss: 0.02311132661998272, acc: 0.9943116903305054)
[2025-02-13 03:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45][root][INFO] - Training Epoch: 2/2, step 1549/7134 completed (loss: 0.022093668580055237, acc: 0.9900332093238831)
[2025-02-13 03:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45][root][INFO] - Training Epoch: 2/2, step 1550/7134 completed (loss: 0.016031552106142044, acc: 0.9936628937721252)
[2025-02-13 03:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45][root][INFO] - Training Epoch: 2/2, step 1551/7134 completed (loss: 0.022207185626029968, acc: 0.9935064911842346)
[2025-02-13 03:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:46][root][INFO] - Training Epoch: 2/2, step 1552/7134 completed (loss: 0.021120622754096985, acc: 0.9946120977401733)
[2025-02-13 03:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:46][root][INFO] - Training Epoch: 2/2, step 1553/7134 completed (loss: 0.02450726181268692, acc: 0.9947984218597412)
[2025-02-13 03:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:47][root][INFO] - Training Epoch: 2/2, step 1554/7134 completed (loss: 0.029152637347579002, acc: 0.9898534417152405)
[2025-02-13 03:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:47][root][INFO] - Training Epoch: 2/2, step 1555/7134 completed (loss: 0.024593807756900787, acc: 0.9921612739562988)
[2025-02-13 03:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:48][root][INFO] - Training Epoch: 2/2, step 1556/7134 completed (loss: 0.020831594243645668, acc: 0.9929494857788086)
[2025-02-13 03:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:48][root][INFO] - Training Epoch: 2/2, step 1557/7134 completed (loss: 0.015034645795822144, acc: 0.9921700358390808)
[2025-02-13 03:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:49][root][INFO] - Training Epoch: 2/2, step 1558/7134 completed (loss: 0.02247113175690174, acc: 0.9915611743927002)
[2025-02-13 03:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:49][root][INFO] - Training Epoch: 2/2, step 1559/7134 completed (loss: 0.010438526049256325, acc: 0.9973226189613342)
[2025-02-13 03:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:50][root][INFO] - Training Epoch: 2/2, step 1560/7134 completed (loss: 0.013863973319530487, acc: 0.9987293481826782)
[2025-02-13 03:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:50][root][INFO] - Training Epoch: 2/2, step 1561/7134 completed (loss: 0.061172209680080414, acc: 0.9848320484161377)
[2025-02-13 03:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51][root][INFO] - Training Epoch: 2/2, step 1562/7134 completed (loss: 0.032398615032434464, acc: 0.9894099831581116)
[2025-02-13 03:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51][root][INFO] - Training Epoch: 2/2, step 1563/7134 completed (loss: 0.02270771749317646, acc: 0.9942857027053833)
[2025-02-13 03:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52][root][INFO] - Training Epoch: 2/2, step 1564/7134 completed (loss: 0.0639733225107193, acc: 0.9826732873916626)
[2025-02-13 03:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52][root][INFO] - Training Epoch: 2/2, step 1565/7134 completed (loss: 0.044067684561014175, acc: 0.9815126061439514)
[2025-02-13 03:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52][root][INFO] - Training Epoch: 2/2, step 1566/7134 completed (loss: 0.0295407697558403, acc: 0.9890561103820801)
[2025-02-13 03:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:53][root][INFO] - Training Epoch: 2/2, step 1567/7134 completed (loss: 0.03449348360300064, acc: 0.9912717938423157)
[2025-02-13 03:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:53][root][INFO] - Training Epoch: 2/2, step 1568/7134 completed (loss: 0.028818245977163315, acc: 0.9905325174331665)
[2025-02-13 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54][root][INFO] - Training Epoch: 2/2, step 1569/7134 completed (loss: 0.027217792347073555, acc: 0.9926900863647461)
[2025-02-13 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54][root][INFO] - Training Epoch: 2/2, step 1570/7134 completed (loss: 0.0670774057507515, acc: 0.9878721237182617)
[2025-02-13 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55][root][INFO] - Training Epoch: 2/2, step 1571/7134 completed (loss: 0.035373859107494354, acc: 0.991696298122406)
[2025-02-13 03:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55][root][INFO] - Training Epoch: 2/2, step 1572/7134 completed (loss: 0.033022090792655945, acc: 0.9882214665412903)
[2025-02-13 03:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:56][root][INFO] - Training Epoch: 2/2, step 1573/7134 completed (loss: 0.012604981660842896, acc: 0.9956458806991577)
[2025-02-13 03:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:56][root][INFO] - Training Epoch: 2/2, step 1574/7134 completed (loss: 0.042196981608867645, acc: 0.9890590906143188)
[2025-02-13 03:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57][root][INFO] - Training Epoch: 2/2, step 1575/7134 completed (loss: 0.0267495047301054, acc: 0.9955654144287109)
[2025-02-13 03:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57][root][INFO] - Training Epoch: 2/2, step 1576/7134 completed (loss: 0.03548339381814003, acc: 0.9918224215507507)
[2025-02-13 03:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58][root][INFO] - Training Epoch: 2/2, step 1577/7134 completed (loss: 0.02747957408428192, acc: 0.9893993139266968)
[2025-02-13 03:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58][root][INFO] - Training Epoch: 2/2, step 1578/7134 completed (loss: 0.02360416203737259, acc: 0.9905277490615845)
[2025-02-13 03:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59][root][INFO] - Training Epoch: 2/2, step 1579/7134 completed (loss: 0.021880071610212326, acc: 0.9953917264938354)
[2025-02-13 03:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59][root][INFO] - Training Epoch: 2/2, step 1580/7134 completed (loss: 0.045079782605171204, acc: 0.9856828451156616)
[2025-02-13 03:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59][root][INFO] - Training Epoch: 2/2, step 1581/7134 completed (loss: 0.06017661467194557, acc: 0.9861111044883728)
[2025-02-13 03:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00][root][INFO] - Training Epoch: 2/2, step 1582/7134 completed (loss: 0.02018674649298191, acc: 0.9984350800514221)
[2025-02-13 03:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00][root][INFO] - Training Epoch: 2/2, step 1583/7134 completed (loss: 0.014340388588607311, acc: 0.9942693114280701)
[2025-02-13 03:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01][root][INFO] - Training Epoch: 2/2, step 1584/7134 completed (loss: 0.02100505866110325, acc: 0.9957173466682434)
[2025-02-13 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01][root][INFO] - Training Epoch: 2/2, step 1585/7134 completed (loss: 0.014150542207062244, acc: 0.9964664578437805)
[2025-02-13 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02][root][INFO] - Training Epoch: 2/2, step 1586/7134 completed (loss: 0.04524429515004158, acc: 0.9904988408088684)
[2025-02-13 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02][root][INFO] - Training Epoch: 2/2, step 1587/7134 completed (loss: 0.020822452381253242, acc: 0.9937205910682678)
[2025-02-13 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03][root][INFO] - Training Epoch: 2/2, step 1588/7134 completed (loss: 0.03377258777618408, acc: 0.9933422207832336)
[2025-02-13 03:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03][root][INFO] - Training Epoch: 2/2, step 1589/7134 completed (loss: 0.03612712770700455, acc: 0.991183876991272)
[2025-02-13 03:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03][root][INFO] - Training Epoch: 2/2, step 1590/7134 completed (loss: 0.024245155975222588, acc: 0.9924471378326416)
[2025-02-13 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:04][root][INFO] - Training Epoch: 2/2, step 1591/7134 completed (loss: 0.025613851845264435, acc: 0.9915397763252258)
[2025-02-13 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:04][root][INFO] - Training Epoch: 2/2, step 1592/7134 completed (loss: 0.018298352137207985, acc: 0.9942938685417175)
[2025-02-13 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05][root][INFO] - Training Epoch: 2/2, step 1593/7134 completed (loss: 0.0373065248131752, acc: 0.9908046126365662)
[2025-02-13 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05][root][INFO] - Training Epoch: 2/2, step 1594/7134 completed (loss: 0.015854863449931145, acc: 0.9950413107872009)
[2025-02-13 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:06][root][INFO] - Training Epoch: 2/2, step 1595/7134 completed (loss: 0.01171108614653349, acc: 0.9946236610412598)
[2025-02-13 03:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:06][root][INFO] - Training Epoch: 2/2, step 1596/7134 completed (loss: 0.046308793127536774, acc: 0.98740154504776)
[2025-02-13 03:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:07][root][INFO] - Training Epoch: 2/2, step 1597/7134 completed (loss: 0.008823857642710209, acc: 0.9971346855163574)
[2025-02-13 03:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:07][root][INFO] - Training Epoch: 2/2, step 1598/7134 completed (loss: 0.06041109189391136, acc: 0.9820359349250793)
[2025-02-13 03:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:08][root][INFO] - Training Epoch: 2/2, step 1599/7134 completed (loss: 0.08764872699975967, acc: 0.9785124063491821)
[2025-02-13 03:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:08][root][INFO] - Training Epoch: 2/2, step 1600/7134 completed (loss: 0.04697268828749657, acc: 0.9896073937416077)
[2025-02-13 03:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09][root][INFO] - Training Epoch: 2/2, step 1601/7134 completed (loss: 0.017232347279787064, acc: 0.993122398853302)
[2025-02-13 03:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09][root][INFO] - Training Epoch: 2/2, step 1602/7134 completed (loss: 0.037520840764045715, acc: 0.993966817855835)
[2025-02-13 03:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09][root][INFO] - Training Epoch: 2/2, step 1603/7134 completed (loss: 0.04634387418627739, acc: 0.9923273921012878)
[2025-02-13 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:10][root][INFO] - Training Epoch: 2/2, step 1604/7134 completed (loss: 0.024221958592534065, acc: 0.9933686852455139)
[2025-02-13 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:10][root][INFO] - Training Epoch: 2/2, step 1605/7134 completed (loss: 0.030997544527053833, acc: 0.9895697236061096)
[2025-02-13 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11][root][INFO] - Training Epoch: 2/2, step 1606/7134 completed (loss: 0.0207256767898798, acc: 0.9938744306564331)
[2025-02-13 03:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11][root][INFO] - Training Epoch: 2/2, step 1607/7134 completed (loss: 0.006979087833315134, acc: 0.9979633688926697)
[2025-02-13 03:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:12][root][INFO] - Training Epoch: 2/2, step 1608/7134 completed (loss: 0.006939582992345095, acc: 0.9962825179100037)
[2025-02-13 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:12][root][INFO] - Training Epoch: 2/2, step 1609/7134 completed (loss: 0.030446186661720276, acc: 0.9922118186950684)
[2025-02-13 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:12][root][INFO] - Training Epoch: 2/2, step 1610/7134 completed (loss: 0.026956181973218918, acc: 0.989983320236206)
[2025-02-13 03:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:13][root][INFO] - Training Epoch: 2/2, step 1611/7134 completed (loss: 0.0340295284986496, acc: 0.9867724776268005)
[2025-02-13 03:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:13][root][INFO] - Training Epoch: 2/2, step 1612/7134 completed (loss: 0.050233300775289536, acc: 0.9885495901107788)
[2025-02-13 03:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14][root][INFO] - Training Epoch: 2/2, step 1613/7134 completed (loss: 0.05272242799401283, acc: 0.9881423115730286)
[2025-02-13 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14][root][INFO] - Training Epoch: 2/2, step 1614/7134 completed (loss: 0.03292977809906006, acc: 0.9908257126808167)
[2025-02-13 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:15][root][INFO] - Training Epoch: 2/2, step 1615/7134 completed (loss: 0.044481389224529266, acc: 0.9879194498062134)
[2025-02-13 03:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:15][root][INFO] - Training Epoch: 2/2, step 1616/7134 completed (loss: 0.02524472214281559, acc: 0.9942280054092407)
[2025-02-13 03:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:16][root][INFO] - Training Epoch: 2/2, step 1617/7134 completed (loss: 0.04047198221087456, acc: 0.9953197836875916)
[2025-02-13 03:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:16][root][INFO] - Training Epoch: 2/2, step 1618/7134 completed (loss: 0.04841117560863495, acc: 0.9877192974090576)
[2025-02-13 03:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17][root][INFO] - Training Epoch: 2/2, step 1619/7134 completed (loss: 0.03375490754842758, acc: 0.9877384305000305)
[2025-02-13 03:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17][root][INFO] - Training Epoch: 2/2, step 1620/7134 completed (loss: 0.0498538576066494, acc: 0.9833585619926453)
[2025-02-13 03:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17][root][INFO] - Training Epoch: 2/2, step 1621/7134 completed (loss: 0.05776799097657204, acc: 0.9837837815284729)
[2025-02-13 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:18][root][INFO] - Training Epoch: 2/2, step 1622/7134 completed (loss: 0.03840760886669159, acc: 0.9944289922714233)
[2025-02-13 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:18][root][INFO] - Training Epoch: 2/2, step 1623/7134 completed (loss: 0.0483415424823761, acc: 0.9873595237731934)
[2025-02-13 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:19][root][INFO] - Training Epoch: 2/2, step 1624/7134 completed (loss: 0.04118642210960388, acc: 0.9867841601371765)
[2025-02-13 03:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:19][root][INFO] - Training Epoch: 2/2, step 1625/7134 completed (loss: 0.01688031293451786, acc: 0.995726466178894)
[2025-02-13 03:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20][root][INFO] - Training Epoch: 2/2, step 1626/7134 completed (loss: 0.0767635852098465, acc: 0.9814814925193787)
[2025-02-13 03:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20][root][INFO] - Training Epoch: 2/2, step 1627/7134 completed (loss: 0.02133435383439064, acc: 0.9927007555961609)
[2025-02-13 03:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:21][root][INFO] - Training Epoch: 2/2, step 1628/7134 completed (loss: 0.009468468837440014, acc: 0.9985422492027283)
[2025-02-13 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:21][root][INFO] - Training Epoch: 2/2, step 1629/7134 completed (loss: 0.019559023901820183, acc: 0.9890909194946289)
[2025-02-13 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:21][root][INFO] - Training Epoch: 2/2, step 1630/7134 completed (loss: 0.01568032242357731, acc: 0.995555579662323)
[2025-02-13 03:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:22][root][INFO] - Training Epoch: 2/2, step 1631/7134 completed (loss: 0.04507118836045265, acc: 0.9887459874153137)
[2025-02-13 03:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:22][root][INFO] - Training Epoch: 2/2, step 1632/7134 completed (loss: 0.011980450712144375, acc: 0.9966832399368286)
[2025-02-13 03:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:23][root][INFO] - Training Epoch: 2/2, step 1633/7134 completed (loss: 0.015578565187752247, acc: 0.9937205910682678)
[2025-02-13 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:23][root][INFO] - Training Epoch: 2/2, step 1634/7134 completed (loss: 0.007403516210615635, acc: 0.9969372153282166)
[2025-02-13 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24][root][INFO] - Training Epoch: 2/2, step 1635/7134 completed (loss: 0.013279183767735958, acc: 0.9938900470733643)
[2025-02-13 03:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24][root][INFO] - Training Epoch: 2/2, step 1636/7134 completed (loss: 0.02033812180161476, acc: 0.9942857027053833)
[2025-02-13 03:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24][root][INFO] - Training Epoch: 2/2, step 1637/7134 completed (loss: 0.010277860797941685, acc: 0.9971751570701599)
[2025-02-13 03:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:25][root][INFO] - Training Epoch: 2/2, step 1638/7134 completed (loss: 0.05225429683923721, acc: 0.9847198724746704)
[2025-02-13 03:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:25][root][INFO] - Training Epoch: 2/2, step 1639/7134 completed (loss: 0.08267565816640854, acc: 0.9822747707366943)
[2025-02-13 03:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:26][root][INFO] - Training Epoch: 2/2, step 1640/7134 completed (loss: 0.032136108726263046, acc: 0.9908536672592163)
[2025-02-13 03:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:26][root][INFO] - Training Epoch: 2/2, step 1641/7134 completed (loss: 0.00900261290371418, acc: 0.9971264600753784)
[2025-02-13 03:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:27][root][INFO] - Training Epoch: 2/2, step 1642/7134 completed (loss: 0.05017569288611412, acc: 0.9842105507850647)
[2025-02-13 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:27][root][INFO] - Training Epoch: 2/2, step 1643/7134 completed (loss: 0.06283929944038391, acc: 0.9809358716011047)
[2025-02-13 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28][root][INFO] - Training Epoch: 2/2, step 1644/7134 completed (loss: 0.042430002242326736, acc: 0.9882659912109375)
[2025-02-13 03:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28][root][INFO] - Training Epoch: 2/2, step 1645/7134 completed (loss: 0.04148467630147934, acc: 0.9870689511299133)
[2025-02-13 03:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:29][root][INFO] - Training Epoch: 2/2, step 1646/7134 completed (loss: 0.04884390905499458, acc: 0.9856230020523071)
[2025-02-13 03:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:29][root][INFO] - Training Epoch: 2/2, step 1647/7134 completed (loss: 0.017893003299832344, acc: 0.9932810664176941)
[2025-02-13 03:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:30][root][INFO] - Training Epoch: 2/2, step 1648/7134 completed (loss: 0.04913397133350372, acc: 0.9880319237709045)
[2025-02-13 03:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:30][root][INFO] - Training Epoch: 2/2, step 1649/7134 completed (loss: 0.02579440549015999, acc: 0.9922308325767517)
[2025-02-13 03:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:30][root][INFO] - Training Epoch: 2/2, step 1650/7134 completed (loss: 0.026203034445643425, acc: 0.9896907210350037)
[2025-02-13 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31][root][INFO] - Training Epoch: 2/2, step 1651/7134 completed (loss: 0.04374031350016594, acc: 0.98893803358078)
[2025-02-13 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31][root][INFO] - Training Epoch: 2/2, step 1652/7134 completed (loss: 0.02109970711171627, acc: 0.994547426700592)
[2025-02-13 03:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:32][root][INFO] - Training Epoch: 2/2, step 1653/7134 completed (loss: 0.0281886775046587, acc: 0.993773341178894)
[2025-02-13 03:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:32][root][INFO] - Training Epoch: 2/2, step 1654/7134 completed (loss: 0.020588556304574013, acc: 0.9928644299507141)
[2025-02-13 03:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33][root][INFO] - Training Epoch: 2/2, step 1655/7134 completed (loss: 0.02782292850315571, acc: 0.9900867342948914)
[2025-02-13 03:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33][root][INFO] - Training Epoch: 2/2, step 1656/7134 completed (loss: 0.022260522469878197, acc: 0.9909909963607788)
[2025-02-13 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:34][root][INFO] - Training Epoch: 2/2, step 1657/7134 completed (loss: 0.023544874042272568, acc: 0.9936072826385498)
[2025-02-13 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:34][root][INFO] - Training Epoch: 2/2, step 1658/7134 completed (loss: 0.018704859539866447, acc: 0.9928571581840515)
[2025-02-13 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:35][root][INFO] - Training Epoch: 2/2, step 1659/7134 completed (loss: 0.03286198899149895, acc: 0.9895734786987305)
[2025-02-13 03:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:35][root][INFO] - Training Epoch: 2/2, step 1660/7134 completed (loss: 0.025408277288079262, acc: 0.9951691031455994)
[2025-02-13 03:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:36][root][INFO] - Training Epoch: 2/2, step 1661/7134 completed (loss: 0.028441686183214188, acc: 0.9901531934738159)
[2025-02-13 03:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:36][root][INFO] - Training Epoch: 2/2, step 1662/7134 completed (loss: 0.025212362408638, acc: 0.9904559850692749)
[2025-02-13 03:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37][root][INFO] - Training Epoch: 2/2, step 1663/7134 completed (loss: 0.020304743200540543, acc: 0.9969167709350586)
[2025-02-13 03:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37][root][INFO] - Training Epoch: 2/2, step 1664/7134 completed (loss: 0.01041178498417139, acc: 0.9964243173599243)
[2025-02-13 03:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:38][root][INFO] - Training Epoch: 2/2, step 1665/7134 completed (loss: 0.018898440524935722, acc: 0.9942445755004883)
[2025-02-13 03:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:38][root][INFO] - Training Epoch: 2/2, step 1666/7134 completed (loss: 0.014475692063570023, acc: 0.9960079789161682)
[2025-02-13 03:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:39][root][INFO] - Training Epoch: 2/2, step 1667/7134 completed (loss: 0.019597018137574196, acc: 0.9952493906021118)
[2025-02-13 03:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:39][root][INFO] - Training Epoch: 2/2, step 1668/7134 completed (loss: 0.015875330194830894, acc: 0.9947478771209717)
[2025-02-13 03:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40][root][INFO] - Training Epoch: 2/2, step 1669/7134 completed (loss: 0.011880768463015556, acc: 0.9934086799621582)
[2025-02-13 03:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40][root][INFO] - Training Epoch: 2/2, step 1670/7134 completed (loss: 0.040088459849357605, acc: 0.989503800868988)
[2025-02-13 03:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:41][root][INFO] - Training Epoch: 2/2, step 1671/7134 completed (loss: 0.04084828868508339, acc: 0.9880525469779968)
[2025-02-13 03:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:41][root][INFO] - Training Epoch: 2/2, step 1672/7134 completed (loss: 0.0351482629776001, acc: 0.9880478382110596)
[2025-02-13 03:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:42][root][INFO] - Training Epoch: 2/2, step 1673/7134 completed (loss: 0.038198646157979965, acc: 0.9932795763015747)
[2025-02-13 03:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:42][root][INFO] - Training Epoch: 2/2, step 1674/7134 completed (loss: 0.04328296333551407, acc: 0.994413435459137)
[2025-02-13 03:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43][root][INFO] - Training Epoch: 2/2, step 1675/7134 completed (loss: 0.05599303916096687, acc: 0.9918367266654968)
[2025-02-13 03:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43][root][INFO] - Training Epoch: 2/2, step 1676/7134 completed (loss: 0.01586872898042202, acc: 0.9972375631332397)
[2025-02-13 03:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:44][root][INFO] - Training Epoch: 2/2, step 1677/7134 completed (loss: 0.054314371198415756, acc: 0.9852700233459473)
[2025-02-13 03:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:44][root][INFO] - Training Epoch: 2/2, step 1678/7134 completed (loss: 0.01536239217966795, acc: 0.9972602725028992)
[2025-02-13 03:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:45][root][INFO] - Training Epoch: 2/2, step 1679/7134 completed (loss: 0.04009346663951874, acc: 0.9882869720458984)
[2025-02-13 03:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:45][root][INFO] - Training Epoch: 2/2, step 1680/7134 completed (loss: 0.03521185368299484, acc: 0.9932975769042969)
[2025-02-13 03:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:45][root][INFO] - Training Epoch: 2/2, step 1681/7134 completed (loss: 0.02517625503242016, acc: 0.9957386255264282)
[2025-02-13 03:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46][root][INFO] - Training Epoch: 2/2, step 1682/7134 completed (loss: 0.04005303978919983, acc: 0.9899135231971741)
[2025-02-13 03:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46][root][INFO] - Training Epoch: 2/2, step 1683/7134 completed (loss: 0.02730567753314972, acc: 0.9902912378311157)
[2025-02-13 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:47][root][INFO] - Training Epoch: 2/2, step 1684/7134 completed (loss: 0.03431317210197449, acc: 0.9863013625144958)
[2025-02-13 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:47][root][INFO] - Training Epoch: 2/2, step 1685/7134 completed (loss: 0.02430705539882183, acc: 0.9916550517082214)
[2025-02-13 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48][root][INFO] - Training Epoch: 2/2, step 1686/7134 completed (loss: 0.022029167041182518, acc: 0.9932432174682617)
[2025-02-13 03:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48][root][INFO] - Training Epoch: 2/2, step 1687/7134 completed (loss: 0.016350166872143745, acc: 0.9942965507507324)
[2025-02-13 03:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49][root][INFO] - Training Epoch: 2/2, step 1688/7134 completed (loss: 0.05182274430990219, acc: 0.9907264113426208)
[2025-02-13 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49][root][INFO] - Training Epoch: 2/2, step 1689/7134 completed (loss: 0.0191622544080019, acc: 0.9956709742546082)
[2025-02-13 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49][root][INFO] - Training Epoch: 2/2, step 1690/7134 completed (loss: 0.02044953964650631, acc: 0.9959349632263184)
[2025-02-13 03:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50][root][INFO] - Training Epoch: 2/2, step 1691/7134 completed (loss: 0.009360157884657383, acc: 0.998344361782074)
[2025-02-13 03:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50][root][INFO] - Training Epoch: 2/2, step 1692/7134 completed (loss: 0.029903996735811234, acc: 0.9881955981254578)
[2025-02-13 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:51][root][INFO] - Training Epoch: 2/2, step 1693/7134 completed (loss: 0.025384405627846718, acc: 0.9955089688301086)
[2025-02-13 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:51][root][INFO] - Training Epoch: 2/2, step 1694/7134 completed (loss: 0.023137036710977554, acc: 0.9937402009963989)
[2025-02-13 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52][root][INFO] - Training Epoch: 2/2, step 1695/7134 completed (loss: 0.00894017331302166, acc: 0.9963964223861694)
[2025-02-13 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52][root][INFO] - Training Epoch: 2/2, step 1696/7134 completed (loss: 0.036580197513103485, acc: 0.9882746934890747)
[2025-02-13 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53][root][INFO] - Training Epoch: 2/2, step 1697/7134 completed (loss: 0.024814117699861526, acc: 0.9906103014945984)
[2025-02-13 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53][root][INFO] - Training Epoch: 2/2, step 1698/7134 completed (loss: 0.015870505943894386, acc: 0.9952977895736694)
[2025-02-13 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53][root][INFO] - Training Epoch: 2/2, step 1699/7134 completed (loss: 0.02503708377480507, acc: 0.9861591458320618)
[2025-02-13 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:54][root][INFO] - Training Epoch: 2/2, step 1700/7134 completed (loss: 0.028536079451441765, acc: 0.991055428981781)
[2025-02-13 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:54][root][INFO] - Training Epoch: 2/2, step 1701/7134 completed (loss: 0.020674532279372215, acc: 0.9944367408752441)
[2025-02-13 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55][root][INFO] - Training Epoch: 2/2, step 1702/7134 completed (loss: 0.0404764786362648, acc: 0.9875776171684265)
[2025-02-13 03:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55][root][INFO] - Training Epoch: 2/2, step 1703/7134 completed (loss: 0.037754837423563004, acc: 0.9917126893997192)
[2025-02-13 03:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:56][root][INFO] - Training Epoch: 2/2, step 1704/7134 completed (loss: 0.06257732957601547, acc: 0.9823369383811951)
[2025-02-13 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:56][root][INFO] - Training Epoch: 2/2, step 1705/7134 completed (loss: 0.04395347461104393, acc: 0.990138053894043)
[2025-02-13 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57][root][INFO] - Training Epoch: 2/2, step 1706/7134 completed (loss: 0.018610922619700432, acc: 0.9962121248245239)
[2025-02-13 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57][root][INFO] - Training Epoch: 2/2, step 1707/7134 completed (loss: 0.03707423061132431, acc: 0.9936575293540955)
[2025-02-13 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57][root][INFO] - Training Epoch: 2/2, step 1708/7134 completed (loss: 0.03508375212550163, acc: 0.9853658676147461)
[2025-02-13 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:58][root][INFO] - Training Epoch: 2/2, step 1709/7134 completed (loss: 0.037530187517404556, acc: 0.9879310131072998)
[2025-02-13 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:58][root][INFO] - Training Epoch: 2/2, step 1710/7134 completed (loss: 0.03216363489627838, acc: 0.9899425506591797)
[2025-02-13 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59][root][INFO] - Training Epoch: 2/2, step 1711/7134 completed (loss: 0.0372052937746048, acc: 0.9930915236473083)
[2025-02-13 03:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59][root][INFO] - Training Epoch: 2/2, step 1712/7134 completed (loss: 0.052864283323287964, acc: 0.9824561476707458)
[2025-02-13 03:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:00][root][INFO] - Training Epoch: 2/2, step 1713/7134 completed (loss: 0.0340263806283474, acc: 0.9860279560089111)
[2025-02-13 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:00][root][INFO] - Training Epoch: 2/2, step 1714/7134 completed (loss: 0.0429229736328125, acc: 0.9835526347160339)
[2025-02-13 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01][root][INFO] - Training Epoch: 2/2, step 1715/7134 completed (loss: 0.008675556629896164, acc: 0.9979715943336487)
[2025-02-13 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01][root][INFO] - Training Epoch: 2/2, step 1716/7134 completed (loss: 0.04292783886194229, acc: 0.9883333444595337)
[2025-02-13 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01][root][INFO] - Training Epoch: 2/2, step 1717/7134 completed (loss: 0.027614960446953773, acc: 0.9922178983688354)
[2025-02-13 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:02][root][INFO] - Training Epoch: 2/2, step 1718/7134 completed (loss: 0.041565537452697754, acc: 0.9844098091125488)
[2025-02-13 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:02][root][INFO] - Training Epoch: 2/2, step 1719/7134 completed (loss: 0.06084596738219261, acc: 0.9863013625144958)
[2025-02-13 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03][root][INFO] - Training Epoch: 2/2, step 1720/7134 completed (loss: 0.05488858371973038, acc: 0.9813664555549622)
[2025-02-13 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03][root][INFO] - Training Epoch: 2/2, step 1721/7134 completed (loss: 0.024664342403411865, acc: 0.9898819327354431)
[2025-02-13 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03][root][INFO] - Training Epoch: 2/2, step 1722/7134 completed (loss: 0.01995391957461834, acc: 0.99210524559021)
[2025-02-13 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04][root][INFO] - Training Epoch: 2/2, step 1723/7134 completed (loss: 0.06290363520383835, acc: 0.9865871667861938)
[2025-02-13 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04][root][INFO] - Training Epoch: 2/2, step 1724/7134 completed (loss: 0.06081483140587807, acc: 0.9811617136001587)
[2025-02-13 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:05][root][INFO] - Training Epoch: 2/2, step 1725/7134 completed (loss: 0.008349362760782242, acc: 1.0)
[2025-02-13 03:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:05][root][INFO] - Training Epoch: 2/2, step 1726/7134 completed (loss: 0.030073070898652077, acc: 0.9955157041549683)
[2025-02-13 03:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06][root][INFO] - Training Epoch: 2/2, step 1727/7134 completed (loss: 0.02190430276095867, acc: 0.9963503479957581)
[2025-02-13 03:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06][root][INFO] - Training Epoch: 2/2, step 1728/7134 completed (loss: 0.013203472830355167, acc: 0.996688723564148)
[2025-02-13 03:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06][root][INFO] - Training Epoch: 2/2, step 1729/7134 completed (loss: 0.059507787227630615, acc: 0.9898167252540588)
[2025-02-13 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:07][root][INFO] - Training Epoch: 2/2, step 1730/7134 completed (loss: 0.012258741073310375, acc: 0.9957173466682434)
[2025-02-13 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:07][root][INFO] - Training Epoch: 2/2, step 1731/7134 completed (loss: 0.04966865852475166, acc: 0.9942196607589722)
[2025-02-13 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:08][root][INFO] - Training Epoch: 2/2, step 1732/7134 completed (loss: 0.02574072778224945, acc: 0.9938398599624634)
[2025-02-13 03:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:08][root][INFO] - Training Epoch: 2/2, step 1733/7134 completed (loss: 0.022171935066580772, acc: 0.991769552230835)
[2025-02-13 03:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09][root][INFO] - Training Epoch: 2/2, step 1734/7134 completed (loss: 0.036704253405332565, acc: 0.9889435172080994)
[2025-02-13 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09][root][INFO] - Training Epoch: 2/2, step 1735/7134 completed (loss: 0.010713519528508186, acc: 0.9971949458122253)
[2025-02-13 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09][root][INFO] - Training Epoch: 2/2, step 1736/7134 completed (loss: 0.031506992876529694, acc: 0.9915151596069336)
[2025-02-13 03:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10][root][INFO] - Training Epoch: 2/2, step 1737/7134 completed (loss: 0.02842494659125805, acc: 0.9943820238113403)
[2025-02-13 03:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10][root][INFO] - Training Epoch: 2/2, step 1738/7134 completed (loss: 0.014125334098935127, acc: 0.9951748847961426)
[2025-02-13 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:11][root][INFO] - Training Epoch: 2/2, step 1739/7134 completed (loss: 0.038111623376607895, acc: 0.985029935836792)
[2025-02-13 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:11][root][INFO] - Training Epoch: 2/2, step 1740/7134 completed (loss: 0.01602139323949814, acc: 0.9941520690917969)
[2025-02-13 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:12][root][INFO] - Training Epoch: 2/2, step 1741/7134 completed (loss: 0.0424441397190094, acc: 0.9894179701805115)
[2025-02-13 03:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:12][root][INFO] - Training Epoch: 2/2, step 1742/7134 completed (loss: 0.027135653421282768, acc: 0.9929078221321106)
[2025-02-13 03:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13][root][INFO] - Training Epoch: 2/2, step 1743/7134 completed (loss: 0.04252644628286362, acc: 0.9915966391563416)
[2025-02-13 03:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13][root][INFO] - Training Epoch: 2/2, step 1744/7134 completed (loss: 0.02317330799996853, acc: 0.9967426657676697)
[2025-02-13 03:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:14][root][INFO] - Training Epoch: 2/2, step 1745/7134 completed (loss: 0.01827871799468994, acc: 0.9962406158447266)
[2025-02-13 03:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:14][root][INFO] - Training Epoch: 2/2, step 1746/7134 completed (loss: 0.00848312396556139, acc: 0.9970930218696594)
[2025-02-13 03:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:15][root][INFO] - Training Epoch: 2/2, step 1747/7134 completed (loss: 0.024716760963201523, acc: 0.9937421679496765)
[2025-02-13 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:15][root][INFO] - Training Epoch: 2/2, step 1748/7134 completed (loss: 0.023723626509308815, acc: 0.9900744557380676)
[2025-02-13 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16][root][INFO] - Training Epoch: 2/2, step 1749/7134 completed (loss: 0.027834737673401833, acc: 0.9909194111824036)
[2025-02-13 03:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16][root][INFO] - Training Epoch: 2/2, step 1750/7134 completed (loss: 0.029825899749994278, acc: 0.9940047860145569)
[2025-02-13 03:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16][root][INFO] - Training Epoch: 2/2, step 1751/7134 completed (loss: 0.03854808583855629, acc: 0.9854545593261719)
[2025-02-13 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:17][root][INFO] - Training Epoch: 2/2, step 1752/7134 completed (loss: 0.04650389775633812, acc: 0.9858247637748718)
[2025-02-13 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:17][root][INFO] - Training Epoch: 2/2, step 1753/7134 completed (loss: 0.043659985065460205, acc: 0.9869961142539978)
[2025-02-13 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:18][root][INFO] - Training Epoch: 2/2, step 1754/7134 completed (loss: 0.047395385801792145, acc: 0.9884726405143738)
[2025-02-13 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:18][root][INFO] - Training Epoch: 2/2, step 1755/7134 completed (loss: 0.01117901224642992, acc: 0.9960317611694336)
[2025-02-13 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19][root][INFO] - Training Epoch: 2/2, step 1756/7134 completed (loss: 0.02869892492890358, acc: 0.9934980273246765)
[2025-02-13 03:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19][root][INFO] - Training Epoch: 2/2, step 1757/7134 completed (loss: 0.034519605338573456, acc: 0.9883527159690857)
[2025-02-13 03:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:20][root][INFO] - Training Epoch: 2/2, step 1758/7134 completed (loss: 0.013082576915621758, acc: 0.9949495196342468)
[2025-02-13 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:20][root][INFO] - Training Epoch: 2/2, step 1759/7134 completed (loss: 0.0210705678910017, acc: 0.9916550517082214)
[2025-02-13 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21][root][INFO] - Training Epoch: 2/2, step 1760/7134 completed (loss: 0.009355453774333, acc: 1.0)
[2025-02-13 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21][root][INFO] - Training Epoch: 2/2, step 1761/7134 completed (loss: 0.016690248623490334, acc: 0.9957386255264282)
[2025-02-13 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21][root][INFO] - Training Epoch: 2/2, step 1762/7134 completed (loss: 0.012442892417311668, acc: 0.9979381561279297)
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22][root][INFO] - Training Epoch: 2/2, step 1763/7134 completed (loss: 0.022618263959884644, acc: 0.9919871687889099)
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22][root][INFO] - Training Epoch: 2/2, step 1764/7134 completed (loss: 0.014617579989135265, acc: 0.9962825179100037)
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:23][root][INFO] - Training Epoch: 2/2, step 1765/7134 completed (loss: 0.00969766452908516, acc: 0.9969135522842407)
[2025-02-13 03:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:23][root][INFO] - Training Epoch: 2/2, step 1766/7134 completed (loss: 0.013952997513115406, acc: 0.9970674514770508)
[2025-02-13 03:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24][root][INFO] - Training Epoch: 2/2, step 1767/7134 completed (loss: 0.03944207355380058, acc: 0.9900990128517151)
[2025-02-13 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24][root][INFO] - Training Epoch: 2/2, step 1768/7134 completed (loss: 0.011737703345716, acc: 0.9942196607589722)
[2025-02-13 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25][root][INFO] - Training Epoch: 2/2, step 1769/7134 completed (loss: 0.014915666542947292, acc: 0.9970588088035583)
[2025-02-13 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25][root][INFO] - Training Epoch: 2/2, step 1770/7134 completed (loss: 0.026834912598133087, acc: 0.9908952713012695)
[2025-02-13 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25][root][INFO] - Training Epoch: 2/2, step 1771/7134 completed (loss: 0.007871403358876705, acc: 0.998603343963623)
[2025-02-13 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26][root][INFO] - Training Epoch: 2/2, step 1772/7134 completed (loss: 0.018235519528388977, acc: 0.9949748516082764)
[2025-02-13 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26][root][INFO] - Training Epoch: 2/2, step 1773/7134 completed (loss: 0.010762385092675686, acc: 0.9956395626068115)
[2025-02-13 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:27][root][INFO] - Training Epoch: 2/2, step 1774/7134 completed (loss: 0.015317186713218689, acc: 0.9950166344642639)
[2025-02-13 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:27][root][INFO] - Training Epoch: 2/2, step 1775/7134 completed (loss: 0.0035096544306725264, acc: 1.0)
[2025-02-13 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28][root][INFO] - Training Epoch: 2/2, step 1776/7134 completed (loss: 0.017874404788017273, acc: 0.9940119981765747)
[2025-02-13 03:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28][root][INFO] - Training Epoch: 2/2, step 1777/7134 completed (loss: 0.002853587968274951, acc: 1.0)
[2025-02-13 03:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28][root][INFO] - Training Epoch: 2/2, step 1778/7134 completed (loss: 0.039521683007478714, acc: 0.9940828680992126)
[2025-02-13 03:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29][root][INFO] - Training Epoch: 2/2, step 1779/7134 completed (loss: 0.018147069960832596, acc: 0.9959595799446106)
[2025-02-13 03:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29][root][INFO] - Training Epoch: 2/2, step 1780/7134 completed (loss: 0.02668985351920128, acc: 0.9957567453384399)
[2025-02-13 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:57][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0473, device='cuda:0') eval_epoch_loss=tensor(0.0462, device='cuda:0') eval_epoch_acc=tensor(0.9878, device='cuda:0')
[2025-02-13 03:52:57][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:52:57][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:52:57][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_1781_loss_0.046216245740652084/model.pt
[2025-02-13 03:52:57][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:52:57][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.046216245740652084
[2025-02-13 03:52:57][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9878435134887695
[2025-02-13 03:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:58][root][INFO] - Training Epoch: 2/2, step 1781/7134 completed (loss: 0.011214702390134335, acc: 0.9938931465148926)
[2025-02-13 03:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:58][root][INFO] - Training Epoch: 2/2, step 1782/7134 completed (loss: 0.008165398612618446, acc: 0.9984779357910156)
[2025-02-13 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59][root][INFO] - Training Epoch: 2/2, step 1783/7134 completed (loss: 0.023296864703297615, acc: 0.995945930480957)
[2025-02-13 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59][root][INFO] - Training Epoch: 2/2, step 1784/7134 completed (loss: 0.016377201303839684, acc: 0.9970674514770508)
[2025-02-13 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:00][root][INFO] - Training Epoch: 2/2, step 1785/7134 completed (loss: 0.025699511170387268, acc: 0.9917469024658203)
[2025-02-13 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:00][root][INFO] - Training Epoch: 2/2, step 1786/7134 completed (loss: 0.012892304919660091, acc: 0.9968798756599426)
[2025-02-13 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:01][root][INFO] - Training Epoch: 2/2, step 1787/7134 completed (loss: 0.01359539944678545, acc: 0.9946428537368774)
[2025-02-13 03:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:01][root][INFO] - Training Epoch: 2/2, step 1788/7134 completed (loss: 0.025302721187472343, acc: 0.9918166995048523)
[2025-02-13 03:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02][root][INFO] - Training Epoch: 2/2, step 1789/7134 completed (loss: 0.008649200201034546, acc: 0.996927797794342)
[2025-02-13 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02][root][INFO] - Training Epoch: 2/2, step 1790/7134 completed (loss: 0.09938081353902817, acc: 0.988727867603302)
[2025-02-13 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02][root][INFO] - Training Epoch: 2/2, step 1791/7134 completed (loss: 0.08857747912406921, acc: 0.984308123588562)
[2025-02-13 03:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:03][root][INFO] - Training Epoch: 2/2, step 1792/7134 completed (loss: 0.14628095924854279, acc: 0.9769647717475891)
[2025-02-13 03:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:03][root][INFO] - Training Epoch: 2/2, step 1793/7134 completed (loss: 0.1064520850777626, acc: 0.9824324250221252)
[2025-02-13 03:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:04][root][INFO] - Training Epoch: 2/2, step 1794/7134 completed (loss: 0.07026593387126923, acc: 0.9842767119407654)
[2025-02-13 03:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:04][root][INFO] - Training Epoch: 2/2, step 1795/7134 completed (loss: 0.10711375623941422, acc: 0.9769503474235535)
[2025-02-13 03:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:05][root][INFO] - Training Epoch: 2/2, step 1796/7134 completed (loss: 0.07504010945558548, acc: 0.9733542203903198)
[2025-02-13 03:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:05][root][INFO] - Training Epoch: 2/2, step 1797/7134 completed (loss: 0.04091838747262955, acc: 0.9887955188751221)
[2025-02-13 03:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06][root][INFO] - Training Epoch: 2/2, step 1798/7134 completed (loss: 0.06755458563566208, acc: 0.9710424542427063)
[2025-02-13 03:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06][root][INFO] - Training Epoch: 2/2, step 1799/7134 completed (loss: 0.07038667798042297, acc: 0.9824120402336121)
[2025-02-13 03:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:07][root][INFO] - Training Epoch: 2/2, step 1800/7134 completed (loss: 0.07841445505619049, acc: 0.980140209197998)
[2025-02-13 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:07][root][INFO] - Training Epoch: 2/2, step 1801/7134 completed (loss: 0.05963533744215965, acc: 0.9872881174087524)
[2025-02-13 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:07][root][INFO] - Training Epoch: 2/2, step 1802/7134 completed (loss: 0.0647139921784401, acc: 0.9824780821800232)
[2025-02-13 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:08][root][INFO] - Training Epoch: 2/2, step 1803/7134 completed (loss: 0.04130139946937561, acc: 0.9892141819000244)
[2025-02-13 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:08][root][INFO] - Training Epoch: 2/2, step 1804/7134 completed (loss: 0.031458530575037, acc: 0.9840294718742371)
[2025-02-13 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09][root][INFO] - Training Epoch: 2/2, step 1805/7134 completed (loss: 0.041668374091386795, acc: 0.9870129823684692)
[2025-02-13 03:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09][root][INFO] - Training Epoch: 2/2, step 1806/7134 completed (loss: 0.03660157695412636, acc: 0.9934210777282715)
[2025-02-13 03:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:10][root][INFO] - Training Epoch: 2/2, step 1807/7134 completed (loss: 0.04611145704984665, acc: 0.9844412803649902)
[2025-02-13 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:10][root][INFO] - Training Epoch: 2/2, step 1808/7134 completed (loss: 0.03161826357245445, acc: 0.9947984218597412)
[2025-02-13 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:11][root][INFO] - Training Epoch: 2/2, step 1809/7134 completed (loss: 0.00936172716319561, acc: 0.9986486434936523)
[2025-02-13 03:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:11][root][INFO] - Training Epoch: 2/2, step 1810/7134 completed (loss: 0.022194065153598785, acc: 0.9974226951599121)
[2025-02-13 03:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12][root][INFO] - Training Epoch: 2/2, step 1811/7134 completed (loss: 0.031078245490789413, acc: 0.9891451597213745)
[2025-02-13 03:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12][root][INFO] - Training Epoch: 2/2, step 1812/7134 completed (loss: 0.031969644129276276, acc: 0.9910714030265808)
[2025-02-13 03:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12][root][INFO] - Training Epoch: 2/2, step 1813/7134 completed (loss: 0.03140263631939888, acc: 0.9880239367485046)
[2025-02-13 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:13][root][INFO] - Training Epoch: 2/2, step 1814/7134 completed (loss: 0.01890740543603897, acc: 0.9941725134849548)
[2025-02-13 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:13][root][INFO] - Training Epoch: 2/2, step 1815/7134 completed (loss: 0.019070835784077644, acc: 0.9928160905838013)
[2025-02-13 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:14][root][INFO] - Training Epoch: 2/2, step 1816/7134 completed (loss: 0.02968386746942997, acc: 0.9937402009963989)
[2025-02-13 03:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:14][root][INFO] - Training Epoch: 2/2, step 1817/7134 completed (loss: 0.027657153084874153, acc: 0.9916467666625977)
[2025-02-13 03:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15][root][INFO] - Training Epoch: 2/2, step 1818/7134 completed (loss: 0.048568304628133774, acc: 0.9888476133346558)
[2025-02-13 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15][root][INFO] - Training Epoch: 2/2, step 1819/7134 completed (loss: 0.015153652057051659, acc: 0.9925187230110168)
[2025-02-13 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:16][root][INFO] - Training Epoch: 2/2, step 1820/7134 completed (loss: 0.01633089780807495, acc: 0.9945414662361145)
[2025-02-13 03:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:16][root][INFO] - Training Epoch: 2/2, step 1821/7134 completed (loss: 0.02641802467405796, acc: 0.9923413395881653)
[2025-02-13 03:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17][root][INFO] - Training Epoch: 2/2, step 1822/7134 completed (loss: 0.007872408255934715, acc: 0.9970760345458984)
[2025-02-13 03:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17][root][INFO] - Training Epoch: 2/2, step 1823/7134 completed (loss: 0.01578827202320099, acc: 0.9972714781761169)
[2025-02-13 03:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17][root][INFO] - Training Epoch: 2/2, step 1824/7134 completed (loss: 0.014396639540791512, acc: 0.9962916970252991)
[2025-02-13 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:18][root][INFO] - Training Epoch: 2/2, step 1825/7134 completed (loss: 0.01159210130572319, acc: 0.9955157041549683)
[2025-02-13 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:18][root][INFO] - Training Epoch: 2/2, step 1826/7134 completed (loss: 0.013595850206911564, acc: 0.9946164488792419)
[2025-02-13 03:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19][root][INFO] - Training Epoch: 2/2, step 1827/7134 completed (loss: 0.03571639209985733, acc: 0.9922822713851929)
[2025-02-13 03:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19][root][INFO] - Training Epoch: 2/2, step 1828/7134 completed (loss: 0.005637603346258402, acc: 0.9987130165100098)
[2025-02-13 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:20][root][INFO] - Training Epoch: 2/2, step 1829/7134 completed (loss: 0.019697686657309532, acc: 0.992546558380127)
[2025-02-13 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:20][root][INFO] - Training Epoch: 2/2, step 1830/7134 completed (loss: 0.01310024969279766, acc: 0.9963680505752563)
[2025-02-13 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:21][root][INFO] - Training Epoch: 2/2, step 1831/7134 completed (loss: 0.010047796182334423, acc: 0.996874988079071)
[2025-02-13 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:21][root][INFO] - Training Epoch: 2/2, step 1832/7134 completed (loss: 0.0662827119231224, acc: 0.9876325130462646)
[2025-02-13 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22][root][INFO] - Training Epoch: 2/2, step 1833/7134 completed (loss: 0.13720551133155823, acc: 0.9661733508110046)
[2025-02-13 03:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22][root][INFO] - Training Epoch: 2/2, step 1834/7134 completed (loss: 0.04006616398692131, acc: 0.9901408553123474)
[2025-02-13 03:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22][root][INFO] - Training Epoch: 2/2, step 1835/7134 completed (loss: 0.0549028180539608, acc: 0.9883720874786377)
[2025-02-13 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:23][root][INFO] - Training Epoch: 2/2, step 1836/7134 completed (loss: 0.030118428170681, acc: 0.9905437231063843)
[2025-02-13 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:23][root][INFO] - Training Epoch: 2/2, step 1837/7134 completed (loss: 0.020342925563454628, acc: 0.9926289916038513)
[2025-02-13 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:24][root][INFO] - Training Epoch: 2/2, step 1838/7134 completed (loss: 0.006707904860377312, acc: 0.9979591965675354)
[2025-02-13 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:24][root][INFO] - Training Epoch: 2/2, step 1839/7134 completed (loss: 0.017731687054038048, acc: 0.9935232996940613)
[2025-02-13 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:25][root][INFO] - Training Epoch: 2/2, step 1840/7134 completed (loss: 0.024816084653139114, acc: 0.9902371168136597)
[2025-02-13 03:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:25][root][INFO] - Training Epoch: 2/2, step 1841/7134 completed (loss: 0.027948671951889992, acc: 0.9872340559959412)
[2025-02-13 03:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26][root][INFO] - Training Epoch: 2/2, step 1842/7134 completed (loss: 0.035346619784832, acc: 0.9904761910438538)
[2025-02-13 03:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26][root][INFO] - Training Epoch: 2/2, step 1843/7134 completed (loss: 0.037123903632164, acc: 0.9900000095367432)
[2025-02-13 03:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26][root][INFO] - Training Epoch: 2/2, step 1844/7134 completed (loss: 0.032738424837589264, acc: 0.9918699264526367)
[2025-02-13 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:27][root][INFO] - Training Epoch: 2/2, step 1845/7134 completed (loss: 0.02508208528161049, acc: 0.9979122877120972)
[2025-02-13 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:27][root][INFO] - Training Epoch: 2/2, step 1846/7134 completed (loss: 0.01258340384811163, acc: 0.9985755085945129)
[2025-02-13 03:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28][root][INFO] - Training Epoch: 2/2, step 1847/7134 completed (loss: 0.015373565256595612, acc: 0.996927797794342)
[2025-02-13 03:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28][root][INFO] - Training Epoch: 2/2, step 1848/7134 completed (loss: 0.038256317377090454, acc: 0.9872881174087524)
[2025-02-13 03:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:29][root][INFO] - Training Epoch: 2/2, step 1849/7134 completed (loss: 0.0523117296397686, acc: 0.9859402179718018)
[2025-02-13 03:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:29][root][INFO] - Training Epoch: 2/2, step 1850/7134 completed (loss: 0.024693796411156654, acc: 0.9902200698852539)
[2025-02-13 03:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:30][root][INFO] - Training Epoch: 2/2, step 1851/7134 completed (loss: 0.04056843742728233, acc: 0.9865689873695374)
[2025-02-13 03:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:30][root][INFO] - Training Epoch: 2/2, step 1852/7134 completed (loss: 0.028655629605054855, acc: 0.989130437374115)
[2025-02-13 03:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31][root][INFO] - Training Epoch: 2/2, step 1853/7134 completed (loss: 0.039803020656108856, acc: 0.9928229451179504)
[2025-02-13 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31][root][INFO] - Training Epoch: 2/2, step 1854/7134 completed (loss: 0.03612714260816574, acc: 0.9857312440872192)
[2025-02-13 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31][root][INFO] - Training Epoch: 2/2, step 1855/7134 completed (loss: 0.03867766633629799, acc: 0.9872832298278809)
[2025-02-13 03:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:32][root][INFO] - Training Epoch: 2/2, step 1856/7134 completed (loss: 0.036553315818309784, acc: 0.9876998662948608)
[2025-02-13 03:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:32][root][INFO] - Training Epoch: 2/2, step 1857/7134 completed (loss: 0.043595731258392334, acc: 0.9852216839790344)
[2025-02-13 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33][root][INFO] - Training Epoch: 2/2, step 1858/7134 completed (loss: 0.021864278241991997, acc: 0.9916267991065979)
[2025-02-13 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33][root][INFO] - Training Epoch: 2/2, step 1859/7134 completed (loss: 0.01791437342762947, acc: 0.9954493641853333)
[2025-02-13 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:34][root][INFO] - Training Epoch: 2/2, step 1860/7134 completed (loss: 0.014041903428733349, acc: 0.9936708807945251)
[2025-02-13 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:34][root][INFO] - Training Epoch: 2/2, step 1861/7134 completed (loss: 0.039902638643980026, acc: 0.9922480583190918)
[2025-02-13 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35][root][INFO] - Training Epoch: 2/2, step 1862/7134 completed (loss: 0.01926479861140251, acc: 0.9959294199943542)
[2025-02-13 03:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35][root][INFO] - Training Epoch: 2/2, step 1863/7134 completed (loss: 0.03346935287117958, acc: 0.9928057789802551)
[2025-02-13 03:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36][root][INFO] - Training Epoch: 2/2, step 1864/7134 completed (loss: 0.11728349328041077, acc: 0.9782313108444214)
[2025-02-13 03:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36][root][INFO] - Training Epoch: 2/2, step 1865/7134 completed (loss: 0.026958612725138664, acc: 0.989847719669342)
[2025-02-13 03:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36][root][INFO] - Training Epoch: 2/2, step 1866/7134 completed (loss: 0.007506062742322683, acc: 0.9971988797187805)
[2025-02-13 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:37][root][INFO] - Training Epoch: 2/2, step 1867/7134 completed (loss: 0.020778218284249306, acc: 0.9936948418617249)
[2025-02-13 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:37][root][INFO] - Training Epoch: 2/2, step 1868/7134 completed (loss: 0.0503680594265461, acc: 0.9884488582611084)
[2025-02-13 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:38][root][INFO] - Training Epoch: 2/2, step 1869/7134 completed (loss: 0.030789591372013092, acc: 0.9918434023857117)
[2025-02-13 03:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:38][root][INFO] - Training Epoch: 2/2, step 1870/7134 completed (loss: 0.007190680596977472, acc: 0.9986979365348816)
[2025-02-13 03:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:39][root][INFO] - Training Epoch: 2/2, step 1871/7134 completed (loss: 0.05862129479646683, acc: 0.990212082862854)
[2025-02-13 03:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:39][root][INFO] - Training Epoch: 2/2, step 1872/7134 completed (loss: 0.36093178391456604, acc: 0.9116021990776062)
[2025-02-13 03:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40][root][INFO] - Training Epoch: 2/2, step 1873/7134 completed (loss: 0.1290493905544281, acc: 0.9643705487251282)
[2025-02-13 03:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40][root][INFO] - Training Epoch: 2/2, step 1874/7134 completed (loss: 0.027691902592778206, acc: 0.9960681796073914)
[2025-02-13 03:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41][root][INFO] - Training Epoch: 2/2, step 1875/7134 completed (loss: 0.02131275273859501, acc: 0.9956331849098206)
[2025-02-13 03:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41][root][INFO] - Training Epoch: 2/2, step 1876/7134 completed (loss: 0.10810066014528275, acc: 0.9643463492393494)
[2025-02-13 03:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41][root][INFO] - Training Epoch: 2/2, step 1877/7134 completed (loss: 0.03249964863061905, acc: 0.988664984703064)
[2025-02-13 03:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:42][root][INFO] - Training Epoch: 2/2, step 1878/7134 completed (loss: 0.04265432432293892, acc: 0.9830508232116699)
[2025-02-13 03:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:42][root][INFO] - Training Epoch: 2/2, step 1879/7134 completed (loss: 0.07556744664907455, acc: 0.9787928462028503)
[2025-02-13 03:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43][root][INFO] - Training Epoch: 2/2, step 1880/7134 completed (loss: 0.05136619135737419, acc: 0.98591548204422)
[2025-02-13 03:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43][root][INFO] - Training Epoch: 2/2, step 1881/7134 completed (loss: 0.06935594975948334, acc: 0.9803921580314636)
[2025-02-13 03:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43][root][INFO] - Training Epoch: 2/2, step 1882/7134 completed (loss: 0.027254601940512657, acc: 0.9852216839790344)
[2025-02-13 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:44][root][INFO] - Training Epoch: 2/2, step 1883/7134 completed (loss: 0.03747471049427986, acc: 0.9901574850082397)
[2025-02-13 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:44][root][INFO] - Training Epoch: 2/2, step 1884/7134 completed (loss: 0.0660829097032547, acc: 0.9763033390045166)
[2025-02-13 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45][root][INFO] - Training Epoch: 2/2, step 1885/7134 completed (loss: 0.054787952452898026, acc: 0.9756097793579102)
[2025-02-13 03:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45][root][INFO] - Training Epoch: 2/2, step 1886/7134 completed (loss: 0.07527057826519012, acc: 0.9791231751441956)
[2025-02-13 03:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:46][root][INFO] - Training Epoch: 2/2, step 1887/7134 completed (loss: 0.040623344480991364, acc: 0.9867172837257385)
[2025-02-13 03:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:46][root][INFO] - Training Epoch: 2/2, step 1888/7134 completed (loss: 0.05306260287761688, acc: 0.9884836673736572)
[2025-02-13 03:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47][root][INFO] - Training Epoch: 2/2, step 1889/7134 completed (loss: 0.016600457951426506, acc: 0.9913294911384583)
[2025-02-13 03:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47][root][INFO] - Training Epoch: 2/2, step 1890/7134 completed (loss: 0.029528915882110596, acc: 0.99071204662323)
[2025-02-13 03:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47][root][INFO] - Training Epoch: 2/2, step 1891/7134 completed (loss: 0.07360764592885971, acc: 0.9772727489471436)
[2025-02-13 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:48][root][INFO] - Training Epoch: 2/2, step 1892/7134 completed (loss: 0.03571389615535736, acc: 0.9876352548599243)
[2025-02-13 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:48][root][INFO] - Training Epoch: 2/2, step 1893/7134 completed (loss: 0.032167356461286545, acc: 0.9875195026397705)
[2025-02-13 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:49][root][INFO] - Training Epoch: 2/2, step 1894/7134 completed (loss: 0.0736410990357399, acc: 0.9756944179534912)
[2025-02-13 03:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:49][root][INFO] - Training Epoch: 2/2, step 1895/7134 completed (loss: 0.175714910030365, acc: 0.9575163125991821)
[2025-02-13 03:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:49][root][INFO] - Training Epoch: 2/2, step 1896/7134 completed (loss: 0.02962585724890232, acc: 0.9882352948188782)
[2025-02-13 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50][root][INFO] - Training Epoch: 2/2, step 1897/7134 completed (loss: 0.009700928814709187, acc: 0.9981481432914734)
[2025-02-13 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50][root][INFO] - Training Epoch: 2/2, step 1898/7134 completed (loss: 0.027428915724158287, acc: 0.9900497794151306)
[2025-02-13 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:51][root][INFO] - Training Epoch: 2/2, step 1899/7134 completed (loss: 0.03319767117500305, acc: 0.9927667379379272)
[2025-02-13 03:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:51][root][INFO] - Training Epoch: 2/2, step 1900/7134 completed (loss: 0.01317074429243803, acc: 0.9967637658119202)
[2025-02-13 03:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:51][root][INFO] - Training Epoch: 2/2, step 1901/7134 completed (loss: 0.05442539602518082, acc: 0.9872029423713684)
[2025-02-13 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52][root][INFO] - Training Epoch: 2/2, step 1902/7134 completed (loss: 0.033102452754974365, acc: 0.9929701089859009)
[2025-02-13 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52][root][INFO] - Training Epoch: 2/2, step 1903/7134 completed (loss: 0.035099271684885025, acc: 0.9861111044883728)
[2025-02-13 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53][root][INFO] - Training Epoch: 2/2, step 1904/7134 completed (loss: 0.022893648594617844, acc: 0.991525411605835)
[2025-02-13 03:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53][root][INFO] - Training Epoch: 2/2, step 1905/7134 completed (loss: 0.04228045046329498, acc: 0.9897750616073608)
[2025-02-13 03:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54][root][INFO] - Training Epoch: 2/2, step 1906/7134 completed (loss: 0.04312695562839508, acc: 0.9863547682762146)
[2025-02-13 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54][root][INFO] - Training Epoch: 2/2, step 1907/7134 completed (loss: 0.023454710841178894, acc: 0.9949579834938049)
[2025-02-13 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54][root][INFO] - Training Epoch: 2/2, step 1908/7134 completed (loss: 0.023270705714821815, acc: 0.994629442691803)
[2025-02-13 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55][root][INFO] - Training Epoch: 2/2, step 1909/7134 completed (loss: 0.035843465477228165, acc: 0.9867549538612366)
[2025-02-13 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55][root][INFO] - Training Epoch: 2/2, step 1910/7134 completed (loss: 0.03046698123216629, acc: 0.9923245906829834)
[2025-02-13 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:56][root][INFO] - Training Epoch: 2/2, step 1911/7134 completed (loss: 0.01939859800040722, acc: 0.9902912378311157)
[2025-02-13 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:56][root][INFO] - Training Epoch: 2/2, step 1912/7134 completed (loss: 0.006505491677671671, acc: 0.9986684322357178)
[2025-02-13 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57][root][INFO] - Training Epoch: 2/2, step 1913/7134 completed (loss: 0.04817973077297211, acc: 0.9851577281951904)
[2025-02-13 03:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57][root][INFO] - Training Epoch: 2/2, step 1914/7134 completed (loss: 0.05350019410252571, acc: 0.9824086427688599)
[2025-02-13 03:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58][root][INFO] - Training Epoch: 2/2, step 1915/7134 completed (loss: 0.02665450982749462, acc: 0.9934747219085693)
[2025-02-13 03:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58][root][INFO] - Training Epoch: 2/2, step 1916/7134 completed (loss: 0.020649246871471405, acc: 0.9902557730674744)
[2025-02-13 03:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58][root][INFO] - Training Epoch: 2/2, step 1917/7134 completed (loss: 0.013785559684038162, acc: 0.9957982897758484)
[2025-02-13 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:59][root][INFO] - Training Epoch: 2/2, step 1918/7134 completed (loss: 0.05081799626350403, acc: 0.9905956387519836)
[2025-02-13 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:59][root][INFO] - Training Epoch: 2/2, step 1919/7134 completed (loss: 0.019316675141453743, acc: 0.9971305727958679)
[2025-02-13 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:00][root][INFO] - Training Epoch: 2/2, step 1920/7134 completed (loss: 0.013430600985884666, acc: 0.9984567761421204)
[2025-02-13 03:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:00][root][INFO] - Training Epoch: 2/2, step 1921/7134 completed (loss: 0.011982809752225876, acc: 0.9958449006080627)
[2025-02-13 03:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01][root][INFO] - Training Epoch: 2/2, step 1922/7134 completed (loss: 0.02408042550086975, acc: 0.9940357804298401)
[2025-02-13 03:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01][root][INFO] - Training Epoch: 2/2, step 1923/7134 completed (loss: 0.023286039009690285, acc: 0.9932998418807983)
[2025-02-13 03:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:02][root][INFO] - Training Epoch: 2/2, step 1924/7134 completed (loss: 0.03829940780997276, acc: 0.9892703890800476)
[2025-02-13 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:02][root][INFO] - Training Epoch: 2/2, step 1925/7134 completed (loss: 0.05627582594752312, acc: 0.9830687642097473)
[2025-02-13 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:03][root][INFO] - Training Epoch: 2/2, step 1926/7134 completed (loss: 0.008939961902797222, acc: 0.9969356656074524)
[2025-02-13 03:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:03][root][INFO] - Training Epoch: 2/2, step 1927/7134 completed (loss: 0.04495904967188835, acc: 0.99005526304245)
[2025-02-13 03:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:03][root][INFO] - Training Epoch: 2/2, step 1928/7134 completed (loss: 0.026634853333234787, acc: 0.9908814430236816)
[2025-02-13 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:04][root][INFO] - Training Epoch: 2/2, step 1929/7134 completed (loss: 0.011317923665046692, acc: 0.9957627058029175)
[2025-02-13 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:04][root][INFO] - Training Epoch: 2/2, step 1930/7134 completed (loss: 0.05099957436323166, acc: 0.9842519760131836)
[2025-02-13 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05][root][INFO] - Training Epoch: 2/2, step 1931/7134 completed (loss: 0.07689102739095688, acc: 0.984829306602478)
[2025-02-13 03:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05][root][INFO] - Training Epoch: 2/2, step 1932/7134 completed (loss: 0.06882545351982117, acc: 0.983582079410553)
[2025-02-13 03:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:06][root][INFO] - Training Epoch: 2/2, step 1933/7134 completed (loss: 0.08920358866453171, acc: 0.9803921580314636)
[2025-02-13 03:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:06][root][INFO] - Training Epoch: 2/2, step 1934/7134 completed (loss: 0.02207871526479721, acc: 0.9971751570701599)
[2025-02-13 03:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:06][root][INFO] - Training Epoch: 2/2, step 1935/7134 completed (loss: 0.06811603158712387, acc: 0.983988344669342)
[2025-02-13 03:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:07][root][INFO] - Training Epoch: 2/2, step 1936/7134 completed (loss: 0.010471207089722157, acc: 0.9968404173851013)
[2025-02-13 03:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:07][root][INFO] - Training Epoch: 2/2, step 1937/7134 completed (loss: 0.03243197873234749, acc: 0.9918830990791321)
[2025-02-13 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:08][root][INFO] - Training Epoch: 2/2, step 1938/7134 completed (loss: 0.05236201360821724, acc: 0.9857549667358398)
[2025-02-13 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:08][root][INFO] - Training Epoch: 2/2, step 1939/7134 completed (loss: 0.04621066898107529, acc: 0.9894067645072937)
[2025-02-13 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:09][root][INFO] - Training Epoch: 2/2, step 1940/7134 completed (loss: 0.05454030632972717, acc: 0.9836734533309937)
[2025-02-13 03:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:09][root][INFO] - Training Epoch: 2/2, step 1941/7134 completed (loss: 0.0468088760972023, acc: 0.9839572310447693)
[2025-02-13 03:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10][root][INFO] - Training Epoch: 2/2, step 1942/7134 completed (loss: 0.04174674302339554, acc: 0.9918699264526367)
[2025-02-13 03:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10][root][INFO] - Training Epoch: 2/2, step 1943/7134 completed (loss: 0.040605805814266205, acc: 0.9878048896789551)
[2025-02-13 03:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10][root][INFO] - Training Epoch: 2/2, step 1944/7134 completed (loss: 0.023816389963030815, acc: 0.9908592104911804)
[2025-02-13 03:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:11][root][INFO] - Training Epoch: 2/2, step 1945/7134 completed (loss: 0.026996204629540443, acc: 0.9862204790115356)
[2025-02-13 03:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:11][root][INFO] - Training Epoch: 2/2, step 1946/7134 completed (loss: 0.03296704962849617, acc: 0.9864176511764526)
[2025-02-13 03:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:12][root][INFO] - Training Epoch: 2/2, step 1947/7134 completed (loss: 0.037850040942430496, acc: 0.9828947186470032)
[2025-02-13 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:12][root][INFO] - Training Epoch: 2/2, step 1948/7134 completed (loss: 0.03819217160344124, acc: 0.9848197102546692)
[2025-02-13 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:13][root][INFO] - Training Epoch: 2/2, step 1949/7134 completed (loss: 0.024523796513676643, acc: 0.9933110475540161)
[2025-02-13 03:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:13][root][INFO] - Training Epoch: 2/2, step 1950/7134 completed (loss: 0.04177803173661232, acc: 0.9908758997917175)
[2025-02-13 03:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14][root][INFO] - Training Epoch: 2/2, step 1951/7134 completed (loss: 0.032463349401950836, acc: 0.9949832558631897)
[2025-02-13 03:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14][root][INFO] - Training Epoch: 2/2, step 1952/7134 completed (loss: 0.048521608114242554, acc: 0.9833610653877258)
[2025-02-13 03:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14][root][INFO] - Training Epoch: 2/2, step 1953/7134 completed (loss: 0.018508730456233025, acc: 0.9891641139984131)
[2025-02-13 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15][root][INFO] - Training Epoch: 2/2, step 1954/7134 completed (loss: 0.023911159485578537, acc: 0.992682933807373)
[2025-02-13 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15][root][INFO] - Training Epoch: 2/2, step 1955/7134 completed (loss: 0.04491305351257324, acc: 0.9912126660346985)
[2025-02-13 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16][root][INFO] - Training Epoch: 2/2, step 1956/7134 completed (loss: 0.008883793838322163, acc: 0.9973118305206299)
[2025-02-13 03:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16][root][INFO] - Training Epoch: 2/2, step 1957/7134 completed (loss: 0.05004497244954109, acc: 0.9888888597488403)
[2025-02-13 03:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16][root][INFO] - Training Epoch: 2/2, step 1958/7134 completed (loss: 0.020952485501766205, acc: 0.9971346855163574)
[2025-02-13 03:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17][root][INFO] - Training Epoch: 2/2, step 1959/7134 completed (loss: 0.022961275652050972, acc: 0.9978678226470947)
[2025-02-13 03:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17][root][INFO] - Training Epoch: 2/2, step 1960/7134 completed (loss: 0.010579472407698631, acc: 1.0)
[2025-02-13 03:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17][root][INFO] - Training Epoch: 2/2, step 1961/7134 completed (loss: 0.017212772741913795, acc: 0.9966555237770081)
[2025-02-13 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:18][root][INFO] - Training Epoch: 2/2, step 1962/7134 completed (loss: 0.032097529619932175, acc: 0.9916201233863831)
[2025-02-13 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:18][root][INFO] - Training Epoch: 2/2, step 1963/7134 completed (loss: 0.02104746550321579, acc: 0.9965457916259766)
[2025-02-13 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19][root][INFO] - Training Epoch: 2/2, step 1964/7134 completed (loss: 0.0363059937953949, acc: 0.9853479862213135)
[2025-02-13 03:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19][root][INFO] - Training Epoch: 2/2, step 1965/7134 completed (loss: 0.013934980146586895, acc: 0.9986053109169006)
[2025-02-13 03:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20][root][INFO] - Training Epoch: 2/2, step 1966/7134 completed (loss: 0.02426823228597641, acc: 0.9923547506332397)
[2025-02-13 03:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20][root][INFO] - Training Epoch: 2/2, step 1967/7134 completed (loss: 0.048647161573171616, acc: 0.9885350465774536)
[2025-02-13 03:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20][root][INFO] - Training Epoch: 2/2, step 1968/7134 completed (loss: 0.01576380431652069, acc: 0.9938271641731262)
[2025-02-13 03:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:21][root][INFO] - Training Epoch: 2/2, step 1969/7134 completed (loss: 0.0339231863617897, acc: 0.9906666874885559)
[2025-02-13 03:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:21][root][INFO] - Training Epoch: 2/2, step 1970/7134 completed (loss: 0.021202312782406807, acc: 0.995398759841919)
[2025-02-13 03:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:22][root][INFO] - Training Epoch: 2/2, step 1971/7134 completed (loss: 0.039284490048885345, acc: 0.988063633441925)
[2025-02-13 03:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:22][root][INFO] - Training Epoch: 2/2, step 1972/7134 completed (loss: 0.12034987658262253, acc: 0.9631449580192566)
[2025-02-13 03:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:23][root][INFO] - Training Epoch: 2/2, step 1973/7134 completed (loss: 0.0632861778140068, acc: 0.9790475964546204)
[2025-02-13 03:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:23][root][INFO] - Training Epoch: 2/2, step 1974/7134 completed (loss: 0.028323397040367126, acc: 0.9924623370170593)
[2025-02-13 03:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:23][root][INFO] - Training Epoch: 2/2, step 1975/7134 completed (loss: 0.018745634704828262, acc: 0.9954545497894287)
[2025-02-13 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24][root][INFO] - Training Epoch: 2/2, step 1976/7134 completed (loss: 0.025072872638702393, acc: 0.9932050108909607)
[2025-02-13 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24][root][INFO] - Training Epoch: 2/2, step 1977/7134 completed (loss: 0.028730131685733795, acc: 0.9927536249160767)
[2025-02-13 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:25][root][INFO] - Training Epoch: 2/2, step 1978/7134 completed (loss: 0.010390794835984707, acc: 0.9986577033996582)
[2025-02-13 03:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:25][root][INFO] - Training Epoch: 2/2, step 1979/7134 completed (loss: 0.019774524495005608, acc: 0.9921875)
[2025-02-13 03:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:26][root][INFO] - Training Epoch: 2/2, step 1980/7134 completed (loss: 0.05811697617173195, acc: 0.9843546152114868)
[2025-02-13 03:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:26][root][INFO] - Training Epoch: 2/2, step 1981/7134 completed (loss: 0.0395263135433197, acc: 0.9916083812713623)
[2025-02-13 03:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27][root][INFO] - Training Epoch: 2/2, step 1982/7134 completed (loss: 0.01812581717967987, acc: 0.9948520064353943)
[2025-02-13 03:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27][root][INFO] - Training Epoch: 2/2, step 1983/7134 completed (loss: 0.01581558771431446, acc: 0.9974554777145386)
[2025-02-13 03:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27][root][INFO] - Training Epoch: 2/2, step 1984/7134 completed (loss: 0.016241544857621193, acc: 0.9959623217582703)
[2025-02-13 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:28][root][INFO] - Training Epoch: 2/2, step 1985/7134 completed (loss: 0.02912413328886032, acc: 0.9916434288024902)
[2025-02-13 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:28][root][INFO] - Training Epoch: 2/2, step 1986/7134 completed (loss: 0.02641507051885128, acc: 0.993565022945404)
[2025-02-13 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:29][root][INFO] - Training Epoch: 2/2, step 1987/7134 completed (loss: 0.03454211726784706, acc: 0.9915013909339905)
[2025-02-13 03:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:29][root][INFO] - Training Epoch: 2/2, step 1988/7134 completed (loss: 0.03502509370446205, acc: 0.9885877370834351)
[2025-02-13 03:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30][root][INFO] - Training Epoch: 2/2, step 1989/7134 completed (loss: 0.04934235289692879, acc: 0.9904305934906006)
[2025-02-13 03:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30][root][INFO] - Training Epoch: 2/2, step 1990/7134 completed (loss: 0.025052474811673164, acc: 0.9951456189155579)
[2025-02-13 03:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30][root][INFO] - Training Epoch: 2/2, step 1991/7134 completed (loss: 0.011586795561015606, acc: 0.9966777563095093)
[2025-02-13 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:31][root][INFO] - Training Epoch: 2/2, step 1992/7134 completed (loss: 0.02832840569317341, acc: 0.9922480583190918)
[2025-02-13 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:31][root][INFO] - Training Epoch: 2/2, step 1993/7134 completed (loss: 0.010311923921108246, acc: 0.9964850544929504)
[2025-02-13 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:32][root][INFO] - Training Epoch: 2/2, step 1994/7134 completed (loss: 0.015054002404212952, acc: 0.998275876045227)
[2025-02-13 03:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:32][root][INFO] - Training Epoch: 2/2, step 1995/7134 completed (loss: 0.02197136916220188, acc: 0.9953632354736328)
[2025-02-13 03:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:33][root][INFO] - Training Epoch: 2/2, step 1996/7134 completed (loss: 0.024173934012651443, acc: 0.9917355179786682)
[2025-02-13 03:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:33][root][INFO] - Training Epoch: 2/2, step 1997/7134 completed (loss: 0.018576940521597862, acc: 0.9985443949699402)
[2025-02-13 03:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34][root][INFO] - Training Epoch: 2/2, step 1998/7134 completed (loss: 0.015118000097572803, acc: 0.9952903985977173)
[2025-02-13 03:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34][root][INFO] - Training Epoch: 2/2, step 1999/7134 completed (loss: 0.06216935068368912, acc: 0.9864176511764526)
[2025-02-13 03:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34][root][INFO] - Training Epoch: 2/2, step 2000/7134 completed (loss: 0.027670586481690407, acc: 0.9936908483505249)
[2025-02-13 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:35][root][INFO] - Training Epoch: 2/2, step 2001/7134 completed (loss: 0.021188389509916306, acc: 0.9909090995788574)
[2025-02-13 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:35][root][INFO] - Training Epoch: 2/2, step 2002/7134 completed (loss: 0.016117801889777184, acc: 0.9965217113494873)
[2025-02-13 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:36][root][INFO] - Training Epoch: 2/2, step 2003/7134 completed (loss: 0.01207483746111393, acc: 0.9979166388511658)
[2025-02-13 03:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:36][root][INFO] - Training Epoch: 2/2, step 2004/7134 completed (loss: 0.1386362761259079, acc: 0.9599156379699707)
[2025-02-13 03:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37][root][INFO] - Training Epoch: 2/2, step 2005/7134 completed (loss: 0.08151204884052277, acc: 0.9863013625144958)
[2025-02-13 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37][root][INFO] - Training Epoch: 2/2, step 2006/7134 completed (loss: 0.019599057734012604, acc: 0.9884726405143738)
[2025-02-13 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37][root][INFO] - Training Epoch: 2/2, step 2007/7134 completed (loss: 0.048494480550289154, acc: 0.9879518151283264)
[2025-02-13 03:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:38][root][INFO] - Training Epoch: 2/2, step 2008/7134 completed (loss: 0.034200165420770645, acc: 0.9875444769859314)
[2025-02-13 03:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:38][root][INFO] - Training Epoch: 2/2, step 2009/7134 completed (loss: 0.0315534882247448, acc: 0.9927797913551331)
[2025-02-13 03:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39][root][INFO] - Training Epoch: 2/2, step 2010/7134 completed (loss: 0.016727780923247337, acc: 0.994854211807251)
[2025-02-13 03:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39][root][INFO] - Training Epoch: 2/2, step 2011/7134 completed (loss: 0.01341945305466652, acc: 0.9941291809082031)
[2025-02-13 03:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:40][root][INFO] - Training Epoch: 2/2, step 2012/7134 completed (loss: 0.021213620901107788, acc: 0.9894291758537292)
[2025-02-13 03:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:40][root][INFO] - Training Epoch: 2/2, step 2013/7134 completed (loss: 0.017129193991422653, acc: 0.9959999918937683)
[2025-02-13 03:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:40][root][INFO] - Training Epoch: 2/2, step 2014/7134 completed (loss: 0.014724574983119965, acc: 0.9968503713607788)
[2025-02-13 03:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41][root][INFO] - Training Epoch: 2/2, step 2015/7134 completed (loss: 0.01880992017686367, acc: 0.9950658082962036)
[2025-02-13 03:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41][root][INFO] - Training Epoch: 2/2, step 2016/7134 completed (loss: 0.040400031954050064, acc: 0.9907692074775696)
[2025-02-13 03:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:42][root][INFO] - Training Epoch: 2/2, step 2017/7134 completed (loss: 0.06272241473197937, acc: 0.9851351380348206)
[2025-02-13 03:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:42][root][INFO] - Training Epoch: 2/2, step 2018/7134 completed (loss: 0.013605011627078056, acc: 0.9961538314819336)
[2025-02-13 03:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43][root][INFO] - Training Epoch: 2/2, step 2019/7134 completed (loss: 0.0242106094956398, acc: 0.9944827556610107)
[2025-02-13 03:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43][root][INFO] - Training Epoch: 2/2, step 2020/7134 completed (loss: 0.006038467865437269, acc: 0.9983079433441162)
[2025-02-13 03:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:44][root][INFO] - Training Epoch: 2/2, step 2021/7134 completed (loss: 0.048781126737594604, acc: 0.9896103739738464)
[2025-02-13 03:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:44][root][INFO] - Training Epoch: 2/2, step 2022/7134 completed (loss: 0.030402731150388718, acc: 0.9930675625801086)
[2025-02-13 03:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:44][root][INFO] - Training Epoch: 2/2, step 2023/7134 completed (loss: 0.022748984396457672, acc: 0.9956204295158386)
[2025-02-13 03:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45][root][INFO] - Training Epoch: 2/2, step 2024/7134 completed (loss: 0.02630828134715557, acc: 0.9972222447395325)
[2025-02-13 03:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45][root][INFO] - Training Epoch: 2/2, step 2025/7134 completed (loss: 0.023621810600161552, acc: 0.9959999918937683)
[2025-02-13 03:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:46][root][INFO] - Training Epoch: 2/2, step 2026/7134 completed (loss: 0.03804450482130051, acc: 0.9864457845687866)
[2025-02-13 03:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:46][root][INFO] - Training Epoch: 2/2, step 2027/7134 completed (loss: 0.0247398242354393, acc: 0.9933554530143738)
[2025-02-13 03:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47][root][INFO] - Training Epoch: 2/2, step 2028/7134 completed (loss: 0.037621304392814636, acc: 0.9912280440330505)
[2025-02-13 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47][root][INFO] - Training Epoch: 2/2, step 2029/7134 completed (loss: 0.02567032165825367, acc: 0.9914772510528564)
[2025-02-13 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47][root][INFO] - Training Epoch: 2/2, step 2030/7134 completed (loss: 0.04141443595290184, acc: 0.9815950989723206)
[2025-02-13 03:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:48][root][INFO] - Training Epoch: 2/2, step 2031/7134 completed (loss: 0.031063543632626534, acc: 0.9889570474624634)
[2025-02-13 03:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:48][root][INFO] - Training Epoch: 2/2, step 2032/7134 completed (loss: 0.02192864380776882, acc: 0.9959677457809448)
[2025-02-13 03:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49][root][INFO] - Training Epoch: 2/2, step 2033/7134 completed (loss: 0.016401512548327446, acc: 0.9982847571372986)
[2025-02-13 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49][root][INFO] - Training Epoch: 2/2, step 2034/7134 completed (loss: 0.0495329387485981, acc: 0.9877488613128662)
[2025-02-13 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:50][root][INFO] - Training Epoch: 2/2, step 2035/7134 completed (loss: 0.01140210498124361, acc: 0.9950186610221863)
[2025-02-13 03:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:50][root][INFO] - Training Epoch: 2/2, step 2036/7134 completed (loss: 0.018656784668564796, acc: 0.9930796027183533)
[2025-02-13 03:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:50][root][INFO] - Training Epoch: 2/2, step 2037/7134 completed (loss: 0.016176817938685417, acc: 0.9925816059112549)
[2025-02-13 03:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51][root][INFO] - Training Epoch: 2/2, step 2038/7134 completed (loss: 0.01064613088965416, acc: 0.9976359605789185)
[2025-02-13 03:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51][root][INFO] - Training Epoch: 2/2, step 2039/7134 completed (loss: 0.004037668462842703, acc: 1.0)
[2025-02-13 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:52][root][INFO] - Training Epoch: 2/2, step 2040/7134 completed (loss: 0.03639131039381027, acc: 0.991769552230835)
[2025-02-13 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:52][root][INFO] - Training Epoch: 2/2, step 2041/7134 completed (loss: 0.008819405920803547, acc: 0.9972527623176575)
[2025-02-13 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53][root][INFO] - Training Epoch: 2/2, step 2042/7134 completed (loss: 0.010710545815527439, acc: 0.9979838728904724)
[2025-02-13 03:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53][root][INFO] - Training Epoch: 2/2, step 2043/7134 completed (loss: 0.018807915970683098, acc: 0.9937499761581421)
[2025-02-13 03:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:54][root][INFO] - Training Epoch: 2/2, step 2044/7134 completed (loss: 0.014515170827507973, acc: 0.9964221715927124)
[2025-02-13 03:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:54][root][INFO] - Training Epoch: 2/2, step 2045/7134 completed (loss: 0.01503646932542324, acc: 0.9963833689689636)
[2025-02-13 03:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:54][root][INFO] - Training Epoch: 2/2, step 2046/7134 completed (loss: 0.01809915155172348, acc: 0.9918699264526367)
[2025-02-13 03:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:55][root][INFO] - Training Epoch: 2/2, step 2047/7134 completed (loss: 0.013550495728850365, acc: 0.9966611266136169)
[2025-02-13 03:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:55][root][INFO] - Training Epoch: 2/2, step 2048/7134 completed (loss: 0.044798802584409714, acc: 0.9895366430282593)
[2025-02-13 03:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56][root][INFO] - Training Epoch: 2/2, step 2049/7134 completed (loss: 0.05715128406882286, acc: 0.981176495552063)
[2025-02-13 03:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56][root][INFO] - Training Epoch: 2/2, step 2050/7134 completed (loss: 0.06193695589900017, acc: 0.9863247871398926)
[2025-02-13 03:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:57][root][INFO] - Training Epoch: 2/2, step 2051/7134 completed (loss: 0.03763236477971077, acc: 0.98828125)
[2025-02-13 03:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:57][root][INFO] - Training Epoch: 2/2, step 2052/7134 completed (loss: 0.06775075942277908, acc: 0.979522168636322)
[2025-02-13 03:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:57][root][INFO] - Training Epoch: 2/2, step 2053/7134 completed (loss: 0.030885111540555954, acc: 0.9900000095367432)
[2025-02-13 03:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:58][root][INFO] - Training Epoch: 2/2, step 2054/7134 completed (loss: 0.09768068790435791, acc: 0.9729299545288086)
[2025-02-13 03:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:58][root][INFO] - Training Epoch: 2/2, step 2055/7134 completed (loss: 0.06022946909070015, acc: 0.9842519760131836)
[2025-02-13 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59][root][INFO] - Training Epoch: 2/2, step 2056/7134 completed (loss: 0.032364364713430405, acc: 0.9922178983688354)
[2025-02-13 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59][root][INFO] - Training Epoch: 2/2, step 2057/7134 completed (loss: 0.0502454936504364, acc: 0.9850746393203735)
[2025-02-13 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:00][root][INFO] - Training Epoch: 2/2, step 2058/7134 completed (loss: 0.08651590347290039, acc: 0.9756097793579102)
[2025-02-13 03:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:00][root][INFO] - Training Epoch: 2/2, step 2059/7134 completed (loss: 0.08334068953990936, acc: 0.984402060508728)
[2025-02-13 03:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01][root][INFO] - Training Epoch: 2/2, step 2060/7134 completed (loss: 0.02659761719405651, acc: 0.9899857044219971)
[2025-02-13 03:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01][root][INFO] - Training Epoch: 2/2, step 2061/7134 completed (loss: 0.06149454042315483, acc: 0.9830028414726257)
[2025-02-13 03:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:02][root][INFO] - Training Epoch: 2/2, step 2062/7134 completed (loss: 0.047262657433748245, acc: 0.9848197102546692)
[2025-02-13 03:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:02][root][INFO] - Training Epoch: 2/2, step 2063/7134 completed (loss: 0.055534180253744125, acc: 0.983208954334259)
[2025-02-13 03:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:02][root][INFO] - Training Epoch: 2/2, step 2064/7134 completed (loss: 0.012934485450387001, acc: 0.996874988079071)
[2025-02-13 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:03][root][INFO] - Training Epoch: 2/2, step 2065/7134 completed (loss: 0.04551510140299797, acc: 0.9889763593673706)
[2025-02-13 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:03][root][INFO] - Training Epoch: 2/2, step 2066/7134 completed (loss: 0.03143608942627907, acc: 0.991983950138092)
[2025-02-13 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:04][root][INFO] - Training Epoch: 2/2, step 2067/7134 completed (loss: 0.019363565370440483, acc: 0.992409884929657)
[2025-02-13 03:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:04][root][INFO] - Training Epoch: 2/2, step 2068/7134 completed (loss: 0.06132430210709572, acc: 0.9789227247238159)
[2025-02-13 03:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:04][root][INFO] - Training Epoch: 2/2, step 2069/7134 completed (loss: 0.025367828086018562, acc: 0.9907692074775696)
[2025-02-13 03:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05][root][INFO] - Training Epoch: 2/2, step 2070/7134 completed (loss: 0.03811253234744072, acc: 0.9923954606056213)
[2025-02-13 03:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05][root][INFO] - Training Epoch: 2/2, step 2071/7134 completed (loss: 0.008825107477605343, acc: 0.9976580739021301)
[2025-02-13 03:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:06][root][INFO] - Training Epoch: 2/2, step 2072/7134 completed (loss: 0.02119424380362034, acc: 0.9925742745399475)
[2025-02-13 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:06][root][INFO] - Training Epoch: 2/2, step 2073/7134 completed (loss: 0.03531826287508011, acc: 0.9923076629638672)
[2025-02-13 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:07][root][INFO] - Training Epoch: 2/2, step 2074/7134 completed (loss: 0.02781592309474945, acc: 0.9929577708244324)
[2025-02-13 03:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:07][root][INFO] - Training Epoch: 2/2, step 2075/7134 completed (loss: 0.0225367471575737, acc: 0.9916527271270752)
[2025-02-13 03:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08][root][INFO] - Training Epoch: 2/2, step 2076/7134 completed (loss: 0.025406062602996826, acc: 0.9938461780548096)
[2025-02-13 03:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08][root][INFO] - Training Epoch: 2/2, step 2077/7134 completed (loss: 0.022860445082187653, acc: 0.9893758296966553)
[2025-02-13 03:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08][root][INFO] - Training Epoch: 2/2, step 2078/7134 completed (loss: 0.02400156296789646, acc: 0.9930232763290405)
[2025-02-13 03:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09][root][INFO] - Training Epoch: 2/2, step 2079/7134 completed (loss: 0.01274827029556036, acc: 0.9947090148925781)
[2025-02-13 03:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09][root][INFO] - Training Epoch: 2/2, step 2080/7134 completed (loss: 0.02400589920580387, acc: 0.9941176176071167)
[2025-02-13 03:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:10][root][INFO] - Training Epoch: 2/2, step 2081/7134 completed (loss: 0.010073202662169933, acc: 0.9985486268997192)
[2025-02-13 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:10][root][INFO] - Training Epoch: 2/2, step 2082/7134 completed (loss: 0.06706084311008453, acc: 0.9853249192237854)
[2025-02-13 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11][root][INFO] - Training Epoch: 2/2, step 2083/7134 completed (loss: 0.03470776230096817, acc: 0.9944444298744202)
[2025-02-13 03:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11][root][INFO] - Training Epoch: 2/2, step 2084/7134 completed (loss: 0.01850615069270134, acc: 0.9984471797943115)
[2025-02-13 03:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11][root][INFO] - Training Epoch: 2/2, step 2085/7134 completed (loss: 0.009198814630508423, acc: 1.0)
[2025-02-13 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:12][root][INFO] - Training Epoch: 2/2, step 2086/7134 completed (loss: 0.01057146955281496, acc: 0.9954338073730469)
[2025-02-13 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:12][root][INFO] - Training Epoch: 2/2, step 2087/7134 completed (loss: 0.004287687595933676, acc: 1.0)
[2025-02-13 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13][root][INFO] - Training Epoch: 2/2, step 2088/7134 completed (loss: 0.027885645627975464, acc: 0.9913544654846191)
[2025-02-13 03:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13][root][INFO] - Training Epoch: 2/2, step 2089/7134 completed (loss: 0.020803119987249374, acc: 0.9936908483505249)
[2025-02-13 03:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14][root][INFO] - Training Epoch: 2/2, step 2090/7134 completed (loss: 0.006278956774622202, acc: 1.0)
[2025-02-13 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14][root][INFO] - Training Epoch: 2/2, step 2091/7134 completed (loss: 0.012880709022283554, acc: 0.9983108043670654)
[2025-02-13 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14][root][INFO] - Training Epoch: 2/2, step 2092/7134 completed (loss: 0.010696631856262684, acc: 0.9984227418899536)
[2025-02-13 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15][root][INFO] - Training Epoch: 2/2, step 2093/7134 completed (loss: 0.01917719841003418, acc: 0.9932157397270203)
[2025-02-13 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15][root][INFO] - Training Epoch: 2/2, step 2094/7134 completed (loss: 0.040239594876766205, acc: 0.9898580312728882)
[2025-02-13 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:16][root][INFO] - Training Epoch: 2/2, step 2095/7134 completed (loss: 0.03673478960990906, acc: 0.9907651543617249)
[2025-02-13 03:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:16][root][INFO] - Training Epoch: 2/2, step 2096/7134 completed (loss: 0.013291495852172375, acc: 0.9965517520904541)
[2025-02-13 03:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17][root][INFO] - Training Epoch: 2/2, step 2097/7134 completed (loss: 0.011005056090652943, acc: 0.9943898916244507)
[2025-02-13 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17][root][INFO] - Training Epoch: 2/2, step 2098/7134 completed (loss: 0.019815992563962936, acc: 0.9967845678329468)
[2025-02-13 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17][root][INFO] - Training Epoch: 2/2, step 2099/7134 completed (loss: 0.01646721549332142, acc: 0.9981651306152344)
[2025-02-13 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:18][root][INFO] - Training Epoch: 2/2, step 2100/7134 completed (loss: 0.020037835463881493, acc: 0.9946879148483276)
[2025-02-13 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:18][root][INFO] - Training Epoch: 2/2, step 2101/7134 completed (loss: 0.011789447627961636, acc: 0.992443323135376)
[2025-02-13 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19][root][INFO] - Training Epoch: 2/2, step 2102/7134 completed (loss: 0.028022216632962227, acc: 0.9910714030265808)
[2025-02-13 03:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19][root][INFO] - Training Epoch: 2/2, step 2103/7134 completed (loss: 0.014947541989386082, acc: 0.9938650131225586)
[2025-02-13 03:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:20][root][INFO] - Training Epoch: 2/2, step 2104/7134 completed (loss: 0.011857225559651852, acc: 0.9974226951599121)
[2025-02-13 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:20][root][INFO] - Training Epoch: 2/2, step 2105/7134 completed (loss: 0.027275878936052322, acc: 0.9906103014945984)
[2025-02-13 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:20][root][INFO] - Training Epoch: 2/2, step 2106/7134 completed (loss: 0.02310156635940075, acc: 0.9928160905838013)
[2025-02-13 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21][root][INFO] - Training Epoch: 2/2, step 2107/7134 completed (loss: 0.02127843163907528, acc: 0.9932523369789124)
[2025-02-13 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21][root][INFO] - Training Epoch: 2/2, step 2108/7134 completed (loss: 0.009623486548662186, acc: 0.9955157041549683)
[2025-02-13 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:22][root][INFO] - Training Epoch: 2/2, step 2109/7134 completed (loss: 0.040837790817022324, acc: 0.9894179701805115)
[2025-02-13 03:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:22][root][INFO] - Training Epoch: 2/2, step 2110/7134 completed (loss: 0.042629458010196686, acc: 0.9869186282157898)
[2025-02-13 03:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:23][root][INFO] - Training Epoch: 2/2, step 2111/7134 completed (loss: 0.03514549508690834, acc: 0.9922178983688354)
[2025-02-13 03:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:23][root][INFO] - Training Epoch: 2/2, step 2112/7134 completed (loss: 0.0459442213177681, acc: 0.9856687784194946)
[2025-02-13 03:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:23][root][INFO] - Training Epoch: 2/2, step 2113/7134 completed (loss: 0.014337731525301933, acc: 0.9940387606620789)
[2025-02-13 03:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24][root][INFO] - Training Epoch: 2/2, step 2114/7134 completed (loss: 0.022045304998755455, acc: 0.9921568632125854)
[2025-02-13 03:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24][root][INFO] - Training Epoch: 2/2, step 2115/7134 completed (loss: 0.026066815480589867, acc: 0.9924905896186829)
[2025-02-13 03:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:25][root][INFO] - Training Epoch: 2/2, step 2116/7134 completed (loss: 0.020628660917282104, acc: 0.995398759841919)
[2025-02-13 03:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:25][root][INFO] - Training Epoch: 2/2, step 2117/7134 completed (loss: 0.014751376584172249, acc: 0.9950248599052429)
[2025-02-13 03:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:25][root][INFO] - Training Epoch: 2/2, step 2118/7134 completed (loss: 0.024978717789053917, acc: 0.9959072470664978)
[2025-02-13 03:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:26][root][INFO] - Training Epoch: 2/2, step 2119/7134 completed (loss: 0.019768260419368744, acc: 0.9921259880065918)
[2025-02-13 03:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:26][root][INFO] - Training Epoch: 2/2, step 2120/7134 completed (loss: 0.022745011374354362, acc: 0.990867555141449)
[2025-02-13 03:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27][root][INFO] - Training Epoch: 2/2, step 2121/7134 completed (loss: 0.017373215407133102, acc: 0.9936507940292358)
[2025-02-13 03:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27][root][INFO] - Training Epoch: 2/2, step 2122/7134 completed (loss: 0.011778297834098339, acc: 0.99609375)
[2025-02-13 03:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28][root][INFO] - Training Epoch: 2/2, step 2123/7134 completed (loss: 0.01991277001798153, acc: 0.99842768907547)
[2025-02-13 03:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28][root][INFO] - Training Epoch: 2/2, step 2124/7134 completed (loss: 0.008300768211483955, acc: 0.9984423518180847)
[2025-02-13 03:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28][root][INFO] - Training Epoch: 2/2, step 2125/7134 completed (loss: 0.007569450419396162, acc: 0.9969696998596191)
[2025-02-13 03:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29][root][INFO] - Training Epoch: 2/2, step 2126/7134 completed (loss: 0.01665402390062809, acc: 0.9949579834938049)
[2025-02-13 03:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29][root][INFO] - Training Epoch: 2/2, step 2127/7134 completed (loss: 0.014346778392791748, acc: 0.9955752491950989)
[2025-02-13 03:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:30][root][INFO] - Training Epoch: 2/2, step 2128/7134 completed (loss: 0.0201523769646883, acc: 0.9934383034706116)
[2025-02-13 03:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:30][root][INFO] - Training Epoch: 2/2, step 2129/7134 completed (loss: 0.01878674328327179, acc: 0.993966817855835)
[2025-02-13 03:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:30][root][INFO] - Training Epoch: 2/2, step 2130/7134 completed (loss: 0.024966398254036903, acc: 0.9917491674423218)
[2025-02-13 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:31][root][INFO] - Training Epoch: 2/2, step 2131/7134 completed (loss: 0.009984142147004604, acc: 0.9969040155410767)
[2025-02-13 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:31][root][INFO] - Training Epoch: 2/2, step 2132/7134 completed (loss: 0.009487679228186607, acc: 0.9959568977355957)
[2025-02-13 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:32][root][INFO] - Training Epoch: 2/2, step 2133/7134 completed (loss: 0.019361943006515503, acc: 0.9947159886360168)
[2025-02-13 03:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:32][root][INFO] - Training Epoch: 2/2, step 2134/7134 completed (loss: 0.010836352594196796, acc: 0.9959839582443237)
[2025-02-13 03:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33][root][INFO] - Training Epoch: 2/2, step 2135/7134 completed (loss: 0.020157698541879654, acc: 0.9924699068069458)
[2025-02-13 03:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33][root][INFO] - Training Epoch: 2/2, step 2136/7134 completed (loss: 0.011158568784594536, acc: 0.9946595430374146)
[2025-02-13 03:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33][root][INFO] - Training Epoch: 2/2, step 2137/7134 completed (loss: 0.015258785337209702, acc: 0.996129035949707)
[2025-02-13 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:34][root][INFO] - Training Epoch: 2/2, step 2138/7134 completed (loss: 0.004771938547492027, acc: 0.9984756112098694)
[2025-02-13 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:34][root][INFO] - Training Epoch: 2/2, step 2139/7134 completed (loss: 0.007184856105595827, acc: 0.9958391189575195)
[2025-02-13 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:35][root][INFO] - Training Epoch: 2/2, step 2140/7134 completed (loss: 0.0433470793068409, acc: 0.9903581142425537)
[2025-02-13 03:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:35][root][INFO] - Training Epoch: 2/2, step 2141/7134 completed (loss: 0.01496943924576044, acc: 0.9941520690917969)
[2025-02-13 03:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36][root][INFO] - Training Epoch: 2/2, step 2142/7134 completed (loss: 0.01939883828163147, acc: 0.9929178357124329)
[2025-02-13 03:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36][root][INFO] - Training Epoch: 2/2, step 2143/7134 completed (loss: 0.025509610772132874, acc: 0.9947368502616882)
[2025-02-13 03:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:37][root][INFO] - Training Epoch: 2/2, step 2144/7134 completed (loss: 0.0017194917891174555, acc: 1.0)
[2025-02-13 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:37][root][INFO] - Training Epoch: 2/2, step 2145/7134 completed (loss: 0.03791497275233269, acc: 0.9899857044219971)
[2025-02-13 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:38][root][INFO] - Training Epoch: 2/2, step 2146/7134 completed (loss: 0.013075071386992931, acc: 0.9947183132171631)
[2025-02-13 03:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:38][root][INFO] - Training Epoch: 2/2, step 2147/7134 completed (loss: 0.020156197249889374, acc: 0.9925187230110168)
[2025-02-13 03:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:38][root][INFO] - Training Epoch: 2/2, step 2148/7134 completed (loss: 0.03535168617963791, acc: 0.9898107647895813)
[2025-02-13 03:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39][root][INFO] - Training Epoch: 2/2, step 2149/7134 completed (loss: 0.010995239019393921, acc: 0.9985915422439575)
[2025-02-13 03:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39][root][INFO] - Training Epoch: 2/2, step 2150/7134 completed (loss: 0.014738678932189941, acc: 0.9946666955947876)
[2025-02-13 03:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:40][root][INFO] - Training Epoch: 2/2, step 2151/7134 completed (loss: 0.014055496081709862, acc: 0.9957143068313599)
[2025-02-13 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:40][root][INFO] - Training Epoch: 2/2, step 2152/7134 completed (loss: 0.05290974676609039, acc: 0.9779411554336548)
[2025-02-13 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:41][root][INFO] - Training Epoch: 2/2, step 2153/7134 completed (loss: 0.03373873606324196, acc: 0.9911209940910339)
[2025-02-13 03:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:41][root][INFO] - Training Epoch: 2/2, step 2154/7134 completed (loss: 0.0351373553276062, acc: 0.9886220097541809)
[2025-02-13 03:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:42][root][INFO] - Training Epoch: 2/2, step 2155/7134 completed (loss: 0.042103856801986694, acc: 0.9972527623176575)
[2025-02-13 03:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:42][root][INFO] - Training Epoch: 2/2, step 2156/7134 completed (loss: 0.006853190716356039, acc: 0.9982638955116272)
[2025-02-13 03:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:42][root][INFO] - Training Epoch: 2/2, step 2157/7134 completed (loss: 0.013083993457257748, acc: 0.9955307245254517)
[2025-02-13 03:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:43][root][INFO] - Training Epoch: 2/2, step 2158/7134 completed (loss: 0.03648720309138298, acc: 0.9920634627342224)
[2025-02-13 03:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:43][root][INFO] - Training Epoch: 2/2, step 2159/7134 completed (loss: 0.016103070229291916, acc: 0.9974457025527954)
[2025-02-13 03:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44][root][INFO] - Training Epoch: 2/2, step 2160/7134 completed (loss: 0.01989801414310932, acc: 0.993122398853302)
[2025-02-13 03:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44][root][INFO] - Training Epoch: 2/2, step 2161/7134 completed (loss: 0.037869781255722046, acc: 0.991037130355835)
[2025-02-13 03:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:45][root][INFO] - Training Epoch: 2/2, step 2162/7134 completed (loss: 0.0330212228000164, acc: 0.9924356937408447)
[2025-02-13 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:45][root][INFO] - Training Epoch: 2/2, step 2163/7134 completed (loss: 0.021689239889383316, acc: 0.9905511736869812)
[2025-02-13 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:46][root][INFO] - Training Epoch: 2/2, step 2164/7134 completed (loss: 0.01197054609656334, acc: 0.9965694546699524)
[2025-02-13 03:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:46][root][INFO] - Training Epoch: 2/2, step 2165/7134 completed (loss: 0.023470746353268623, acc: 0.9942611455917358)
[2025-02-13 03:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47][root][INFO] - Training Epoch: 2/2, step 2166/7134 completed (loss: 0.009312101639807224, acc: 0.9986979365348816)
[2025-02-13 03:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47][root][INFO] - Training Epoch: 2/2, step 2167/7134 completed (loss: 0.028612878173589706, acc: 0.9955456852912903)
[2025-02-13 03:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47][root][INFO] - Training Epoch: 2/2, step 2168/7134 completed (loss: 0.01647844910621643, acc: 0.9952380657196045)
[2025-02-13 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:48][root][INFO] - Training Epoch: 2/2, step 2169/7134 completed (loss: 0.03949395939707756, acc: 0.9941995143890381)
[2025-02-13 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:48][root][INFO] - Training Epoch: 2/2, step 2170/7134 completed (loss: 0.03275561332702637, acc: 0.9912663698196411)
[2025-02-13 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49][root][INFO] - Training Epoch: 2/2, step 2171/7134 completed (loss: 0.03565293923020363, acc: 0.9920724630355835)
[2025-02-13 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49][root][INFO] - Training Epoch: 2/2, step 2172/7134 completed (loss: 0.017064262181520462, acc: 0.9948347210884094)
[2025-02-13 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:50][root][INFO] - Training Epoch: 2/2, step 2173/7134 completed (loss: 0.027329403907060623, acc: 0.9939576983451843)
[2025-02-13 03:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:50][root][INFO] - Training Epoch: 2/2, step 2174/7134 completed (loss: 0.012225939892232418, acc: 0.9957447052001953)
[2025-02-13 03:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51][root][INFO] - Training Epoch: 2/2, step 2175/7134 completed (loss: 0.0552184097468853, acc: 0.9916897416114807)
[2025-02-13 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51][root][INFO] - Training Epoch: 2/2, step 2176/7134 completed (loss: 0.01877347193658352, acc: 0.9928910136222839)
[2025-02-13 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:52][root][INFO] - Training Epoch: 2/2, step 2177/7134 completed (loss: 0.028294114395976067, acc: 0.9909090995788574)
[2025-02-13 03:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:52][root][INFO] - Training Epoch: 2/2, step 2178/7134 completed (loss: 0.012227868661284447, acc: 0.996688723564148)
[2025-02-13 03:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53][root][INFO] - Training Epoch: 2/2, step 2179/7134 completed (loss: 0.023205135017633438, acc: 0.9949367046356201)
[2025-02-13 03:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53][root][INFO] - Training Epoch: 2/2, step 2180/7134 completed (loss: 0.023632438853383064, acc: 0.9952718615531921)
[2025-02-13 03:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53][root][INFO] - Training Epoch: 2/2, step 2181/7134 completed (loss: 0.00776686193421483, acc: 0.9971387982368469)
[2025-02-13 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:54][root][INFO] - Training Epoch: 2/2, step 2182/7134 completed (loss: 0.03286797180771828, acc: 0.9884659647941589)
[2025-02-13 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:54][root][INFO] - Training Epoch: 2/2, step 2183/7134 completed (loss: 0.02741427905857563, acc: 0.9918509721755981)
[2025-02-13 03:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55][root][INFO] - Training Epoch: 2/2, step 2184/7134 completed (loss: 0.024861866608262062, acc: 0.9936102032661438)
[2025-02-13 03:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55][root][INFO] - Training Epoch: 2/2, step 2185/7134 completed (loss: 0.04851300269365311, acc: 0.9844192862510681)
[2025-02-13 03:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:56][root][INFO] - Training Epoch: 2/2, step 2186/7134 completed (loss: 0.039996836334466934, acc: 0.987034022808075)
[2025-02-13 03:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:56][root][INFO] - Training Epoch: 2/2, step 2187/7134 completed (loss: 0.06476406753063202, acc: 0.984000027179718)
[2025-02-13 03:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:57][root][INFO] - Training Epoch: 2/2, step 2188/7134 completed (loss: 0.0177884791046381, acc: 0.9917920827865601)
[2025-02-13 03:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:57][root][INFO] - Training Epoch: 2/2, step 2189/7134 completed (loss: 0.03628476336598396, acc: 0.9908257126808167)
[2025-02-13 03:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58][root][INFO] - Training Epoch: 2/2, step 2190/7134 completed (loss: 0.035181984305381775, acc: 0.9894459247589111)
[2025-02-13 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58][root][INFO] - Training Epoch: 2/2, step 2191/7134 completed (loss: 0.01880725286900997, acc: 0.9918699264526367)
[2025-02-13 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58][root][INFO] - Training Epoch: 2/2, step 2192/7134 completed (loss: 0.004422892350703478, acc: 1.0)
[2025-02-13 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59][root][INFO] - Training Epoch: 2/2, step 2193/7134 completed (loss: 0.01282497774809599, acc: 0.9960106611251831)
[2025-02-13 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59][root][INFO] - Training Epoch: 2/2, step 2194/7134 completed (loss: 0.021746108308434486, acc: 0.9899497628211975)
[2025-02-13 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:00][root][INFO] - Training Epoch: 2/2, step 2195/7134 completed (loss: 0.010023068636655807, acc: 0.9968000054359436)
[2025-02-13 03:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:00][root][INFO] - Training Epoch: 2/2, step 2196/7134 completed (loss: 0.02220768667757511, acc: 0.9948186278343201)
[2025-02-13 03:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:01][root][INFO] - Training Epoch: 2/2, step 2197/7134 completed (loss: 0.019770273938775063, acc: 0.9927140474319458)
[2025-02-13 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:01][root][INFO] - Training Epoch: 2/2, step 2198/7134 completed (loss: 0.027749810367822647, acc: 0.9909502267837524)
[2025-02-13 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:02][root][INFO] - Training Epoch: 2/2, step 2199/7134 completed (loss: 0.01554337702691555, acc: 0.9969788789749146)
[2025-02-13 03:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:02][root][INFO] - Training Epoch: 2/2, step 2200/7134 completed (loss: 0.01823536306619644, acc: 0.9927667379379272)
[2025-02-13 03:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:02][root][INFO] - Training Epoch: 2/2, step 2201/7134 completed (loss: 0.011001831851899624, acc: 0.9954545497894287)
[2025-02-13 03:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03][root][INFO] - Training Epoch: 2/2, step 2202/7134 completed (loss: 0.017149807885289192, acc: 0.9916527271270752)
[2025-02-13 03:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03][root][INFO] - Training Epoch: 2/2, step 2203/7134 completed (loss: 0.0054953633807599545, acc: 0.998630166053772)
[2025-02-13 03:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:04][root][INFO] - Training Epoch: 2/2, step 2204/7134 completed (loss: 0.011343094520270824, acc: 0.9956140518188477)
[2025-02-13 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:04][root][INFO] - Training Epoch: 2/2, step 2205/7134 completed (loss: 0.020256921648979187, acc: 0.9964912533760071)
[2025-02-13 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05][root][INFO] - Training Epoch: 2/2, step 2206/7134 completed (loss: 0.04529061168432236, acc: 0.994727611541748)
[2025-02-13 03:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05][root][INFO] - Training Epoch: 2/2, step 2207/7134 completed (loss: 0.02120479941368103, acc: 0.9951534867286682)
[2025-02-13 03:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05][root][INFO] - Training Epoch: 2/2, step 2208/7134 completed (loss: 0.03201913833618164, acc: 0.9933884143829346)
[2025-02-13 03:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06][root][INFO] - Training Epoch: 2/2, step 2209/7134 completed (loss: 0.01947108469903469, acc: 0.9886845946311951)
[2025-02-13 03:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06][root][INFO] - Training Epoch: 2/2, step 2210/7134 completed (loss: 0.028112443163990974, acc: 0.9924127459526062)
[2025-02-13 03:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:07][root][INFO] - Training Epoch: 2/2, step 2211/7134 completed (loss: 0.02668277733027935, acc: 0.9958449006080627)
[2025-02-13 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:07][root][INFO] - Training Epoch: 2/2, step 2212/7134 completed (loss: 0.01278079766780138, acc: 0.9944444298744202)
[2025-02-13 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:08][root][INFO] - Training Epoch: 2/2, step 2213/7134 completed (loss: 0.016658030450344086, acc: 0.994350254535675)
[2025-02-13 03:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:08][root][INFO] - Training Epoch: 2/2, step 2214/7134 completed (loss: 0.02203170768916607, acc: 0.9895522594451904)
[2025-02-13 03:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:08][root][INFO] - Training Epoch: 2/2, step 2215/7134 completed (loss: 0.010430935770273209, acc: 0.9957864880561829)
[2025-02-13 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09][root][INFO] - Training Epoch: 2/2, step 2216/7134 completed (loss: 0.01783597283065319, acc: 0.9959946870803833)
[2025-02-13 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09][root][INFO] - Training Epoch: 2/2, step 2217/7134 completed (loss: 0.015888385474681854, acc: 0.9950186610221863)
[2025-02-13 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:10][root][INFO] - Training Epoch: 2/2, step 2218/7134 completed (loss: 0.02236543782055378, acc: 0.9956896305084229)
[2025-02-13 03:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:10][root][INFO] - Training Epoch: 2/2, step 2219/7134 completed (loss: 0.012057674117386341, acc: 0.9969087839126587)
[2025-02-13 03:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:11][root][INFO] - Training Epoch: 2/2, step 2220/7134 completed (loss: 0.005585495848208666, acc: 1.0)
[2025-02-13 03:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:11][root][INFO] - Training Epoch: 2/2, step 2221/7134 completed (loss: 0.014378171414136887, acc: 0.9974026083946228)
[2025-02-13 03:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12][root][INFO] - Training Epoch: 2/2, step 2222/7134 completed (loss: 0.02267405390739441, acc: 0.9927745461463928)
[2025-02-13 03:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12][root][INFO] - Training Epoch: 2/2, step 2223/7134 completed (loss: 0.01488330028951168, acc: 0.9958158731460571)
[2025-02-13 03:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12][root][INFO] - Training Epoch: 2/2, step 2224/7134 completed (loss: 0.01989070512354374, acc: 0.9945130348205566)
[2025-02-13 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:13][root][INFO] - Training Epoch: 2/2, step 2225/7134 completed (loss: 0.011142016388475895, acc: 0.9971056580543518)
[2025-02-13 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:13][root][INFO] - Training Epoch: 2/2, step 2226/7134 completed (loss: 0.015517417341470718, acc: 0.9939117431640625)
[2025-02-13 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:14][root][INFO] - Training Epoch: 2/2, step 2227/7134 completed (loss: 0.015450060367584229, acc: 0.9956076145172119)
[2025-02-13 03:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:14][root][INFO] - Training Epoch: 2/2, step 2228/7134 completed (loss: 0.0030837850645184517, acc: 1.0)
[2025-02-13 03:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15][root][INFO] - Training Epoch: 2/2, step 2229/7134 completed (loss: 0.013294858857989311, acc: 0.9959999918937683)
[2025-02-13 03:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15][root][INFO] - Training Epoch: 2/2, step 2230/7134 completed (loss: 0.019047817215323448, acc: 0.9935275316238403)
[2025-02-13 03:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15][root][INFO] - Training Epoch: 2/2, step 2231/7134 completed (loss: 0.02709275670349598, acc: 0.9900398254394531)
[2025-02-13 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:16][root][INFO] - Training Epoch: 2/2, step 2232/7134 completed (loss: 0.008843041956424713, acc: 0.9973261952400208)
[2025-02-13 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:16][root][INFO] - Training Epoch: 2/2, step 2233/7134 completed (loss: 0.021018002182245255, acc: 0.9900426864624023)
[2025-02-13 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:17][root][INFO] - Training Epoch: 2/2, step 2234/7134 completed (loss: 0.003401027759537101, acc: 0.998633861541748)
[2025-02-13 03:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:17][root][INFO] - Training Epoch: 2/2, step 2235/7134 completed (loss: 0.02777179144322872, acc: 0.9952977895736694)
[2025-02-13 03:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:18][root][INFO] - Training Epoch: 2/2, step 2236/7134 completed (loss: 0.021520979702472687, acc: 0.9925037622451782)
[2025-02-13 03:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:18][root][INFO] - Training Epoch: 2/2, step 2237/7134 completed (loss: 0.027453450486063957, acc: 0.9900990128517151)
[2025-02-13 03:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:18][root][INFO] - Training Epoch: 2/2, step 2238/7134 completed (loss: 0.059619560837745667, acc: 0.9840810298919678)
[2025-02-13 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:19][root][INFO] - Training Epoch: 2/2, step 2239/7134 completed (loss: 0.055932652205228806, acc: 0.984415590763092)
[2025-02-13 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:19][root][INFO] - Training Epoch: 2/2, step 2240/7134 completed (loss: 0.02720007672905922, acc: 0.9922077655792236)
[2025-02-13 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20][root][INFO] - Training Epoch: 2/2, step 2241/7134 completed (loss: 0.058128755539655685, acc: 0.9834515452384949)
[2025-02-13 03:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20][root][INFO] - Training Epoch: 2/2, step 2242/7134 completed (loss: 0.07613404840230942, acc: 0.978622317314148)
[2025-02-13 03:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:21][root][INFO] - Training Epoch: 2/2, step 2243/7134 completed (loss: 0.09245143085718155, acc: 0.9786259531974792)
[2025-02-13 03:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:21][root][INFO] - Training Epoch: 2/2, step 2244/7134 completed (loss: 0.019742915406823158, acc: 0.9952830076217651)
[2025-02-13 03:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22][root][INFO] - Training Epoch: 2/2, step 2245/7134 completed (loss: 0.04920724406838417, acc: 0.9741100072860718)
[2025-02-13 03:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22][root][INFO] - Training Epoch: 2/2, step 2246/7134 completed (loss: 0.07907356321811676, acc: 0.9729729890823364)
[2025-02-13 03:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22][root][INFO] - Training Epoch: 2/2, step 2247/7134 completed (loss: 0.06625107675790787, acc: 0.977011501789093)
[2025-02-13 03:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23][root][INFO] - Training Epoch: 2/2, step 2248/7134 completed (loss: 0.05541408807039261, acc: 0.9861687421798706)
[2025-02-13 03:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23][root][INFO] - Training Epoch: 2/2, step 2249/7134 completed (loss: 0.030839646235108376, acc: 0.9902234673500061)
[2025-02-13 03:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:24][root][INFO] - Training Epoch: 2/2, step 2250/7134 completed (loss: 0.031176088377833366, acc: 0.9919785857200623)
[2025-02-13 03:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:24][root][INFO] - Training Epoch: 2/2, step 2251/7134 completed (loss: 0.03175202012062073, acc: 0.9863945841789246)
[2025-02-13 03:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:25][root][INFO] - Training Epoch: 2/2, step 2252/7134 completed (loss: 0.01534554548561573, acc: 0.994397759437561)
[2025-02-13 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:25][root][INFO] - Training Epoch: 2/2, step 2253/7134 completed (loss: 0.02655886299908161, acc: 0.9899665713310242)
[2025-02-13 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:25][root][INFO] - Training Epoch: 2/2, step 2254/7134 completed (loss: 0.07001504302024841, acc: 0.9820716977119446)
[2025-02-13 03:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26][root][INFO] - Training Epoch: 2/2, step 2255/7134 completed (loss: 0.032336052507162094, acc: 0.987860381603241)
[2025-02-13 03:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26][root][INFO] - Training Epoch: 2/2, step 2256/7134 completed (loss: 0.04027125984430313, acc: 0.9845132827758789)
[2025-02-13 03:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:27][root][INFO] - Training Epoch: 2/2, step 2257/7134 completed (loss: 0.023634381592273712, acc: 0.9924471378326416)
[2025-02-13 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:27][root][INFO] - Training Epoch: 2/2, step 2258/7134 completed (loss: 0.022926529869437218, acc: 0.9923076629638672)
[2025-02-13 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:27][root][INFO] - Training Epoch: 2/2, step 2259/7134 completed (loss: 0.017780032008886337, acc: 0.9933333396911621)
[2025-02-13 03:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:28][root][INFO] - Training Epoch: 2/2, step 2260/7134 completed (loss: 0.04664471000432968, acc: 0.9909090995788574)
[2025-02-13 03:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:28][root][INFO] - Training Epoch: 2/2, step 2261/7134 completed (loss: 0.0638858899474144, acc: 0.9835766553878784)
[2025-02-13 03:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29][root][INFO] - Training Epoch: 2/2, step 2262/7134 completed (loss: 0.011191308498382568, acc: 0.9981583952903748)
[2025-02-13 03:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29][root][INFO] - Training Epoch: 2/2, step 2263/7134 completed (loss: 0.03548884391784668, acc: 0.9909909963607788)
[2025-02-13 03:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:30][root][INFO] - Training Epoch: 2/2, step 2264/7134 completed (loss: 0.0659530833363533, acc: 0.9803094267845154)
[2025-02-13 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:30][root][INFO] - Training Epoch: 2/2, step 2265/7134 completed (loss: 0.05260024592280388, acc: 0.9793814420700073)
[2025-02-13 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:31][root][INFO] - Training Epoch: 2/2, step 2266/7134 completed (loss: 0.028686875477433205, acc: 0.9932975769042969)
[2025-02-13 03:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:31][root][INFO] - Training Epoch: 2/2, step 2267/7134 completed (loss: 0.013088921085000038, acc: 0.9958847761154175)
[2025-02-13 03:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:31][root][INFO] - Training Epoch: 2/2, step 2268/7134 completed (loss: 0.023979561403393745, acc: 0.991391658782959)
[2025-02-13 03:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32][root][INFO] - Training Epoch: 2/2, step 2269/7134 completed (loss: 0.008878096006810665, acc: 0.99622642993927)
[2025-02-13 03:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32][root][INFO] - Training Epoch: 2/2, step 2270/7134 completed (loss: 0.06078311428427696, acc: 0.9834162592887878)
[2025-02-13 03:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:33][root][INFO] - Training Epoch: 2/2, step 2271/7134 completed (loss: 0.028185421600937843, acc: 0.9930955171585083)
[2025-02-13 03:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:33][root][INFO] - Training Epoch: 2/2, step 2272/7134 completed (loss: 0.02600172720849514, acc: 0.995110034942627)
[2025-02-13 03:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:34][root][INFO] - Training Epoch: 2/2, step 2273/7134 completed (loss: 0.011970240622758865, acc: 0.9940119981765747)
[2025-02-13 03:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:34][root][INFO] - Training Epoch: 2/2, step 2274/7134 completed (loss: 0.08325359225273132, acc: 0.9808917045593262)
[2025-02-13 03:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35][root][INFO] - Training Epoch: 2/2, step 2275/7134 completed (loss: 0.024907564744353294, acc: 0.9917840361595154)
[2025-02-13 03:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35][root][INFO] - Training Epoch: 2/2, step 2276/7134 completed (loss: 0.024541491642594337, acc: 0.9940758347511292)
[2025-02-13 03:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:36][root][INFO] - Training Epoch: 2/2, step 2277/7134 completed (loss: 0.01929059624671936, acc: 0.9965357780456543)
[2025-02-13 03:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:36][root][INFO] - Training Epoch: 2/2, step 2278/7134 completed (loss: 0.05676842853426933, acc: 0.9824281334877014)
[2025-02-13 03:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:36][root][INFO] - Training Epoch: 2/2, step 2279/7134 completed (loss: 0.01824036054313183, acc: 0.996458113193512)
[2025-02-13 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:37][root][INFO] - Training Epoch: 2/2, step 2280/7134 completed (loss: 0.03812544420361519, acc: 0.9893742799758911)
[2025-02-13 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:37][root][INFO] - Training Epoch: 2/2, step 2281/7134 completed (loss: 0.07264836132526398, acc: 0.9853333234786987)
[2025-02-13 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38][root][INFO] - Training Epoch: 2/2, step 2282/7134 completed (loss: 0.00960183423012495, acc: 0.998763918876648)
[2025-02-13 03:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38][root][INFO] - Training Epoch: 2/2, step 2283/7134 completed (loss: 0.015832871198654175, acc: 0.9963325262069702)
[2025-02-13 03:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:39][root][INFO] - Training Epoch: 2/2, step 2284/7134 completed (loss: 0.02615097537636757, acc: 0.9911373853683472)
[2025-02-13 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:39][root][INFO] - Training Epoch: 2/2, step 2285/7134 completed (loss: 0.01344796922057867, acc: 0.9966555237770081)
[2025-02-13 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40][root][INFO] - Training Epoch: 2/2, step 2286/7134 completed (loss: 0.026938021183013916, acc: 0.990920901298523)
[2025-02-13 03:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40][root][INFO] - Training Epoch: 2/2, step 2287/7134 completed (loss: 0.018479084596037865, acc: 0.9932975769042969)
[2025-02-13 03:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41][root][INFO] - Training Epoch: 2/2, step 2288/7134 completed (loss: 0.018638383597135544, acc: 0.9964664578437805)
[2025-02-13 03:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41][root][INFO] - Training Epoch: 2/2, step 2289/7134 completed (loss: 0.02065490558743477, acc: 0.9910141229629517)
[2025-02-13 03:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41][root][INFO] - Training Epoch: 2/2, step 2290/7134 completed (loss: 0.016609178856015205, acc: 0.993630588054657)
[2025-02-13 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42][root][INFO] - Training Epoch: 2/2, step 2291/7134 completed (loss: 0.017495321109890938, acc: 0.9946019053459167)
[2025-02-13 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42][root][INFO] - Training Epoch: 2/2, step 2292/7134 completed (loss: 0.042297299951314926, acc: 0.9896774291992188)
[2025-02-13 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:43][root][INFO] - Training Epoch: 2/2, step 2293/7134 completed (loss: 0.028524480760097504, acc: 0.993318498134613)
[2025-02-13 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:43][root][INFO] - Training Epoch: 2/2, step 2294/7134 completed (loss: 0.034009456634521484, acc: 0.9910011291503906)
[2025-02-13 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:44][root][INFO] - Training Epoch: 2/2, step 2295/7134 completed (loss: 0.027857793495059013, acc: 0.9914529919624329)
[2025-02-13 03:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:44][root][INFO] - Training Epoch: 2/2, step 2296/7134 completed (loss: 0.015015519224107265, acc: 0.9946808218955994)
[2025-02-13 03:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45][root][INFO] - Training Epoch: 2/2, step 2297/7134 completed (loss: 0.015947580337524414, acc: 0.9959349632263184)
[2025-02-13 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45][root][INFO] - Training Epoch: 2/2, step 2298/7134 completed (loss: 0.042981695383787155, acc: 0.9905362725257874)
[2025-02-13 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:46][root][INFO] - Training Epoch: 2/2, step 2299/7134 completed (loss: 0.023275887593626976, acc: 0.9954493641853333)
[2025-02-13 03:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:46][root][INFO] - Training Epoch: 2/2, step 2300/7134 completed (loss: 0.007015588227659464, acc: 0.9988725781440735)
[2025-02-13 03:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47][root][INFO] - Training Epoch: 2/2, step 2301/7134 completed (loss: 0.0321337953209877, acc: 0.9920529723167419)
[2025-02-13 03:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47][root][INFO] - Training Epoch: 2/2, step 2302/7134 completed (loss: 0.038785699754953384, acc: 0.9865196347236633)
[2025-02-13 03:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47][root][INFO] - Training Epoch: 2/2, step 2303/7134 completed (loss: 0.02202507294714451, acc: 0.9943052530288696)
[2025-02-13 03:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:48][root][INFO] - Training Epoch: 2/2, step 2304/7134 completed (loss: 0.03178975358605385, acc: 0.9886947870254517)
[2025-02-13 03:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:48][root][INFO] - Training Epoch: 2/2, step 2305/7134 completed (loss: 0.014080079272389412, acc: 0.9968051314353943)
[2025-02-13 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49][root][INFO] - Training Epoch: 2/2, step 2306/7134 completed (loss: 0.010811877436935902, acc: 0.9966740608215332)
[2025-02-13 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49][root][INFO] - Training Epoch: 2/2, step 2307/7134 completed (loss: 0.030730681493878365, acc: 0.9948347210884094)
[2025-02-13 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:50][root][INFO] - Training Epoch: 2/2, step 2308/7134 completed (loss: 0.031235115602612495, acc: 0.9928498268127441)
[2025-02-13 03:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:50][root][INFO] - Training Epoch: 2/2, step 2309/7134 completed (loss: 0.02180887572467327, acc: 0.9952038526535034)
[2025-02-13 03:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51][root][INFO] - Training Epoch: 2/2, step 2310/7134 completed (loss: 0.011871426366269588, acc: 0.996610164642334)
[2025-02-13 03:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51][root][INFO] - Training Epoch: 2/2, step 2311/7134 completed (loss: 0.02448931336402893, acc: 0.9942660331726074)
[2025-02-13 03:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:52][root][INFO] - Training Epoch: 2/2, step 2312/7134 completed (loss: 0.0178004689514637, acc: 0.9946004152297974)
[2025-02-13 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:52][root][INFO] - Training Epoch: 2/2, step 2313/7134 completed (loss: 0.027503591030836105, acc: 0.9942922592163086)
[2025-02-13 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:52][root][INFO] - Training Epoch: 2/2, step 2314/7134 completed (loss: 0.02080165408551693, acc: 0.9922308325767517)
[2025-02-13 03:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:53][root][INFO] - Training Epoch: 2/2, step 2315/7134 completed (loss: 0.011302105151116848, acc: 0.9976470470428467)
[2025-02-13 03:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:53][root][INFO] - Training Epoch: 2/2, step 2316/7134 completed (loss: 0.013916841708123684, acc: 0.9953970313072205)
[2025-02-13 03:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54][root][INFO] - Training Epoch: 2/2, step 2317/7134 completed (loss: 0.025150394067168236, acc: 0.9957671761512756)
[2025-02-13 03:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54][root][INFO] - Training Epoch: 2/2, step 2318/7134 completed (loss: 0.014313366264104843, acc: 0.9968619346618652)
[2025-02-13 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:55][root][INFO] - Training Epoch: 2/2, step 2319/7134 completed (loss: 0.013516144827008247, acc: 0.9945295453071594)
[2025-02-13 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:55][root][INFO] - Training Epoch: 2/2, step 2320/7134 completed (loss: 0.009512620978057384, acc: 0.9965909123420715)
[2025-02-13 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56][root][INFO] - Training Epoch: 2/2, step 2321/7134 completed (loss: 0.0205022394657135, acc: 0.9944444298744202)
[2025-02-13 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56][root][INFO] - Training Epoch: 2/2, step 2322/7134 completed (loss: 0.020261896774172783, acc: 0.9881481528282166)
[2025-02-13 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:57][root][INFO] - Training Epoch: 2/2, step 2323/7134 completed (loss: 0.02602306194603443, acc: 0.9905362725257874)
[2025-02-13 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:57][root][INFO] - Training Epoch: 2/2, step 2324/7134 completed (loss: 0.04013427346944809, acc: 0.9900285005569458)
[2025-02-13 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:57][root][INFO] - Training Epoch: 2/2, step 2325/7134 completed (loss: 0.021551406010985374, acc: 0.9948717951774597)
[2025-02-13 03:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58][root][INFO] - Training Epoch: 2/2, step 2326/7134 completed (loss: 0.01329270750284195, acc: 0.9908088445663452)
[2025-02-13 03:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58][root][INFO] - Training Epoch: 2/2, step 2327/7134 completed (loss: 0.009820274077355862, acc: 0.9950819611549377)
[2025-02-13 03:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:59][root][INFO] - Training Epoch: 2/2, step 2328/7134 completed (loss: 0.059170398861169815, acc: 0.9856630563735962)
[2025-02-13 03:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:59][root][INFO] - Training Epoch: 2/2, step 2329/7134 completed (loss: 0.02608935348689556, acc: 0.9953488111495972)
[2025-02-13 03:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00][root][INFO] - Training Epoch: 2/2, step 2330/7134 completed (loss: 0.008765274658799171, acc: 0.9984732866287231)
[2025-02-13 03:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00][root][INFO] - Training Epoch: 2/2, step 2331/7134 completed (loss: 0.004721274599432945, acc: 1.0)
[2025-02-13 03:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00][root][INFO] - Training Epoch: 2/2, step 2332/7134 completed (loss: 0.03303627669811249, acc: 0.9940564632415771)
[2025-02-13 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:01][root][INFO] - Training Epoch: 2/2, step 2333/7134 completed (loss: 0.008826298639178276, acc: 0.9970238208770752)
[2025-02-13 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:01][root][INFO] - Training Epoch: 2/2, step 2334/7134 completed (loss: 0.014099243097007275, acc: 0.9955223798751831)
[2025-02-13 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02][root][INFO] - Training Epoch: 2/2, step 2335/7134 completed (loss: 0.03161529079079628, acc: 0.9919354915618896)
[2025-02-13 03:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02][root][INFO] - Training Epoch: 2/2, step 2336/7134 completed (loss: 0.008055515587329865, acc: 0.9984543919563293)
[2025-02-13 03:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:03][root][INFO] - Training Epoch: 2/2, step 2337/7134 completed (loss: 0.0034997027833014727, acc: 1.0)
[2025-02-13 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:03][root][INFO] - Training Epoch: 2/2, step 2338/7134 completed (loss: 0.01258443295955658, acc: 0.9938176274299622)
[2025-02-13 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:03][root][INFO] - Training Epoch: 2/2, step 2339/7134 completed (loss: 0.029449477791786194, acc: 0.9918434023857117)
[2025-02-13 03:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04][root][INFO] - Training Epoch: 2/2, step 2340/7134 completed (loss: 0.011219365522265434, acc: 0.9969834089279175)
[2025-02-13 03:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04][root][INFO] - Training Epoch: 2/2, step 2341/7134 completed (loss: 0.005264998879283667, acc: 0.9981203079223633)
[2025-02-13 03:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:05][root][INFO] - Training Epoch: 2/2, step 2342/7134 completed (loss: 0.021467342972755432, acc: 0.9940915703773499)
[2025-02-13 03:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:05][root][INFO] - Training Epoch: 2/2, step 2343/7134 completed (loss: 0.012240070849657059, acc: 0.9968000054359436)
[2025-02-13 03:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06][root][INFO] - Training Epoch: 2/2, step 2344/7134 completed (loss: 0.009850434958934784, acc: 0.9953051805496216)
[2025-02-13 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06][root][INFO] - Training Epoch: 2/2, step 2345/7134 completed (loss: 0.016547953709959984, acc: 0.9944055676460266)
[2025-02-13 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06][root][INFO] - Training Epoch: 2/2, step 2346/7134 completed (loss: 0.010442382656037807, acc: 0.9957507252693176)
[2025-02-13 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:07][root][INFO] - Training Epoch: 2/2, step 2347/7134 completed (loss: 0.019085347652435303, acc: 0.9950658082962036)
[2025-02-13 03:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:07][root][INFO] - Training Epoch: 2/2, step 2348/7134 completed (loss: 0.06834959238767624, acc: 0.9810844659805298)
[2025-02-13 03:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08][root][INFO] - Training Epoch: 2/2, step 2349/7134 completed (loss: 0.03228360414505005, acc: 0.9916666746139526)
[2025-02-13 03:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08][root][INFO] - Training Epoch: 2/2, step 2350/7134 completed (loss: 0.056717053055763245, acc: 0.9878048896789551)
[2025-02-13 03:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09][root][INFO] - Training Epoch: 2/2, step 2351/7134 completed (loss: 0.03484603017568588, acc: 0.9918032884597778)
[2025-02-13 03:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09][root][INFO] - Training Epoch: 2/2, step 2352/7134 completed (loss: 0.0313134491443634, acc: 0.9935897588729858)
[2025-02-13 03:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09][root][INFO] - Training Epoch: 2/2, step 2353/7134 completed (loss: 0.04200907424092293, acc: 0.9886524677276611)
[2025-02-13 03:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:10][root][INFO] - Training Epoch: 2/2, step 2354/7134 completed (loss: 0.017705349251627922, acc: 0.9924050569534302)
[2025-02-13 03:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:10][root][INFO] - Training Epoch: 2/2, step 2355/7134 completed (loss: 0.0240581426769495, acc: 0.9925925731658936)
[2025-02-13 03:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11][root][INFO] - Training Epoch: 2/2, step 2356/7134 completed (loss: 0.039499443024396896, acc: 0.9893993139266968)
[2025-02-13 03:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11][root][INFO] - Training Epoch: 2/2, step 2357/7134 completed (loss: 0.02401469647884369, acc: 0.9940476417541504)
[2025-02-13 03:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12][root][INFO] - Training Epoch: 2/2, step 2358/7134 completed (loss: 0.028708377853035927, acc: 0.9903614521026611)
[2025-02-13 03:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12][root][INFO] - Training Epoch: 2/2, step 2359/7134 completed (loss: 0.0333227701485157, acc: 0.988252580165863)
[2025-02-13 03:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12][root][INFO] - Training Epoch: 2/2, step 2360/7134 completed (loss: 0.02430613338947296, acc: 0.9941434860229492)
[2025-02-13 03:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:13][root][INFO] - Training Epoch: 2/2, step 2361/7134 completed (loss: 0.0253616850823164, acc: 0.9926035404205322)
[2025-02-13 03:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:13][root][INFO] - Training Epoch: 2/2, step 2362/7134 completed (loss: 0.023041095584630966, acc: 0.9910314083099365)
[2025-02-13 03:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14][root][INFO] - Training Epoch: 2/2, step 2363/7134 completed (loss: 0.03128650784492493, acc: 0.9918887615203857)
[2025-02-13 03:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14][root][INFO] - Training Epoch: 2/2, step 2364/7134 completed (loss: 0.013607398606836796, acc: 0.9954338073730469)
[2025-02-13 03:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15][root][INFO] - Training Epoch: 2/2, step 2365/7134 completed (loss: 0.011105936951935291, acc: 0.9958677887916565)
[2025-02-13 03:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15][root][INFO] - Training Epoch: 2/2, step 2366/7134 completed (loss: 0.00786726363003254, acc: 0.9988425970077515)
[2025-02-13 03:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15][root][INFO] - Training Epoch: 2/2, step 2367/7134 completed (loss: 0.03820330277085304, acc: 0.9924623370170593)
[2025-02-13 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:16][root][INFO] - Training Epoch: 2/2, step 2368/7134 completed (loss: 0.020477566868066788, acc: 0.9951515197753906)
[2025-02-13 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:16][root][INFO] - Training Epoch: 2/2, step 2369/7134 completed (loss: 0.008451495319604874, acc: 0.9976470470428467)
[2025-02-13 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17][root][INFO] - Training Epoch: 2/2, step 2370/7134 completed (loss: 0.013450113125145435, acc: 0.9969135522842407)
[2025-02-13 03:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17][root][INFO] - Training Epoch: 2/2, step 2371/7134 completed (loss: 0.028383556753396988, acc: 0.9971181750297546)
[2025-02-13 03:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:18][root][INFO] - Training Epoch: 2/2, step 2372/7134 completed (loss: 0.018281888216733932, acc: 0.9965986609458923)
[2025-02-13 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:18][root][INFO] - Training Epoch: 2/2, step 2373/7134 completed (loss: 0.002913110423833132, acc: 1.0)
[2025-02-13 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19][root][INFO] - Training Epoch: 2/2, step 2374/7134 completed (loss: 0.01681392639875412, acc: 0.9946452379226685)
[2025-02-13 03:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19][root][INFO] - Training Epoch: 2/2, step 2375/7134 completed (loss: 0.023357652127742767, acc: 0.9954545497894287)
[2025-02-13 03:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19][root][INFO] - Training Epoch: 2/2, step 2376/7134 completed (loss: 0.06826820224523544, acc: 0.9846677780151367)
[2025-02-13 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:20][root][INFO] - Training Epoch: 2/2, step 2377/7134 completed (loss: 0.0451219268143177, acc: 0.9880525469779968)
[2025-02-13 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:20][root][INFO] - Training Epoch: 2/2, step 2378/7134 completed (loss: 0.03421027958393097, acc: 0.9882628917694092)
[2025-02-13 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21][root][INFO] - Training Epoch: 2/2, step 2379/7134 completed (loss: 0.0692872405052185, acc: 0.9765517115592957)
[2025-02-13 03:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21][root][INFO] - Training Epoch: 2/2, step 2380/7134 completed (loss: 0.025465363636612892, acc: 0.9915356636047363)
[2025-02-13 03:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22][root][INFO] - Training Epoch: 2/2, step 2381/7134 completed (loss: 0.07803685963153839, acc: 0.9780033826828003)
[2025-02-13 03:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22][root][INFO] - Training Epoch: 2/2, step 2382/7134 completed (loss: 0.023951124399900436, acc: 0.9923547506332397)
[2025-02-13 03:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22][root][INFO] - Training Epoch: 2/2, step 2383/7134 completed (loss: 0.06936878710985184, acc: 0.9788838624954224)
[2025-02-13 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23][root][INFO] - Training Epoch: 2/2, step 2384/7134 completed (loss: 0.06630431860685349, acc: 0.9869281053543091)
[2025-02-13 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23][root][INFO] - Training Epoch: 2/2, step 2385/7134 completed (loss: 0.023193249478936195, acc: 0.988950252532959)
[2025-02-13 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:24][root][INFO] - Training Epoch: 2/2, step 2386/7134 completed (loss: 0.029554134234786034, acc: 0.9882199168205261)
[2025-02-13 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:24][root][INFO] - Training Epoch: 2/2, step 2387/7134 completed (loss: 0.03549527749419212, acc: 0.988399088382721)
[2025-02-13 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:25][root][INFO] - Training Epoch: 2/2, step 2388/7134 completed (loss: 0.07168473303318024, acc: 0.9785459041595459)
[2025-02-13 03:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:25][root][INFO] - Training Epoch: 2/2, step 2389/7134 completed (loss: 0.05629117041826248, acc: 0.9850746393203735)
[2025-02-13 03:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26][root][INFO] - Training Epoch: 2/2, step 2390/7134 completed (loss: 0.056904248893260956, acc: 0.9820788502693176)
[2025-02-13 03:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26][root][INFO] - Training Epoch: 2/2, step 2391/7134 completed (loss: 0.059213727712631226, acc: 0.9783989787101746)
[2025-02-13 03:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:27][root][INFO] - Training Epoch: 2/2, step 2392/7134 completed (loss: 0.03115127794444561, acc: 0.9919893145561218)
[2025-02-13 03:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:27][root][INFO] - Training Epoch: 2/2, step 2393/7134 completed (loss: 0.05087867006659508, acc: 0.9805447459220886)
[2025-02-13 03:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:27][root][INFO] - Training Epoch: 2/2, step 2394/7134 completed (loss: 0.031021375209093094, acc: 0.9915151596069336)
[2025-02-13 03:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28][root][INFO] - Training Epoch: 2/2, step 2395/7134 completed (loss: 0.10739811509847641, acc: 0.9706840515136719)
[2025-02-13 03:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28][root][INFO] - Training Epoch: 2/2, step 2396/7134 completed (loss: 0.058444492518901825, acc: 0.9848484992980957)
[2025-02-13 03:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:29][root][INFO] - Training Epoch: 2/2, step 2397/7134 completed (loss: 0.05294493958353996, acc: 0.9854439496994019)
[2025-02-13 03:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:29][root][INFO] - Training Epoch: 2/2, step 2398/7134 completed (loss: 0.02426949143409729, acc: 0.9903714060783386)
[2025-02-13 03:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:30][root][INFO] - Training Epoch: 2/2, step 2399/7134 completed (loss: 0.044035863131284714, acc: 0.9856938719749451)
[2025-02-13 03:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:30][root][INFO] - Training Epoch: 2/2, step 2400/7134 completed (loss: 0.01707085408270359, acc: 0.9972413778305054)
[2025-02-13 03:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31][root][INFO] - Training Epoch: 2/2, step 2401/7134 completed (loss: 0.0885811373591423, acc: 0.9841772317886353)
[2025-02-13 03:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31][root][INFO] - Training Epoch: 2/2, step 2402/7134 completed (loss: 0.0549420528113842, acc: 0.980079710483551)
[2025-02-13 03:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31][root][INFO] - Training Epoch: 2/2, step 2403/7134 completed (loss: 0.03530154004693031, acc: 0.9858155846595764)
[2025-02-13 03:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:32][root][INFO] - Training Epoch: 2/2, step 2404/7134 completed (loss: 0.015462211333215237, acc: 0.994535505771637)
[2025-02-13 03:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:32][root][INFO] - Training Epoch: 2/2, step 2405/7134 completed (loss: 0.042375076562166214, acc: 0.9832776188850403)
[2025-02-13 03:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33][root][INFO] - Training Epoch: 2/2, step 2406/7134 completed (loss: 0.013527492061257362, acc: 0.9946808218955994)
[2025-02-13 03:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33][root][INFO] - Training Epoch: 2/2, step 2407/7134 completed (loss: 0.018883472308516502, acc: 0.9954407215118408)
[2025-02-13 03:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:34][root][INFO] - Training Epoch: 2/2, step 2408/7134 completed (loss: 0.0843091532588005, acc: 0.9736841917037964)
[2025-02-13 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:34][root][INFO] - Training Epoch: 2/2, step 2409/7134 completed (loss: 0.013890266418457031, acc: 0.998633861541748)
[2025-02-13 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35][root][INFO] - Training Epoch: 2/2, step 2410/7134 completed (loss: 0.02564823441207409, acc: 0.9942611455917358)
[2025-02-13 03:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35][root][INFO] - Training Epoch: 2/2, step 2411/7134 completed (loss: 0.02014351077377796, acc: 0.9961190223693848)
[2025-02-13 03:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35][root][INFO] - Training Epoch: 2/2, step 2412/7134 completed (loss: 0.02560070902109146, acc: 0.9897435903549194)
[2025-02-13 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:36][root][INFO] - Training Epoch: 2/2, step 2413/7134 completed (loss: 0.01190898846834898, acc: 0.9923760890960693)
[2025-02-13 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:36][root][INFO] - Training Epoch: 2/2, step 2414/7134 completed (loss: 0.01109379343688488, acc: 0.996268630027771)
[2025-02-13 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37][root][INFO] - Training Epoch: 2/2, step 2415/7134 completed (loss: 0.0573933906853199, acc: 0.9890260696411133)
[2025-02-13 03:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37][root][INFO] - Training Epoch: 2/2, step 2416/7134 completed (loss: 0.02642345055937767, acc: 0.9889349937438965)
[2025-02-13 03:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:38][root][INFO] - Training Epoch: 2/2, step 2417/7134 completed (loss: 0.04048039764165878, acc: 0.9921962022781372)
[2025-02-13 03:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:38][root][INFO] - Training Epoch: 2/2, step 2418/7134 completed (loss: 0.01974092237651348, acc: 0.9943740963935852)
[2025-02-13 03:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:39][root][INFO] - Training Epoch: 2/2, step 2419/7134 completed (loss: 0.017882604151964188, acc: 0.9942775368690491)
[2025-02-13 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:39][root][INFO] - Training Epoch: 2/2, step 2420/7134 completed (loss: 0.013241524808108807, acc: 0.9966832399368286)
[2025-02-13 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:39][root][INFO] - Training Epoch: 2/2, step 2421/7134 completed (loss: 0.022506127133965492, acc: 0.9900497794151306)
[2025-02-13 03:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40][root][INFO] - Training Epoch: 2/2, step 2422/7134 completed (loss: 0.010313205420970917, acc: 0.9988179802894592)
[2025-02-13 03:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40][root][INFO] - Training Epoch: 2/2, step 2423/7134 completed (loss: 0.010698038153350353, acc: 0.9958158731460571)
[2025-02-13 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:41][root][INFO] - Training Epoch: 2/2, step 2424/7134 completed (loss: 0.015409175306558609, acc: 0.9971910119056702)
[2025-02-13 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:41][root][INFO] - Training Epoch: 2/2, step 2425/7134 completed (loss: 0.021343212574720383, acc: 0.9933333396911621)
[2025-02-13 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42][root][INFO] - Training Epoch: 2/2, step 2426/7134 completed (loss: 0.013453695923089981, acc: 0.9940546751022339)
[2025-02-13 03:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42][root][INFO] - Training Epoch: 2/2, step 2427/7134 completed (loss: 0.006618502549827099, acc: 0.9976798295974731)
[2025-02-13 03:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:43][root][INFO] - Training Epoch: 2/2, step 2428/7134 completed (loss: 0.01616251841187477, acc: 0.9961783289909363)
[2025-02-13 03:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:43][root][INFO] - Training Epoch: 2/2, step 2429/7134 completed (loss: 0.025397928431630135, acc: 0.9937694668769836)
[2025-02-13 03:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44][root][INFO] - Training Epoch: 2/2, step 2430/7134 completed (loss: 0.019145416095852852, acc: 0.9910581111907959)
[2025-02-13 03:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44][root][INFO] - Training Epoch: 2/2, step 2431/7134 completed (loss: 0.021924814209342003, acc: 0.992668628692627)
[2025-02-13 03:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44][root][INFO] - Training Epoch: 2/2, step 2432/7134 completed (loss: 0.039819322526454926, acc: 0.9881154298782349)
[2025-02-13 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:45][root][INFO] - Training Epoch: 2/2, step 2433/7134 completed (loss: 0.027262521907687187, acc: 0.9928977489471436)
[2025-02-13 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:45][root][INFO] - Training Epoch: 2/2, step 2434/7134 completed (loss: 0.04296581819653511, acc: 0.9922360181808472)
[2025-02-13 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46][root][INFO] - Training Epoch: 2/2, step 2435/7134 completed (loss: 0.024156037718057632, acc: 0.992094874382019)
[2025-02-13 03:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46][root][INFO] - Training Epoch: 2/2, step 2436/7134 completed (loss: 0.023128047585487366, acc: 0.9918509721755981)
[2025-02-13 03:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47][root][INFO] - Training Epoch: 2/2, step 2437/7134 completed (loss: 0.02153119072318077, acc: 0.9924585223197937)
[2025-02-13 03:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47][root][INFO] - Training Epoch: 2/2, step 2438/7134 completed (loss: 0.03266366198658943, acc: 0.9916267991065979)
[2025-02-13 03:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:48][root][INFO] - Training Epoch: 2/2, step 2439/7134 completed (loss: 0.04267137497663498, acc: 0.9881592988967896)
[2025-02-13 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:48][root][INFO] - Training Epoch: 2/2, step 2440/7134 completed (loss: 0.019965985789895058, acc: 0.9910314083099365)
[2025-02-13 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:48][root][INFO] - Training Epoch: 2/2, step 2441/7134 completed (loss: 0.07831890881061554, acc: 0.9769673943519592)
[2025-02-13 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:49][root][INFO] - Training Epoch: 2/2, step 2442/7134 completed (loss: 0.037021853029727936, acc: 0.9846153855323792)
[2025-02-13 03:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:49][root][INFO] - Training Epoch: 2/2, step 2443/7134 completed (loss: 0.021107204258441925, acc: 0.9930459260940552)
[2025-02-13 03:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50][root][INFO] - Training Epoch: 2/2, step 2444/7134 completed (loss: 0.035479187965393066, acc: 0.9867424368858337)
[2025-02-13 03:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50][root][INFO] - Training Epoch: 2/2, step 2445/7134 completed (loss: 0.03151875361800194, acc: 0.9885786771774292)
[2025-02-13 03:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51][root][INFO] - Training Epoch: 2/2, step 2446/7134 completed (loss: 0.07981748878955841, acc: 0.9791666865348816)
[2025-02-13 03:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51][root][INFO] - Training Epoch: 2/2, step 2447/7134 completed (loss: 0.027438322082161903, acc: 0.9900110960006714)
[2025-02-13 03:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:52][root][INFO] - Training Epoch: 2/2, step 2448/7134 completed (loss: 0.023656075820326805, acc: 0.9919785857200623)
[2025-02-13 03:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:52][root][INFO] - Training Epoch: 2/2, step 2449/7134 completed (loss: 0.025071389973163605, acc: 0.9907161593437195)
[2025-02-13 03:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:52][root][INFO] - Training Epoch: 2/2, step 2450/7134 completed (loss: 0.023971201851963997, acc: 0.9911054372787476)
[2025-02-13 03:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53][root][INFO] - Training Epoch: 2/2, step 2451/7134 completed (loss: 0.009323645383119583, acc: 0.9961783289909363)
[2025-02-13 03:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53][root][INFO] - Training Epoch: 2/2, step 2452/7134 completed (loss: 0.01842854917049408, acc: 0.9934980273246765)
[2025-02-13 03:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:54][root][INFO] - Training Epoch: 2/2, step 2453/7134 completed (loss: 0.031985748559236526, acc: 0.9870967864990234)
[2025-02-13 03:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:54][root][INFO] - Training Epoch: 2/2, step 2454/7134 completed (loss: 0.014369280077517033, acc: 0.9933110475540161)
[2025-02-13 03:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:55][root][INFO] - Training Epoch: 2/2, step 2455/7134 completed (loss: 0.021817440167069435, acc: 0.9929178357124329)
[2025-02-13 03:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:55][root][INFO] - Training Epoch: 2/2, step 2456/7134 completed (loss: 0.02990400232374668, acc: 0.9922178983688354)
[2025-02-13 03:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:56][root][INFO] - Training Epoch: 2/2, step 2457/7134 completed (loss: 0.03151978179812431, acc: 0.9883527159690857)
[2025-02-13 03:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:56][root][INFO] - Training Epoch: 2/2, step 2458/7134 completed (loss: 0.07707072049379349, acc: 0.9842767119407654)
[2025-02-13 03:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57][root][INFO] - Training Epoch: 2/2, step 2459/7134 completed (loss: 0.007961335591971874, acc: 0.9955621361732483)
[2025-02-13 03:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57][root][INFO] - Training Epoch: 2/2, step 2460/7134 completed (loss: 0.021125316619873047, acc: 0.9944674968719482)
[2025-02-13 03:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57][root][INFO] - Training Epoch: 2/2, step 2461/7134 completed (loss: 0.015405461192131042, acc: 0.9942029118537903)
[2025-02-13 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:58][root][INFO] - Training Epoch: 2/2, step 2462/7134 completed (loss: 0.03490843623876572, acc: 0.9889655113220215)
[2025-02-13 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:58][root][INFO] - Training Epoch: 2/2, step 2463/7134 completed (loss: 0.014847702346742153, acc: 0.9954614043235779)
[2025-02-13 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:59][root][INFO] - Training Epoch: 2/2, step 2464/7134 completed (loss: 0.025650763884186745, acc: 0.9925093650817871)
[2025-02-13 03:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:59][root][INFO] - Training Epoch: 2/2, step 2465/7134 completed (loss: 0.03032655268907547, acc: 0.9920529723167419)
[2025-02-13 03:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00][root][INFO] - Training Epoch: 2/2, step 2466/7134 completed (loss: 0.011723373085260391, acc: 0.9973509907722473)
[2025-02-13 03:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00][root][INFO] - Training Epoch: 2/2, step 2467/7134 completed (loss: 0.014274795539677143, acc: 0.9922879338264465)
[2025-02-13 03:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00][root][INFO] - Training Epoch: 2/2, step 2468/7134 completed (loss: 0.00871218740940094, acc: 0.9972028136253357)
[2025-02-13 03:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:01][root][INFO] - Training Epoch: 2/2, step 2469/7134 completed (loss: 0.012005772441625595, acc: 0.9959072470664978)
[2025-02-13 03:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:01][root][INFO] - Training Epoch: 2/2, step 2470/7134 completed (loss: 0.017973847687244415, acc: 0.9949495196342468)
[2025-02-13 03:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02][root][INFO] - Training Epoch: 2/2, step 2471/7134 completed (loss: 0.013354465365409851, acc: 0.9966555237770081)
[2025-02-13 03:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02][root][INFO] - Training Epoch: 2/2, step 2472/7134 completed (loss: 0.00551043264567852, acc: 1.0)
[2025-02-13 03:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:03][root][INFO] - Training Epoch: 2/2, step 2473/7134 completed (loss: 0.0185537189245224, acc: 0.9959016442298889)
[2025-02-13 03:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:03][root][INFO] - Training Epoch: 2/2, step 2474/7134 completed (loss: 0.016900109127163887, acc: 0.9943661689758301)
[2025-02-13 03:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:03][root][INFO] - Training Epoch: 2/2, step 2475/7134 completed (loss: 0.017737003043293953, acc: 0.994854211807251)
[2025-02-13 03:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:04][root][INFO] - Training Epoch: 2/2, step 2476/7134 completed (loss: 0.03281422331929207, acc: 0.9902507066726685)
[2025-02-13 03:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:04][root][INFO] - Training Epoch: 2/2, step 2477/7134 completed (loss: 0.014626413583755493, acc: 0.9958563446998596)
[2025-02-13 03:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05][root][INFO] - Training Epoch: 2/2, step 2478/7134 completed (loss: 0.008986644446849823, acc: 0.9953271150588989)
[2025-02-13 03:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05][root][INFO] - Training Epoch: 2/2, step 2479/7134 completed (loss: 0.06664395332336426, acc: 0.9777424335479736)
[2025-02-13 03:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:06][root][INFO] - Training Epoch: 2/2, step 2480/7134 completed (loss: 0.0035350245889276266, acc: 1.0)
[2025-02-13 03:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:06][root][INFO] - Training Epoch: 2/2, step 2481/7134 completed (loss: 0.020145345479249954, acc: 0.9900497794151306)
[2025-02-13 03:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:06][root][INFO] - Training Epoch: 2/2, step 2482/7134 completed (loss: 0.03465033322572708, acc: 0.9839486479759216)
[2025-02-13 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07][root][INFO] - Training Epoch: 2/2, step 2483/7134 completed (loss: 0.010336630046367645, acc: 0.9972260594367981)
[2025-02-13 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07][root][INFO] - Training Epoch: 2/2, step 2484/7134 completed (loss: 0.014927986077964306, acc: 0.9944444298744202)
[2025-02-13 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:08][root][INFO] - Training Epoch: 2/2, step 2485/7134 completed (loss: 0.012267697602510452, acc: 0.9952380657196045)
[2025-02-13 03:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:08][root][INFO] - Training Epoch: 2/2, step 2486/7134 completed (loss: 0.012164773419499397, acc: 0.9985632300376892)
[2025-02-13 03:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09][root][INFO] - Training Epoch: 2/2, step 2487/7134 completed (loss: 0.06075575202703476, acc: 0.98740154504776)
[2025-02-13 03:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09][root][INFO] - Training Epoch: 2/2, step 2488/7134 completed (loss: 0.013768530450761318, acc: 0.9958563446998596)
[2025-02-13 03:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09][root][INFO] - Training Epoch: 2/2, step 2489/7134 completed (loss: 0.013704651035368443, acc: 0.9944289922714233)
[2025-02-13 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:10][root][INFO] - Training Epoch: 2/2, step 2490/7134 completed (loss: 0.008615917526185513, acc: 0.9971469044685364)
[2025-02-13 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:10][root][INFO] - Training Epoch: 2/2, step 2491/7134 completed (loss: 0.07102830708026886, acc: 0.9775132536888123)
[2025-02-13 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11][root][INFO] - Training Epoch: 2/2, step 2492/7134 completed (loss: 0.02393721602857113, acc: 0.9900744557380676)
[2025-02-13 03:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11][root][INFO] - Training Epoch: 2/2, step 2493/7134 completed (loss: 0.03479916974902153, acc: 0.98591548204422)
[2025-02-13 03:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:12][root][INFO] - Training Epoch: 2/2, step 2494/7134 completed (loss: 0.035784319043159485, acc: 0.9887359142303467)
[2025-02-13 03:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:12][root][INFO] - Training Epoch: 2/2, step 2495/7134 completed (loss: 0.03308449313044548, acc: 0.9918588995933533)
[2025-02-13 03:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13][root][INFO] - Training Epoch: 2/2, step 2496/7134 completed (loss: 0.017579367384314537, acc: 0.9943820238113403)
[2025-02-13 03:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13][root][INFO] - Training Epoch: 2/2, step 2497/7134 completed (loss: 0.014902621507644653, acc: 0.9963503479957581)
[2025-02-13 03:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13][root][INFO] - Training Epoch: 2/2, step 2498/7134 completed (loss: 0.017107808962464333, acc: 0.9943116903305054)
[2025-02-13 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:14][root][INFO] - Training Epoch: 2/2, step 2499/7134 completed (loss: 0.013992677442729473, acc: 0.9975845217704773)
[2025-02-13 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:14][root][INFO] - Training Epoch: 2/2, step 2500/7134 completed (loss: 0.015552278608083725, acc: 0.9948520064353943)
[2025-02-13 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:15][root][INFO] - Training Epoch: 2/2, step 2501/7134 completed (loss: 0.022759560495615005, acc: 0.9931350350379944)
[2025-02-13 03:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:15][root][INFO] - Training Epoch: 2/2, step 2502/7134 completed (loss: 0.02058795653283596, acc: 0.9946996569633484)
[2025-02-13 03:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16][root][INFO] - Training Epoch: 2/2, step 2503/7134 completed (loss: 0.02631966769695282, acc: 0.994301974773407)
[2025-02-13 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16][root][INFO] - Training Epoch: 2/2, step 2504/7134 completed (loss: 0.019567640498280525, acc: 0.9933333396911621)
[2025-02-13 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:17][root][INFO] - Training Epoch: 2/2, step 2505/7134 completed (loss: 0.01751473918557167, acc: 0.9920364022254944)
[2025-02-13 03:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:17][root][INFO] - Training Epoch: 2/2, step 2506/7134 completed (loss: 0.015258032828569412, acc: 0.995192289352417)
[2025-02-13 03:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:17][root][INFO] - Training Epoch: 2/2, step 2507/7134 completed (loss: 0.006476871203631163, acc: 0.9976771473884583)
[2025-02-13 03:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:18][root][INFO] - Training Epoch: 2/2, step 2508/7134 completed (loss: 0.03281402960419655, acc: 0.9896313548088074)
[2025-02-13 03:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:18][root][INFO] - Training Epoch: 2/2, step 2509/7134 completed (loss: 0.009597111493349075, acc: 0.9987760186195374)
[2025-02-13 03:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19][root][INFO] - Training Epoch: 2/2, step 2510/7134 completed (loss: 0.018887249752879143, acc: 0.9939302206039429)
[2025-02-13 03:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19][root][INFO] - Training Epoch: 2/2, step 2511/7134 completed (loss: 0.03863203525543213, acc: 0.9910314083099365)
[2025-02-13 03:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:20][root][INFO] - Training Epoch: 2/2, step 2512/7134 completed (loss: 0.05785953998565674, acc: 0.9878542423248291)
[2025-02-13 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:20][root][INFO] - Training Epoch: 2/2, step 2513/7134 completed (loss: 0.01898166909813881, acc: 0.9928910136222839)
[2025-02-13 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21][root][INFO] - Training Epoch: 2/2, step 2514/7134 completed (loss: 0.015316609293222427, acc: 0.9929988384246826)
[2025-02-13 03:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21][root][INFO] - Training Epoch: 2/2, step 2515/7134 completed (loss: 0.007954527623951435, acc: 0.9973992109298706)
[2025-02-13 03:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:22][root][INFO] - Training Epoch: 2/2, step 2516/7134 completed (loss: 0.00911807268857956, acc: 0.9977272748947144)
[2025-02-13 03:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:22][root][INFO] - Training Epoch: 2/2, step 2517/7134 completed (loss: 0.0279269739985466, acc: 0.9908151626586914)
[2025-02-13 03:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:22][root][INFO] - Training Epoch: 2/2, step 2518/7134 completed (loss: 0.021194348111748695, acc: 0.9928469061851501)
[2025-02-13 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:23][root][INFO] - Training Epoch: 2/2, step 2519/7134 completed (loss: 0.0045096599496901035, acc: 0.9972337484359741)
[2025-02-13 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:23][root][INFO] - Training Epoch: 2/2, step 2520/7134 completed (loss: 0.01128584798425436, acc: 0.9971510171890259)
[2025-02-13 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24][root][INFO] - Training Epoch: 2/2, step 2521/7134 completed (loss: 0.05840063840150833, acc: 0.9845559597015381)
[2025-02-13 03:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24][root][INFO] - Training Epoch: 2/2, step 2522/7134 completed (loss: 0.028222056105732918, acc: 0.9914039969444275)
[2025-02-13 03:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:25][root][INFO] - Training Epoch: 2/2, step 2523/7134 completed (loss: 0.03368164598941803, acc: 0.9947643876075745)
[2025-02-13 03:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:25][root][INFO] - Training Epoch: 2/2, step 2524/7134 completed (loss: 0.007107164245098829, acc: 0.9986187815666199)
[2025-02-13 03:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:25][root][INFO] - Training Epoch: 2/2, step 2525/7134 completed (loss: 0.02396334335207939, acc: 0.9883138537406921)
[2025-02-13 03:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26][root][INFO] - Training Epoch: 2/2, step 2526/7134 completed (loss: 0.0021293836180120707, acc: 1.0)
[2025-02-13 03:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26][root][INFO] - Training Epoch: 2/2, step 2527/7134 completed (loss: 0.0181661918759346, acc: 0.995121955871582)
[2025-02-13 03:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:27][root][INFO] - Training Epoch: 2/2, step 2528/7134 completed (loss: 0.0064264764077961445, acc: 0.99726402759552)
[2025-02-13 03:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:27][root][INFO] - Training Epoch: 2/2, step 2529/7134 completed (loss: 0.009822502732276917, acc: 0.9973404407501221)
[2025-02-13 03:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:28][root][INFO] - Training Epoch: 2/2, step 2530/7134 completed (loss: 0.017783058807253838, acc: 0.9974026083946228)
[2025-02-13 03:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:28][root][INFO] - Training Epoch: 2/2, step 2531/7134 completed (loss: 0.0053992923349142075, acc: 0.9972375631332397)
[2025-02-13 03:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29][root][INFO] - Training Epoch: 2/2, step 2532/7134 completed (loss: 0.012832011096179485, acc: 0.9984591603279114)
[2025-02-13 03:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29][root][INFO] - Training Epoch: 2/2, step 2533/7134 completed (loss: 0.02246907167136669, acc: 0.9924585223197937)
[2025-02-13 03:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29][root][INFO] - Training Epoch: 2/2, step 2534/7134 completed (loss: 0.0333411768078804, acc: 0.993630588054657)
[2025-02-13 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:30][root][INFO] - Training Epoch: 2/2, step 2535/7134 completed (loss: 0.017019275575876236, acc: 0.9922928810119629)
[2025-02-13 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:30][root][INFO] - Training Epoch: 2/2, step 2536/7134 completed (loss: 0.01711564138531685, acc: 0.9950799345970154)
[2025-02-13 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31][root][INFO] - Training Epoch: 2/2, step 2537/7134 completed (loss: 0.04384385421872139, acc: 0.9847095012664795)
[2025-02-13 03:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31][root][INFO] - Training Epoch: 2/2, step 2538/7134 completed (loss: 0.05675775185227394, acc: 0.9830028414726257)
[2025-02-13 03:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:32][root][INFO] - Training Epoch: 2/2, step 2539/7134 completed (loss: 0.013843131251633167, acc: 0.9946428537368774)
[2025-02-13 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:32][root][INFO] - Training Epoch: 2/2, step 2540/7134 completed (loss: 0.051148220896720886, acc: 0.9862385392189026)
[2025-02-13 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:32][root][INFO] - Training Epoch: 2/2, step 2541/7134 completed (loss: 0.010657472535967827, acc: 0.996688723564148)
[2025-02-13 03:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:33][root][INFO] - Training Epoch: 2/2, step 2542/7134 completed (loss: 0.05624973028898239, acc: 0.9852070808410645)
[2025-02-13 03:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:33][root][INFO] - Training Epoch: 2/2, step 2543/7134 completed (loss: 0.055642325431108475, acc: 0.9890710115432739)
[2025-02-13 03:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34][root][INFO] - Training Epoch: 2/2, step 2544/7134 completed (loss: 0.027335867285728455, acc: 0.9948453903198242)
[2025-02-13 03:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34][root][INFO] - Training Epoch: 2/2, step 2545/7134 completed (loss: 0.02218225598335266, acc: 0.9930192232131958)
[2025-02-13 03:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:35][root][INFO] - Training Epoch: 2/2, step 2546/7134 completed (loss: 0.09108637273311615, acc: 0.9723889827728271)
[2025-02-13 03:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:35][root][INFO] - Training Epoch: 2/2, step 2547/7134 completed (loss: 0.0388248972594738, acc: 0.9871175289154053)
[2025-02-13 03:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36][root][INFO] - Training Epoch: 2/2, step 2548/7134 completed (loss: 0.025233116000890732, acc: 0.9921259880065918)
[2025-02-13 03:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36][root][INFO] - Training Epoch: 2/2, step 2549/7134 completed (loss: 0.021976454183459282, acc: 0.9944751262664795)
[2025-02-13 03:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36][root][INFO] - Training Epoch: 2/2, step 2550/7134 completed (loss: 0.004869659896939993, acc: 1.0)
[2025-02-13 03:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:37][root][INFO] - Training Epoch: 2/2, step 2551/7134 completed (loss: 0.037031129002571106, acc: 0.9900990128517151)
[2025-02-13 03:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:37][root][INFO] - Training Epoch: 2/2, step 2552/7134 completed (loss: 0.008880838751792908, acc: 0.9968454241752625)
[2025-02-13 03:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:38][root][INFO] - Training Epoch: 2/2, step 2553/7134 completed (loss: 0.028395231813192368, acc: 0.9909365773200989)
[2025-02-13 03:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:38][root][INFO] - Training Epoch: 2/2, step 2554/7134 completed (loss: 0.04136770963668823, acc: 0.9916247725486755)
[2025-02-13 03:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:38][root][INFO] - Training Epoch: 2/2, step 2555/7134 completed (loss: 0.020090341567993164, acc: 0.9958677887916565)
[2025-02-13 03:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39][root][INFO] - Training Epoch: 2/2, step 2556/7134 completed (loss: 0.033054422587156296, acc: 0.9895178079605103)
[2025-02-13 03:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39][root][INFO] - Training Epoch: 2/2, step 2557/7134 completed (loss: 0.031189853325486183, acc: 0.9934425950050354)
[2025-02-13 03:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:40][root][INFO] - Training Epoch: 2/2, step 2558/7134 completed (loss: 0.011884473264217377, acc: 0.9967637658119202)
[2025-02-13 03:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:40][root][INFO] - Training Epoch: 2/2, step 2559/7134 completed (loss: 0.01879853941500187, acc: 0.9972936511039734)
[2025-02-13 03:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:41][root][INFO] - Training Epoch: 2/2, step 2560/7134 completed (loss: 0.020833995193243027, acc: 0.9918699264526367)
[2025-02-13 03:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:41][root][INFO] - Training Epoch: 2/2, step 2561/7134 completed (loss: 0.028982220217585564, acc: 0.9912853837013245)
[2025-02-13 03:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42][root][INFO] - Training Epoch: 2/2, step 2562/7134 completed (loss: 0.028779687359929085, acc: 0.9897435903549194)
[2025-02-13 03:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42][root][INFO] - Training Epoch: 2/2, step 2563/7134 completed (loss: 0.030320346355438232, acc: 0.9912023544311523)
[2025-02-13 03:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42][root][INFO] - Training Epoch: 2/2, step 2564/7134 completed (loss: 0.07264959067106247, acc: 0.9832214713096619)
[2025-02-13 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:43][root][INFO] - Training Epoch: 2/2, step 2565/7134 completed (loss: 0.020989898592233658, acc: 0.9924471378326416)
[2025-02-13 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:43][root][INFO] - Training Epoch: 2/2, step 2566/7134 completed (loss: 0.02174871601164341, acc: 0.9915730357170105)
[2025-02-13 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44][root][INFO] - Training Epoch: 2/2, step 2567/7134 completed (loss: 0.021185753867030144, acc: 0.9894179701805115)
[2025-02-13 03:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44][root][INFO] - Training Epoch: 2/2, step 2568/7134 completed (loss: 0.029939213767647743, acc: 0.9898843765258789)
[2025-02-13 03:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45][root][INFO] - Training Epoch: 2/2, step 2569/7134 completed (loss: 0.023409269750118256, acc: 0.9924699068069458)
[2025-02-13 03:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45][root][INFO] - Training Epoch: 2/2, step 2570/7134 completed (loss: 0.02814474143087864, acc: 0.9940029978752136)
[2025-02-13 03:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45][root][INFO] - Training Epoch: 2/2, step 2571/7134 completed (loss: 0.016808481886982918, acc: 0.9964850544929504)
[2025-02-13 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46][root][INFO] - Training Epoch: 2/2, step 2572/7134 completed (loss: 0.01962221786379814, acc: 0.9936908483505249)
[2025-02-13 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46][root][INFO] - Training Epoch: 2/2, step 2573/7134 completed (loss: 0.03183987736701965, acc: 0.9934425950050354)
[2025-02-13 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47][root][INFO] - Training Epoch: 2/2, step 2574/7134 completed (loss: 0.030803008005023003, acc: 0.9912663698196411)
[2025-02-13 03:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47][root][INFO] - Training Epoch: 2/2, step 2575/7134 completed (loss: 0.018361233174800873, acc: 0.9944238066673279)
[2025-02-13 03:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:48][root][INFO] - Training Epoch: 2/2, step 2576/7134 completed (loss: 0.01762431487441063, acc: 0.9931034445762634)
[2025-02-13 03:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:48][root][INFO] - Training Epoch: 2/2, step 2577/7134 completed (loss: 0.04535572603344917, acc: 0.9897435903549194)
[2025-02-13 03:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49][root][INFO] - Training Epoch: 2/2, step 2578/7134 completed (loss: 0.044580940157175064, acc: 0.9816232919692993)
[2025-02-13 03:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49][root][INFO] - Training Epoch: 2/2, step 2579/7134 completed (loss: 0.04250442236661911, acc: 0.9841772317886353)
[2025-02-13 03:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49][root][INFO] - Training Epoch: 2/2, step 2580/7134 completed (loss: 0.037922702729701996, acc: 0.9882506728172302)
[2025-02-13 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50][root][INFO] - Training Epoch: 2/2, step 2581/7134 completed (loss: 0.0386245921254158, acc: 0.9905787110328674)
[2025-02-13 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50][root][INFO] - Training Epoch: 2/2, step 2582/7134 completed (loss: 0.031169822439551353, acc: 0.989159882068634)
[2025-02-13 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:51][root][INFO] - Training Epoch: 2/2, step 2583/7134 completed (loss: 0.032141879200935364, acc: 0.9900426864624023)
[2025-02-13 03:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:51][root][INFO] - Training Epoch: 2/2, step 2584/7134 completed (loss: 0.04658966511487961, acc: 0.9877836108207703)
[2025-02-13 03:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52][root][INFO] - Training Epoch: 2/2, step 2585/7134 completed (loss: 0.046944499015808105, acc: 0.9884892106056213)
[2025-02-13 03:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52][root][INFO] - Training Epoch: 2/2, step 2586/7134 completed (loss: 0.04696964845061302, acc: 0.9894366264343262)
[2025-02-13 03:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52][root][INFO] - Training Epoch: 2/2, step 2587/7134 completed (loss: 0.028651028871536255, acc: 0.9914004802703857)
[2025-02-13 03:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:53][root][INFO] - Training Epoch: 2/2, step 2588/7134 completed (loss: 0.007472668774425983, acc: 0.9986357688903809)
[2025-02-13 03:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:53][root][INFO] - Training Epoch: 2/2, step 2589/7134 completed (loss: 0.03697822615504265, acc: 0.9911660552024841)
[2025-02-13 03:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54][root][INFO] - Training Epoch: 2/2, step 2590/7134 completed (loss: 0.023576658219099045, acc: 0.9950980544090271)
[2025-02-13 03:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54][root][INFO] - Training Epoch: 2/2, step 2591/7134 completed (loss: 0.042246997356414795, acc: 0.9820972084999084)
[2025-02-13 03:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:55][root][INFO] - Training Epoch: 2/2, step 2592/7134 completed (loss: 0.034776393324136734, acc: 0.9926199316978455)
[2025-02-13 03:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:55][root][INFO] - Training Epoch: 2/2, step 2593/7134 completed (loss: 0.02430291287600994, acc: 0.9973822236061096)
[2025-02-13 03:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56][root][INFO] - Training Epoch: 2/2, step 2594/7134 completed (loss: 0.040976520627737045, acc: 0.9892966151237488)
[2025-02-13 03:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56][root][INFO] - Training Epoch: 2/2, step 2595/7134 completed (loss: 0.0150248222053051, acc: 0.9959677457809448)
[2025-02-13 03:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56][root][INFO] - Training Epoch: 2/2, step 2596/7134 completed (loss: 0.031093232333660126, acc: 0.9949874877929688)
[2025-02-13 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:57][root][INFO] - Training Epoch: 2/2, step 2597/7134 completed (loss: 0.03529038280248642, acc: 0.9905405640602112)
[2025-02-13 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:57][root][INFO] - Training Epoch: 2/2, step 2598/7134 completed (loss: 0.04093800112605095, acc: 0.99071204662323)
[2025-02-13 03:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58][root][INFO] - Training Epoch: 2/2, step 2599/7134 completed (loss: 0.0027047842741012573, acc: 1.0)
[2025-02-13 03:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58][root][INFO] - Training Epoch: 2/2, step 2600/7134 completed (loss: 0.027861032634973526, acc: 0.98975670337677)
[2025-02-13 03:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:59][root][INFO] - Training Epoch: 2/2, step 2601/7134 completed (loss: 0.03320732340216637, acc: 0.9932340979576111)
[2025-02-13 03:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:59][root][INFO] - Training Epoch: 2/2, step 2602/7134 completed (loss: 0.04016546532511711, acc: 0.990314781665802)
[2025-02-13 03:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:00][root][INFO] - Training Epoch: 2/2, step 2603/7134 completed (loss: 0.03271666541695595, acc: 0.9922480583190918)
[2025-02-13 03:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:00][root][INFO] - Training Epoch: 2/2, step 2604/7134 completed (loss: 0.018311452120542526, acc: 0.9945175647735596)
[2025-02-13 03:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01][root][INFO] - Training Epoch: 2/2, step 2605/7134 completed (loss: 0.03285901993513107, acc: 0.9855595827102661)
[2025-02-13 03:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01][root][INFO] - Training Epoch: 2/2, step 2606/7134 completed (loss: 0.01602979376912117, acc: 0.9913669228553772)
[2025-02-13 03:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:02][root][INFO] - Training Epoch: 2/2, step 2607/7134 completed (loss: 0.03551778569817543, acc: 0.9882352948188782)
[2025-02-13 03:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:02][root][INFO] - Training Epoch: 2/2, step 2608/7134 completed (loss: 0.034360941499471664, acc: 0.9913473129272461)
[2025-02-13 03:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:02][root][INFO] - Training Epoch: 2/2, step 2609/7134 completed (loss: 0.033837176859378815, acc: 0.9860724210739136)
[2025-02-13 03:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03][root][INFO] - Training Epoch: 2/2, step 2610/7134 completed (loss: 0.0550379678606987, acc: 0.9908397197723389)
[2025-02-13 03:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03][root][INFO] - Training Epoch: 2/2, step 2611/7134 completed (loss: 0.05598907545208931, acc: 0.9781860113143921)
[2025-02-13 03:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:04][root][INFO] - Training Epoch: 2/2, step 2612/7134 completed (loss: 0.025624489411711693, acc: 0.9886105060577393)
[2025-02-13 03:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:04][root][INFO] - Training Epoch: 2/2, step 2613/7134 completed (loss: 0.0192501749843359, acc: 0.9962025284767151)
[2025-02-13 03:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05][root][INFO] - Training Epoch: 2/2, step 2614/7134 completed (loss: 0.05453967675566673, acc: 0.9772209525108337)
[2025-02-13 03:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05][root][INFO] - Training Epoch: 2/2, step 2615/7134 completed (loss: 0.03327612578868866, acc: 0.9910025596618652)
[2025-02-13 03:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06][root][INFO] - Training Epoch: 2/2, step 2616/7134 completed (loss: 0.05600474402308464, acc: 0.9814814925193787)
[2025-02-13 03:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06][root][INFO] - Training Epoch: 2/2, step 2617/7134 completed (loss: 0.031615179032087326, acc: 0.9905533194541931)
[2025-02-13 03:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06][root][INFO] - Training Epoch: 2/2, step 2618/7134 completed (loss: 0.011301331222057343, acc: 1.0)
[2025-02-13 03:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07][root][INFO] - Training Epoch: 2/2, step 2619/7134 completed (loss: 0.017779313027858734, acc: 0.9932885766029358)
[2025-02-13 03:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07][root][INFO] - Training Epoch: 2/2, step 2620/7134 completed (loss: 0.04096193239092827, acc: 0.9900867342948914)
[2025-02-13 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:08][root][INFO] - Training Epoch: 2/2, step 2621/7134 completed (loss: 0.028310567140579224, acc: 0.9964454770088196)
[2025-02-13 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:08][root][INFO] - Training Epoch: 2/2, step 2622/7134 completed (loss: 0.02615087293088436, acc: 0.9934533834457397)
[2025-02-13 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09][root][INFO] - Training Epoch: 2/2, step 2623/7134 completed (loss: 0.03650827333331108, acc: 0.9885057210922241)
[2025-02-13 03:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09][root][INFO] - Training Epoch: 2/2, step 2624/7134 completed (loss: 0.02977079711854458, acc: 0.9921259880065918)
[2025-02-13 03:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:10][root][INFO] - Training Epoch: 2/2, step 2625/7134 completed (loss: 0.03654026612639427, acc: 0.9888888597488403)
[2025-02-13 03:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:10][root][INFO] - Training Epoch: 2/2, step 2626/7134 completed (loss: 0.013103996403515339, acc: 0.9950494766235352)
[2025-02-13 03:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11][root][INFO] - Training Epoch: 2/2, step 2627/7134 completed (loss: 0.01732182502746582, acc: 0.9959404468536377)
[2025-02-13 03:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11][root][INFO] - Training Epoch: 2/2, step 2628/7134 completed (loss: 0.04373221844434738, acc: 0.9848484992980957)
[2025-02-13 03:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:12][root][INFO] - Training Epoch: 2/2, step 2629/7134 completed (loss: 0.014173747971653938, acc: 0.9962073564529419)
[2025-02-13 03:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:12][root][INFO] - Training Epoch: 2/2, step 2630/7134 completed (loss: 0.020783623680472374, acc: 0.9920127987861633)
[2025-02-13 03:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:12][root][INFO] - Training Epoch: 2/2, step 2631/7134 completed (loss: 0.02511020377278328, acc: 0.9918128848075867)
[2025-02-13 03:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:13][root][INFO] - Training Epoch: 2/2, step 2632/7134 completed (loss: 0.004448461811989546, acc: 1.0)
[2025-02-13 03:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:13][root][INFO] - Training Epoch: 2/2, step 2633/7134 completed (loss: 0.02106563001871109, acc: 0.9929178357124329)
[2025-02-13 03:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14][root][INFO] - Training Epoch: 2/2, step 2634/7134 completed (loss: 0.015150690451264381, acc: 0.995502233505249)
[2025-02-13 03:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14][root][INFO] - Training Epoch: 2/2, step 2635/7134 completed (loss: 0.06749382615089417, acc: 0.980555534362793)
[2025-02-13 03:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:15][root][INFO] - Training Epoch: 2/2, step 2636/7134 completed (loss: 0.0441838763654232, acc: 0.9886202216148376)
[2025-02-13 03:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:15][root][INFO] - Training Epoch: 2/2, step 2637/7134 completed (loss: 0.13265584409236908, acc: 0.971137523651123)
[2025-02-13 03:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:16][root][INFO] - Training Epoch: 2/2, step 2638/7134 completed (loss: 0.07349345088005066, acc: 0.9778357148170471)
[2025-02-13 03:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:16][root][INFO] - Training Epoch: 2/2, step 2639/7134 completed (loss: 0.036330025643110275, acc: 0.9875518679618835)
[2025-02-13 03:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:16][root][INFO] - Training Epoch: 2/2, step 2640/7134 completed (loss: 0.07934220135211945, acc: 0.9825218319892883)
[2025-02-13 03:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17][root][INFO] - Training Epoch: 2/2, step 2641/7134 completed (loss: 0.051010433584451675, acc: 0.9837996959686279)
[2025-02-13 03:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17][root][INFO] - Training Epoch: 2/2, step 2642/7134 completed (loss: 0.07370985299348831, acc: 0.982206404209137)
[2025-02-13 03:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:18][root][INFO] - Training Epoch: 2/2, step 2643/7134 completed (loss: 0.03710557147860527, acc: 0.9867109656333923)
[2025-02-13 03:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:18][root][INFO] - Training Epoch: 2/2, step 2644/7134 completed (loss: 0.038351643830537796, acc: 0.9902912378311157)
[2025-02-13 03:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:19][root][INFO] - Training Epoch: 2/2, step 2645/7134 completed (loss: 0.03775547072291374, acc: 0.9854227304458618)
[2025-02-13 03:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:19][root][INFO] - Training Epoch: 2/2, step 2646/7134 completed (loss: 0.096152164041996, acc: 0.9665604829788208)
[2025-02-13 03:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:20][root][INFO] - Training Epoch: 2/2, step 2647/7134 completed (loss: 0.04406214877963066, acc: 0.9895969033241272)
[2025-02-13 03:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:20][root][INFO] - Training Epoch: 2/2, step 2648/7134 completed (loss: 0.02746422216296196, acc: 0.9883871078491211)
[2025-02-13 03:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:20][root][INFO] - Training Epoch: 2/2, step 2649/7134 completed (loss: 0.08387909829616547, acc: 0.9791356325149536)
[2025-02-13 03:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21][root][INFO] - Training Epoch: 2/2, step 2650/7134 completed (loss: 0.09062829613685608, acc: 0.9727685451507568)
[2025-02-13 03:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21][root][INFO] - Training Epoch: 2/2, step 2651/7134 completed (loss: 0.03515917435288429, acc: 0.9904648661613464)
[2025-02-13 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:22][root][INFO] - Training Epoch: 2/2, step 2652/7134 completed (loss: 0.037630803883075714, acc: 0.9886506795883179)
[2025-02-13 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:22][root][INFO] - Training Epoch: 2/2, step 2653/7134 completed (loss: 0.03285554051399231, acc: 0.9868420958518982)
[2025-02-13 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:23][root][INFO] - Training Epoch: 2/2, step 2654/7134 completed (loss: 0.029461301863193512, acc: 0.9885246157646179)
[2025-02-13 03:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:23][root][INFO] - Training Epoch: 2/2, step 2655/7134 completed (loss: 0.018416177481412888, acc: 0.9939485788345337)
[2025-02-13 03:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24][root][INFO] - Training Epoch: 2/2, step 2656/7134 completed (loss: 0.044298723340034485, acc: 0.9888734221458435)
[2025-02-13 03:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24][root][INFO] - Training Epoch: 2/2, step 2657/7134 completed (loss: 0.0321262888610363, acc: 0.9903225898742676)
[2025-02-13 03:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24][root][INFO] - Training Epoch: 2/2, step 2658/7134 completed (loss: 0.0823107659816742, acc: 0.9786381721496582)
[2025-02-13 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25][root][INFO] - Training Epoch: 2/2, step 2659/7134 completed (loss: 0.035357024520635605, acc: 0.9942528605461121)
[2025-02-13 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25][root][INFO] - Training Epoch: 2/2, step 2660/7134 completed (loss: 0.020979473367333412, acc: 0.9939516186714172)
[2025-02-13 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:26][root][INFO] - Training Epoch: 2/2, step 2661/7134 completed (loss: 0.023447012528777122, acc: 0.9929873943328857)
[2025-02-13 03:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:26][root][INFO] - Training Epoch: 2/2, step 2662/7134 completed (loss: 0.01097998395562172, acc: 0.9971550703048706)
[2025-02-13 03:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:27][root][INFO] - Training Epoch: 2/2, step 2663/7134 completed (loss: 0.020758386701345444, acc: 0.9953216314315796)
[2025-02-13 03:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:27][root][INFO] - Training Epoch: 2/2, step 2664/7134 completed (loss: 0.0056179724633693695, acc: 1.0)
[2025-02-13 03:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28][root][INFO] - Training Epoch: 2/2, step 2665/7134 completed (loss: 0.010394919663667679, acc: 0.9947916865348816)
[2025-02-13 03:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28][root][INFO] - Training Epoch: 2/2, step 2666/7134 completed (loss: 0.008183320052921772, acc: 0.9975520372390747)
[2025-02-13 03:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:29][root][INFO] - Training Epoch: 2/2, step 2667/7134 completed (loss: 0.009210089221596718, acc: 0.9957507252693176)
[2025-02-13 03:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:29][root][INFO] - Training Epoch: 2/2, step 2668/7134 completed (loss: 0.02198154293000698, acc: 0.9932705163955688)
[2025-02-13 03:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:29][root][INFO] - Training Epoch: 2/2, step 2669/7134 completed (loss: 0.00350833497941494, acc: 1.0)
[2025-02-13 03:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30][root][INFO] - Training Epoch: 2/2, step 2670/7134 completed (loss: 0.014840136282145977, acc: 0.9960681796073914)
[2025-02-13 03:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30][root][INFO] - Training Epoch: 2/2, step 2671/7134 completed (loss: 0.01363256387412548, acc: 0.9959893226623535)
[2025-02-13 03:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:31][root][INFO] - Training Epoch: 2/2, step 2672/7134 completed (loss: 0.01017178874462843, acc: 0.9985422492027283)
[2025-02-13 03:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:31][root][INFO] - Training Epoch: 2/2, step 2673/7134 completed (loss: 0.0051391953602433205, acc: 0.9987373948097229)
[2025-02-13 03:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:32][root][INFO] - Training Epoch: 2/2, step 2674/7134 completed (loss: 0.011230573989450932, acc: 0.9944211840629578)
[2025-02-13 03:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:32][root][INFO] - Training Epoch: 2/2, step 2675/7134 completed (loss: 0.0029411399737000465, acc: 1.0)
[2025-02-13 03:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33][root][INFO] - Training Epoch: 2/2, step 2676/7134 completed (loss: 0.010408048518002033, acc: 0.9961038827896118)
[2025-02-13 03:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33][root][INFO] - Training Epoch: 2/2, step 2677/7134 completed (loss: 0.008436420001089573, acc: 0.9970238208770752)
[2025-02-13 03:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:34][root][INFO] - Training Epoch: 2/2, step 2678/7134 completed (loss: 0.0027017584070563316, acc: 1.0)
[2025-02-13 03:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:34][root][INFO] - Training Epoch: 2/2, step 2679/7134 completed (loss: 0.004957632627338171, acc: 0.9988776445388794)
[2025-02-13 03:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:34][root][INFO] - Training Epoch: 2/2, step 2680/7134 completed (loss: 0.010381793603301048, acc: 0.9963008761405945)
[2025-02-13 03:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:35][root][INFO] - Training Epoch: 2/2, step 2681/7134 completed (loss: 0.004786291625350714, acc: 0.9969651103019714)
[2025-02-13 03:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:35][root][INFO] - Training Epoch: 2/2, step 2682/7134 completed (loss: 0.005350373685359955, acc: 0.9984050989151001)
[2025-02-13 03:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36][root][INFO] - Training Epoch: 2/2, step 2683/7134 completed (loss: 0.004024409223347902, acc: 0.9984825253486633)
[2025-02-13 03:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36][root][INFO] - Training Epoch: 2/2, step 2684/7134 completed (loss: 0.02898528054356575, acc: 0.9946236610412598)
[2025-02-13 03:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37][root][INFO] - Training Epoch: 2/2, step 2685/7134 completed (loss: 0.008555682376027107, acc: 0.9973992109298706)
[2025-02-13 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37][root][INFO] - Training Epoch: 2/2, step 2686/7134 completed (loss: 0.03142976760864258, acc: 0.9895833134651184)
[2025-02-13 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:38][root][INFO] - Training Epoch: 2/2, step 2687/7134 completed (loss: 0.018692146986722946, acc: 0.9929328560829163)
[2025-02-13 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:38][root][INFO] - Training Epoch: 2/2, step 2688/7134 completed (loss: 0.023483891040086746, acc: 0.9937106966972351)
[2025-02-13 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:38][root][INFO] - Training Epoch: 2/2, step 2689/7134 completed (loss: 0.02578062005341053, acc: 0.994575023651123)
[2025-02-13 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39][root][INFO] - Training Epoch: 2/2, step 2690/7134 completed (loss: 0.04144241288304329, acc: 0.9876352548599243)
[2025-02-13 03:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39][root][INFO] - Training Epoch: 2/2, step 2691/7134 completed (loss: 0.048413459211587906, acc: 0.984375)
[2025-02-13 03:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40][root][INFO] - Training Epoch: 2/2, step 2692/7134 completed (loss: 0.07089942693710327, acc: 0.9813780188560486)
[2025-02-13 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40][root][INFO] - Training Epoch: 2/2, step 2693/7134 completed (loss: 0.05659881606698036, acc: 0.9846153855323792)
[2025-02-13 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40][root][INFO] - Training Epoch: 2/2, step 2694/7134 completed (loss: 0.08716089278459549, acc: 0.9838235378265381)
[2025-02-13 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41][root][INFO] - Training Epoch: 2/2, step 2695/7134 completed (loss: 0.014014373533427715, acc: 0.9916247725486755)
[2025-02-13 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41][root][INFO] - Training Epoch: 2/2, step 2696/7134 completed (loss: 0.0345141664147377, acc: 0.9936407208442688)
[2025-02-13 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42][root][INFO] - Training Epoch: 2/2, step 2697/7134 completed (loss: 0.041585639119148254, acc: 0.9815100431442261)
[2025-02-13 03:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42][root][INFO] - Training Epoch: 2/2, step 2698/7134 completed (loss: 0.012693099677562714, acc: 0.9965811967849731)
[2025-02-13 03:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43][root][INFO] - Training Epoch: 2/2, step 2699/7134 completed (loss: 0.023906216025352478, acc: 0.9911699891090393)
[2025-02-13 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43][root][INFO] - Training Epoch: 2/2, step 2700/7134 completed (loss: 0.023362504318356514, acc: 0.997032642364502)
[2025-02-13 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43][root][INFO] - Training Epoch: 2/2, step 2701/7134 completed (loss: 0.047467973083257675, acc: 0.9882352948188782)
[2025-02-13 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44][root][INFO] - Training Epoch: 2/2, step 2702/7134 completed (loss: 0.05558161810040474, acc: 0.984375)
[2025-02-13 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44][root][INFO] - Training Epoch: 2/2, step 2703/7134 completed (loss: 0.013768566772341728, acc: 1.0)
[2025-02-13 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44][root][INFO] - Training Epoch: 2/2, step 2704/7134 completed (loss: 0.04486878961324692, acc: 0.9946091771125793)
[2025-02-13 03:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:45][root][INFO] - Training Epoch: 2/2, step 2705/7134 completed (loss: 0.05444171279668808, acc: 0.9876543283462524)
[2025-02-13 03:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:45][root][INFO] - Training Epoch: 2/2, step 2706/7134 completed (loss: 0.046295445412397385, acc: 0.9869375824928284)
[2025-02-13 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46][root][INFO] - Training Epoch: 2/2, step 2707/7134 completed (loss: 0.017853794619441032, acc: 0.9931034445762634)
[2025-02-13 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46][root][INFO] - Training Epoch: 2/2, step 2708/7134 completed (loss: 0.04215134307742119, acc: 0.9845722317695618)
[2025-02-13 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47][root][INFO] - Training Epoch: 2/2, step 2709/7134 completed (loss: 0.020829545333981514, acc: 0.9935064911842346)
[2025-02-13 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47][root][INFO] - Training Epoch: 2/2, step 2710/7134 completed (loss: 0.015184287913143635, acc: 0.9953380227088928)
[2025-02-13 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:48][root][INFO] - Training Epoch: 2/2, step 2711/7134 completed (loss: 0.015074589289724827, acc: 0.9929245114326477)
[2025-02-13 03:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:48][root][INFO] - Training Epoch: 2/2, step 2712/7134 completed (loss: 0.020688040181994438, acc: 0.9910714030265808)
[2025-02-13 03:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49][root][INFO] - Training Epoch: 2/2, step 2713/7134 completed (loss: 0.04732557013630867, acc: 0.9944289922714233)
[2025-02-13 03:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49][root][INFO] - Training Epoch: 2/2, step 2714/7134 completed (loss: 0.02437433786690235, acc: 0.9900166392326355)
[2025-02-13 03:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49][root][INFO] - Training Epoch: 2/2, step 2715/7134 completed (loss: 0.02038317732512951, acc: 0.9939849376678467)
[2025-02-13 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50][root][INFO] - Training Epoch: 2/2, step 2716/7134 completed (loss: 0.03525601699948311, acc: 0.9908257126808167)
[2025-02-13 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50][root][INFO] - Training Epoch: 2/2, step 2717/7134 completed (loss: 0.050106290727853775, acc: 0.987679660320282)
[2025-02-13 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:51][root][INFO] - Training Epoch: 2/2, step 2718/7134 completed (loss: 0.06962659955024719, acc: 0.9788732528686523)
[2025-02-13 03:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:51][root][INFO] - Training Epoch: 2/2, step 2719/7134 completed (loss: 0.046594955027103424, acc: 0.9881556630134583)
[2025-02-13 03:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52][root][INFO] - Training Epoch: 2/2, step 2720/7134 completed (loss: 0.03637835383415222, acc: 0.9873816967010498)
[2025-02-13 03:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52][root][INFO] - Training Epoch: 2/2, step 2721/7134 completed (loss: 0.04323812946677208, acc: 0.9898550510406494)
[2025-02-13 03:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52][root][INFO] - Training Epoch: 2/2, step 2722/7134 completed (loss: 0.07348373532295227, acc: 0.9847826361656189)
[2025-02-13 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53][root][INFO] - Training Epoch: 2/2, step 2723/7134 completed (loss: 0.06239059194922447, acc: 0.984009861946106)
[2025-02-13 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53][root][INFO] - Training Epoch: 2/2, step 2724/7134 completed (loss: 0.03079378604888916, acc: 0.9902794361114502)
[2025-02-13 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54][root][INFO] - Training Epoch: 2/2, step 2725/7134 completed (loss: 0.011857946403324604, acc: 0.9950739145278931)
[2025-02-13 03:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54][root][INFO] - Training Epoch: 2/2, step 2726/7134 completed (loss: 0.0604146271944046, acc: 0.9889298677444458)
[2025-02-13 03:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55][root][INFO] - Training Epoch: 2/2, step 2727/7134 completed (loss: 0.016392551362514496, acc: 0.9925925731658936)
[2025-02-13 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55][root][INFO] - Training Epoch: 2/2, step 2728/7134 completed (loss: 0.07232169061899185, acc: 0.9770641922950745)
[2025-02-13 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55][root][INFO] - Training Epoch: 2/2, step 2729/7134 completed (loss: 0.04784839600324631, acc: 0.9860627055168152)
[2025-02-13 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56][root][INFO] - Training Epoch: 2/2, step 2730/7134 completed (loss: 0.054585445672273636, acc: 0.981249988079071)
[2025-02-13 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56][root][INFO] - Training Epoch: 2/2, step 2731/7134 completed (loss: 0.0711754858493805, acc: 0.9829683899879456)
[2025-02-13 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57][root][INFO] - Training Epoch: 2/2, step 2732/7134 completed (loss: 0.032234422862529755, acc: 0.9936548471450806)
[2025-02-13 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57][root][INFO] - Training Epoch: 2/2, step 2733/7134 completed (loss: 0.0330478772521019, acc: 0.9877883195877075)
[2025-02-13 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58][root][INFO] - Training Epoch: 2/2, step 2734/7134 completed (loss: 0.03883033245801926, acc: 0.9879275560379028)
[2025-02-13 03:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58][root][INFO] - Training Epoch: 2/2, step 2735/7134 completed (loss: 0.03269736468791962, acc: 0.9918699264526367)
[2025-02-13 03:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58][root][INFO] - Training Epoch: 2/2, step 2736/7134 completed (loss: 0.04375745728611946, acc: 0.986940324306488)
[2025-02-13 03:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59][root][INFO] - Training Epoch: 2/2, step 2737/7134 completed (loss: 0.023265626281499863, acc: 0.9966996908187866)
[2025-02-13 03:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59][root][INFO] - Training Epoch: 2/2, step 2738/7134 completed (loss: 0.02045680396258831, acc: 0.9900000095367432)
[2025-02-13 03:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00][root][INFO] - Training Epoch: 2/2, step 2739/7134 completed (loss: 0.04765227064490318, acc: 0.9890909194946289)
[2025-02-13 04:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00][root][INFO] - Training Epoch: 2/2, step 2740/7134 completed (loss: 0.024353455752134323, acc: 0.9898403286933899)
[2025-02-13 04:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00][root][INFO] - Training Epoch: 2/2, step 2741/7134 completed (loss: 0.02811416983604431, acc: 0.9929906725883484)
[2025-02-13 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01][root][INFO] - Training Epoch: 2/2, step 2742/7134 completed (loss: 0.009839370846748352, acc: 0.9979166388511658)
[2025-02-13 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01][root][INFO] - Training Epoch: 2/2, step 2743/7134 completed (loss: 0.007347266189754009, acc: 0.9973683953285217)
[2025-02-13 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02][root][INFO] - Training Epoch: 2/2, step 2744/7134 completed (loss: 0.011849507689476013, acc: 0.9961685538291931)
[2025-02-13 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02][root][INFO] - Training Epoch: 2/2, step 2745/7134 completed (loss: 0.059292104095220566, acc: 0.9808362126350403)
[2025-02-13 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02][root][INFO] - Training Epoch: 2/2, step 2746/7134 completed (loss: 0.013219370506703854, acc: 0.9967948794364929)
[2025-02-13 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03][root][INFO] - Training Epoch: 2/2, step 2747/7134 completed (loss: 0.01095699891448021, acc: 0.9975429773330688)
[2025-02-13 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03][root][INFO] - Training Epoch: 2/2, step 2748/7134 completed (loss: 0.0025402342434972525, acc: 1.0)
[2025-02-13 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03][root][INFO] - Training Epoch: 2/2, step 2749/7134 completed (loss: 0.03661184385418892, acc: 0.9836065769195557)
[2025-02-13 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04][root][INFO] - Training Epoch: 2/2, step 2750/7134 completed (loss: 0.016966072842478752, acc: 0.9924812316894531)
[2025-02-13 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04][root][INFO] - Training Epoch: 2/2, step 2751/7134 completed (loss: 0.029512537643313408, acc: 0.9898734092712402)
[2025-02-13 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:05][root][INFO] - Training Epoch: 2/2, step 2752/7134 completed (loss: 0.0378996841609478, acc: 0.9866920113563538)
[2025-02-13 04:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:05][root][INFO] - Training Epoch: 2/2, step 2753/7134 completed (loss: 0.05567134916782379, acc: 0.9799498915672302)
[2025-02-13 04:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06][root][INFO] - Training Epoch: 2/2, step 2754/7134 completed (loss: 0.034941259771585464, acc: 0.9898580312728882)
[2025-02-13 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06][root][INFO] - Training Epoch: 2/2, step 2755/7134 completed (loss: 0.014198116026818752, acc: 0.998062014579773)
[2025-02-13 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06][root][INFO] - Training Epoch: 2/2, step 2756/7134 completed (loss: 0.05627693980932236, acc: 0.977142870426178)
[2025-02-13 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07][root][INFO] - Training Epoch: 2/2, step 2757/7134 completed (loss: 0.04922071471810341, acc: 0.9863387942314148)
[2025-02-13 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07][root][INFO] - Training Epoch: 2/2, step 2758/7134 completed (loss: 0.04069023206830025, acc: 0.9885844588279724)
[2025-02-13 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08][root][INFO] - Training Epoch: 2/2, step 2759/7134 completed (loss: 0.03128603473305702, acc: 0.9899665713310242)
[2025-02-13 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08][root][INFO] - Training Epoch: 2/2, step 2760/7134 completed (loss: 0.02251109853386879, acc: 0.9904761910438538)
[2025-02-13 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08][root][INFO] - Training Epoch: 2/2, step 2761/7134 completed (loss: 0.05758688598871231, acc: 0.9835858345031738)
[2025-02-13 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09][root][INFO] - Training Epoch: 2/2, step 2762/7134 completed (loss: 0.04035800322890282, acc: 0.9884925484657288)
[2025-02-13 04:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09][root][INFO] - Training Epoch: 2/2, step 2763/7134 completed (loss: 0.03955206647515297, acc: 0.9886934757232666)
[2025-02-13 04:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10][root][INFO] - Training Epoch: 2/2, step 2764/7134 completed (loss: 0.06794029474258423, acc: 0.9749670624732971)
[2025-02-13 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10][root][INFO] - Training Epoch: 2/2, step 2765/7134 completed (loss: 0.024884717538952827, acc: 0.9956896305084229)
[2025-02-13 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11][root][INFO] - Training Epoch: 2/2, step 2766/7134 completed (loss: 0.01593877375125885, acc: 0.993537962436676)
[2025-02-13 04:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11][root][INFO] - Training Epoch: 2/2, step 2767/7134 completed (loss: 0.05172659456729889, acc: 0.9839506149291992)
[2025-02-13 04:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12][root][INFO] - Training Epoch: 2/2, step 2768/7134 completed (loss: 0.03404008597135544, acc: 0.9886845946311951)
[2025-02-13 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12][root][INFO] - Training Epoch: 2/2, step 2769/7134 completed (loss: 0.03234510496258736, acc: 0.9921171069145203)
[2025-02-13 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12][root][INFO] - Training Epoch: 2/2, step 2770/7134 completed (loss: 0.02028517983853817, acc: 0.9916550517082214)
[2025-02-13 04:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13][root][INFO] - Training Epoch: 2/2, step 2771/7134 completed (loss: 0.015227359719574451, acc: 0.9946409463882446)
[2025-02-13 04:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13][root][INFO] - Training Epoch: 2/2, step 2772/7134 completed (loss: 0.021680623292922974, acc: 0.9932885766029358)
[2025-02-13 04:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:14][root][INFO] - Training Epoch: 2/2, step 2773/7134 completed (loss: 0.01082461979240179, acc: 0.99657142162323)
[2025-02-13 04:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:14][root][INFO] - Training Epoch: 2/2, step 2774/7134 completed (loss: 0.05341952666640282, acc: 0.9868420958518982)
[2025-02-13 04:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15][root][INFO] - Training Epoch: 2/2, step 2775/7134 completed (loss: 0.027012208476662636, acc: 0.9938650131225586)
[2025-02-13 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15][root][INFO] - Training Epoch: 2/2, step 2776/7134 completed (loss: 0.011168043129146099, acc: 0.9948875308036804)
[2025-02-13 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16][root][INFO] - Training Epoch: 2/2, step 2777/7134 completed (loss: 0.013692531734704971, acc: 0.995121955871582)
[2025-02-13 04:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16][root][INFO] - Training Epoch: 2/2, step 2778/7134 completed (loss: 0.02918093651533127, acc: 0.9918887615203857)
[2025-02-13 04:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17][root][INFO] - Training Epoch: 2/2, step 2779/7134 completed (loss: 0.03638438135385513, acc: 0.9915611743927002)
[2025-02-13 04:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17][root][INFO] - Training Epoch: 2/2, step 2780/7134 completed (loss: 0.04402502253651619, acc: 0.9852320551872253)
[2025-02-13 04:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:18][root][INFO] - Training Epoch: 2/2, step 2781/7134 completed (loss: 0.018980540335178375, acc: 0.9961140155792236)
[2025-02-13 04:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:18][root][INFO] - Training Epoch: 2/2, step 2782/7134 completed (loss: 0.025519510731101036, acc: 0.9926470518112183)
[2025-02-13 04:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19][root][INFO] - Training Epoch: 2/2, step 2783/7134 completed (loss: 0.01493349764496088, acc: 0.9954954981803894)
[2025-02-13 04:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19][root][INFO] - Training Epoch: 2/2, step 2784/7134 completed (loss: 0.01772148162126541, acc: 0.995793879032135)
[2025-02-13 04:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19][root][INFO] - Training Epoch: 2/2, step 2785/7134 completed (loss: 0.01361938752233982, acc: 0.9978166222572327)
[2025-02-13 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20][root][INFO] - Training Epoch: 2/2, step 2786/7134 completed (loss: 0.03340265527367592, acc: 0.9885773658752441)
[2025-02-13 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20][root][INFO] - Training Epoch: 2/2, step 2787/7134 completed (loss: 0.018358947709202766, acc: 0.9933599233627319)
[2025-02-13 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21][root][INFO] - Training Epoch: 2/2, step 2788/7134 completed (loss: 0.03405027464032173, acc: 0.9895287752151489)
[2025-02-13 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21][root][INFO] - Training Epoch: 2/2, step 2789/7134 completed (loss: 0.11735546588897705, acc: 0.9654178619384766)
[2025-02-13 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22][root][INFO] - Training Epoch: 2/2, step 2790/7134 completed (loss: 0.11267033219337463, acc: 0.9735099077224731)
[2025-02-13 04:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22][root][INFO] - Training Epoch: 2/2, step 2791/7134 completed (loss: 0.03056582622230053, acc: 0.988041877746582)
[2025-02-13 04:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:23][root][INFO] - Training Epoch: 2/2, step 2792/7134 completed (loss: 0.03698570281267166, acc: 0.9930151104927063)
[2025-02-13 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:23][root][INFO] - Training Epoch: 2/2, step 2793/7134 completed (loss: 0.044569604098796844, acc: 0.9838056564331055)
[2025-02-13 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24][root][INFO] - Training Epoch: 2/2, step 2794/7134 completed (loss: 0.011850285343825817, acc: 1.0)
[2025-02-13 04:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24][root][INFO] - Training Epoch: 2/2, step 2795/7134 completed (loss: 0.06467683613300323, acc: 0.9744779467582703)
[2025-02-13 04:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24][root][INFO] - Training Epoch: 2/2, step 2796/7134 completed (loss: 0.060826465487480164, acc: 0.981566846370697)
[2025-02-13 04:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25][root][INFO] - Training Epoch: 2/2, step 2797/7134 completed (loss: 0.014076308347284794, acc: 0.9936708807945251)
[2025-02-13 04:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25][root][INFO] - Training Epoch: 2/2, step 2798/7134 completed (loss: 0.04932796210050583, acc: 0.9894958138465881)
[2025-02-13 04:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26][root][INFO] - Training Epoch: 2/2, step 2799/7134 completed (loss: 0.03914868086576462, acc: 0.9842382073402405)
[2025-02-13 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26][root][INFO] - Training Epoch: 2/2, step 2800/7134 completed (loss: 0.08320780098438263, acc: 0.9736379384994507)
[2025-02-13 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27][root][INFO] - Training Epoch: 2/2, step 2801/7134 completed (loss: 0.03080904670059681, acc: 0.9910485744476318)
[2025-02-13 04:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27][root][INFO] - Training Epoch: 2/2, step 2802/7134 completed (loss: 0.038503676652908325, acc: 0.9887640476226807)
[2025-02-13 04:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28][root][INFO] - Training Epoch: 2/2, step 2803/7134 completed (loss: 0.004697759635746479, acc: 1.0)
[2025-02-13 04:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28][root][INFO] - Training Epoch: 2/2, step 2804/7134 completed (loss: 0.03132358938455582, acc: 0.9938119053840637)
[2025-02-13 04:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28][root][INFO] - Training Epoch: 2/2, step 2805/7134 completed (loss: 0.018090875819325447, acc: 0.9960474371910095)
[2025-02-13 04:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:29][root][INFO] - Training Epoch: 2/2, step 2806/7134 completed (loss: 0.02656865306198597, acc: 0.9950819611549377)
[2025-02-13 04:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:29][root][INFO] - Training Epoch: 2/2, step 2807/7134 completed (loss: 0.03051372431218624, acc: 0.9937629699707031)
[2025-02-13 04:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30][root][INFO] - Training Epoch: 2/2, step 2808/7134 completed (loss: 0.014487502165138721, acc: 0.9942857027053833)
[2025-02-13 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30][root][INFO] - Training Epoch: 2/2, step 2809/7134 completed (loss: 0.028765663504600525, acc: 0.9928698539733887)
[2025-02-13 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31][root][INFO] - Training Epoch: 2/2, step 2810/7134 completed (loss: 0.022845488041639328, acc: 0.9952493906021118)
[2025-02-13 04:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31][root][INFO] - Training Epoch: 2/2, step 2811/7134 completed (loss: 0.005835546180605888, acc: 0.9988066554069519)
[2025-02-13 04:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:32][root][INFO] - Training Epoch: 2/2, step 2812/7134 completed (loss: 0.010896543972194195, acc: 0.9953703880310059)
[2025-02-13 04:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:32][root][INFO] - Training Epoch: 2/2, step 2813/7134 completed (loss: 0.007631991524249315, acc: 0.998740553855896)
[2025-02-13 04:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33][root][INFO] - Training Epoch: 2/2, step 2814/7134 completed (loss: 0.026446836069226265, acc: 0.9961038827896118)
[2025-02-13 04:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33][root][INFO] - Training Epoch: 2/2, step 2815/7134 completed (loss: 0.04121299460530281, acc: 0.9881376028060913)
[2025-02-13 04:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33][root][INFO] - Training Epoch: 2/2, step 2816/7134 completed (loss: 0.055904850363731384, acc: 0.9881423115730286)
[2025-02-13 04:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:34][root][INFO] - Training Epoch: 2/2, step 2817/7134 completed (loss: 0.029416926205158234, acc: 0.9916142821311951)
[2025-02-13 04:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:34][root][INFO] - Training Epoch: 2/2, step 2818/7134 completed (loss: 0.02661752514541149, acc: 0.9890350699424744)
[2025-02-13 04:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35][root][INFO] - Training Epoch: 2/2, step 2819/7134 completed (loss: 0.01837507076561451, acc: 0.9951534867286682)
[2025-02-13 04:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35][root][INFO] - Training Epoch: 2/2, step 2820/7134 completed (loss: 0.0513000413775444, acc: 0.9837996959686279)
[2025-02-13 04:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:36][root][INFO] - Training Epoch: 2/2, step 2821/7134 completed (loss: 0.03131655976176262, acc: 0.9904420375823975)
[2025-02-13 04:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:36][root][INFO] - Training Epoch: 2/2, step 2822/7134 completed (loss: 0.05300092697143555, acc: 0.982300877571106)
[2025-02-13 04:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37][root][INFO] - Training Epoch: 2/2, step 2823/7134 completed (loss: 0.039034876972436905, acc: 0.9843971729278564)
[2025-02-13 04:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37][root][INFO] - Training Epoch: 2/2, step 2824/7134 completed (loss: 0.059871114790439606, acc: 0.9788960814476013)
[2025-02-13 04:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38][root][INFO] - Training Epoch: 2/2, step 2825/7134 completed (loss: 0.04513904079794884, acc: 0.991946280002594)
[2025-02-13 04:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38][root][INFO] - Training Epoch: 2/2, step 2826/7134 completed (loss: 0.061035823076963425, acc: 0.9879518151283264)
[2025-02-13 04:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39][root][INFO] - Training Epoch: 2/2, step 2827/7134 completed (loss: 0.03575125336647034, acc: 0.9872286319732666)
[2025-02-13 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39][root][INFO] - Training Epoch: 2/2, step 2828/7134 completed (loss: 0.07112020999193192, acc: 0.984000027179718)
[2025-02-13 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39][root][INFO] - Training Epoch: 2/2, step 2829/7134 completed (loss: 0.041170548647642136, acc: 0.9918144345283508)
[2025-02-13 04:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40][root][INFO] - Training Epoch: 2/2, step 2830/7134 completed (loss: 0.00949881225824356, acc: 0.9981784820556641)
[2025-02-13 04:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40][root][INFO] - Training Epoch: 2/2, step 2831/7134 completed (loss: 0.014790469780564308, acc: 0.9952606558799744)
[2025-02-13 04:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41][root][INFO] - Training Epoch: 2/2, step 2832/7134 completed (loss: 0.042715366929769516, acc: 0.9903846383094788)
[2025-02-13 04:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41][root][INFO] - Training Epoch: 2/2, step 2833/7134 completed (loss: 0.027606364339590073, acc: 0.9894737005233765)
[2025-02-13 04:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42][root][INFO] - Training Epoch: 2/2, step 2834/7134 completed (loss: 0.052427466958761215, acc: 0.9873096346855164)
[2025-02-13 04:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42][root][INFO] - Training Epoch: 2/2, step 2835/7134 completed (loss: 0.03567502275109291, acc: 0.9886914491653442)
[2025-02-13 04:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43][root][INFO] - Training Epoch: 2/2, step 2836/7134 completed (loss: 0.04014808312058449, acc: 0.9905213117599487)
[2025-02-13 04:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43][root][INFO] - Training Epoch: 2/2, step 2837/7134 completed (loss: 0.018295252695679665, acc: 0.9945945739746094)
[2025-02-13 04:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44][root][INFO] - Training Epoch: 2/2, step 2838/7134 completed (loss: 0.0711013600230217, acc: 0.9790794849395752)
[2025-02-13 04:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44][root][INFO] - Training Epoch: 2/2, step 2839/7134 completed (loss: 0.031425170600414276, acc: 0.9954476356506348)
[2025-02-13 04:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44][root][INFO] - Training Epoch: 2/2, step 2840/7134 completed (loss: 0.016803212463855743, acc: 0.9944751262664795)
[2025-02-13 04:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:45][root][INFO] - Training Epoch: 2/2, step 2841/7134 completed (loss: 0.014206438325345516, acc: 0.9953917264938354)
[2025-02-13 04:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:45][root][INFO] - Training Epoch: 2/2, step 2842/7134 completed (loss: 0.01859084516763687, acc: 0.9935064911842346)
[2025-02-13 04:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46][root][INFO] - Training Epoch: 2/2, step 2843/7134 completed (loss: 0.020106656476855278, acc: 0.9941176176071167)
[2025-02-13 04:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46][root][INFO] - Training Epoch: 2/2, step 2844/7134 completed (loss: 0.06930892914533615, acc: 0.9822161197662354)
[2025-02-13 04:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47][root][INFO] - Training Epoch: 2/2, step 2845/7134 completed (loss: 0.02372737228870392, acc: 0.9948006868362427)
[2025-02-13 04:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47][root][INFO] - Training Epoch: 2/2, step 2846/7134 completed (loss: 0.058341313153505325, acc: 0.9884678721427917)
[2025-02-13 04:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48][root][INFO] - Training Epoch: 2/2, step 2847/7134 completed (loss: 0.029660381376743317, acc: 0.9912023544311523)
[2025-02-13 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48][root][INFO] - Training Epoch: 2/2, step 2848/7134 completed (loss: 0.017408275976777077, acc: 0.9953380227088928)
[2025-02-13 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49][root][INFO] - Training Epoch: 2/2, step 2849/7134 completed (loss: 0.03668403998017311, acc: 0.9917012453079224)
[2025-02-13 04:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49][root][INFO] - Training Epoch: 2/2, step 2850/7134 completed (loss: 0.03059012070298195, acc: 0.9935483932495117)
[2025-02-13 04:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49][root][INFO] - Training Epoch: 2/2, step 2851/7134 completed (loss: 0.0220451969653368, acc: 0.9926335215568542)
[2025-02-13 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50][root][INFO] - Training Epoch: 2/2, step 2852/7134 completed (loss: 0.011237997561693192, acc: 0.9960238337516785)
[2025-02-13 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50][root][INFO] - Training Epoch: 2/2, step 2853/7134 completed (loss: 0.029603665694594383, acc: 0.989847719669342)
[2025-02-13 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51][root][INFO] - Training Epoch: 2/2, step 2854/7134 completed (loss: 0.03260546922683716, acc: 0.9881129264831543)
[2025-02-13 04:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51][root][INFO] - Training Epoch: 2/2, step 2855/7134 completed (loss: 0.028795303776860237, acc: 0.9919224381446838)
[2025-02-13 04:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52][root][INFO] - Training Epoch: 2/2, step 2856/7134 completed (loss: 0.005762217100709677, acc: 0.9971791505813599)
[2025-02-13 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52][root][INFO] - Training Epoch: 2/2, step 2857/7134 completed (loss: 0.03592384606599808, acc: 0.9913294911384583)
[2025-02-13 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52][root][INFO] - Training Epoch: 2/2, step 2858/7134 completed (loss: 0.02319670096039772, acc: 0.9936908483505249)
[2025-02-13 04:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53][root][INFO] - Training Epoch: 2/2, step 2859/7134 completed (loss: 0.04297323152422905, acc: 0.9860529899597168)
[2025-02-13 04:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53][root][INFO] - Training Epoch: 2/2, step 2860/7134 completed (loss: 0.024879874661564827, acc: 0.9926605224609375)
[2025-02-13 04:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54][root][INFO] - Training Epoch: 2/2, step 2861/7134 completed (loss: 0.007288467139005661, acc: 0.9979715943336487)
[2025-02-13 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54][root][INFO] - Training Epoch: 2/2, step 2862/7134 completed (loss: 0.0060006314888596535, acc: 0.9982847571372986)
[2025-02-13 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55][root][INFO] - Training Epoch: 2/2, step 2863/7134 completed (loss: 0.008783393539488316, acc: 0.9974522590637207)
[2025-02-13 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55][root][INFO] - Training Epoch: 2/2, step 2864/7134 completed (loss: 0.029390385374426842, acc: 0.9922077655792236)
[2025-02-13 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55][root][INFO] - Training Epoch: 2/2, step 2865/7134 completed (loss: 0.023819388821721077, acc: 0.991304337978363)
[2025-02-13 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56][root][INFO] - Training Epoch: 2/2, step 2866/7134 completed (loss: 0.029504001140594482, acc: 0.994020938873291)
[2025-02-13 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56][root][INFO] - Training Epoch: 2/2, step 2867/7134 completed (loss: 0.015081765130162239, acc: 0.9938650131225586)
[2025-02-13 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57][root][INFO] - Training Epoch: 2/2, step 2868/7134 completed (loss: 0.0035037389025092125, acc: 1.0)
[2025-02-13 04:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57][root][INFO] - Training Epoch: 2/2, step 2869/7134 completed (loss: 0.04411323368549347, acc: 0.9956076145172119)
[2025-02-13 04:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58][root][INFO] - Training Epoch: 2/2, step 2870/7134 completed (loss: 0.058979690074920654, acc: 0.9916142821311951)
[2025-02-13 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58][root][INFO] - Training Epoch: 2/2, step 2871/7134 completed (loss: 0.029355499893426895, acc: 0.9890795350074768)
[2025-02-13 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58][root][INFO] - Training Epoch: 2/2, step 2872/7134 completed (loss: 0.004137538373470306, acc: 0.998046875)
[2025-02-13 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59][root][INFO] - Training Epoch: 2/2, step 2873/7134 completed (loss: 0.019045324996113777, acc: 0.9927007555961609)
[2025-02-13 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59][root][INFO] - Training Epoch: 2/2, step 2874/7134 completed (loss: 0.019109221175312996, acc: 0.9937597513198853)
[2025-02-13 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00][root][INFO] - Training Epoch: 2/2, step 2875/7134 completed (loss: 0.029669275507330894, acc: 0.9943100810050964)
[2025-02-13 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00][root][INFO] - Training Epoch: 2/2, step 2876/7134 completed (loss: 0.07802793383598328, acc: 0.981566846370697)
[2025-02-13 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:01][root][INFO] - Training Epoch: 2/2, step 2877/7134 completed (loss: 0.008593951351940632, acc: 0.9978166222572327)
[2025-02-13 04:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:01][root][INFO] - Training Epoch: 2/2, step 2878/7134 completed (loss: 0.025079885497689247, acc: 0.9978813529014587)
[2025-02-13 04:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:01][root][INFO] - Training Epoch: 2/2, step 2879/7134 completed (loss: 0.010325989685952663, acc: 0.9968652129173279)
[2025-02-13 04:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:02][root][INFO] - Training Epoch: 2/2, step 2880/7134 completed (loss: 0.005672756116837263, acc: 1.0)
[2025-02-13 04:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:02][root][INFO] - Training Epoch: 2/2, step 2881/7134 completed (loss: 0.018419047817587852, acc: 0.9946332573890686)
[2025-02-13 04:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03][root][INFO] - Training Epoch: 2/2, step 2882/7134 completed (loss: 0.06123355031013489, acc: 0.9852034449577332)
[2025-02-13 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03][root][INFO] - Training Epoch: 2/2, step 2883/7134 completed (loss: 0.023092718794941902, acc: 0.9947090148925781)
[2025-02-13 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04][root][INFO] - Training Epoch: 2/2, step 2884/7134 completed (loss: 0.04908574000000954, acc: 0.9864253401756287)
[2025-02-13 04:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04][root][INFO] - Training Epoch: 2/2, step 2885/7134 completed (loss: 0.03097701258957386, acc: 0.9893778562545776)
[2025-02-13 04:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04][root][INFO] - Training Epoch: 2/2, step 2886/7134 completed (loss: 0.06323734670877457, acc: 0.9850746393203735)
[2025-02-13 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05][root][INFO] - Training Epoch: 2/2, step 2887/7134 completed (loss: 0.03486499935388565, acc: 0.9869822263717651)
[2025-02-13 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05][root][INFO] - Training Epoch: 2/2, step 2888/7134 completed (loss: 0.058940280228853226, acc: 0.9861303567886353)
[2025-02-13 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06][root][INFO] - Training Epoch: 2/2, step 2889/7134 completed (loss: 0.01804615929722786, acc: 0.9930747747421265)
[2025-02-13 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06][root][INFO] - Training Epoch: 2/2, step 2890/7134 completed (loss: 0.02561199851334095, acc: 0.9920106530189514)
[2025-02-13 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07][root][INFO] - Training Epoch: 2/2, step 2891/7134 completed (loss: 0.029701344668865204, acc: 0.9923076629638672)
[2025-02-13 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07][root][INFO] - Training Epoch: 2/2, step 2892/7134 completed (loss: 0.057018376886844635, acc: 0.98740553855896)
[2025-02-13 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:08][root][INFO] - Training Epoch: 2/2, step 2893/7134 completed (loss: 0.029862407594919205, acc: 0.9899117350578308)
[2025-02-13 04:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:08][root][INFO] - Training Epoch: 2/2, step 2894/7134 completed (loss: 0.023201055824756622, acc: 0.9915966391563416)
[2025-02-13 04:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09][root][INFO] - Training Epoch: 2/2, step 2895/7134 completed (loss: 0.018970470875501633, acc: 0.992438554763794)
[2025-02-13 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09][root][INFO] - Training Epoch: 2/2, step 2896/7134 completed (loss: 0.020573822781443596, acc: 0.9936386942863464)
[2025-02-13 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09][root][INFO] - Training Epoch: 2/2, step 2897/7134 completed (loss: 0.029489507898688316, acc: 0.9875665903091431)
[2025-02-13 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:10][root][INFO] - Training Epoch: 2/2, step 2898/7134 completed (loss: 0.019443992525339127, acc: 0.9973856210708618)
[2025-02-13 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:10][root][INFO] - Training Epoch: 2/2, step 2899/7134 completed (loss: 0.013398581184446812, acc: 0.9956395626068115)
[2025-02-13 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11][root][INFO] - Training Epoch: 2/2, step 2900/7134 completed (loss: 0.029266707599163055, acc: 0.9962962865829468)
[2025-02-13 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11][root][INFO] - Training Epoch: 2/2, step 2901/7134 completed (loss: 0.01630750112235546, acc: 0.9952229261398315)
[2025-02-13 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12][root][INFO] - Training Epoch: 2/2, step 2902/7134 completed (loss: 0.012682224623858929, acc: 0.9943609237670898)
[2025-02-13 04:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12][root][INFO] - Training Epoch: 2/2, step 2903/7134 completed (loss: 0.0336964875459671, acc: 0.9919028282165527)
[2025-02-13 04:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12][root][INFO] - Training Epoch: 2/2, step 2904/7134 completed (loss: 0.010482064448297024, acc: 0.995006263256073)
[2025-02-13 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13][root][INFO] - Training Epoch: 2/2, step 2905/7134 completed (loss: 0.016947664320468903, acc: 0.9962453246116638)
[2025-02-13 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13][root][INFO] - Training Epoch: 2/2, step 2906/7134 completed (loss: 0.004912843462079763, acc: 1.0)
[2025-02-13 04:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:14][root][INFO] - Training Epoch: 2/2, step 2907/7134 completed (loss: 0.03337942063808441, acc: 0.9851951599121094)
[2025-02-13 04:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:14][root][INFO] - Training Epoch: 2/2, step 2908/7134 completed (loss: 0.023465830832719803, acc: 0.9935317039489746)
[2025-02-13 04:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15][root][INFO] - Training Epoch: 2/2, step 2909/7134 completed (loss: 0.013658029958605766, acc: 0.9952038526535034)
[2025-02-13 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15][root][INFO] - Training Epoch: 2/2, step 2910/7134 completed (loss: 0.04186135530471802, acc: 0.9892933368682861)
[2025-02-13 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:16][root][INFO] - Training Epoch: 2/2, step 2911/7134 completed (loss: 0.005264223553240299, acc: 0.9986577033996582)
[2025-02-13 04:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:16][root][INFO] - Training Epoch: 2/2, step 2912/7134 completed (loss: 0.007282853592187166, acc: 0.9986206889152527)
[2025-02-13 04:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:17][root][INFO] - Training Epoch: 2/2, step 2913/7134 completed (loss: 0.02111726999282837, acc: 0.994301974773407)
[2025-02-13 04:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:17][root][INFO] - Training Epoch: 2/2, step 2914/7134 completed (loss: 0.031996071338653564, acc: 0.993968665599823)
[2025-02-13 04:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18][root][INFO] - Training Epoch: 2/2, step 2915/7134 completed (loss: 0.057635825127363205, acc: 0.9899135231971741)
[2025-02-13 04:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18][root][INFO] - Training Epoch: 2/2, step 2916/7134 completed (loss: 0.03451487049460411, acc: 0.993127167224884)
[2025-02-13 04:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18][root][INFO] - Training Epoch: 2/2, step 2917/7134 completed (loss: 0.013198538683354855, acc: 0.9966499209403992)
[2025-02-13 04:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:19][root][INFO] - Training Epoch: 2/2, step 2918/7134 completed (loss: 0.008340018801391125, acc: 0.997668981552124)
[2025-02-13 04:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:19][root][INFO] - Training Epoch: 2/2, step 2919/7134 completed (loss: 0.030294938012957573, acc: 0.9882491230964661)
[2025-02-13 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20][root][INFO] - Training Epoch: 2/2, step 2920/7134 completed (loss: 0.018080106005072594, acc: 0.990728497505188)
[2025-02-13 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20][root][INFO] - Training Epoch: 2/2, step 2921/7134 completed (loss: 0.008922399021685123, acc: 0.9956896305084229)
[2025-02-13 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21][root][INFO] - Training Epoch: 2/2, step 2922/7134 completed (loss: 0.041266877204179764, acc: 0.9924623370170593)
[2025-02-13 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21][root][INFO] - Training Epoch: 2/2, step 2923/7134 completed (loss: 0.13130603730678558, acc: 0.9754224419593811)
[2025-02-13 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22][root][INFO] - Training Epoch: 2/2, step 2924/7134 completed (loss: 0.012434926815330982, acc: 0.9952830076217651)
[2025-02-13 04:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22][root][INFO] - Training Epoch: 2/2, step 2925/7134 completed (loss: 0.029282517731189728, acc: 0.9914004802703857)
[2025-02-13 04:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23][root][INFO] - Training Epoch: 2/2, step 2926/7134 completed (loss: 0.1270950436592102, acc: 0.9783653616905212)
[2025-02-13 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23][root][INFO] - Training Epoch: 2/2, step 2927/7134 completed (loss: 0.017549000680446625, acc: 0.9951632618904114)
[2025-02-13 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24][root][INFO] - Training Epoch: 2/2, step 2928/7134 completed (loss: 0.029076721519231796, acc: 0.9908536672592163)
[2025-02-13 04:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24][root][INFO] - Training Epoch: 2/2, step 2929/7134 completed (loss: 0.018116600811481476, acc: 0.9960835576057434)
[2025-02-13 04:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25][root][INFO] - Training Epoch: 2/2, step 2930/7134 completed (loss: 0.018896598368883133, acc: 0.9926578402519226)
[2025-02-13 04:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25][root][INFO] - Training Epoch: 2/2, step 2931/7134 completed (loss: 0.04193836450576782, acc: 0.9895366430282593)
[2025-02-13 04:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25][root][INFO] - Training Epoch: 2/2, step 2932/7134 completed (loss: 0.0181648638099432, acc: 0.9942062497138977)
[2025-02-13 04:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26][root][INFO] - Training Epoch: 2/2, step 2933/7134 completed (loss: 0.02440592646598816, acc: 0.9927849769592285)
[2025-02-13 04:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26][root][INFO] - Training Epoch: 2/2, step 2934/7134 completed (loss: 0.01714359037578106, acc: 0.994358241558075)
[2025-02-13 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27][root][INFO] - Training Epoch: 2/2, step 2935/7134 completed (loss: 0.02996344305574894, acc: 0.9889867901802063)
[2025-02-13 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27][root][INFO] - Training Epoch: 2/2, step 2936/7134 completed (loss: 0.031076716259121895, acc: 0.9908443689346313)
[2025-02-13 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28][root][INFO] - Training Epoch: 2/2, step 2937/7134 completed (loss: 0.04362073913216591, acc: 0.9870874881744385)
[2025-02-13 04:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28][root][INFO] - Training Epoch: 2/2, step 2938/7134 completed (loss: 0.01533374935388565, acc: 0.9955307245254517)
[2025-02-13 04:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29][root][INFO] - Training Epoch: 2/2, step 2939/7134 completed (loss: 0.01914210058748722, acc: 0.9935897588729858)
[2025-02-13 04:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29][root][INFO] - Training Epoch: 2/2, step 2940/7134 completed (loss: 0.047782521694898605, acc: 0.9876881241798401)
[2025-02-13 04:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29][root][INFO] - Training Epoch: 2/2, step 2941/7134 completed (loss: 0.020320190116763115, acc: 0.9927431344985962)
[2025-02-13 04:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30][root][INFO] - Training Epoch: 2/2, step 2942/7134 completed (loss: 0.03952605649828911, acc: 0.9910846948623657)
[2025-02-13 04:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30][root][INFO] - Training Epoch: 2/2, step 2943/7134 completed (loss: 0.012603898532688618, acc: 0.9966273307800293)
[2025-02-13 04:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:31][root][INFO] - Training Epoch: 2/2, step 2944/7134 completed (loss: 0.02084031142294407, acc: 0.9941262602806091)
[2025-02-13 04:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:31][root][INFO] - Training Epoch: 2/2, step 2945/7134 completed (loss: 0.01221099030226469, acc: 0.9953343868255615)
[2025-02-13 04:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32][root][INFO] - Training Epoch: 2/2, step 2946/7134 completed (loss: 0.028178242966532707, acc: 0.992732584476471)
[2025-02-13 04:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32][root][INFO] - Training Epoch: 2/2, step 2947/7134 completed (loss: 0.03741545230150223, acc: 0.9858490824699402)
[2025-02-13 04:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32][root][INFO] - Training Epoch: 2/2, step 2948/7134 completed (loss: 0.03580006584525108, acc: 0.9868203997612)
[2025-02-13 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33][root][INFO] - Training Epoch: 2/2, step 2949/7134 completed (loss: 0.00602343212813139, acc: 0.9984709620475769)
[2025-02-13 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33][root][INFO] - Training Epoch: 2/2, step 2950/7134 completed (loss: 0.021000031381845474, acc: 0.9931034445762634)
[2025-02-13 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34][root][INFO] - Training Epoch: 2/2, step 2951/7134 completed (loss: 0.006788736209273338, acc: 0.9985507130622864)
[2025-02-13 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34][root][INFO] - Training Epoch: 2/2, step 2952/7134 completed (loss: 0.014770804904401302, acc: 0.9940652847290039)
[2025-02-13 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35][root][INFO] - Training Epoch: 2/2, step 2953/7134 completed (loss: 0.02397020161151886, acc: 0.9915540814399719)
[2025-02-13 04:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35][root][INFO] - Training Epoch: 2/2, step 2954/7134 completed (loss: 0.026686061173677444, acc: 0.992277979850769)
[2025-02-13 04:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35][root][INFO] - Training Epoch: 2/2, step 2955/7134 completed (loss: 0.018009619787335396, acc: 0.9943181872367859)
[2025-02-13 04:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36][root][INFO] - Training Epoch: 2/2, step 2956/7134 completed (loss: 0.01902306079864502, acc: 0.9941434860229492)
[2025-02-13 04:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36][root][INFO] - Training Epoch: 2/2, step 2957/7134 completed (loss: 0.011834416538476944, acc: 0.9973261952400208)
[2025-02-13 04:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37][root][INFO] - Training Epoch: 2/2, step 2958/7134 completed (loss: 0.0150204012170434, acc: 0.9948717951774597)
[2025-02-13 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37][root][INFO] - Training Epoch: 2/2, step 2959/7134 completed (loss: 0.013939596712589264, acc: 0.9941520690917969)
[2025-02-13 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38][root][INFO] - Training Epoch: 2/2, step 2960/7134 completed (loss: 0.01945609040558338, acc: 0.9931129217147827)
[2025-02-13 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38][root][INFO] - Training Epoch: 2/2, step 2961/7134 completed (loss: 0.01345130242407322, acc: 0.9931972622871399)
[2025-02-13 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38][root][INFO] - Training Epoch: 2/2, step 2962/7134 completed (loss: 0.011594806797802448, acc: 0.9961190223693848)
[2025-02-13 04:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:39][root][INFO] - Training Epoch: 2/2, step 2963/7134 completed (loss: 0.016304584220051765, acc: 0.9934554696083069)
[2025-02-13 04:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:39][root][INFO] - Training Epoch: 2/2, step 2964/7134 completed (loss: 0.01231154054403305, acc: 0.9956076145172119)
[2025-02-13 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40][root][INFO] - Training Epoch: 2/2, step 2965/7134 completed (loss: 0.054364144802093506, acc: 0.9865671396255493)
[2025-02-13 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40][root][INFO] - Training Epoch: 2/2, step 2966/7134 completed (loss: 0.003926477394998074, acc: 1.0)
[2025-02-13 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41][root][INFO] - Training Epoch: 2/2, step 2967/7134 completed (loss: 0.027389707043766975, acc: 0.9902234673500061)
[2025-02-13 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41][root][INFO] - Training Epoch: 2/2, step 2968/7134 completed (loss: 0.1158560961484909, acc: 0.9694533944129944)
[2025-02-13 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:42][root][INFO] - Training Epoch: 2/2, step 2969/7134 completed (loss: 0.19792692363262177, acc: 0.9526627063751221)
[2025-02-13 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:42][root][INFO] - Training Epoch: 2/2, step 2970/7134 completed (loss: 0.18741725385189056, acc: 0.9564270377159119)
[2025-02-13 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:42][root][INFO] - Training Epoch: 2/2, step 2971/7134 completed (loss: 0.038217369467020035, acc: 0.9894875288009644)
[2025-02-13 04:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:43][root][INFO] - Training Epoch: 2/2, step 2972/7134 completed (loss: 0.02547193318605423, acc: 0.9955489635467529)
[2025-02-13 04:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:43][root][INFO] - Training Epoch: 2/2, step 2973/7134 completed (loss: 0.0331028513610363, acc: 0.9898734092712402)
[2025-02-13 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:44][root][INFO] - Training Epoch: 2/2, step 2974/7134 completed (loss: 0.04254726693034172, acc: 0.9870298504829407)
[2025-02-13 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:44][root][INFO] - Training Epoch: 2/2, step 2975/7134 completed (loss: 0.027628185227513313, acc: 0.988950252532959)
[2025-02-13 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:45][root][INFO] - Training Epoch: 2/2, step 2976/7134 completed (loss: 0.039226118475198746, acc: 0.9893617033958435)
[2025-02-13 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:45][root][INFO] - Training Epoch: 2/2, step 2977/7134 completed (loss: 0.043236635625362396, acc: 0.9884393215179443)
[2025-02-13 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46][root][INFO] - Training Epoch: 2/2, step 2978/7134 completed (loss: 0.0332045741379261, acc: 0.9914893507957458)
[2025-02-13 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46][root][INFO] - Training Epoch: 2/2, step 2979/7134 completed (loss: 0.008154723793268204, acc: 1.0)
[2025-02-13 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47][root][INFO] - Training Epoch: 2/2, step 2980/7134 completed (loss: 0.03620366007089615, acc: 0.9923567175865173)
[2025-02-13 04:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47][root][INFO] - Training Epoch: 2/2, step 2981/7134 completed (loss: 0.05294658988714218, acc: 0.9877883195877075)
[2025-02-13 04:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47][root][INFO] - Training Epoch: 2/2, step 2982/7134 completed (loss: 0.04569792002439499, acc: 0.9895287752151489)
[2025-02-13 04:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48][root][INFO] - Training Epoch: 2/2, step 2983/7134 completed (loss: 0.05055005103349686, acc: 0.9867724776268005)
[2025-02-13 04:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48][root][INFO] - Training Epoch: 2/2, step 2984/7134 completed (loss: 0.02547507919371128, acc: 0.9937106966972351)
[2025-02-13 04:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49][root][INFO] - Training Epoch: 2/2, step 2985/7134 completed (loss: 0.024678070098161697, acc: 0.9916567206382751)
[2025-02-13 04:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49][root][INFO] - Training Epoch: 2/2, step 2986/7134 completed (loss: 0.012803122401237488, acc: 0.9952718615531921)
[2025-02-13 04:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50][root][INFO] - Training Epoch: 2/2, step 2987/7134 completed (loss: 0.030826684087514877, acc: 0.9917550086975098)
[2025-02-13 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50][root][INFO] - Training Epoch: 2/2, step 2988/7134 completed (loss: 0.02093484438955784, acc: 0.9963503479957581)
[2025-02-13 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51][root][INFO] - Training Epoch: 2/2, step 2989/7134 completed (loss: 0.019039221107959747, acc: 0.9895424842834473)
[2025-02-13 04:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51][root][INFO] - Training Epoch: 2/2, step 2990/7134 completed (loss: 0.09785506129264832, acc: 0.9737762212753296)
[2025-02-13 04:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51][root][INFO] - Training Epoch: 2/2, step 2991/7134 completed (loss: 0.11420056223869324, acc: 0.9639389514923096)
[2025-02-13 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52][root][INFO] - Training Epoch: 2/2, step 2992/7134 completed (loss: 0.06321299076080322, acc: 0.9834087491035461)
[2025-02-13 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52][root][INFO] - Training Epoch: 2/2, step 2993/7134 completed (loss: 0.03134816139936447, acc: 0.9902200698852539)
[2025-02-13 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53][root][INFO] - Training Epoch: 2/2, step 2994/7134 completed (loss: 0.02034849300980568, acc: 0.9910072088241577)
[2025-02-13 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53][root][INFO] - Training Epoch: 2/2, step 2995/7134 completed (loss: 0.015755578875541687, acc: 0.9954802393913269)
[2025-02-13 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54][root][INFO] - Training Epoch: 2/2, step 2996/7134 completed (loss: 0.03179917857050896, acc: 0.9894737005233765)
[2025-02-13 04:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54][root][INFO] - Training Epoch: 2/2, step 2997/7134 completed (loss: 0.01874849572777748, acc: 0.9958419799804688)
[2025-02-13 04:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55][root][INFO] - Training Epoch: 2/2, step 2998/7134 completed (loss: 0.006286940071731806, acc: 0.9987878799438477)
[2025-02-13 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55][root][INFO] - Training Epoch: 2/2, step 2999/7134 completed (loss: 0.01550346240401268, acc: 0.9971056580543518)
[2025-02-13 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56][root][INFO] - Training Epoch: 2/2, step 3000/7134 completed (loss: 0.033403486013412476, acc: 0.9935759902000427)
[2025-02-13 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56][root][INFO] - Training Epoch: 2/2, step 3001/7134 completed (loss: 0.010581128299236298, acc: 1.0)
[2025-02-13 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56][root][INFO] - Training Epoch: 2/2, step 3002/7134 completed (loss: 0.019558049738407135, acc: 0.9954648613929749)
[2025-02-13 04:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57][root][INFO] - Training Epoch: 2/2, step 3003/7134 completed (loss: 0.01291892770677805, acc: 0.9988399147987366)
[2025-02-13 04:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57][root][INFO] - Training Epoch: 2/2, step 3004/7134 completed (loss: 0.00852331705391407, acc: 0.9977653622627258)
[2025-02-13 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58][root][INFO] - Training Epoch: 2/2, step 3005/7134 completed (loss: 0.010884563438594341, acc: 0.9977195262908936)
[2025-02-13 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58][root][INFO] - Training Epoch: 2/2, step 3006/7134 completed (loss: 0.009699949063360691, acc: 0.9961685538291931)
[2025-02-13 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59][root][INFO] - Training Epoch: 2/2, step 3007/7134 completed (loss: 0.01175602339208126, acc: 0.9964871406555176)
[2025-02-13 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59][root][INFO] - Training Epoch: 2/2, step 3008/7134 completed (loss: 0.029405023902654648, acc: 0.9902098178863525)
[2025-02-13 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00][root][INFO] - Training Epoch: 2/2, step 3009/7134 completed (loss: 0.06659487634897232, acc: 0.9886075854301453)
[2025-02-13 04:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00][root][INFO] - Training Epoch: 2/2, step 3010/7134 completed (loss: 0.03181546553969383, acc: 0.988950252532959)
[2025-02-13 04:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:01][root][INFO] - Training Epoch: 2/2, step 3011/7134 completed (loss: 0.07203160971403122, acc: 0.9743223786354065)
[2025-02-13 04:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:01][root][INFO] - Training Epoch: 2/2, step 3012/7134 completed (loss: 0.03952975943684578, acc: 0.9904912710189819)
[2025-02-13 04:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02][root][INFO] - Training Epoch: 2/2, step 3013/7134 completed (loss: 0.007212009280920029, acc: 0.9976958632469177)
[2025-02-13 04:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02][root][INFO] - Training Epoch: 2/2, step 3014/7134 completed (loss: 0.026764793321490288, acc: 0.9906213283538818)
[2025-02-13 04:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02][root][INFO] - Training Epoch: 2/2, step 3015/7134 completed (loss: 0.017049245536327362, acc: 0.997245192527771)
[2025-02-13 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03][root][INFO] - Training Epoch: 2/2, step 3016/7134 completed (loss: 0.013111397624015808, acc: 0.9957805871963501)
[2025-02-13 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03][root][INFO] - Training Epoch: 2/2, step 3017/7134 completed (loss: 0.017425846308469772, acc: 0.994915246963501)
[2025-02-13 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04][root][INFO] - Training Epoch: 2/2, step 3018/7134 completed (loss: 0.02237492799758911, acc: 0.9966942071914673)
[2025-02-13 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04][root][INFO] - Training Epoch: 2/2, step 3019/7134 completed (loss: 0.0195161160081625, acc: 0.9950980544090271)
[2025-02-13 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05][root][INFO] - Training Epoch: 2/2, step 3020/7134 completed (loss: 0.02167838253080845, acc: 0.9928698539733887)
[2025-02-13 04:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05][root][INFO] - Training Epoch: 2/2, step 3021/7134 completed (loss: 0.019082821905612946, acc: 0.9945454597473145)
[2025-02-13 04:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06][root][INFO] - Training Epoch: 2/2, step 3022/7134 completed (loss: 0.033282484859228134, acc: 0.9935691356658936)
[2025-02-13 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06][root][INFO] - Training Epoch: 2/2, step 3023/7134 completed (loss: 0.03488178923726082, acc: 0.9860835075378418)
[2025-02-13 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06][root][INFO] - Training Epoch: 2/2, step 3024/7134 completed (loss: 0.005060994066298008, acc: 0.998084306716919)
[2025-02-13 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07][root][INFO] - Training Epoch: 2/2, step 3025/7134 completed (loss: 0.055449940264225006, acc: 0.9862068891525269)
[2025-02-13 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07][root][INFO] - Training Epoch: 2/2, step 3026/7134 completed (loss: 0.00575276417657733, acc: 0.9982046484947205)
[2025-02-13 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08][root][INFO] - Training Epoch: 2/2, step 3027/7134 completed (loss: 0.03820798546075821, acc: 0.9939393997192383)
[2025-02-13 04:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08][root][INFO] - Training Epoch: 2/2, step 3028/7134 completed (loss: 0.010605710558593273, acc: 0.9954648613929749)
[2025-02-13 04:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09][root][INFO] - Training Epoch: 2/2, step 3029/7134 completed (loss: 0.026669872924685478, acc: 0.9938176274299622)
[2025-02-13 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09][root][INFO] - Training Epoch: 2/2, step 3030/7134 completed (loss: 0.013192147947847843, acc: 0.9938931465148926)
[2025-02-13 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09][root][INFO] - Training Epoch: 2/2, step 3031/7134 completed (loss: 0.006984353065490723, acc: 0.9957627058029175)
[2025-02-13 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:10][root][INFO] - Training Epoch: 2/2, step 3032/7134 completed (loss: 0.03073820099234581, acc: 0.9909256100654602)
[2025-02-13 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:10][root][INFO] - Training Epoch: 2/2, step 3033/7134 completed (loss: 0.022048063576221466, acc: 0.9911816716194153)
[2025-02-13 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11][root][INFO] - Training Epoch: 2/2, step 3034/7134 completed (loss: 0.024713026359677315, acc: 0.9936169981956482)
[2025-02-13 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11][root][INFO] - Training Epoch: 2/2, step 3035/7134 completed (loss: 0.008923751302063465, acc: 0.9968503713607788)
[2025-02-13 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12][root][INFO] - Training Epoch: 2/2, step 3036/7134 completed (loss: 0.03046543151140213, acc: 0.9916387796401978)
[2025-02-13 04:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12][root][INFO] - Training Epoch: 2/2, step 3037/7134 completed (loss: 0.017763398587703705, acc: 0.9938367009162903)
[2025-02-13 04:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12][root][INFO] - Training Epoch: 2/2, step 3038/7134 completed (loss: 0.0073509239591658115, acc: 0.9982608556747437)
[2025-02-13 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13][root][INFO] - Training Epoch: 2/2, step 3039/7134 completed (loss: 0.02433658577501774, acc: 0.9954057931900024)
[2025-02-13 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13][root][INFO] - Training Epoch: 2/2, step 3040/7134 completed (loss: 0.0200255885720253, acc: 0.994434118270874)
[2025-02-13 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14][root][INFO] - Training Epoch: 2/2, step 3041/7134 completed (loss: 0.052104536443948746, acc: 0.9904305934906006)
[2025-02-13 04:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14][root][INFO] - Training Epoch: 2/2, step 3042/7134 completed (loss: 0.017340736463665962, acc: 0.9955686926841736)
[2025-02-13 04:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15][root][INFO] - Training Epoch: 2/2, step 3043/7134 completed (loss: 0.01152875181287527, acc: 0.9961977005004883)
[2025-02-13 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15][root][INFO] - Training Epoch: 2/2, step 3044/7134 completed (loss: 0.02101694978773594, acc: 0.9948186278343201)
[2025-02-13 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15][root][INFO] - Training Epoch: 2/2, step 3045/7134 completed (loss: 0.01516643911600113, acc: 0.994915246963501)
[2025-02-13 04:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16][root][INFO] - Training Epoch: 2/2, step 3046/7134 completed (loss: 0.014877377077937126, acc: 0.9931694269180298)
[2025-02-13 04:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16][root][INFO] - Training Epoch: 2/2, step 3047/7134 completed (loss: 0.005533530376851559, acc: 0.9987421631813049)
[2025-02-13 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17][root][INFO] - Training Epoch: 2/2, step 3048/7134 completed (loss: 0.018769321963191032, acc: 0.996221661567688)
[2025-02-13 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17][root][INFO] - Training Epoch: 2/2, step 3049/7134 completed (loss: 0.011934999376535416, acc: 0.9956395626068115)
[2025-02-13 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18][root][INFO] - Training Epoch: 2/2, step 3050/7134 completed (loss: 0.020599059760570526, acc: 0.9940617680549622)
[2025-02-13 04:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18][root][INFO] - Training Epoch: 2/2, step 3051/7134 completed (loss: 0.02401099167764187, acc: 0.9894737005233765)
[2025-02-13 04:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19][root][INFO] - Training Epoch: 2/2, step 3052/7134 completed (loss: 0.021278081461787224, acc: 0.9922680258750916)
[2025-02-13 04:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19][root][INFO] - Training Epoch: 2/2, step 3053/7134 completed (loss: 0.013324842788279057, acc: 0.9939613342285156)
[2025-02-13 04:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20][root][INFO] - Training Epoch: 2/2, step 3054/7134 completed (loss: 0.03636639937758446, acc: 0.9883117079734802)
[2025-02-13 04:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20][root][INFO] - Training Epoch: 2/2, step 3055/7134 completed (loss: 0.020938189700245857, acc: 0.9914529919624329)
[2025-02-13 04:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20][root][INFO] - Training Epoch: 2/2, step 3056/7134 completed (loss: 0.008599163964390755, acc: 0.9961439371109009)
[2025-02-13 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21][root][INFO] - Training Epoch: 2/2, step 3057/7134 completed (loss: 0.009650640189647675, acc: 0.9958391189575195)
[2025-02-13 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21][root][INFO] - Training Epoch: 2/2, step 3058/7134 completed (loss: 0.04514371231198311, acc: 0.9883419871330261)
[2025-02-13 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22][root][INFO] - Training Epoch: 2/2, step 3059/7134 completed (loss: 0.006785692647099495, acc: 0.9972602725028992)
[2025-02-13 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22][root][INFO] - Training Epoch: 2/2, step 3060/7134 completed (loss: 0.012660199776291847, acc: 0.9940333962440491)
[2025-02-13 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23][root][INFO] - Training Epoch: 2/2, step 3061/7134 completed (loss: 0.018867889419198036, acc: 0.9939393997192383)
[2025-02-13 04:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23][root][INFO] - Training Epoch: 2/2, step 3062/7134 completed (loss: 0.007593601010739803, acc: 0.9973045587539673)
[2025-02-13 04:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24][root][INFO] - Training Epoch: 2/2, step 3063/7134 completed (loss: 0.017054615542292595, acc: 0.9961488842964172)
[2025-02-13 04:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24][root][INFO] - Training Epoch: 2/2, step 3064/7134 completed (loss: 0.00982274767011404, acc: 0.997183084487915)
[2025-02-13 04:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24][root][INFO] - Training Epoch: 2/2, step 3065/7134 completed (loss: 0.01035302598029375, acc: 0.9959677457809448)
[2025-02-13 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25][root][INFO] - Training Epoch: 2/2, step 3066/7134 completed (loss: 0.005437188781797886, acc: 0.9983870983123779)
[2025-02-13 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25][root][INFO] - Training Epoch: 2/2, step 3067/7134 completed (loss: 0.021927937865257263, acc: 0.9945454597473145)
[2025-02-13 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:26][root][INFO] - Training Epoch: 2/2, step 3068/7134 completed (loss: 0.019353728741407394, acc: 0.9932340979576111)
[2025-02-13 04:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:26][root][INFO] - Training Epoch: 2/2, step 3069/7134 completed (loss: 0.010455451905727386, acc: 0.9988479018211365)
[2025-02-13 04:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27][root][INFO] - Training Epoch: 2/2, step 3070/7134 completed (loss: 0.010376826860010624, acc: 0.9946977496147156)
[2025-02-13 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27][root][INFO] - Training Epoch: 2/2, step 3071/7134 completed (loss: 0.018805991858243942, acc: 0.9961389899253845)
[2025-02-13 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28][root][INFO] - Training Epoch: 2/2, step 3072/7134 completed (loss: 0.016577932983636856, acc: 0.9964994192123413)
[2025-02-13 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28][root][INFO] - Training Epoch: 2/2, step 3073/7134 completed (loss: 0.007597943767905235, acc: 0.9974554777145386)
[2025-02-13 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28][root][INFO] - Training Epoch: 2/2, step 3074/7134 completed (loss: 0.011066491715610027, acc: 0.9966722130775452)
[2025-02-13 04:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29][root][INFO] - Training Epoch: 2/2, step 3075/7134 completed (loss: 0.007361444644629955, acc: 1.0)
[2025-02-13 04:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29][root][INFO] - Training Epoch: 2/2, step 3076/7134 completed (loss: 0.013177388347685337, acc: 0.9944506287574768)
[2025-02-13 04:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30][root][INFO] - Training Epoch: 2/2, step 3077/7134 completed (loss: 0.029618553817272186, acc: 0.991256833076477)
[2025-02-13 04:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30][root][INFO] - Training Epoch: 2/2, step 3078/7134 completed (loss: 0.04223984107375145, acc: 0.9890710115432739)
[2025-02-13 04:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:31][root][INFO] - Training Epoch: 2/2, step 3079/7134 completed (loss: 0.02637309394776821, acc: 0.9925925731658936)
[2025-02-13 04:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:31][root][INFO] - Training Epoch: 2/2, step 3080/7134 completed (loss: 0.03292388096451759, acc: 0.9880043864250183)
[2025-02-13 04:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32][root][INFO] - Training Epoch: 2/2, step 3081/7134 completed (loss: 0.017135733738541603, acc: 0.9956896305084229)
[2025-02-13 04:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32][root][INFO] - Training Epoch: 2/2, step 3082/7134 completed (loss: 0.047278206795454025, acc: 0.9819494485855103)
[2025-02-13 04:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33][root][INFO] - Training Epoch: 2/2, step 3083/7134 completed (loss: 0.027435587719082832, acc: 0.9921466112136841)
[2025-02-13 04:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33][root][INFO] - Training Epoch: 2/2, step 3084/7134 completed (loss: 0.03446860611438751, acc: 0.9904761910438538)
[2025-02-13 04:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34][root][INFO] - Training Epoch: 2/2, step 3085/7134 completed (loss: 0.0420561246573925, acc: 0.9868612885475159)
[2025-02-13 04:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34][root][INFO] - Training Epoch: 2/2, step 3086/7134 completed (loss: 0.04680953919887543, acc: 0.985409677028656)
[2025-02-13 04:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34][root][INFO] - Training Epoch: 2/2, step 3087/7134 completed (loss: 0.03327486291527748, acc: 0.9885222315788269)
[2025-02-13 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35][root][INFO] - Training Epoch: 2/2, step 3088/7134 completed (loss: 0.023701408877968788, acc: 0.9921082258224487)
[2025-02-13 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35][root][INFO] - Training Epoch: 2/2, step 3089/7134 completed (loss: 0.020418308675289154, acc: 0.9935275316238403)
[2025-02-13 04:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36][root][INFO] - Training Epoch: 2/2, step 3090/7134 completed (loss: 0.043659910559654236, acc: 0.98777174949646)
[2025-02-13 04:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36][root][INFO] - Training Epoch: 2/2, step 3091/7134 completed (loss: 0.035400789231061935, acc: 0.9884792566299438)
[2025-02-13 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37][root][INFO] - Training Epoch: 2/2, step 3092/7134 completed (loss: 0.029706481844186783, acc: 0.9889094233512878)
[2025-02-13 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37][root][INFO] - Training Epoch: 2/2, step 3093/7134 completed (loss: 0.03112730197608471, acc: 0.9878587126731873)
[2025-02-13 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38][root][INFO] - Training Epoch: 2/2, step 3094/7134 completed (loss: 0.00682045379653573, acc: 0.9972752332687378)
[2025-02-13 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38][root][INFO] - Training Epoch: 2/2, step 3095/7134 completed (loss: 0.03884422406554222, acc: 0.9916666746139526)
[2025-02-13 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39][root][INFO] - Training Epoch: 2/2, step 3096/7134 completed (loss: 0.03974394500255585, acc: 0.9858793616294861)
[2025-02-13 04:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39][root][INFO] - Training Epoch: 2/2, step 3097/7134 completed (loss: 0.039275381714105606, acc: 0.9886040091514587)
[2025-02-13 04:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:40][root][INFO] - Training Epoch: 2/2, step 3098/7134 completed (loss: 0.013116057962179184, acc: 0.9952830076217651)
[2025-02-13 04:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:40][root][INFO] - Training Epoch: 2/2, step 3099/7134 completed (loss: 0.0092020183801651, acc: 0.9983792304992676)
[2025-02-13 04:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41][root][INFO] - Training Epoch: 2/2, step 3100/7134 completed (loss: 0.01914684660732746, acc: 0.9946808218955994)
[2025-02-13 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41][root][INFO] - Training Epoch: 2/2, step 3101/7134 completed (loss: 0.012061655521392822, acc: 0.9975669384002686)
[2025-02-13 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41][root][INFO] - Training Epoch: 2/2, step 3102/7134 completed (loss: 0.010162455961108208, acc: 0.995555579662323)
[2025-02-13 04:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42][root][INFO] - Training Epoch: 2/2, step 3103/7134 completed (loss: 0.01926034688949585, acc: 0.9936102032661438)
[2025-02-13 04:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42][root][INFO] - Training Epoch: 2/2, step 3104/7134 completed (loss: 0.013708572834730148, acc: 0.9980915784835815)
[2025-02-13 04:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:43][root][INFO] - Training Epoch: 2/2, step 3105/7134 completed (loss: 0.03513947129249573, acc: 0.9868131875991821)
[2025-02-13 04:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:43][root][INFO] - Training Epoch: 2/2, step 3106/7134 completed (loss: 0.014520087279379368, acc: 0.9949832558631897)
[2025-02-13 04:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44][root][INFO] - Training Epoch: 2/2, step 3107/7134 completed (loss: 0.008477813564240932, acc: 0.9973856210708618)
[2025-02-13 04:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44][root][INFO] - Training Epoch: 2/2, step 3108/7134 completed (loss: 0.03578175604343414, acc: 0.9938461780548096)
[2025-02-13 04:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45][root][INFO] - Training Epoch: 2/2, step 3109/7134 completed (loss: 0.024109279736876488, acc: 0.9944649338722229)
[2025-02-13 04:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45][root][INFO] - Training Epoch: 2/2, step 3110/7134 completed (loss: 0.010822860524058342, acc: 0.995121955871582)
[2025-02-13 04:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45][root][INFO] - Training Epoch: 2/2, step 3111/7134 completed (loss: 0.09256979078054428, acc: 0.975944995880127)
[2025-02-13 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46][root][INFO] - Training Epoch: 2/2, step 3112/7134 completed (loss: 0.02540694922208786, acc: 0.9947368502616882)
[2025-02-13 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46][root][INFO] - Training Epoch: 2/2, step 3113/7134 completed (loss: 0.018608365207910538, acc: 0.9943820238113403)
[2025-02-13 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47][root][INFO] - Training Epoch: 2/2, step 3114/7134 completed (loss: 0.03744398057460785, acc: 0.9879310131072998)
[2025-02-13 04:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47][root][INFO] - Training Epoch: 2/2, step 3115/7134 completed (loss: 0.04913812130689621, acc: 0.9891975522041321)
[2025-02-13 04:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48][root][INFO] - Training Epoch: 2/2, step 3116/7134 completed (loss: 0.014802861958742142, acc: 0.9948717951774597)
[2025-02-13 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48][root][INFO] - Training Epoch: 2/2, step 3117/7134 completed (loss: 0.05389028042554855, acc: 0.98777174949646)
[2025-02-13 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49][root][INFO] - Training Epoch: 2/2, step 3118/7134 completed (loss: 0.024702655151486397, acc: 0.9934123754501343)
[2025-02-13 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49][root][INFO] - Training Epoch: 2/2, step 3119/7134 completed (loss: 0.05291307345032692, acc: 0.9844357967376709)
[2025-02-13 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49][root][INFO] - Training Epoch: 2/2, step 3120/7134 completed (loss: 0.04449470713734627, acc: 0.9929478168487549)
[2025-02-13 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50][root][INFO] - Training Epoch: 2/2, step 3121/7134 completed (loss: 0.026152323931455612, acc: 0.9935732483863831)
[2025-02-13 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50][root][INFO] - Training Epoch: 2/2, step 3122/7134 completed (loss: 0.036990176886320114, acc: 0.9918604493141174)
[2025-02-13 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51][root][INFO] - Training Epoch: 2/2, step 3123/7134 completed (loss: 0.03898504376411438, acc: 0.9875621795654297)
[2025-02-13 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51][root][INFO] - Training Epoch: 2/2, step 3124/7134 completed (loss: 0.02718367427587509, acc: 0.9938119053840637)
[2025-02-13 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52][root][INFO] - Training Epoch: 2/2, step 3125/7134 completed (loss: 0.014188206754624844, acc: 0.998487114906311)
[2025-02-13 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52][root][INFO] - Training Epoch: 2/2, step 3126/7134 completed (loss: 0.02031341940164566, acc: 0.9961880445480347)
[2025-02-13 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53][root][INFO] - Training Epoch: 2/2, step 3127/7134 completed (loss: 0.022779839113354683, acc: 0.9930232763290405)
[2025-02-13 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53][root][INFO] - Training Epoch: 2/2, step 3128/7134 completed (loss: 0.02340824529528618, acc: 0.9935483932495117)
[2025-02-13 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54][root][INFO] - Training Epoch: 2/2, step 3129/7134 completed (loss: 0.02147866040468216, acc: 0.993819534778595)
[2025-02-13 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54][root][INFO] - Training Epoch: 2/2, step 3130/7134 completed (loss: 0.011659608222544193, acc: 0.9965596199035645)
[2025-02-13 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54][root][INFO] - Training Epoch: 2/2, step 3131/7134 completed (loss: 0.031660255044698715, acc: 0.9884058237075806)
[2025-02-13 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:55][root][INFO] - Training Epoch: 2/2, step 3132/7134 completed (loss: 0.008298262022435665, acc: 1.0)
[2025-02-13 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:55][root][INFO] - Training Epoch: 2/2, step 3133/7134 completed (loss: 0.07108399271965027, acc: 0.9848901033401489)
[2025-02-13 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56][root][INFO] - Training Epoch: 2/2, step 3134/7134 completed (loss: 0.019172487780451775, acc: 0.9963369965553284)
[2025-02-13 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56][root][INFO] - Training Epoch: 2/2, step 3135/7134 completed (loss: 0.016497356817126274, acc: 0.9907621145248413)
[2025-02-13 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57][root][INFO] - Training Epoch: 2/2, step 3136/7134 completed (loss: 0.014360945671796799, acc: 0.9930434823036194)
[2025-02-13 04:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57][root][INFO] - Training Epoch: 2/2, step 3137/7134 completed (loss: 0.03142096474766731, acc: 0.9900793433189392)
[2025-02-13 04:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57][root][INFO] - Training Epoch: 2/2, step 3138/7134 completed (loss: 0.037612296640872955, acc: 0.9941176176071167)
[2025-02-13 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58][root][INFO] - Training Epoch: 2/2, step 3139/7134 completed (loss: 0.054696328938007355, acc: 0.9828326106071472)
[2025-02-13 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58][root][INFO] - Training Epoch: 2/2, step 3140/7134 completed (loss: 0.022664714604616165, acc: 0.994163453578949)
[2025-02-13 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:59][root][INFO] - Training Epoch: 2/2, step 3141/7134 completed (loss: 0.04324730858206749, acc: 0.9912587404251099)
[2025-02-13 04:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:59][root][INFO] - Training Epoch: 2/2, step 3142/7134 completed (loss: 0.004332092124968767, acc: 1.0)
[2025-02-13 04:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:00][root][INFO] - Training Epoch: 2/2, step 3143/7134 completed (loss: 0.012427244335412979, acc: 0.9983922839164734)
[2025-02-13 04:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:00][root][INFO] - Training Epoch: 2/2, step 3144/7134 completed (loss: 0.013146813027560711, acc: 0.9952830076217651)
[2025-02-13 04:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:00][root][INFO] - Training Epoch: 2/2, step 3145/7134 completed (loss: 0.02381780929863453, acc: 0.9890590906143188)
[2025-02-13 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01][root][INFO] - Training Epoch: 2/2, step 3146/7134 completed (loss: 0.015242408961057663, acc: 0.9921875)
[2025-02-13 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01][root][INFO] - Training Epoch: 2/2, step 3147/7134 completed (loss: 0.031821638345718384, acc: 0.9935794472694397)
[2025-02-13 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02][root][INFO] - Training Epoch: 2/2, step 3148/7134 completed (loss: 0.01745097152888775, acc: 0.9961734414100647)
[2025-02-13 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02][root][INFO] - Training Epoch: 2/2, step 3149/7134 completed (loss: 0.005920668598264456, acc: 0.9985358715057373)
[2025-02-13 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03][root][INFO] - Training Epoch: 2/2, step 3150/7134 completed (loss: 0.1001521646976471, acc: 0.9802761077880859)
[2025-02-13 04:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03][root][INFO] - Training Epoch: 2/2, step 3151/7134 completed (loss: 0.0554211288690567, acc: 0.9865900278091431)
[2025-02-13 04:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03][root][INFO] - Training Epoch: 2/2, step 3152/7134 completed (loss: 0.02902098558843136, acc: 0.9909090995788574)
[2025-02-13 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04][root][INFO] - Training Epoch: 2/2, step 3153/7134 completed (loss: 0.033767689019441605, acc: 0.9862155318260193)
[2025-02-13 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04][root][INFO] - Training Epoch: 2/2, step 3154/7134 completed (loss: 0.028794029727578163, acc: 0.9911190271377563)
[2025-02-13 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05][root][INFO] - Training Epoch: 2/2, step 3155/7134 completed (loss: 0.057860005646944046, acc: 0.9837728142738342)
[2025-02-13 04:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05][root][INFO] - Training Epoch: 2/2, step 3156/7134 completed (loss: 0.0719466358423233, acc: 0.9762258529663086)
[2025-02-13 04:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06][root][INFO] - Training Epoch: 2/2, step 3157/7134 completed (loss: 0.00810146238654852, acc: 1.0)
[2025-02-13 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06][root][INFO] - Training Epoch: 2/2, step 3158/7134 completed (loss: 0.010485291481018066, acc: 0.9983659982681274)
[2025-02-13 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:07][root][INFO] - Training Epoch: 2/2, step 3159/7134 completed (loss: 0.02429421991109848, acc: 0.9877451062202454)
[2025-02-13 04:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:07][root][INFO] - Training Epoch: 2/2, step 3160/7134 completed (loss: 0.01581615023314953, acc: 0.9944674968719482)
[2025-02-13 04:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:07][root][INFO] - Training Epoch: 2/2, step 3161/7134 completed (loss: 0.02968508191406727, acc: 0.9929971694946289)
[2025-02-13 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08][root][INFO] - Training Epoch: 2/2, step 3162/7134 completed (loss: 0.010535268113017082, acc: 0.9985422492027283)
[2025-02-13 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08][root][INFO] - Training Epoch: 2/2, step 3163/7134 completed (loss: 0.029453666880726814, acc: 0.994991660118103)
[2025-02-13 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09][root][INFO] - Training Epoch: 2/2, step 3164/7134 completed (loss: 0.031236710026860237, acc: 0.9932998418807983)
[2025-02-13 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09][root][INFO] - Training Epoch: 2/2, step 3165/7134 completed (loss: 0.01336155366152525, acc: 0.994991660118103)
[2025-02-13 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10][root][INFO] - Training Epoch: 2/2, step 3166/7134 completed (loss: 0.041989006102085114, acc: 0.9834586381912231)
[2025-02-13 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10][root][INFO] - Training Epoch: 2/2, step 3167/7134 completed (loss: 0.06731614470481873, acc: 0.971875011920929)
[2025-02-13 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10][root][INFO] - Training Epoch: 2/2, step 3168/7134 completed (loss: 0.056525446474552155, acc: 0.9786585569381714)
[2025-02-13 04:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11][root][INFO] - Training Epoch: 2/2, step 3169/7134 completed (loss: 0.03270285949110985, acc: 0.9897540807723999)
[2025-02-13 04:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11][root][INFO] - Training Epoch: 2/2, step 3170/7134 completed (loss: 0.028390521183609962, acc: 0.9927272796630859)
[2025-02-13 04:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12][root][INFO] - Training Epoch: 2/2, step 3171/7134 completed (loss: 0.034222621470689774, acc: 0.9895150661468506)
[2025-02-13 04:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12][root][INFO] - Training Epoch: 2/2, step 3172/7134 completed (loss: 0.01212836243212223, acc: 0.9961685538291931)
[2025-02-13 04:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12][root][INFO] - Training Epoch: 2/2, step 3173/7134 completed (loss: 0.057860977947711945, acc: 0.9872449040412903)
[2025-02-13 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13][root][INFO] - Training Epoch: 2/2, step 3174/7134 completed (loss: 0.05417332425713539, acc: 0.9837278127670288)
[2025-02-13 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13][root][INFO] - Training Epoch: 2/2, step 3175/7134 completed (loss: 0.04453321173787117, acc: 0.98591548204422)
[2025-02-13 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:14][root][INFO] - Training Epoch: 2/2, step 3176/7134 completed (loss: 0.055522896349430084, acc: 0.9819548726081848)
[2025-02-13 04:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:14][root][INFO] - Training Epoch: 2/2, step 3177/7134 completed (loss: 0.050827112048864365, acc: 0.9878472089767456)
[2025-02-13 04:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:15][root][INFO] - Training Epoch: 2/2, step 3178/7134 completed (loss: 0.01564847119152546, acc: 0.9967159032821655)
[2025-02-13 04:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:15][root][INFO] - Training Epoch: 2/2, step 3179/7134 completed (loss: 0.05844741687178612, acc: 0.9850746393203735)
[2025-02-13 04:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:15][root][INFO] - Training Epoch: 2/2, step 3180/7134 completed (loss: 0.022710857912898064, acc: 0.9957143068313599)
[2025-02-13 04:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:16][root][INFO] - Training Epoch: 2/2, step 3181/7134 completed (loss: 0.03656116500496864, acc: 0.9875195026397705)
[2025-02-13 04:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:16][root][INFO] - Training Epoch: 2/2, step 3182/7134 completed (loss: 0.031755343079566956, acc: 0.9940387606620789)
[2025-02-13 04:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:17][root][INFO] - Training Epoch: 2/2, step 3183/7134 completed (loss: 0.04126877710223198, acc: 0.989847719669342)
[2025-02-13 04:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:17][root][INFO] - Training Epoch: 2/2, step 3184/7134 completed (loss: 0.030499160289764404, acc: 0.9910714030265808)
[2025-02-13 04:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:18][root][INFO] - Training Epoch: 2/2, step 3185/7134 completed (loss: 0.008894256316125393, acc: 0.9985755085945129)
[2025-02-13 04:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:18][root][INFO] - Training Epoch: 2/2, step 3186/7134 completed (loss: 0.011055526323616505, acc: 0.9970370531082153)
[2025-02-13 04:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:18][root][INFO] - Training Epoch: 2/2, step 3187/7134 completed (loss: 0.03018856979906559, acc: 0.9905063509941101)
[2025-02-13 04:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19][root][INFO] - Training Epoch: 2/2, step 3188/7134 completed (loss: 0.02850164659321308, acc: 0.9935317039489746)
[2025-02-13 04:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19][root][INFO] - Training Epoch: 2/2, step 3189/7134 completed (loss: 0.031154461205005646, acc: 0.9884225726127625)
[2025-02-13 04:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20][root][INFO] - Training Epoch: 2/2, step 3190/7134 completed (loss: 0.032256852835416794, acc: 0.9886075854301453)
[2025-02-13 04:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20][root][INFO] - Training Epoch: 2/2, step 3191/7134 completed (loss: 0.023252524435520172, acc: 0.9971428513526917)
[2025-02-13 04:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21][root][INFO] - Training Epoch: 2/2, step 3192/7134 completed (loss: 0.0347641296684742, acc: 0.9905481934547424)
[2025-02-13 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21][root][INFO] - Training Epoch: 2/2, step 3193/7134 completed (loss: 0.04962751269340515, acc: 0.980966329574585)
[2025-02-13 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21][root][INFO] - Training Epoch: 2/2, step 3194/7134 completed (loss: 0.031935978680849075, acc: 0.9885621070861816)
[2025-02-13 04:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22][root][INFO] - Training Epoch: 2/2, step 3195/7134 completed (loss: 0.03640839830040932, acc: 0.9914236664772034)
[2025-02-13 04:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22][root][INFO] - Training Epoch: 2/2, step 3196/7134 completed (loss: 0.018068786710500717, acc: 0.9925233721733093)
[2025-02-13 04:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:23][root][INFO] - Training Epoch: 2/2, step 3197/7134 completed (loss: 0.026502735912799835, acc: 0.997183084487915)
[2025-02-13 04:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:23][root][INFO] - Training Epoch: 2/2, step 3198/7134 completed (loss: 0.02409384585916996, acc: 0.9954614043235779)
[2025-02-13 04:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24][root][INFO] - Training Epoch: 2/2, step 3199/7134 completed (loss: 0.023105617612600327, acc: 0.9926578402519226)
[2025-02-13 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24][root][INFO] - Training Epoch: 2/2, step 3200/7134 completed (loss: 0.02447950467467308, acc: 0.9929701089859009)
[2025-02-13 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24][root][INFO] - Training Epoch: 2/2, step 3201/7134 completed (loss: 0.06835649907588959, acc: 0.9848155975341797)
[2025-02-13 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25][root][INFO] - Training Epoch: 2/2, step 3202/7134 completed (loss: 0.027740010991692543, acc: 0.9903181195259094)
[2025-02-13 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25][root][INFO] - Training Epoch: 2/2, step 3203/7134 completed (loss: 0.028403261676430702, acc: 0.9921011328697205)
[2025-02-13 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26][root][INFO] - Training Epoch: 2/2, step 3204/7134 completed (loss: 0.05952383577823639, acc: 0.981203019618988)
[2025-02-13 04:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26][root][INFO] - Training Epoch: 2/2, step 3205/7134 completed (loss: 0.0041084131225943565, acc: 1.0)
[2025-02-13 04:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26][root][INFO] - Training Epoch: 2/2, step 3206/7134 completed (loss: 0.030540037900209427, acc: 0.9913169145584106)
[2025-02-13 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27][root][INFO] - Training Epoch: 2/2, step 3207/7134 completed (loss: 0.024307388812303543, acc: 0.9908116459846497)
[2025-02-13 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27][root][INFO] - Training Epoch: 2/2, step 3208/7134 completed (loss: 0.05691245198249817, acc: 0.9869281053543091)
[2025-02-13 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28][root][INFO] - Training Epoch: 2/2, step 3209/7134 completed (loss: 0.013837730512022972, acc: 0.9970238208770752)
[2025-02-13 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28][root][INFO] - Training Epoch: 2/2, step 3210/7134 completed (loss: 0.006983455270528793, acc: 0.9984939694404602)
[2025-02-13 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29][root][INFO] - Training Epoch: 2/2, step 3211/7134 completed (loss: 0.021017473191022873, acc: 0.9958419799804688)
[2025-02-13 04:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29][root][INFO] - Training Epoch: 2/2, step 3212/7134 completed (loss: 0.002377305878326297, acc: 1.0)
[2025-02-13 04:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29][root][INFO] - Training Epoch: 2/2, step 3213/7134 completed (loss: 0.03741981461644173, acc: 0.991391658782959)
[2025-02-13 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30][root][INFO] - Training Epoch: 2/2, step 3214/7134 completed (loss: 0.021333985030651093, acc: 0.9959839582443237)
[2025-02-13 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30][root][INFO] - Training Epoch: 2/2, step 3215/7134 completed (loss: 0.026878220960497856, acc: 0.994397759437561)
[2025-02-13 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31][root][INFO] - Training Epoch: 2/2, step 3216/7134 completed (loss: 0.008231393061578274, acc: 0.9971346855163574)
[2025-02-13 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31][root][INFO] - Training Epoch: 2/2, step 3217/7134 completed (loss: 0.01014724187552929, acc: 0.9974193572998047)
[2025-02-13 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32][root][INFO] - Training Epoch: 2/2, step 3218/7134 completed (loss: 0.03727180138230324, acc: 0.9906103014945984)
[2025-02-13 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32][root][INFO] - Training Epoch: 2/2, step 3219/7134 completed (loss: 0.027202783152461052, acc: 0.9881831407546997)
[2025-02-13 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:33][root][INFO] - Training Epoch: 2/2, step 3220/7134 completed (loss: 0.026885978877544403, acc: 0.9933244585990906)
[2025-02-13 04:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:33][root][INFO] - Training Epoch: 2/2, step 3221/7134 completed (loss: 0.01864529959857464, acc: 0.9917808175086975)
[2025-02-13 04:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:33][root][INFO] - Training Epoch: 2/2, step 3222/7134 completed (loss: 0.04750142991542816, acc: 0.9887359142303467)
[2025-02-13 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:34][root][INFO] - Training Epoch: 2/2, step 3223/7134 completed (loss: 0.05105530098080635, acc: 0.9904761910438538)
[2025-02-13 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:34][root][INFO] - Training Epoch: 2/2, step 3224/7134 completed (loss: 0.045557234436273575, acc: 0.9898697733879089)
[2025-02-13 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:35][root][INFO] - Training Epoch: 2/2, step 3225/7134 completed (loss: 0.016641447320580482, acc: 0.993678867816925)
[2025-02-13 04:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:35][root][INFO] - Training Epoch: 2/2, step 3226/7134 completed (loss: 0.02464648149907589, acc: 0.9874326586723328)
[2025-02-13 04:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36][root][INFO] - Training Epoch: 2/2, step 3227/7134 completed (loss: 0.044578853994607925, acc: 0.9824175834655762)
[2025-02-13 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36][root][INFO] - Training Epoch: 2/2, step 3228/7134 completed (loss: 0.017853422090411186, acc: 0.9960578083992004)
[2025-02-13 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:37][root][INFO] - Training Epoch: 2/2, step 3229/7134 completed (loss: 0.026005098596215248, acc: 0.990208089351654)
[2025-02-13 04:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:37][root][INFO] - Training Epoch: 2/2, step 3230/7134 completed (loss: 0.031241245567798615, acc: 0.9911242723464966)
[2025-02-13 04:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:37][root][INFO] - Training Epoch: 2/2, step 3231/7134 completed (loss: 0.03496991842985153, acc: 0.9924012422561646)
[2025-02-13 04:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38][root][INFO] - Training Epoch: 2/2, step 3232/7134 completed (loss: 0.008545408956706524, acc: 0.996632993221283)
[2025-02-13 04:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38][root][INFO] - Training Epoch: 2/2, step 3233/7134 completed (loss: 0.04028017073869705, acc: 0.990138053894043)
[2025-02-13 04:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:39][root][INFO] - Training Epoch: 2/2, step 3234/7134 completed (loss: 0.0071192351169884205, acc: 0.998420238494873)
[2025-02-13 04:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:39][root][INFO] - Training Epoch: 2/2, step 3235/7134 completed (loss: 0.027256658300757408, acc: 0.9866666793823242)
[2025-02-13 04:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:40][root][INFO] - Training Epoch: 2/2, step 3236/7134 completed (loss: 0.05840526893734932, acc: 0.9808542132377625)
[2025-02-13 04:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:40][root][INFO] - Training Epoch: 2/2, step 3237/7134 completed (loss: 0.014734379015862942, acc: 0.9935587644577026)
[2025-02-13 04:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:40][root][INFO] - Training Epoch: 2/2, step 3238/7134 completed (loss: 0.026224300265312195, acc: 0.9907975196838379)
[2025-02-13 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41][root][INFO] - Training Epoch: 2/2, step 3239/7134 completed (loss: 0.011518106795847416, acc: 0.9952681660652161)
[2025-02-13 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41][root][INFO] - Training Epoch: 2/2, step 3240/7134 completed (loss: 0.023699460551142693, acc: 0.9910314083099365)
[2025-02-13 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:42][root][INFO] - Training Epoch: 2/2, step 3241/7134 completed (loss: 0.013168727979063988, acc: 0.9955489635467529)
[2025-02-13 04:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:42][root][INFO] - Training Epoch: 2/2, step 3242/7134 completed (loss: 0.010530510917305946, acc: 0.9967319965362549)
[2025-02-13 04:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43][root][INFO] - Training Epoch: 2/2, step 3243/7134 completed (loss: 0.016872936859726906, acc: 0.9959127902984619)
[2025-02-13 04:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43][root][INFO] - Training Epoch: 2/2, step 3244/7134 completed (loss: 0.00791039690375328, acc: 0.9985652565956116)
[2025-02-13 04:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43][root][INFO] - Training Epoch: 2/2, step 3245/7134 completed (loss: 0.007265733554959297, acc: 0.998711347579956)
[2025-02-13 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:44][root][INFO] - Training Epoch: 2/2, step 3246/7134 completed (loss: 0.02165733277797699, acc: 0.9935897588729858)
[2025-02-13 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:44][root][INFO] - Training Epoch: 2/2, step 3247/7134 completed (loss: 0.023364506661891937, acc: 0.9967585206031799)
[2025-02-13 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45][root][INFO] - Training Epoch: 2/2, step 3248/7134 completed (loss: 0.015883227810263634, acc: 0.9967159032821655)
[2025-02-13 04:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45][root][INFO] - Training Epoch: 2/2, step 3249/7134 completed (loss: 0.024825336411595345, acc: 0.9958391189575195)
[2025-02-13 04:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:46][root][INFO] - Training Epoch: 2/2, step 3250/7134 completed (loss: 0.025888517498970032, acc: 0.9884892106056213)
[2025-02-13 04:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:46][root][INFO] - Training Epoch: 2/2, step 3251/7134 completed (loss: 0.008383781649172306, acc: 0.9971469044685364)
[2025-02-13 04:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47][root][INFO] - Training Epoch: 2/2, step 3252/7134 completed (loss: 0.009697752073407173, acc: 0.9957447052001953)
[2025-02-13 04:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47][root][INFO] - Training Epoch: 2/2, step 3253/7134 completed (loss: 0.01666192337870598, acc: 0.995184600353241)
[2025-02-13 04:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47][root][INFO] - Training Epoch: 2/2, step 3254/7134 completed (loss: 0.0037981788627803326, acc: 1.0)
[2025-02-13 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48][root][INFO] - Training Epoch: 2/2, step 3255/7134 completed (loss: 0.007007702253758907, acc: 0.9973368644714355)
[2025-02-13 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48][root][INFO] - Training Epoch: 2/2, step 3256/7134 completed (loss: 0.01313869096338749, acc: 0.9970370531082153)
[2025-02-13 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:49][root][INFO] - Training Epoch: 2/2, step 3257/7134 completed (loss: 0.015494288876652718, acc: 0.994452178478241)
[2025-02-13 04:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:49][root][INFO] - Training Epoch: 2/2, step 3258/7134 completed (loss: 0.022511225193738937, acc: 0.9903846383094788)
[2025-02-13 04:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50][root][INFO] - Training Epoch: 2/2, step 3259/7134 completed (loss: 0.011809216812252998, acc: 0.9955882430076599)
[2025-02-13 04:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50][root][INFO] - Training Epoch: 2/2, step 3260/7134 completed (loss: 0.07432487607002258, acc: 0.9881305694580078)
[2025-02-13 04:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50][root][INFO] - Training Epoch: 2/2, step 3261/7134 completed (loss: 0.0341130793094635, acc: 0.9882550239562988)
[2025-02-13 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:51][root][INFO] - Training Epoch: 2/2, step 3262/7134 completed (loss: 0.023483768105506897, acc: 0.9927641153335571)
[2025-02-13 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:51][root][INFO] - Training Epoch: 2/2, step 3263/7134 completed (loss: 0.05202924832701683, acc: 0.988252580165863)
[2025-02-13 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52][root][INFO] - Training Epoch: 2/2, step 3264/7134 completed (loss: 0.01667638309299946, acc: 0.9972222447395325)
[2025-02-13 04:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52][root][INFO] - Training Epoch: 2/2, step 3265/7134 completed (loss: 0.06350848823785782, acc: 0.9829059839248657)
[2025-02-13 04:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:53][root][INFO] - Training Epoch: 2/2, step 3266/7134 completed (loss: 0.02813093736767769, acc: 0.9909909963607788)
[2025-02-13 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:53][root][INFO] - Training Epoch: 2/2, step 3267/7134 completed (loss: 0.05185587331652641, acc: 0.9813753366470337)
[2025-02-13 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:53][root][INFO] - Training Epoch: 2/2, step 3268/7134 completed (loss: 0.028231419622898102, acc: 0.9922958612442017)
[2025-02-13 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:54][root][INFO] - Training Epoch: 2/2, step 3269/7134 completed (loss: 0.028836775571107864, acc: 0.9898989796638489)
[2025-02-13 04:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:54][root][INFO] - Training Epoch: 2/2, step 3270/7134 completed (loss: 0.011765187606215477, acc: 0.9964115023612976)
[2025-02-13 04:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:55][root][INFO] - Training Epoch: 2/2, step 3271/7134 completed (loss: 0.0237120408564806, acc: 0.9933110475540161)
[2025-02-13 04:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:55][root][INFO] - Training Epoch: 2/2, step 3272/7134 completed (loss: 0.027826247736811638, acc: 0.9969465732574463)
[2025-02-13 04:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56][root][INFO] - Training Epoch: 2/2, step 3273/7134 completed (loss: 0.011043275706470013, acc: 0.9974457025527954)
[2025-02-13 04:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56][root][INFO] - Training Epoch: 2/2, step 3274/7134 completed (loss: 0.03372332826256752, acc: 0.988252580165863)
[2025-02-13 04:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56][root][INFO] - Training Epoch: 2/2, step 3275/7134 completed (loss: 0.1291441172361374, acc: 0.9736841917037964)
[2025-02-13 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:57][root][INFO] - Training Epoch: 2/2, step 3276/7134 completed (loss: 0.0378272645175457, acc: 0.9848484992980957)
[2025-02-13 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:57][root][INFO] - Training Epoch: 2/2, step 3277/7134 completed (loss: 0.02675734832882881, acc: 0.9878048896789551)
[2025-02-13 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:58][root][INFO] - Training Epoch: 2/2, step 3278/7134 completed (loss: 0.028105653822422028, acc: 0.9866156578063965)
[2025-02-13 04:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:58][root][INFO] - Training Epoch: 2/2, step 3279/7134 completed (loss: 0.025813153013586998, acc: 0.9950310587882996)
[2025-02-13 04:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:58][root][INFO] - Training Epoch: 2/2, step 3280/7134 completed (loss: 0.1135018914937973, acc: 0.9688796401023865)
[2025-02-13 04:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59][root][INFO] - Training Epoch: 2/2, step 3281/7134 completed (loss: 0.0392889678478241, acc: 0.9869565367698669)
[2025-02-13 04:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59][root][INFO] - Training Epoch: 2/2, step 3282/7134 completed (loss: 0.012735839001834393, acc: 0.9952531456947327)
[2025-02-13 04:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:00][root][INFO] - Training Epoch: 2/2, step 3283/7134 completed (loss: 0.03222450241446495, acc: 0.9938144087791443)
[2025-02-13 04:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:00][root][INFO] - Training Epoch: 2/2, step 3284/7134 completed (loss: 0.0269212294369936, acc: 0.990867555141449)
[2025-02-13 04:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01][root][INFO] - Training Epoch: 2/2, step 3285/7134 completed (loss: 0.0187288336455822, acc: 0.995356023311615)
[2025-02-13 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01][root][INFO] - Training Epoch: 2/2, step 3286/7134 completed (loss: 0.0210217647254467, acc: 0.9969924688339233)
[2025-02-13 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01][root][INFO] - Training Epoch: 2/2, step 3287/7134 completed (loss: 0.07690649479627609, acc: 0.9802631735801697)
[2025-02-13 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02][root][INFO] - Training Epoch: 2/2, step 3288/7134 completed (loss: 0.02337685041129589, acc: 0.9955089688301086)
[2025-02-13 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02][root][INFO] - Training Epoch: 2/2, step 3289/7134 completed (loss: 0.04008059576153755, acc: 0.9912280440330505)
[2025-02-13 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:03][root][INFO] - Training Epoch: 2/2, step 3290/7134 completed (loss: 0.033691953867673874, acc: 0.9930555820465088)
[2025-02-13 04:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:03][root][INFO] - Training Epoch: 2/2, step 3291/7134 completed (loss: 0.03236198425292969, acc: 0.9777283072471619)
[2025-02-13 04:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04][root][INFO] - Training Epoch: 2/2, step 3292/7134 completed (loss: 0.02078133448958397, acc: 0.9947090148925781)
[2025-02-13 04:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04][root][INFO] - Training Epoch: 2/2, step 3293/7134 completed (loss: 0.025267137214541435, acc: 0.991150438785553)
[2025-02-13 04:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04][root][INFO] - Training Epoch: 2/2, step 3294/7134 completed (loss: 0.055689964443445206, acc: 0.9877488613128662)
[2025-02-13 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:05][root][INFO] - Training Epoch: 2/2, step 3295/7134 completed (loss: 0.013615747913718224, acc: 0.9982935190200806)
[2025-02-13 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:05][root][INFO] - Training Epoch: 2/2, step 3296/7134 completed (loss: 0.03909625858068466, acc: 0.9853747487068176)
[2025-02-13 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06][root][INFO] - Training Epoch: 2/2, step 3297/7134 completed (loss: 0.02359456568956375, acc: 0.9929577708244324)
[2025-02-13 04:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06][root][INFO] - Training Epoch: 2/2, step 3298/7134 completed (loss: 0.058152999728918076, acc: 0.9840255379676819)
[2025-02-13 04:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:07][root][INFO] - Training Epoch: 2/2, step 3299/7134 completed (loss: 0.019726496189832687, acc: 0.9932885766029358)
[2025-02-13 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:07][root][INFO] - Training Epoch: 2/2, step 3300/7134 completed (loss: 0.04627296328544617, acc: 0.9857397675514221)
[2025-02-13 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:07][root][INFO] - Training Epoch: 2/2, step 3301/7134 completed (loss: 0.012059472501277924, acc: 0.9979252815246582)
[2025-02-13 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08][root][INFO] - Training Epoch: 2/2, step 3302/7134 completed (loss: 0.03821644186973572, acc: 0.9932659864425659)
[2025-02-13 04:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08][root][INFO] - Training Epoch: 2/2, step 3303/7134 completed (loss: 0.021096473559737206, acc: 0.9911894202232361)
[2025-02-13 04:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:09][root][INFO] - Training Epoch: 2/2, step 3304/7134 completed (loss: 0.010081246495246887, acc: 0.9976133704185486)
[2025-02-13 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:09][root][INFO] - Training Epoch: 2/2, step 3305/7134 completed (loss: 0.02095225267112255, acc: 0.993966817855835)
[2025-02-13 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:10][root][INFO] - Training Epoch: 2/2, step 3306/7134 completed (loss: 0.016376027837395668, acc: 0.9948119521141052)
[2025-02-13 04:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:10][root][INFO] - Training Epoch: 2/2, step 3307/7134 completed (loss: 0.019368380308151245, acc: 0.9932318329811096)
[2025-02-13 04:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:10][root][INFO] - Training Epoch: 2/2, step 3308/7134 completed (loss: 0.04171585291624069, acc: 0.9855491518974304)
[2025-02-13 04:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11][root][INFO] - Training Epoch: 2/2, step 3309/7134 completed (loss: 0.040991488844156265, acc: 0.9896755218505859)
[2025-02-13 04:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11][root][INFO] - Training Epoch: 2/2, step 3310/7134 completed (loss: 0.026406873017549515, acc: 0.9957355856895447)
[2025-02-13 04:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12][root][INFO] - Training Epoch: 2/2, step 3311/7134 completed (loss: 0.02152096852660179, acc: 0.9913978576660156)
[2025-02-13 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12][root][INFO] - Training Epoch: 2/2, step 3312/7134 completed (loss: 0.01841644197702408, acc: 0.9945454597473145)
[2025-02-13 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12][root][INFO] - Training Epoch: 2/2, step 3313/7134 completed (loss: 0.006930655799806118, acc: 0.9976133704185486)
[2025-02-13 04:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13][root][INFO] - Training Epoch: 2/2, step 3314/7134 completed (loss: 0.023890776559710503, acc: 0.9945651888847351)
[2025-02-13 04:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13][root][INFO] - Training Epoch: 2/2, step 3315/7134 completed (loss: 0.03816228732466698, acc: 0.9891451597213745)
[2025-02-13 04:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14][root][INFO] - Training Epoch: 2/2, step 3316/7134 completed (loss: 0.013764897361397743, acc: 0.9949579834938049)
[2025-02-13 04:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14][root][INFO] - Training Epoch: 2/2, step 3317/7134 completed (loss: 0.047615792602300644, acc: 0.9923076629638672)
[2025-02-13 04:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14][root][INFO] - Training Epoch: 2/2, step 3318/7134 completed (loss: 0.014063099399209023, acc: 0.9963898658752441)
[2025-02-13 04:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:15][root][INFO] - Training Epoch: 2/2, step 3319/7134 completed (loss: 0.02514580637216568, acc: 0.9934853315353394)
[2025-02-13 04:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:15][root][INFO] - Training Epoch: 2/2, step 3320/7134 completed (loss: 0.05504113435745239, acc: 0.9921466112136841)
[2025-02-13 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:16][root][INFO] - Training Epoch: 2/2, step 3321/7134 completed (loss: 0.07350708544254303, acc: 0.9834862351417542)
[2025-02-13 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:16][root][INFO] - Training Epoch: 2/2, step 3322/7134 completed (loss: 0.016880637034773827, acc: 0.9924924969673157)
[2025-02-13 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17][root][INFO] - Training Epoch: 2/2, step 3323/7134 completed (loss: 0.024377010762691498, acc: 0.9944827556610107)
[2025-02-13 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17][root][INFO] - Training Epoch: 2/2, step 3324/7134 completed (loss: 0.054150309413671494, acc: 0.9787985682487488)
[2025-02-13 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18][root][INFO] - Training Epoch: 2/2, step 3325/7134 completed (loss: 0.021027229726314545, acc: 0.9932795763015747)
[2025-02-13 04:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18][root][INFO] - Training Epoch: 2/2, step 3326/7134 completed (loss: 0.04910163953900337, acc: 0.98740154504776)
[2025-02-13 04:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:19][root][INFO] - Training Epoch: 2/2, step 3327/7134 completed (loss: 0.02385656349360943, acc: 0.9905277490615845)
[2025-02-13 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:19][root][INFO] - Training Epoch: 2/2, step 3328/7134 completed (loss: 0.036231040954589844, acc: 0.9887217879295349)
[2025-02-13 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:20][root][INFO] - Training Epoch: 2/2, step 3329/7134 completed (loss: 0.01369557436555624, acc: 0.9975639581680298)
[2025-02-13 04:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:20][root][INFO] - Training Epoch: 2/2, step 3330/7134 completed (loss: 0.03602701425552368, acc: 0.9919354915618896)
[2025-02-13 04:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:20][root][INFO] - Training Epoch: 2/2, step 3331/7134 completed (loss: 0.06780276447534561, acc: 0.979651153087616)
[2025-02-13 04:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21][root][INFO] - Training Epoch: 2/2, step 3332/7134 completed (loss: 0.08236540853977203, acc: 0.9775086641311646)
[2025-02-13 04:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21][root][INFO] - Training Epoch: 2/2, step 3333/7134 completed (loss: 0.03805433586239815, acc: 0.9918032884597778)
[2025-02-13 04:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:22][root][INFO] - Training Epoch: 2/2, step 3334/7134 completed (loss: 0.04454750940203667, acc: 0.9891892075538635)
[2025-02-13 04:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:22][root][INFO] - Training Epoch: 2/2, step 3335/7134 completed (loss: 0.02454209513962269, acc: 0.9920886158943176)
[2025-02-13 04:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:23][root][INFO] - Training Epoch: 2/2, step 3336/7134 completed (loss: 0.00971982441842556, acc: 0.9985422492027283)
[2025-02-13 04:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:23][root][INFO] - Training Epoch: 2/2, step 3337/7134 completed (loss: 0.007583339232951403, acc: 0.9980276226997375)
[2025-02-13 04:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:23][root][INFO] - Training Epoch: 2/2, step 3338/7134 completed (loss: 0.006296860985457897, acc: 0.9987577795982361)
[2025-02-13 04:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24][root][INFO] - Training Epoch: 2/2, step 3339/7134 completed (loss: 0.047157786786556244, acc: 0.9868735074996948)
[2025-02-13 04:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24][root][INFO] - Training Epoch: 2/2, step 3340/7134 completed (loss: 0.08131992071866989, acc: 0.9758672714233398)
[2025-02-13 04:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:25][root][INFO] - Training Epoch: 2/2, step 3341/7134 completed (loss: 0.038554273545742035, acc: 0.9911110997200012)
[2025-02-13 04:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:25][root][INFO] - Training Epoch: 2/2, step 3342/7134 completed (loss: 0.017062991857528687, acc: 0.9949324131011963)
[2025-02-13 04:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:26][root][INFO] - Training Epoch: 2/2, step 3343/7134 completed (loss: 0.019979791715741158, acc: 0.9939516186714172)
[2025-02-13 04:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:26][root][INFO] - Training Epoch: 2/2, step 3344/7134 completed (loss: 0.011269250884652138, acc: 0.994854211807251)
[2025-02-13 04:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27][root][INFO] - Training Epoch: 2/2, step 3345/7134 completed (loss: 0.05464779958128929, acc: 0.9886547923088074)
[2025-02-13 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27][root][INFO] - Training Epoch: 2/2, step 3346/7134 completed (loss: 0.022461596876382828, acc: 0.9912280440330505)
[2025-02-13 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27][root][INFO] - Training Epoch: 2/2, step 3347/7134 completed (loss: 0.05061719939112663, acc: 0.9853420257568359)
[2025-02-13 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:28][root][INFO] - Training Epoch: 2/2, step 3348/7134 completed (loss: 0.030397798866033554, acc: 0.9911110997200012)
[2025-02-13 04:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:28][root][INFO] - Training Epoch: 2/2, step 3349/7134 completed (loss: 0.02699378691613674, acc: 0.9947090148925781)
[2025-02-13 04:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29][root][INFO] - Training Epoch: 2/2, step 3350/7134 completed (loss: 0.020947417244315147, acc: 0.9939024448394775)
[2025-02-13 04:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29][root][INFO] - Training Epoch: 2/2, step 3351/7134 completed (loss: 0.055221278220415115, acc: 0.9893898963928223)
[2025-02-13 04:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29][root][INFO] - Training Epoch: 2/2, step 3352/7134 completed (loss: 0.014016690663993359, acc: 0.9965217113494873)
[2025-02-13 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:30][root][INFO] - Training Epoch: 2/2, step 3353/7134 completed (loss: 0.03266197070479393, acc: 0.9881656765937805)
[2025-02-13 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:30][root][INFO] - Training Epoch: 2/2, step 3354/7134 completed (loss: 0.014522811397910118, acc: 0.9953488111495972)
[2025-02-13 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:31][root][INFO] - Training Epoch: 2/2, step 3355/7134 completed (loss: 0.014299619942903519, acc: 0.995768666267395)
[2025-02-13 04:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:31][root][INFO] - Training Epoch: 2/2, step 3356/7134 completed (loss: 0.007395812310278416, acc: 0.9982638955116272)
[2025-02-13 04:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:31][root][INFO] - Training Epoch: 2/2, step 3357/7134 completed (loss: 0.02677944488823414, acc: 0.9898989796638489)
[2025-02-13 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32][root][INFO] - Training Epoch: 2/2, step 3358/7134 completed (loss: 0.03141271695494652, acc: 0.9896193742752075)
[2025-02-13 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32][root][INFO] - Training Epoch: 2/2, step 3359/7134 completed (loss: 0.014353801496326923, acc: 0.9967637658119202)
[2025-02-13 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:33][root][INFO] - Training Epoch: 2/2, step 3360/7134 completed (loss: 0.06950481235980988, acc: 0.9828326106071472)
[2025-02-13 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:33][root][INFO] - Training Epoch: 2/2, step 3361/7134 completed (loss: 0.040818892419338226, acc: 0.9843400716781616)
[2025-02-13 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:33][root][INFO] - Training Epoch: 2/2, step 3362/7134 completed (loss: 0.03866233676671982, acc: 0.9886621236801147)
[2025-02-13 04:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:34][root][INFO] - Training Epoch: 2/2, step 3363/7134 completed (loss: 0.00872640497982502, acc: 0.9976525902748108)
[2025-02-13 04:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:34][root][INFO] - Training Epoch: 2/2, step 3364/7134 completed (loss: 0.06777678430080414, acc: 0.9846389889717102)
[2025-02-13 04:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35][root][INFO] - Training Epoch: 2/2, step 3365/7134 completed (loss: 0.041911352425813675, acc: 0.9855421781539917)
[2025-02-13 04:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35][root][INFO] - Training Epoch: 2/2, step 3366/7134 completed (loss: 0.02330722287297249, acc: 0.9910314083099365)
[2025-02-13 04:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35][root][INFO] - Training Epoch: 2/2, step 3367/7134 completed (loss: 0.02539042755961418, acc: 0.9931129217147827)
[2025-02-13 04:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:36][root][INFO] - Training Epoch: 2/2, step 3368/7134 completed (loss: 0.028692923486232758, acc: 0.9937597513198853)
[2025-02-13 04:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:36][root][INFO] - Training Epoch: 2/2, step 3369/7134 completed (loss: 0.02739615924656391, acc: 0.9942445755004883)
[2025-02-13 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:37][root][INFO] - Training Epoch: 2/2, step 3370/7134 completed (loss: 0.009743939153850079, acc: 0.9985693693161011)
[2025-02-13 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:37][root][INFO] - Training Epoch: 2/2, step 3371/7134 completed (loss: 0.005367065779864788, acc: 1.0)
[2025-02-13 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:38][root][INFO] - Training Epoch: 2/2, step 3372/7134 completed (loss: 0.018933046609163284, acc: 0.9968652129173279)
[2025-02-13 04:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:38][root][INFO] - Training Epoch: 2/2, step 3373/7134 completed (loss: 0.008482064120471478, acc: 0.9969135522842407)
[2025-02-13 04:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39][root][INFO] - Training Epoch: 2/2, step 3374/7134 completed (loss: 0.015460306778550148, acc: 0.9944289922714233)
[2025-02-13 04:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39][root][INFO] - Training Epoch: 2/2, step 3375/7134 completed (loss: 0.02101541869342327, acc: 0.9971056580543518)
[2025-02-13 04:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39][root][INFO] - Training Epoch: 2/2, step 3376/7134 completed (loss: 0.014985572546720505, acc: 0.9961685538291931)
[2025-02-13 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:40][root][INFO] - Training Epoch: 2/2, step 3377/7134 completed (loss: 0.012004644609987736, acc: 0.9952324032783508)
[2025-02-13 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:40][root][INFO] - Training Epoch: 2/2, step 3378/7134 completed (loss: 0.008692905306816101, acc: 0.99609375)
[2025-02-13 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:41][root][INFO] - Training Epoch: 2/2, step 3379/7134 completed (loss: 0.013220698572695255, acc: 0.9969924688339233)
[2025-02-13 04:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:41][root][INFO] - Training Epoch: 2/2, step 3380/7134 completed (loss: 0.013077267445623875, acc: 0.9969230890274048)
[2025-02-13 04:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42][root][INFO] - Training Epoch: 2/2, step 3381/7134 completed (loss: 0.020940588787198067, acc: 0.9896640777587891)
[2025-02-13 04:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42][root][INFO] - Training Epoch: 2/2, step 3382/7134 completed (loss: 0.003275388153269887, acc: 1.0)
[2025-02-13 04:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42][root][INFO] - Training Epoch: 2/2, step 3383/7134 completed (loss: 0.013363214209675789, acc: 0.9971305727958679)
[2025-02-13 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:43][root][INFO] - Training Epoch: 2/2, step 3384/7134 completed (loss: 0.024691827595233917, acc: 0.9938837885856628)
[2025-02-13 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:43][root][INFO] - Training Epoch: 2/2, step 3385/7134 completed (loss: 0.024193916469812393, acc: 0.9928057789802551)
[2025-02-13 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:44][root][INFO] - Training Epoch: 2/2, step 3386/7134 completed (loss: 0.01162649691104889, acc: 0.9957627058029175)
[2025-02-13 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:44][root][INFO] - Training Epoch: 2/2, step 3387/7134 completed (loss: 0.040169745683670044, acc: 0.994557797908783)
[2025-02-13 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45][root][INFO] - Training Epoch: 2/2, step 3388/7134 completed (loss: 0.03873840346932411, acc: 0.9873417615890503)
[2025-02-13 04:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45][root][INFO] - Training Epoch: 2/2, step 3389/7134 completed (loss: 0.011933340691030025, acc: 0.9973649382591248)
[2025-02-13 04:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45][root][INFO] - Training Epoch: 2/2, step 3390/7134 completed (loss: 0.015937836840748787, acc: 0.9935232996940613)
[2025-02-13 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46][root][INFO] - Training Epoch: 2/2, step 3391/7134 completed (loss: 0.0033633823040872812, acc: 1.0)
[2025-02-13 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46][root][INFO] - Training Epoch: 2/2, step 3392/7134 completed (loss: 0.0067509980872273445, acc: 0.99842768907547)
[2025-02-13 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:47][root][INFO] - Training Epoch: 2/2, step 3393/7134 completed (loss: 0.01656511425971985, acc: 0.9953703880310059)
[2025-02-13 04:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:47][root][INFO] - Training Epoch: 2/2, step 3394/7134 completed (loss: 0.020679062232375145, acc: 0.9952531456947327)
[2025-02-13 04:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48][root][INFO] - Training Epoch: 2/2, step 3395/7134 completed (loss: 0.0578816756606102, acc: 0.9821428656578064)
[2025-02-13 04:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48][root][INFO] - Training Epoch: 2/2, step 3396/7134 completed (loss: 0.026209775358438492, acc: 0.9928143620491028)
[2025-02-13 04:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48][root][INFO] - Training Epoch: 2/2, step 3397/7134 completed (loss: 0.01622258685529232, acc: 0.9987775087356567)
[2025-02-13 04:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:49][root][INFO] - Training Epoch: 2/2, step 3398/7134 completed (loss: 0.017629053443670273, acc: 0.9946236610412598)
[2025-02-13 04:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:49][root][INFO] - Training Epoch: 2/2, step 3399/7134 completed (loss: 0.05000457912683487, acc: 0.9871323704719543)
[2025-02-13 04:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:50][root][INFO] - Training Epoch: 2/2, step 3400/7134 completed (loss: 0.02306944876909256, acc: 0.9959431886672974)
[2025-02-13 04:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:50][root][INFO] - Training Epoch: 2/2, step 3401/7134 completed (loss: 0.023014521226286888, acc: 0.9947552680969238)
[2025-02-13 04:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:51][root][INFO] - Training Epoch: 2/2, step 3402/7134 completed (loss: 0.04571632668375969, acc: 0.9868247509002686)
[2025-02-13 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:51][root][INFO] - Training Epoch: 2/2, step 3403/7134 completed (loss: 0.06671147793531418, acc: 0.9830729365348816)
[2025-02-13 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52][root][INFO] - Training Epoch: 2/2, step 3404/7134 completed (loss: 0.01769399270415306, acc: 0.9955157041549683)
[2025-02-13 04:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52][root][INFO] - Training Epoch: 2/2, step 3405/7134 completed (loss: 0.05938901752233505, acc: 0.9928057789802551)
[2025-02-13 04:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52][root][INFO] - Training Epoch: 2/2, step 3406/7134 completed (loss: 0.030555253848433495, acc: 0.9909793734550476)
[2025-02-13 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:53][root][INFO] - Training Epoch: 2/2, step 3407/7134 completed (loss: 0.026904383674263954, acc: 0.9906396269798279)
[2025-02-13 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:53][root][INFO] - Training Epoch: 2/2, step 3408/7134 completed (loss: 0.031941551715135574, acc: 0.9884792566299438)
[2025-02-13 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54][root][INFO] - Training Epoch: 2/2, step 3409/7134 completed (loss: 0.028271885588765144, acc: 0.9918808937072754)
[2025-02-13 04:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54][root][INFO] - Training Epoch: 2/2, step 3410/7134 completed (loss: 0.02043570950627327, acc: 0.997245192527771)
[2025-02-13 04:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55][root][INFO] - Training Epoch: 2/2, step 3411/7134 completed (loss: 0.02647891454398632, acc: 0.9935317039489746)
[2025-02-13 04:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55][root][INFO] - Training Epoch: 2/2, step 3412/7134 completed (loss: 0.024831373244524002, acc: 0.992438554763794)
[2025-02-13 04:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55][root][INFO] - Training Epoch: 2/2, step 3413/7134 completed (loss: 0.02099793590605259, acc: 0.9940000176429749)
[2025-02-13 04:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56][root][INFO] - Training Epoch: 2/2, step 3414/7134 completed (loss: 0.03507447987794876, acc: 0.9866443872451782)
[2025-02-13 04:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56][root][INFO] - Training Epoch: 2/2, step 3415/7134 completed (loss: 0.008872251026332378, acc: 0.9983974099159241)
[2025-02-13 04:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:57][root][INFO] - Training Epoch: 2/2, step 3416/7134 completed (loss: 0.029362445697188377, acc: 0.989195704460144)
[2025-02-13 04:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:57][root][INFO] - Training Epoch: 2/2, step 3417/7134 completed (loss: 0.018888873979449272, acc: 0.9932523369789124)
[2025-02-13 04:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:58][root][INFO] - Training Epoch: 2/2, step 3418/7134 completed (loss: 0.016129223629832268, acc: 0.9955817461013794)
[2025-02-13 04:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:58][root][INFO] - Training Epoch: 2/2, step 3419/7134 completed (loss: 0.012876748107373714, acc: 0.9971181750297546)
[2025-02-13 04:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:58][root][INFO] - Training Epoch: 2/2, step 3420/7134 completed (loss: 0.008093680255115032, acc: 0.998420238494873)
[2025-02-13 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59][root][INFO] - Training Epoch: 2/2, step 3421/7134 completed (loss: 0.017477786168456078, acc: 0.9984447956085205)
[2025-02-13 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59][root][INFO] - Training Epoch: 2/2, step 3422/7134 completed (loss: 0.00712082302197814, acc: 0.996497392654419)
[2025-02-13 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:00][root][INFO] - Training Epoch: 2/2, step 3423/7134 completed (loss: 0.014688611961901188, acc: 0.9939393997192383)
[2025-02-13 04:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:00][root][INFO] - Training Epoch: 2/2, step 3424/7134 completed (loss: 0.04311364144086838, acc: 0.9849397540092468)
[2025-02-13 04:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:01][root][INFO] - Training Epoch: 2/2, step 3425/7134 completed (loss: 0.017152870073914528, acc: 0.9948717951774597)
[2025-02-13 04:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:01][root][INFO] - Training Epoch: 2/2, step 3426/7134 completed (loss: 0.02630537562072277, acc: 0.9972413778305054)
[2025-02-13 04:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:01][root][INFO] - Training Epoch: 2/2, step 3427/7134 completed (loss: 0.027034491300582886, acc: 0.9952606558799744)
[2025-02-13 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:02][root][INFO] - Training Epoch: 2/2, step 3428/7134 completed (loss: 0.019827494397759438, acc: 0.9942693114280701)
[2025-02-13 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:02][root][INFO] - Training Epoch: 2/2, step 3429/7134 completed (loss: 0.031402889639139175, acc: 0.9870550036430359)
[2025-02-13 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03][root][INFO] - Training Epoch: 2/2, step 3430/7134 completed (loss: 0.06074955314397812, acc: 0.9886547923088074)
[2025-02-13 04:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03][root][INFO] - Training Epoch: 2/2, step 3431/7134 completed (loss: 0.021410765126347542, acc: 0.9966044425964355)
[2025-02-13 04:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04][root][INFO] - Training Epoch: 2/2, step 3432/7134 completed (loss: 0.02309933863580227, acc: 0.9958847761154175)
[2025-02-13 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04][root][INFO] - Training Epoch: 2/2, step 3433/7134 completed (loss: 0.04908549413084984, acc: 0.983660101890564)
[2025-02-13 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04][root][INFO] - Training Epoch: 2/2, step 3434/7134 completed (loss: 0.021800365298986435, acc: 0.9936407208442688)
[2025-02-13 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05][root][INFO] - Training Epoch: 2/2, step 3435/7134 completed (loss: 0.04391026869416237, acc: 0.9839228391647339)
[2025-02-13 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05][root][INFO] - Training Epoch: 2/2, step 3436/7134 completed (loss: 0.02692408673465252, acc: 0.9886178970336914)
[2025-02-13 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:06][root][INFO] - Training Epoch: 2/2, step 3437/7134 completed (loss: 0.014578617177903652, acc: 0.994140625)
[2025-02-13 04:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:06][root][INFO] - Training Epoch: 2/2, step 3438/7134 completed (loss: 0.03814854100346565, acc: 0.9898989796638489)
[2025-02-13 04:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07][root][INFO] - Training Epoch: 2/2, step 3439/7134 completed (loss: 0.029181798920035362, acc: 0.9909583926200867)
[2025-02-13 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07][root][INFO] - Training Epoch: 2/2, step 3440/7134 completed (loss: 0.03177303448319435, acc: 0.9835391044616699)
[2025-02-13 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07][root][INFO] - Training Epoch: 2/2, step 3441/7134 completed (loss: 0.039681326597929, acc: 0.9855491518974304)
[2025-02-13 04:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:08][root][INFO] - Training Epoch: 2/2, step 3442/7134 completed (loss: 0.015173379331827164, acc: 0.9945255517959595)
[2025-02-13 04:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:08][root][INFO] - Training Epoch: 2/2, step 3443/7134 completed (loss: 0.051264528185129166, acc: 0.983146071434021)
[2025-02-13 04:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09][root][INFO] - Training Epoch: 2/2, step 3444/7134 completed (loss: 0.06636805087327957, acc: 0.9873417615890503)
[2025-02-13 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09][root][INFO] - Training Epoch: 2/2, step 3445/7134 completed (loss: 0.0868556872010231, acc: 0.9784946441650391)
[2025-02-13 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09][root][INFO] - Training Epoch: 2/2, step 3446/7134 completed (loss: 0.05472487583756447, acc: 0.9882943034172058)
[2025-02-13 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10][root][INFO] - Training Epoch: 2/2, step 3447/7134 completed (loss: 0.04353994131088257, acc: 0.9869706630706787)
[2025-02-13 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10][root][INFO] - Training Epoch: 2/2, step 3448/7134 completed (loss: 0.017304152250289917, acc: 0.9909909963607788)
[2025-02-13 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:11][root][INFO] - Training Epoch: 2/2, step 3449/7134 completed (loss: 0.039289142936468124, acc: 0.9948979616165161)
[2025-02-13 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:11][root][INFO] - Training Epoch: 2/2, step 3450/7134 completed (loss: 0.028806336224079132, acc: 0.9924050569534302)
[2025-02-13 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12][root][INFO] - Training Epoch: 2/2, step 3451/7134 completed (loss: 0.023028036579489708, acc: 0.9911660552024841)
[2025-02-13 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12][root][INFO] - Training Epoch: 2/2, step 3452/7134 completed (loss: 0.02469748817384243, acc: 0.9934924244880676)
[2025-02-13 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12][root][INFO] - Training Epoch: 2/2, step 3453/7134 completed (loss: 0.04932628199458122, acc: 0.9898132681846619)
[2025-02-13 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13][root][INFO] - Training Epoch: 2/2, step 3454/7134 completed (loss: 0.014924275688827038, acc: 0.994358241558075)
[2025-02-13 04:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13][root][INFO] - Training Epoch: 2/2, step 3455/7134 completed (loss: 0.0060158888809382915, acc: 1.0)
[2025-02-13 04:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:14][root][INFO] - Training Epoch: 2/2, step 3456/7134 completed (loss: 0.0267652440816164, acc: 0.9941176176071167)
[2025-02-13 04:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:14][root][INFO] - Training Epoch: 2/2, step 3457/7134 completed (loss: 0.007030576001852751, acc: 0.998305082321167)
[2025-02-13 04:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15][root][INFO] - Training Epoch: 2/2, step 3458/7134 completed (loss: 0.04123174771666527, acc: 0.9891745448112488)
[2025-02-13 04:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15][root][INFO] - Training Epoch: 2/2, step 3459/7134 completed (loss: 0.031282588839530945, acc: 0.9923896789550781)
[2025-02-13 04:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15][root][INFO] - Training Epoch: 2/2, step 3460/7134 completed (loss: 0.10068460553884506, acc: 0.9743589758872986)
[2025-02-13 04:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:16][root][INFO] - Training Epoch: 2/2, step 3461/7134 completed (loss: 0.16765087842941284, acc: 0.9609195590019226)
[2025-02-13 04:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:16][root][INFO] - Training Epoch: 2/2, step 3462/7134 completed (loss: 0.08443184942007065, acc: 0.9780219793319702)
[2025-02-13 04:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17][root][INFO] - Training Epoch: 2/2, step 3463/7134 completed (loss: 0.01418334525078535, acc: 0.993966817855835)
[2025-02-13 04:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17][root][INFO] - Training Epoch: 2/2, step 3464/7134 completed (loss: 0.038129013031721115, acc: 0.9865951538085938)
[2025-02-13 04:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17][root][INFO] - Training Epoch: 2/2, step 3465/7134 completed (loss: 0.05530709773302078, acc: 0.9837067127227783)
[2025-02-13 04:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:18][root][INFO] - Training Epoch: 2/2, step 3466/7134 completed (loss: 0.015999212861061096, acc: 0.9936608672142029)
[2025-02-13 04:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:18][root][INFO] - Training Epoch: 2/2, step 3467/7134 completed (loss: 0.02353132702410221, acc: 0.9893842935562134)
[2025-02-13 04:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19][root][INFO] - Training Epoch: 2/2, step 3468/7134 completed (loss: 0.009605051949620247, acc: 0.9971056580543518)
[2025-02-13 04:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19][root][INFO] - Training Epoch: 2/2, step 3469/7134 completed (loss: 0.02797514945268631, acc: 0.99245285987854)
[2025-02-13 04:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:20][root][INFO] - Training Epoch: 2/2, step 3470/7134 completed (loss: 0.01293574832379818, acc: 0.9953917264938354)
[2025-02-13 04:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:20][root][INFO] - Training Epoch: 2/2, step 3471/7134 completed (loss: 0.04898032918572426, acc: 0.9917126893997192)
[2025-02-13 04:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:20][root][INFO] - Training Epoch: 2/2, step 3472/7134 completed (loss: 0.012728254310786724, acc: 0.995529055595398)
[2025-02-13 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21][root][INFO] - Training Epoch: 2/2, step 3473/7134 completed (loss: 0.01707995869219303, acc: 0.9933775067329407)
[2025-02-13 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21][root][INFO] - Training Epoch: 2/2, step 3474/7134 completed (loss: 0.041299913078546524, acc: 0.9929328560829163)
[2025-02-13 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:22][root][INFO] - Training Epoch: 2/2, step 3475/7134 completed (loss: 0.02292252890765667, acc: 0.9946714043617249)
[2025-02-13 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:22][root][INFO] - Training Epoch: 2/2, step 3476/7134 completed (loss: 0.03193726763129234, acc: 0.9811643958091736)
[2025-02-13 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23][root][INFO] - Training Epoch: 2/2, step 3477/7134 completed (loss: 0.0355934239923954, acc: 0.9901315569877625)
[2025-02-13 04:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23][root][INFO] - Training Epoch: 2/2, step 3478/7134 completed (loss: 0.01500989031046629, acc: 0.9940119981765747)
[2025-02-13 04:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23][root][INFO] - Training Epoch: 2/2, step 3479/7134 completed (loss: 0.0023317893501371145, acc: 1.0)
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24][root][INFO] - Training Epoch: 2/2, step 3480/7134 completed (loss: 0.017367718741297722, acc: 0.9959127902984619)
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24][root][INFO] - Training Epoch: 2/2, step 3481/7134 completed (loss: 0.030251510441303253, acc: 0.9964538812637329)
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:25][root][INFO] - Training Epoch: 2/2, step 3482/7134 completed (loss: 0.008244666270911694, acc: 0.9964028596878052)
[2025-02-13 04:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:25][root][INFO] - Training Epoch: 2/2, step 3483/7134 completed (loss: 0.0028510489501059055, acc: 1.0)
[2025-02-13 04:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26][root][INFO] - Training Epoch: 2/2, step 3484/7134 completed (loss: 0.018804479390382767, acc: 0.9955752491950989)
[2025-02-13 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26][root][INFO] - Training Epoch: 2/2, step 3485/7134 completed (loss: 0.01330298651009798, acc: 0.9956709742546082)
[2025-02-13 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26][root][INFO] - Training Epoch: 2/2, step 3486/7134 completed (loss: 0.00879185926169157, acc: 0.9982014298439026)
[2025-02-13 04:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:27][root][INFO] - Training Epoch: 2/2, step 3487/7134 completed (loss: 0.027357526123523712, acc: 0.9896907210350037)
[2025-02-13 04:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:27][root][INFO] - Training Epoch: 2/2, step 3488/7134 completed (loss: 0.010289200581610203, acc: 0.9967105388641357)
[2025-02-13 04:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28][root][INFO] - Training Epoch: 2/2, step 3489/7134 completed (loss: 0.049935970455408096, acc: 0.9852670431137085)
[2025-02-13 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28][root][INFO] - Training Epoch: 2/2, step 3490/7134 completed (loss: 0.054366666823625565, acc: 0.9911373853683472)
[2025-02-13 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:29][root][INFO] - Training Epoch: 2/2, step 3491/7134 completed (loss: 0.03331790864467621, acc: 0.9943262338638306)
[2025-02-13 04:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:29][root][INFO] - Training Epoch: 2/2, step 3492/7134 completed (loss: 0.018568508327007294, acc: 0.9917864203453064)
[2025-02-13 04:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:29][root][INFO] - Training Epoch: 2/2, step 3493/7134 completed (loss: 0.01154217030853033, acc: 0.994397759437561)
[2025-02-13 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30][root][INFO] - Training Epoch: 2/2, step 3494/7134 completed (loss: 0.02346421591937542, acc: 0.9948275685310364)
[2025-02-13 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30][root][INFO] - Training Epoch: 2/2, step 3495/7134 completed (loss: 0.008548571728169918, acc: 0.9976019263267517)
[2025-02-13 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:31][root][INFO] - Training Epoch: 2/2, step 3496/7134 completed (loss: 0.013909466564655304, acc: 0.9954954981803894)
[2025-02-13 04:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:31][root][INFO] - Training Epoch: 2/2, step 3497/7134 completed (loss: 0.02083749510347843, acc: 0.9955489635467529)
[2025-02-13 04:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32][root][INFO] - Training Epoch: 2/2, step 3498/7134 completed (loss: 0.030036253854632378, acc: 0.9893292784690857)
[2025-02-13 04:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32][root][INFO] - Training Epoch: 2/2, step 3499/7134 completed (loss: 0.007120071444660425, acc: 1.0)
[2025-02-13 04:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33][root][INFO] - Training Epoch: 2/2, step 3500/7134 completed (loss: 0.0283433236181736, acc: 0.9867841601371765)
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33][root][INFO] - Training Epoch: 2/2, step 3501/7134 completed (loss: 0.0740208551287651, acc: 0.9778671860694885)
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33][root][INFO] - Training Epoch: 2/2, step 3502/7134 completed (loss: 0.07182420045137405, acc: 0.9789473414421082)
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:34][root][INFO] - Training Epoch: 2/2, step 3503/7134 completed (loss: 0.03365122899413109, acc: 0.9948006868362427)
[2025-02-13 04:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:34][root][INFO] - Training Epoch: 2/2, step 3504/7134 completed (loss: 0.009550631046295166, acc: 0.9984917044639587)
[2025-02-13 04:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35][root][INFO] - Training Epoch: 2/2, step 3505/7134 completed (loss: 0.017405124381184578, acc: 0.9946332573890686)
[2025-02-13 04:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35][root][INFO] - Training Epoch: 2/2, step 3506/7134 completed (loss: 0.02155899815261364, acc: 0.992438554763794)
[2025-02-13 04:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35][root][INFO] - Training Epoch: 2/2, step 3507/7134 completed (loss: 0.005894162226468325, acc: 0.996688723564148)
[2025-02-13 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:36][root][INFO] - Training Epoch: 2/2, step 3508/7134 completed (loss: 0.024854963645339012, acc: 0.9941520690917969)
[2025-02-13 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:36][root][INFO] - Training Epoch: 2/2, step 3509/7134 completed (loss: 0.010433920659124851, acc: 0.9958563446998596)
[2025-02-13 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:37][root][INFO] - Training Epoch: 2/2, step 3510/7134 completed (loss: 0.01999870501458645, acc: 0.994854211807251)
[2025-02-13 04:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:37][root][INFO] - Training Epoch: 2/2, step 3511/7134 completed (loss: 0.003892637323588133, acc: 1.0)
[2025-02-13 04:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38][root][INFO] - Training Epoch: 2/2, step 3512/7134 completed (loss: 0.023398887366056442, acc: 0.9904912710189819)
[2025-02-13 04:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38][root][INFO] - Training Epoch: 2/2, step 3513/7134 completed (loss: 0.021699680015444756, acc: 0.9946902394294739)
[2025-02-13 04:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38][root][INFO] - Training Epoch: 2/2, step 3514/7134 completed (loss: 0.01755271479487419, acc: 0.9976958632469177)
[2025-02-13 04:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:39][root][INFO] - Training Epoch: 2/2, step 3515/7134 completed (loss: 0.0024738709907978773, acc: 1.0)
[2025-02-13 04:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:39][root][INFO] - Training Epoch: 2/2, step 3516/7134 completed (loss: 0.004292652476578951, acc: 0.9978331327438354)
[2025-02-13 04:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:40][root][INFO] - Training Epoch: 2/2, step 3517/7134 completed (loss: 0.008191530592739582, acc: 0.9976162314414978)
[2025-02-13 04:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:40][root][INFO] - Training Epoch: 2/2, step 3518/7134 completed (loss: 0.009535541757941246, acc: 0.9944547414779663)
[2025-02-13 04:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41][root][INFO] - Training Epoch: 2/2, step 3519/7134 completed (loss: 0.009900332428514957, acc: 0.9977527856826782)
[2025-02-13 04:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41][root][INFO] - Training Epoch: 2/2, step 3520/7134 completed (loss: 0.037227921187877655, acc: 0.9929412007331848)
[2025-02-13 04:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:42][root][INFO] - Training Epoch: 2/2, step 3521/7134 completed (loss: 0.008500502444803715, acc: 0.9957325458526611)
[2025-02-13 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:42][root][INFO] - Training Epoch: 2/2, step 3522/7134 completed (loss: 0.0025782829616218805, acc: 1.0)
[2025-02-13 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43][root][INFO] - Training Epoch: 2/2, step 3523/7134 completed (loss: 0.0021309952717274427, acc: 1.0)
[2025-02-13 04:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43][root][INFO] - Training Epoch: 2/2, step 3524/7134 completed (loss: 0.019251422956585884, acc: 0.99609375)
[2025-02-13 04:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43][root][INFO] - Training Epoch: 2/2, step 3525/7134 completed (loss: 0.011650050058960915, acc: 0.9967051148414612)
[2025-02-13 04:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44][root][INFO] - Training Epoch: 2/2, step 3526/7134 completed (loss: 0.03141354024410248, acc: 0.9885931611061096)
[2025-02-13 04:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44][root][INFO] - Training Epoch: 2/2, step 3527/7134 completed (loss: 0.028312228620052338, acc: 0.9935979247093201)
[2025-02-13 04:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:45][root][INFO] - Training Epoch: 2/2, step 3528/7134 completed (loss: 0.0058797323144972324, acc: 0.9976931810379028)
[2025-02-13 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:45][root][INFO] - Training Epoch: 2/2, step 3529/7134 completed (loss: 0.004121178295463324, acc: 1.0)
[2025-02-13 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:46][root][INFO] - Training Epoch: 2/2, step 3530/7134 completed (loss: 0.01247572060674429, acc: 0.9931192398071289)
[2025-02-13 04:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:46][root][INFO] - Training Epoch: 2/2, step 3531/7134 completed (loss: 0.011940154246985912, acc: 0.9969879388809204)
[2025-02-13 04:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47][root][INFO] - Training Epoch: 2/2, step 3532/7134 completed (loss: 0.006387624423950911, acc: 0.9983165264129639)
[2025-02-13 04:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47][root][INFO] - Training Epoch: 2/2, step 3533/7134 completed (loss: 0.011410822160542011, acc: 0.9976717233657837)
[2025-02-13 04:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47][root][INFO] - Training Epoch: 2/2, step 3534/7134 completed (loss: 0.029487017542123795, acc: 0.9902642369270325)
[2025-02-13 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:48][root][INFO] - Training Epoch: 2/2, step 3535/7134 completed (loss: 0.02573312446475029, acc: 0.9907833933830261)
[2025-02-13 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:48][root][INFO] - Training Epoch: 2/2, step 3536/7134 completed (loss: 0.0348413810133934, acc: 0.9890710115432739)
[2025-02-13 04:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:49][root][INFO] - Training Epoch: 2/2, step 3537/7134 completed (loss: 0.02647797018289566, acc: 0.994966447353363)
[2025-02-13 04:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:49][root][INFO] - Training Epoch: 2/2, step 3538/7134 completed (loss: 0.027868110686540604, acc: 0.9942965507507324)
[2025-02-13 04:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50][root][INFO] - Training Epoch: 2/2, step 3539/7134 completed (loss: 0.003894146764650941, acc: 1.0)
[2025-02-13 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50][root][INFO] - Training Epoch: 2/2, step 3540/7134 completed (loss: 0.03306161239743233, acc: 0.9908952713012695)
[2025-02-13 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50][root][INFO] - Training Epoch: 2/2, step 3541/7134 completed (loss: 0.029264556244015694, acc: 0.988252580165863)
[2025-02-13 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:51][root][INFO] - Training Epoch: 2/2, step 3542/7134 completed (loss: 0.009343951009213924, acc: 0.9984615445137024)
[2025-02-13 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:51][root][INFO] - Training Epoch: 2/2, step 3543/7134 completed (loss: 0.019607597962021828, acc: 0.9929577708244324)
[2025-02-13 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:52][root][INFO] - Training Epoch: 2/2, step 3544/7134 completed (loss: 0.018356725573539734, acc: 0.9923076629638672)
[2025-02-13 04:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:52][root][INFO] - Training Epoch: 2/2, step 3545/7134 completed (loss: 0.012861724011600018, acc: 0.9953415989875793)
[2025-02-13 04:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53][root][INFO] - Training Epoch: 2/2, step 3546/7134 completed (loss: 0.017162077128887177, acc: 0.9933993220329285)
[2025-02-13 04:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53][root][INFO] - Training Epoch: 2/2, step 3547/7134 completed (loss: 0.023856686428189278, acc: 0.9902098178863525)
[2025-02-13 04:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53][root][INFO] - Training Epoch: 2/2, step 3548/7134 completed (loss: 0.013117553666234016, acc: 0.9933628439903259)
[2025-02-13 04:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:54][root][INFO] - Training Epoch: 2/2, step 3549/7134 completed (loss: 0.027470877394080162, acc: 0.9876543283462524)
[2025-02-13 04:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:54][root][INFO] - Training Epoch: 2/2, step 3550/7134 completed (loss: 0.005482320208102465, acc: 0.996497392654419)
[2025-02-13 04:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:55][root][INFO] - Training Epoch: 2/2, step 3551/7134 completed (loss: 0.016664225608110428, acc: 0.9932659864425659)
[2025-02-13 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:55][root][INFO] - Training Epoch: 2/2, step 3552/7134 completed (loss: 0.07498099654912949, acc: 0.9812734127044678)
[2025-02-13 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:56][root][INFO] - Training Epoch: 2/2, step 3553/7134 completed (loss: 0.02895338274538517, acc: 0.9917762875556946)
[2025-02-13 04:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:56][root][INFO] - Training Epoch: 2/2, step 3554/7134 completed (loss: 0.045328397303819656, acc: 0.9894578456878662)
[2025-02-13 04:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:56][root][INFO] - Training Epoch: 2/2, step 3555/7134 completed (loss: 0.02448444254696369, acc: 0.9921507239341736)
[2025-02-13 04:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57][root][INFO] - Training Epoch: 2/2, step 3556/7134 completed (loss: 0.02734275348484516, acc: 0.9923224449157715)
[2025-02-13 04:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57][root][INFO] - Training Epoch: 2/2, step 3557/7134 completed (loss: 0.026102138683199883, acc: 0.9927404522895813)
[2025-02-13 04:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58][root][INFO] - Training Epoch: 2/2, step 3558/7134 completed (loss: 0.029228026047348976, acc: 0.991752564907074)
[2025-02-13 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58][root][INFO] - Training Epoch: 2/2, step 3559/7134 completed (loss: 0.00981669221073389, acc: 0.9970930218696594)
[2025-02-13 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58][root][INFO] - Training Epoch: 2/2, step 3560/7134 completed (loss: 0.012003600597381592, acc: 0.9945205450057983)
[2025-02-13 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:59][root][INFO] - Training Epoch: 2/2, step 3561/7134 completed (loss: 0.02243903838098049, acc: 0.9915966391563416)
[2025-02-13 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:59][root][INFO] - Training Epoch: 2/2, step 3562/7134 completed (loss: 0.03446212783455849, acc: 0.9892857074737549)
[2025-02-13 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:00][root][INFO] - Training Epoch: 2/2, step 3563/7134 completed (loss: 0.014830848202109337, acc: 0.995230495929718)
[2025-02-13 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0454, device='cuda:0') eval_epoch_loss=tensor(0.0444, device='cuda:0') eval_epoch_acc=tensor(0.9881, device='cuda:0')
[2025-02-13 04:10:23][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:10:23][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:10:23][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04442635551095009/model.pt
[2025-02-13 04:10:23][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:10:23][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.04442635551095009
[2025-02-13 04:10:23][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9880722761154175
[2025-02-13 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:24][root][INFO] - Training Epoch: 2/2, step 3564/7134 completed (loss: 0.039761826395988464, acc: 0.9943609237670898)
[2025-02-13 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:24][root][INFO] - Training Epoch: 2/2, step 3565/7134 completed (loss: 0.05601451173424721, acc: 0.9774436354637146)
[2025-02-13 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25][root][INFO] - Training Epoch: 2/2, step 3566/7134 completed (loss: 0.027546046301722527, acc: 0.9900398254394531)
[2025-02-13 04:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25][root][INFO] - Training Epoch: 2/2, step 3567/7134 completed (loss: 0.07062256336212158, acc: 0.9870634078979492)
[2025-02-13 04:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26][root][INFO] - Training Epoch: 2/2, step 3568/7134 completed (loss: 0.06796559691429138, acc: 0.9869067072868347)
[2025-02-13 04:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26][root][INFO] - Training Epoch: 2/2, step 3569/7134 completed (loss: 0.02375209890305996, acc: 0.9946236610412598)
[2025-02-13 04:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26][root][INFO] - Training Epoch: 2/2, step 3570/7134 completed (loss: 0.0642690509557724, acc: 0.9818181991577148)
[2025-02-13 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27][root][INFO] - Training Epoch: 2/2, step 3571/7134 completed (loss: 0.056890737265348434, acc: 0.988063633441925)
[2025-02-13 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27][root][INFO] - Training Epoch: 2/2, step 3572/7134 completed (loss: 0.013899413868784904, acc: 0.9973649382591248)
[2025-02-13 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:28][root][INFO] - Training Epoch: 2/2, step 3573/7134 completed (loss: 0.04788551479578018, acc: 0.9866666793823242)
[2025-02-13 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:28][root][INFO] - Training Epoch: 2/2, step 3574/7134 completed (loss: 0.0361761674284935, acc: 0.982758641242981)
[2025-02-13 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29][root][INFO] - Training Epoch: 2/2, step 3575/7134 completed (loss: 0.0785597711801529, acc: 0.9725557565689087)
[2025-02-13 04:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29][root][INFO] - Training Epoch: 2/2, step 3576/7134 completed (loss: 0.04988177865743637, acc: 0.9815837740898132)
[2025-02-13 04:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29][root][INFO] - Training Epoch: 2/2, step 3577/7134 completed (loss: 0.023213138803839684, acc: 0.993678867816925)
[2025-02-13 04:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:30][root][INFO] - Training Epoch: 2/2, step 3578/7134 completed (loss: 0.03553115949034691, acc: 0.9898107647895813)
[2025-02-13 04:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:30][root][INFO] - Training Epoch: 2/2, step 3579/7134 completed (loss: 0.03455227240920067, acc: 0.9879999756813049)
[2025-02-13 04:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31][root][INFO] - Training Epoch: 2/2, step 3580/7134 completed (loss: 0.044434644281864166, acc: 0.9842519760131836)
[2025-02-13 04:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31][root][INFO] - Training Epoch: 2/2, step 3581/7134 completed (loss: 0.0507875494658947, acc: 0.9867452383041382)
[2025-02-13 04:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32][root][INFO] - Training Epoch: 2/2, step 3582/7134 completed (loss: 0.0626513734459877, acc: 0.9834123253822327)
[2025-02-13 04:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32][root][INFO] - Training Epoch: 2/2, step 3583/7134 completed (loss: 0.05385247617959976, acc: 0.9832826852798462)
[2025-02-13 04:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32][root][INFO] - Training Epoch: 2/2, step 3584/7134 completed (loss: 0.05314812809228897, acc: 0.9894179701805115)
[2025-02-13 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33][root][INFO] - Training Epoch: 2/2, step 3585/7134 completed (loss: 0.039958976209163666, acc: 0.9861111044883728)
[2025-02-13 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33][root][INFO] - Training Epoch: 2/2, step 3586/7134 completed (loss: 0.04013293981552124, acc: 0.9871588945388794)
[2025-02-13 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:34][root][INFO] - Training Epoch: 2/2, step 3587/7134 completed (loss: 0.05045843869447708, acc: 0.9847715497016907)
[2025-02-13 04:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:34][root][INFO] - Training Epoch: 2/2, step 3588/7134 completed (loss: 0.054307665675878525, acc: 0.9822485446929932)
[2025-02-13 04:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35][root][INFO] - Training Epoch: 2/2, step 3589/7134 completed (loss: 0.06368688493967056, acc: 0.9894099831581116)
[2025-02-13 04:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35][root][INFO] - Training Epoch: 2/2, step 3590/7134 completed (loss: 0.012682890519499779, acc: 0.9957864880561829)
[2025-02-13 04:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35][root][INFO] - Training Epoch: 2/2, step 3591/7134 completed (loss: 0.020181776955723763, acc: 0.9954954981803894)
[2025-02-13 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:36][root][INFO] - Training Epoch: 2/2, step 3592/7134 completed (loss: 0.011506366543471813, acc: 0.9966942071914673)
[2025-02-13 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:36][root][INFO] - Training Epoch: 2/2, step 3593/7134 completed (loss: 0.030478421598672867, acc: 0.9929947257041931)
[2025-02-13 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:37][root][INFO] - Training Epoch: 2/2, step 3594/7134 completed (loss: 0.018988370895385742, acc: 0.9918830990791321)
[2025-02-13 04:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:37][root][INFO] - Training Epoch: 2/2, step 3595/7134 completed (loss: 0.02552650310099125, acc: 0.9892802238464355)
[2025-02-13 04:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38][root][INFO] - Training Epoch: 2/2, step 3596/7134 completed (loss: 0.02745133824646473, acc: 0.9917469024658203)
[2025-02-13 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38][root][INFO] - Training Epoch: 2/2, step 3597/7134 completed (loss: 0.021163469180464745, acc: 0.9956834316253662)
[2025-02-13 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:39][root][INFO] - Training Epoch: 2/2, step 3598/7134 completed (loss: 0.04313835874199867, acc: 0.9908466935157776)
[2025-02-13 04:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:39][root][INFO] - Training Epoch: 2/2, step 3599/7134 completed (loss: 0.0426667146384716, acc: 0.9911727905273438)
[2025-02-13 04:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40][root][INFO] - Training Epoch: 2/2, step 3600/7134 completed (loss: 0.04342944175004959, acc: 0.9876543283462524)
[2025-02-13 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40][root][INFO] - Training Epoch: 2/2, step 3601/7134 completed (loss: 0.08290072530508041, acc: 0.9765517115592957)
[2025-02-13 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40][root][INFO] - Training Epoch: 2/2, step 3602/7134 completed (loss: 0.06297798454761505, acc: 0.97782963514328)
[2025-02-13 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:41][root][INFO] - Training Epoch: 2/2, step 3603/7134 completed (loss: 0.01988551951944828, acc: 0.989062488079071)
[2025-02-13 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:41][root][INFO] - Training Epoch: 2/2, step 3604/7134 completed (loss: 0.02574775367975235, acc: 0.9849905967712402)
[2025-02-13 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:42][root][INFO] - Training Epoch: 2/2, step 3605/7134 completed (loss: 0.043591082096099854, acc: 0.9896480441093445)
[2025-02-13 04:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:42][root][INFO] - Training Epoch: 2/2, step 3606/7134 completed (loss: 0.04090593010187149, acc: 0.9911190271377563)
[2025-02-13 04:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43][root][INFO] - Training Epoch: 2/2, step 3607/7134 completed (loss: 0.024344181641936302, acc: 0.994966447353363)
[2025-02-13 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43][root][INFO] - Training Epoch: 2/2, step 3608/7134 completed (loss: 0.05707559362053871, acc: 0.9900442361831665)
[2025-02-13 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44][root][INFO] - Training Epoch: 2/2, step 3609/7134 completed (loss: 0.03703301399946213, acc: 0.989847719669342)
[2025-02-13 04:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44][root][INFO] - Training Epoch: 2/2, step 3610/7134 completed (loss: 0.05122765526175499, acc: 0.9818181991577148)
[2025-02-13 04:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44][root][INFO] - Training Epoch: 2/2, step 3611/7134 completed (loss: 0.02693823352456093, acc: 0.9905914068222046)
[2025-02-13 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45][root][INFO] - Training Epoch: 2/2, step 3612/7134 completed (loss: 0.056736625730991364, acc: 0.9813277721405029)
[2025-02-13 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45][root][INFO] - Training Epoch: 2/2, step 3613/7134 completed (loss: 0.03945012763142586, acc: 0.9847328066825867)
[2025-02-13 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:46][root][INFO] - Training Epoch: 2/2, step 3614/7134 completed (loss: 0.03292093798518181, acc: 0.9906291961669922)
[2025-02-13 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:46][root][INFO] - Training Epoch: 2/2, step 3615/7134 completed (loss: 0.031594086438417435, acc: 0.9954128265380859)
[2025-02-13 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:47][root][INFO] - Training Epoch: 2/2, step 3616/7134 completed (loss: 0.03814411908388138, acc: 0.9858712553977966)
[2025-02-13 04:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:47][root][INFO] - Training Epoch: 2/2, step 3617/7134 completed (loss: 0.021434372290968895, acc: 0.9938119053840637)
[2025-02-13 04:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48][root][INFO] - Training Epoch: 2/2, step 3618/7134 completed (loss: 0.05886166915297508, acc: 0.9803921580314636)
[2025-02-13 04:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48][root][INFO] - Training Epoch: 2/2, step 3619/7134 completed (loss: 0.06433791667222977, acc: 0.9785714149475098)
[2025-02-13 04:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:49][root][INFO] - Training Epoch: 2/2, step 3620/7134 completed (loss: 0.03484253212809563, acc: 0.9859353303909302)
[2025-02-13 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:49][root][INFO] - Training Epoch: 2/2, step 3621/7134 completed (loss: 0.019277095794677734, acc: 0.9916782379150391)
[2025-02-13 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:49][root][INFO] - Training Epoch: 2/2, step 3622/7134 completed (loss: 0.024728400632739067, acc: 0.9909909963607788)
[2025-02-13 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:50][root][INFO] - Training Epoch: 2/2, step 3623/7134 completed (loss: 0.009558986872434616, acc: 1.0)
[2025-02-13 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:50][root][INFO] - Training Epoch: 2/2, step 3624/7134 completed (loss: 0.016050374135375023, acc: 0.9950000047683716)
[2025-02-13 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51][root][INFO] - Training Epoch: 2/2, step 3625/7134 completed (loss: 0.040217407047748566, acc: 0.98591548204422)
[2025-02-13 04:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51][root][INFO] - Training Epoch: 2/2, step 3626/7134 completed (loss: 0.10602150112390518, acc: 0.9733333587646484)
[2025-02-13 04:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51][root][INFO] - Training Epoch: 2/2, step 3627/7134 completed (loss: 0.051364466547966, acc: 0.9858155846595764)
[2025-02-13 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:52][root][INFO] - Training Epoch: 2/2, step 3628/7134 completed (loss: 0.11095672100782394, acc: 0.9659090638160706)
[2025-02-13 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:52][root][INFO] - Training Epoch: 2/2, step 3629/7134 completed (loss: 0.05750318244099617, acc: 0.9817073345184326)
[2025-02-13 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53][root][INFO] - Training Epoch: 2/2, step 3630/7134 completed (loss: 0.029565246775746346, acc: 0.9870800971984863)
[2025-02-13 04:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53][root][INFO] - Training Epoch: 2/2, step 3631/7134 completed (loss: 0.05383327230811119, acc: 0.9836065769195557)
[2025-02-13 04:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53][root][INFO] - Training Epoch: 2/2, step 3632/7134 completed (loss: 0.03773225471377373, acc: 0.9879275560379028)
[2025-02-13 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:54][root][INFO] - Training Epoch: 2/2, step 3633/7134 completed (loss: 0.06729554384946823, acc: 0.9842180609703064)
[2025-02-13 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:54][root][INFO] - Training Epoch: 2/2, step 3634/7134 completed (loss: 0.06510060280561447, acc: 0.9878542423248291)
[2025-02-13 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55][root][INFO] - Training Epoch: 2/2, step 3635/7134 completed (loss: 0.009966782294213772, acc: 0.9964157938957214)
[2025-02-13 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55][root][INFO] - Training Epoch: 2/2, step 3636/7134 completed (loss: 0.007886577397584915, acc: 1.0)
[2025-02-13 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55][root][INFO] - Training Epoch: 2/2, step 3637/7134 completed (loss: 0.024923600256443024, acc: 0.9919517040252686)
[2025-02-13 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:56][root][INFO] - Training Epoch: 2/2, step 3638/7134 completed (loss: 0.009496184065937996, acc: 0.9952940940856934)
[2025-02-13 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:56][root][INFO] - Training Epoch: 2/2, step 3639/7134 completed (loss: 0.016367251053452492, acc: 0.9954954981803894)
[2025-02-13 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:57][root][INFO] - Training Epoch: 2/2, step 3640/7134 completed (loss: 0.0343136303126812, acc: 0.9942747950553894)
[2025-02-13 04:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:57][root][INFO] - Training Epoch: 2/2, step 3641/7134 completed (loss: 0.0246743131428957, acc: 0.9922178983688354)
[2025-02-13 04:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:57][root][INFO] - Training Epoch: 2/2, step 3642/7134 completed (loss: 0.012811504304409027, acc: 0.9969087839126587)
[2025-02-13 04:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58][root][INFO] - Training Epoch: 2/2, step 3643/7134 completed (loss: 0.029210306704044342, acc: 0.9914089441299438)
[2025-02-13 04:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58][root][INFO] - Training Epoch: 2/2, step 3644/7134 completed (loss: 0.05474100634455681, acc: 0.9869918823242188)
[2025-02-13 04:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:59][root][INFO] - Training Epoch: 2/2, step 3645/7134 completed (loss: 0.0354955717921257, acc: 0.9876760840415955)
[2025-02-13 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:59][root][INFO] - Training Epoch: 2/2, step 3646/7134 completed (loss: 0.08457933366298676, acc: 0.9809027910232544)
[2025-02-13 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:00][root][INFO] - Training Epoch: 2/2, step 3647/7134 completed (loss: 0.006197534501552582, acc: 1.0)
[2025-02-13 04:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:00][root][INFO] - Training Epoch: 2/2, step 3648/7134 completed (loss: 0.04023929312825203, acc: 0.9835391044616699)
[2025-02-13 04:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:00][root][INFO] - Training Epoch: 2/2, step 3649/7134 completed (loss: 0.04353535920381546, acc: 0.9887955188751221)
[2025-02-13 04:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01][root][INFO] - Training Epoch: 2/2, step 3650/7134 completed (loss: 0.037410687655210495, acc: 0.9895424842834473)
[2025-02-13 04:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01][root][INFO] - Training Epoch: 2/2, step 3651/7134 completed (loss: 0.055866360664367676, acc: 0.98525470495224)
[2025-02-13 04:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:02][root][INFO] - Training Epoch: 2/2, step 3652/7134 completed (loss: 0.04532749205827713, acc: 0.9849170446395874)
[2025-02-13 04:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:02][root][INFO] - Training Epoch: 2/2, step 3653/7134 completed (loss: 0.0702744647860527, acc: 0.9819004535675049)
[2025-02-13 04:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03][root][INFO] - Training Epoch: 2/2, step 3654/7134 completed (loss: 0.017066238448023796, acc: 0.9930915236473083)
[2025-02-13 04:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03][root][INFO] - Training Epoch: 2/2, step 3655/7134 completed (loss: 0.04051130264997482, acc: 0.9931318759918213)
[2025-02-13 04:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03][root][INFO] - Training Epoch: 2/2, step 3656/7134 completed (loss: 0.08656372129917145, acc: 0.9800000190734863)
[2025-02-13 04:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:04][root][INFO] - Training Epoch: 2/2, step 3657/7134 completed (loss: 0.01291793305426836, acc: 0.9967948794364929)
[2025-02-13 04:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:04][root][INFO] - Training Epoch: 2/2, step 3658/7134 completed (loss: 0.04297734797000885, acc: 0.9875389337539673)
[2025-02-13 04:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:05][root][INFO] - Training Epoch: 2/2, step 3659/7134 completed (loss: 0.027836134657263756, acc: 0.9910314083099365)
[2025-02-13 04:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:05][root][INFO] - Training Epoch: 2/2, step 3660/7134 completed (loss: 0.03644074127078056, acc: 0.9836065769195557)
[2025-02-13 04:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:05][root][INFO] - Training Epoch: 2/2, step 3661/7134 completed (loss: 0.04262637719511986, acc: 0.9821428656578064)
[2025-02-13 04:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:06][root][INFO] - Training Epoch: 2/2, step 3662/7134 completed (loss: 0.05408599227666855, acc: 0.9862805008888245)
[2025-02-13 04:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:06][root][INFO] - Training Epoch: 2/2, step 3663/7134 completed (loss: 0.03600519523024559, acc: 0.9926739931106567)
[2025-02-13 04:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07][root][INFO] - Training Epoch: 2/2, step 3664/7134 completed (loss: 0.04429661110043526, acc: 0.9908814430236816)
[2025-02-13 04:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07][root][INFO] - Training Epoch: 2/2, step 3665/7134 completed (loss: 0.028390029445290565, acc: 0.9917898178100586)
[2025-02-13 04:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:08][root][INFO] - Training Epoch: 2/2, step 3666/7134 completed (loss: 0.04433383047580719, acc: 0.985401451587677)
[2025-02-13 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:08][root][INFO] - Training Epoch: 2/2, step 3667/7134 completed (loss: 0.12155202776193619, acc: 0.965831458568573)
[2025-02-13 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:08][root][INFO] - Training Epoch: 2/2, step 3668/7134 completed (loss: 0.03778618201613426, acc: 0.9912739992141724)
[2025-02-13 04:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:09][root][INFO] - Training Epoch: 2/2, step 3669/7134 completed (loss: 0.008247476071119308, acc: 0.9981096386909485)
[2025-02-13 04:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:09][root][INFO] - Training Epoch: 2/2, step 3670/7134 completed (loss: 0.021727895364165306, acc: 0.9972527623176575)
[2025-02-13 04:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:10][root][INFO] - Training Epoch: 2/2, step 3671/7134 completed (loss: 0.019131788983941078, acc: 0.9927745461463928)
[2025-02-13 04:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:10][root][INFO] - Training Epoch: 2/2, step 3672/7134 completed (loss: 0.014895266853272915, acc: 0.997474730014801)
[2025-02-13 04:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11][root][INFO] - Training Epoch: 2/2, step 3673/7134 completed (loss: 0.03918725252151489, acc: 0.9849498271942139)
[2025-02-13 04:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11][root][INFO] - Training Epoch: 2/2, step 3674/7134 completed (loss: 0.015223681926727295, acc: 0.9968503713607788)
[2025-02-13 04:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11][root][INFO] - Training Epoch: 2/2, step 3675/7134 completed (loss: 0.015928354114294052, acc: 0.9948979616165161)
[2025-02-13 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:12][root][INFO] - Training Epoch: 2/2, step 3676/7134 completed (loss: 0.010514265857636929, acc: 0.9985422492027283)
[2025-02-13 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:12][root][INFO] - Training Epoch: 2/2, step 3677/7134 completed (loss: 0.02462437003850937, acc: 0.9890410900115967)
[2025-02-13 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13][root][INFO] - Training Epoch: 2/2, step 3678/7134 completed (loss: 0.03933010250329971, acc: 0.9878542423248291)
[2025-02-13 04:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13][root][INFO] - Training Epoch: 2/2, step 3679/7134 completed (loss: 0.0067381104454398155, acc: 0.9983471035957336)
[2025-02-13 04:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14][root][INFO] - Training Epoch: 2/2, step 3680/7134 completed (loss: 0.02511494979262352, acc: 0.9918830990791321)
[2025-02-13 04:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14][root][INFO] - Training Epoch: 2/2, step 3681/7134 completed (loss: 0.016995325684547424, acc: 0.994413435459137)
[2025-02-13 04:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14][root][INFO] - Training Epoch: 2/2, step 3682/7134 completed (loss: 0.010873368941247463, acc: 0.9973649382591248)
[2025-02-13 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:15][root][INFO] - Training Epoch: 2/2, step 3683/7134 completed (loss: 0.012033408507704735, acc: 0.9984251856803894)
[2025-02-13 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:15][root][INFO] - Training Epoch: 2/2, step 3684/7134 completed (loss: 0.018702277913689613, acc: 0.9956709742546082)
[2025-02-13 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16][root][INFO] - Training Epoch: 2/2, step 3685/7134 completed (loss: 0.023306911811232567, acc: 0.9884393215179443)
[2025-02-13 04:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16][root][INFO] - Training Epoch: 2/2, step 3686/7134 completed (loss: 0.015923792496323586, acc: 0.9940029978752136)
[2025-02-13 04:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:17][root][INFO] - Training Epoch: 2/2, step 3687/7134 completed (loss: 0.008895770646631718, acc: 0.9953846335411072)
[2025-02-13 04:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:17][root][INFO] - Training Epoch: 2/2, step 3688/7134 completed (loss: 0.008863786235451698, acc: 0.9979715943336487)
[2025-02-13 04:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:17][root][INFO] - Training Epoch: 2/2, step 3689/7134 completed (loss: 0.014077624306082726, acc: 0.9934640526771545)
[2025-02-13 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18][root][INFO] - Training Epoch: 2/2, step 3690/7134 completed (loss: 0.009012497961521149, acc: 0.995708167552948)
[2025-02-13 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18][root][INFO] - Training Epoch: 2/2, step 3691/7134 completed (loss: 0.0211921539157629, acc: 0.9925558567047119)
[2025-02-13 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:19][root][INFO] - Training Epoch: 2/2, step 3692/7134 completed (loss: 0.025948714464902878, acc: 0.990867555141449)
[2025-02-13 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:19][root][INFO] - Training Epoch: 2/2, step 3693/7134 completed (loss: 0.013775843195617199, acc: 0.9936908483505249)
[2025-02-13 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20][root][INFO] - Training Epoch: 2/2, step 3694/7134 completed (loss: 0.025676293298602104, acc: 0.9928876161575317)
[2025-02-13 04:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20][root][INFO] - Training Epoch: 2/2, step 3695/7134 completed (loss: 0.020137567073106766, acc: 0.9943342804908752)
[2025-02-13 04:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20][root][INFO] - Training Epoch: 2/2, step 3696/7134 completed (loss: 0.01033021230250597, acc: 0.9961439371109009)
[2025-02-13 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:21][root][INFO] - Training Epoch: 2/2, step 3697/7134 completed (loss: 0.007159184664487839, acc: 0.9956268072128296)
[2025-02-13 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:21][root][INFO] - Training Epoch: 2/2, step 3698/7134 completed (loss: 0.014492467045783997, acc: 0.9935794472694397)
[2025-02-13 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:22][root][INFO] - Training Epoch: 2/2, step 3699/7134 completed (loss: 0.0032354863360524178, acc: 1.0)
[2025-02-13 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:22][root][INFO] - Training Epoch: 2/2, step 3700/7134 completed (loss: 0.020624667406082153, acc: 0.9956268072128296)
[2025-02-13 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23][root][INFO] - Training Epoch: 2/2, step 3701/7134 completed (loss: 0.016828222200274467, acc: 0.996688723564148)
[2025-02-13 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23][root][INFO] - Training Epoch: 2/2, step 3702/7134 completed (loss: 0.01774183288216591, acc: 0.9931129217147827)
[2025-02-13 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:24][root][INFO] - Training Epoch: 2/2, step 3703/7134 completed (loss: 0.01623189076781273, acc: 0.9961685538291931)
[2025-02-13 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:24][root][INFO] - Training Epoch: 2/2, step 3704/7134 completed (loss: 0.004584931302815676, acc: 0.9979079365730286)
[2025-02-13 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:24][root][INFO] - Training Epoch: 2/2, step 3705/7134 completed (loss: 0.034401435405015945, acc: 0.987261176109314)
[2025-02-13 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:25][root][INFO] - Training Epoch: 2/2, step 3706/7134 completed (loss: 0.08598468452692032, acc: 0.9803197979927063)
[2025-02-13 04:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:25][root][INFO] - Training Epoch: 2/2, step 3707/7134 completed (loss: 0.005860230419784784, acc: 0.9985975027084351)
[2025-02-13 04:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26][root][INFO] - Training Epoch: 2/2, step 3708/7134 completed (loss: 0.03789803758263588, acc: 0.9869047403335571)
[2025-02-13 04:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26][root][INFO] - Training Epoch: 2/2, step 3709/7134 completed (loss: 0.01363787055015564, acc: 0.9949290156364441)
[2025-02-13 04:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:27][root][INFO] - Training Epoch: 2/2, step 3710/7134 completed (loss: 0.021554388105869293, acc: 0.9948770403862)
[2025-02-13 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:27][root][INFO] - Training Epoch: 2/2, step 3711/7134 completed (loss: 0.022378919646143913, acc: 0.9950371980667114)
[2025-02-13 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28][root][INFO] - Training Epoch: 2/2, step 3712/7134 completed (loss: 0.017045794054865837, acc: 0.9964157938957214)
[2025-02-13 04:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28][root][INFO] - Training Epoch: 2/2, step 3713/7134 completed (loss: 0.04094240814447403, acc: 0.9885057210922241)
[2025-02-13 04:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28][root][INFO] - Training Epoch: 2/2, step 3714/7134 completed (loss: 0.02170606330037117, acc: 0.9933701753616333)
[2025-02-13 04:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:29][root][INFO] - Training Epoch: 2/2, step 3715/7134 completed (loss: 0.024379655718803406, acc: 0.9951632618904114)
[2025-02-13 04:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:29][root][INFO] - Training Epoch: 2/2, step 3716/7134 completed (loss: 0.024848828092217445, acc: 0.992601752281189)
[2025-02-13 04:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:30][root][INFO] - Training Epoch: 2/2, step 3717/7134 completed (loss: 0.02257898449897766, acc: 0.9931694269180298)
[2025-02-13 04:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:30][root][INFO] - Training Epoch: 2/2, step 3718/7134 completed (loss: 0.04014090448617935, acc: 0.9882766604423523)
[2025-02-13 04:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31][root][INFO] - Training Epoch: 2/2, step 3719/7134 completed (loss: 0.03403383493423462, acc: 0.9882352948188782)
[2025-02-13 04:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31][root][INFO] - Training Epoch: 2/2, step 3720/7134 completed (loss: 0.014844127930700779, acc: 0.9956011772155762)
[2025-02-13 04:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:32][root][INFO] - Training Epoch: 2/2, step 3721/7134 completed (loss: 0.022523315623402596, acc: 0.9906790852546692)
[2025-02-13 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:32][root][INFO] - Training Epoch: 2/2, step 3722/7134 completed (loss: 0.014959938824176788, acc: 0.9950739145278931)
[2025-02-13 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:33][root][INFO] - Training Epoch: 2/2, step 3723/7134 completed (loss: 0.03467828407883644, acc: 0.9899280667304993)
[2025-02-13 04:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:33][root][INFO] - Training Epoch: 2/2, step 3724/7134 completed (loss: 0.024417323991656303, acc: 0.9913793206214905)
[2025-02-13 04:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:33][root][INFO] - Training Epoch: 2/2, step 3725/7134 completed (loss: 0.082440584897995, acc: 0.982300877571106)
[2025-02-13 04:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34][root][INFO] - Training Epoch: 2/2, step 3726/7134 completed (loss: 0.015744034200906754, acc: 0.994686484336853)
[2025-02-13 04:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34][root][INFO] - Training Epoch: 2/2, step 3727/7134 completed (loss: 0.028264913707971573, acc: 0.9933244585990906)
[2025-02-13 04:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:35][root][INFO] - Training Epoch: 2/2, step 3728/7134 completed (loss: 0.00903259590268135, acc: 0.9977452158927917)
[2025-02-13 04:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:35][root][INFO] - Training Epoch: 2/2, step 3729/7134 completed (loss: 0.014829284511506557, acc: 0.9954545497894287)
[2025-02-13 04:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36][root][INFO] - Training Epoch: 2/2, step 3730/7134 completed (loss: 0.01664847321808338, acc: 0.9959294199943542)
[2025-02-13 04:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36][root][INFO] - Training Epoch: 2/2, step 3731/7134 completed (loss: 0.004176851361989975, acc: 1.0)
[2025-02-13 04:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:37][root][INFO] - Training Epoch: 2/2, step 3732/7134 completed (loss: 0.009094640612602234, acc: 0.9977298378944397)
[2025-02-13 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:37][root][INFO] - Training Epoch: 2/2, step 3733/7134 completed (loss: 0.022648470476269722, acc: 0.9971949458122253)
[2025-02-13 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:37][root][INFO] - Training Epoch: 2/2, step 3734/7134 completed (loss: 0.01785726100206375, acc: 0.9967637658119202)
[2025-02-13 04:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38][root][INFO] - Training Epoch: 2/2, step 3735/7134 completed (loss: 0.03967776522040367, acc: 0.990963876247406)
[2025-02-13 04:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38][root][INFO] - Training Epoch: 2/2, step 3736/7134 completed (loss: 0.06522301584482193, acc: 0.9791304469108582)
[2025-02-13 04:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:39][root][INFO] - Training Epoch: 2/2, step 3737/7134 completed (loss: 0.056871507316827774, acc: 0.9849170446395874)
[2025-02-13 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:39][root][INFO] - Training Epoch: 2/2, step 3738/7134 completed (loss: 0.009889843873679638, acc: 0.9966996908187866)
[2025-02-13 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:40][root][INFO] - Training Epoch: 2/2, step 3739/7134 completed (loss: 0.028846116736531258, acc: 0.9915730357170105)
[2025-02-13 04:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:40][root][INFO] - Training Epoch: 2/2, step 3740/7134 completed (loss: 0.026900671422481537, acc: 0.9951298832893372)
[2025-02-13 04:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:40][root][INFO] - Training Epoch: 2/2, step 3741/7134 completed (loss: 0.04052969068288803, acc: 0.9914407730102539)
[2025-02-13 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41][root][INFO] - Training Epoch: 2/2, step 3742/7134 completed (loss: 0.04254007339477539, acc: 0.9863247871398926)
[2025-02-13 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41][root][INFO] - Training Epoch: 2/2, step 3743/7134 completed (loss: 0.034039732068777084, acc: 0.9867256879806519)
[2025-02-13 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:42][root][INFO] - Training Epoch: 2/2, step 3744/7134 completed (loss: 0.04807141795754433, acc: 0.9894598126411438)
[2025-02-13 04:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:42][root][INFO] - Training Epoch: 2/2, step 3745/7134 completed (loss: 0.058346014469861984, acc: 0.9837586879730225)
[2025-02-13 04:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43][root][INFO] - Training Epoch: 2/2, step 3746/7134 completed (loss: 0.02861110121011734, acc: 0.996073305606842)
[2025-02-13 04:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43][root][INFO] - Training Epoch: 2/2, step 3747/7134 completed (loss: 0.03596450015902519, acc: 0.9927113652229309)
[2025-02-13 04:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43][root][INFO] - Training Epoch: 2/2, step 3748/7134 completed (loss: 0.06005105748772621, acc: 0.98591548204422)
[2025-02-13 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:44][root][INFO] - Training Epoch: 2/2, step 3749/7134 completed (loss: 0.0754668340086937, acc: 0.9790105223655701)
[2025-02-13 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:44][root][INFO] - Training Epoch: 2/2, step 3750/7134 completed (loss: 0.010308532044291496, acc: 0.9972106218338013)
[2025-02-13 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45][root][INFO] - Training Epoch: 2/2, step 3751/7134 completed (loss: 0.03469604253768921, acc: 0.9928876161575317)
[2025-02-13 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45][root][INFO] - Training Epoch: 2/2, step 3752/7134 completed (loss: 0.019327722489833832, acc: 0.9940298795700073)
[2025-02-13 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:46][root][INFO] - Training Epoch: 2/2, step 3753/7134 completed (loss: 0.05582904815673828, acc: 0.980567991733551)
[2025-02-13 04:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:46][root][INFO] - Training Epoch: 2/2, step 3754/7134 completed (loss: 0.024283841252326965, acc: 0.9939302206039429)
[2025-02-13 04:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:46][root][INFO] - Training Epoch: 2/2, step 3755/7134 completed (loss: 0.025145094841718674, acc: 0.9907235503196716)
[2025-02-13 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47][root][INFO] - Training Epoch: 2/2, step 3756/7134 completed (loss: 0.031289126724004745, acc: 0.9930747747421265)
[2025-02-13 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47][root][INFO] - Training Epoch: 2/2, step 3757/7134 completed (loss: 0.05566012114286423, acc: 0.9786535501480103)
[2025-02-13 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48][root][INFO] - Training Epoch: 2/2, step 3758/7134 completed (loss: 0.019925741478800774, acc: 0.9909909963607788)
[2025-02-13 04:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48][root][INFO] - Training Epoch: 2/2, step 3759/7134 completed (loss: 0.037442468106746674, acc: 0.9864864945411682)
[2025-02-13 04:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49][root][INFO] - Training Epoch: 2/2, step 3760/7134 completed (loss: 0.03566567227244377, acc: 0.991525411605835)
[2025-02-13 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49][root][INFO] - Training Epoch: 2/2, step 3761/7134 completed (loss: 0.014659188687801361, acc: 0.9969087839126587)
[2025-02-13 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49][root][INFO] - Training Epoch: 2/2, step 3762/7134 completed (loss: 0.024498101323843002, acc: 0.9917469024658203)
[2025-02-13 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:50][root][INFO] - Training Epoch: 2/2, step 3763/7134 completed (loss: 0.015526692382991314, acc: 0.9938271641731262)
[2025-02-13 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:50][root][INFO] - Training Epoch: 2/2, step 3764/7134 completed (loss: 0.04018481820821762, acc: 0.9849520921707153)
[2025-02-13 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:51][root][INFO] - Training Epoch: 2/2, step 3765/7134 completed (loss: 0.031765639781951904, acc: 0.9903961420059204)
[2025-02-13 04:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:51][root][INFO] - Training Epoch: 2/2, step 3766/7134 completed (loss: 0.04475641995668411, acc: 0.9871794581413269)
[2025-02-13 04:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52][root][INFO] - Training Epoch: 2/2, step 3767/7134 completed (loss: 0.011600777506828308, acc: 0.9950124621391296)
[2025-02-13 04:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52][root][INFO] - Training Epoch: 2/2, step 3768/7134 completed (loss: 0.006164318881928921, acc: 1.0)
[2025-02-13 04:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53][root][INFO] - Training Epoch: 2/2, step 3769/7134 completed (loss: 0.030857717618346214, acc: 0.989924430847168)
[2025-02-13 04:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53][root][INFO] - Training Epoch: 2/2, step 3770/7134 completed (loss: 0.026393162086606026, acc: 0.9957627058029175)
[2025-02-13 04:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53][root][INFO] - Training Epoch: 2/2, step 3771/7134 completed (loss: 0.019944095984101295, acc: 0.9922077655792236)
[2025-02-13 04:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54][root][INFO] - Training Epoch: 2/2, step 3772/7134 completed (loss: 0.027175817638635635, acc: 0.9895366430282593)
[2025-02-13 04:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54][root][INFO] - Training Epoch: 2/2, step 3773/7134 completed (loss: 0.026598554104566574, acc: 0.9955357313156128)
[2025-02-13 04:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:55][root][INFO] - Training Epoch: 2/2, step 3774/7134 completed (loss: 0.018244188278913498, acc: 0.9953595995903015)
[2025-02-13 04:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:55][root][INFO] - Training Epoch: 2/2, step 3775/7134 completed (loss: 0.02085864543914795, acc: 0.9942693114280701)
[2025-02-13 04:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56][root][INFO] - Training Epoch: 2/2, step 3776/7134 completed (loss: 0.03358754888176918, acc: 0.9866666793823242)
[2025-02-13 04:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56][root][INFO] - Training Epoch: 2/2, step 3777/7134 completed (loss: 0.012798077426850796, acc: 0.9937343597412109)
[2025-02-13 04:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:57][root][INFO] - Training Epoch: 2/2, step 3778/7134 completed (loss: 0.04450394958257675, acc: 0.9871976971626282)
[2025-02-13 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:57][root][INFO] - Training Epoch: 2/2, step 3779/7134 completed (loss: 0.024500669911503792, acc: 0.9914529919624329)
[2025-02-13 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:57][root][INFO] - Training Epoch: 2/2, step 3780/7134 completed (loss: 0.01712696999311447, acc: 0.9927954077720642)
[2025-02-13 04:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:58][root][INFO] - Training Epoch: 2/2, step 3781/7134 completed (loss: 0.04615143686532974, acc: 0.9903978109359741)
[2025-02-13 04:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:58][root][INFO] - Training Epoch: 2/2, step 3782/7134 completed (loss: 0.02819237671792507, acc: 0.9857549667358398)
[2025-02-13 04:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59][root][INFO] - Training Epoch: 2/2, step 3783/7134 completed (loss: 0.028101375326514244, acc: 0.9922580718994141)
[2025-02-13 04:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59][root][INFO] - Training Epoch: 2/2, step 3784/7134 completed (loss: 0.041433095932006836, acc: 0.9907264113426208)
[2025-02-13 04:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:00][root][INFO] - Training Epoch: 2/2, step 3785/7134 completed (loss: 0.01143103651702404, acc: 0.9953197836875916)
[2025-02-13 04:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:00][root][INFO] - Training Epoch: 2/2, step 3786/7134 completed (loss: 0.027445832267403603, acc: 0.9896507263183594)
[2025-02-13 04:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:00][root][INFO] - Training Epoch: 2/2, step 3787/7134 completed (loss: 0.02734881266951561, acc: 0.9920254945755005)
[2025-02-13 04:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01][root][INFO] - Training Epoch: 2/2, step 3788/7134 completed (loss: 0.021925916895270348, acc: 0.9932523369789124)
[2025-02-13 04:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01][root][INFO] - Training Epoch: 2/2, step 3789/7134 completed (loss: 0.02701282873749733, acc: 0.9887482523918152)
[2025-02-13 04:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:02][root][INFO] - Training Epoch: 2/2, step 3790/7134 completed (loss: 0.06710077077150345, acc: 0.9847908616065979)
[2025-02-13 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:02][root][INFO] - Training Epoch: 2/2, step 3791/7134 completed (loss: 0.010754918679594994, acc: 0.996129035949707)
[2025-02-13 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:03][root][INFO] - Training Epoch: 2/2, step 3792/7134 completed (loss: 0.01943632960319519, acc: 0.9917898178100586)
[2025-02-13 04:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:03][root][INFO] - Training Epoch: 2/2, step 3793/7134 completed (loss: 0.021674606949090958, acc: 0.9945945739746094)
[2025-02-13 04:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04][root][INFO] - Training Epoch: 2/2, step 3794/7134 completed (loss: 0.023997830227017403, acc: 0.990920901298523)
[2025-02-13 04:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04][root][INFO] - Training Epoch: 2/2, step 3795/7134 completed (loss: 0.021165015175938606, acc: 0.9939758777618408)
[2025-02-13 04:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04][root][INFO] - Training Epoch: 2/2, step 3796/7134 completed (loss: 0.015543949790298939, acc: 0.9962311387062073)
[2025-02-13 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:05][root][INFO] - Training Epoch: 2/2, step 3797/7134 completed (loss: 0.013884714804589748, acc: 0.9960886836051941)
[2025-02-13 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:05][root][INFO] - Training Epoch: 2/2, step 3798/7134 completed (loss: 0.03659599646925926, acc: 0.9928160905838013)
[2025-02-13 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06][root][INFO] - Training Epoch: 2/2, step 3799/7134 completed (loss: 0.0315498523414135, acc: 0.9884169697761536)
[2025-02-13 04:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06][root][INFO] - Training Epoch: 2/2, step 3800/7134 completed (loss: 0.03270688280463219, acc: 0.9892966151237488)
[2025-02-13 04:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:07][root][INFO] - Training Epoch: 2/2, step 3801/7134 completed (loss: 0.016435407102108, acc: 0.9950617551803589)
[2025-02-13 04:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:07][root][INFO] - Training Epoch: 2/2, step 3802/7134 completed (loss: 0.024217093363404274, acc: 0.9940898418426514)
[2025-02-13 04:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:07][root][INFO] - Training Epoch: 2/2, step 3803/7134 completed (loss: 0.035762809216976166, acc: 0.9887955188751221)
[2025-02-13 04:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08][root][INFO] - Training Epoch: 2/2, step 3804/7134 completed (loss: 0.01708950661122799, acc: 0.9922239780426025)
[2025-02-13 04:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08][root][INFO] - Training Epoch: 2/2, step 3805/7134 completed (loss: 0.009573178365826607, acc: 0.996073305606842)
[2025-02-13 04:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:09][root][INFO] - Training Epoch: 2/2, step 3806/7134 completed (loss: 0.020927419885993004, acc: 0.990138053894043)
[2025-02-13 04:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:09][root][INFO] - Training Epoch: 2/2, step 3807/7134 completed (loss: 0.010548099875450134, acc: 0.9985380172729492)
[2025-02-13 04:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:10][root][INFO] - Training Epoch: 2/2, step 3808/7134 completed (loss: 0.033577270805835724, acc: 0.9924050569534302)
[2025-02-13 04:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:10][root][INFO] - Training Epoch: 2/2, step 3809/7134 completed (loss: 0.037129588425159454, acc: 0.9885786771774292)
[2025-02-13 04:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11][root][INFO] - Training Epoch: 2/2, step 3810/7134 completed (loss: 0.023594044148921967, acc: 0.9940476417541504)
[2025-02-13 04:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11][root][INFO] - Training Epoch: 2/2, step 3811/7134 completed (loss: 0.03114822693169117, acc: 0.9903448224067688)
[2025-02-13 04:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11][root][INFO] - Training Epoch: 2/2, step 3812/7134 completed (loss: 0.03681042417883873, acc: 0.9845626354217529)
[2025-02-13 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:12][root][INFO] - Training Epoch: 2/2, step 3813/7134 completed (loss: 0.04174154996871948, acc: 0.9894459247589111)
[2025-02-13 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:12][root][INFO] - Training Epoch: 2/2, step 3814/7134 completed (loss: 0.017975404858589172, acc: 0.9946380853652954)
[2025-02-13 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:13][root][INFO] - Training Epoch: 2/2, step 3815/7134 completed (loss: 0.019236262887716293, acc: 0.9940740466117859)
[2025-02-13 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:13][root][INFO] - Training Epoch: 2/2, step 3816/7134 completed (loss: 0.012553158216178417, acc: 0.9984227418899536)
[2025-02-13 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14][root][INFO] - Training Epoch: 2/2, step 3817/7134 completed (loss: 0.04199974983930588, acc: 0.9880095720291138)
[2025-02-13 04:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14][root][INFO] - Training Epoch: 2/2, step 3818/7134 completed (loss: 0.030855948105454445, acc: 0.9939393997192383)
[2025-02-13 04:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:15][root][INFO] - Training Epoch: 2/2, step 3819/7134 completed (loss: 0.04328586906194687, acc: 0.9839357137680054)
[2025-02-13 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:15][root][INFO] - Training Epoch: 2/2, step 3820/7134 completed (loss: 0.026933953166007996, acc: 0.9904988408088684)
[2025-02-13 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:15][root][INFO] - Training Epoch: 2/2, step 3821/7134 completed (loss: 0.03185814246535301, acc: 0.989847719669342)
[2025-02-13 04:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16][root][INFO] - Training Epoch: 2/2, step 3822/7134 completed (loss: 0.025215519592165947, acc: 0.9921414256095886)
[2025-02-13 04:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16][root][INFO] - Training Epoch: 2/2, step 3823/7134 completed (loss: 0.017656218260526657, acc: 0.9958506226539612)
[2025-02-13 04:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:17][root][INFO] - Training Epoch: 2/2, step 3824/7134 completed (loss: 0.0680174008011818, acc: 0.9848229289054871)
[2025-02-13 04:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:17][root][INFO] - Training Epoch: 2/2, step 3825/7134 completed (loss: 0.01495490875095129, acc: 0.9920159578323364)
[2025-02-13 04:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18][root][INFO] - Training Epoch: 2/2, step 3826/7134 completed (loss: 0.030612239614129066, acc: 0.9911764860153198)
[2025-02-13 04:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18][root][INFO] - Training Epoch: 2/2, step 3827/7134 completed (loss: 0.020151743665337563, acc: 0.9929971694946289)
[2025-02-13 04:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:19][root][INFO] - Training Epoch: 2/2, step 3828/7134 completed (loss: 0.026835285127162933, acc: 0.9946949481964111)
[2025-02-13 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:19][root][INFO] - Training Epoch: 2/2, step 3829/7134 completed (loss: 0.014926052652299404, acc: 0.9969834089279175)
[2025-02-13 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:19][root][INFO] - Training Epoch: 2/2, step 3830/7134 completed (loss: 0.04266555234789848, acc: 0.9949748516082764)
[2025-02-13 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20][root][INFO] - Training Epoch: 2/2, step 3831/7134 completed (loss: 0.02317354641854763, acc: 0.9909909963607788)
[2025-02-13 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20][root][INFO] - Training Epoch: 2/2, step 3832/7134 completed (loss: 0.022748935967683792, acc: 0.994106113910675)
[2025-02-13 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:21][root][INFO] - Training Epoch: 2/2, step 3833/7134 completed (loss: 0.00806245394051075, acc: 0.9962871074676514)
[2025-02-13 04:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:21][root][INFO] - Training Epoch: 2/2, step 3834/7134 completed (loss: 0.00490753585472703, acc: 0.998701274394989)
[2025-02-13 04:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22][root][INFO] - Training Epoch: 2/2, step 3835/7134 completed (loss: 0.011306105181574821, acc: 0.9951456189155579)
[2025-02-13 04:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22][root][INFO] - Training Epoch: 2/2, step 3836/7134 completed (loss: 0.035126879811286926, acc: 0.991304337978363)
[2025-02-13 04:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22][root][INFO] - Training Epoch: 2/2, step 3837/7134 completed (loss: 0.024190256372094154, acc: 0.9956896305084229)
[2025-02-13 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23][root][INFO] - Training Epoch: 2/2, step 3838/7134 completed (loss: 0.0038878819905221462, acc: 0.9986720085144043)
[2025-02-13 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23][root][INFO] - Training Epoch: 2/2, step 3839/7134 completed (loss: 0.016048872843384743, acc: 0.9944751262664795)
[2025-02-13 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:24][root][INFO] - Training Epoch: 2/2, step 3840/7134 completed (loss: 0.007570096291601658, acc: 0.9986613392829895)
[2025-02-13 04:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:24][root][INFO] - Training Epoch: 2/2, step 3841/7134 completed (loss: 0.043348345905542374, acc: 0.9911209940910339)
[2025-02-13 04:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:25][root][INFO] - Training Epoch: 2/2, step 3842/7134 completed (loss: 0.0036046046297997236, acc: 1.0)
[2025-02-13 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:25][root][INFO] - Training Epoch: 2/2, step 3843/7134 completed (loss: 0.016266414895653725, acc: 0.9937434792518616)
[2025-02-13 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26][root][INFO] - Training Epoch: 2/2, step 3844/7134 completed (loss: 0.016994839534163475, acc: 0.9961880445480347)
[2025-02-13 04:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26][root][INFO] - Training Epoch: 2/2, step 3845/7134 completed (loss: 0.0269100870937109, acc: 0.9932659864425659)
[2025-02-13 04:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26][root][INFO] - Training Epoch: 2/2, step 3846/7134 completed (loss: 0.010622517205774784, acc: 0.9983022212982178)
[2025-02-13 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:27][root][INFO] - Training Epoch: 2/2, step 3847/7134 completed (loss: 0.014073233120143414, acc: 0.9936000108718872)
[2025-02-13 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:27][root][INFO] - Training Epoch: 2/2, step 3848/7134 completed (loss: 0.024389317259192467, acc: 0.9974554777145386)
[2025-02-13 04:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:28][root][INFO] - Training Epoch: 2/2, step 3849/7134 completed (loss: 0.02862076833844185, acc: 0.9910873174667358)
[2025-02-13 04:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:28][root][INFO] - Training Epoch: 2/2, step 3850/7134 completed (loss: 0.040600698441267014, acc: 0.9889975786209106)
[2025-02-13 04:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29][root][INFO] - Training Epoch: 2/2, step 3851/7134 completed (loss: 0.027739237993955612, acc: 0.9902371168136597)
[2025-02-13 04:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29][root][INFO] - Training Epoch: 2/2, step 3852/7134 completed (loss: 0.014356432482600212, acc: 0.9951768517494202)
[2025-02-13 04:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:30][root][INFO] - Training Epoch: 2/2, step 3853/7134 completed (loss: 0.03104308806359768, acc: 0.9873417615890503)
[2025-02-13 04:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:30][root][INFO] - Training Epoch: 2/2, step 3854/7134 completed (loss: 0.007921203970909119, acc: 1.0)
[2025-02-13 04:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31][root][INFO] - Training Epoch: 2/2, step 3855/7134 completed (loss: 0.04607747495174408, acc: 0.9875311851501465)
[2025-02-13 04:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31][root][INFO] - Training Epoch: 2/2, step 3856/7134 completed (loss: 0.028501661494374275, acc: 0.9930264949798584)
[2025-02-13 04:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31][root][INFO] - Training Epoch: 2/2, step 3857/7134 completed (loss: 0.016409412026405334, acc: 0.9935691356658936)
[2025-02-13 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:32][root][INFO] - Training Epoch: 2/2, step 3858/7134 completed (loss: 0.04267400503158569, acc: 0.9900850057601929)
[2025-02-13 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:32][root][INFO] - Training Epoch: 2/2, step 3859/7134 completed (loss: 0.03647701069712639, acc: 0.9898219108581543)
[2025-02-13 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:33][root][INFO] - Training Epoch: 2/2, step 3860/7134 completed (loss: 0.021426379680633545, acc: 0.9921787977218628)
[2025-02-13 04:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:33][root][INFO] - Training Epoch: 2/2, step 3861/7134 completed (loss: 0.02377232164144516, acc: 0.9931972622871399)
[2025-02-13 04:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34][root][INFO] - Training Epoch: 2/2, step 3862/7134 completed (loss: 0.03441151976585388, acc: 0.9835796356201172)
[2025-02-13 04:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34][root][INFO] - Training Epoch: 2/2, step 3863/7134 completed (loss: 0.014530761167407036, acc: 0.9950617551803589)
[2025-02-13 04:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:35][root][INFO] - Training Epoch: 2/2, step 3864/7134 completed (loss: 0.02994508482515812, acc: 0.9947780966758728)
[2025-02-13 04:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:35][root][INFO] - Training Epoch: 2/2, step 3865/7134 completed (loss: 0.019586946815252304, acc: 0.9940239191055298)
[2025-02-13 04:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36][root][INFO] - Training Epoch: 2/2, step 3866/7134 completed (loss: 0.03613244369626045, acc: 0.9914529919624329)
[2025-02-13 04:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36][root][INFO] - Training Epoch: 2/2, step 3867/7134 completed (loss: 0.02947717346251011, acc: 0.9905303120613098)
[2025-02-13 04:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36][root][INFO] - Training Epoch: 2/2, step 3868/7134 completed (loss: 0.01741335168480873, acc: 0.9949238300323486)
[2025-02-13 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:37][root][INFO] - Training Epoch: 2/2, step 3869/7134 completed (loss: 0.02045099250972271, acc: 0.9939485788345337)
[2025-02-13 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:37][root][INFO] - Training Epoch: 2/2, step 3870/7134 completed (loss: 0.0500260666012764, acc: 0.987500011920929)
[2025-02-13 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:38][root][INFO] - Training Epoch: 2/2, step 3871/7134 completed (loss: 0.03194763511419296, acc: 0.9912717938423157)
[2025-02-13 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:38][root][INFO] - Training Epoch: 2/2, step 3872/7134 completed (loss: 0.04012150317430496, acc: 0.9914346933364868)
[2025-02-13 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39][root][INFO] - Training Epoch: 2/2, step 3873/7134 completed (loss: 0.024738382548093796, acc: 0.9936061501502991)
[2025-02-13 04:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39][root][INFO] - Training Epoch: 2/2, step 3874/7134 completed (loss: 0.043937798589468, acc: 0.9903181195259094)
[2025-02-13 04:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:40][root][INFO] - Training Epoch: 2/2, step 3875/7134 completed (loss: 0.053162720054388046, acc: 0.9840425252914429)
[2025-02-13 04:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:40][root][INFO] - Training Epoch: 2/2, step 3876/7134 completed (loss: 0.01587258279323578, acc: 0.9924952983856201)
[2025-02-13 04:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:41][root][INFO] - Training Epoch: 2/2, step 3877/7134 completed (loss: 0.034430328756570816, acc: 0.9934725761413574)
[2025-02-13 04:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:41][root][INFO] - Training Epoch: 2/2, step 3878/7134 completed (loss: 0.05976115167140961, acc: 0.9838274717330933)
[2025-02-13 04:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:41][root][INFO] - Training Epoch: 2/2, step 3879/7134 completed (loss: 0.011277370154857635, acc: 1.0)
[2025-02-13 04:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42][root][INFO] - Training Epoch: 2/2, step 3880/7134 completed (loss: 0.06292784214019775, acc: 0.9867841601371765)
[2025-02-13 04:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42][root][INFO] - Training Epoch: 2/2, step 3881/7134 completed (loss: 0.08829715102910995, acc: 0.9800000190734863)
[2025-02-13 04:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:43][root][INFO] - Training Epoch: 2/2, step 3882/7134 completed (loss: 0.035255663096904755, acc: 0.9912499785423279)
[2025-02-13 04:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:43][root][INFO] - Training Epoch: 2/2, step 3883/7134 completed (loss: 0.00751641858369112, acc: 0.9972028136253357)
[2025-02-13 04:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44][root][INFO] - Training Epoch: 2/2, step 3884/7134 completed (loss: 0.009607711806893349, acc: 0.9986394643783569)
[2025-02-13 04:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44][root][INFO] - Training Epoch: 2/2, step 3885/7134 completed (loss: 0.022479139268398285, acc: 0.9947780966758728)
[2025-02-13 04:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44][root][INFO] - Training Epoch: 2/2, step 3886/7134 completed (loss: 0.019742857664823532, acc: 0.9927536249160767)
[2025-02-13 04:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:45][root][INFO] - Training Epoch: 2/2, step 3887/7134 completed (loss: 0.027434807270765305, acc: 0.9954751133918762)
[2025-02-13 04:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:45][root][INFO] - Training Epoch: 2/2, step 3888/7134 completed (loss: 0.029311690479516983, acc: 0.9925000071525574)
[2025-02-13 04:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:46][root][INFO] - Training Epoch: 2/2, step 3889/7134 completed (loss: 0.018499700352549553, acc: 0.9961685538291931)
[2025-02-13 04:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:46][root][INFO] - Training Epoch: 2/2, step 3890/7134 completed (loss: 0.012204413302242756, acc: 0.9975932836532593)
[2025-02-13 04:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47][root][INFO] - Training Epoch: 2/2, step 3891/7134 completed (loss: 0.012667791917920113, acc: 0.9934640526771545)
[2025-02-13 04:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47][root][INFO] - Training Epoch: 2/2, step 3892/7134 completed (loss: 0.01570388302206993, acc: 0.9953846335411072)
[2025-02-13 04:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48][root][INFO] - Training Epoch: 2/2, step 3893/7134 completed (loss: 0.04487691447138786, acc: 0.9871134161949158)
[2025-02-13 04:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48][root][INFO] - Training Epoch: 2/2, step 3894/7134 completed (loss: 0.00645940937101841, acc: 1.0)
[2025-02-13 04:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48][root][INFO] - Training Epoch: 2/2, step 3895/7134 completed (loss: 0.01641339622437954, acc: 0.9948387145996094)
[2025-02-13 04:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:49][root][INFO] - Training Epoch: 2/2, step 3896/7134 completed (loss: 0.01482075359672308, acc: 0.9966139793395996)
[2025-02-13 04:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:49][root][INFO] - Training Epoch: 2/2, step 3897/7134 completed (loss: 0.041964344680309296, acc: 0.9915493130683899)
[2025-02-13 04:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50][root][INFO] - Training Epoch: 2/2, step 3898/7134 completed (loss: 0.025945063680410385, acc: 0.9936386942863464)
[2025-02-13 04:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50][root][INFO] - Training Epoch: 2/2, step 3899/7134 completed (loss: 0.019053615629673004, acc: 0.9961190223693848)
[2025-02-13 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:51][root][INFO] - Training Epoch: 2/2, step 3900/7134 completed (loss: 0.020329533144831657, acc: 0.9930475354194641)
[2025-02-13 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:51][root][INFO] - Training Epoch: 2/2, step 3901/7134 completed (loss: 0.07337698340415955, acc: 0.9851751923561096)
[2025-02-13 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:52][root][INFO] - Training Epoch: 2/2, step 3902/7134 completed (loss: 0.0647963359951973, acc: 0.9841059446334839)
[2025-02-13 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:52][root][INFO] - Training Epoch: 2/2, step 3903/7134 completed (loss: 0.05201254412531853, acc: 0.989276111125946)
[2025-02-13 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53][root][INFO] - Training Epoch: 2/2, step 3904/7134 completed (loss: 0.028532596305012703, acc: 0.990867555141449)
[2025-02-13 04:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53][root][INFO] - Training Epoch: 2/2, step 3905/7134 completed (loss: 0.015120375901460648, acc: 0.995768666267395)
[2025-02-13 04:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54][root][INFO] - Training Epoch: 2/2, step 3906/7134 completed (loss: 0.009453925304114819, acc: 0.9956896305084229)
[2025-02-13 04:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54][root][INFO] - Training Epoch: 2/2, step 3907/7134 completed (loss: 0.037362825125455856, acc: 0.9922879338264465)
[2025-02-13 04:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54][root][INFO] - Training Epoch: 2/2, step 3908/7134 completed (loss: 0.05239857733249664, acc: 0.9877488613128662)
[2025-02-13 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:55][root][INFO] - Training Epoch: 2/2, step 3909/7134 completed (loss: 0.025455618277192116, acc: 0.9919224381446838)
[2025-02-13 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:55][root][INFO] - Training Epoch: 2/2, step 3910/7134 completed (loss: 0.034269899129867554, acc: 0.9916550517082214)
[2025-02-13 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56][root][INFO] - Training Epoch: 2/2, step 3911/7134 completed (loss: 0.015423965640366077, acc: 0.9967637658119202)
[2025-02-13 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56][root][INFO] - Training Epoch: 2/2, step 3912/7134 completed (loss: 0.008683980442583561, acc: 0.9940476417541504)
[2025-02-13 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:57][root][INFO] - Training Epoch: 2/2, step 3913/7134 completed (loss: 0.021895620971918106, acc: 0.9897435903549194)
[2025-02-13 04:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:57][root][INFO] - Training Epoch: 2/2, step 3914/7134 completed (loss: 0.034675948321819305, acc: 0.9915825128555298)
[2025-02-13 04:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58][root][INFO] - Training Epoch: 2/2, step 3915/7134 completed (loss: 0.02015553042292595, acc: 0.9929478168487549)
[2025-02-13 04:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58][root][INFO] - Training Epoch: 2/2, step 3916/7134 completed (loss: 0.011143253184854984, acc: 0.9974522590637207)
[2025-02-13 04:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58][root][INFO] - Training Epoch: 2/2, step 3917/7134 completed (loss: 0.007114647887647152, acc: 0.9987966418266296)
[2025-02-13 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:59][root][INFO] - Training Epoch: 2/2, step 3918/7134 completed (loss: 0.021616997197270393, acc: 0.9934895634651184)
[2025-02-13 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:59][root][INFO] - Training Epoch: 2/2, step 3919/7134 completed (loss: 0.04119235277175903, acc: 0.9934853315353394)
[2025-02-13 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:00][root][INFO] - Training Epoch: 2/2, step 3920/7134 completed (loss: 0.024525264278054237, acc: 0.9922118186950684)
[2025-02-13 04:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:00][root][INFO] - Training Epoch: 2/2, step 3921/7134 completed (loss: 0.02003522962331772, acc: 0.9919999837875366)
[2025-02-13 04:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01][root][INFO] - Training Epoch: 2/2, step 3922/7134 completed (loss: 0.044003892689943314, acc: 0.9853249192237854)
[2025-02-13 04:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01][root][INFO] - Training Epoch: 2/2, step 3923/7134 completed (loss: 0.07015853375196457, acc: 0.982876718044281)
[2025-02-13 04:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:02][root][INFO] - Training Epoch: 2/2, step 3924/7134 completed (loss: 0.0216837115585804, acc: 0.9942528605461121)
[2025-02-13 04:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:02][root][INFO] - Training Epoch: 2/2, step 3925/7134 completed (loss: 0.038770031183958054, acc: 0.9882869720458984)
[2025-02-13 04:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:02][root][INFO] - Training Epoch: 2/2, step 3926/7134 completed (loss: 0.04254796728491783, acc: 0.9899280667304993)
[2025-02-13 04:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03][root][INFO] - Training Epoch: 2/2, step 3927/7134 completed (loss: 0.016967065632343292, acc: 0.993122398853302)
[2025-02-13 04:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03][root][INFO] - Training Epoch: 2/2, step 3928/7134 completed (loss: 0.046955641359090805, acc: 0.9856321811676025)
[2025-02-13 04:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:04][root][INFO] - Training Epoch: 2/2, step 3929/7134 completed (loss: 0.02682500332593918, acc: 0.9936407208442688)
[2025-02-13 04:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:04][root][INFO] - Training Epoch: 2/2, step 3930/7134 completed (loss: 0.027639396488666534, acc: 0.9894958138465881)
[2025-02-13 04:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05][root][INFO] - Training Epoch: 2/2, step 3931/7134 completed (loss: 0.01709502749145031, acc: 0.9955157041549683)
[2025-02-13 04:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05][root][INFO] - Training Epoch: 2/2, step 3932/7134 completed (loss: 0.02845940925180912, acc: 0.9881423115730286)
[2025-02-13 04:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05][root][INFO] - Training Epoch: 2/2, step 3933/7134 completed (loss: 0.016169026494026184, acc: 0.9934210777282715)
[2025-02-13 04:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06][root][INFO] - Training Epoch: 2/2, step 3934/7134 completed (loss: 0.019869515672326088, acc: 0.9906322956085205)
[2025-02-13 04:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06][root][INFO] - Training Epoch: 2/2, step 3935/7134 completed (loss: 0.006942493841052055, acc: 1.0)
[2025-02-13 04:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:07][root][INFO] - Training Epoch: 2/2, step 3936/7134 completed (loss: 0.0337306447327137, acc: 0.994339644908905)
[2025-02-13 04:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:07][root][INFO] - Training Epoch: 2/2, step 3937/7134 completed (loss: 0.03761608153581619, acc: 0.9795918464660645)
[2025-02-13 04:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08][root][INFO] - Training Epoch: 2/2, step 3938/7134 completed (loss: 0.010887117125093937, acc: 0.9977375268936157)
[2025-02-13 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08][root][INFO] - Training Epoch: 2/2, step 3939/7134 completed (loss: 0.052190233021974564, acc: 0.9813874959945679)
[2025-02-13 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08][root][INFO] - Training Epoch: 2/2, step 3940/7134 completed (loss: 0.02841138280928135, acc: 0.9924623370170593)
[2025-02-13 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:09][root][INFO] - Training Epoch: 2/2, step 3941/7134 completed (loss: 0.01959274336695671, acc: 0.9908536672592163)
[2025-02-13 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:09][root][INFO] - Training Epoch: 2/2, step 3942/7134 completed (loss: 0.016272274777293205, acc: 0.9918032884597778)
[2025-02-13 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:10][root][INFO] - Training Epoch: 2/2, step 3943/7134 completed (loss: 0.010947161354124546, acc: 0.992277979850769)
[2025-02-13 04:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:10][root][INFO] - Training Epoch: 2/2, step 3944/7134 completed (loss: 0.029021130874753, acc: 0.9926199316978455)
[2025-02-13 04:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:10][root][INFO] - Training Epoch: 2/2, step 3945/7134 completed (loss: 0.01104592252522707, acc: 0.9939024448394775)
[2025-02-13 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11][root][INFO] - Training Epoch: 2/2, step 3946/7134 completed (loss: 0.014770776964724064, acc: 0.9958246350288391)
[2025-02-13 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11][root][INFO] - Training Epoch: 2/2, step 3947/7134 completed (loss: 0.019461773335933685, acc: 0.9948186278343201)
[2025-02-13 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12][root][INFO] - Training Epoch: 2/2, step 3948/7134 completed (loss: 0.00585084268823266, acc: 0.9974874258041382)
[2025-02-13 04:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12][root][INFO] - Training Epoch: 2/2, step 3949/7134 completed (loss: 0.024447502568364143, acc: 0.9944030046463013)
[2025-02-13 04:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12][root][INFO] - Training Epoch: 2/2, step 3950/7134 completed (loss: 0.026863019913434982, acc: 0.9933035969734192)
[2025-02-13 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:13][root][INFO] - Training Epoch: 2/2, step 3951/7134 completed (loss: 0.015275359153747559, acc: 0.9949874877929688)
[2025-02-13 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:13][root][INFO] - Training Epoch: 2/2, step 3952/7134 completed (loss: 0.006677178665995598, acc: 0.9948717951774597)
[2025-02-13 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14][root][INFO] - Training Epoch: 2/2, step 3953/7134 completed (loss: 0.027915090322494507, acc: 0.9943073987960815)
[2025-02-13 04:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14][root][INFO] - Training Epoch: 2/2, step 3954/7134 completed (loss: 0.03545080125331879, acc: 0.9896907210350037)
[2025-02-13 04:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14][root][INFO] - Training Epoch: 2/2, step 3955/7134 completed (loss: 0.019825847819447517, acc: 0.9932546615600586)
[2025-02-13 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:15][root][INFO] - Training Epoch: 2/2, step 3956/7134 completed (loss: 0.009344730526208878, acc: 0.9983999729156494)
[2025-02-13 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:15][root][INFO] - Training Epoch: 2/2, step 3957/7134 completed (loss: 0.020247386768460274, acc: 0.9887387156486511)
[2025-02-13 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16][root][INFO] - Training Epoch: 2/2, step 3958/7134 completed (loss: 0.04175422713160515, acc: 0.9871794581413269)
[2025-02-13 04:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16][root][INFO] - Training Epoch: 2/2, step 3959/7134 completed (loss: 0.006784655153751373, acc: 0.997802197933197)
[2025-02-13 04:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17][root][INFO] - Training Epoch: 2/2, step 3960/7134 completed (loss: 0.03701323643326759, acc: 0.9857549667358398)
[2025-02-13 04:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17][root][INFO] - Training Epoch: 2/2, step 3961/7134 completed (loss: 0.009234944358468056, acc: 0.995502233505249)
[2025-02-13 04:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17][root][INFO] - Training Epoch: 2/2, step 3962/7134 completed (loss: 0.010155309922993183, acc: 0.9951456189155579)
[2025-02-13 04:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18][root][INFO] - Training Epoch: 2/2, step 3963/7134 completed (loss: 0.0540793314576149, acc: 0.9912023544311523)
[2025-02-13 04:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18][root][INFO] - Training Epoch: 2/2, step 3964/7134 completed (loss: 0.023966357111930847, acc: 0.9907407164573669)
[2025-02-13 04:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:19][root][INFO] - Training Epoch: 2/2, step 3965/7134 completed (loss: 0.020184632390737534, acc: 0.9971387982368469)
[2025-02-13 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:19][root][INFO] - Training Epoch: 2/2, step 3966/7134 completed (loss: 0.0128584373742342, acc: 0.994584858417511)
[2025-02-13 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20][root][INFO] - Training Epoch: 2/2, step 3967/7134 completed (loss: 0.06263598054647446, acc: 0.9861351847648621)
[2025-02-13 04:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20][root][INFO] - Training Epoch: 2/2, step 3968/7134 completed (loss: 0.022013789042830467, acc: 0.9921976327896118)
[2025-02-13 04:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20][root][INFO] - Training Epoch: 2/2, step 3969/7134 completed (loss: 0.032959435135126114, acc: 0.9895287752151489)
[2025-02-13 04:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:21][root][INFO] - Training Epoch: 2/2, step 3970/7134 completed (loss: 0.04452391713857651, acc: 0.9913669228553772)
[2025-02-13 04:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:21][root][INFO] - Training Epoch: 2/2, step 3971/7134 completed (loss: 0.09582260251045227, acc: 0.97826087474823)
[2025-02-13 04:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22][root][INFO] - Training Epoch: 2/2, step 3972/7134 completed (loss: 0.08455817401409149, acc: 0.9819587469100952)
[2025-02-13 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22][root][INFO] - Training Epoch: 2/2, step 3973/7134 completed (loss: 0.046393632888793945, acc: 0.984455943107605)
[2025-02-13 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:23][root][INFO] - Training Epoch: 2/2, step 3974/7134 completed (loss: 0.08977097272872925, acc: 0.9756097793579102)
[2025-02-13 04:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:23][root][INFO] - Training Epoch: 2/2, step 3975/7134 completed (loss: 0.027489766478538513, acc: 0.9879153966903687)
[2025-02-13 04:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:23][root][INFO] - Training Epoch: 2/2, step 3976/7134 completed (loss: 0.06909936666488647, acc: 0.983775794506073)
[2025-02-13 04:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24][root][INFO] - Training Epoch: 2/2, step 3977/7134 completed (loss: 0.011460947804152966, acc: 0.9965217113494873)
[2025-02-13 04:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24][root][INFO] - Training Epoch: 2/2, step 3978/7134 completed (loss: 0.012334811501204967, acc: 0.9974126815795898)
[2025-02-13 04:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:25][root][INFO] - Training Epoch: 2/2, step 3979/7134 completed (loss: 0.030644379556179047, acc: 0.9890710115432739)
[2025-02-13 04:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:25][root][INFO] - Training Epoch: 2/2, step 3980/7134 completed (loss: 0.02375122159719467, acc: 0.9939024448394775)
[2025-02-13 04:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26][root][INFO] - Training Epoch: 2/2, step 3981/7134 completed (loss: 0.0521005280315876, acc: 0.9862385392189026)
[2025-02-13 04:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26][root][INFO] - Training Epoch: 2/2, step 3982/7134 completed (loss: 0.07337594032287598, acc: 0.9799138903617859)
[2025-02-13 04:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26][root][INFO] - Training Epoch: 2/2, step 3983/7134 completed (loss: 0.2382499873638153, acc: 0.9510638117790222)
[2025-02-13 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:27][root][INFO] - Training Epoch: 2/2, step 3984/7134 completed (loss: 0.05565904080867767, acc: 0.9870129823684692)
[2025-02-13 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:27][root][INFO] - Training Epoch: 2/2, step 3985/7134 completed (loss: 0.01925015263259411, acc: 0.9946523904800415)
[2025-02-13 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28][root][INFO] - Training Epoch: 2/2, step 3986/7134 completed (loss: 0.1352745145559311, acc: 0.9712121486663818)
[2025-02-13 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28][root][INFO] - Training Epoch: 2/2, step 3987/7134 completed (loss: 0.04562172293663025, acc: 0.9904030561447144)
[2025-02-13 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29][root][INFO] - Training Epoch: 2/2, step 3988/7134 completed (loss: 0.057595089077949524, acc: 0.983565092086792)
[2025-02-13 04:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29][root][INFO] - Training Epoch: 2/2, step 3989/7134 completed (loss: 0.0812898725271225, acc: 0.9829424023628235)
[2025-02-13 04:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29][root][INFO] - Training Epoch: 2/2, step 3990/7134 completed (loss: 0.03234699368476868, acc: 0.9933664798736572)
[2025-02-13 04:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30][root][INFO] - Training Epoch: 2/2, step 3991/7134 completed (loss: 0.0251277107745409, acc: 0.9966996908187866)
[2025-02-13 04:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30][root][INFO] - Training Epoch: 2/2, step 3992/7134 completed (loss: 0.03364618495106697, acc: 0.9945945739746094)
[2025-02-13 04:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31][root][INFO] - Training Epoch: 2/2, step 3993/7134 completed (loss: 0.0452071912586689, acc: 0.9885321259498596)
[2025-02-13 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31][root][INFO] - Training Epoch: 2/2, step 3994/7134 completed (loss: 0.04725707322359085, acc: 0.9928571581840515)
[2025-02-13 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32][root][INFO] - Training Epoch: 2/2, step 3995/7134 completed (loss: 0.0790574699640274, acc: 0.9825581312179565)
[2025-02-13 04:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32][root][INFO] - Training Epoch: 2/2, step 3996/7134 completed (loss: 0.02902543917298317, acc: 0.9938398599624634)
[2025-02-13 04:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32][root][INFO] - Training Epoch: 2/2, step 3997/7134 completed (loss: 0.1097392737865448, acc: 0.9802197813987732)
[2025-02-13 04:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:33][root][INFO] - Training Epoch: 2/2, step 3998/7134 completed (loss: 0.07994440943002701, acc: 0.9781181812286377)
[2025-02-13 04:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:33][root][INFO] - Training Epoch: 2/2, step 3999/7134 completed (loss: 0.03150898218154907, acc: 0.9900199770927429)
[2025-02-13 04:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34][root][INFO] - Training Epoch: 2/2, step 4000/7134 completed (loss: 0.023463567718863487, acc: 0.992277979850769)
[2025-02-13 04:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34][root][INFO] - Training Epoch: 2/2, step 4001/7134 completed (loss: 0.04649540036916733, acc: 0.9821073412895203)
[2025-02-13 04:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34][root][INFO] - Training Epoch: 2/2, step 4002/7134 completed (loss: 0.06017559394240379, acc: 0.983132541179657)
[2025-02-13 04:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:35][root][INFO] - Training Epoch: 2/2, step 4003/7134 completed (loss: 0.067908875644207, acc: 0.9797047972679138)
[2025-02-13 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:35][root][INFO] - Training Epoch: 2/2, step 4004/7134 completed (loss: 0.03454890847206116, acc: 0.9930070042610168)
[2025-02-13 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36][root][INFO] - Training Epoch: 2/2, step 4005/7134 completed (loss: 0.014981962740421295, acc: 0.9968602657318115)
[2025-02-13 04:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36][root][INFO] - Training Epoch: 2/2, step 4006/7134 completed (loss: 0.01805703714489937, acc: 0.9927158951759338)
[2025-02-13 04:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36][root][INFO] - Training Epoch: 2/2, step 4007/7134 completed (loss: 0.021696163341403008, acc: 0.9925037622451782)
[2025-02-13 04:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:37][root][INFO] - Training Epoch: 2/2, step 4008/7134 completed (loss: 0.017049001529812813, acc: 0.9936575293540955)
[2025-02-13 04:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:37][root][INFO] - Training Epoch: 2/2, step 4009/7134 completed (loss: 0.012808725237846375, acc: 0.9960052967071533)
[2025-02-13 04:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38][root][INFO] - Training Epoch: 2/2, step 4010/7134 completed (loss: 0.009245336055755615, acc: 0.9977628588676453)
[2025-02-13 04:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38][root][INFO] - Training Epoch: 2/2, step 4011/7134 completed (loss: 0.02178300730884075, acc: 0.9973439574241638)
[2025-02-13 04:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:39][root][INFO] - Training Epoch: 2/2, step 4012/7134 completed (loss: 0.006052118260413408, acc: 1.0)
[2025-02-13 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:39][root][INFO] - Training Epoch: 2/2, step 4013/7134 completed (loss: 0.0048165093176066875, acc: 0.9983922839164734)
[2025-02-13 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:40][root][INFO] - Training Epoch: 2/2, step 4014/7134 completed (loss: 0.016401028260588646, acc: 0.9968652129173279)
[2025-02-13 04:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:40][root][INFO] - Training Epoch: 2/2, step 4015/7134 completed (loss: 0.009284120053052902, acc: 0.996927797794342)
[2025-02-13 04:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41][root][INFO] - Training Epoch: 2/2, step 4016/7134 completed (loss: 0.025316253304481506, acc: 0.9910714030265808)
[2025-02-13 04:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41][root][INFO] - Training Epoch: 2/2, step 4017/7134 completed (loss: 0.019292635843157768, acc: 0.994226336479187)
[2025-02-13 04:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41][root][INFO] - Training Epoch: 2/2, step 4018/7134 completed (loss: 0.022386083379387856, acc: 0.9939393997192383)
[2025-02-13 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:42][root][INFO] - Training Epoch: 2/2, step 4019/7134 completed (loss: 0.038627706468105316, acc: 0.9923858046531677)
[2025-02-13 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:42][root][INFO] - Training Epoch: 2/2, step 4020/7134 completed (loss: 0.02603042870759964, acc: 0.9890710115432739)
[2025-02-13 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43][root][INFO] - Training Epoch: 2/2, step 4021/7134 completed (loss: 0.01903541199862957, acc: 0.987522304058075)
[2025-02-13 04:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43][root][INFO] - Training Epoch: 2/2, step 4022/7134 completed (loss: 0.007622153032571077, acc: 0.9975460171699524)
[2025-02-13 04:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44][root][INFO] - Training Epoch: 2/2, step 4023/7134 completed (loss: 0.022632241249084473, acc: 0.9911949634552002)
[2025-02-13 04:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44][root][INFO] - Training Epoch: 2/2, step 4024/7134 completed (loss: 0.022529659792780876, acc: 0.9934383034706116)
[2025-02-13 04:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44][root][INFO] - Training Epoch: 2/2, step 4025/7134 completed (loss: 0.05430302768945694, acc: 0.9864099621772766)
[2025-02-13 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45][root][INFO] - Training Epoch: 2/2, step 4026/7134 completed (loss: 0.061342835426330566, acc: 0.983775794506073)
[2025-02-13 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45][root][INFO] - Training Epoch: 2/2, step 4027/7134 completed (loss: 0.035448890179395676, acc: 0.9904076457023621)
[2025-02-13 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:46][root][INFO] - Training Epoch: 2/2, step 4028/7134 completed (loss: 0.03293007239699364, acc: 0.9913941621780396)
[2025-02-13 04:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:46][root][INFO] - Training Epoch: 2/2, step 4029/7134 completed (loss: 0.010277435183525085, acc: 0.9963099360466003)
[2025-02-13 04:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47][root][INFO] - Training Epoch: 2/2, step 4030/7134 completed (loss: 0.009553970769047737, acc: 0.9938398599624634)
[2025-02-13 04:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47][root][INFO] - Training Epoch: 2/2, step 4031/7134 completed (loss: 0.010468985885381699, acc: 0.9970674514770508)
[2025-02-13 04:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:48][root][INFO] - Training Epoch: 2/2, step 4032/7134 completed (loss: 0.0527571365237236, acc: 0.9906291961669922)
[2025-02-13 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:48][root][INFO] - Training Epoch: 2/2, step 4033/7134 completed (loss: 0.008957292884588242, acc: 0.99622642993927)
[2025-02-13 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:48][root][INFO] - Training Epoch: 2/2, step 4034/7134 completed (loss: 0.0029933424666523933, acc: 1.0)
[2025-02-13 04:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49][root][INFO] - Training Epoch: 2/2, step 4035/7134 completed (loss: 0.023349881172180176, acc: 0.9940968155860901)
[2025-02-13 04:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49][root][INFO] - Training Epoch: 2/2, step 4036/7134 completed (loss: 0.022911809384822845, acc: 0.9926739931106567)
[2025-02-13 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:50][root][INFO] - Training Epoch: 2/2, step 4037/7134 completed (loss: 0.012505905702710152, acc: 0.9963898658752441)
[2025-02-13 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:50][root][INFO] - Training Epoch: 2/2, step 4038/7134 completed (loss: 0.023796137422323227, acc: 0.9958734512329102)
[2025-02-13 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51][root][INFO] - Training Epoch: 2/2, step 4039/7134 completed (loss: 0.02824402041733265, acc: 0.9900709390640259)
[2025-02-13 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51][root][INFO] - Training Epoch: 2/2, step 4040/7134 completed (loss: 0.008542703464627266, acc: 0.9987684488296509)
[2025-02-13 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52][root][INFO] - Training Epoch: 2/2, step 4041/7134 completed (loss: 0.05209775269031525, acc: 0.9879032373428345)
[2025-02-13 04:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52][root][INFO] - Training Epoch: 2/2, step 4042/7134 completed (loss: 0.02214740961790085, acc: 0.9926470518112183)
[2025-02-13 04:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52][root][INFO] - Training Epoch: 2/2, step 4043/7134 completed (loss: 0.0124044893309474, acc: 0.9941262602806091)
[2025-02-13 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:53][root][INFO] - Training Epoch: 2/2, step 4044/7134 completed (loss: 0.00917151290923357, acc: 0.9962121248245239)
[2025-02-13 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:53][root][INFO] - Training Epoch: 2/2, step 4045/7134 completed (loss: 0.028147872537374496, acc: 0.9916666746139526)
[2025-02-13 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54][root][INFO] - Training Epoch: 2/2, step 4046/7134 completed (loss: 0.023699836805462837, acc: 0.9917582273483276)
[2025-02-13 04:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54][root][INFO] - Training Epoch: 2/2, step 4047/7134 completed (loss: 0.036912862211465836, acc: 0.9916467666625977)
[2025-02-13 04:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:55][root][INFO] - Training Epoch: 2/2, step 4048/7134 completed (loss: 0.020575430244207382, acc: 0.9941725134849548)
[2025-02-13 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:55][root][INFO] - Training Epoch: 2/2, step 4049/7134 completed (loss: 0.026346461847424507, acc: 0.9931034445762634)
[2025-02-13 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:56][root][INFO] - Training Epoch: 2/2, step 4050/7134 completed (loss: 0.020741956308484077, acc: 0.9957447052001953)
[2025-02-13 04:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:56][root][INFO] - Training Epoch: 2/2, step 4051/7134 completed (loss: 0.05364786088466644, acc: 0.9880383014678955)
[2025-02-13 04:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:56][root][INFO] - Training Epoch: 2/2, step 4052/7134 completed (loss: 0.014387193135917187, acc: 0.9948320388793945)
[2025-02-13 04:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57][root][INFO] - Training Epoch: 2/2, step 4053/7134 completed (loss: 0.020154496654868126, acc: 0.9920424222946167)
[2025-02-13 04:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57][root][INFO] - Training Epoch: 2/2, step 4054/7134 completed (loss: 0.034491200000047684, acc: 0.9959999918937683)
[2025-02-13 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:58][root][INFO] - Training Epoch: 2/2, step 4055/7134 completed (loss: 0.01909521222114563, acc: 0.9927184581756592)
[2025-02-13 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:58][root][INFO] - Training Epoch: 2/2, step 4056/7134 completed (loss: 0.01412937417626381, acc: 0.9939393997192383)
[2025-02-13 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:59][root][INFO] - Training Epoch: 2/2, step 4057/7134 completed (loss: 0.04127251356840134, acc: 0.9930459260940552)
[2025-02-13 04:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:59][root][INFO] - Training Epoch: 2/2, step 4058/7134 completed (loss: 0.02747321128845215, acc: 0.9885277152061462)
[2025-02-13 04:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00][root][INFO] - Training Epoch: 2/2, step 4059/7134 completed (loss: 0.021982045844197273, acc: 0.9945255517959595)
[2025-02-13 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00][root][INFO] - Training Epoch: 2/2, step 4060/7134 completed (loss: 0.006691832561045885, acc: 0.9984543919563293)
[2025-02-13 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00][root][INFO] - Training Epoch: 2/2, step 4061/7134 completed (loss: 0.01653268188238144, acc: 0.994301974773407)
[2025-02-13 04:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:01][root][INFO] - Training Epoch: 2/2, step 4062/7134 completed (loss: 0.005589569453150034, acc: 0.9984251856803894)
[2025-02-13 04:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:01][root][INFO] - Training Epoch: 2/2, step 4063/7134 completed (loss: 0.014919928275048733, acc: 0.9946996569633484)
[2025-02-13 04:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02][root][INFO] - Training Epoch: 2/2, step 4064/7134 completed (loss: 0.003430224722251296, acc: 1.0)
[2025-02-13 04:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02][root][INFO] - Training Epoch: 2/2, step 4065/7134 completed (loss: 0.011114547960460186, acc: 0.9972413778305054)
[2025-02-13 04:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02][root][INFO] - Training Epoch: 2/2, step 4066/7134 completed (loss: 0.0030082776211202145, acc: 1.0)
[2025-02-13 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:03][root][INFO] - Training Epoch: 2/2, step 4067/7134 completed (loss: 0.02780182845890522, acc: 0.9950980544090271)
[2025-02-13 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:03][root][INFO] - Training Epoch: 2/2, step 4068/7134 completed (loss: 0.03164957836270332, acc: 0.9925925731658936)
[2025-02-13 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04][root][INFO] - Training Epoch: 2/2, step 4069/7134 completed (loss: 0.012764896266162395, acc: 0.9968701004981995)
[2025-02-13 04:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04][root][INFO] - Training Epoch: 2/2, step 4070/7134 completed (loss: 0.01823190040886402, acc: 0.9939117431640625)
[2025-02-13 04:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:05][root][INFO] - Training Epoch: 2/2, step 4071/7134 completed (loss: 0.015878744423389435, acc: 0.9950576424598694)
[2025-02-13 04:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:05][root][INFO] - Training Epoch: 2/2, step 4072/7134 completed (loss: 0.0077974833548069, acc: 0.9956709742546082)
[2025-02-13 04:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:05][root][INFO] - Training Epoch: 2/2, step 4073/7134 completed (loss: 0.012852353975176811, acc: 0.9956896305084229)
[2025-02-13 04:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:06][root][INFO] - Training Epoch: 2/2, step 4074/7134 completed (loss: 0.020587975159287453, acc: 0.9942775368690491)
[2025-02-13 04:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:06][root][INFO] - Training Epoch: 2/2, step 4075/7134 completed (loss: 0.00893884152173996, acc: 0.995502233505249)
[2025-02-13 04:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:07][root][INFO] - Training Epoch: 2/2, step 4076/7134 completed (loss: 0.008602956309914589, acc: 0.9971056580543518)
[2025-02-13 04:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:07][root][INFO] - Training Epoch: 2/2, step 4077/7134 completed (loss: 0.02873631753027439, acc: 0.9915397763252258)
[2025-02-13 04:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08][root][INFO] - Training Epoch: 2/2, step 4078/7134 completed (loss: 0.015170521102845669, acc: 0.995726466178894)
[2025-02-13 04:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08][root][INFO] - Training Epoch: 2/2, step 4079/7134 completed (loss: 0.012504538521170616, acc: 0.9946523904800415)
[2025-02-13 04:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08][root][INFO] - Training Epoch: 2/2, step 4080/7134 completed (loss: 0.008584808558225632, acc: 0.9984177350997925)
[2025-02-13 04:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:09][root][INFO] - Training Epoch: 2/2, step 4081/7134 completed (loss: 0.004098235163837671, acc: 1.0)
[2025-02-13 04:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:09][root][INFO] - Training Epoch: 2/2, step 4082/7134 completed (loss: 0.039821650832891464, acc: 0.9815157055854797)
[2025-02-13 04:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:10][root][INFO] - Training Epoch: 2/2, step 4083/7134 completed (loss: 0.03578662499785423, acc: 0.9895209670066833)
[2025-02-13 04:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:10][root][INFO] - Training Epoch: 2/2, step 4084/7134 completed (loss: 0.061350662261247635, acc: 0.9745097756385803)
[2025-02-13 04:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11][root][INFO] - Training Epoch: 2/2, step 4085/7134 completed (loss: 0.025084875524044037, acc: 0.9919137358665466)
[2025-02-13 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11][root][INFO] - Training Epoch: 2/2, step 4086/7134 completed (loss: 0.03917902708053589, acc: 0.9885433912277222)
[2025-02-13 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11][root][INFO] - Training Epoch: 2/2, step 4087/7134 completed (loss: 0.01952611468732357, acc: 0.9961240291595459)
[2025-02-13 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:12][root][INFO] - Training Epoch: 2/2, step 4088/7134 completed (loss: 0.06314777582883835, acc: 0.9838308691978455)
[2025-02-13 04:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:12][root][INFO] - Training Epoch: 2/2, step 4089/7134 completed (loss: 0.04684382677078247, acc: 0.9857904314994812)
[2025-02-13 04:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:13][root][INFO] - Training Epoch: 2/2, step 4090/7134 completed (loss: 0.07813021540641785, acc: 0.980211079120636)
[2025-02-13 04:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:13][root][INFO] - Training Epoch: 2/2, step 4091/7134 completed (loss: 0.09517472982406616, acc: 0.9747048616409302)
[2025-02-13 04:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14][root][INFO] - Training Epoch: 2/2, step 4092/7134 completed (loss: 0.04332375153899193, acc: 0.992277979850769)
[2025-02-13 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14][root][INFO] - Training Epoch: 2/2, step 4093/7134 completed (loss: 0.025948958471417427, acc: 0.9927140474319458)
[2025-02-13 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14][root][INFO] - Training Epoch: 2/2, step 4094/7134 completed (loss: 0.04316918924450874, acc: 0.9879879951477051)
[2025-02-13 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:15][root][INFO] - Training Epoch: 2/2, step 4095/7134 completed (loss: 0.01353996992111206, acc: 0.9971056580543518)
[2025-02-13 04:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:15][root][INFO] - Training Epoch: 2/2, step 4096/7134 completed (loss: 0.05776657536625862, acc: 0.9910447597503662)
[2025-02-13 04:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:16][root][INFO] - Training Epoch: 2/2, step 4097/7134 completed (loss: 0.04229870066046715, acc: 0.9903225898742676)
[2025-02-13 04:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:16][root][INFO] - Training Epoch: 2/2, step 4098/7134 completed (loss: 0.025773636996746063, acc: 0.9932249188423157)
[2025-02-13 04:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:16][root][INFO] - Training Epoch: 2/2, step 4099/7134 completed (loss: 0.023904312402009964, acc: 0.9911971688270569)
[2025-02-13 04:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17][root][INFO] - Training Epoch: 2/2, step 4100/7134 completed (loss: 0.010612133890390396, acc: 0.9969742894172668)
[2025-02-13 04:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17][root][INFO] - Training Epoch: 2/2, step 4101/7134 completed (loss: 0.012366420589387417, acc: 0.9961977005004883)
[2025-02-13 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:18][root][INFO] - Training Epoch: 2/2, step 4102/7134 completed (loss: 0.013801418244838715, acc: 0.9945155382156372)
[2025-02-13 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:18][root][INFO] - Training Epoch: 2/2, step 4103/7134 completed (loss: 0.002463642042130232, acc: 1.0)
[2025-02-13 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:19][root][INFO] - Training Epoch: 2/2, step 4104/7134 completed (loss: 0.0046079461462795734, acc: 1.0)
[2025-02-13 04:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:19][root][INFO] - Training Epoch: 2/2, step 4105/7134 completed (loss: 0.013640562072396278, acc: 0.995468258857727)
[2025-02-13 04:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:19][root][INFO] - Training Epoch: 2/2, step 4106/7134 completed (loss: 0.018992284312844276, acc: 0.9923312664031982)
[2025-02-13 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20][root][INFO] - Training Epoch: 2/2, step 4107/7134 completed (loss: 0.022362830117344856, acc: 0.9966386556625366)
[2025-02-13 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20][root][INFO] - Training Epoch: 2/2, step 4108/7134 completed (loss: 0.029438534751534462, acc: 0.9935275316238403)
[2025-02-13 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:21][root][INFO] - Training Epoch: 2/2, step 4109/7134 completed (loss: 0.014464975334703922, acc: 0.9967897534370422)
[2025-02-13 04:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:21][root][INFO] - Training Epoch: 2/2, step 4110/7134 completed (loss: 0.03333617001771927, acc: 0.9965986609458923)
[2025-02-13 04:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:22][root][INFO] - Training Epoch: 2/2, step 4111/7134 completed (loss: 0.007672619540244341, acc: 0.9984227418899536)
[2025-02-13 04:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:22][root][INFO] - Training Epoch: 2/2, step 4112/7134 completed (loss: 0.006482921540737152, acc: 0.9982078671455383)
[2025-02-13 04:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23][root][INFO] - Training Epoch: 2/2, step 4113/7134 completed (loss: 0.010246144607663155, acc: 0.9953488111495972)
[2025-02-13 04:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23][root][INFO] - Training Epoch: 2/2, step 4114/7134 completed (loss: 0.0710413008928299, acc: 0.9817073345184326)
[2025-02-13 04:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23][root][INFO] - Training Epoch: 2/2, step 4115/7134 completed (loss: 0.018190165981650352, acc: 0.9925373196601868)
[2025-02-13 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:24][root][INFO] - Training Epoch: 2/2, step 4116/7134 completed (loss: 0.0025019459426403046, acc: 1.0)
[2025-02-13 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:24][root][INFO] - Training Epoch: 2/2, step 4117/7134 completed (loss: 0.011119100265204906, acc: 0.9968454241752625)
[2025-02-13 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:25][root][INFO] - Training Epoch: 2/2, step 4118/7134 completed (loss: 0.015918677672743797, acc: 0.9926289916038513)
[2025-02-13 04:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:25][root][INFO] - Training Epoch: 2/2, step 4119/7134 completed (loss: 0.040664639323949814, acc: 0.9919224381446838)
[2025-02-13 04:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:25][root][INFO] - Training Epoch: 2/2, step 4120/7134 completed (loss: 0.026274044066667557, acc: 0.9950576424598694)
[2025-02-13 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26][root][INFO] - Training Epoch: 2/2, step 4121/7134 completed (loss: 0.035611774772405624, acc: 0.9928977489471436)
[2025-02-13 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26][root][INFO] - Training Epoch: 2/2, step 4122/7134 completed (loss: 0.025699682533740997, acc: 0.9961832165718079)
[2025-02-13 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:27][root][INFO] - Training Epoch: 2/2, step 4123/7134 completed (loss: 0.02189679443836212, acc: 0.9969834089279175)
[2025-02-13 04:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:27][root][INFO] - Training Epoch: 2/2, step 4124/7134 completed (loss: 0.012265240773558617, acc: 0.9957567453384399)
[2025-02-13 04:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28][root][INFO] - Training Epoch: 2/2, step 4125/7134 completed (loss: 0.015802733600139618, acc: 0.9955882430076599)
[2025-02-13 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28][root][INFO] - Training Epoch: 2/2, step 4126/7134 completed (loss: 0.009772758930921555, acc: 0.9985835552215576)
[2025-02-13 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28][root][INFO] - Training Epoch: 2/2, step 4127/7134 completed (loss: 0.022151771932840347, acc: 0.9906250238418579)
[2025-02-13 04:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:29][root][INFO] - Training Epoch: 2/2, step 4128/7134 completed (loss: 0.015393940731883049, acc: 0.9953051805496216)
[2025-02-13 04:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:29][root][INFO] - Training Epoch: 2/2, step 4129/7134 completed (loss: 0.0074083139188587666, acc: 0.997032642364502)
[2025-02-13 04:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:30][root][INFO] - Training Epoch: 2/2, step 4130/7134 completed (loss: 0.010447993874549866, acc: 0.9961464405059814)
[2025-02-13 04:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:30][root][INFO] - Training Epoch: 2/2, step 4131/7134 completed (loss: 0.01308487169444561, acc: 0.996303141117096)
[2025-02-13 04:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31][root][INFO] - Training Epoch: 2/2, step 4132/7134 completed (loss: 0.011901294812560081, acc: 0.9910714030265808)
[2025-02-13 04:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31][root][INFO] - Training Epoch: 2/2, step 4133/7134 completed (loss: 0.023343754932284355, acc: 0.9907407164573669)
[2025-02-13 04:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31][root][INFO] - Training Epoch: 2/2, step 4134/7134 completed (loss: 0.03790756314992905, acc: 0.9834123253822327)
[2025-02-13 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:32][root][INFO] - Training Epoch: 2/2, step 4135/7134 completed (loss: 0.015652989968657494, acc: 0.9946714043617249)
[2025-02-13 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:32][root][INFO] - Training Epoch: 2/2, step 4136/7134 completed (loss: 0.0032743944320827723, acc: 1.0)
[2025-02-13 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33][root][INFO] - Training Epoch: 2/2, step 4137/7134 completed (loss: 0.02243255265057087, acc: 0.990234375)
[2025-02-13 04:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33][root][INFO] - Training Epoch: 2/2, step 4138/7134 completed (loss: 0.02297537960112095, acc: 0.9921630024909973)
[2025-02-13 04:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34][root][INFO] - Training Epoch: 2/2, step 4139/7134 completed (loss: 0.018001196905970573, acc: 0.9978540539741516)
[2025-02-13 04:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34][root][INFO] - Training Epoch: 2/2, step 4140/7134 completed (loss: 0.03950270637869835, acc: 0.9931034445762634)
[2025-02-13 04:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34][root][INFO] - Training Epoch: 2/2, step 4141/7134 completed (loss: 0.03421073034405708, acc: 0.9899713397026062)
[2025-02-13 04:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:35][root][INFO] - Training Epoch: 2/2, step 4142/7134 completed (loss: 0.06390935182571411, acc: 0.986270010471344)
[2025-02-13 04:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:35][root][INFO] - Training Epoch: 2/2, step 4143/7134 completed (loss: 0.04083874449133873, acc: 0.9948979616165161)
[2025-02-13 04:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36][root][INFO] - Training Epoch: 2/2, step 4144/7134 completed (loss: 0.04876071959733963, acc: 0.9851936101913452)
[2025-02-13 04:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36][root][INFO] - Training Epoch: 2/2, step 4145/7134 completed (loss: 0.02070333994925022, acc: 0.994584858417511)
[2025-02-13 04:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:37][root][INFO] - Training Epoch: 2/2, step 4146/7134 completed (loss: 0.06880505383014679, acc: 0.9838383793830872)
[2025-02-13 04:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:37][root][INFO] - Training Epoch: 2/2, step 4147/7134 completed (loss: 0.061116527765989304, acc: 0.984674334526062)
[2025-02-13 04:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:38][root][INFO] - Training Epoch: 2/2, step 4148/7134 completed (loss: 0.10129096359014511, acc: 0.9749034643173218)
[2025-02-13 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:38][root][INFO] - Training Epoch: 2/2, step 4149/7134 completed (loss: 0.08598344027996063, acc: 0.9786666631698608)
[2025-02-13 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:38][root][INFO] - Training Epoch: 2/2, step 4150/7134 completed (loss: 0.09157445281744003, acc: 0.9678068161010742)
[2025-02-13 04:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:39][root][INFO] - Training Epoch: 2/2, step 4151/7134 completed (loss: 0.10701238363981247, acc: 0.9836956262588501)
[2025-02-13 04:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:39][root][INFO] - Training Epoch: 2/2, step 4152/7134 completed (loss: 0.02966783009469509, acc: 0.9876352548599243)
[2025-02-13 04:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40][root][INFO] - Training Epoch: 2/2, step 4153/7134 completed (loss: 0.04631742089986801, acc: 0.987730085849762)
[2025-02-13 04:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40][root][INFO] - Training Epoch: 2/2, step 4154/7134 completed (loss: 0.04066159948706627, acc: 0.9861111044883728)
[2025-02-13 04:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:41][root][INFO] - Training Epoch: 2/2, step 4155/7134 completed (loss: 0.04715650528669357, acc: 0.9862778782844543)
[2025-02-13 04:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:41][root][INFO] - Training Epoch: 2/2, step 4156/7134 completed (loss: 0.0371558740735054, acc: 0.9874826073646545)
[2025-02-13 04:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42][root][INFO] - Training Epoch: 2/2, step 4157/7134 completed (loss: 0.04665105417370796, acc: 0.9880239367485046)
[2025-02-13 04:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42][root][INFO] - Training Epoch: 2/2, step 4158/7134 completed (loss: 0.11239568889141083, acc: 0.9768637418746948)
[2025-02-13 04:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42][root][INFO] - Training Epoch: 2/2, step 4159/7134 completed (loss: 0.01788690872490406, acc: 0.9983022212982178)
[2025-02-13 04:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:43][root][INFO] - Training Epoch: 2/2, step 4160/7134 completed (loss: 0.03853479027748108, acc: 0.9849056601524353)
[2025-02-13 04:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:43][root][INFO] - Training Epoch: 2/2, step 4161/7134 completed (loss: 0.02561134845018387, acc: 0.9939758777618408)
[2025-02-13 04:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44][root][INFO] - Training Epoch: 2/2, step 4162/7134 completed (loss: 0.05097101256251335, acc: 0.9836065769195557)
[2025-02-13 04:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44][root][INFO] - Training Epoch: 2/2, step 4163/7134 completed (loss: 0.049656469374895096, acc: 0.9848713874816895)
[2025-02-13 04:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:45][root][INFO] - Training Epoch: 2/2, step 4164/7134 completed (loss: 0.04222773760557175, acc: 0.9883889555931091)
[2025-02-13 04:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:45][root][INFO] - Training Epoch: 2/2, step 4165/7134 completed (loss: 0.047057487070560455, acc: 0.9850187301635742)
[2025-02-13 04:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:46][root][INFO] - Training Epoch: 2/2, step 4166/7134 completed (loss: 0.03929715231060982, acc: 0.988252580165863)
[2025-02-13 04:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:46][root][INFO] - Training Epoch: 2/2, step 4167/7134 completed (loss: 0.0054807839915156364, acc: 1.0)
[2025-02-13 04:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47][root][INFO] - Training Epoch: 2/2, step 4168/7134 completed (loss: 0.033254507929086685, acc: 0.9913194179534912)
[2025-02-13 04:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47][root][INFO] - Training Epoch: 2/2, step 4169/7134 completed (loss: 0.0467195026576519, acc: 0.9873284101486206)
[2025-02-13 04:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47][root][INFO] - Training Epoch: 2/2, step 4170/7134 completed (loss: 0.022088738158345222, acc: 0.9957537055015564)
[2025-02-13 04:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:48][root][INFO] - Training Epoch: 2/2, step 4171/7134 completed (loss: 0.013899487443268299, acc: 0.9942330121994019)
[2025-02-13 04:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:48][root][INFO] - Training Epoch: 2/2, step 4172/7134 completed (loss: 0.040972478687763214, acc: 0.9925187230110168)
[2025-02-13 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49][root][INFO] - Training Epoch: 2/2, step 4173/7134 completed (loss: 0.037418823689222336, acc: 0.9935622215270996)
[2025-02-13 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49][root][INFO] - Training Epoch: 2/2, step 4174/7134 completed (loss: 0.003059553448110819, acc: 1.0)
[2025-02-13 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:50][root][INFO] - Training Epoch: 2/2, step 4175/7134 completed (loss: 0.01862975023686886, acc: 0.9948453903198242)
[2025-02-13 04:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:50][root][INFO] - Training Epoch: 2/2, step 4176/7134 completed (loss: 0.016796890646219254, acc: 0.995529055595398)
[2025-02-13 04:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51][root][INFO] - Training Epoch: 2/2, step 4177/7134 completed (loss: 0.01004647184163332, acc: 0.9973718523979187)
[2025-02-13 04:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51][root][INFO] - Training Epoch: 2/2, step 4178/7134 completed (loss: 0.02394530363380909, acc: 0.9848942756652832)
[2025-02-13 04:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:52][root][INFO] - Training Epoch: 2/2, step 4179/7134 completed (loss: 0.009025506675243378, acc: 0.9973753094673157)
[2025-02-13 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:52][root][INFO] - Training Epoch: 2/2, step 4180/7134 completed (loss: 0.01033399160951376, acc: 0.9985875487327576)
[2025-02-13 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:52][root][INFO] - Training Epoch: 2/2, step 4181/7134 completed (loss: 0.006420353893190622, acc: 0.9967373609542847)
[2025-02-13 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53][root][INFO] - Training Epoch: 2/2, step 4182/7134 completed (loss: 0.0042485264129936695, acc: 1.0)
[2025-02-13 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53][root][INFO] - Training Epoch: 2/2, step 4183/7134 completed (loss: 0.006788343191146851, acc: 0.9972183704376221)
[2025-02-13 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54][root][INFO] - Training Epoch: 2/2, step 4184/7134 completed (loss: 0.02213378995656967, acc: 0.9960212111473083)
[2025-02-13 04:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54][root][INFO] - Training Epoch: 2/2, step 4185/7134 completed (loss: 0.011810583993792534, acc: 0.9931507110595703)
[2025-02-13 04:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:55][root][INFO] - Training Epoch: 2/2, step 4186/7134 completed (loss: 0.06175032630562782, acc: 0.9891696572303772)
[2025-02-13 04:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:55][root][INFO] - Training Epoch: 2/2, step 4187/7134 completed (loss: 0.0076604485511779785, acc: 0.998046875)
[2025-02-13 04:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:56][root][INFO] - Training Epoch: 2/2, step 4188/7134 completed (loss: 0.008925206027925014, acc: 0.9962499737739563)
[2025-02-13 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:56][root][INFO] - Training Epoch: 2/2, step 4189/7134 completed (loss: 0.018317850306630135, acc: 0.9986013770103455)
[2025-02-13 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:56][root][INFO] - Training Epoch: 2/2, step 4190/7134 completed (loss: 0.007471950259059668, acc: 0.9971590638160706)
[2025-02-13 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57][root][INFO] - Training Epoch: 2/2, step 4191/7134 completed (loss: 0.029063694179058075, acc: 0.9953917264938354)
[2025-02-13 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57][root][INFO] - Training Epoch: 2/2, step 4192/7134 completed (loss: 0.015555032528936863, acc: 0.995207667350769)
[2025-02-13 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:58][root][INFO] - Training Epoch: 2/2, step 4193/7134 completed (loss: 0.0159586314111948, acc: 0.9973261952400208)
[2025-02-13 04:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:58][root][INFO] - Training Epoch: 2/2, step 4194/7134 completed (loss: 0.011565804481506348, acc: 0.9954614043235779)
[2025-02-13 04:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59][root][INFO] - Training Epoch: 2/2, step 4195/7134 completed (loss: 0.0265173502266407, acc: 0.989159882068634)
[2025-02-13 04:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59][root][INFO] - Training Epoch: 2/2, step 4196/7134 completed (loss: 0.007508225739002228, acc: 0.9982699155807495)
[2025-02-13 04:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59][root][INFO] - Training Epoch: 2/2, step 4197/7134 completed (loss: 0.020602943375706673, acc: 0.9948253631591797)
[2025-02-13 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:00][root][INFO] - Training Epoch: 2/2, step 4198/7134 completed (loss: 0.016365105286240578, acc: 0.9972222447395325)
[2025-02-13 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:00][root][INFO] - Training Epoch: 2/2, step 4199/7134 completed (loss: 0.01152864657342434, acc: 0.9963503479957581)
[2025-02-13 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01][root][INFO] - Training Epoch: 2/2, step 4200/7134 completed (loss: 0.02488943748176098, acc: 0.9928315281867981)
[2025-02-13 04:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01][root][INFO] - Training Epoch: 2/2, step 4201/7134 completed (loss: 0.019092116504907608, acc: 0.9965753555297852)
[2025-02-13 04:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02][root][INFO] - Training Epoch: 2/2, step 4202/7134 completed (loss: 0.03993704915046692, acc: 0.9864341020584106)
[2025-02-13 04:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02][root][INFO] - Training Epoch: 2/2, step 4203/7134 completed (loss: 0.027945322915911674, acc: 0.9921996593475342)
[2025-02-13 04:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02][root][INFO] - Training Epoch: 2/2, step 4204/7134 completed (loss: 0.05614956095814705, acc: 0.9853801131248474)
[2025-02-13 04:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03][root][INFO] - Training Epoch: 2/2, step 4205/7134 completed (loss: 0.030715717002749443, acc: 0.9913669228553772)
[2025-02-13 04:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03][root][INFO] - Training Epoch: 2/2, step 4206/7134 completed (loss: 0.042807791382074356, acc: 0.9929478168487549)
[2025-02-13 04:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:04][root][INFO] - Training Epoch: 2/2, step 4207/7134 completed (loss: 0.049713168293237686, acc: 0.993630588054657)
[2025-02-13 04:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:04][root][INFO] - Training Epoch: 2/2, step 4208/7134 completed (loss: 0.040622077882289886, acc: 0.9888357520103455)
[2025-02-13 04:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05][root][INFO] - Training Epoch: 2/2, step 4209/7134 completed (loss: 0.029560048133134842, acc: 0.9915013909339905)
[2025-02-13 04:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05][root][INFO] - Training Epoch: 2/2, step 4210/7134 completed (loss: 0.005197226069867611, acc: 1.0)
[2025-02-13 04:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05][root][INFO] - Training Epoch: 2/2, step 4211/7134 completed (loss: 0.012362408451735973, acc: 0.9953917264938354)
[2025-02-13 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:06][root][INFO] - Training Epoch: 2/2, step 4212/7134 completed (loss: 0.01793157309293747, acc: 0.9934533834457397)
[2025-02-13 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:06][root][INFO] - Training Epoch: 2/2, step 4213/7134 completed (loss: 0.009089268743991852, acc: 0.9971387982368469)
[2025-02-13 04:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:07][root][INFO] - Training Epoch: 2/2, step 4214/7134 completed (loss: 0.04742557927966118, acc: 0.9880596995353699)
[2025-02-13 04:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:07][root][INFO] - Training Epoch: 2/2, step 4215/7134 completed (loss: 0.03039642982184887, acc: 0.9934065937995911)
[2025-02-13 04:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08][root][INFO] - Training Epoch: 2/2, step 4216/7134 completed (loss: 0.041038163006305695, acc: 0.9908592104911804)
[2025-02-13 04:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08][root][INFO] - Training Epoch: 2/2, step 4217/7134 completed (loss: 0.015522506088018417, acc: 0.9944055676460266)
[2025-02-13 04:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08][root][INFO] - Training Epoch: 2/2, step 4218/7134 completed (loss: 0.036216698586940765, acc: 0.9938144087791443)
[2025-02-13 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:09][root][INFO] - Training Epoch: 2/2, step 4219/7134 completed (loss: 0.018503081053495407, acc: 0.9879518151283264)
[2025-02-13 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:09][root][INFO] - Training Epoch: 2/2, step 4220/7134 completed (loss: 0.029974043369293213, acc: 0.9913669228553772)
[2025-02-13 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:10][root][INFO] - Training Epoch: 2/2, step 4221/7134 completed (loss: 0.013571665622293949, acc: 0.9964601993560791)
[2025-02-13 04:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:10][root][INFO] - Training Epoch: 2/2, step 4222/7134 completed (loss: 0.0771409422159195, acc: 0.9844444394111633)
[2025-02-13 04:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11][root][INFO] - Training Epoch: 2/2, step 4223/7134 completed (loss: 0.03629918023943901, acc: 0.9867549538612366)
[2025-02-13 04:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11][root][INFO] - Training Epoch: 2/2, step 4224/7134 completed (loss: 0.0500367172062397, acc: 0.9870410561561584)
[2025-02-13 04:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11][root][INFO] - Training Epoch: 2/2, step 4225/7134 completed (loss: 0.054729610681533813, acc: 0.9864176511764526)
[2025-02-13 04:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:12][root][INFO] - Training Epoch: 2/2, step 4226/7134 completed (loss: 0.04310191050171852, acc: 0.9885057210922241)
[2025-02-13 04:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:12][root][INFO] - Training Epoch: 2/2, step 4227/7134 completed (loss: 0.08297857642173767, acc: 0.9852579832077026)
[2025-02-13 04:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13][root][INFO] - Training Epoch: 2/2, step 4228/7134 completed (loss: 0.09150368720293045, acc: 0.9741379022598267)
[2025-02-13 04:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13][root][INFO] - Training Epoch: 2/2, step 4229/7134 completed (loss: 0.06170392036437988, acc: 0.9853249192237854)
[2025-02-13 04:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:14][root][INFO] - Training Epoch: 2/2, step 4230/7134 completed (loss: 0.0373823456466198, acc: 0.9882352948188782)
[2025-02-13 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:14][root][INFO] - Training Epoch: 2/2, step 4231/7134 completed (loss: 0.11361328512430191, acc: 0.9779005646705627)
[2025-02-13 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:14][root][INFO] - Training Epoch: 2/2, step 4232/7134 completed (loss: 0.08147026598453522, acc: 0.9755725264549255)
[2025-02-13 04:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15][root][INFO] - Training Epoch: 2/2, step 4233/7134 completed (loss: 0.05330450087785721, acc: 0.989983320236206)
[2025-02-13 04:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15][root][INFO] - Training Epoch: 2/2, step 4234/7134 completed (loss: 0.051541343331336975, acc: 0.9888712167739868)
[2025-02-13 04:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:16][root][INFO] - Training Epoch: 2/2, step 4235/7134 completed (loss: 0.027332104742527008, acc: 0.9921011328697205)
[2025-02-13 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:16][root][INFO] - Training Epoch: 2/2, step 4236/7134 completed (loss: 0.008538968861103058, acc: 1.0)
[2025-02-13 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:17][root][INFO] - Training Epoch: 2/2, step 4237/7134 completed (loss: 0.012040560133755207, acc: 0.9979381561279297)
[2025-02-13 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:17][root][INFO] - Training Epoch: 2/2, step 4238/7134 completed (loss: 0.02613784745335579, acc: 0.9950900077819824)
[2025-02-13 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:17][root][INFO] - Training Epoch: 2/2, step 4239/7134 completed (loss: 0.014992032200098038, acc: 0.9968000054359436)
[2025-02-13 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18][root][INFO] - Training Epoch: 2/2, step 4240/7134 completed (loss: 0.038992561399936676, acc: 0.9903069734573364)
[2025-02-13 04:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18][root][INFO] - Training Epoch: 2/2, step 4241/7134 completed (loss: 0.015322326682507992, acc: 0.9956616163253784)
[2025-02-13 04:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:19][root][INFO] - Training Epoch: 2/2, step 4242/7134 completed (loss: 0.02683354541659355, acc: 0.9888641238212585)
[2025-02-13 04:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:19][root][INFO] - Training Epoch: 2/2, step 4243/7134 completed (loss: 0.051830362528562546, acc: 0.9891501069068909)
[2025-02-13 04:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:19][root][INFO] - Training Epoch: 2/2, step 4244/7134 completed (loss: 0.02221749536693096, acc: 0.9933481216430664)
[2025-02-13 04:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:20][root][INFO] - Training Epoch: 2/2, step 4245/7134 completed (loss: 0.029865693300962448, acc: 0.9926650524139404)
[2025-02-13 04:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:20][root][INFO] - Training Epoch: 2/2, step 4246/7134 completed (loss: 0.035916123539209366, acc: 0.9936608672142029)
[2025-02-13 04:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21][root][INFO] - Training Epoch: 2/2, step 4247/7134 completed (loss: 0.014023303054273129, acc: 0.9946523904800415)
[2025-02-13 04:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21][root][INFO] - Training Epoch: 2/2, step 4248/7134 completed (loss: 0.041498634964227676, acc: 0.9887820482254028)
[2025-02-13 04:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:22][root][INFO] - Training Epoch: 2/2, step 4249/7134 completed (loss: 0.04951828718185425, acc: 0.9936507940292358)
[2025-02-13 04:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:22][root][INFO] - Training Epoch: 2/2, step 4250/7134 completed (loss: 0.06635667383670807, acc: 0.9830148816108704)
[2025-02-13 04:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:22][root][INFO] - Training Epoch: 2/2, step 4251/7134 completed (loss: 0.009891747497022152, acc: 0.9967105388641357)
[2025-02-13 04:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:23][root][INFO] - Training Epoch: 2/2, step 4252/7134 completed (loss: 0.018837368115782738, acc: 0.9920508861541748)
[2025-02-13 04:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:23][root][INFO] - Training Epoch: 2/2, step 4253/7134 completed (loss: 0.03293990343809128, acc: 0.9913232326507568)
[2025-02-13 04:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24][root][INFO] - Training Epoch: 2/2, step 4254/7134 completed (loss: 0.016216471791267395, acc: 0.9950124621391296)
[2025-02-13 04:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24][root][INFO] - Training Epoch: 2/2, step 4255/7134 completed (loss: 0.025643186643719673, acc: 0.9931507110595703)
[2025-02-13 04:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:25][root][INFO] - Training Epoch: 2/2, step 4256/7134 completed (loss: 0.025917788967490196, acc: 0.9932126402854919)
[2025-02-13 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:25][root][INFO] - Training Epoch: 2/2, step 4257/7134 completed (loss: 0.008575073443353176, acc: 0.9946523904800415)
[2025-02-13 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:25][root][INFO] - Training Epoch: 2/2, step 4258/7134 completed (loss: 0.00567685766145587, acc: 1.0)
[2025-02-13 04:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:26][root][INFO] - Training Epoch: 2/2, step 4259/7134 completed (loss: 0.06887410581111908, acc: 0.9871060252189636)
[2025-02-13 04:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:26][root][INFO] - Training Epoch: 2/2, step 4260/7134 completed (loss: 0.011670214124023914, acc: 0.9937888383865356)
[2025-02-13 04:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27][root][INFO] - Training Epoch: 2/2, step 4261/7134 completed (loss: 0.019592104479670525, acc: 0.9923518300056458)
[2025-02-13 04:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27][root][INFO] - Training Epoch: 2/2, step 4262/7134 completed (loss: 0.01381459180265665, acc: 0.9953051805496216)
[2025-02-13 04:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:28][root][INFO] - Training Epoch: 2/2, step 4263/7134 completed (loss: 0.046337950974702835, acc: 0.9915540814399719)
[2025-02-13 04:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:28][root][INFO] - Training Epoch: 2/2, step 4264/7134 completed (loss: 0.031788893043994904, acc: 0.9849812388420105)
[2025-02-13 04:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:28][root][INFO] - Training Epoch: 2/2, step 4265/7134 completed (loss: 0.010571174323558807, acc: 0.997474730014801)
[2025-02-13 04:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29][root][INFO] - Training Epoch: 2/2, step 4266/7134 completed (loss: 0.024983186274766922, acc: 0.9928143620491028)
[2025-02-13 04:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29][root][INFO] - Training Epoch: 2/2, step 4267/7134 completed (loss: 0.022334815934300423, acc: 0.9939393997192383)
[2025-02-13 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:30][root][INFO] - Training Epoch: 2/2, step 4268/7134 completed (loss: 0.05527850612998009, acc: 0.9879759550094604)
[2025-02-13 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:30][root][INFO] - Training Epoch: 2/2, step 4269/7134 completed (loss: 0.046300258487463, acc: 0.9870503544807434)
[2025-02-13 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:31][root][INFO] - Training Epoch: 2/2, step 4270/7134 completed (loss: 0.026047460734844208, acc: 0.9908854365348816)
[2025-02-13 04:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:31][root][INFO] - Training Epoch: 2/2, step 4271/7134 completed (loss: 0.03502969443798065, acc: 0.9887920022010803)
[2025-02-13 04:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32][root][INFO] - Training Epoch: 2/2, step 4272/7134 completed (loss: 0.01607116311788559, acc: 0.9941314458847046)
[2025-02-13 04:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32][root][INFO] - Training Epoch: 2/2, step 4273/7134 completed (loss: 0.019708549603819847, acc: 0.9924717545509338)
[2025-02-13 04:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:33][root][INFO] - Training Epoch: 2/2, step 4274/7134 completed (loss: 0.056311603635549545, acc: 0.9836448431015015)
[2025-02-13 04:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:33][root][INFO] - Training Epoch: 2/2, step 4275/7134 completed (loss: 0.016134178265929222, acc: 0.9949495196342468)
[2025-02-13 04:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:33][root][INFO] - Training Epoch: 2/2, step 4276/7134 completed (loss: 0.029700709506869316, acc: 0.9922680258750916)
[2025-02-13 04:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:34][root][INFO] - Training Epoch: 2/2, step 4277/7134 completed (loss: 0.018179140985012054, acc: 0.9952380657196045)
[2025-02-13 04:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:34][root][INFO] - Training Epoch: 2/2, step 4278/7134 completed (loss: 0.02564336732029915, acc: 0.9925925731658936)
[2025-02-13 04:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35][root][INFO] - Training Epoch: 2/2, step 4279/7134 completed (loss: 0.025196708738803864, acc: 0.9940476417541504)
[2025-02-13 04:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35][root][INFO] - Training Epoch: 2/2, step 4280/7134 completed (loss: 0.0063995844684541225, acc: 0.9984126687049866)
[2025-02-13 04:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:36][root][INFO] - Training Epoch: 2/2, step 4281/7134 completed (loss: 0.02917107194662094, acc: 0.9913366436958313)
[2025-02-13 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:36][root][INFO] - Training Epoch: 2/2, step 4282/7134 completed (loss: 0.004142329562455416, acc: 1.0)
[2025-02-13 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37][root][INFO] - Training Epoch: 2/2, step 4283/7134 completed (loss: 0.008944167755544186, acc: 0.997706413269043)
[2025-02-13 04:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37][root][INFO] - Training Epoch: 2/2, step 4284/7134 completed (loss: 0.029516179114580154, acc: 0.9909420013427734)
[2025-02-13 04:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38][root][INFO] - Training Epoch: 2/2, step 4285/7134 completed (loss: 0.0076941391453146935, acc: 1.0)
[2025-02-13 04:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38][root][INFO] - Training Epoch: 2/2, step 4286/7134 completed (loss: 0.02501908876001835, acc: 0.9910072088241577)
[2025-02-13 04:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38][root][INFO] - Training Epoch: 2/2, step 4287/7134 completed (loss: 0.01903349719941616, acc: 0.9961340427398682)
[2025-02-13 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:39][root][INFO] - Training Epoch: 2/2, step 4288/7134 completed (loss: 0.020454710349440575, acc: 0.9943262338638306)
[2025-02-13 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:39][root][INFO] - Training Epoch: 2/2, step 4289/7134 completed (loss: 0.02622527815401554, acc: 0.9909774661064148)
[2025-02-13 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40][root][INFO] - Training Epoch: 2/2, step 4290/7134 completed (loss: 0.012075977399945259, acc: 0.9972936511039734)
[2025-02-13 04:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40][root][INFO] - Training Epoch: 2/2, step 4291/7134 completed (loss: 0.014766068197786808, acc: 0.9976162314414978)
[2025-02-13 04:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:41][root][INFO] - Training Epoch: 2/2, step 4292/7134 completed (loss: 0.017864204943180084, acc: 0.9939758777618408)
[2025-02-13 04:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:41][root][INFO] - Training Epoch: 2/2, step 4293/7134 completed (loss: 0.010799017734825611, acc: 0.9959677457809448)
[2025-02-13 04:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:41][root][INFO] - Training Epoch: 2/2, step 4294/7134 completed (loss: 0.01586860977113247, acc: 0.9956331849098206)
[2025-02-13 04:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42][root][INFO] - Training Epoch: 2/2, step 4295/7134 completed (loss: 0.028022918850183487, acc: 0.9908758997917175)
[2025-02-13 04:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42][root][INFO] - Training Epoch: 2/2, step 4296/7134 completed (loss: 0.02060900256037712, acc: 0.9975669384002686)
[2025-02-13 04:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:43][root][INFO] - Training Epoch: 2/2, step 4297/7134 completed (loss: 0.006413102149963379, acc: 0.9976525902748108)
[2025-02-13 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:43][root][INFO] - Training Epoch: 2/2, step 4298/7134 completed (loss: 0.025312993675470352, acc: 0.9944979548454285)
[2025-02-13 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44][root][INFO] - Training Epoch: 2/2, step 4299/7134 completed (loss: 0.016339965164661407, acc: 0.9957020282745361)
[2025-02-13 04:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44][root][INFO] - Training Epoch: 2/2, step 4300/7134 completed (loss: 0.006854014005511999, acc: 0.9978070259094238)
[2025-02-13 04:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44][root][INFO] - Training Epoch: 2/2, step 4301/7134 completed (loss: 0.01945420168340206, acc: 0.9934425950050354)
[2025-02-13 04:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:45][root][INFO] - Training Epoch: 2/2, step 4302/7134 completed (loss: 0.023369282484054565, acc: 0.9910447597503662)
[2025-02-13 04:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:45][root][INFO] - Training Epoch: 2/2, step 4303/7134 completed (loss: 0.026570390909910202, acc: 0.9940333962440491)
[2025-02-13 04:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46][root][INFO] - Training Epoch: 2/2, step 4304/7134 completed (loss: 0.01692098192870617, acc: 0.9940564632415771)
[2025-02-13 04:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46][root][INFO] - Training Epoch: 2/2, step 4305/7134 completed (loss: 0.012106949463486671, acc: 0.9941314458847046)
[2025-02-13 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:47][root][INFO] - Training Epoch: 2/2, step 4306/7134 completed (loss: 0.06211131066083908, acc: 0.9868593811988831)
[2025-02-13 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:47][root][INFO] - Training Epoch: 2/2, step 4307/7134 completed (loss: 0.04617662727832794, acc: 0.9839486479759216)
[2025-02-13 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:48][root][INFO] - Training Epoch: 2/2, step 4308/7134 completed (loss: 0.024339526891708374, acc: 0.9930394291877747)
[2025-02-13 04:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:48][root][INFO] - Training Epoch: 2/2, step 4309/7134 completed (loss: 0.024611636996269226, acc: 0.9938347935676575)
[2025-02-13 04:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49][root][INFO] - Training Epoch: 2/2, step 4310/7134 completed (loss: 0.012973864562809467, acc: 0.9964747428894043)
[2025-02-13 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49][root][INFO] - Training Epoch: 2/2, step 4311/7134 completed (loss: 0.006484010722488165, acc: 0.9985507130622864)
[2025-02-13 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:50][root][INFO] - Training Epoch: 2/2, step 4312/7134 completed (loss: 0.01011121179908514, acc: 0.9959127902984619)
[2025-02-13 04:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:50][root][INFO] - Training Epoch: 2/2, step 4313/7134 completed (loss: 0.017002034932374954, acc: 0.9961038827896118)
[2025-02-13 04:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:50][root][INFO] - Training Epoch: 2/2, step 4314/7134 completed (loss: 0.014386833645403385, acc: 0.995468258857727)
[2025-02-13 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51][root][INFO] - Training Epoch: 2/2, step 4315/7134 completed (loss: 0.01404782384634018, acc: 0.9947712421417236)
[2025-02-13 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51][root][INFO] - Training Epoch: 2/2, step 4316/7134 completed (loss: 0.022820256650447845, acc: 0.9936224222183228)
[2025-02-13 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:52][root][INFO] - Training Epoch: 2/2, step 4317/7134 completed (loss: 0.027877531945705414, acc: 0.9938042163848877)
[2025-02-13 04:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:52][root][INFO] - Training Epoch: 2/2, step 4318/7134 completed (loss: 0.031618520617485046, acc: 0.9910314083099365)
[2025-02-13 04:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:53][root][INFO] - Training Epoch: 2/2, step 4319/7134 completed (loss: 0.01034258957952261, acc: 0.9955406785011292)
[2025-02-13 04:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:53][root][INFO] - Training Epoch: 2/2, step 4320/7134 completed (loss: 0.015539789572358131, acc: 0.9972972869873047)
[2025-02-13 04:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:53][root][INFO] - Training Epoch: 2/2, step 4321/7134 completed (loss: 0.022138869389891624, acc: 0.9907621145248413)
[2025-02-13 04:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54][root][INFO] - Training Epoch: 2/2, step 4322/7134 completed (loss: 0.01872018724679947, acc: 0.9937106966972351)
[2025-02-13 04:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54][root][INFO] - Training Epoch: 2/2, step 4323/7134 completed (loss: 0.04058881476521492, acc: 0.9865125417709351)
[2025-02-13 04:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:55][root][INFO] - Training Epoch: 2/2, step 4324/7134 completed (loss: 0.011989050544798374, acc: 0.9961240291595459)
[2025-02-13 04:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:55][root][INFO] - Training Epoch: 2/2, step 4325/7134 completed (loss: 0.01675078272819519, acc: 0.9936203956604004)
[2025-02-13 04:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:56][root][INFO] - Training Epoch: 2/2, step 4326/7134 completed (loss: 0.02013739012181759, acc: 0.9929577708244324)
[2025-02-13 04:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:56][root][INFO] - Training Epoch: 2/2, step 4327/7134 completed (loss: 0.02358049899339676, acc: 0.9947552680969238)
[2025-02-13 04:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:56][root][INFO] - Training Epoch: 2/2, step 4328/7134 completed (loss: 0.03086010180413723, acc: 0.9924812316894531)
[2025-02-13 04:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:57][root][INFO] - Training Epoch: 2/2, step 4329/7134 completed (loss: 0.028005000203847885, acc: 0.993446946144104)
[2025-02-13 04:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:57][root][INFO] - Training Epoch: 2/2, step 4330/7134 completed (loss: 0.04594286531209946, acc: 0.9893389940261841)
[2025-02-13 04:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58][root][INFO] - Training Epoch: 2/2, step 4331/7134 completed (loss: 0.0073840501718223095, acc: 0.9971671104431152)
[2025-02-13 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58][root][INFO] - Training Epoch: 2/2, step 4332/7134 completed (loss: 0.029397746548056602, acc: 0.9885931611061096)
[2025-02-13 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59][root][INFO] - Training Epoch: 2/2, step 4333/7134 completed (loss: 0.02573593705892563, acc: 0.9924127459526062)
[2025-02-13 04:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59][root][INFO] - Training Epoch: 2/2, step 4334/7134 completed (loss: 0.029292643070220947, acc: 0.992514967918396)
[2025-02-13 04:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00][root][INFO] - Training Epoch: 2/2, step 4335/7134 completed (loss: 0.033039625734090805, acc: 0.9919354915618896)
[2025-02-13 04:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00][root][INFO] - Training Epoch: 2/2, step 4336/7134 completed (loss: 0.0030289343558251858, acc: 1.0)
[2025-02-13 04:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00][root][INFO] - Training Epoch: 2/2, step 4337/7134 completed (loss: 0.016047531738877296, acc: 0.9936708807945251)
[2025-02-13 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01][root][INFO] - Training Epoch: 2/2, step 4338/7134 completed (loss: 0.0052205235697329044, acc: 0.998420238494873)
[2025-02-13 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01][root][INFO] - Training Epoch: 2/2, step 4339/7134 completed (loss: 0.03464960306882858, acc: 0.9897959232330322)
[2025-02-13 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02][root][INFO] - Training Epoch: 2/2, step 4340/7134 completed (loss: 0.018984805792570114, acc: 0.9949324131011963)
[2025-02-13 04:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02][root][INFO] - Training Epoch: 2/2, step 4341/7134 completed (loss: 0.038620494306087494, acc: 0.9910846948623657)
[2025-02-13 04:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03][root][INFO] - Training Epoch: 2/2, step 4342/7134 completed (loss: 0.021498937159776688, acc: 0.995192289352417)
[2025-02-13 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03][root][INFO] - Training Epoch: 2/2, step 4343/7134 completed (loss: 0.013110983185470104, acc: 0.9966722130775452)
[2025-02-13 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03][root][INFO] - Training Epoch: 2/2, step 4344/7134 completed (loss: 0.004882812965661287, acc: 1.0)
[2025-02-13 04:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04][root][INFO] - Training Epoch: 2/2, step 4345/7134 completed (loss: 0.01817948929965496, acc: 0.9917184114456177)
[2025-02-13 04:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04][root][INFO] - Training Epoch: 2/2, step 4346/7134 completed (loss: 0.00820639356970787, acc: 0.9958592057228088)
[2025-02-13 04:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05][root][INFO] - Training Epoch: 2/2, step 4347/7134 completed (loss: 0.03723052889108658, acc: 0.9915134310722351)
[2025-02-13 04:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05][root][INFO] - Training Epoch: 2/2, step 4348/7134 completed (loss: 0.014895778149366379, acc: 0.995199978351593)
[2025-02-13 04:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05][root][INFO] - Training Epoch: 2/2, step 4349/7134 completed (loss: 0.007970609702169895, acc: 0.9976905584335327)
[2025-02-13 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06][root][INFO] - Training Epoch: 2/2, step 4350/7134 completed (loss: 0.02368275821208954, acc: 0.9921466112136841)
[2025-02-13 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06][root][INFO] - Training Epoch: 2/2, step 4351/7134 completed (loss: 0.027625784277915955, acc: 0.991769552230835)
[2025-02-13 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:07][root][INFO] - Training Epoch: 2/2, step 4352/7134 completed (loss: 0.02675478532910347, acc: 0.9923954606056213)
[2025-02-13 04:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:07][root][INFO] - Training Epoch: 2/2, step 4353/7134 completed (loss: 0.025145230814814568, acc: 0.9933775067329407)
[2025-02-13 04:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08][root][INFO] - Training Epoch: 2/2, step 4354/7134 completed (loss: 0.019728828221559525, acc: 0.9946666955947876)
[2025-02-13 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08][root][INFO] - Training Epoch: 2/2, step 4355/7134 completed (loss: 0.030967984348535538, acc: 0.991239070892334)
[2025-02-13 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09][root][INFO] - Training Epoch: 2/2, step 4356/7134 completed (loss: 0.02662121318280697, acc: 0.9892473220825195)
[2025-02-13 04:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09][root][INFO] - Training Epoch: 2/2, step 4357/7134 completed (loss: 0.034378618001937866, acc: 0.9921259880065918)
[2025-02-13 04:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09][root][INFO] - Training Epoch: 2/2, step 4358/7134 completed (loss: 0.024565305560827255, acc: 0.9904191493988037)
[2025-02-13 04:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10][root][INFO] - Training Epoch: 2/2, step 4359/7134 completed (loss: 0.027497688308358192, acc: 0.9920760989189148)
[2025-02-13 04:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10][root][INFO] - Training Epoch: 2/2, step 4360/7134 completed (loss: 0.04364846646785736, acc: 0.9860334992408752)
[2025-02-13 04:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11][root][INFO] - Training Epoch: 2/2, step 4361/7134 completed (loss: 0.021659143269062042, acc: 0.9927745461463928)
[2025-02-13 04:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11][root][INFO] - Training Epoch: 2/2, step 4362/7134 completed (loss: 0.02455870807170868, acc: 0.9901356101036072)
[2025-02-13 04:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:12][root][INFO] - Training Epoch: 2/2, step 4363/7134 completed (loss: 0.01809658296406269, acc: 0.9950413107872009)
[2025-02-13 04:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:12][root][INFO] - Training Epoch: 2/2, step 4364/7134 completed (loss: 0.01087005902081728, acc: 0.995312511920929)
[2025-02-13 04:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:12][root][INFO] - Training Epoch: 2/2, step 4365/7134 completed (loss: 0.024114908650517464, acc: 0.9921383857727051)
[2025-02-13 04:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13][root][INFO] - Training Epoch: 2/2, step 4366/7134 completed (loss: 0.04167561233043671, acc: 0.9845361113548279)
[2025-02-13 04:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13][root][INFO] - Training Epoch: 2/2, step 4367/7134 completed (loss: 0.0197460800409317, acc: 0.9957020282745361)
[2025-02-13 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14][root][INFO] - Training Epoch: 2/2, step 4368/7134 completed (loss: 0.015825215727090836, acc: 0.992443323135376)
[2025-02-13 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14][root][INFO] - Training Epoch: 2/2, step 4369/7134 completed (loss: 0.01796940714120865, acc: 0.9933333396911621)
[2025-02-13 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15][root][INFO] - Training Epoch: 2/2, step 4370/7134 completed (loss: 0.039039310067892075, acc: 0.9899117350578308)
[2025-02-13 04:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15][root][INFO] - Training Epoch: 2/2, step 4371/7134 completed (loss: 0.03366165980696678, acc: 0.9920844435691833)
[2025-02-13 04:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16][root][INFO] - Training Epoch: 2/2, step 4372/7134 completed (loss: 0.025507064536213875, acc: 0.9948365092277527)
[2025-02-13 04:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16][root][INFO] - Training Epoch: 2/2, step 4373/7134 completed (loss: 0.01305395271629095, acc: 0.9964328408241272)
[2025-02-13 04:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16][root][INFO] - Training Epoch: 2/2, step 4374/7134 completed (loss: 0.021954786032438278, acc: 0.9942445755004883)
[2025-02-13 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17][root][INFO] - Training Epoch: 2/2, step 4375/7134 completed (loss: 0.01444720383733511, acc: 0.993565022945404)
[2025-02-13 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17][root][INFO] - Training Epoch: 2/2, step 4376/7134 completed (loss: 0.02594297006726265, acc: 0.9932126402854919)
[2025-02-13 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18][root][INFO] - Training Epoch: 2/2, step 4377/7134 completed (loss: 0.048547517508268356, acc: 0.995055615901947)
[2025-02-13 04:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18][root][INFO] - Training Epoch: 2/2, step 4378/7134 completed (loss: 0.027405651286244392, acc: 0.991830050945282)
[2025-02-13 04:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19][root][INFO] - Training Epoch: 2/2, step 4379/7134 completed (loss: 0.06735997647047043, acc: 0.9734848737716675)
[2025-02-13 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19][root][INFO] - Training Epoch: 2/2, step 4380/7134 completed (loss: 0.048294249922037125, acc: 0.9912434220314026)
[2025-02-13 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19][root][INFO] - Training Epoch: 2/2, step 4381/7134 completed (loss: 0.0190720371901989, acc: 0.9931034445762634)
[2025-02-13 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20][root][INFO] - Training Epoch: 2/2, step 4382/7134 completed (loss: 0.009072099812328815, acc: 0.995230495929718)
[2025-02-13 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20][root][INFO] - Training Epoch: 2/2, step 4383/7134 completed (loss: 0.02413404919207096, acc: 0.9903714060783386)
[2025-02-13 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21][root][INFO] - Training Epoch: 2/2, step 4384/7134 completed (loss: 0.013192012906074524, acc: 0.9949748516082764)
[2025-02-13 04:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21][root][INFO] - Training Epoch: 2/2, step 4385/7134 completed (loss: 0.03918629139661789, acc: 0.9869621992111206)
[2025-02-13 04:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:22][root][INFO] - Training Epoch: 2/2, step 4386/7134 completed (loss: 0.033120036125183105, acc: 0.9881516695022583)
[2025-02-13 04:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:22][root][INFO] - Training Epoch: 2/2, step 4387/7134 completed (loss: 0.0411393828690052, acc: 0.9871175289154053)
[2025-02-13 04:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:22][root][INFO] - Training Epoch: 2/2, step 4388/7134 completed (loss: 0.021056581288576126, acc: 0.9953415989875793)
[2025-02-13 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23][root][INFO] - Training Epoch: 2/2, step 4389/7134 completed (loss: 0.0584401972591877, acc: 0.985318124294281)
[2025-02-13 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23][root][INFO] - Training Epoch: 2/2, step 4390/7134 completed (loss: 0.05751361697912216, acc: 0.9845722317695618)
[2025-02-13 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24][root][INFO] - Training Epoch: 2/2, step 4391/7134 completed (loss: 0.05213666707277298, acc: 0.9837133288383484)
[2025-02-13 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24][root][INFO] - Training Epoch: 2/2, step 4392/7134 completed (loss: 0.013081555254757404, acc: 0.9963235259056091)
[2025-02-13 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25][root][INFO] - Training Epoch: 2/2, step 4393/7134 completed (loss: 0.015625590458512306, acc: 0.9929676651954651)
[2025-02-13 04:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25][root][INFO] - Training Epoch: 2/2, step 4394/7134 completed (loss: 0.022159546613693237, acc: 0.995768666267395)
[2025-02-13 04:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26][root][INFO] - Training Epoch: 2/2, step 4395/7134 completed (loss: 0.03394896537065506, acc: 0.9890109896659851)
[2025-02-13 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26][root][INFO] - Training Epoch: 2/2, step 4396/7134 completed (loss: 0.054528120905160904, acc: 0.9850948452949524)
[2025-02-13 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26][root][INFO] - Training Epoch: 2/2, step 4397/7134 completed (loss: 0.033785127103328705, acc: 0.9929412007331848)
[2025-02-13 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27][root][INFO] - Training Epoch: 2/2, step 4398/7134 completed (loss: 0.008727207779884338, acc: 0.9949495196342468)
[2025-02-13 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27][root][INFO] - Training Epoch: 2/2, step 4399/7134 completed (loss: 0.03377992659807205, acc: 0.9951768517494202)
[2025-02-13 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28][root][INFO] - Training Epoch: 2/2, step 4400/7134 completed (loss: 0.05269366130232811, acc: 0.9851301312446594)
[2025-02-13 04:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28][root][INFO] - Training Epoch: 2/2, step 4401/7134 completed (loss: 0.03416391462087631, acc: 0.9927536249160767)
[2025-02-13 04:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29][root][INFO] - Training Epoch: 2/2, step 4402/7134 completed (loss: 0.08347932249307632, acc: 0.979651153087616)
[2025-02-13 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29][root][INFO] - Training Epoch: 2/2, step 4403/7134 completed (loss: 0.09418479353189468, acc: 0.9753885865211487)
[2025-02-13 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29][root][INFO] - Training Epoch: 2/2, step 4404/7134 completed (loss: 0.09836789220571518, acc: 0.977393627166748)
[2025-02-13 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30][root][INFO] - Training Epoch: 2/2, step 4405/7134 completed (loss: 0.015724293887615204, acc: 0.9956803321838379)
[2025-02-13 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30][root][INFO] - Training Epoch: 2/2, step 4406/7134 completed (loss: 0.019482193514704704, acc: 0.993318498134613)
[2025-02-13 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:31][root][INFO] - Training Epoch: 2/2, step 4407/7134 completed (loss: 0.028633536770939827, acc: 0.9942196607589722)
[2025-02-13 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:31][root][INFO] - Training Epoch: 2/2, step 4408/7134 completed (loss: 0.05219733342528343, acc: 0.9895226955413818)
[2025-02-13 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32][root][INFO] - Training Epoch: 2/2, step 4409/7134 completed (loss: 0.041697680950164795, acc: 0.9877750873565674)
[2025-02-13 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32][root][INFO] - Training Epoch: 2/2, step 4410/7134 completed (loss: 0.05391855537891388, acc: 0.9856630563735962)
[2025-02-13 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:33][root][INFO] - Training Epoch: 2/2, step 4411/7134 completed (loss: 0.0763641968369484, acc: 0.9798561334609985)
[2025-02-13 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:33][root][INFO] - Training Epoch: 2/2, step 4412/7134 completed (loss: 0.11176418513059616, acc: 0.96856290102005)
[2025-02-13 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34][root][INFO] - Training Epoch: 2/2, step 4413/7134 completed (loss: 0.07073219865560532, acc: 0.9795918464660645)
[2025-02-13 04:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34][root][INFO] - Training Epoch: 2/2, step 4414/7134 completed (loss: 0.024472776800394058, acc: 0.9940758347511292)
[2025-02-13 04:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34][root][INFO] - Training Epoch: 2/2, step 4415/7134 completed (loss: 0.027954859659075737, acc: 0.9912827014923096)
[2025-02-13 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35][root][INFO] - Training Epoch: 2/2, step 4416/7134 completed (loss: 0.04612104594707489, acc: 0.9871645569801331)
[2025-02-13 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35][root][INFO] - Training Epoch: 2/2, step 4417/7134 completed (loss: 0.044936653226614, acc: 0.989182710647583)
[2025-02-13 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36][root][INFO] - Training Epoch: 2/2, step 4418/7134 completed (loss: 0.04557099565863609, acc: 0.9895833134651184)
[2025-02-13 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36][root][INFO] - Training Epoch: 2/2, step 4419/7134 completed (loss: 0.057680148631334305, acc: 0.9819711446762085)
[2025-02-13 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37][root][INFO] - Training Epoch: 2/2, step 4420/7134 completed (loss: 0.0485323928296566, acc: 0.9820193648338318)
[2025-02-13 04:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37][root][INFO] - Training Epoch: 2/2, step 4421/7134 completed (loss: 0.04510755091905594, acc: 0.9876265525817871)
[2025-02-13 04:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38][root][INFO] - Training Epoch: 2/2, step 4422/7134 completed (loss: 0.05203971639275551, acc: 0.9827315807342529)
[2025-02-13 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38][root][INFO] - Training Epoch: 2/2, step 4423/7134 completed (loss: 0.03078101947903633, acc: 0.9885844588279724)
[2025-02-13 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38][root][INFO] - Training Epoch: 2/2, step 4424/7134 completed (loss: 0.023647911846637726, acc: 0.9954954981803894)
[2025-02-13 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39][root][INFO] - Training Epoch: 2/2, step 4425/7134 completed (loss: 0.01865382306277752, acc: 0.9929824471473694)
[2025-02-13 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39][root][INFO] - Training Epoch: 2/2, step 4426/7134 completed (loss: 0.038853712379932404, acc: 0.986556351184845)
[2025-02-13 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40][root][INFO] - Training Epoch: 2/2, step 4427/7134 completed (loss: 0.07491086423397064, acc: 0.9855421781539917)
[2025-02-13 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40][root][INFO] - Training Epoch: 2/2, step 4428/7134 completed (loss: 0.019786398857831955, acc: 0.9957746267318726)
[2025-02-13 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41][root][INFO] - Training Epoch: 2/2, step 4429/7134 completed (loss: 0.027485787868499756, acc: 0.9871134161949158)
[2025-02-13 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41][root][INFO] - Training Epoch: 2/2, step 4430/7134 completed (loss: 0.027061935514211655, acc: 0.9939024448394775)
[2025-02-13 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41][root][INFO] - Training Epoch: 2/2, step 4431/7134 completed (loss: 0.024372383952140808, acc: 0.9953343868255615)
[2025-02-13 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42][root][INFO] - Training Epoch: 2/2, step 4432/7134 completed (loss: 0.01248208899050951, acc: 0.996927797794342)
[2025-02-13 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42][root][INFO] - Training Epoch: 2/2, step 4433/7134 completed (loss: 0.03240978345274925, acc: 0.9918414950370789)
[2025-02-13 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43][root][INFO] - Training Epoch: 2/2, step 4434/7134 completed (loss: 0.024611610919237137, acc: 0.9939939975738525)
[2025-02-13 04:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43][root][INFO] - Training Epoch: 2/2, step 4435/7134 completed (loss: 0.022874819114804268, acc: 0.9870610237121582)
[2025-02-13 04:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44][root][INFO] - Training Epoch: 2/2, step 4436/7134 completed (loss: 0.04146464169025421, acc: 0.9846368432044983)
[2025-02-13 04:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44][root][INFO] - Training Epoch: 2/2, step 4437/7134 completed (loss: 0.010264295153319836, acc: 0.9960629940032959)
[2025-02-13 04:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44][root][INFO] - Training Epoch: 2/2, step 4438/7134 completed (loss: 0.03511415421962738, acc: 0.991525411605835)
[2025-02-13 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45][root][INFO] - Training Epoch: 2/2, step 4439/7134 completed (loss: 0.010874667204916477, acc: 0.9967479705810547)
[2025-02-13 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45][root][INFO] - Training Epoch: 2/2, step 4440/7134 completed (loss: 0.0182944368571043, acc: 0.9930939078330994)
[2025-02-13 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46][root][INFO] - Training Epoch: 2/2, step 4441/7134 completed (loss: 0.02397916093468666, acc: 0.9907833933830261)
[2025-02-13 04:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46][root][INFO] - Training Epoch: 2/2, step 4442/7134 completed (loss: 0.05153344199061394, acc: 0.9909909963607788)
[2025-02-13 04:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47][root][INFO] - Training Epoch: 2/2, step 4443/7134 completed (loss: 0.02609855681657791, acc: 0.9910256266593933)
[2025-02-13 04:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47][root][INFO] - Training Epoch: 2/2, step 4444/7134 completed (loss: 0.017712747678160667, acc: 0.9934747219085693)
[2025-02-13 04:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47][root][INFO] - Training Epoch: 2/2, step 4445/7134 completed (loss: 0.024471914395689964, acc: 0.9874301552772522)
[2025-02-13 04:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:48][root][INFO] - Training Epoch: 2/2, step 4446/7134 completed (loss: 0.011469184421002865, acc: 0.9967793822288513)
[2025-02-13 04:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:48][root][INFO] - Training Epoch: 2/2, step 4447/7134 completed (loss: 0.017920492216944695, acc: 0.9941037893295288)
[2025-02-13 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49][root][INFO] - Training Epoch: 2/2, step 4448/7134 completed (loss: 0.034094296395778656, acc: 0.9872029423713684)
[2025-02-13 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49][root][INFO] - Training Epoch: 2/2, step 4449/7134 completed (loss: 0.022666411474347115, acc: 0.9907063245773315)
[2025-02-13 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50][root][INFO] - Training Epoch: 2/2, step 4450/7134 completed (loss: 0.01900050789117813, acc: 0.9940740466117859)
[2025-02-13 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50][root][INFO] - Training Epoch: 2/2, step 4451/7134 completed (loss: 0.07896239310503006, acc: 0.9834162592887878)
[2025-02-13 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51][root][INFO] - Training Epoch: 2/2, step 4452/7134 completed (loss: 0.009265707805752754, acc: 0.9968000054359436)
[2025-02-13 04:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51][root][INFO] - Training Epoch: 2/2, step 4453/7134 completed (loss: 0.04079118371009827, acc: 0.9918032884597778)
[2025-02-13 04:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51][root][INFO] - Training Epoch: 2/2, step 4454/7134 completed (loss: 0.028028082102537155, acc: 0.9929178357124329)
[2025-02-13 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52][root][INFO] - Training Epoch: 2/2, step 4455/7134 completed (loss: 0.070589579641819, acc: 0.9834254384040833)
[2025-02-13 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52][root][INFO] - Training Epoch: 2/2, step 4456/7134 completed (loss: 0.005845238454639912, acc: 0.9983818531036377)
[2025-02-13 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:53][root][INFO] - Training Epoch: 2/2, step 4457/7134 completed (loss: 0.011179264634847641, acc: 0.9925093650817871)
[2025-02-13 04:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:53][root][INFO] - Training Epoch: 2/2, step 4458/7134 completed (loss: 0.0030836332589387894, acc: 0.9983633160591125)
[2025-02-13 04:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54][root][INFO] - Training Epoch: 2/2, step 4459/7134 completed (loss: 0.018973901867866516, acc: 0.9945054650306702)
[2025-02-13 04:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54][root][INFO] - Training Epoch: 2/2, step 4460/7134 completed (loss: 0.019775910302996635, acc: 0.995110034942627)
[2025-02-13 04:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54][root][INFO] - Training Epoch: 2/2, step 4461/7134 completed (loss: 0.04198744520545006, acc: 0.9913899302482605)
[2025-02-13 04:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:55][root][INFO] - Training Epoch: 2/2, step 4462/7134 completed (loss: 0.018262501806020737, acc: 0.9972752332687378)
[2025-02-13 04:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:55][root][INFO] - Training Epoch: 2/2, step 4463/7134 completed (loss: 0.011128001846373081, acc: 0.9949685335159302)
[2025-02-13 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56][root][INFO] - Training Epoch: 2/2, step 4464/7134 completed (loss: 0.03177245333790779, acc: 0.9915682673454285)
[2025-02-13 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56][root][INFO] - Training Epoch: 2/2, step 4465/7134 completed (loss: 0.007788114715367556, acc: 0.9974226951599121)
[2025-02-13 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57][root][INFO] - Training Epoch: 2/2, step 4466/7134 completed (loss: 0.013037627562880516, acc: 0.9962962865829468)
[2025-02-13 04:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57][root][INFO] - Training Epoch: 2/2, step 4467/7134 completed (loss: 0.023865368217229843, acc: 0.9937185645103455)
[2025-02-13 04:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:58][root][INFO] - Training Epoch: 2/2, step 4468/7134 completed (loss: 0.0032819854095578194, acc: 1.0)
[2025-02-13 04:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:58][root][INFO] - Training Epoch: 2/2, step 4469/7134 completed (loss: 0.01848587766289711, acc: 0.9953756928443909)
[2025-02-13 04:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59][root][INFO] - Training Epoch: 2/2, step 4470/7134 completed (loss: 0.010477563366293907, acc: 0.9977900385856628)
[2025-02-13 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59][root][INFO] - Training Epoch: 2/2, step 4471/7134 completed (loss: 0.004988427739590406, acc: 0.9987714886665344)
[2025-02-13 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59][root][INFO] - Training Epoch: 2/2, step 4472/7134 completed (loss: 0.028222108259797096, acc: 0.9947299361228943)
[2025-02-13 04:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:00][root][INFO] - Training Epoch: 2/2, step 4473/7134 completed (loss: 0.04883478209376335, acc: 0.9831932783126831)
[2025-02-13 04:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:00][root][INFO] - Training Epoch: 2/2, step 4474/7134 completed (loss: 0.05559277534484863, acc: 0.9804741740226746)
[2025-02-13 04:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01][root][INFO] - Training Epoch: 2/2, step 4475/7134 completed (loss: 0.07973046600818634, acc: 0.9764267802238464)
[2025-02-13 04:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01][root][INFO] - Training Epoch: 2/2, step 4476/7134 completed (loss: 0.04142380505800247, acc: 0.9862825870513916)
[2025-02-13 04:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02][root][INFO] - Training Epoch: 2/2, step 4477/7134 completed (loss: 0.012513457797467709, acc: 0.9938875436782837)
[2025-02-13 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02][root][INFO] - Training Epoch: 2/2, step 4478/7134 completed (loss: 0.017809458076953888, acc: 0.9962916970252991)
[2025-02-13 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03][root][INFO] - Training Epoch: 2/2, step 4479/7134 completed (loss: 0.01001876313239336, acc: 0.9959568977355957)
[2025-02-13 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03][root][INFO] - Training Epoch: 2/2, step 4480/7134 completed (loss: 0.017810329794883728, acc: 0.9941588640213013)
[2025-02-13 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03][root][INFO] - Training Epoch: 2/2, step 4481/7134 completed (loss: 0.02707619220018387, acc: 0.9941588640213013)
[2025-02-13 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04][root][INFO] - Training Epoch: 2/2, step 4482/7134 completed (loss: 0.009739451110363007, acc: 0.9976019263267517)
[2025-02-13 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04][root][INFO] - Training Epoch: 2/2, step 4483/7134 completed (loss: 0.024184225127100945, acc: 0.9955423474311829)
[2025-02-13 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05][root][INFO] - Training Epoch: 2/2, step 4484/7134 completed (loss: 0.01137156505137682, acc: 0.9973545074462891)
[2025-02-13 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05][root][INFO] - Training Epoch: 2/2, step 4485/7134 completed (loss: 0.0034199305810034275, acc: 1.0)
[2025-02-13 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06][root][INFO] - Training Epoch: 2/2, step 4486/7134 completed (loss: 0.006366735324263573, acc: 0.9985074400901794)
[2025-02-13 04:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06][root][INFO] - Training Epoch: 2/2, step 4487/7134 completed (loss: 0.006500578485429287, acc: 0.9986206889152527)
[2025-02-13 04:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06][root][INFO] - Training Epoch: 2/2, step 4488/7134 completed (loss: 0.004213064908981323, acc: 1.0)
[2025-02-13 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07][root][INFO] - Training Epoch: 2/2, step 4489/7134 completed (loss: 0.0059563033282756805, acc: 1.0)
[2025-02-13 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07][root][INFO] - Training Epoch: 2/2, step 4490/7134 completed (loss: 0.004492848180234432, acc: 1.0)
[2025-02-13 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08][root][INFO] - Training Epoch: 2/2, step 4491/7134 completed (loss: 0.0024322594981640577, acc: 1.0)
[2025-02-13 04:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08][root][INFO] - Training Epoch: 2/2, step 4492/7134 completed (loss: 0.006601893808692694, acc: 0.998161792755127)
[2025-02-13 04:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09][root][INFO] - Training Epoch: 2/2, step 4493/7134 completed (loss: 0.004595831502228975, acc: 0.9986245036125183)
[2025-02-13 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09][root][INFO] - Training Epoch: 2/2, step 4494/7134 completed (loss: 0.016383439302444458, acc: 0.9971056580543518)
[2025-02-13 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09][root][INFO] - Training Epoch: 2/2, step 4495/7134 completed (loss: 0.0058914534747600555, acc: 0.9987228512763977)
[2025-02-13 04:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:10][root][INFO] - Training Epoch: 2/2, step 4496/7134 completed (loss: 0.010359124280512333, acc: 0.9971510171890259)
[2025-02-13 04:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:10][root][INFO] - Training Epoch: 2/2, step 4497/7134 completed (loss: 0.00946455541998148, acc: 0.998675525188446)
[2025-02-13 04:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11][root][INFO] - Training Epoch: 2/2, step 4498/7134 completed (loss: 0.045350052416324615, acc: 0.9927431344985962)
[2025-02-13 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11][root][INFO] - Training Epoch: 2/2, step 4499/7134 completed (loss: 0.022454699501395226, acc: 0.9930434823036194)
[2025-02-13 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12][root][INFO] - Training Epoch: 2/2, step 4500/7134 completed (loss: 0.01689091883599758, acc: 0.9958041906356812)
[2025-02-13 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12][root][INFO] - Training Epoch: 2/2, step 4501/7134 completed (loss: 0.010276584886014462, acc: 0.99726402759552)
[2025-02-13 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12][root][INFO] - Training Epoch: 2/2, step 4502/7134 completed (loss: 0.013089455664157867, acc: 0.9970015287399292)
[2025-02-13 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13][root][INFO] - Training Epoch: 2/2, step 4503/7134 completed (loss: 0.007018809672445059, acc: 0.9983948469161987)
[2025-02-13 04:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13][root][INFO] - Training Epoch: 2/2, step 4504/7134 completed (loss: 0.020536547526717186, acc: 0.9910314083099365)
[2025-02-13 04:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14][root][INFO] - Training Epoch: 2/2, step 4505/7134 completed (loss: 0.016431409865617752, acc: 0.9950860142707825)
[2025-02-13 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14][root][INFO] - Training Epoch: 2/2, step 4506/7134 completed (loss: 0.007845476269721985, acc: 0.9974259734153748)
[2025-02-13 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14][root][INFO] - Training Epoch: 2/2, step 4507/7134 completed (loss: 0.010412514209747314, acc: 0.9971550703048706)
[2025-02-13 04:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15][root][INFO] - Training Epoch: 2/2, step 4508/7134 completed (loss: 0.02879597619175911, acc: 0.9928774833679199)
[2025-02-13 04:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15][root][INFO] - Training Epoch: 2/2, step 4509/7134 completed (loss: 0.017378410324454308, acc: 0.991946280002594)
[2025-02-13 04:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16][root][INFO] - Training Epoch: 2/2, step 4510/7134 completed (loss: 0.041038159281015396, acc: 0.9858956336975098)
[2025-02-13 04:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16][root][INFO] - Training Epoch: 2/2, step 4511/7134 completed (loss: 0.03731635585427284, acc: 0.9901546835899353)
[2025-02-13 04:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17][root][INFO] - Training Epoch: 2/2, step 4512/7134 completed (loss: 0.02090958133339882, acc: 0.9964157938957214)
[2025-02-13 04:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17][root][INFO] - Training Epoch: 2/2, step 4513/7134 completed (loss: 0.017183471471071243, acc: 0.9943820238113403)
[2025-02-13 04:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18][root][INFO] - Training Epoch: 2/2, step 4514/7134 completed (loss: 0.06992130726575851, acc: 0.9839416146278381)
[2025-02-13 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18][root][INFO] - Training Epoch: 2/2, step 4515/7134 completed (loss: 0.02405504882335663, acc: 0.9910447597503662)
[2025-02-13 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18][root][INFO] - Training Epoch: 2/2, step 4516/7134 completed (loss: 0.016354890540242195, acc: 0.9951040148735046)
[2025-02-13 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19][root][INFO] - Training Epoch: 2/2, step 4517/7134 completed (loss: 0.029203852638602257, acc: 0.9884763360023499)
[2025-02-13 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19][root][INFO] - Training Epoch: 2/2, step 4518/7134 completed (loss: 0.02992832101881504, acc: 0.9923469424247742)
[2025-02-13 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20][root][INFO] - Training Epoch: 2/2, step 4519/7134 completed (loss: 0.019888218492269516, acc: 0.993954062461853)
[2025-02-13 04:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20][root][INFO] - Training Epoch: 2/2, step 4520/7134 completed (loss: 0.014549818821251392, acc: 0.9957627058029175)
[2025-02-13 04:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21][root][INFO] - Training Epoch: 2/2, step 4521/7134 completed (loss: 0.040322184562683105, acc: 0.9915764331817627)
[2025-02-13 04:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21][root][INFO] - Training Epoch: 2/2, step 4522/7134 completed (loss: 0.030143721029162407, acc: 0.98740553855896)
[2025-02-13 04:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:22][root][INFO] - Training Epoch: 2/2, step 4523/7134 completed (loss: 0.024885041639208794, acc: 0.9917159676551819)
[2025-02-13 04:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:22][root][INFO] - Training Epoch: 2/2, step 4524/7134 completed (loss: 0.04408251866698265, acc: 0.9878542423248291)
[2025-02-13 04:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:22][root][INFO] - Training Epoch: 2/2, step 4525/7134 completed (loss: 0.01628427766263485, acc: 0.9944444298744202)
[2025-02-13 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23][root][INFO] - Training Epoch: 2/2, step 4526/7134 completed (loss: 0.01964234560728073, acc: 0.996129035949707)
[2025-02-13 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23][root][INFO] - Training Epoch: 2/2, step 4527/7134 completed (loss: 0.019152987748384476, acc: 0.994425892829895)
[2025-02-13 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24][root][INFO] - Training Epoch: 2/2, step 4528/7134 completed (loss: 0.027011435478925705, acc: 0.9884318709373474)
[2025-02-13 04:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24][root][INFO] - Training Epoch: 2/2, step 4529/7134 completed (loss: 0.04860832914710045, acc: 0.9896373152732849)
[2025-02-13 04:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25][root][INFO] - Training Epoch: 2/2, step 4530/7134 completed (loss: 0.012385707348585129, acc: 0.9971988797187805)
[2025-02-13 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25][root][INFO] - Training Epoch: 2/2, step 4531/7134 completed (loss: 0.010264625772833824, acc: 0.9946140050888062)
[2025-02-13 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26][root][INFO] - Training Epoch: 2/2, step 4532/7134 completed (loss: 0.03042621910572052, acc: 0.9930278658866882)
[2025-02-13 04:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26][root][INFO] - Training Epoch: 2/2, step 4533/7134 completed (loss: 0.017897259443998337, acc: 0.9952550530433655)
[2025-02-13 04:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26][root][INFO] - Training Epoch: 2/2, step 4534/7134 completed (loss: 0.015973471105098724, acc: 0.9953051805496216)
[2025-02-13 04:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27][root][INFO] - Training Epoch: 2/2, step 4535/7134 completed (loss: 0.01227745320647955, acc: 0.9954441785812378)
[2025-02-13 04:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27][root][INFO] - Training Epoch: 2/2, step 4536/7134 completed (loss: 0.02347731404006481, acc: 0.9961685538291931)
[2025-02-13 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:28][root][INFO] - Training Epoch: 2/2, step 4537/7134 completed (loss: 0.005941405426710844, acc: 0.9977653622627258)
[2025-02-13 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:28][root][INFO] - Training Epoch: 2/2, step 4538/7134 completed (loss: 0.03170069307088852, acc: 0.9919540286064148)
[2025-02-13 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29][root][INFO] - Training Epoch: 2/2, step 4539/7134 completed (loss: 0.021703269332647324, acc: 0.9924487471580505)
[2025-02-13 04:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29][root][INFO] - Training Epoch: 2/2, step 4540/7134 completed (loss: 0.02194230630993843, acc: 0.9942196607589722)
[2025-02-13 04:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30][root][INFO] - Training Epoch: 2/2, step 4541/7134 completed (loss: 0.023790424689650536, acc: 0.987679660320282)
[2025-02-13 04:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30][root][INFO] - Training Epoch: 2/2, step 4542/7134 completed (loss: 0.04454991966485977, acc: 0.9870129823684692)
[2025-02-13 04:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30][root][INFO] - Training Epoch: 2/2, step 4543/7134 completed (loss: 0.043981362134218216, acc: 0.9857397675514221)
[2025-02-13 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31][root][INFO] - Training Epoch: 2/2, step 4544/7134 completed (loss: 0.021878093481063843, acc: 0.9939024448394775)
[2025-02-13 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31][root][INFO] - Training Epoch: 2/2, step 4545/7134 completed (loss: 0.05448692664504051, acc: 0.9865125417709351)
[2025-02-13 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:32][root][INFO] - Training Epoch: 2/2, step 4546/7134 completed (loss: 0.04474029690027237, acc: 0.9932773113250732)
[2025-02-13 04:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:32][root][INFO] - Training Epoch: 2/2, step 4547/7134 completed (loss: 0.01883217878639698, acc: 0.991349458694458)
[2025-02-13 04:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33][root][INFO] - Training Epoch: 2/2, step 4548/7134 completed (loss: 0.020713983103632927, acc: 0.9933664798736572)
[2025-02-13 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33][root][INFO] - Training Epoch: 2/2, step 4549/7134 completed (loss: 0.030857160687446594, acc: 0.9849785566329956)
[2025-02-13 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33][root][INFO] - Training Epoch: 2/2, step 4550/7134 completed (loss: 0.01998773217201233, acc: 0.9949748516082764)
[2025-02-13 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34][root][INFO] - Training Epoch: 2/2, step 4551/7134 completed (loss: 0.02747434936463833, acc: 0.9957143068313599)
[2025-02-13 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34][root][INFO] - Training Epoch: 2/2, step 4552/7134 completed (loss: 0.02037045545876026, acc: 0.9923809766769409)
[2025-02-13 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35][root][INFO] - Training Epoch: 2/2, step 4553/7134 completed (loss: 0.022993624210357666, acc: 0.9932318329811096)
[2025-02-13 04:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35][root][INFO] - Training Epoch: 2/2, step 4554/7134 completed (loss: 0.03927089273929596, acc: 0.9873873591423035)
[2025-02-13 04:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35][root][INFO] - Training Epoch: 2/2, step 4555/7134 completed (loss: 0.019756227731704712, acc: 0.9889196753501892)
[2025-02-13 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:36][root][INFO] - Training Epoch: 2/2, step 4556/7134 completed (loss: 0.04189949482679367, acc: 0.9864661693572998)
[2025-02-13 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:36][root][INFO] - Training Epoch: 2/2, step 4557/7134 completed (loss: 0.03532310575246811, acc: 0.989051103591919)
[2025-02-13 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:37][root][INFO] - Training Epoch: 2/2, step 4558/7134 completed (loss: 0.028582505881786346, acc: 0.9898989796638489)
[2025-02-13 04:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:37][root][INFO] - Training Epoch: 2/2, step 4559/7134 completed (loss: 0.0395977646112442, acc: 0.9865546226501465)
[2025-02-13 04:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38][root][INFO] - Training Epoch: 2/2, step 4560/7134 completed (loss: 0.01973111927509308, acc: 0.9945651888847351)
[2025-02-13 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38][root][INFO] - Training Epoch: 2/2, step 4561/7134 completed (loss: 0.04444049298763275, acc: 0.9843137264251709)
[2025-02-13 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38][root][INFO] - Training Epoch: 2/2, step 4562/7134 completed (loss: 0.02873818576335907, acc: 0.9871086478233337)
[2025-02-13 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:39][root][INFO] - Training Epoch: 2/2, step 4563/7134 completed (loss: 0.021565398201346397, acc: 0.9954198598861694)
[2025-02-13 04:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:39][root][INFO] - Training Epoch: 2/2, step 4564/7134 completed (loss: 0.05076434463262558, acc: 0.9858956336975098)
[2025-02-13 04:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40][root][INFO] - Training Epoch: 2/2, step 4565/7134 completed (loss: 0.016848644241690636, acc: 0.9923518300056458)
[2025-02-13 04:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40][root][INFO] - Training Epoch: 2/2, step 4566/7134 completed (loss: 0.033964384347200394, acc: 0.9910394549369812)
[2025-02-13 04:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40][root][INFO] - Training Epoch: 2/2, step 4567/7134 completed (loss: 0.01747187227010727, acc: 0.998161792755127)
[2025-02-13 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:41][root][INFO] - Training Epoch: 2/2, step 4568/7134 completed (loss: 0.04303298890590668, acc: 0.9885321259498596)
[2025-02-13 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:41][root][INFO] - Training Epoch: 2/2, step 4569/7134 completed (loss: 0.053872063755989075, acc: 0.9921568632125854)
[2025-02-13 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42][root][INFO] - Training Epoch: 2/2, step 4570/7134 completed (loss: 0.010379893705248833, acc: 0.9985422492027283)
[2025-02-13 04:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42][root][INFO] - Training Epoch: 2/2, step 4571/7134 completed (loss: 0.014024462550878525, acc: 0.9961488842964172)
[2025-02-13 04:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43][root][INFO] - Training Epoch: 2/2, step 4572/7134 completed (loss: 0.02142958715558052, acc: 0.9937694668769836)
[2025-02-13 04:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43][root][INFO] - Training Epoch: 2/2, step 4573/7134 completed (loss: 0.01594778336584568, acc: 0.9957982897758484)
[2025-02-13 04:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43][root][INFO] - Training Epoch: 2/2, step 4574/7134 completed (loss: 0.00718500092625618, acc: 0.9964664578437805)
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44][root][INFO] - Training Epoch: 2/2, step 4575/7134 completed (loss: 0.02777063101530075, acc: 0.9906914830207825)
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44][root][INFO] - Training Epoch: 2/2, step 4576/7134 completed (loss: 0.010065040551126003, acc: 0.9974905848503113)
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45][root][INFO] - Training Epoch: 2/2, step 4577/7134 completed (loss: 0.028341814875602722, acc: 0.9943820238113403)
[2025-02-13 04:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45][root][INFO] - Training Epoch: 2/2, step 4578/7134 completed (loss: 0.007165874820202589, acc: 0.9985443949699402)
[2025-02-13 04:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46][root][INFO] - Training Epoch: 2/2, step 4579/7134 completed (loss: 0.03587806969881058, acc: 0.9920760989189148)
[2025-02-13 04:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46][root][INFO] - Training Epoch: 2/2, step 4580/7134 completed (loss: 0.014676179736852646, acc: 0.9955223798751831)
[2025-02-13 04:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46][root][INFO] - Training Epoch: 2/2, step 4581/7134 completed (loss: 0.008828756399452686, acc: 0.998236358165741)
[2025-02-13 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47][root][INFO] - Training Epoch: 2/2, step 4582/7134 completed (loss: 0.012756011448800564, acc: 0.9950658082962036)
[2025-02-13 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47][root][INFO] - Training Epoch: 2/2, step 4583/7134 completed (loss: 0.005963156931102276, acc: 0.9967319965362549)
[2025-02-13 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48][root][INFO] - Training Epoch: 2/2, step 4584/7134 completed (loss: 0.007610308937728405, acc: 0.9967793822288513)
[2025-02-13 04:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48][root][INFO] - Training Epoch: 2/2, step 4585/7134 completed (loss: 0.009171788580715656, acc: 0.9967637658119202)
[2025-02-13 04:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49][root][INFO] - Training Epoch: 2/2, step 4586/7134 completed (loss: 0.01816580444574356, acc: 0.9960988163948059)
[2025-02-13 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49][root][INFO] - Training Epoch: 2/2, step 4587/7134 completed (loss: 0.02048696205019951, acc: 0.9972602725028992)
[2025-02-13 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50][root][INFO] - Training Epoch: 2/2, step 4588/7134 completed (loss: 0.036470700055360794, acc: 0.9840116500854492)
[2025-02-13 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50][root][INFO] - Training Epoch: 2/2, step 4589/7134 completed (loss: 0.04429613798856735, acc: 0.9835329055786133)
[2025-02-13 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50][root][INFO] - Training Epoch: 2/2, step 4590/7134 completed (loss: 0.028992341831326485, acc: 0.9930939078330994)
[2025-02-13 04:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:51][root][INFO] - Training Epoch: 2/2, step 4591/7134 completed (loss: 0.018703294917941093, acc: 0.9947229623794556)
[2025-02-13 04:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:51][root][INFO] - Training Epoch: 2/2, step 4592/7134 completed (loss: 0.02799845300614834, acc: 0.9883889555931091)
[2025-02-13 04:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52][root][INFO] - Training Epoch: 2/2, step 4593/7134 completed (loss: 0.02649558149278164, acc: 0.9940740466117859)
[2025-02-13 04:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52][root][INFO] - Training Epoch: 2/2, step 4594/7134 completed (loss: 0.03494524210691452, acc: 0.9919999837875366)
[2025-02-13 04:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:53][root][INFO] - Training Epoch: 2/2, step 4595/7134 completed (loss: 0.014759001322090626, acc: 0.9925705790519714)
[2025-02-13 04:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:53][root][INFO] - Training Epoch: 2/2, step 4596/7134 completed (loss: 0.024834346026182175, acc: 0.9873015880584717)
[2025-02-13 04:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:53][root][INFO] - Training Epoch: 2/2, step 4597/7134 completed (loss: 0.02009638398885727, acc: 0.9939246773719788)
[2025-02-13 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54][root][INFO] - Training Epoch: 2/2, step 4598/7134 completed (loss: 0.04282059893012047, acc: 0.9889570474624634)
[2025-02-13 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54][root][INFO] - Training Epoch: 2/2, step 4599/7134 completed (loss: 0.012206783518195152, acc: 0.995230495929718)
[2025-02-13 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55][root][INFO] - Training Epoch: 2/2, step 4600/7134 completed (loss: 0.014604726806282997, acc: 0.9972936511039734)
[2025-02-13 04:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55][root][INFO] - Training Epoch: 2/2, step 4601/7134 completed (loss: 0.03495338559150696, acc: 0.9895697236061096)
[2025-02-13 04:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56][root][INFO] - Training Epoch: 2/2, step 4602/7134 completed (loss: 0.017142634838819504, acc: 0.9931880235671997)
[2025-02-13 04:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56][root][INFO] - Training Epoch: 2/2, step 4603/7134 completed (loss: 0.027522390708327293, acc: 0.9931034445762634)
[2025-02-13 04:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57][root][INFO] - Training Epoch: 2/2, step 4604/7134 completed (loss: 0.010090595111250877, acc: 0.9983948469161987)
[2025-02-13 04:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57][root][INFO] - Training Epoch: 2/2, step 4605/7134 completed (loss: 0.0378149189054966, acc: 0.9868995547294617)
[2025-02-13 04:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57][root][INFO] - Training Epoch: 2/2, step 4606/7134 completed (loss: 0.012760724872350693, acc: 0.9969419240951538)
[2025-02-13 04:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58][root][INFO] - Training Epoch: 2/2, step 4607/7134 completed (loss: 0.013001596555113792, acc: 0.996632993221283)
[2025-02-13 04:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58][root][INFO] - Training Epoch: 2/2, step 4608/7134 completed (loss: 0.010263175703585148, acc: 0.9970370531082153)
[2025-02-13 04:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:59][root][INFO] - Training Epoch: 2/2, step 4609/7134 completed (loss: 0.019192472100257874, acc: 0.9944953918457031)
[2025-02-13 04:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:59][root][INFO] - Training Epoch: 2/2, step 4610/7134 completed (loss: 0.014096105471253395, acc: 0.9960159659385681)
[2025-02-13 04:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00][root][INFO] - Training Epoch: 2/2, step 4611/7134 completed (loss: 0.006226222030818462, acc: 0.9984350800514221)
[2025-02-13 04:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00][root][INFO] - Training Epoch: 2/2, step 4612/7134 completed (loss: 0.014441591687500477, acc: 0.9962825179100037)
[2025-02-13 04:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01][root][INFO] - Training Epoch: 2/2, step 4613/7134 completed (loss: 0.015877271071076393, acc: 0.9965277910232544)
[2025-02-13 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01][root][INFO] - Training Epoch: 2/2, step 4614/7134 completed (loss: 0.023985644802451134, acc: 0.9924242496490479)
[2025-02-13 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01][root][INFO] - Training Epoch: 2/2, step 4615/7134 completed (loss: 0.0357479564845562, acc: 0.9864314794540405)
[2025-02-13 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02][root][INFO] - Training Epoch: 2/2, step 4616/7134 completed (loss: 0.03143344819545746, acc: 0.9926035404205322)
[2025-02-13 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02][root][INFO] - Training Epoch: 2/2, step 4617/7134 completed (loss: 0.022993767634034157, acc: 0.9894894957542419)
[2025-02-13 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03][root][INFO] - Training Epoch: 2/2, step 4618/7134 completed (loss: 0.01724112220108509, acc: 0.9980582594871521)
[2025-02-13 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03][root][INFO] - Training Epoch: 2/2, step 4619/7134 completed (loss: 0.006869052071124315, acc: 1.0)
[2025-02-13 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:04][root][INFO] - Training Epoch: 2/2, step 4620/7134 completed (loss: 0.0066736433655023575, acc: 0.9985507130622864)
[2025-02-13 04:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:04][root][INFO] - Training Epoch: 2/2, step 4621/7134 completed (loss: 0.008572596125304699, acc: 0.9969135522842407)
[2025-02-13 04:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:04][root][INFO] - Training Epoch: 2/2, step 4622/7134 completed (loss: 0.01625985838472843, acc: 0.9937577843666077)
[2025-02-13 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:05][root][INFO] - Training Epoch: 2/2, step 4623/7134 completed (loss: 0.03407871350646019, acc: 0.9895424842834473)
[2025-02-13 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:05][root][INFO] - Training Epoch: 2/2, step 4624/7134 completed (loss: 0.04460837319493294, acc: 0.9876543283462524)
[2025-02-13 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:06][root][INFO] - Training Epoch: 2/2, step 4625/7134 completed (loss: 0.07288278639316559, acc: 0.9786535501480103)
[2025-02-13 04:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:06][root][INFO] - Training Epoch: 2/2, step 4626/7134 completed (loss: 0.037579599767923355, acc: 0.991946280002594)
[2025-02-13 04:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07][root][INFO] - Training Epoch: 2/2, step 4627/7134 completed (loss: 0.049850862473249435, acc: 0.9893778562545776)
[2025-02-13 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07][root][INFO] - Training Epoch: 2/2, step 4628/7134 completed (loss: 0.03266338258981705, acc: 0.9927536249160767)
[2025-02-13 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08][root][INFO] - Training Epoch: 2/2, step 4629/7134 completed (loss: 0.04508610814809799, acc: 0.988034188747406)
[2025-02-13 04:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08][root][INFO] - Training Epoch: 2/2, step 4630/7134 completed (loss: 0.04803212359547615, acc: 0.9881109595298767)
[2025-02-13 04:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08][root][INFO] - Training Epoch: 2/2, step 4631/7134 completed (loss: 0.021989189088344574, acc: 0.9900442361831665)
[2025-02-13 04:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:09][root][INFO] - Training Epoch: 2/2, step 4632/7134 completed (loss: 0.02733466774225235, acc: 0.9941176176071167)
[2025-02-13 04:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:09][root][INFO] - Training Epoch: 2/2, step 4633/7134 completed (loss: 0.029812054708600044, acc: 0.9930635690689087)
[2025-02-13 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10][root][INFO] - Training Epoch: 2/2, step 4634/7134 completed (loss: 0.020854029804468155, acc: 0.9922330379486084)
[2025-02-13 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10][root][INFO] - Training Epoch: 2/2, step 4635/7134 completed (loss: 0.04520949348807335, acc: 0.9908536672592163)
[2025-02-13 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11][root][INFO] - Training Epoch: 2/2, step 4636/7134 completed (loss: 0.026223760098218918, acc: 0.9974226951599121)
[2025-02-13 04:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11][root][INFO] - Training Epoch: 2/2, step 4637/7134 completed (loss: 0.008932771161198616, acc: 0.9979838728904724)
[2025-02-13 04:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12][root][INFO] - Training Epoch: 2/2, step 4638/7134 completed (loss: 0.030413178727030754, acc: 0.9881109595298767)
[2025-02-13 04:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12][root][INFO] - Training Epoch: 2/2, step 4639/7134 completed (loss: 0.036081358790397644, acc: 0.9914841651916504)
[2025-02-13 04:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12][root][INFO] - Training Epoch: 2/2, step 4640/7134 completed (loss: 0.030348025262355804, acc: 0.9897435903549194)
[2025-02-13 04:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:13][root][INFO] - Training Epoch: 2/2, step 4641/7134 completed (loss: 0.016455944627523422, acc: 0.9943181872367859)
[2025-02-13 04:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:13][root][INFO] - Training Epoch: 2/2, step 4642/7134 completed (loss: 0.011918698437511921, acc: 0.9960317611694336)
[2025-02-13 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14][root][INFO] - Training Epoch: 2/2, step 4643/7134 completed (loss: 0.0191050935536623, acc: 0.997633159160614)
[2025-02-13 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14][root][INFO] - Training Epoch: 2/2, step 4644/7134 completed (loss: 0.015937242656946182, acc: 0.9952662587165833)
[2025-02-13 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15][root][INFO] - Training Epoch: 2/2, step 4645/7134 completed (loss: 0.008285964839160442, acc: 0.9958275556564331)
[2025-02-13 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15][root][INFO] - Training Epoch: 2/2, step 4646/7134 completed (loss: 0.007381882984191179, acc: 1.0)
[2025-02-13 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16][root][INFO] - Training Epoch: 2/2, step 4647/7134 completed (loss: 0.06346482783555984, acc: 0.9908854365348816)
[2025-02-13 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16][root][INFO] - Training Epoch: 2/2, step 4648/7134 completed (loss: 0.0200058426707983, acc: 0.991769552230835)
[2025-02-13 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17][root][INFO] - Training Epoch: 2/2, step 4649/7134 completed (loss: 0.024539180099964142, acc: 0.9912434220314026)
[2025-02-13 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17][root][INFO] - Training Epoch: 2/2, step 4650/7134 completed (loss: 0.02344675362110138, acc: 0.9910714030265808)
[2025-02-13 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18][root][INFO] - Training Epoch: 2/2, step 4651/7134 completed (loss: 0.05548548325896263, acc: 0.9860582947731018)
[2025-02-13 04:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18][root][INFO] - Training Epoch: 2/2, step 4652/7134 completed (loss: 0.018971743062138557, acc: 0.9939024448394775)
[2025-02-13 04:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18][root][INFO] - Training Epoch: 2/2, step 4653/7134 completed (loss: 0.03169996663928032, acc: 0.9903225898742676)
[2025-02-13 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19][root][INFO] - Training Epoch: 2/2, step 4654/7134 completed (loss: 0.050376396626234055, acc: 0.9841656684875488)
[2025-02-13 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19][root][INFO] - Training Epoch: 2/2, step 4655/7134 completed (loss: 0.03174066171050072, acc: 0.9931600689888)
[2025-02-13 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20][root][INFO] - Training Epoch: 2/2, step 4656/7134 completed (loss: 0.023434363305568695, acc: 0.9943181872367859)
[2025-02-13 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20][root][INFO] - Training Epoch: 2/2, step 4657/7134 completed (loss: 0.046865906566381454, acc: 0.9886685609817505)
[2025-02-13 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21][root][INFO] - Training Epoch: 2/2, step 4658/7134 completed (loss: 0.02887731045484543, acc: 0.9932340979576111)
[2025-02-13 04:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21][root][INFO] - Training Epoch: 2/2, step 4659/7134 completed (loss: 0.043487560003995895, acc: 0.9884318709373474)
[2025-02-13 04:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22][root][INFO] - Training Epoch: 2/2, step 4660/7134 completed (loss: 0.0481858029961586, acc: 0.9874607920646667)
[2025-02-13 04:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22][root][INFO] - Training Epoch: 2/2, step 4661/7134 completed (loss: 0.02496497891843319, acc: 0.9939024448394775)
[2025-02-13 04:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22][root][INFO] - Training Epoch: 2/2, step 4662/7134 completed (loss: 0.06155923008918762, acc: 0.9829931855201721)
[2025-02-13 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:23][root][INFO] - Training Epoch: 2/2, step 4663/7134 completed (loss: 0.04479699954390526, acc: 0.9879679083824158)
[2025-02-13 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:23][root][INFO] - Training Epoch: 2/2, step 4664/7134 completed (loss: 0.024281175807118416, acc: 0.9920739531517029)
[2025-02-13 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24][root][INFO] - Training Epoch: 2/2, step 4665/7134 completed (loss: 0.016079260036349297, acc: 0.9972714781761169)
[2025-02-13 04:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24][root][INFO] - Training Epoch: 2/2, step 4666/7134 completed (loss: 0.03469237685203552, acc: 0.9895697236061096)
[2025-02-13 04:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25][root][INFO] - Training Epoch: 2/2, step 4667/7134 completed (loss: 0.041090723127126694, acc: 0.9845094680786133)
[2025-02-13 04:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25][root][INFO] - Training Epoch: 2/2, step 4668/7134 completed (loss: 0.013522789813578129, acc: 0.9946236610412598)
[2025-02-13 04:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25][root][INFO] - Training Epoch: 2/2, step 4669/7134 completed (loss: 0.01608424074947834, acc: 0.9940828680992126)
[2025-02-13 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26][root][INFO] - Training Epoch: 2/2, step 4670/7134 completed (loss: 0.03673743084073067, acc: 0.988304078578949)
[2025-02-13 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26][root][INFO] - Training Epoch: 2/2, step 4671/7134 completed (loss: 0.03970761597156525, acc: 0.9844852089881897)
[2025-02-13 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27][root][INFO] - Training Epoch: 2/2, step 4672/7134 completed (loss: 0.026296712458133698, acc: 0.99303138256073)
[2025-02-13 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27][root][INFO] - Training Epoch: 2/2, step 4673/7134 completed (loss: 0.032512862235307693, acc: 0.9921996593475342)
[2025-02-13 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28][root][INFO] - Training Epoch: 2/2, step 4674/7134 completed (loss: 0.030176540836691856, acc: 0.9936608672142029)
[2025-02-13 04:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28][root][INFO] - Training Epoch: 2/2, step 4675/7134 completed (loss: 0.0290052592754364, acc: 0.9923567175865173)
[2025-02-13 04:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28][root][INFO] - Training Epoch: 2/2, step 4676/7134 completed (loss: 0.022766895592212677, acc: 0.9969879388809204)
[2025-02-13 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29][root][INFO] - Training Epoch: 2/2, step 4677/7134 completed (loss: 0.029395096004009247, acc: 0.9912280440330505)
[2025-02-13 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29][root][INFO] - Training Epoch: 2/2, step 4678/7134 completed (loss: 0.021851519122719765, acc: 0.9970282316207886)
[2025-02-13 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:30][root][INFO] - Training Epoch: 2/2, step 4679/7134 completed (loss: 0.01930389180779457, acc: 0.9917241334915161)
[2025-02-13 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:30][root][INFO] - Training Epoch: 2/2, step 4680/7134 completed (loss: 0.03899998217821121, acc: 0.9906542301177979)
[2025-02-13 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31][root][INFO] - Training Epoch: 2/2, step 4681/7134 completed (loss: 0.032966651022434235, acc: 0.9953846335411072)
[2025-02-13 04:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31][root][INFO] - Training Epoch: 2/2, step 4682/7134 completed (loss: 0.013289266265928745, acc: 0.9963503479957581)
[2025-02-13 04:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31][root][INFO] - Training Epoch: 2/2, step 4683/7134 completed (loss: 0.018075881525874138, acc: 0.995720386505127)
[2025-02-13 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32][root][INFO] - Training Epoch: 2/2, step 4684/7134 completed (loss: 0.04784268140792847, acc: 0.9863731861114502)
[2025-02-13 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32][root][INFO] - Training Epoch: 2/2, step 4685/7134 completed (loss: 0.017719201743602753, acc: 0.9948770403862)
[2025-02-13 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33][root][INFO] - Training Epoch: 2/2, step 4686/7134 completed (loss: 0.021581381559371948, acc: 0.9930151104927063)
[2025-02-13 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33][root][INFO] - Training Epoch: 2/2, step 4687/7134 completed (loss: 0.07271651178598404, acc: 0.9754601120948792)
[2025-02-13 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34][root][INFO] - Training Epoch: 2/2, step 4688/7134 completed (loss: 0.10502436012029648, acc: 0.9733059406280518)
[2025-02-13 04:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34][root][INFO] - Training Epoch: 2/2, step 4689/7134 completed (loss: 0.07724642753601074, acc: 0.9779086709022522)
[2025-02-13 04:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35][root][INFO] - Training Epoch: 2/2, step 4690/7134 completed (loss: 0.06894785165786743, acc: 0.9832285046577454)
[2025-02-13 04:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35][root][INFO] - Training Epoch: 2/2, step 4691/7134 completed (loss: 0.0204412043094635, acc: 0.9920634627342224)
[2025-02-13 04:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36][root][INFO] - Training Epoch: 2/2, step 4692/7134 completed (loss: 0.07718957960605621, acc: 0.9766454100608826)
[2025-02-13 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36][root][INFO] - Training Epoch: 2/2, step 4693/7134 completed (loss: 0.10648622363805771, acc: 0.9729323387145996)
[2025-02-13 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36][root][INFO] - Training Epoch: 2/2, step 4694/7134 completed (loss: 0.10044698417186737, acc: 0.9776875972747803)
[2025-02-13 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37][root][INFO] - Training Epoch: 2/2, step 4695/7134 completed (loss: 0.035415586084127426, acc: 0.9901477694511414)
[2025-02-13 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37][root][INFO] - Training Epoch: 2/2, step 4696/7134 completed (loss: 0.014339178800582886, acc: 0.9925705790519714)
[2025-02-13 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:38][root][INFO] - Training Epoch: 2/2, step 4697/7134 completed (loss: 0.019005563110113144, acc: 0.9943609237670898)
[2025-02-13 04:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:38][root][INFO] - Training Epoch: 2/2, step 4698/7134 completed (loss: 0.068391352891922, acc: 0.9790076613426208)
[2025-02-13 04:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39][root][INFO] - Training Epoch: 2/2, step 4699/7134 completed (loss: 0.04287306219339371, acc: 0.9890310764312744)
[2025-02-13 04:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39][root][INFO] - Training Epoch: 2/2, step 4700/7134 completed (loss: 0.041047342121601105, acc: 0.9850968718528748)
[2025-02-13 04:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39][root][INFO] - Training Epoch: 2/2, step 4701/7134 completed (loss: 0.02777819335460663, acc: 0.9904153347015381)
[2025-02-13 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40][root][INFO] - Training Epoch: 2/2, step 4702/7134 completed (loss: 0.017077861353754997, acc: 0.993630588054657)
[2025-02-13 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40][root][INFO] - Training Epoch: 2/2, step 4703/7134 completed (loss: 0.03239838778972626, acc: 0.9945945739746094)
[2025-02-13 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41][root][INFO] - Training Epoch: 2/2, step 4704/7134 completed (loss: 0.010613677091896534, acc: 0.9953106641769409)
[2025-02-13 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41][root][INFO] - Training Epoch: 2/2, step 4705/7134 completed (loss: 0.016136623919010162, acc: 0.9941520690917969)
[2025-02-13 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42][root][INFO] - Training Epoch: 2/2, step 4706/7134 completed (loss: 0.04760440066456795, acc: 0.985981285572052)
[2025-02-13 04:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42][root][INFO] - Training Epoch: 2/2, step 4707/7134 completed (loss: 0.1386498212814331, acc: 0.9693877696990967)
[2025-02-13 04:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43][root][INFO] - Training Epoch: 2/2, step 4708/7134 completed (loss: 0.013032611459493637, acc: 0.996259331703186)
[2025-02-13 04:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43][root][INFO] - Training Epoch: 2/2, step 4709/7134 completed (loss: 0.016959089785814285, acc: 0.9960474371910095)
[2025-02-13 04:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43][root][INFO] - Training Epoch: 2/2, step 4710/7134 completed (loss: 0.02304580621421337, acc: 0.9912663698196411)
[2025-02-13 04:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44][root][INFO] - Training Epoch: 2/2, step 4711/7134 completed (loss: 0.040766116231679916, acc: 0.9864176511764526)
[2025-02-13 04:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44][root][INFO] - Training Epoch: 2/2, step 4712/7134 completed (loss: 0.03907470405101776, acc: 0.9848484992980957)
[2025-02-13 04:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45][root][INFO] - Training Epoch: 2/2, step 4713/7134 completed (loss: 0.01989317312836647, acc: 0.9964285492897034)
[2025-02-13 04:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45][root][INFO] - Training Epoch: 2/2, step 4714/7134 completed (loss: 0.03137938678264618, acc: 0.9943925142288208)
[2025-02-13 04:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46][root][INFO] - Training Epoch: 2/2, step 4715/7134 completed (loss: 0.0738508328795433, acc: 0.9786780476570129)
[2025-02-13 04:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46][root][INFO] - Training Epoch: 2/2, step 4716/7134 completed (loss: 0.006706702057272196, acc: 0.9984756112098694)
[2025-02-13 04:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46][root][INFO] - Training Epoch: 2/2, step 4717/7134 completed (loss: 0.027063556015491486, acc: 0.995555579662323)
[2025-02-13 04:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:47][root][INFO] - Training Epoch: 2/2, step 4718/7134 completed (loss: 0.01837044022977352, acc: 0.996688723564148)
[2025-02-13 04:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:47][root][INFO] - Training Epoch: 2/2, step 4719/7134 completed (loss: 0.015743695199489594, acc: 0.9941002726554871)
[2025-02-13 04:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:48][root][INFO] - Training Epoch: 2/2, step 4720/7134 completed (loss: 0.011169705539941788, acc: 0.9957924485206604)
[2025-02-13 04:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:48][root][INFO] - Training Epoch: 2/2, step 4721/7134 completed (loss: 0.0026345502119511366, acc: 1.0)
[2025-02-13 04:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49][root][INFO] - Training Epoch: 2/2, step 4722/7134 completed (loss: 0.008106643334031105, acc: 0.9985207319259644)
[2025-02-13 04:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49][root][INFO] - Training Epoch: 2/2, step 4723/7134 completed (loss: 0.007379468064755201, acc: 0.9969512224197388)
[2025-02-13 04:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49][root][INFO] - Training Epoch: 2/2, step 4724/7134 completed (loss: 0.030084948986768723, acc: 0.9896449446678162)
[2025-02-13 04:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50][root][INFO] - Training Epoch: 2/2, step 4725/7134 completed (loss: 0.004950189962983131, acc: 0.9986807107925415)
[2025-02-13 04:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50][root][INFO] - Training Epoch: 2/2, step 4726/7134 completed (loss: 0.03085464797914028, acc: 0.9886524677276611)
[2025-02-13 04:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:51][root][INFO] - Training Epoch: 2/2, step 4727/7134 completed (loss: 0.03126886487007141, acc: 0.9905956387519836)
[2025-02-13 04:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:51][root][INFO] - Training Epoch: 2/2, step 4728/7134 completed (loss: 0.026942187920212746, acc: 0.9917627573013306)
[2025-02-13 04:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52][root][INFO] - Training Epoch: 2/2, step 4729/7134 completed (loss: 0.011741465888917446, acc: 0.9957746267318726)
[2025-02-13 04:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52][root][INFO] - Training Epoch: 2/2, step 4730/7134 completed (loss: 0.01928744651377201, acc: 0.9892802238464355)
[2025-02-13 04:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52][root][INFO] - Training Epoch: 2/2, step 4731/7134 completed (loss: 0.03167414665222168, acc: 0.9938931465148926)
[2025-02-13 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53][root][INFO] - Training Epoch: 2/2, step 4732/7134 completed (loss: 0.043865106999874115, acc: 0.9854809641838074)
[2025-02-13 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53][root][INFO] - Training Epoch: 2/2, step 4733/7134 completed (loss: 0.008764716796576977, acc: 0.9967690110206604)
[2025-02-13 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54][root][INFO] - Training Epoch: 2/2, step 4734/7134 completed (loss: 0.030033960938453674, acc: 0.9888712167739868)
[2025-02-13 04:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54][root][INFO] - Training Epoch: 2/2, step 4735/7134 completed (loss: 0.0163594800978899, acc: 0.9924924969673157)
[2025-02-13 04:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:55][root][INFO] - Training Epoch: 2/2, step 4736/7134 completed (loss: 0.050010163336992264, acc: 0.990138053894043)
[2025-02-13 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:55][root][INFO] - Training Epoch: 2/2, step 4737/7134 completed (loss: 0.01262197270989418, acc: 0.9968652129173279)
[2025-02-13 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:55][root][INFO] - Training Epoch: 2/2, step 4738/7134 completed (loss: 0.015447570011019707, acc: 0.9954476356506348)
[2025-02-13 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56][root][INFO] - Training Epoch: 2/2, step 4739/7134 completed (loss: 0.08462633192539215, acc: 0.97444087266922)
[2025-02-13 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56][root][INFO] - Training Epoch: 2/2, step 4740/7134 completed (loss: 0.045698415488004684, acc: 0.9820359349250793)
[2025-02-13 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57][root][INFO] - Training Epoch: 2/2, step 4741/7134 completed (loss: 0.023566944524645805, acc: 0.9889094233512878)
[2025-02-13 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57][root][INFO] - Training Epoch: 2/2, step 4742/7134 completed (loss: 0.0962095633149147, acc: 0.9805309772491455)
[2025-02-13 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57][root][INFO] - Training Epoch: 2/2, step 4743/7134 completed (loss: 0.01991928368806839, acc: 0.9911032319068909)
[2025-02-13 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58][root][INFO] - Training Epoch: 2/2, step 4744/7134 completed (loss: 0.03208072483539581, acc: 0.9896694421768188)
[2025-02-13 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58][root][INFO] - Training Epoch: 2/2, step 4745/7134 completed (loss: 0.02001599408686161, acc: 0.9966555237770081)
[2025-02-13 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:59][root][INFO] - Training Epoch: 2/2, step 4746/7134 completed (loss: 0.01610085926949978, acc: 0.9963235259056091)
[2025-02-13 04:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:59][root][INFO] - Training Epoch: 2/2, step 4747/7134 completed (loss: 0.018485428765416145, acc: 0.9932523369789124)
[2025-02-13 04:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00][root][INFO] - Training Epoch: 2/2, step 4748/7134 completed (loss: 0.03894352540373802, acc: 0.9867256879806519)
[2025-02-13 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00][root][INFO] - Training Epoch: 2/2, step 4749/7134 completed (loss: 0.029316559433937073, acc: 0.9946666955947876)
[2025-02-13 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00][root][INFO] - Training Epoch: 2/2, step 4750/7134 completed (loss: 0.07036811113357544, acc: 0.9845857620239258)
[2025-02-13 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01][root][INFO] - Training Epoch: 2/2, step 4751/7134 completed (loss: 0.011866620741784573, acc: 0.99717116355896)
[2025-02-13 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01][root][INFO] - Training Epoch: 2/2, step 4752/7134 completed (loss: 0.03251253068447113, acc: 0.9938555955886841)
[2025-02-13 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02][root][INFO] - Training Epoch: 2/2, step 4753/7134 completed (loss: 0.06343602389097214, acc: 0.9864457845687866)
[2025-02-13 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02][root][INFO] - Training Epoch: 2/2, step 4754/7134 completed (loss: 0.006697850301861763, acc: 0.997063159942627)
[2025-02-13 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03][root][INFO] - Training Epoch: 2/2, step 4755/7134 completed (loss: 0.041654836386442184, acc: 0.9875173568725586)
[2025-02-13 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03][root][INFO] - Training Epoch: 2/2, step 4756/7134 completed (loss: 0.04598311334848404, acc: 0.9895397424697876)
[2025-02-13 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04][root][INFO] - Training Epoch: 2/2, step 4757/7134 completed (loss: 0.018550850450992584, acc: 0.9954075813293457)
[2025-02-13 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04][root][INFO] - Training Epoch: 2/2, step 4758/7134 completed (loss: 0.026922043412923813, acc: 0.9901960492134094)
[2025-02-13 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05][root][INFO] - Training Epoch: 2/2, step 4759/7134 completed (loss: 0.009282552637159824, acc: 0.998275876045227)
[2025-02-13 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05][root][INFO] - Training Epoch: 2/2, step 4760/7134 completed (loss: 0.0248354934155941, acc: 0.9937888383865356)
[2025-02-13 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05][root][INFO] - Training Epoch: 2/2, step 4761/7134 completed (loss: 0.05063212290406227, acc: 0.9923664331436157)
[2025-02-13 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06][root][INFO] - Training Epoch: 2/2, step 4762/7134 completed (loss: 0.047053415328264236, acc: 0.9891892075538635)
[2025-02-13 04:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06][root][INFO] - Training Epoch: 2/2, step 4763/7134 completed (loss: 0.044835761189460754, acc: 0.9893617033958435)
[2025-02-13 04:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:07][root][INFO] - Training Epoch: 2/2, step 4764/7134 completed (loss: 0.03264157474040985, acc: 0.9925834536552429)
[2025-02-13 04:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:07][root][INFO] - Training Epoch: 2/2, step 4765/7134 completed (loss: 0.03626174479722977, acc: 0.9913669228553772)
[2025-02-13 04:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08][root][INFO] - Training Epoch: 2/2, step 4766/7134 completed (loss: 0.02924909070134163, acc: 0.9909909963607788)
[2025-02-13 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08][root][INFO] - Training Epoch: 2/2, step 4767/7134 completed (loss: 0.03032250702381134, acc: 0.9898219108581543)
[2025-02-13 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08][root][INFO] - Training Epoch: 2/2, step 4768/7134 completed (loss: 0.05504150316119194, acc: 0.9850948452949524)
[2025-02-13 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09][root][INFO] - Training Epoch: 2/2, step 4769/7134 completed (loss: 0.03917459398508072, acc: 0.9919742941856384)
[2025-02-13 04:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09][root][INFO] - Training Epoch: 2/2, step 4770/7134 completed (loss: 0.018673010170459747, acc: 0.9939393997192383)
[2025-02-13 04:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10][root][INFO] - Training Epoch: 2/2, step 4771/7134 completed (loss: 0.04252961277961731, acc: 0.9840116500854492)
[2025-02-13 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10][root][INFO] - Training Epoch: 2/2, step 4772/7134 completed (loss: 0.029327964410185814, acc: 0.9968101978302002)
[2025-02-13 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10][root][INFO] - Training Epoch: 2/2, step 4773/7134 completed (loss: 0.020092356950044632, acc: 0.9972602725028992)
[2025-02-13 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11][root][INFO] - Training Epoch: 2/2, step 4774/7134 completed (loss: 0.027068277820944786, acc: 0.9943820238113403)
[2025-02-13 04:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11][root][INFO] - Training Epoch: 2/2, step 4775/7134 completed (loss: 0.027219120413064957, acc: 0.9919484853744507)
[2025-02-13 04:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:12][root][INFO] - Training Epoch: 2/2, step 4776/7134 completed (loss: 0.027393396943807602, acc: 0.9919742941856384)
[2025-02-13 04:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:12][root][INFO] - Training Epoch: 2/2, step 4777/7134 completed (loss: 0.01195058785378933, acc: 0.9966611266136169)
[2025-02-13 04:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13][root][INFO] - Training Epoch: 2/2, step 4778/7134 completed (loss: 0.03206496313214302, acc: 0.9912472367286682)
[2025-02-13 04:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13][root][INFO] - Training Epoch: 2/2, step 4779/7134 completed (loss: 0.014062107540667057, acc: 0.9944751262664795)
[2025-02-13 04:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13][root][INFO] - Training Epoch: 2/2, step 4780/7134 completed (loss: 0.008560233749449253, acc: 0.9984423518180847)
[2025-02-13 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14][root][INFO] - Training Epoch: 2/2, step 4781/7134 completed (loss: 0.010359161533415318, acc: 0.9960861206054688)
[2025-02-13 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14][root][INFO] - Training Epoch: 2/2, step 4782/7134 completed (loss: 0.012285491451621056, acc: 0.9967585206031799)
[2025-02-13 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:15][root][INFO] - Training Epoch: 2/2, step 4783/7134 completed (loss: 0.052782461047172546, acc: 0.9857650995254517)
[2025-02-13 04:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:15][root][INFO] - Training Epoch: 2/2, step 4784/7134 completed (loss: 0.023132584989070892, acc: 0.9959072470664978)
[2025-02-13 04:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16][root][INFO] - Training Epoch: 2/2, step 4785/7134 completed (loss: 0.037755873054265976, acc: 0.9943820238113403)
[2025-02-13 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16][root][INFO] - Training Epoch: 2/2, step 4786/7134 completed (loss: 0.027104970067739487, acc: 0.9906832575798035)
[2025-02-13 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16][root][INFO] - Training Epoch: 2/2, step 4787/7134 completed (loss: 0.0035090262535959482, acc: 1.0)
[2025-02-13 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17][root][INFO] - Training Epoch: 2/2, step 4788/7134 completed (loss: 0.00402067182585597, acc: 0.9982269406318665)
[2025-02-13 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17][root][INFO] - Training Epoch: 2/2, step 4789/7134 completed (loss: 0.023040644824504852, acc: 0.9920381903648376)
[2025-02-13 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:18][root][INFO] - Training Epoch: 2/2, step 4790/7134 completed (loss: 0.017989972606301308, acc: 0.9959239363670349)
[2025-02-13 04:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:18][root][INFO] - Training Epoch: 2/2, step 4791/7134 completed (loss: 0.009797337464988232, acc: 0.9962756037712097)
[2025-02-13 04:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:18][root][INFO] - Training Epoch: 2/2, step 4792/7134 completed (loss: 0.016015060245990753, acc: 0.9916387796401978)
[2025-02-13 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19][root][INFO] - Training Epoch: 2/2, step 4793/7134 completed (loss: 0.03772926703095436, acc: 0.9890410900115967)
[2025-02-13 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19][root][INFO] - Training Epoch: 2/2, step 4794/7134 completed (loss: 0.053764961659908295, acc: 0.9847618937492371)
[2025-02-13 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20][root][INFO] - Training Epoch: 2/2, step 4795/7134 completed (loss: 0.01515893917530775, acc: 0.9957627058029175)
[2025-02-13 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20][root][INFO] - Training Epoch: 2/2, step 4796/7134 completed (loss: 0.020296918228268623, acc: 0.9937106966972351)
[2025-02-13 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:21][root][INFO] - Training Epoch: 2/2, step 4797/7134 completed (loss: 0.024399902671575546, acc: 0.9937888383865356)
[2025-02-13 04:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:21][root][INFO] - Training Epoch: 2/2, step 4798/7134 completed (loss: 0.019748233258724213, acc: 0.9920106530189514)
[2025-02-13 04:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:21][root][INFO] - Training Epoch: 2/2, step 4799/7134 completed (loss: 0.016184432432055473, acc: 0.9929278492927551)
[2025-02-13 04:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:22][root][INFO] - Training Epoch: 2/2, step 4800/7134 completed (loss: 0.02245679683983326, acc: 0.9940387606620789)
[2025-02-13 04:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:22][root][INFO] - Training Epoch: 2/2, step 4801/7134 completed (loss: 0.015533026307821274, acc: 0.9932126402854919)
[2025-02-13 04:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23][root][INFO] - Training Epoch: 2/2, step 4802/7134 completed (loss: 0.03676211088895798, acc: 0.9895287752151489)
[2025-02-13 04:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23][root][INFO] - Training Epoch: 2/2, step 4803/7134 completed (loss: 0.023281803354620934, acc: 0.9911764860153198)
[2025-02-13 04:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:24][root][INFO] - Training Epoch: 2/2, step 4804/7134 completed (loss: 0.0297715961933136, acc: 0.9861538410186768)
[2025-02-13 04:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:24][root][INFO] - Training Epoch: 2/2, step 4805/7134 completed (loss: 0.018127992749214172, acc: 0.9951456189155579)
[2025-02-13 04:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:25][root][INFO] - Training Epoch: 2/2, step 4806/7134 completed (loss: 0.021271152421832085, acc: 0.9983360767364502)
[2025-02-13 04:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:25][root][INFO] - Training Epoch: 2/2, step 4807/7134 completed (loss: 0.035013921558856964, acc: 0.9939209818840027)
[2025-02-13 04:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:25][root][INFO] - Training Epoch: 2/2, step 4808/7134 completed (loss: 0.014598692767322063, acc: 0.9982269406318665)
[2025-02-13 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26][root][INFO] - Training Epoch: 2/2, step 4809/7134 completed (loss: 0.013802477158606052, acc: 0.9952380657196045)
[2025-02-13 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26][root][INFO] - Training Epoch: 2/2, step 4810/7134 completed (loss: 0.0408625490963459, acc: 0.9942938685417175)
[2025-02-13 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27][root][INFO] - Training Epoch: 2/2, step 4811/7134 completed (loss: 0.03062836453318596, acc: 0.9941860437393188)
[2025-02-13 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27][root][INFO] - Training Epoch: 2/2, step 4812/7134 completed (loss: 0.07187452167272568, acc: 0.9847009778022766)
[2025-02-13 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28][root][INFO] - Training Epoch: 2/2, step 4813/7134 completed (loss: 0.004705293569713831, acc: 0.9974026083946228)
[2025-02-13 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28][root][INFO] - Training Epoch: 2/2, step 4814/7134 completed (loss: 0.013547621667385101, acc: 0.9948979616165161)
[2025-02-13 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28][root][INFO] - Training Epoch: 2/2, step 4815/7134 completed (loss: 0.004115051124244928, acc: 1.0)
[2025-02-13 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29][root][INFO] - Training Epoch: 2/2, step 4816/7134 completed (loss: 0.005308490712195635, acc: 0.9987623691558838)
[2025-02-13 04:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29][root][INFO] - Training Epoch: 2/2, step 4817/7134 completed (loss: 0.0069589391350746155, acc: 0.9975903630256653)
[2025-02-13 04:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30][root][INFO] - Training Epoch: 2/2, step 4818/7134 completed (loss: 0.014574184082448483, acc: 0.9984615445137024)
[2025-02-13 04:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30][root][INFO] - Training Epoch: 2/2, step 4819/7134 completed (loss: 0.007195289712399244, acc: 0.9979715943336487)
[2025-02-13 04:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31][root][INFO] - Training Epoch: 2/2, step 4820/7134 completed (loss: 0.01467636413872242, acc: 0.9948520064353943)
[2025-02-13 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31][root][INFO] - Training Epoch: 2/2, step 4821/7134 completed (loss: 0.022299913689494133, acc: 0.994413435459137)
[2025-02-13 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31][root][INFO] - Training Epoch: 2/2, step 4822/7134 completed (loss: 0.018145477399230003, acc: 0.996219277381897)
[2025-02-13 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32][root][INFO] - Training Epoch: 2/2, step 4823/7134 completed (loss: 0.0518631786108017, acc: 0.9873617887496948)
[2025-02-13 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32][root][INFO] - Training Epoch: 2/2, step 4824/7134 completed (loss: 0.03325524926185608, acc: 0.9863013625144958)
[2025-02-13 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:33][root][INFO] - Training Epoch: 2/2, step 4825/7134 completed (loss: 0.005168164614588022, acc: 0.9981752038002014)
[2025-02-13 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:33][root][INFO] - Training Epoch: 2/2, step 4826/7134 completed (loss: 0.03328395262360573, acc: 0.991416335105896)
[2025-02-13 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34][root][INFO] - Training Epoch: 2/2, step 4827/7134 completed (loss: 0.020991381257772446, acc: 0.991349458694458)
[2025-02-13 04:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34][root][INFO] - Training Epoch: 2/2, step 4828/7134 completed (loss: 0.01191563718020916, acc: 0.9975698590278625)
[2025-02-13 04:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34][root][INFO] - Training Epoch: 2/2, step 4829/7134 completed (loss: 0.006433355621993542, acc: 0.9987179636955261)
[2025-02-13 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35][root][INFO] - Training Epoch: 2/2, step 4830/7134 completed (loss: 0.008029602468013763, acc: 0.9970414042472839)
[2025-02-13 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35][root][INFO] - Training Epoch: 2/2, step 4831/7134 completed (loss: 0.00339819910004735, acc: 0.9985380172729492)
[2025-02-13 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36][root][INFO] - Training Epoch: 2/2, step 4832/7134 completed (loss: 0.03219887986779213, acc: 0.9934853315353394)
[2025-02-13 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36][root][INFO] - Training Epoch: 2/2, step 4833/7134 completed (loss: 0.009317437186837196, acc: 0.9974586963653564)
[2025-02-13 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37][root][INFO] - Training Epoch: 2/2, step 4834/7134 completed (loss: 0.01417505368590355, acc: 0.9964285492897034)
[2025-02-13 04:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37][root][INFO] - Training Epoch: 2/2, step 4835/7134 completed (loss: 0.012139581143856049, acc: 0.9945945739746094)
[2025-02-13 04:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38][root][INFO] - Training Epoch: 2/2, step 4836/7134 completed (loss: 0.007973835803568363, acc: 0.9984732866287231)
[2025-02-13 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38][root][INFO] - Training Epoch: 2/2, step 4837/7134 completed (loss: 0.049301765859127045, acc: 0.9871323704719543)
[2025-02-13 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38][root][INFO] - Training Epoch: 2/2, step 4838/7134 completed (loss: 0.020110292360186577, acc: 0.9919484853744507)
[2025-02-13 04:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:39][root][INFO] - Training Epoch: 2/2, step 4839/7134 completed (loss: 0.028378915041685104, acc: 0.9931153059005737)
[2025-02-13 04:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:39][root][INFO] - Training Epoch: 2/2, step 4840/7134 completed (loss: 0.015757335349917412, acc: 0.9956647157669067)
[2025-02-13 04:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:40][root][INFO] - Training Epoch: 2/2, step 4841/7134 completed (loss: 0.01860967092216015, acc: 0.9940387606620789)
[2025-02-13 04:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:40][root][INFO] - Training Epoch: 2/2, step 4842/7134 completed (loss: 0.03190429136157036, acc: 0.9936224222183228)
[2025-02-13 04:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:41][root][INFO] - Training Epoch: 2/2, step 4843/7134 completed (loss: 0.018158704042434692, acc: 0.9940564632415771)
[2025-02-13 04:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:41][root][INFO] - Training Epoch: 2/2, step 4844/7134 completed (loss: 0.012520773336291313, acc: 0.9951534867286682)
[2025-02-13 04:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:41][root][INFO] - Training Epoch: 2/2, step 4845/7134 completed (loss: 0.02415049448609352, acc: 0.9918032884597778)
[2025-02-13 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42][root][INFO] - Training Epoch: 2/2, step 4846/7134 completed (loss: 0.018115948885679245, acc: 0.9945945739746094)
[2025-02-13 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42][root][INFO] - Training Epoch: 2/2, step 4847/7134 completed (loss: 0.014241167344152927, acc: 0.9973045587539673)
[2025-02-13 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:43][root][INFO] - Training Epoch: 2/2, step 4848/7134 completed (loss: 0.021292483434081078, acc: 0.9972375631332397)
[2025-02-13 04:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:43][root][INFO] - Training Epoch: 2/2, step 4849/7134 completed (loss: 0.01256706565618515, acc: 0.9955423474311829)
[2025-02-13 04:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:44][root][INFO] - Training Epoch: 2/2, step 4850/7134 completed (loss: 0.031709641218185425, acc: 0.9926560521125793)
[2025-02-13 04:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:44][root][INFO] - Training Epoch: 2/2, step 4851/7134 completed (loss: 0.01762533001601696, acc: 0.9929906725883484)
[2025-02-13 04:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:44][root][INFO] - Training Epoch: 2/2, step 4852/7134 completed (loss: 0.020074989646673203, acc: 0.9920106530189514)
[2025-02-13 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45][root][INFO] - Training Epoch: 2/2, step 4853/7134 completed (loss: 0.04918836057186127, acc: 0.984308123588562)
[2025-02-13 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45][root][INFO] - Training Epoch: 2/2, step 4854/7134 completed (loss: 0.01215109508484602, acc: 0.9973333477973938)
[2025-02-13 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:46][root][INFO] - Training Epoch: 2/2, step 4855/7134 completed (loss: 0.01616572216153145, acc: 0.993954062461853)
[2025-02-13 04:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:46][root][INFO] - Training Epoch: 2/2, step 4856/7134 completed (loss: 0.026821529492735863, acc: 0.9923664331436157)
[2025-02-13 04:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47][root][INFO] - Training Epoch: 2/2, step 4857/7134 completed (loss: 0.015074444934725761, acc: 0.9956834316253662)
[2025-02-13 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47][root][INFO] - Training Epoch: 2/2, step 4858/7134 completed (loss: 0.020755203440785408, acc: 0.995726466178894)
[2025-02-13 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:48][root][INFO] - Training Epoch: 2/2, step 4859/7134 completed (loss: 0.01832052879035473, acc: 0.9948520064353943)
[2025-02-13 04:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:48][root][INFO] - Training Epoch: 2/2, step 4860/7134 completed (loss: 0.014040041714906693, acc: 0.9972752332687378)
[2025-02-13 04:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:48][root][INFO] - Training Epoch: 2/2, step 4861/7134 completed (loss: 0.008821087889373302, acc: 0.9959568977355957)
[2025-02-13 04:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49][root][INFO] - Training Epoch: 2/2, step 4862/7134 completed (loss: 0.016465231776237488, acc: 0.9971387982368469)
[2025-02-13 04:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49][root][INFO] - Training Epoch: 2/2, step 4863/7134 completed (loss: 0.023409338667988777, acc: 0.9908758997917175)
[2025-02-13 04:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:50][root][INFO] - Training Epoch: 2/2, step 4864/7134 completed (loss: 0.014485969208180904, acc: 0.9938461780548096)
[2025-02-13 04:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:50][root][INFO] - Training Epoch: 2/2, step 4865/7134 completed (loss: 0.025237848982214928, acc: 0.9926739931106567)
[2025-02-13 04:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][root][INFO] - Training Epoch: 2/2, step 4866/7134 completed (loss: 0.01776035875082016, acc: 0.9961089491844177)
[2025-02-13 04:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][root][INFO] - Training Epoch: 2/2, step 4867/7134 completed (loss: 0.028193525969982147, acc: 0.9933862686157227)
[2025-02-13 04:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][root][INFO] - Training Epoch: 2/2, step 4868/7134 completed (loss: 0.034436170011758804, acc: 0.9883720874786377)
[2025-02-13 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:52][root][INFO] - Training Epoch: 2/2, step 4869/7134 completed (loss: 0.05060155317187309, acc: 0.9933333396911621)
[2025-02-13 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:52][root][INFO] - Training Epoch: 2/2, step 4870/7134 completed (loss: 0.011001275852322578, acc: 0.9964788556098938)
[2025-02-13 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53][root][INFO] - Training Epoch: 2/2, step 4871/7134 completed (loss: 0.011056635528802872, acc: 0.9984567761421204)
[2025-02-13 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53][root][INFO] - Training Epoch: 2/2, step 4872/7134 completed (loss: 0.01316668651998043, acc: 0.9958275556564331)
[2025-02-13 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54][root][INFO] - Training Epoch: 2/2, step 4873/7134 completed (loss: 0.015306794084608555, acc: 0.9947916865348816)
[2025-02-13 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54][root][INFO] - Training Epoch: 2/2, step 4874/7134 completed (loss: 0.015330464579164982, acc: 0.9942857027053833)
[2025-02-13 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55][root][INFO] - Training Epoch: 2/2, step 4875/7134 completed (loss: 0.021002892404794693, acc: 0.9930651783943176)
[2025-02-13 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55][root][INFO] - Training Epoch: 2/2, step 4876/7134 completed (loss: 0.043629951775074005, acc: 0.9873149991035461)
[2025-02-13 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55][root][INFO] - Training Epoch: 2/2, step 4877/7134 completed (loss: 0.03685779869556427, acc: 0.9858064651489258)
[2025-02-13 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:56][root][INFO] - Training Epoch: 2/2, step 4878/7134 completed (loss: 0.023051679134368896, acc: 0.9896907210350037)
[2025-02-13 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:56][root][INFO] - Training Epoch: 2/2, step 4879/7134 completed (loss: 0.039207953959703445, acc: 0.9879518151283264)
[2025-02-13 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57][root][INFO] - Training Epoch: 2/2, step 4880/7134 completed (loss: 0.006206876132637262, acc: 0.9987030029296875)
[2025-02-13 04:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57][root][INFO] - Training Epoch: 2/2, step 4881/7134 completed (loss: 0.03339257091283798, acc: 0.9941605925559998)
[2025-02-13 04:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58][root][INFO] - Training Epoch: 2/2, step 4882/7134 completed (loss: 0.03034822829067707, acc: 0.9917762875556946)
[2025-02-13 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58][root][INFO] - Training Epoch: 2/2, step 4883/7134 completed (loss: 0.017746830359101295, acc: 0.9956204295158386)
[2025-02-13 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58][root][INFO] - Training Epoch: 2/2, step 4884/7134 completed (loss: 0.008467075414955616, acc: 0.996927797794342)
[2025-02-13 04:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:59][root][INFO] - Training Epoch: 2/2, step 4885/7134 completed (loss: 0.00638535525649786, acc: 1.0)
[2025-02-13 04:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:59][root][INFO] - Training Epoch: 2/2, step 4886/7134 completed (loss: 0.018193770200014114, acc: 0.9948717951774597)
[2025-02-13 04:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:00][root][INFO] - Training Epoch: 2/2, step 4887/7134 completed (loss: 0.011050127446651459, acc: 0.9964157938957214)
[2025-02-13 04:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:00][root][INFO] - Training Epoch: 2/2, step 4888/7134 completed (loss: 0.012936421670019627, acc: 0.9972527623176575)
[2025-02-13 04:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:01][root][INFO] - Training Epoch: 2/2, step 4889/7134 completed (loss: 0.003966623917222023, acc: 0.9987966418266296)
[2025-02-13 04:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:01][root][INFO] - Training Epoch: 2/2, step 4890/7134 completed (loss: 0.019710568711161613, acc: 0.9976190328598022)
[2025-02-13 04:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02][root][INFO] - Training Epoch: 2/2, step 4891/7134 completed (loss: 0.029725253582000732, acc: 0.9912935495376587)
[2025-02-13 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02][root][INFO] - Training Epoch: 2/2, step 4892/7134 completed (loss: 0.017710572108626366, acc: 0.9932885766029358)
[2025-02-13 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02][root][INFO] - Training Epoch: 2/2, step 4893/7134 completed (loss: 0.12352967262268066, acc: 0.9709762334823608)
[2025-02-13 04:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:03][root][INFO] - Training Epoch: 2/2, step 4894/7134 completed (loss: 0.009212685748934746, acc: 0.9985590577125549)
[2025-02-13 04:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:03][root][INFO] - Training Epoch: 2/2, step 4895/7134 completed (loss: 0.004072146490216255, acc: 1.0)
[2025-02-13 04:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:04][root][INFO] - Training Epoch: 2/2, step 4896/7134 completed (loss: 0.0032494196202605963, acc: 0.9985954761505127)
[2025-02-13 04:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:04][root][INFO] - Training Epoch: 2/2, step 4897/7134 completed (loss: 0.019836358726024628, acc: 0.9906432628631592)
[2025-02-13 04:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05][root][INFO] - Training Epoch: 2/2, step 4898/7134 completed (loss: 0.01262015663087368, acc: 0.99622642993927)
[2025-02-13 04:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05][root][INFO] - Training Epoch: 2/2, step 4899/7134 completed (loss: 0.014127736911177635, acc: 0.9956331849098206)
[2025-02-13 04:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:06][root][INFO] - Training Epoch: 2/2, step 4900/7134 completed (loss: 0.024721696972846985, acc: 0.9947368502616882)
[2025-02-13 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:06][root][INFO] - Training Epoch: 2/2, step 4901/7134 completed (loss: 0.010771430097520351, acc: 0.995488703250885)
[2025-02-13 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:06][root][INFO] - Training Epoch: 2/2, step 4902/7134 completed (loss: 0.008600507862865925, acc: 0.9986737370491028)
[2025-02-13 04:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:07][root][INFO] - Training Epoch: 2/2, step 4903/7134 completed (loss: 0.008972306735813618, acc: 0.9964157938957214)
[2025-02-13 04:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:07][root][INFO] - Training Epoch: 2/2, step 4904/7134 completed (loss: 0.016242200508713722, acc: 0.9970149397850037)
[2025-02-13 04:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08][root][INFO] - Training Epoch: 2/2, step 4905/7134 completed (loss: 0.011753457598388195, acc: 0.993630588054657)
[2025-02-13 04:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08][root][INFO] - Training Epoch: 2/2, step 4906/7134 completed (loss: 0.008039160631597042, acc: 0.9983136653900146)
[2025-02-13 04:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:09][root][INFO] - Training Epoch: 2/2, step 4907/7134 completed (loss: 0.01771683432161808, acc: 0.996835470199585)
[2025-02-13 04:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:09][root][INFO] - Training Epoch: 2/2, step 4908/7134 completed (loss: 0.0025366824120283127, acc: 1.0)
[2025-02-13 04:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:10][root][INFO] - Training Epoch: 2/2, step 4909/7134 completed (loss: 0.013768414035439491, acc: 0.9967585206031799)
[2025-02-13 04:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:10][root][INFO] - Training Epoch: 2/2, step 4910/7134 completed (loss: 0.016867293044924736, acc: 0.9921630024909973)
[2025-02-13 04:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:10][root][INFO] - Training Epoch: 2/2, step 4911/7134 completed (loss: 0.01662205532193184, acc: 0.9934036731719971)
[2025-02-13 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11][root][INFO] - Training Epoch: 2/2, step 4912/7134 completed (loss: 0.01859976164996624, acc: 0.9927361011505127)
[2025-02-13 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11][root][INFO] - Training Epoch: 2/2, step 4913/7134 completed (loss: 0.004793425556272268, acc: 1.0)
[2025-02-13 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:12][root][INFO] - Training Epoch: 2/2, step 4914/7134 completed (loss: 0.020581815391778946, acc: 0.9938837885856628)
[2025-02-13 04:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:12][root][INFO] - Training Epoch: 2/2, step 4915/7134 completed (loss: 0.0026005327235907316, acc: 1.0)
[2025-02-13 04:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13][root][INFO] - Training Epoch: 2/2, step 4916/7134 completed (loss: 0.017739146947860718, acc: 0.9951456189155579)
[2025-02-13 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13][root][INFO] - Training Epoch: 2/2, step 4917/7134 completed (loss: 0.030870219692587852, acc: 0.9916550517082214)
[2025-02-13 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13][root][INFO] - Training Epoch: 2/2, step 4918/7134 completed (loss: 0.004158461466431618, acc: 0.998633861541748)
[2025-02-13 04:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:14][root][INFO] - Training Epoch: 2/2, step 4919/7134 completed (loss: 0.025033283978700638, acc: 0.99210524559021)
[2025-02-13 04:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:14][root][INFO] - Training Epoch: 2/2, step 4920/7134 completed (loss: 0.04290466010570526, acc: 0.9896640777587891)
[2025-02-13 04:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15][root][INFO] - Training Epoch: 2/2, step 4921/7134 completed (loss: 0.024040251970291138, acc: 0.9956958293914795)
[2025-02-13 04:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15][root][INFO] - Training Epoch: 2/2, step 4922/7134 completed (loss: 0.012869632802903652, acc: 0.994557797908783)
[2025-02-13 04:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:16][root][INFO] - Training Epoch: 2/2, step 4923/7134 completed (loss: 0.009013420902192593, acc: 0.9985315799713135)
[2025-02-13 04:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:16][root][INFO] - Training Epoch: 2/2, step 4924/7134 completed (loss: 0.026647105813026428, acc: 0.9919999837875366)
[2025-02-13 04:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:17][root][INFO] - Training Epoch: 2/2, step 4925/7134 completed (loss: 0.013125179335474968, acc: 0.9942775368690491)
[2025-02-13 04:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:17][root][INFO] - Training Epoch: 2/2, step 4926/7134 completed (loss: 0.009485581889748573, acc: 0.9971305727958679)
[2025-02-13 04:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:17][root][INFO] - Training Epoch: 2/2, step 4927/7134 completed (loss: 0.02081792987883091, acc: 0.9929971694946289)
[2025-02-13 04:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18][root][INFO] - Training Epoch: 2/2, step 4928/7134 completed (loss: 0.004679486621171236, acc: 1.0)
[2025-02-13 04:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18][root][INFO] - Training Epoch: 2/2, step 4929/7134 completed (loss: 0.014500540681183338, acc: 0.9937984347343445)
[2025-02-13 04:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:19][root][INFO] - Training Epoch: 2/2, step 4930/7134 completed (loss: 0.030465222895145416, acc: 0.993966817855835)
[2025-02-13 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:19][root][INFO] - Training Epoch: 2/2, step 4931/7134 completed (loss: 0.04890178143978119, acc: 0.990867555141449)
[2025-02-13 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20][root][INFO] - Training Epoch: 2/2, step 4932/7134 completed (loss: 0.01418246515095234, acc: 0.9946019053459167)
[2025-02-13 04:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20][root][INFO] - Training Epoch: 2/2, step 4933/7134 completed (loss: 0.01692253164947033, acc: 0.993819534778595)
[2025-02-13 04:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20][root][INFO] - Training Epoch: 2/2, step 4934/7134 completed (loss: 0.006775304209440947, acc: 0.9972936511039734)
[2025-02-13 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:21][root][INFO] - Training Epoch: 2/2, step 4935/7134 completed (loss: 0.008612760342657566, acc: 0.99726402759552)
[2025-02-13 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:21][root][INFO] - Training Epoch: 2/2, step 4936/7134 completed (loss: 0.013092989102005959, acc: 0.9938271641731262)
[2025-02-13 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:22][root][INFO] - Training Epoch: 2/2, step 4937/7134 completed (loss: 0.013787929899990559, acc: 0.994020938873291)
[2025-02-13 04:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:22][root][INFO] - Training Epoch: 2/2, step 4938/7134 completed (loss: 0.013918495737016201, acc: 0.9932523369789124)
[2025-02-13 04:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23][root][INFO] - Training Epoch: 2/2, step 4939/7134 completed (loss: 0.020053047686815262, acc: 0.990212082862854)
[2025-02-13 04:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23][root][INFO] - Training Epoch: 2/2, step 4940/7134 completed (loss: 0.030112026259303093, acc: 0.9924242496490479)
[2025-02-13 04:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23][root][INFO] - Training Epoch: 2/2, step 4941/7134 completed (loss: 0.015604267828166485, acc: 0.9959623217582703)
[2025-02-13 04:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:24][root][INFO] - Training Epoch: 2/2, step 4942/7134 completed (loss: 0.02849937044084072, acc: 0.9919571280479431)
[2025-02-13 04:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:24][root][INFO] - Training Epoch: 2/2, step 4943/7134 completed (loss: 0.020334646105766296, acc: 0.99589604139328)
[2025-02-13 04:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25][root][INFO] - Training Epoch: 2/2, step 4944/7134 completed (loss: 0.020803673192858696, acc: 0.9915789365768433)
[2025-02-13 04:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25][root][INFO] - Training Epoch: 2/2, step 4945/7134 completed (loss: 0.02497529424726963, acc: 0.9919354915618896)
[2025-02-13 04:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:26][root][INFO] - Training Epoch: 2/2, step 4946/7134 completed (loss: 0.03863414749503136, acc: 0.9824175834655762)
[2025-02-13 04:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:26][root][INFO] - Training Epoch: 2/2, step 4947/7134 completed (loss: 0.02049005776643753, acc: 0.9911308288574219)
[2025-02-13 04:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:26][root][INFO] - Training Epoch: 2/2, step 4948/7134 completed (loss: 0.0199198629707098, acc: 0.9946523904800415)
[2025-02-13 04:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:27][root][INFO] - Training Epoch: 2/2, step 4949/7134 completed (loss: 0.018440958112478256, acc: 0.9968253970146179)
[2025-02-13 04:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:27][root][INFO] - Training Epoch: 2/2, step 4950/7134 completed (loss: 0.011887642554938793, acc: 0.9982486963272095)
[2025-02-13 04:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28][root][INFO] - Training Epoch: 2/2, step 4951/7134 completed (loss: 0.018947849050164223, acc: 0.994301974773407)
[2025-02-13 04:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28][root][INFO] - Training Epoch: 2/2, step 4952/7134 completed (loss: 0.01456046849489212, acc: 0.996688723564148)
[2025-02-13 04:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:29][root][INFO] - Training Epoch: 2/2, step 4953/7134 completed (loss: 0.009507432579994202, acc: 1.0)
[2025-02-13 04:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:29][root][INFO] - Training Epoch: 2/2, step 4954/7134 completed (loss: 0.005850569810718298, acc: 1.0)
[2025-02-13 04:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:29][root][INFO] - Training Epoch: 2/2, step 4955/7134 completed (loss: 0.012628777883946896, acc: 0.9976580739021301)
[2025-02-13 04:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:30][root][INFO] - Training Epoch: 2/2, step 4956/7134 completed (loss: 0.009246181696653366, acc: 0.9985775351524353)
[2025-02-13 04:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:30][root][INFO] - Training Epoch: 2/2, step 4957/7134 completed (loss: 0.020795738324522972, acc: 0.9894179701805115)
[2025-02-13 04:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31][root][INFO] - Training Epoch: 2/2, step 4958/7134 completed (loss: 0.01581488735973835, acc: 0.9956834316253662)
[2025-02-13 04:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31][root][INFO] - Training Epoch: 2/2, step 4959/7134 completed (loss: 0.006564809940755367, acc: 0.9970588088035583)
[2025-02-13 04:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31][root][INFO] - Training Epoch: 2/2, step 4960/7134 completed (loss: 0.0038527261931449175, acc: 1.0)
[2025-02-13 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:32][root][INFO] - Training Epoch: 2/2, step 4961/7134 completed (loss: 0.0042404052801430225, acc: 1.0)
[2025-02-13 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:32][root][INFO] - Training Epoch: 2/2, step 4962/7134 completed (loss: 0.0014886849094182253, acc: 1.0)
[2025-02-13 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33][root][INFO] - Training Epoch: 2/2, step 4963/7134 completed (loss: 0.020354604348540306, acc: 0.9918830990791321)
[2025-02-13 04:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33][root][INFO] - Training Epoch: 2/2, step 4964/7134 completed (loss: 0.01543345581740141, acc: 0.995121955871582)
[2025-02-13 04:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:34][root][INFO] - Training Epoch: 2/2, step 4965/7134 completed (loss: 0.024785228073596954, acc: 0.9915966391563416)
[2025-02-13 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:34][root][INFO] - Training Epoch: 2/2, step 4966/7134 completed (loss: 0.03806169331073761, acc: 0.9861496090888977)
[2025-02-13 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:34][root][INFO] - Training Epoch: 2/2, step 4967/7134 completed (loss: 0.013631386682391167, acc: 0.994140625)
[2025-02-13 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35][root][INFO] - Training Epoch: 2/2, step 4968/7134 completed (loss: 0.03957776725292206, acc: 0.9857650995254517)
[2025-02-13 04:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35][root][INFO] - Training Epoch: 2/2, step 4969/7134 completed (loss: 0.032928552478551865, acc: 0.9862542748451233)
[2025-02-13 04:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:36][root][INFO] - Training Epoch: 2/2, step 4970/7134 completed (loss: 0.016821417957544327, acc: 0.9916387796401978)
[2025-02-13 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:36][root][INFO] - Training Epoch: 2/2, step 4971/7134 completed (loss: 0.01664389856159687, acc: 0.993220329284668)
[2025-02-13 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:36][root][INFO] - Training Epoch: 2/2, step 4972/7134 completed (loss: 0.009829654358327389, acc: 0.995768666267395)
[2025-02-13 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37][root][INFO] - Training Epoch: 2/2, step 4973/7134 completed (loss: 0.03792330622673035, acc: 0.9876543283462524)
[2025-02-13 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37][root][INFO] - Training Epoch: 2/2, step 4974/7134 completed (loss: 0.03579056262969971, acc: 0.9879759550094604)
[2025-02-13 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38][root][INFO] - Training Epoch: 2/2, step 4975/7134 completed (loss: 0.05515425652265549, acc: 0.9856230020523071)
[2025-02-13 04:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38][root][INFO] - Training Epoch: 2/2, step 4976/7134 completed (loss: 0.018943751230835915, acc: 0.9976580739021301)
[2025-02-13 04:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38][root][INFO] - Training Epoch: 2/2, step 4977/7134 completed (loss: 0.059276096522808075, acc: 0.9892473220825195)
[2025-02-13 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39][root][INFO] - Training Epoch: 2/2, step 4978/7134 completed (loss: 0.012332132086157799, acc: 0.9950310587882996)
[2025-02-13 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39][root][INFO] - Training Epoch: 2/2, step 4979/7134 completed (loss: 0.06627865880727768, acc: 0.9868203997612)
[2025-02-13 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:40][root][INFO] - Training Epoch: 2/2, step 4980/7134 completed (loss: 0.03608926758170128, acc: 0.9871175289154053)
[2025-02-13 04:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:40][root][INFO] - Training Epoch: 2/2, step 4981/7134 completed (loss: 0.028546974062919617, acc: 0.9920254945755005)
[2025-02-13 04:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41][root][INFO] - Training Epoch: 2/2, step 4982/7134 completed (loss: 0.043246690183877945, acc: 0.9838056564331055)
[2025-02-13 04:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41][root][INFO] - Training Epoch: 2/2, step 4983/7134 completed (loss: 0.04120847210288048, acc: 0.9922118186950684)
[2025-02-13 04:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41][root][INFO] - Training Epoch: 2/2, step 4984/7134 completed (loss: 0.04440458491444588, acc: 0.9958333373069763)
[2025-02-13 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:42][root][INFO] - Training Epoch: 2/2, step 4985/7134 completed (loss: 0.03371543437242508, acc: 0.9908257126808167)
[2025-02-13 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:42][root][INFO] - Training Epoch: 2/2, step 4986/7134 completed (loss: 0.031908515840768814, acc: 0.9910846948623657)
[2025-02-13 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:43][root][INFO] - Training Epoch: 2/2, step 4987/7134 completed (loss: 0.026735415682196617, acc: 0.9900596141815186)
[2025-02-13 04:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:43][root][INFO] - Training Epoch: 2/2, step 4988/7134 completed (loss: 0.008111853152513504, acc: 0.9982331991195679)
[2025-02-13 04:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44][root][INFO] - Training Epoch: 2/2, step 4989/7134 completed (loss: 0.02157394029200077, acc: 0.9950494766235352)
[2025-02-13 04:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44][root][INFO] - Training Epoch: 2/2, step 4990/7134 completed (loss: 0.024784022942185402, acc: 0.9932432174682617)
[2025-02-13 04:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44][root][INFO] - Training Epoch: 2/2, step 4991/7134 completed (loss: 0.011208479292690754, acc: 0.9966499209403992)
[2025-02-13 04:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:45][root][INFO] - Training Epoch: 2/2, step 4992/7134 completed (loss: 0.08286023885011673, acc: 0.9803063273429871)
[2025-02-13 04:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:45][root][INFO] - Training Epoch: 2/2, step 4993/7134 completed (loss: 0.0038261120207607746, acc: 1.0)
[2025-02-13 04:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46][root][INFO] - Training Epoch: 2/2, step 4994/7134 completed (loss: 0.0206858292222023, acc: 0.9971631169319153)
[2025-02-13 04:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46][root][INFO] - Training Epoch: 2/2, step 4995/7134 completed (loss: 0.03180622309446335, acc: 0.9918367266654968)
[2025-02-13 04:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46][root][INFO] - Training Epoch: 2/2, step 4996/7134 completed (loss: 0.009211638942360878, acc: 0.9963099360466003)
[2025-02-13 04:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47][root][INFO] - Training Epoch: 2/2, step 4997/7134 completed (loss: 0.029084395617246628, acc: 0.9931507110595703)
[2025-02-13 04:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47][root][INFO] - Training Epoch: 2/2, step 4998/7134 completed (loss: 0.05254378169775009, acc: 0.9890710115432739)
[2025-02-13 04:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:48][root][INFO] - Training Epoch: 2/2, step 4999/7134 completed (loss: 0.010223043151199818, acc: 0.9965576529502869)
[2025-02-13 04:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:48][root][INFO] - Training Epoch: 2/2, step 5000/7134 completed (loss: 0.013380307704210281, acc: 0.9953488111495972)
[2025-02-13 04:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:49][root][INFO] - Training Epoch: 2/2, step 5001/7134 completed (loss: 0.027727898210287094, acc: 0.9956395626068115)
[2025-02-13 04:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:49][root][INFO] - Training Epoch: 2/2, step 5002/7134 completed (loss: 0.035847049206495285, acc: 0.9962825179100037)
[2025-02-13 04:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:49][root][INFO] - Training Epoch: 2/2, step 5003/7134 completed (loss: 0.024953458458185196, acc: 0.995199978351593)
[2025-02-13 04:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50][root][INFO] - Training Epoch: 2/2, step 5004/7134 completed (loss: 0.01346653699874878, acc: 0.9955849647521973)
[2025-02-13 04:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50][root][INFO] - Training Epoch: 2/2, step 5005/7134 completed (loss: 0.015203407034277916, acc: 0.995121955871582)
[2025-02-13 04:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:51][root][INFO] - Training Epoch: 2/2, step 5006/7134 completed (loss: 0.010661010630428791, acc: 0.9985315799713135)
[2025-02-13 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:51][root][INFO] - Training Epoch: 2/2, step 5007/7134 completed (loss: 0.0086917569860816, acc: 0.994350254535675)
[2025-02-13 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:52][root][INFO] - Training Epoch: 2/2, step 5008/7134 completed (loss: 0.03479611128568649, acc: 0.9901574850082397)
[2025-02-13 04:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:52][root][INFO] - Training Epoch: 2/2, step 5009/7134 completed (loss: 0.028868071734905243, acc: 0.9900744557380676)
[2025-02-13 04:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:52][root][INFO] - Training Epoch: 2/2, step 5010/7134 completed (loss: 0.02533382549881935, acc: 0.9959514141082764)
[2025-02-13 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:53][root][INFO] - Training Epoch: 2/2, step 5011/7134 completed (loss: 0.023405972868204117, acc: 0.9918699264526367)
[2025-02-13 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:53][root][INFO] - Training Epoch: 2/2, step 5012/7134 completed (loss: 0.02217472903430462, acc: 0.9885057210922241)
[2025-02-13 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54][root][INFO] - Training Epoch: 2/2, step 5013/7134 completed (loss: 0.004631765652447939, acc: 1.0)
[2025-02-13 04:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54][root][INFO] - Training Epoch: 2/2, step 5014/7134 completed (loss: 0.022379254922270775, acc: 0.9959016442298889)
[2025-02-13 04:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:55][root][INFO] - Training Epoch: 2/2, step 5015/7134 completed (loss: 0.057860150933265686, acc: 0.9875862002372742)
[2025-02-13 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:55][root][INFO] - Training Epoch: 2/2, step 5016/7134 completed (loss: 0.006999542471021414, acc: 0.9981916546821594)
[2025-02-13 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:55][root][INFO] - Training Epoch: 2/2, step 5017/7134 completed (loss: 0.011073405854403973, acc: 0.9963964223861694)
[2025-02-13 04:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:56][root][INFO] - Training Epoch: 2/2, step 5018/7134 completed (loss: 0.021633831784129143, acc: 0.98758864402771)
[2025-02-13 04:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:56][root][INFO] - Training Epoch: 2/2, step 5019/7134 completed (loss: 0.01773935928940773, acc: 0.9928774833679199)
[2025-02-13 04:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:57][root][INFO] - Training Epoch: 2/2, step 5020/7134 completed (loss: 0.02556011639535427, acc: 0.987500011920929)
[2025-02-13 04:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:57][root][INFO] - Training Epoch: 2/2, step 5021/7134 completed (loss: 0.024706749245524406, acc: 0.9977116584777832)
[2025-02-13 04:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:58][root][INFO] - Training Epoch: 2/2, step 5022/7134 completed (loss: 0.013835009187459946, acc: 0.9969135522842407)
[2025-02-13 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:58][root][INFO] - Training Epoch: 2/2, step 5023/7134 completed (loss: 0.023096121847629547, acc: 0.9893617033958435)
[2025-02-13 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:59][root][INFO] - Training Epoch: 2/2, step 5024/7134 completed (loss: 0.01621701754629612, acc: 0.9987130165100098)
[2025-02-13 04:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:59][root][INFO] - Training Epoch: 2/2, step 5025/7134 completed (loss: 0.00512340385466814, acc: 0.9981883764266968)
[2025-02-13 04:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:59][root][INFO] - Training Epoch: 2/2, step 5026/7134 completed (loss: 0.016808858141303062, acc: 0.9929947257041931)
[2025-02-13 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00][root][INFO] - Training Epoch: 2/2, step 5027/7134 completed (loss: 0.017445171251893044, acc: 0.9927140474319458)
[2025-02-13 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00][root][INFO] - Training Epoch: 2/2, step 5028/7134 completed (loss: 0.008997056633234024, acc: 0.9952977895736694)
[2025-02-13 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:01][root][INFO] - Training Epoch: 2/2, step 5029/7134 completed (loss: 0.02120516076683998, acc: 0.9943181872367859)
[2025-02-13 04:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:01][root][INFO] - Training Epoch: 2/2, step 5030/7134 completed (loss: 0.007617963943630457, acc: 0.9986357688903809)
[2025-02-13 04:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02][root][INFO] - Training Epoch: 2/2, step 5031/7134 completed (loss: 0.05146647244691849, acc: 0.988095223903656)
[2025-02-13 04:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02][root][INFO] - Training Epoch: 2/2, step 5032/7134 completed (loss: 0.017548391595482826, acc: 0.9967532753944397)
[2025-02-13 04:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02][root][INFO] - Training Epoch: 2/2, step 5033/7134 completed (loss: 0.0038119943346828222, acc: 0.9983974099159241)
[2025-02-13 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03][root][INFO] - Training Epoch: 2/2, step 5034/7134 completed (loss: 0.009362765587866306, acc: 0.9946808218955994)
[2025-02-13 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03][root][INFO] - Training Epoch: 2/2, step 5035/7134 completed (loss: 0.0029538904782384634, acc: 1.0)
[2025-02-13 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:04][root][INFO] - Training Epoch: 2/2, step 5036/7134 completed (loss: 0.033383000642061234, acc: 0.9933035969734192)
[2025-02-13 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:04][root][INFO] - Training Epoch: 2/2, step 5037/7134 completed (loss: 0.04434538632631302, acc: 0.989276111125946)
[2025-02-13 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:04][root][INFO] - Training Epoch: 2/2, step 5038/7134 completed (loss: 0.014336327090859413, acc: 0.9977728128433228)
[2025-02-13 04:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:05][root][INFO] - Training Epoch: 2/2, step 5039/7134 completed (loss: 0.023451270535588264, acc: 0.9905063509941101)
[2025-02-13 04:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:05][root][INFO] - Training Epoch: 2/2, step 5040/7134 completed (loss: 0.008482526056468487, acc: 0.9961758852005005)
[2025-02-13 04:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06][root][INFO] - Training Epoch: 2/2, step 5041/7134 completed (loss: 0.009494812227785587, acc: 0.9960212111473083)
[2025-02-13 04:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06][root][INFO] - Training Epoch: 2/2, step 5042/7134 completed (loss: 0.028683315962553024, acc: 0.9906759858131409)
[2025-02-13 04:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:07][root][INFO] - Training Epoch: 2/2, step 5043/7134 completed (loss: 0.019307967275381088, acc: 0.9940711259841919)
[2025-02-13 04:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:07][root][INFO] - Training Epoch: 2/2, step 5044/7134 completed (loss: 0.012725725769996643, acc: 0.9956140518188477)
[2025-02-13 04:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:07][root][INFO] - Training Epoch: 2/2, step 5045/7134 completed (loss: 0.040204428136348724, acc: 0.983849287033081)
[2025-02-13 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:08][root][INFO] - Training Epoch: 2/2, step 5046/7134 completed (loss: 0.031845688819885254, acc: 0.9877192974090576)
[2025-02-13 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:08][root][INFO] - Training Epoch: 2/2, step 5047/7134 completed (loss: 0.03533000871539116, acc: 0.9917355179786682)
[2025-02-13 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09][root][INFO] - Training Epoch: 2/2, step 5048/7134 completed (loss: 0.028097962960600853, acc: 0.9954338073730469)
[2025-02-13 04:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09][root][INFO] - Training Epoch: 2/2, step 5049/7134 completed (loss: 0.04216643050312996, acc: 0.9904631972312927)
[2025-02-13 04:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10][root][INFO] - Training Epoch: 2/2, step 5050/7134 completed (loss: 0.04295242950320244, acc: 0.9915611743927002)
[2025-02-13 04:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10][root][INFO] - Training Epoch: 2/2, step 5051/7134 completed (loss: 0.03144575655460358, acc: 0.9942029118537903)
[2025-02-13 04:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10][root][INFO] - Training Epoch: 2/2, step 5052/7134 completed (loss: 0.014292984269559383, acc: 0.9939393997192383)
[2025-02-13 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:11][root][INFO] - Training Epoch: 2/2, step 5053/7134 completed (loss: 0.05129004269838333, acc: 0.9867924451828003)
[2025-02-13 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:11][root][INFO] - Training Epoch: 2/2, step 5054/7134 completed (loss: 0.03222290426492691, acc: 0.9921135902404785)
[2025-02-13 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12][root][INFO] - Training Epoch: 2/2, step 5055/7134 completed (loss: 0.01869070529937744, acc: 0.9918367266654968)
[2025-02-13 04:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12][root][INFO] - Training Epoch: 2/2, step 5056/7134 completed (loss: 0.05736415088176727, acc: 0.9873015880584717)
[2025-02-13 04:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:13][root][INFO] - Training Epoch: 2/2, step 5057/7134 completed (loss: 0.08114037662744522, acc: 0.9815078377723694)
[2025-02-13 04:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:13][root][INFO] - Training Epoch: 2/2, step 5058/7134 completed (loss: 0.05258627608418465, acc: 0.9826086759567261)
[2025-02-13 04:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:14][root][INFO] - Training Epoch: 2/2, step 5059/7134 completed (loss: 0.027318645268678665, acc: 0.9921362996101379)
[2025-02-13 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:14][root][INFO] - Training Epoch: 2/2, step 5060/7134 completed (loss: 0.019184501841664314, acc: 0.9970282316207886)
[2025-02-13 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:14][root][INFO] - Training Epoch: 2/2, step 5061/7134 completed (loss: 0.02343916706740856, acc: 0.9913606643676758)
[2025-02-13 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15][root][INFO] - Training Epoch: 2/2, step 5062/7134 completed (loss: 0.03878230229020119, acc: 0.9879931211471558)
[2025-02-13 04:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15][root][INFO] - Training Epoch: 2/2, step 5063/7134 completed (loss: 0.03027932532131672, acc: 0.989983320236206)
[2025-02-13 04:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:16][root][INFO] - Training Epoch: 2/2, step 5064/7134 completed (loss: 0.014737674966454506, acc: 0.9931507110595703)
[2025-02-13 04:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:16][root][INFO] - Training Epoch: 2/2, step 5065/7134 completed (loss: 0.023961251601576805, acc: 0.994358241558075)
[2025-02-13 04:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:17][root][INFO] - Training Epoch: 2/2, step 5066/7134 completed (loss: 0.04560370370745659, acc: 0.9894099831581116)
[2025-02-13 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:17][root][INFO] - Training Epoch: 2/2, step 5067/7134 completed (loss: 0.009440118446946144, acc: 0.9961013793945312)
[2025-02-13 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:17][root][INFO] - Training Epoch: 2/2, step 5068/7134 completed (loss: 0.013119188137352467, acc: 0.9968404173851013)
[2025-02-13 04:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18][root][INFO] - Training Epoch: 2/2, step 5069/7134 completed (loss: 0.013211661949753761, acc: 0.9937499761581421)
[2025-02-13 04:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18][root][INFO] - Training Epoch: 2/2, step 5070/7134 completed (loss: 0.003153233788907528, acc: 1.0)
[2025-02-13 04:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:19][root][INFO] - Training Epoch: 2/2, step 5071/7134 completed (loss: 0.02429298125207424, acc: 0.991584837436676)
[2025-02-13 04:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:19][root][INFO] - Training Epoch: 2/2, step 5072/7134 completed (loss: 0.03469051048159599, acc: 0.9912170767784119)
[2025-02-13 04:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:20][root][INFO] - Training Epoch: 2/2, step 5073/7134 completed (loss: 0.03001565858721733, acc: 0.9941973090171814)
[2025-02-13 04:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:20][root][INFO] - Training Epoch: 2/2, step 5074/7134 completed (loss: 0.03245662525296211, acc: 0.9917080998420715)
[2025-02-13 04:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:20][root][INFO] - Training Epoch: 2/2, step 5075/7134 completed (loss: 0.03309883922338486, acc: 0.9903100728988647)
[2025-02-13 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:21][root][INFO] - Training Epoch: 2/2, step 5076/7134 completed (loss: 0.02531568333506584, acc: 0.9916267991065979)
[2025-02-13 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:21][root][INFO] - Training Epoch: 2/2, step 5077/7134 completed (loss: 0.009590432047843933, acc: 0.996835470199585)
[2025-02-13 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22][root][INFO] - Training Epoch: 2/2, step 5078/7134 completed (loss: 0.039467133581638336, acc: 0.9895397424697876)
[2025-02-13 04:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22][root][INFO] - Training Epoch: 2/2, step 5079/7134 completed (loss: 0.005506305489689112, acc: 0.9986394643783569)
[2025-02-13 04:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:23][root][INFO] - Training Epoch: 2/2, step 5080/7134 completed (loss: 0.014228720217943192, acc: 0.9964538812637329)
[2025-02-13 04:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:23][root][INFO] - Training Epoch: 2/2, step 5081/7134 completed (loss: 0.008022284135222435, acc: 0.9972826242446899)
[2025-02-13 04:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:24][root][INFO] - Training Epoch: 2/2, step 5082/7134 completed (loss: 0.015592802315950394, acc: 0.9931972622871399)
[2025-02-13 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:24][root][INFO] - Training Epoch: 2/2, step 5083/7134 completed (loss: 0.03502372279763222, acc: 0.9943740963935852)
[2025-02-13 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:24][root][INFO] - Training Epoch: 2/2, step 5084/7134 completed (loss: 0.015345944091677666, acc: 0.9965277910232544)
[2025-02-13 04:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:25][root][INFO] - Training Epoch: 2/2, step 5085/7134 completed (loss: 0.018260084092617035, acc: 0.9940828680992126)
[2025-02-13 04:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:25][root][INFO] - Training Epoch: 2/2, step 5086/7134 completed (loss: 0.027554525062441826, acc: 0.9923760890960693)
[2025-02-13 04:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26][root][INFO] - Training Epoch: 2/2, step 5087/7134 completed (loss: 0.004681536462157965, acc: 1.0)
[2025-02-13 04:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26][root][INFO] - Training Epoch: 2/2, step 5088/7134 completed (loss: 0.008221645839512348, acc: 0.9985358715057373)
[2025-02-13 04:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:27][root][INFO] - Training Epoch: 2/2, step 5089/7134 completed (loss: 0.017303025349974632, acc: 0.996259331703186)
[2025-02-13 04:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:27][root][INFO] - Training Epoch: 2/2, step 5090/7134 completed (loss: 0.008709311485290527, acc: 0.9988331198692322)
[2025-02-13 04:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:28][root][INFO] - Training Epoch: 2/2, step 5091/7134 completed (loss: 0.005458058323711157, acc: 1.0)
[2025-02-13 04:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:28][root][INFO] - Training Epoch: 2/2, step 5092/7134 completed (loss: 0.02523173950612545, acc: 0.9972337484359741)
[2025-02-13 04:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:28][root][INFO] - Training Epoch: 2/2, step 5093/7134 completed (loss: 0.01465404499322176, acc: 0.9944751262664795)
[2025-02-13 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29][root][INFO] - Training Epoch: 2/2, step 5094/7134 completed (loss: 0.006627250928431749, acc: 0.996363639831543)
[2025-02-13 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29][root][INFO] - Training Epoch: 2/2, step 5095/7134 completed (loss: 0.04162822291254997, acc: 0.9859594106674194)
[2025-02-13 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:30][root][INFO] - Training Epoch: 2/2, step 5096/7134 completed (loss: 0.02283407375216484, acc: 0.99190753698349)
[2025-02-13 04:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:30][root][INFO] - Training Epoch: 2/2, step 5097/7134 completed (loss: 0.016522731631994247, acc: 0.9926739931106567)
[2025-02-13 04:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31][root][INFO] - Training Epoch: 2/2, step 5098/7134 completed (loss: 0.012389344163239002, acc: 0.9955005645751953)
[2025-02-13 04:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31][root][INFO] - Training Epoch: 2/2, step 5099/7134 completed (loss: 0.011215637437999249, acc: 0.9976905584335327)
[2025-02-13 04:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31][root][INFO] - Training Epoch: 2/2, step 5100/7134 completed (loss: 0.02549922652542591, acc: 0.9944933652877808)
[2025-02-13 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32][root][INFO] - Training Epoch: 2/2, step 5101/7134 completed (loss: 0.011120347306132317, acc: 0.9941725134849548)
[2025-02-13 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32][root][INFO] - Training Epoch: 2/2, step 5102/7134 completed (loss: 0.021084554493427277, acc: 0.9918144345283508)
[2025-02-13 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:33][root][INFO] - Training Epoch: 2/2, step 5103/7134 completed (loss: 0.026069272309541702, acc: 0.9885452389717102)
[2025-02-13 04:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:33][root][INFO] - Training Epoch: 2/2, step 5104/7134 completed (loss: 0.012946332804858685, acc: 0.9966777563095093)
[2025-02-13 04:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:34][root][INFO] - Training Epoch: 2/2, step 5105/7134 completed (loss: 0.01363146398216486, acc: 0.995529055595398)
[2025-02-13 04:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:34][root][INFO] - Training Epoch: 2/2, step 5106/7134 completed (loss: 0.007585459854453802, acc: 0.9985185265541077)
[2025-02-13 04:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35][root][INFO] - Training Epoch: 2/2, step 5107/7134 completed (loss: 0.06584376096725464, acc: 0.9857369065284729)
[2025-02-13 04:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35][root][INFO] - Training Epoch: 2/2, step 5108/7134 completed (loss: 0.03974692150950432, acc: 0.9825783967971802)
[2025-02-13 04:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35][root][INFO] - Training Epoch: 2/2, step 5109/7134 completed (loss: 0.03720832243561745, acc: 0.9933628439903259)
[2025-02-13 04:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:36][root][INFO] - Training Epoch: 2/2, step 5110/7134 completed (loss: 0.03827795386314392, acc: 0.9871794581413269)
[2025-02-13 04:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:36][root][INFO] - Training Epoch: 2/2, step 5111/7134 completed (loss: 0.008466487750411034, acc: 0.998630166053772)
[2025-02-13 04:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:37][root][INFO] - Training Epoch: 2/2, step 5112/7134 completed (loss: 0.05364608392119408, acc: 0.9925280213356018)
[2025-02-13 04:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:37][root][INFO] - Training Epoch: 2/2, step 5113/7134 completed (loss: 0.025738989934325218, acc: 0.9925373196601868)
[2025-02-13 04:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38][root][INFO] - Training Epoch: 2/2, step 5114/7134 completed (loss: 0.016748817637562752, acc: 0.9972183704376221)
[2025-02-13 04:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38][root][INFO] - Training Epoch: 2/2, step 5115/7134 completed (loss: 0.013207647018134594, acc: 0.9948387145996094)
[2025-02-13 04:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38][root][INFO] - Training Epoch: 2/2, step 5116/7134 completed (loss: 0.009583204984664917, acc: 0.9961038827896118)
[2025-02-13 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:39][root][INFO] - Training Epoch: 2/2, step 5117/7134 completed (loss: 0.019585397094488144, acc: 0.9958158731460571)
[2025-02-13 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:39][root][INFO] - Training Epoch: 2/2, step 5118/7134 completed (loss: 0.039541423320770264, acc: 0.9924127459526062)
[2025-02-13 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:40][root][INFO] - Training Epoch: 2/2, step 5119/7134 completed (loss: 0.016658306121826172, acc: 0.9958734512329102)
[2025-02-13 04:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:40][root][INFO] - Training Epoch: 2/2, step 5120/7134 completed (loss: 0.03943632170557976, acc: 0.9882199168205261)
[2025-02-13 04:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41][root][INFO] - Training Epoch: 2/2, step 5121/7134 completed (loss: 0.02020394243299961, acc: 0.994452178478241)
[2025-02-13 04:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41][root][INFO] - Training Epoch: 2/2, step 5122/7134 completed (loss: 0.01691964641213417, acc: 0.9929676651954651)
[2025-02-13 04:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41][root][INFO] - Training Epoch: 2/2, step 5123/7134 completed (loss: 0.02409220300614834, acc: 0.9927623867988586)
[2025-02-13 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:42][root][INFO] - Training Epoch: 2/2, step 5124/7134 completed (loss: 0.02098884992301464, acc: 0.991304337978363)
[2025-02-13 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:42][root][INFO] - Training Epoch: 2/2, step 5125/7134 completed (loss: 0.016952740028500557, acc: 0.9952606558799744)
[2025-02-13 04:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:43][root][INFO] - Training Epoch: 2/2, step 5126/7134 completed (loss: 0.012187712825834751, acc: 0.9965811967849731)
[2025-02-13 04:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:43][root][INFO] - Training Epoch: 2/2, step 5127/7134 completed (loss: 0.026641851291060448, acc: 0.9930070042610168)
[2025-02-13 04:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44][root][INFO] - Training Epoch: 2/2, step 5128/7134 completed (loss: 0.008443504571914673, acc: 0.997802197933197)
[2025-02-13 04:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44][root][INFO] - Training Epoch: 2/2, step 5129/7134 completed (loss: 0.028385350480675697, acc: 0.9882352948188782)
[2025-02-13 04:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:45][root][INFO] - Training Epoch: 2/2, step 5130/7134 completed (loss: 0.013543368317186832, acc: 0.9984399080276489)
[2025-02-13 04:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:45][root][INFO] - Training Epoch: 2/2, step 5131/7134 completed (loss: 0.0319538414478302, acc: 0.9951298832893372)
[2025-02-13 04:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:45][root][INFO] - Training Epoch: 2/2, step 5132/7134 completed (loss: 0.006883561611175537, acc: 1.0)
[2025-02-13 04:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46][root][INFO] - Training Epoch: 2/2, step 5133/7134 completed (loss: 0.024957921355962753, acc: 0.9937888383865356)
[2025-02-13 04:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46][root][INFO] - Training Epoch: 2/2, step 5134/7134 completed (loss: 0.011023150756955147, acc: 0.998123824596405)
[2025-02-13 04:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:47][root][INFO] - Training Epoch: 2/2, step 5135/7134 completed (loss: 0.03817274793982506, acc: 0.9942857027053833)
[2025-02-13 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:47][root][INFO] - Training Epoch: 2/2, step 5136/7134 completed (loss: 0.021160610020160675, acc: 0.9931694269180298)
[2025-02-13 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:48][root][INFO] - Training Epoch: 2/2, step 5137/7134 completed (loss: 0.017975151538848877, acc: 0.9926605224609375)
[2025-02-13 04:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:48][root][INFO] - Training Epoch: 2/2, step 5138/7134 completed (loss: 0.012871175073087215, acc: 0.9962499737739563)
[2025-02-13 04:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:48][root][INFO] - Training Epoch: 2/2, step 5139/7134 completed (loss: 0.011671752668917179, acc: 0.9971387982368469)
[2025-02-13 04:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49][root][INFO] - Training Epoch: 2/2, step 5140/7134 completed (loss: 0.012570164166390896, acc: 0.9978790879249573)
[2025-02-13 04:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49][root][INFO] - Training Epoch: 2/2, step 5141/7134 completed (loss: 0.03294769674539566, acc: 0.9884169697761536)
[2025-02-13 04:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50][root][INFO] - Training Epoch: 2/2, step 5142/7134 completed (loss: 0.009310238994657993, acc: 0.9988235235214233)
[2025-02-13 04:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50][root][INFO] - Training Epoch: 2/2, step 5143/7134 completed (loss: 0.007349836174398661, acc: 0.9987309575080872)
[2025-02-13 04:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:51][root][INFO] - Training Epoch: 2/2, step 5144/7134 completed (loss: 0.01014748215675354, acc: 0.9964994192123413)
[2025-02-13 04:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:51][root][INFO] - Training Epoch: 2/2, step 5145/7134 completed (loss: 0.010091584175825119, acc: 0.9969293475151062)
[2025-02-13 04:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:51][root][INFO] - Training Epoch: 2/2, step 5146/7134 completed (loss: 0.039827145636081696, acc: 0.9910813570022583)
[2025-02-13 04:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52][root][INFO] - Training Epoch: 2/2, step 5147/7134 completed (loss: 0.016799157485365868, acc: 0.9931694269180298)
[2025-02-13 04:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52][root][INFO] - Training Epoch: 2/2, step 5148/7134 completed (loss: 0.042495086789131165, acc: 0.9906790852546692)
[2025-02-13 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:53][root][INFO] - Training Epoch: 2/2, step 5149/7134 completed (loss: 0.02692754752933979, acc: 0.9910447597503662)
[2025-02-13 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:53][root][INFO] - Training Epoch: 2/2, step 5150/7134 completed (loss: 0.02665134333074093, acc: 0.9869621992111206)
[2025-02-13 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:54][root][INFO] - Training Epoch: 2/2, step 5151/7134 completed (loss: 0.030683165416121483, acc: 0.9925742745399475)
[2025-02-13 04:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:54][root][INFO] - Training Epoch: 2/2, step 5152/7134 completed (loss: 0.023502640426158905, acc: 0.9959785342216492)
[2025-02-13 04:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55][root][INFO] - Training Epoch: 2/2, step 5153/7134 completed (loss: 0.014390979893505573, acc: 0.9958847761154175)
[2025-02-13 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55][root][INFO] - Training Epoch: 2/2, step 5154/7134 completed (loss: 0.0186022762209177, acc: 0.9954493641853333)
[2025-02-13 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:56][root][INFO] - Training Epoch: 2/2, step 5155/7134 completed (loss: 0.023020753636956215, acc: 0.9954819083213806)
[2025-02-13 04:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:56][root][INFO] - Training Epoch: 2/2, step 5156/7134 completed (loss: 0.009025211445987225, acc: 0.9970208406448364)
[2025-02-13 04:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57][root][INFO] - Training Epoch: 2/2, step 5157/7134 completed (loss: 0.05708577111363411, acc: 0.9865525960922241)
[2025-02-13 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57][root][INFO] - Training Epoch: 2/2, step 5158/7134 completed (loss: 0.054137226194143295, acc: 0.9821958541870117)
[2025-02-13 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57][root][INFO] - Training Epoch: 2/2, step 5159/7134 completed (loss: 0.024747973307967186, acc: 0.9934980273246765)
[2025-02-13 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:58][root][INFO] - Training Epoch: 2/2, step 5160/7134 completed (loss: 0.03732019290328026, acc: 0.9841498732566833)
[2025-02-13 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:58][root][INFO] - Training Epoch: 2/2, step 5161/7134 completed (loss: 0.05836854875087738, acc: 0.9804195761680603)
[2025-02-13 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59][root][INFO] - Training Epoch: 2/2, step 5162/7134 completed (loss: 0.01185034029185772, acc: 0.9951377511024475)
[2025-02-13 04:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59][root][INFO] - Training Epoch: 2/2, step 5163/7134 completed (loss: 0.020813409239053726, acc: 0.9915013909339905)
[2025-02-13 04:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00][root][INFO] - Training Epoch: 2/2, step 5164/7134 completed (loss: 0.007945855148136616, acc: 0.9983193278312683)
[2025-02-13 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00][root][INFO] - Training Epoch: 2/2, step 5165/7134 completed (loss: 0.03118623048067093, acc: 0.9939758777618408)
[2025-02-13 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00][root][INFO] - Training Epoch: 2/2, step 5166/7134 completed (loss: 0.0053807394579052925, acc: 0.9984779357910156)
[2025-02-13 04:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01][root][INFO] - Training Epoch: 2/2, step 5167/7134 completed (loss: 0.024667097255587578, acc: 0.9887820482254028)
[2025-02-13 04:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01][root][INFO] - Training Epoch: 2/2, step 5168/7134 completed (loss: 0.019604206085205078, acc: 0.9953415989875793)
[2025-02-13 04:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:02][root][INFO] - Training Epoch: 2/2, step 5169/7134 completed (loss: 0.04503750801086426, acc: 0.985981285572052)
[2025-02-13 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:02][root][INFO] - Training Epoch: 2/2, step 5170/7134 completed (loss: 0.024659134447574615, acc: 0.9933775067329407)
[2025-02-13 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03][root][INFO] - Training Epoch: 2/2, step 5171/7134 completed (loss: 0.016987282782793045, acc: 0.995726466178894)
[2025-02-13 04:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03][root][INFO] - Training Epoch: 2/2, step 5172/7134 completed (loss: 0.042245008051395416, acc: 0.9863713979721069)
[2025-02-13 04:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03][root][INFO] - Training Epoch: 2/2, step 5173/7134 completed (loss: 0.033392734825611115, acc: 0.9858267903327942)
[2025-02-13 04:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:04][root][INFO] - Training Epoch: 2/2, step 5174/7134 completed (loss: 0.011910119093954563, acc: 0.9987228512763977)
[2025-02-13 04:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:04][root][INFO] - Training Epoch: 2/2, step 5175/7134 completed (loss: 0.012235704809427261, acc: 0.9958563446998596)
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05][root][INFO] - Training Epoch: 2/2, step 5176/7134 completed (loss: 0.006387420929968357, acc: 1.0)
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05][root][INFO] - Training Epoch: 2/2, step 5177/7134 completed (loss: 0.03888137638568878, acc: 0.9895651936531067)
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:06][root][INFO] - Training Epoch: 2/2, step 5178/7134 completed (loss: 0.05972565338015556, acc: 0.9860627055168152)
[2025-02-13 04:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:06][root][INFO] - Training Epoch: 2/2, step 5179/7134 completed (loss: 0.020145632326602936, acc: 0.9911242723464966)
[2025-02-13 04:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07][root][INFO] - Training Epoch: 2/2, step 5180/7134 completed (loss: 0.0416857935488224, acc: 0.9849905967712402)
[2025-02-13 04:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07][root][INFO] - Training Epoch: 2/2, step 5181/7134 completed (loss: 0.05947113037109375, acc: 0.9893617033958435)
[2025-02-13 04:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07][root][INFO] - Training Epoch: 2/2, step 5182/7134 completed (loss: 0.09651494026184082, acc: 0.9666666388511658)
[2025-02-13 04:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:08][root][INFO] - Training Epoch: 2/2, step 5183/7134 completed (loss: 0.04414950683712959, acc: 0.9917491674423218)
[2025-02-13 04:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:08][root][INFO] - Training Epoch: 2/2, step 5184/7134 completed (loss: 0.025496935471892357, acc: 0.9877750873565674)
[2025-02-13 04:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09][root][INFO] - Training Epoch: 2/2, step 5185/7134 completed (loss: 0.020013688132166862, acc: 0.9951807260513306)
[2025-02-13 04:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09][root][INFO] - Training Epoch: 2/2, step 5186/7134 completed (loss: 0.020977947860956192, acc: 0.9918533563613892)
[2025-02-13 04:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10][root][INFO] - Training Epoch: 2/2, step 5187/7134 completed (loss: 0.010332849808037281, acc: 0.9961904883384705)
[2025-02-13 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10][root][INFO] - Training Epoch: 2/2, step 5188/7134 completed (loss: 0.09026369452476501, acc: 0.9763636589050293)
[2025-02-13 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10][root][INFO] - Training Epoch: 2/2, step 5189/7134 completed (loss: 0.05461728945374489, acc: 0.9923469424247742)
[2025-02-13 04:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11][root][INFO] - Training Epoch: 2/2, step 5190/7134 completed (loss: 0.022295106202363968, acc: 0.9952493906021118)
[2025-02-13 04:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11][root][INFO] - Training Epoch: 2/2, step 5191/7134 completed (loss: 0.0841083824634552, acc: 0.9751098155975342)
[2025-02-13 04:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:12][root][INFO] - Training Epoch: 2/2, step 5192/7134 completed (loss: 0.03375260904431343, acc: 0.9881094098091125)
[2025-02-13 04:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:12][root][INFO] - Training Epoch: 2/2, step 5193/7134 completed (loss: 0.011422443203628063, acc: 0.998031497001648)
[2025-02-13 04:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:13][root][INFO] - Training Epoch: 2/2, step 5194/7134 completed (loss: 0.033311448991298676, acc: 0.9900990128517151)
[2025-02-13 04:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:13][root][INFO] - Training Epoch: 2/2, step 5195/7134 completed (loss: 0.04102715104818344, acc: 0.9934210777282715)
[2025-02-13 04:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:13][root][INFO] - Training Epoch: 2/2, step 5196/7134 completed (loss: 0.053525034338235855, acc: 0.9904305934906006)
[2025-02-13 04:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14][root][INFO] - Training Epoch: 2/2, step 5197/7134 completed (loss: 0.04049210622906685, acc: 0.9855967164039612)
[2025-02-13 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14][root][INFO] - Training Epoch: 2/2, step 5198/7134 completed (loss: 0.020765766501426697, acc: 0.9937629699707031)
[2025-02-13 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:15][root][INFO] - Training Epoch: 2/2, step 5199/7134 completed (loss: 0.06683371961116791, acc: 0.9811046719551086)
[2025-02-13 04:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:15][root][INFO] - Training Epoch: 2/2, step 5200/7134 completed (loss: 0.01875835470855236, acc: 0.990176796913147)
[2025-02-13 04:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16][root][INFO] - Training Epoch: 2/2, step 5201/7134 completed (loss: 0.04512794315814972, acc: 0.9927623867988586)
[2025-02-13 04:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16][root][INFO] - Training Epoch: 2/2, step 5202/7134 completed (loss: 0.042485859245061874, acc: 0.9931318759918213)
[2025-02-13 04:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16][root][INFO] - Training Epoch: 2/2, step 5203/7134 completed (loss: 0.04960766062140465, acc: 0.9877551198005676)
[2025-02-13 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17][root][INFO] - Training Epoch: 2/2, step 5204/7134 completed (loss: 0.0085761658847332, acc: 0.9958763122558594)
[2025-02-13 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17][root][INFO] - Training Epoch: 2/2, step 5205/7134 completed (loss: 0.032916758209466934, acc: 0.9887640476226807)
[2025-02-13 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18][root][INFO] - Training Epoch: 2/2, step 5206/7134 completed (loss: 0.03452775999903679, acc: 0.9935794472694397)
[2025-02-13 04:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18][root][INFO] - Training Epoch: 2/2, step 5207/7134 completed (loss: 0.04833616316318512, acc: 0.9778671860694885)
[2025-02-13 04:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:19][root][INFO] - Training Epoch: 2/2, step 5208/7134 completed (loss: 0.009393750689923763, acc: 0.9973614811897278)
[2025-02-13 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:19][root][INFO] - Training Epoch: 2/2, step 5209/7134 completed (loss: 0.03288289159536362, acc: 0.9899598360061646)
[2025-02-13 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20][root][INFO] - Training Epoch: 2/2, step 5210/7134 completed (loss: 0.022031499072909355, acc: 0.9946595430374146)
[2025-02-13 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20][root][INFO] - Training Epoch: 2/2, step 5211/7134 completed (loss: 0.0323699526488781, acc: 0.9930555820465088)
[2025-02-13 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20][root][INFO] - Training Epoch: 2/2, step 5212/7134 completed (loss: 0.039982523769140244, acc: 0.9897611141204834)
[2025-02-13 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21][root][INFO] - Training Epoch: 2/2, step 5213/7134 completed (loss: 0.03483276069164276, acc: 0.9946042895317078)
[2025-02-13 04:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21][root][INFO] - Training Epoch: 2/2, step 5214/7134 completed (loss: 0.07263149321079254, acc: 0.9742489457130432)
[2025-02-13 04:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22][root][INFO] - Training Epoch: 2/2, step 5215/7134 completed (loss: 0.05332249030470848, acc: 0.9859514832496643)
[2025-02-13 04:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22][root][INFO] - Training Epoch: 2/2, step 5216/7134 completed (loss: 0.02768995426595211, acc: 0.9885931611061096)
[2025-02-13 04:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22][root][INFO] - Training Epoch: 2/2, step 5217/7134 completed (loss: 0.03962412849068642, acc: 0.9847561120986938)
[2025-02-13 04:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23][root][INFO] - Training Epoch: 2/2, step 5218/7134 completed (loss: 0.019518768414855003, acc: 0.9955056309700012)
[2025-02-13 04:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23][root][INFO] - Training Epoch: 2/2, step 5219/7134 completed (loss: 0.03164960816502571, acc: 0.9927361011505127)
[2025-02-13 04:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24][root][INFO] - Training Epoch: 2/2, step 5220/7134 completed (loss: 0.027633801102638245, acc: 0.9928443431854248)
[2025-02-13 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24][root][INFO] - Training Epoch: 2/2, step 5221/7134 completed (loss: 0.0331006795167923, acc: 0.9922118186950684)
[2025-02-13 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24][root][INFO] - Training Epoch: 2/2, step 5222/7134 completed (loss: 0.01022097747772932, acc: 0.9968847632408142)
[2025-02-13 04:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:25][root][INFO] - Training Epoch: 2/2, step 5223/7134 completed (loss: 0.014176969416439533, acc: 0.9907833933830261)
[2025-02-13 04:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:25][root][INFO] - Training Epoch: 2/2, step 5224/7134 completed (loss: 0.02740800939500332, acc: 0.9929701089859009)
[2025-02-13 04:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26][root][INFO] - Training Epoch: 2/2, step 5225/7134 completed (loss: 0.05170505866408348, acc: 0.9903069734573364)
[2025-02-13 04:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26][root][INFO] - Training Epoch: 2/2, step 5226/7134 completed (loss: 0.03290576487779617, acc: 0.9934102296829224)
[2025-02-13 04:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:27][root][INFO] - Training Epoch: 2/2, step 5227/7134 completed (loss: 0.018200380727648735, acc: 0.9922077655792236)
[2025-02-13 04:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:27][root][INFO] - Training Epoch: 2/2, step 5228/7134 completed (loss: 0.01978319138288498, acc: 0.992732584476471)
[2025-02-13 04:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28][root][INFO] - Training Epoch: 2/2, step 5229/7134 completed (loss: 0.009202706627547741, acc: 0.9985141158103943)
[2025-02-13 04:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28][root][INFO] - Training Epoch: 2/2, step 5230/7134 completed (loss: 0.018633458763360977, acc: 0.9946091771125793)
[2025-02-13 04:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28][root][INFO] - Training Epoch: 2/2, step 5231/7134 completed (loss: 0.022999292239546776, acc: 0.9974683523178101)
[2025-02-13 04:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:29][root][INFO] - Training Epoch: 2/2, step 5232/7134 completed (loss: 0.024479851126670837, acc: 0.9925261735916138)
[2025-02-13 04:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:29][root][INFO] - Training Epoch: 2/2, step 5233/7134 completed (loss: 0.02110995724797249, acc: 0.994369387626648)
[2025-02-13 04:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30][root][INFO] - Training Epoch: 2/2, step 5234/7134 completed (loss: 0.013378800824284554, acc: 0.993880033493042)
[2025-02-13 04:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30][root][INFO] - Training Epoch: 2/2, step 5235/7134 completed (loss: 0.02934734709560871, acc: 0.9915966391563416)
[2025-02-13 04:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31][root][INFO] - Training Epoch: 2/2, step 5236/7134 completed (loss: 0.006132638081908226, acc: 0.9982638955116272)
[2025-02-13 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31][root][INFO] - Training Epoch: 2/2, step 5237/7134 completed (loss: 0.0047622015699744225, acc: 0.9964157938957214)
[2025-02-13 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31][root][INFO] - Training Epoch: 2/2, step 5238/7134 completed (loss: 0.011999973095953465, acc: 0.9911971688270569)
[2025-02-13 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32][root][INFO] - Training Epoch: 2/2, step 5239/7134 completed (loss: 0.01753360778093338, acc: 0.992277979850769)
[2025-02-13 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32][root][INFO] - Training Epoch: 2/2, step 5240/7134 completed (loss: 0.014494533650577068, acc: 0.9972337484359741)
[2025-02-13 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:33][root][INFO] - Training Epoch: 2/2, step 5241/7134 completed (loss: 0.03545871004462242, acc: 0.9890282154083252)
[2025-02-13 04:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:33][root][INFO] - Training Epoch: 2/2, step 5242/7134 completed (loss: 0.0055608986876904964, acc: 0.9983471035957336)
[2025-02-13 04:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34][root][INFO] - Training Epoch: 2/2, step 5243/7134 completed (loss: 0.018721554428339005, acc: 0.9965517520904541)
[2025-02-13 04:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34][root][INFO] - Training Epoch: 2/2, step 5244/7134 completed (loss: 0.012216695584356785, acc: 0.9972413778305054)
[2025-02-13 04:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35][root][INFO] - Training Epoch: 2/2, step 5245/7134 completed (loss: 0.008976723067462444, acc: 0.9965096116065979)
[2025-02-13 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35][root][INFO] - Training Epoch: 2/2, step 5246/7134 completed (loss: 0.013764020055532455, acc: 0.995398759841919)
[2025-02-13 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35][root][INFO] - Training Epoch: 2/2, step 5247/7134 completed (loss: 0.005050032399594784, acc: 1.0)
[2025-02-13 04:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:36][root][INFO] - Training Epoch: 2/2, step 5248/7134 completed (loss: 0.046167585998773575, acc: 0.9925093650817871)
[2025-02-13 04:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:36][root][INFO] - Training Epoch: 2/2, step 5249/7134 completed (loss: 0.012359735555946827, acc: 0.9937106966972351)
[2025-02-13 04:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37][root][INFO] - Training Epoch: 2/2, step 5250/7134 completed (loss: 0.01762503571808338, acc: 0.996216893196106)
[2025-02-13 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37][root][INFO] - Training Epoch: 2/2, step 5251/7134 completed (loss: 0.011247866787016392, acc: 0.9975757598876953)
[2025-02-13 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38][root][INFO] - Training Epoch: 2/2, step 5252/7134 completed (loss: 0.045462291687726974, acc: 0.9883177280426025)
[2025-02-13 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38][root][INFO] - Training Epoch: 2/2, step 5253/7134 completed (loss: 0.006780421826988459, acc: 0.9967319965362549)
[2025-02-13 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39][root][INFO] - Training Epoch: 2/2, step 5254/7134 completed (loss: 0.03840405493974686, acc: 0.9898132681846619)
[2025-02-13 04:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39][root][INFO] - Training Epoch: 2/2, step 5255/7134 completed (loss: 0.02764531783759594, acc: 0.9958904385566711)
[2025-02-13 04:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39][root][INFO] - Training Epoch: 2/2, step 5256/7134 completed (loss: 0.04818170517683029, acc: 0.9876352548599243)
[2025-02-13 04:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40][root][INFO] - Training Epoch: 2/2, step 5257/7134 completed (loss: 0.05289624631404877, acc: 0.9878234267234802)
[2025-02-13 04:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40][root][INFO] - Training Epoch: 2/2, step 5258/7134 completed (loss: 0.034870050847530365, acc: 0.9885203838348389)
[2025-02-13 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:41][root][INFO] - Training Epoch: 2/2, step 5259/7134 completed (loss: 0.04657179117202759, acc: 0.9873417615890503)
[2025-02-13 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:41][root][INFO] - Training Epoch: 2/2, step 5260/7134 completed (loss: 0.017071032896637917, acc: 0.995121955871582)
[2025-02-13 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42][root][INFO] - Training Epoch: 2/2, step 5261/7134 completed (loss: 0.04030895605683327, acc: 0.9903846383094788)
[2025-02-13 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42][root][INFO] - Training Epoch: 2/2, step 5262/7134 completed (loss: 0.010648926720023155, acc: 0.9972413778305054)
[2025-02-13 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43][root][INFO] - Training Epoch: 2/2, step 5263/7134 completed (loss: 0.033473722636699677, acc: 0.9912280440330505)
[2025-02-13 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43][root][INFO] - Training Epoch: 2/2, step 5264/7134 completed (loss: 0.015192440710961819, acc: 0.9939246773719788)
[2025-02-13 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43][root][INFO] - Training Epoch: 2/2, step 5265/7134 completed (loss: 0.015122457407414913, acc: 0.9949173927307129)
[2025-02-13 04:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:44][root][INFO] - Training Epoch: 2/2, step 5266/7134 completed (loss: 0.013198671862483025, acc: 0.9958904385566711)
[2025-02-13 04:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:44][root][INFO] - Training Epoch: 2/2, step 5267/7134 completed (loss: 0.056951820850372314, acc: 0.9844478964805603)
[2025-02-13 04:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45][root][INFO] - Training Epoch: 2/2, step 5268/7134 completed (loss: 0.026519155129790306, acc: 0.9865642786026001)
[2025-02-13 04:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45][root][INFO] - Training Epoch: 2/2, step 5269/7134 completed (loss: 0.057187750935554504, acc: 0.9785330891609192)
[2025-02-13 04:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46][root][INFO] - Training Epoch: 2/2, step 5270/7134 completed (loss: 0.017039956524968147, acc: 0.9946523904800415)
[2025-02-13 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46][root][INFO] - Training Epoch: 2/2, step 5271/7134 completed (loss: 0.042825423181056976, acc: 0.9876543283462524)
[2025-02-13 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46][root][INFO] - Training Epoch: 2/2, step 5272/7134 completed (loss: 0.029392583295702934, acc: 0.992668628692627)
[2025-02-13 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:47][root][INFO] - Training Epoch: 2/2, step 5273/7134 completed (loss: 0.022888079285621643, acc: 0.9915110468864441)
[2025-02-13 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:47][root][INFO] - Training Epoch: 2/2, step 5274/7134 completed (loss: 0.020086750388145447, acc: 0.9955223798751831)
[2025-02-13 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48][root][INFO] - Training Epoch: 2/2, step 5275/7134 completed (loss: 0.004570494405925274, acc: 0.998171865940094)
[2025-02-13 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48][root][INFO] - Training Epoch: 2/2, step 5276/7134 completed (loss: 0.02912440150976181, acc: 0.9910314083099365)
[2025-02-13 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49][root][INFO] - Training Epoch: 2/2, step 5277/7134 completed (loss: 0.013794951140880585, acc: 0.9942362904548645)
[2025-02-13 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49][root][INFO] - Training Epoch: 2/2, step 5278/7134 completed (loss: 0.03574098274111748, acc: 0.9862385392189026)
[2025-02-13 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50][root][INFO] - Training Epoch: 2/2, step 5279/7134 completed (loss: 0.0032511663157492876, acc: 1.0)
[2025-02-13 04:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50][root][INFO] - Training Epoch: 2/2, step 5280/7134 completed (loss: 0.007508582901209593, acc: 0.9983416199684143)
[2025-02-13 04:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50][root][INFO] - Training Epoch: 2/2, step 5281/7134 completed (loss: 0.00556771969422698, acc: 0.9983360767364502)
[2025-02-13 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:51][root][INFO] - Training Epoch: 2/2, step 5282/7134 completed (loss: 0.015756800770759583, acc: 0.995720386505127)
[2025-02-13 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:51][root][INFO] - Training Epoch: 2/2, step 5283/7134 completed (loss: 0.009526903741061687, acc: 0.99589604139328)
[2025-02-13 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:52][root][INFO] - Training Epoch: 2/2, step 5284/7134 completed (loss: 0.03884018957614899, acc: 0.9958506226539612)
[2025-02-13 04:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:52][root][INFO] - Training Epoch: 2/2, step 5285/7134 completed (loss: 0.036169007420539856, acc: 0.9928994178771973)
[2025-02-13 04:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53][root][INFO] - Training Epoch: 2/2, step 5286/7134 completed (loss: 0.020832998678088188, acc: 0.9941176176071167)
[2025-02-13 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53][root][INFO] - Training Epoch: 2/2, step 5287/7134 completed (loss: 0.013753619976341724, acc: 0.9959349632263184)
[2025-02-13 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53][root][INFO] - Training Epoch: 2/2, step 5288/7134 completed (loss: 0.008604115806519985, acc: 0.9984662532806396)
[2025-02-13 04:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:54][root][INFO] - Training Epoch: 2/2, step 5289/7134 completed (loss: 0.018036412075161934, acc: 0.9957507252693176)
[2025-02-13 04:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:54][root][INFO] - Training Epoch: 2/2, step 5290/7134 completed (loss: 0.022714631631970406, acc: 0.9911392331123352)
[2025-02-13 04:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55][root][INFO] - Training Epoch: 2/2, step 5291/7134 completed (loss: 0.02014465071260929, acc: 0.9975757598876953)
[2025-02-13 04:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55][root][INFO] - Training Epoch: 2/2, step 5292/7134 completed (loss: 0.0068702083081007, acc: 0.9987277388572693)
[2025-02-13 04:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:56][root][INFO] - Training Epoch: 2/2, step 5293/7134 completed (loss: 0.01431309338659048, acc: 0.9970802664756775)
[2025-02-13 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:56][root][INFO] - Training Epoch: 2/2, step 5294/7134 completed (loss: 0.018053138628602028, acc: 0.9944211840629578)
[2025-02-13 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57][root][INFO] - Training Epoch: 2/2, step 5295/7134 completed (loss: 0.010276204906404018, acc: 0.9963235259056091)
[2025-02-13 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57][root][INFO] - Training Epoch: 2/2, step 5296/7134 completed (loss: 0.01500063668936491, acc: 0.9944367408752441)
[2025-02-13 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57][root][INFO] - Training Epoch: 2/2, step 5297/7134 completed (loss: 0.021033311262726784, acc: 0.9942528605461121)
[2025-02-13 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:58][root][INFO] - Training Epoch: 2/2, step 5298/7134 completed (loss: 0.010262754745781422, acc: 0.9965811967849731)
[2025-02-13 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:58][root][INFO] - Training Epoch: 2/2, step 5299/7134 completed (loss: 0.009488068521022797, acc: 0.9967897534370422)
[2025-02-13 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:59][root][INFO] - Training Epoch: 2/2, step 5300/7134 completed (loss: 0.010350240394473076, acc: 0.9955947399139404)
[2025-02-13 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:59][root][INFO] - Training Epoch: 2/2, step 5301/7134 completed (loss: 0.01979159377515316, acc: 0.9955947399139404)
[2025-02-13 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00][root][INFO] - Training Epoch: 2/2, step 5302/7134 completed (loss: 0.04414447769522667, acc: 0.9878419637680054)
[2025-02-13 04:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00][root][INFO] - Training Epoch: 2/2, step 5303/7134 completed (loss: 0.03408125415444374, acc: 0.9929203391075134)
[2025-02-13 04:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00][root][INFO] - Training Epoch: 2/2, step 5304/7134 completed (loss: 0.04827054589986801, acc: 0.9900166392326355)
[2025-02-13 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:01][root][INFO] - Training Epoch: 2/2, step 5305/7134 completed (loss: 0.03849269449710846, acc: 0.9868612885475159)
[2025-02-13 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:01][root][INFO] - Training Epoch: 2/2, step 5306/7134 completed (loss: 0.009834571741521358, acc: 0.9972222447395325)
[2025-02-13 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:02][root][INFO] - Training Epoch: 2/2, step 5307/7134 completed (loss: 0.039863087236881256, acc: 0.9876543283462524)
[2025-02-13 04:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:02][root][INFO] - Training Epoch: 2/2, step 5308/7134 completed (loss: 0.023709701374173164, acc: 0.9924471378326416)
[2025-02-13 04:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:03][root][INFO] - Training Epoch: 2/2, step 5309/7134 completed (loss: 0.07357203215360641, acc: 0.9785330891609192)
[2025-02-13 04:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:03][root][INFO] - Training Epoch: 2/2, step 5310/7134 completed (loss: 0.03677745163440704, acc: 0.9874476790428162)
[2025-02-13 04:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:03][root][INFO] - Training Epoch: 2/2, step 5311/7134 completed (loss: 0.04105568304657936, acc: 0.9887640476226807)
[2025-02-13 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04][root][INFO] - Training Epoch: 2/2, step 5312/7134 completed (loss: 0.01566031575202942, acc: 0.9943714737892151)
[2025-02-13 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04][root][INFO] - Training Epoch: 2/2, step 5313/7134 completed (loss: 0.023798327893018723, acc: 0.9927140474319458)
[2025-02-13 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:05][root][INFO] - Training Epoch: 2/2, step 5314/7134 completed (loss: 0.03272296488285065, acc: 0.9895470142364502)
[2025-02-13 04:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:05][root][INFO] - Training Epoch: 2/2, step 5315/7134 completed (loss: 0.01905793696641922, acc: 0.9934533834457397)
[2025-02-13 04:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06][root][INFO] - Training Epoch: 2/2, step 5316/7134 completed (loss: 0.03835863247513771, acc: 0.9932773113250732)
[2025-02-13 04:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06][root][INFO] - Training Epoch: 2/2, step 5317/7134 completed (loss: 0.018159396946430206, acc: 0.9946236610412598)
[2025-02-13 04:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06][root][INFO] - Training Epoch: 2/2, step 5318/7134 completed (loss: 0.016343722119927406, acc: 0.9913644194602966)
[2025-02-13 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:07][root][INFO] - Training Epoch: 2/2, step 5319/7134 completed (loss: 0.03747262433171272, acc: 0.9927007555961609)
[2025-02-13 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:07][root][INFO] - Training Epoch: 2/2, step 5320/7134 completed (loss: 0.03072892688214779, acc: 0.9937888383865356)
[2025-02-13 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08][root][INFO] - Training Epoch: 2/2, step 5321/7134 completed (loss: 0.014366190880537033, acc: 0.9968992471694946)
[2025-02-13 04:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08][root][INFO] - Training Epoch: 2/2, step 5322/7134 completed (loss: 0.013050021603703499, acc: 0.9954751133918762)
[2025-02-13 04:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09][root][INFO] - Training Epoch: 2/2, step 5323/7134 completed (loss: 0.029308784753084183, acc: 0.9909090995788574)
[2025-02-13 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09][root][INFO] - Training Epoch: 2/2, step 5324/7134 completed (loss: 0.013945523649454117, acc: 0.9969087839126587)
[2025-02-13 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09][root][INFO] - Training Epoch: 2/2, step 5325/7134 completed (loss: 0.013241119682788849, acc: 0.9982964396476746)
[2025-02-13 04:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10][root][INFO] - Training Epoch: 2/2, step 5326/7134 completed (loss: 0.02378627099096775, acc: 0.9903225898742676)
[2025-02-13 04:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10][root][INFO] - Training Epoch: 2/2, step 5327/7134 completed (loss: 0.021774521097540855, acc: 0.994194507598877)
[2025-02-13 04:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:11][root][INFO] - Training Epoch: 2/2, step 5328/7134 completed (loss: 0.01162013504654169, acc: 0.9963302612304688)
[2025-02-13 04:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:11][root][INFO] - Training Epoch: 2/2, step 5329/7134 completed (loss: 0.060458097606897354, acc: 0.987034022808075)
[2025-02-13 04:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12][root][INFO] - Training Epoch: 2/2, step 5330/7134 completed (loss: 0.03474203869700432, acc: 0.987500011920929)
[2025-02-13 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12][root][INFO] - Training Epoch: 2/2, step 5331/7134 completed (loss: 0.03314496576786041, acc: 0.9894921183586121)
[2025-02-13 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12][root][INFO] - Training Epoch: 2/2, step 5332/7134 completed (loss: 0.02203192748129368, acc: 0.9967479705810547)
[2025-02-13 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13][root][INFO] - Training Epoch: 2/2, step 5333/7134 completed (loss: 0.018589908257126808, acc: 0.9937694668769836)
[2025-02-13 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13][root][INFO] - Training Epoch: 2/2, step 5334/7134 completed (loss: 0.04166821390390396, acc: 0.9908116459846497)
[2025-02-13 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:14][root][INFO] - Training Epoch: 2/2, step 5335/7134 completed (loss: 0.013236599043011665, acc: 0.9968051314353943)
[2025-02-13 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:14][root][INFO] - Training Epoch: 2/2, step 5336/7134 completed (loss: 0.024224478751420975, acc: 0.9937499761581421)
[2025-02-13 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15][root][INFO] - Training Epoch: 2/2, step 5337/7134 completed (loss: 0.017287317663431168, acc: 0.993318498134613)
[2025-02-13 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15][root][INFO] - Training Epoch: 2/2, step 5338/7134 completed (loss: 0.02016281709074974, acc: 0.9931507110595703)
[2025-02-13 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16][root][INFO] - Training Epoch: 2/2, step 5339/7134 completed (loss: 0.013096002861857414, acc: 0.9948186278343201)
[2025-02-13 04:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16][root][INFO] - Training Epoch: 2/2, step 5340/7134 completed (loss: 0.03750268742442131, acc: 0.9861351847648621)
[2025-02-13 04:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16][root][INFO] - Training Epoch: 2/2, step 5341/7134 completed (loss: 0.007939339615404606, acc: 0.9977400898933411)
[2025-02-13 04:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17][root][INFO] - Training Epoch: 2/2, step 5342/7134 completed (loss: 0.02982521243393421, acc: 0.9880159497261047)
[2025-02-13 04:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17][root][INFO] - Training Epoch: 2/2, step 5343/7134 completed (loss: 0.04191875830292702, acc: 0.9868074059486389)
[2025-02-13 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:18][root][INFO] - Training Epoch: 2/2, step 5344/7134 completed (loss: 0.022464925423264503, acc: 0.9926315546035767)
[2025-02-13 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:18][root][INFO] - Training Epoch: 2/2, step 5345/7134 completed (loss: 0.022669818252325058, acc: 0.9919540286064148)
[2025-02-13 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19][root][INFO] - Training Epoch: 2/2, step 5346/7134 completed (loss: 0.031837448477745056, acc: 0.993704080581665)
[2025-02-13 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:39][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0456, device='cuda:0') eval_epoch_loss=tensor(0.0446, device='cuda:0') eval_epoch_acc=tensor(0.9884, device='cuda:0')
[2025-02-13 04:27:39][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:27:39][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:27:39][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_5347_loss_0.044570475816726685/model.pt
[2025-02-13 04:27:39][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:27:39][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9884451031684875
[2025-02-13 04:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:40][root][INFO] - Training Epoch: 2/2, step 5347/7134 completed (loss: 0.009412230923771858, acc: 0.9946949481964111)
[2025-02-13 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:40][root][INFO] - Training Epoch: 2/2, step 5348/7134 completed (loss: 0.024898745119571686, acc: 0.9916897416114807)
[2025-02-13 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:40][root][INFO] - Training Epoch: 2/2, step 5349/7134 completed (loss: 0.0370732881128788, acc: 0.9883720874786377)
[2025-02-13 04:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41][root][INFO] - Training Epoch: 2/2, step 5350/7134 completed (loss: 0.025943061336874962, acc: 0.9933949708938599)
[2025-02-13 04:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41][root][INFO] - Training Epoch: 2/2, step 5351/7134 completed (loss: 0.028709033504128456, acc: 0.9925558567047119)
[2025-02-13 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:42][root][INFO] - Training Epoch: 2/2, step 5352/7134 completed (loss: 0.013162131421267986, acc: 0.9948253631591797)
[2025-02-13 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:42][root][INFO] - Training Epoch: 2/2, step 5353/7134 completed (loss: 0.009916617535054684, acc: 0.9977452158927917)
[2025-02-13 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:43][root][INFO] - Training Epoch: 2/2, step 5354/7134 completed (loss: 0.04080408811569214, acc: 0.9867841601371765)
[2025-02-13 04:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:43][root][INFO] - Training Epoch: 2/2, step 5355/7134 completed (loss: 0.05159192159771919, acc: 0.986997663974762)
[2025-02-13 04:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:44][root][INFO] - Training Epoch: 2/2, step 5356/7134 completed (loss: 0.031638406217098236, acc: 0.9875466823577881)
[2025-02-13 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:44][root][INFO] - Training Epoch: 2/2, step 5357/7134 completed (loss: 0.03949696943163872, acc: 0.9912891983985901)
[2025-02-13 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45][root][INFO] - Training Epoch: 2/2, step 5358/7134 completed (loss: 0.032359976321458817, acc: 0.9917241334915161)
[2025-02-13 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45][root][INFO] - Training Epoch: 2/2, step 5359/7134 completed (loss: 0.014344905503094196, acc: 0.9941725134849548)
[2025-02-13 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45][root][INFO] - Training Epoch: 2/2, step 5360/7134 completed (loss: 0.01914055272936821, acc: 0.9945054650306702)
[2025-02-13 04:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:46][root][INFO] - Training Epoch: 2/2, step 5361/7134 completed (loss: 0.02216910570859909, acc: 0.9904191493988037)
[2025-02-13 04:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:46][root][INFO] - Training Epoch: 2/2, step 5362/7134 completed (loss: 0.01099866908043623, acc: 0.9970149397850037)
[2025-02-13 04:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:47][root][INFO] - Training Epoch: 2/2, step 5363/7134 completed (loss: 0.04997924342751503, acc: 0.9898256063461304)
[2025-02-13 04:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:47][root][INFO] - Training Epoch: 2/2, step 5364/7134 completed (loss: 0.01672668382525444, acc: 0.9948052167892456)
[2025-02-13 04:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48][root][INFO] - Training Epoch: 2/2, step 5365/7134 completed (loss: 0.01325416099280119, acc: 0.9929453134536743)
[2025-02-13 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48][root][INFO] - Training Epoch: 2/2, step 5366/7134 completed (loss: 0.038382962346076965, acc: 0.9913194179534912)
[2025-02-13 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49][root][INFO] - Training Epoch: 2/2, step 5367/7134 completed (loss: 0.028530670329928398, acc: 0.9949748516082764)
[2025-02-13 04:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49][root][INFO] - Training Epoch: 2/2, step 5368/7134 completed (loss: 0.025878451764583588, acc: 0.9933110475540161)
[2025-02-13 04:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49][root][INFO] - Training Epoch: 2/2, step 5369/7134 completed (loss: 0.015688341110944748, acc: 0.9957537055015564)
[2025-02-13 04:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:50][root][INFO] - Training Epoch: 2/2, step 5370/7134 completed (loss: 0.029916374012827873, acc: 0.9921507239341736)
[2025-02-13 04:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:50][root][INFO] - Training Epoch: 2/2, step 5371/7134 completed (loss: 0.06900602579116821, acc: 0.9784052968025208)
[2025-02-13 04:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:51][root][INFO] - Training Epoch: 2/2, step 5372/7134 completed (loss: 0.004600902553647757, acc: 0.9985975027084351)
[2025-02-13 04:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:51][root][INFO] - Training Epoch: 2/2, step 5373/7134 completed (loss: 0.0403132326900959, acc: 0.9895397424697876)
[2025-02-13 04:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52][root][INFO] - Training Epoch: 2/2, step 5374/7134 completed (loss: 0.03635530173778534, acc: 0.9903537034988403)
[2025-02-13 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52][root][INFO] - Training Epoch: 2/2, step 5375/7134 completed (loss: 0.02535228803753853, acc: 0.99042147397995)
[2025-02-13 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:53][root][INFO] - Training Epoch: 2/2, step 5376/7134 completed (loss: 0.02267327345907688, acc: 0.99303138256073)
[2025-02-13 04:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:53][root][INFO] - Training Epoch: 2/2, step 5377/7134 completed (loss: 0.03416221961379051, acc: 0.9924127459526062)
[2025-02-13 04:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:53][root][INFO] - Training Epoch: 2/2, step 5378/7134 completed (loss: 0.021365100517868996, acc: 0.9951456189155579)
[2025-02-13 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54][root][INFO] - Training Epoch: 2/2, step 5379/7134 completed (loss: 0.0328708253800869, acc: 0.9896238446235657)
[2025-02-13 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54][root][INFO] - Training Epoch: 2/2, step 5380/7134 completed (loss: 0.029193714261054993, acc: 0.9929453134536743)
[2025-02-13 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:55][root][INFO] - Training Epoch: 2/2, step 5381/7134 completed (loss: 0.016066376119852066, acc: 0.9958158731460571)
[2025-02-13 04:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:55][root][INFO] - Training Epoch: 2/2, step 5382/7134 completed (loss: 0.009747383184731007, acc: 0.9948805570602417)
[2025-02-13 04:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56][root][INFO] - Training Epoch: 2/2, step 5383/7134 completed (loss: 0.012310733087360859, acc: 0.9962121248245239)
[2025-02-13 04:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56][root][INFO] - Training Epoch: 2/2, step 5384/7134 completed (loss: 0.007905733771622181, acc: 1.0)
[2025-02-13 04:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:57][root][INFO] - Training Epoch: 2/2, step 5385/7134 completed (loss: 0.020156150683760643, acc: 0.9960370063781738)
[2025-02-13 04:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:57][root][INFO] - Training Epoch: 2/2, step 5386/7134 completed (loss: 0.009458550252020359, acc: 0.9971910119056702)
[2025-02-13 04:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:57][root][INFO] - Training Epoch: 2/2, step 5387/7134 completed (loss: 0.007815184071660042, acc: 0.9980952143669128)
[2025-02-13 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:58][root][INFO] - Training Epoch: 2/2, step 5388/7134 completed (loss: 0.008789340034127235, acc: 0.997474730014801)
[2025-02-13 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:58][root][INFO] - Training Epoch: 2/2, step 5389/7134 completed (loss: 0.0075065395794808865, acc: 0.9984025359153748)
[2025-02-13 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59][root][INFO] - Training Epoch: 2/2, step 5390/7134 completed (loss: 0.01986711099743843, acc: 0.995245635509491)
[2025-02-13 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59][root][INFO] - Training Epoch: 2/2, step 5391/7134 completed (loss: 0.024359822273254395, acc: 0.9914236664772034)
[2025-02-13 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:00][root][INFO] - Training Epoch: 2/2, step 5392/7134 completed (loss: 0.02390109747648239, acc: 0.9939024448394775)
[2025-02-13 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:00][root][INFO] - Training Epoch: 2/2, step 5393/7134 completed (loss: 0.010847251862287521, acc: 0.99609375)
[2025-02-13 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:00][root][INFO] - Training Epoch: 2/2, step 5394/7134 completed (loss: 0.024550633504986763, acc: 0.9900793433189392)
[2025-02-13 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:01][root][INFO] - Training Epoch: 2/2, step 5395/7134 completed (loss: 0.011516730301082134, acc: 0.9969834089279175)
[2025-02-13 04:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:01][root][INFO] - Training Epoch: 2/2, step 5396/7134 completed (loss: 0.024654654785990715, acc: 0.995488703250885)
[2025-02-13 04:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02][root][INFO] - Training Epoch: 2/2, step 5397/7134 completed (loss: 0.015467343851923943, acc: 0.9932318329811096)
[2025-02-13 04:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02][root][INFO] - Training Epoch: 2/2, step 5398/7134 completed (loss: 0.015313239768147469, acc: 0.9941002726554871)
[2025-02-13 04:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02][root][INFO] - Training Epoch: 2/2, step 5399/7134 completed (loss: 0.0142503147944808, acc: 0.99726402759552)
[2025-02-13 04:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:03][root][INFO] - Training Epoch: 2/2, step 5400/7134 completed (loss: 0.08359930664300919, acc: 0.9796238541603088)
[2025-02-13 04:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:03][root][INFO] - Training Epoch: 2/2, step 5401/7134 completed (loss: 0.021087072789669037, acc: 0.9919742941856384)
[2025-02-13 04:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:04][root][INFO] - Training Epoch: 2/2, step 5402/7134 completed (loss: 0.01299716904759407, acc: 0.9955947399139404)
[2025-02-13 04:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:04][root][INFO] - Training Epoch: 2/2, step 5403/7134 completed (loss: 0.03399791195988655, acc: 0.9918434023857117)
[2025-02-13 04:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05][root][INFO] - Training Epoch: 2/2, step 5404/7134 completed (loss: 0.04351106658577919, acc: 0.9865771532058716)
[2025-02-13 04:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05][root][INFO] - Training Epoch: 2/2, step 5405/7134 completed (loss: 0.017206760123372078, acc: 0.9947643876075745)
[2025-02-13 04:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:06][root][INFO] - Training Epoch: 2/2, step 5406/7134 completed (loss: 0.00775529770180583, acc: 1.0)
[2025-02-13 04:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:06][root][INFO] - Training Epoch: 2/2, step 5407/7134 completed (loss: 0.019587934017181396, acc: 0.9975845217704773)
[2025-02-13 04:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:06][root][INFO] - Training Epoch: 2/2, step 5408/7134 completed (loss: 0.052206505089998245, acc: 0.9906396269798279)
[2025-02-13 04:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:07][root][INFO] - Training Epoch: 2/2, step 5409/7134 completed (loss: 0.06602588295936584, acc: 0.9878970980644226)
[2025-02-13 04:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:07][root][INFO] - Training Epoch: 2/2, step 5410/7134 completed (loss: 0.03193145990371704, acc: 0.9881889820098877)
[2025-02-13 04:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08][root][INFO] - Training Epoch: 2/2, step 5411/7134 completed (loss: 0.0219106525182724, acc: 0.9956458806991577)
[2025-02-13 04:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08][root][INFO] - Training Epoch: 2/2, step 5412/7134 completed (loss: 0.08145897090435028, acc: 0.9846153855323792)
[2025-02-13 04:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:09][root][INFO] - Training Epoch: 2/2, step 5413/7134 completed (loss: 0.048113495111465454, acc: 0.9871428608894348)
[2025-02-13 04:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:09][root][INFO] - Training Epoch: 2/2, step 5414/7134 completed (loss: 0.07698685675859451, acc: 0.974293053150177)
[2025-02-13 04:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10][root][INFO] - Training Epoch: 2/2, step 5415/7134 completed (loss: 0.04489351809024811, acc: 0.9856733679771423)
[2025-02-13 04:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10][root][INFO] - Training Epoch: 2/2, step 5416/7134 completed (loss: 0.024717504158616066, acc: 0.9918699264526367)
[2025-02-13 04:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10][root][INFO] - Training Epoch: 2/2, step 5417/7134 completed (loss: 0.013078069314360619, acc: 0.9987515807151794)
[2025-02-13 04:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:11][root][INFO] - Training Epoch: 2/2, step 5418/7134 completed (loss: 0.025511102750897408, acc: 0.9911373853683472)
[2025-02-13 04:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:11][root][INFO] - Training Epoch: 2/2, step 5419/7134 completed (loss: 0.061589695513248444, acc: 0.9796609878540039)
[2025-02-13 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12][root][INFO] - Training Epoch: 2/2, step 5420/7134 completed (loss: 0.05910733342170715, acc: 0.9807692170143127)
[2025-02-13 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12][root][INFO] - Training Epoch: 2/2, step 5421/7134 completed (loss: 0.06446148455142975, acc: 0.9828392863273621)
[2025-02-13 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:13][root][INFO] - Training Epoch: 2/2, step 5422/7134 completed (loss: 0.04088575765490532, acc: 0.9881656765937805)
[2025-02-13 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:13][root][INFO] - Training Epoch: 2/2, step 5423/7134 completed (loss: 0.025451192632317543, acc: 0.9941349029541016)
[2025-02-13 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14][root][INFO] - Training Epoch: 2/2, step 5424/7134 completed (loss: 0.01098496001213789, acc: 0.9964788556098938)
[2025-02-13 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14][root][INFO] - Training Epoch: 2/2, step 5425/7134 completed (loss: 0.024723688140511513, acc: 0.9961165189743042)
[2025-02-13 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14][root][INFO] - Training Epoch: 2/2, step 5426/7134 completed (loss: 0.060280315577983856, acc: 0.9859437942504883)
[2025-02-13 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15][root][INFO] - Training Epoch: 2/2, step 5427/7134 completed (loss: 0.022779446095228195, acc: 0.9952606558799744)
[2025-02-13 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15][root][INFO] - Training Epoch: 2/2, step 5428/7134 completed (loss: 0.031911760568618774, acc: 0.9863713979721069)
[2025-02-13 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:16][root][INFO] - Training Epoch: 2/2, step 5429/7134 completed (loss: 0.05431830883026123, acc: 0.9855595827102661)
[2025-02-13 04:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:16][root][INFO] - Training Epoch: 2/2, step 5430/7134 completed (loss: 0.02671860344707966, acc: 0.9921630024909973)
[2025-02-13 04:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:16][root][INFO] - Training Epoch: 2/2, step 5431/7134 completed (loss: 0.02266320213675499, acc: 0.9905481934547424)
[2025-02-13 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:17][root][INFO] - Training Epoch: 2/2, step 5432/7134 completed (loss: 0.01182781532406807, acc: 0.9962335228919983)
[2025-02-13 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:17][root][INFO] - Training Epoch: 2/2, step 5433/7134 completed (loss: 0.05969075858592987, acc: 0.9878787994384766)
[2025-02-13 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18][root][INFO] - Training Epoch: 2/2, step 5434/7134 completed (loss: 0.02446035109460354, acc: 0.9939637780189514)
[2025-02-13 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18][root][INFO] - Training Epoch: 2/2, step 5435/7134 completed (loss: 0.021123163402080536, acc: 0.9895397424697876)
[2025-02-13 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18][root][INFO] - Training Epoch: 2/2, step 5436/7134 completed (loss: 0.02682175301015377, acc: 0.9925816059112549)
[2025-02-13 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:19][root][INFO] - Training Epoch: 2/2, step 5437/7134 completed (loss: 0.007251636125147343, acc: 1.0)
[2025-02-13 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:19][root][INFO] - Training Epoch: 2/2, step 5438/7134 completed (loss: 0.019081229344010353, acc: 0.9948320388793945)
[2025-02-13 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20][root][INFO] - Training Epoch: 2/2, step 5439/7134 completed (loss: 0.033409666270017624, acc: 0.9883720874786377)
[2025-02-13 04:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20][root][INFO] - Training Epoch: 2/2, step 5440/7134 completed (loss: 0.03899231180548668, acc: 0.992337167263031)
[2025-02-13 04:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:21][root][INFO] - Training Epoch: 2/2, step 5441/7134 completed (loss: 0.024116799235343933, acc: 0.9926560521125793)
[2025-02-13 04:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:21][root][INFO] - Training Epoch: 2/2, step 5442/7134 completed (loss: 0.025072403252124786, acc: 0.9945873022079468)
[2025-02-13 04:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:21][root][INFO] - Training Epoch: 2/2, step 5443/7134 completed (loss: 0.022470025345683098, acc: 0.9914772510528564)
[2025-02-13 04:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:22][root][INFO] - Training Epoch: 2/2, step 5444/7134 completed (loss: 0.019748462364077568, acc: 0.9945255517959595)
[2025-02-13 04:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:22][root][INFO] - Training Epoch: 2/2, step 5445/7134 completed (loss: 0.029167570173740387, acc: 0.9908592104911804)
[2025-02-13 04:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:23][root][INFO] - Training Epoch: 2/2, step 5446/7134 completed (loss: 0.015609909780323505, acc: 0.9971988797187805)
[2025-02-13 04:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:23][root][INFO] - Training Epoch: 2/2, step 5447/7134 completed (loss: 0.01908712275326252, acc: 0.9951691031455994)
[2025-02-13 04:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:24][root][INFO] - Training Epoch: 2/2, step 5448/7134 completed (loss: 0.039087455719709396, acc: 0.9846368432044983)
[2025-02-13 04:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:24][root][INFO] - Training Epoch: 2/2, step 5449/7134 completed (loss: 0.010716930031776428, acc: 0.9964912533760071)
[2025-02-13 04:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:25][root][INFO] - Training Epoch: 2/2, step 5450/7134 completed (loss: 0.012882341630756855, acc: 0.9948520064353943)
[2025-02-13 04:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:25][root][INFO] - Training Epoch: 2/2, step 5451/7134 completed (loss: 0.023325331509113312, acc: 0.9921362996101379)
[2025-02-13 04:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:25][root][INFO] - Training Epoch: 2/2, step 5452/7134 completed (loss: 0.034613125026226044, acc: 0.9922480583190918)
[2025-02-13 04:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26][root][INFO] - Training Epoch: 2/2, step 5453/7134 completed (loss: 0.028282804414629936, acc: 0.9926380515098572)
[2025-02-13 04:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26][root][INFO] - Training Epoch: 2/2, step 5454/7134 completed (loss: 0.02148241363465786, acc: 0.9921773076057434)
[2025-02-13 04:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:27][root][INFO] - Training Epoch: 2/2, step 5455/7134 completed (loss: 0.061514999717473984, acc: 0.9898989796638489)
[2025-02-13 04:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:27][root][INFO] - Training Epoch: 2/2, step 5456/7134 completed (loss: 0.027386216446757317, acc: 0.992443323135376)
[2025-02-13 04:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:28][root][INFO] - Training Epoch: 2/2, step 5457/7134 completed (loss: 0.04038630798459053, acc: 0.9932885766029358)
[2025-02-13 04:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:28][root][INFO] - Training Epoch: 2/2, step 5458/7134 completed (loss: 0.02123910002410412, acc: 0.993630588054657)
[2025-02-13 04:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:29][root][INFO] - Training Epoch: 2/2, step 5459/7134 completed (loss: 0.005189662333577871, acc: 0.9985097050666809)
[2025-02-13 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:29][root][INFO] - Training Epoch: 2/2, step 5460/7134 completed (loss: 0.021504541859030724, acc: 0.9930459260940552)
[2025-02-13 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:29][root][INFO] - Training Epoch: 2/2, step 5461/7134 completed (loss: 0.026306401938199997, acc: 0.9911699891090393)
[2025-02-13 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:30][root][INFO] - Training Epoch: 2/2, step 5462/7134 completed (loss: 0.005424319300800562, acc: 0.9985693693161011)
[2025-02-13 04:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:30][root][INFO] - Training Epoch: 2/2, step 5463/7134 completed (loss: 0.01953200437128544, acc: 0.9954198598861694)
[2025-02-13 04:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31][root][INFO] - Training Epoch: 2/2, step 5464/7134 completed (loss: 0.028617801144719124, acc: 0.9894319772720337)
[2025-02-13 04:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31][root][INFO] - Training Epoch: 2/2, step 5465/7134 completed (loss: 0.026144618168473244, acc: 0.9925261735916138)
[2025-02-13 04:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:32][root][INFO] - Training Epoch: 2/2, step 5466/7134 completed (loss: 0.011922250501811504, acc: 0.9965477585792542)
[2025-02-13 04:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:32][root][INFO] - Training Epoch: 2/2, step 5467/7134 completed (loss: 0.013845913112163544, acc: 0.9953051805496216)
[2025-02-13 04:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:32][root][INFO] - Training Epoch: 2/2, step 5468/7134 completed (loss: 0.022086795419454575, acc: 0.9930394291877747)
[2025-02-13 04:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33][root][INFO] - Training Epoch: 2/2, step 5469/7134 completed (loss: 0.01603105291724205, acc: 0.9950920343399048)
[2025-02-13 04:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33][root][INFO] - Training Epoch: 2/2, step 5470/7134 completed (loss: 0.00578288733959198, acc: 0.9975903630256653)
[2025-02-13 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:34][root][INFO] - Training Epoch: 2/2, step 5471/7134 completed (loss: 0.012225869111716747, acc: 0.995945930480957)
[2025-02-13 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:34][root][INFO] - Training Epoch: 2/2, step 5472/7134 completed (loss: 0.013720083981752396, acc: 0.996259331703186)
[2025-02-13 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:35][root][INFO] - Training Epoch: 2/2, step 5473/7134 completed (loss: 0.008927586488425732, acc: 0.9963189959526062)
[2025-02-13 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:35][root][INFO] - Training Epoch: 2/2, step 5474/7134 completed (loss: 0.01672108843922615, acc: 0.9928571581840515)
[2025-02-13 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36][root][INFO] - Training Epoch: 2/2, step 5475/7134 completed (loss: 0.015164170414209366, acc: 0.9957325458526611)
[2025-02-13 04:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36][root][INFO] - Training Epoch: 2/2, step 5476/7134 completed (loss: 0.005962181370705366, acc: 0.9956584572792053)
[2025-02-13 04:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36][root][INFO] - Training Epoch: 2/2, step 5477/7134 completed (loss: 0.010924643836915493, acc: 0.9963054060935974)
[2025-02-13 04:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:37][root][INFO] - Training Epoch: 2/2, step 5478/7134 completed (loss: 0.03512222692370415, acc: 0.9862778782844543)
[2025-02-13 04:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:37][root][INFO] - Training Epoch: 2/2, step 5479/7134 completed (loss: 0.01678793504834175, acc: 0.9972826242446899)
[2025-02-13 04:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:38][root][INFO] - Training Epoch: 2/2, step 5480/7134 completed (loss: 0.03894347324967384, acc: 0.9881578683853149)
[2025-02-13 04:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:38][root][INFO] - Training Epoch: 2/2, step 5481/7134 completed (loss: 0.017420385032892227, acc: 0.9939393997192383)
[2025-02-13 04:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:39][root][INFO] - Training Epoch: 2/2, step 5482/7134 completed (loss: 0.03876833990216255, acc: 0.990326464176178)
[2025-02-13 04:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:40][root][INFO] - Training Epoch: 2/2, step 5483/7134 completed (loss: 0.03299489617347717, acc: 0.9888579249382019)
[2025-02-13 04:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:40][root][INFO] - Training Epoch: 2/2, step 5484/7134 completed (loss: 0.040452972054481506, acc: 0.9847009778022766)
[2025-02-13 04:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41][root][INFO] - Training Epoch: 2/2, step 5485/7134 completed (loss: 0.04085492342710495, acc: 0.9854809641838074)
[2025-02-13 04:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41][root][INFO] - Training Epoch: 2/2, step 5486/7134 completed (loss: 0.018916839733719826, acc: 0.9982699155807495)
[2025-02-13 04:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41][root][INFO] - Training Epoch: 2/2, step 5487/7134 completed (loss: 0.01752442680299282, acc: 0.9934959411621094)
[2025-02-13 04:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42][root][INFO] - Training Epoch: 2/2, step 5488/7134 completed (loss: 0.04296240210533142, acc: 0.9868420958518982)
[2025-02-13 04:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42][root][INFO] - Training Epoch: 2/2, step 5489/7134 completed (loss: 0.031603891402482986, acc: 0.99071204662323)
[2025-02-13 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:43][root][INFO] - Training Epoch: 2/2, step 5490/7134 completed (loss: 0.015649894252419472, acc: 0.9954441785812378)
[2025-02-13 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:43][root][INFO] - Training Epoch: 2/2, step 5491/7134 completed (loss: 0.012684941291809082, acc: 0.9970674514770508)
[2025-02-13 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:44][root][INFO] - Training Epoch: 2/2, step 5492/7134 completed (loss: 0.022799678146839142, acc: 0.9932318329811096)
[2025-02-13 04:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:44][root][INFO] - Training Epoch: 2/2, step 5493/7134 completed (loss: 0.021452482789754868, acc: 0.991847813129425)
[2025-02-13 04:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45][root][INFO] - Training Epoch: 2/2, step 5494/7134 completed (loss: 0.00940187182277441, acc: 0.9971550703048706)
[2025-02-13 04:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45][root][INFO] - Training Epoch: 2/2, step 5495/7134 completed (loss: 0.02037614956498146, acc: 0.9928315281867981)
[2025-02-13 04:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45][root][INFO] - Training Epoch: 2/2, step 5496/7134 completed (loss: 0.02639610692858696, acc: 0.9883268475532532)
[2025-02-13 04:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46][root][INFO] - Training Epoch: 2/2, step 5497/7134 completed (loss: 0.053256772458553314, acc: 0.9824304580688477)
[2025-02-13 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46][root][INFO] - Training Epoch: 2/2, step 5498/7134 completed (loss: 0.028344443067908287, acc: 0.9867374300956726)
[2025-02-13 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:47][root][INFO] - Training Epoch: 2/2, step 5499/7134 completed (loss: 0.029729623347520828, acc: 0.9874371886253357)
[2025-02-13 04:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:47][root][INFO] - Training Epoch: 2/2, step 5500/7134 completed (loss: 0.036280613392591476, acc: 0.9904502034187317)
[2025-02-13 04:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:47][root][INFO] - Training Epoch: 2/2, step 5501/7134 completed (loss: 0.01694140024483204, acc: 0.9950617551803589)
[2025-02-13 04:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:48][root][INFO] - Training Epoch: 2/2, step 5502/7134 completed (loss: 0.03871626779437065, acc: 0.9878542423248291)
[2025-02-13 04:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:48][root][INFO] - Training Epoch: 2/2, step 5503/7134 completed (loss: 0.0403759740293026, acc: 0.9900596141815186)
[2025-02-13 04:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49][root][INFO] - Training Epoch: 2/2, step 5504/7134 completed (loss: 0.012226657941937447, acc: 0.9975786805152893)
[2025-02-13 04:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49][root][INFO] - Training Epoch: 2/2, step 5505/7134 completed (loss: 0.030887728556990623, acc: 0.9900709390640259)
[2025-02-13 04:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:50][root][INFO] - Training Epoch: 2/2, step 5506/7134 completed (loss: 0.020031798630952835, acc: 0.9942330121994019)
[2025-02-13 04:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:50][root][INFO] - Training Epoch: 2/2, step 5507/7134 completed (loss: 0.030017411336302757, acc: 0.9915966391563416)
[2025-02-13 04:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:51][root][INFO] - Training Epoch: 2/2, step 5508/7134 completed (loss: 0.018887892365455627, acc: 0.9918224215507507)
[2025-02-13 04:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:51][root][INFO] - Training Epoch: 2/2, step 5509/7134 completed (loss: 0.02838188037276268, acc: 0.9929824471473694)
[2025-02-13 04:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52][root][INFO] - Training Epoch: 2/2, step 5510/7134 completed (loss: 0.042072657495737076, acc: 0.9884318709373474)
[2025-02-13 04:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52][root][INFO] - Training Epoch: 2/2, step 5511/7134 completed (loss: 0.026608120650053024, acc: 0.9898189902305603)
[2025-02-13 04:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52][root][INFO] - Training Epoch: 2/2, step 5512/7134 completed (loss: 0.007205224130302668, acc: 0.9973649382591248)
[2025-02-13 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:53][root][INFO] - Training Epoch: 2/2, step 5513/7134 completed (loss: 0.017160704359412193, acc: 0.9932065010070801)
[2025-02-13 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:53][root][INFO] - Training Epoch: 2/2, step 5514/7134 completed (loss: 0.08146650344133377, acc: 0.9763681888580322)
[2025-02-13 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:54][root][INFO] - Training Epoch: 2/2, step 5515/7134 completed (loss: 0.028018465265631676, acc: 0.9885877370834351)
[2025-02-13 04:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:54][root][INFO] - Training Epoch: 2/2, step 5516/7134 completed (loss: 0.021202152594923973, acc: 0.997187077999115)
[2025-02-13 04:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:55][root][INFO] - Training Epoch: 2/2, step 5517/7134 completed (loss: 0.027086744084954262, acc: 0.9933510422706604)
[2025-02-13 04:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:55][root][INFO] - Training Epoch: 2/2, step 5518/7134 completed (loss: 0.025193048641085625, acc: 0.9932065010070801)
[2025-02-13 04:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56][root][INFO] - Training Epoch: 2/2, step 5519/7134 completed (loss: 0.007834804244339466, acc: 0.9972413778305054)
[2025-02-13 04:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56][root][INFO] - Training Epoch: 2/2, step 5520/7134 completed (loss: 0.015542407520115376, acc: 0.9954699873924255)
[2025-02-13 04:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56][root][INFO] - Training Epoch: 2/2, step 5521/7134 completed (loss: 0.015700628980994225, acc: 0.9924623370170593)
[2025-02-13 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:57][root][INFO] - Training Epoch: 2/2, step 5522/7134 completed (loss: 0.03537454083561897, acc: 0.9830065369606018)
[2025-02-13 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:57][root][INFO] - Training Epoch: 2/2, step 5523/7134 completed (loss: 0.035422712564468384, acc: 0.9957864880561829)
[2025-02-13 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:58][root][INFO] - Training Epoch: 2/2, step 5524/7134 completed (loss: 0.017893651500344276, acc: 0.9938080310821533)
[2025-02-13 04:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:58][root][INFO] - Training Epoch: 2/2, step 5525/7134 completed (loss: 0.016335926949977875, acc: 0.9941434860229492)
[2025-02-13 04:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59][root][INFO] - Training Epoch: 2/2, step 5526/7134 completed (loss: 0.02927078679203987, acc: 0.9911634922027588)
[2025-02-13 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59][root][INFO] - Training Epoch: 2/2, step 5527/7134 completed (loss: 0.01785314455628395, acc: 0.9966386556625366)
[2025-02-13 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:00][root][INFO] - Training Epoch: 2/2, step 5528/7134 completed (loss: 0.016900034621357918, acc: 0.9940828680992126)
[2025-02-13 04:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:00][root][INFO] - Training Epoch: 2/2, step 5529/7134 completed (loss: 0.042513407766819, acc: 0.9858155846595764)
[2025-02-13 04:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:00][root][INFO] - Training Epoch: 2/2, step 5530/7134 completed (loss: 0.01590702123939991, acc: 0.992548406124115)
[2025-02-13 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:01][root][INFO] - Training Epoch: 2/2, step 5531/7134 completed (loss: 0.03028200939297676, acc: 0.9925280213356018)
[2025-02-13 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:01][root][INFO] - Training Epoch: 2/2, step 5532/7134 completed (loss: 0.04207691550254822, acc: 0.9851668477058411)
[2025-02-13 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:02][root][INFO] - Training Epoch: 2/2, step 5533/7134 completed (loss: 0.0036651550326496363, acc: 1.0)
[2025-02-13 04:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:02][root][INFO] - Training Epoch: 2/2, step 5534/7134 completed (loss: 0.03169185295701027, acc: 0.9918224215507507)
[2025-02-13 04:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:03][root][INFO] - Training Epoch: 2/2, step 5535/7134 completed (loss: 0.025510750710964203, acc: 0.9902067184448242)
[2025-02-13 04:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:03][root][INFO] - Training Epoch: 2/2, step 5536/7134 completed (loss: 0.015660587698221207, acc: 0.9930232763290405)
[2025-02-13 04:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:04][root][INFO] - Training Epoch: 2/2, step 5537/7134 completed (loss: 0.025215918198227882, acc: 0.9906651377677917)
[2025-02-13 04:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:04][root][INFO] - Training Epoch: 2/2, step 5538/7134 completed (loss: 0.02037438564002514, acc: 0.9954493641853333)
[2025-02-13 04:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05][root][INFO] - Training Epoch: 2/2, step 5539/7134 completed (loss: 0.023456865921616554, acc: 0.9898648858070374)
[2025-02-13 04:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05][root][INFO] - Training Epoch: 2/2, step 5540/7134 completed (loss: 0.024394813925027847, acc: 0.99210524559021)
[2025-02-13 04:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:06][root][INFO] - Training Epoch: 2/2, step 5541/7134 completed (loss: 0.010901560075581074, acc: 0.998670220375061)
[2025-02-13 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:06][root][INFO] - Training Epoch: 2/2, step 5542/7134 completed (loss: 0.01043280865997076, acc: 0.9971469044685364)
[2025-02-13 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:06][root][INFO] - Training Epoch: 2/2, step 5543/7134 completed (loss: 0.021005261689424515, acc: 0.9955654144287109)
[2025-02-13 04:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:07][root][INFO] - Training Epoch: 2/2, step 5544/7134 completed (loss: 0.01142013631761074, acc: 0.9956989288330078)
[2025-02-13 04:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:07][root][INFO] - Training Epoch: 2/2, step 5545/7134 completed (loss: 0.02071380987763405, acc: 0.9934425950050354)
[2025-02-13 04:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08][root][INFO] - Training Epoch: 2/2, step 5546/7134 completed (loss: 0.02618182823061943, acc: 0.9933554530143738)
[2025-02-13 04:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08][root][INFO] - Training Epoch: 2/2, step 5547/7134 completed (loss: 0.010964345186948776, acc: 0.9942396283149719)
[2025-02-13 04:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:09][root][INFO] - Training Epoch: 2/2, step 5548/7134 completed (loss: 0.013650773093104362, acc: 0.9953488111495972)
[2025-02-13 04:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:09][root][INFO] - Training Epoch: 2/2, step 5549/7134 completed (loss: 0.01806863769888878, acc: 0.9928571581840515)
[2025-02-13 04:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:10][root][INFO] - Training Epoch: 2/2, step 5550/7134 completed (loss: 0.006651431322097778, acc: 0.9976498484611511)
[2025-02-13 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:10][root][INFO] - Training Epoch: 2/2, step 5551/7134 completed (loss: 0.045766208320856094, acc: 0.9891745448112488)
[2025-02-13 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:10][root][INFO] - Training Epoch: 2/2, step 5552/7134 completed (loss: 0.006889402400702238, acc: 0.9988109469413757)
[2025-02-13 04:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11][root][INFO] - Training Epoch: 2/2, step 5553/7134 completed (loss: 0.019040895625948906, acc: 0.9962916970252991)
[2025-02-13 04:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11][root][INFO] - Training Epoch: 2/2, step 5554/7134 completed (loss: 0.009201868437230587, acc: 0.9963325262069702)
[2025-02-13 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:12][root][INFO] - Training Epoch: 2/2, step 5555/7134 completed (loss: 0.007884029299020767, acc: 0.9973261952400208)
[2025-02-13 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:12][root][INFO] - Training Epoch: 2/2, step 5556/7134 completed (loss: 0.018584037199616432, acc: 0.990920901298523)
[2025-02-13 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13][root][INFO] - Training Epoch: 2/2, step 5557/7134 completed (loss: 0.014539682306349277, acc: 0.9971988797187805)
[2025-02-13 04:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13][root][INFO] - Training Epoch: 2/2, step 5558/7134 completed (loss: 0.015131361782550812, acc: 0.9937888383865356)
[2025-02-13 04:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:14][root][INFO] - Training Epoch: 2/2, step 5559/7134 completed (loss: 0.032445028424263, acc: 0.992546558380127)
[2025-02-13 04:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:14][root][INFO] - Training Epoch: 2/2, step 5560/7134 completed (loss: 0.025222832337021828, acc: 0.9922279715538025)
[2025-02-13 04:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:14][root][INFO] - Training Epoch: 2/2, step 5561/7134 completed (loss: 0.024446459487080574, acc: 0.992438554763794)
[2025-02-13 04:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15][root][INFO] - Training Epoch: 2/2, step 5562/7134 completed (loss: 0.02836688980460167, acc: 0.9965870380401611)
[2025-02-13 04:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15][root][INFO] - Training Epoch: 2/2, step 5563/7134 completed (loss: 0.0314091332256794, acc: 0.9954954981803894)
[2025-02-13 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:16][root][INFO] - Training Epoch: 2/2, step 5564/7134 completed (loss: 0.010041593573987484, acc: 0.9972106218338013)
[2025-02-13 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:16][root][INFO] - Training Epoch: 2/2, step 5565/7134 completed (loss: 0.0034082268830388784, acc: 1.0)
[2025-02-13 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17][root][INFO] - Training Epoch: 2/2, step 5566/7134 completed (loss: 0.007208028342574835, acc: 0.9988518953323364)
[2025-02-13 04:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17][root][INFO] - Training Epoch: 2/2, step 5567/7134 completed (loss: 0.006847409997135401, acc: 0.9988584518432617)
[2025-02-13 04:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:18][root][INFO] - Training Epoch: 2/2, step 5568/7134 completed (loss: 0.020294543355703354, acc: 0.9961340427398682)
[2025-02-13 04:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:18][root][INFO] - Training Epoch: 2/2, step 5569/7134 completed (loss: 0.005967083387076855, acc: 0.9977169036865234)
[2025-02-13 04:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19][root][INFO] - Training Epoch: 2/2, step 5570/7134 completed (loss: 0.012983394786715508, acc: 0.9950310587882996)
[2025-02-13 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19][root][INFO] - Training Epoch: 2/2, step 5571/7134 completed (loss: 0.027597732841968536, acc: 0.9940564632415771)
[2025-02-13 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19][root][INFO] - Training Epoch: 2/2, step 5572/7134 completed (loss: 0.02957303263247013, acc: 0.9917582273483276)
[2025-02-13 04:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:20][root][INFO] - Training Epoch: 2/2, step 5573/7134 completed (loss: 0.03240950033068657, acc: 0.9912663698196411)
[2025-02-13 04:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:20][root][INFO] - Training Epoch: 2/2, step 5574/7134 completed (loss: 0.005734151694923639, acc: 0.9984126687049866)
[2025-02-13 04:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21][root][INFO] - Training Epoch: 2/2, step 5575/7134 completed (loss: 0.018067779019474983, acc: 0.9920254945755005)
[2025-02-13 04:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21][root][INFO] - Training Epoch: 2/2, step 5576/7134 completed (loss: 0.016052890568971634, acc: 0.9957020282745361)
[2025-02-13 04:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:22][root][INFO] - Training Epoch: 2/2, step 5577/7134 completed (loss: 0.026887083426117897, acc: 0.9912739992141724)
[2025-02-13 04:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:22][root][INFO] - Training Epoch: 2/2, step 5578/7134 completed (loss: 0.004551389720290899, acc: 1.0)
[2025-02-13 04:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23][root][INFO] - Training Epoch: 2/2, step 5579/7134 completed (loss: 0.012392532080411911, acc: 0.9944674968719482)
[2025-02-13 04:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23][root][INFO] - Training Epoch: 2/2, step 5580/7134 completed (loss: 0.008294046856462955, acc: 0.9972714781761169)
[2025-02-13 04:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23][root][INFO] - Training Epoch: 2/2, step 5581/7134 completed (loss: 0.010908757336437702, acc: 0.9960106611251831)
[2025-02-13 04:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:24][root][INFO] - Training Epoch: 2/2, step 5582/7134 completed (loss: 0.0066190604120492935, acc: 0.9973924160003662)
[2025-02-13 04:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:24][root][INFO] - Training Epoch: 2/2, step 5583/7134 completed (loss: 0.018354570493102074, acc: 0.994962215423584)
[2025-02-13 04:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25][root][INFO] - Training Epoch: 2/2, step 5584/7134 completed (loss: 0.01965191774070263, acc: 0.9940828680992126)
[2025-02-13 04:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25][root][INFO] - Training Epoch: 2/2, step 5585/7134 completed (loss: 0.022272061556577682, acc: 0.9939393997192383)
[2025-02-13 04:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:26][root][INFO] - Training Epoch: 2/2, step 5586/7134 completed (loss: 0.010786986909806728, acc: 0.9946714043617249)
[2025-02-13 04:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:26][root][INFO] - Training Epoch: 2/2, step 5587/7134 completed (loss: 0.01771608181297779, acc: 0.9972260594367981)
[2025-02-13 04:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27][root][INFO] - Training Epoch: 2/2, step 5588/7134 completed (loss: 0.005670842714607716, acc: 0.998487114906311)
[2025-02-13 04:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27][root][INFO] - Training Epoch: 2/2, step 5589/7134 completed (loss: 0.0233641117811203, acc: 0.9952681660652161)
[2025-02-13 04:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28][root][INFO] - Training Epoch: 2/2, step 5590/7134 completed (loss: 0.01377217099070549, acc: 0.9943661689758301)
[2025-02-13 04:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28][root][INFO] - Training Epoch: 2/2, step 5591/7134 completed (loss: 0.010327279567718506, acc: 0.996073305606842)
[2025-02-13 04:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28][root][INFO] - Training Epoch: 2/2, step 5592/7134 completed (loss: 0.010662401095032692, acc: 0.9986282587051392)
[2025-02-13 04:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:29][root][INFO] - Training Epoch: 2/2, step 5593/7134 completed (loss: 0.01497329119592905, acc: 0.9948186278343201)
[2025-02-13 04:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:29][root][INFO] - Training Epoch: 2/2, step 5594/7134 completed (loss: 0.008556144312024117, acc: 0.9973045587539673)
[2025-02-13 04:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30][root][INFO] - Training Epoch: 2/2, step 5595/7134 completed (loss: 0.01601535640656948, acc: 0.9973190426826477)
[2025-02-13 04:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30][root][INFO] - Training Epoch: 2/2, step 5596/7134 completed (loss: 0.010832063853740692, acc: 0.9973118305206299)
[2025-02-13 04:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:31][root][INFO] - Training Epoch: 2/2, step 5597/7134 completed (loss: 0.011894267983734608, acc: 0.9958677887916565)
[2025-02-13 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:31][root][INFO] - Training Epoch: 2/2, step 5598/7134 completed (loss: 0.014271159656345844, acc: 0.994413435459137)
[2025-02-13 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:31][root][INFO] - Training Epoch: 2/2, step 5599/7134 completed (loss: 0.05031857267022133, acc: 0.9904371500015259)
[2025-02-13 04:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32][root][INFO] - Training Epoch: 2/2, step 5600/7134 completed (loss: 0.007257848046720028, acc: 0.9971346855163574)
[2025-02-13 04:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32][root][INFO] - Training Epoch: 2/2, step 5601/7134 completed (loss: 0.02112622559070587, acc: 0.991304337978363)
[2025-02-13 04:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:33][root][INFO] - Training Epoch: 2/2, step 5602/7134 completed (loss: 0.03719165176153183, acc: 0.988727867603302)
[2025-02-13 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:33][root][INFO] - Training Epoch: 2/2, step 5603/7134 completed (loss: 0.007195706944912672, acc: 0.9983633160591125)
[2025-02-13 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34][root][INFO] - Training Epoch: 2/2, step 5604/7134 completed (loss: 0.02879871055483818, acc: 0.9887459874153137)
[2025-02-13 04:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34][root][INFO] - Training Epoch: 2/2, step 5605/7134 completed (loss: 0.020606109872460365, acc: 0.9915110468864441)
[2025-02-13 04:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34][root][INFO] - Training Epoch: 2/2, step 5606/7134 completed (loss: 0.03377450257539749, acc: 0.9871794581413269)
[2025-02-13 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:35][root][INFO] - Training Epoch: 2/2, step 5607/7134 completed (loss: 0.009165165014564991, acc: 0.9969924688339233)
[2025-02-13 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:35][root][INFO] - Training Epoch: 2/2, step 5608/7134 completed (loss: 0.009284975938498974, acc: 0.9983579516410828)
[2025-02-13 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36][root][INFO] - Training Epoch: 2/2, step 5609/7134 completed (loss: 0.016611652448773384, acc: 0.9930434823036194)
[2025-02-13 04:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36][root][INFO] - Training Epoch: 2/2, step 5610/7134 completed (loss: 0.011139178648591042, acc: 0.9959758520126343)
[2025-02-13 04:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37][root][INFO] - Training Epoch: 2/2, step 5611/7134 completed (loss: 0.022621309384703636, acc: 0.9905063509941101)
[2025-02-13 04:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37][root][INFO] - Training Epoch: 2/2, step 5612/7134 completed (loss: 0.029669998213648796, acc: 0.9907975196838379)
[2025-02-13 04:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37][root][INFO] - Training Epoch: 2/2, step 5613/7134 completed (loss: 0.014939617365598679, acc: 0.9964221715927124)
[2025-02-13 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38][root][INFO] - Training Epoch: 2/2, step 5614/7134 completed (loss: 0.04395301267504692, acc: 0.9857142567634583)
[2025-02-13 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38][root][INFO] - Training Epoch: 2/2, step 5615/7134 completed (loss: 0.03207070752978325, acc: 0.984674334526062)
[2025-02-13 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:39][root][INFO] - Training Epoch: 2/2, step 5616/7134 completed (loss: 0.007468292489647865, acc: 0.9985141158103943)
[2025-02-13 04:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:39][root][INFO] - Training Epoch: 2/2, step 5617/7134 completed (loss: 0.0034813981037586927, acc: 1.0)
[2025-02-13 04:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:40][root][INFO] - Training Epoch: 2/2, step 5618/7134 completed (loss: 0.03200177103281021, acc: 0.9886178970336914)
[2025-02-13 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:40][root][INFO] - Training Epoch: 2/2, step 5619/7134 completed (loss: 0.030415726825594902, acc: 0.9918864369392395)
[2025-02-13 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:40][root][INFO] - Training Epoch: 2/2, step 5620/7134 completed (loss: 0.030111394822597504, acc: 0.9887850284576416)
[2025-02-13 04:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41][root][INFO] - Training Epoch: 2/2, step 5621/7134 completed (loss: 0.010647148825228214, acc: 0.9961880445480347)
[2025-02-13 04:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41][root][INFO] - Training Epoch: 2/2, step 5622/7134 completed (loss: 0.04563785716891289, acc: 0.9918256402015686)
[2025-02-13 04:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:42][root][INFO] - Training Epoch: 2/2, step 5623/7134 completed (loss: 0.025152796879410744, acc: 0.9895470142364502)
[2025-02-13 04:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:42][root][INFO] - Training Epoch: 2/2, step 5624/7134 completed (loss: 0.022708429023623466, acc: 0.9918166995048523)
[2025-02-13 04:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43][root][INFO] - Training Epoch: 2/2, step 5625/7134 completed (loss: 0.0037283748388290405, acc: 1.0)
[2025-02-13 04:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43][root][INFO] - Training Epoch: 2/2, step 5626/7134 completed (loss: 0.010199652053415775, acc: 0.9949238300323486)
[2025-02-13 04:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44][root][INFO] - Training Epoch: 2/2, step 5627/7134 completed (loss: 0.07630307972431183, acc: 0.9814471006393433)
[2025-02-13 04:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44][root][INFO] - Training Epoch: 2/2, step 5628/7134 completed (loss: 0.02985406666994095, acc: 0.9872495532035828)
[2025-02-13 04:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44][root][INFO] - Training Epoch: 2/2, step 5629/7134 completed (loss: 0.020217033103108406, acc: 0.9929701089859009)
[2025-02-13 04:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:45][root][INFO] - Training Epoch: 2/2, step 5630/7134 completed (loss: 0.022583622485399246, acc: 0.9938398599624634)
[2025-02-13 04:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:45][root][INFO] - Training Epoch: 2/2, step 5631/7134 completed (loss: 0.028301656246185303, acc: 0.9917080998420715)
[2025-02-13 04:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:46][root][INFO] - Training Epoch: 2/2, step 5632/7134 completed (loss: 0.02573993243277073, acc: 0.9927404522895813)
[2025-02-13 04:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:46][root][INFO] - Training Epoch: 2/2, step 5633/7134 completed (loss: 0.01540722232311964, acc: 0.9934853315353394)
[2025-02-13 04:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47][root][INFO] - Training Epoch: 2/2, step 5634/7134 completed (loss: 0.018424171954393387, acc: 0.9912739992141724)
[2025-02-13 04:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47][root][INFO] - Training Epoch: 2/2, step 5635/7134 completed (loss: 0.009950174018740654, acc: 0.9949324131011963)
[2025-02-13 04:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47][root][INFO] - Training Epoch: 2/2, step 5636/7134 completed (loss: 0.012306050397455692, acc: 0.995207667350769)
[2025-02-13 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:48][root][INFO] - Training Epoch: 2/2, step 5637/7134 completed (loss: 0.02225562371313572, acc: 0.994940996170044)
[2025-02-13 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:48][root][INFO] - Training Epoch: 2/2, step 5638/7134 completed (loss: 0.02785784751176834, acc: 0.9921875)
[2025-02-13 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:49][root][INFO] - Training Epoch: 2/2, step 5639/7134 completed (loss: 0.009213494136929512, acc: 0.9985755085945129)
[2025-02-13 04:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:49][root][INFO] - Training Epoch: 2/2, step 5640/7134 completed (loss: 0.022822437807917595, acc: 0.9918699264526367)
[2025-02-13 04:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:49][root][INFO] - Training Epoch: 2/2, step 5641/7134 completed (loss: 0.021737808361649513, acc: 0.9949495196342468)
[2025-02-13 04:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50][root][INFO] - Training Epoch: 2/2, step 5642/7134 completed (loss: 0.03142216429114342, acc: 0.9909090995788574)
[2025-02-13 04:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50][root][INFO] - Training Epoch: 2/2, step 5643/7134 completed (loss: 0.03695785626769066, acc: 0.9867256879806519)
[2025-02-13 04:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51][root][INFO] - Training Epoch: 2/2, step 5644/7134 completed (loss: 0.025471657514572144, acc: 0.9878048896789551)
[2025-02-13 04:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51][root][INFO] - Training Epoch: 2/2, step 5645/7134 completed (loss: 0.011875644326210022, acc: 0.9985272288322449)
[2025-02-13 04:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51][root][INFO] - Training Epoch: 2/2, step 5646/7134 completed (loss: 0.011428216472268105, acc: 0.9968152642250061)
[2025-02-13 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52][root][INFO] - Training Epoch: 2/2, step 5647/7134 completed (loss: 0.029021911323070526, acc: 0.9922480583190918)
[2025-02-13 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52][root][INFO] - Training Epoch: 2/2, step 5648/7134 completed (loss: 0.04331350699067116, acc: 0.9895522594451904)
[2025-02-13 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:53][root][INFO] - Training Epoch: 2/2, step 5649/7134 completed (loss: 0.03036188706755638, acc: 0.9923547506332397)
[2025-02-13 04:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:53][root][INFO] - Training Epoch: 2/2, step 5650/7134 completed (loss: 0.015776103362441063, acc: 0.9931389093399048)
[2025-02-13 04:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:54][root][INFO] - Training Epoch: 2/2, step 5651/7134 completed (loss: 0.01142306812107563, acc: 0.9940828680992126)
[2025-02-13 04:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:54][root][INFO] - Training Epoch: 2/2, step 5652/7134 completed (loss: 0.024253062903881073, acc: 0.9919871687889099)
[2025-02-13 04:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:54][root][INFO] - Training Epoch: 2/2, step 5653/7134 completed (loss: 0.047880545258522034, acc: 0.984375)
[2025-02-13 04:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55][root][INFO] - Training Epoch: 2/2, step 5654/7134 completed (loss: 0.012275797314941883, acc: 0.9954954981803894)
[2025-02-13 04:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55][root][INFO] - Training Epoch: 2/2, step 5655/7134 completed (loss: 0.023269927129149437, acc: 0.9914966225624084)
[2025-02-13 04:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:56][root][INFO] - Training Epoch: 2/2, step 5656/7134 completed (loss: 0.017263121902942657, acc: 0.9942857027053833)
[2025-02-13 04:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:56][root][INFO] - Training Epoch: 2/2, step 5657/7134 completed (loss: 0.031170524656772614, acc: 0.9941434860229492)
[2025-02-13 04:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:57][root][INFO] - Training Epoch: 2/2, step 5658/7134 completed (loss: 0.038351986557245255, acc: 0.9861878156661987)
[2025-02-13 04:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:57][root][INFO] - Training Epoch: 2/2, step 5659/7134 completed (loss: 0.0077994223684072495, acc: 0.99858158826828)
[2025-02-13 04:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:57][root][INFO] - Training Epoch: 2/2, step 5660/7134 completed (loss: 0.007554924581199884, acc: 0.9983471035957336)
[2025-02-13 04:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58][root][INFO] - Training Epoch: 2/2, step 5661/7134 completed (loss: 0.007513567339628935, acc: 0.9963503479957581)
[2025-02-13 04:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58][root][INFO] - Training Epoch: 2/2, step 5662/7134 completed (loss: 0.011922009289264679, acc: 0.9935897588729858)
[2025-02-13 04:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:59][root][INFO] - Training Epoch: 2/2, step 5663/7134 completed (loss: 0.007423034869134426, acc: 0.9984591603279114)
[2025-02-13 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:59][root][INFO] - Training Epoch: 2/2, step 5664/7134 completed (loss: 0.052527282387018204, acc: 0.9866270422935486)
[2025-02-13 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00][root][INFO] - Training Epoch: 2/2, step 5665/7134 completed (loss: 0.014911459758877754, acc: 0.9928673505783081)
[2025-02-13 04:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00][root][INFO] - Training Epoch: 2/2, step 5666/7134 completed (loss: 0.008099913597106934, acc: 0.9986206889152527)
[2025-02-13 04:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00][root][INFO] - Training Epoch: 2/2, step 5667/7134 completed (loss: 0.01381567120552063, acc: 0.996363639831543)
[2025-02-13 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:01][root][INFO] - Training Epoch: 2/2, step 5668/7134 completed (loss: 0.028867287561297417, acc: 0.9916943311691284)
[2025-02-13 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:01][root][INFO] - Training Epoch: 2/2, step 5669/7134 completed (loss: 0.006377985700964928, acc: 1.0)
[2025-02-13 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:02][root][INFO] - Training Epoch: 2/2, step 5670/7134 completed (loss: 0.012533944100141525, acc: 0.996666669845581)
[2025-02-13 04:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:02][root][INFO] - Training Epoch: 2/2, step 5671/7134 completed (loss: 0.01186169683933258, acc: 0.9938499331474304)
[2025-02-13 04:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03][root][INFO] - Training Epoch: 2/2, step 5672/7134 completed (loss: 0.009933209978044033, acc: 0.9984227418899536)
[2025-02-13 04:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03][root][INFO] - Training Epoch: 2/2, step 5673/7134 completed (loss: 0.007017186842858791, acc: 0.9970458149909973)
[2025-02-13 04:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03][root][INFO] - Training Epoch: 2/2, step 5674/7134 completed (loss: 0.006017932202666998, acc: 0.9984709620475769)
[2025-02-13 04:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:04][root][INFO] - Training Epoch: 2/2, step 5675/7134 completed (loss: 0.028875019401311874, acc: 0.9883913993835449)
[2025-02-13 04:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:04][root][INFO] - Training Epoch: 2/2, step 5676/7134 completed (loss: 0.0814930647611618, acc: 0.9807692170143127)
[2025-02-13 04:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05][root][INFO] - Training Epoch: 2/2, step 5677/7134 completed (loss: 0.049111053347587585, acc: 0.9857954382896423)
[2025-02-13 04:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05][root][INFO] - Training Epoch: 2/2, step 5678/7134 completed (loss: 0.016638724133372307, acc: 0.9941176176071167)
[2025-02-13 04:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06][root][INFO] - Training Epoch: 2/2, step 5679/7134 completed (loss: 0.046232644468545914, acc: 0.9838945865631104)
[2025-02-13 04:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06][root][INFO] - Training Epoch: 2/2, step 5680/7134 completed (loss: 0.05367407947778702, acc: 0.9806259274482727)
[2025-02-13 04:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06][root][INFO] - Training Epoch: 2/2, step 5681/7134 completed (loss: 0.030792156234383583, acc: 0.9879336357116699)
[2025-02-13 04:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:07][root][INFO] - Training Epoch: 2/2, step 5682/7134 completed (loss: 0.016329720616340637, acc: 0.9956709742546082)
[2025-02-13 04:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:07][root][INFO] - Training Epoch: 2/2, step 5683/7134 completed (loss: 0.031231436878442764, acc: 0.9838945865631104)
[2025-02-13 04:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08][root][INFO] - Training Epoch: 2/2, step 5684/7134 completed (loss: 0.05569680407643318, acc: 0.9881955981254578)
[2025-02-13 04:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08][root][INFO] - Training Epoch: 2/2, step 5685/7134 completed (loss: 0.0369274877011776, acc: 0.9926470518112183)
[2025-02-13 04:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:09][root][INFO] - Training Epoch: 2/2, step 5686/7134 completed (loss: 0.03280853480100632, acc: 0.9864636063575745)
[2025-02-13 04:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:09][root][INFO] - Training Epoch: 2/2, step 5687/7134 completed (loss: 0.045326925814151764, acc: 0.9853479862213135)
[2025-02-13 04:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:09][root][INFO] - Training Epoch: 2/2, step 5688/7134 completed (loss: 0.05009967088699341, acc: 0.9866488575935364)
[2025-02-13 04:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:10][root][INFO] - Training Epoch: 2/2, step 5689/7134 completed (loss: 0.04437832534313202, acc: 0.9893454909324646)
[2025-02-13 04:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:10][root][INFO] - Training Epoch: 2/2, step 5690/7134 completed (loss: 0.025368256494402885, acc: 0.9917452931404114)
[2025-02-13 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11][root][INFO] - Training Epoch: 2/2, step 5691/7134 completed (loss: 0.03219599649310112, acc: 0.9900621175765991)
[2025-02-13 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11][root][INFO] - Training Epoch: 2/2, step 5692/7134 completed (loss: 0.04994795471429825, acc: 0.9906166195869446)
[2025-02-13 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:12][root][INFO] - Training Epoch: 2/2, step 5693/7134 completed (loss: 0.007397241424769163, acc: 0.9985007643699646)
[2025-02-13 04:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:12][root][INFO] - Training Epoch: 2/2, step 5694/7134 completed (loss: 0.0400983951985836, acc: 0.9927641153335571)
[2025-02-13 04:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:13][root][INFO] - Training Epoch: 2/2, step 5695/7134 completed (loss: 0.01278384868055582, acc: 0.9957020282745361)
[2025-02-13 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:13][root][INFO] - Training Epoch: 2/2, step 5696/7134 completed (loss: 0.017451591789722443, acc: 0.9952940940856934)
[2025-02-13 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:13][root][INFO] - Training Epoch: 2/2, step 5697/7134 completed (loss: 0.026689141988754272, acc: 0.9929478168487549)
[2025-02-13 04:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14][root][INFO] - Training Epoch: 2/2, step 5698/7134 completed (loss: 0.02010107971727848, acc: 0.9959183931350708)
[2025-02-13 04:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14][root][INFO] - Training Epoch: 2/2, step 5699/7134 completed (loss: 0.030732983723282814, acc: 0.9898089170455933)
[2025-02-13 04:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:15][root][INFO] - Training Epoch: 2/2, step 5700/7134 completed (loss: 0.030887650325894356, acc: 0.9900709390640259)
[2025-02-13 04:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:15][root][INFO] - Training Epoch: 2/2, step 5701/7134 completed (loss: 0.03451838716864586, acc: 0.9937421679496765)
[2025-02-13 04:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16][root][INFO] - Training Epoch: 2/2, step 5702/7134 completed (loss: 0.027857784181833267, acc: 0.9945873022079468)
[2025-02-13 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16][root][INFO] - Training Epoch: 2/2, step 5703/7134 completed (loss: 0.016757981851696968, acc: 0.9943740963935852)
[2025-02-13 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17][root][INFO] - Training Epoch: 2/2, step 5704/7134 completed (loss: 0.041086990386247635, acc: 0.9885877370834351)
[2025-02-13 04:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17][root][INFO] - Training Epoch: 2/2, step 5705/7134 completed (loss: 0.008243848569691181, acc: 0.9964285492897034)
[2025-02-13 04:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17][root][INFO] - Training Epoch: 2/2, step 5706/7134 completed (loss: 0.010232138447463512, acc: 0.9966443181037903)
[2025-02-13 04:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18][root][INFO] - Training Epoch: 2/2, step 5707/7134 completed (loss: 0.018026798963546753, acc: 0.9947183132171631)
[2025-02-13 04:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18][root][INFO] - Training Epoch: 2/2, step 5708/7134 completed (loss: 0.00361136463470757, acc: 1.0)
[2025-02-13 04:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:19][root][INFO] - Training Epoch: 2/2, step 5709/7134 completed (loss: 0.040453433990478516, acc: 0.9917920827865601)
[2025-02-13 04:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:19][root][INFO] - Training Epoch: 2/2, step 5710/7134 completed (loss: 0.016979292035102844, acc: 0.9949109554290771)
[2025-02-13 04:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20][root][INFO] - Training Epoch: 2/2, step 5711/7134 completed (loss: 0.03551473096013069, acc: 0.9895287752151489)
[2025-02-13 04:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20][root][INFO] - Training Epoch: 2/2, step 5712/7134 completed (loss: 0.01927555538713932, acc: 0.9900000095367432)
[2025-02-13 04:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:21][root][INFO] - Training Epoch: 2/2, step 5713/7134 completed (loss: 0.019255025312304497, acc: 0.9949173927307129)
[2025-02-13 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:21][root][INFO] - Training Epoch: 2/2, step 5714/7134 completed (loss: 0.02417316660284996, acc: 0.9925280213356018)
[2025-02-13 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:21][root][INFO] - Training Epoch: 2/2, step 5715/7134 completed (loss: 0.030390486121177673, acc: 0.9888268113136292)
[2025-02-13 04:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22][root][INFO] - Training Epoch: 2/2, step 5716/7134 completed (loss: 0.029978696256875992, acc: 0.9875665903091431)
[2025-02-13 04:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22][root][INFO] - Training Epoch: 2/2, step 5717/7134 completed (loss: 0.03165321797132492, acc: 0.9875583052635193)
[2025-02-13 04:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:23][root][INFO] - Training Epoch: 2/2, step 5718/7134 completed (loss: 0.014913396909832954, acc: 0.9952681660652161)
[2025-02-13 04:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:23][root][INFO] - Training Epoch: 2/2, step 5719/7134 completed (loss: 0.011409756727516651, acc: 0.995502233505249)
[2025-02-13 04:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:23][root][INFO] - Training Epoch: 2/2, step 5720/7134 completed (loss: 0.0171853955835104, acc: 0.9913644194602966)
[2025-02-13 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24][root][INFO] - Training Epoch: 2/2, step 5721/7134 completed (loss: 0.007647222373634577, acc: 0.9986842274665833)
[2025-02-13 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24][root][INFO] - Training Epoch: 2/2, step 5722/7134 completed (loss: 0.007339645177125931, acc: 0.9977827072143555)
[2025-02-13 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:25][root][INFO] - Training Epoch: 2/2, step 5723/7134 completed (loss: 0.025182602927088737, acc: 0.9909228682518005)
[2025-02-13 04:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:25][root][INFO] - Training Epoch: 2/2, step 5724/7134 completed (loss: 0.009867615066468716, acc: 0.9983999729156494)
[2025-02-13 04:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26][root][INFO] - Training Epoch: 2/2, step 5725/7134 completed (loss: 0.010585075244307518, acc: 0.9971428513526917)
[2025-02-13 04:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26][root][INFO] - Training Epoch: 2/2, step 5726/7134 completed (loss: 0.04398956522345543, acc: 0.9923547506332397)
[2025-02-13 04:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26][root][INFO] - Training Epoch: 2/2, step 5727/7134 completed (loss: 0.007780144456773996, acc: 0.9986631274223328)
[2025-02-13 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27][root][INFO] - Training Epoch: 2/2, step 5728/7134 completed (loss: 0.007070258725434542, acc: 1.0)
[2025-02-13 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27][root][INFO] - Training Epoch: 2/2, step 5729/7134 completed (loss: 0.017235416918992996, acc: 0.996666669845581)
[2025-02-13 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:28][root][INFO] - Training Epoch: 2/2, step 5730/7134 completed (loss: 0.03638942167162895, acc: 0.991919219493866)
[2025-02-13 04:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:28][root][INFO] - Training Epoch: 2/2, step 5731/7134 completed (loss: 0.020759280771017075, acc: 0.9916943311691284)
[2025-02-13 04:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29][root][INFO] - Training Epoch: 2/2, step 5732/7134 completed (loss: 0.10418989509344101, acc: 0.968932032585144)
[2025-02-13 04:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29][root][INFO] - Training Epoch: 2/2, step 5733/7134 completed (loss: 0.028808925300836563, acc: 0.9890710115432739)
[2025-02-13 04:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29][root][INFO] - Training Epoch: 2/2, step 5734/7134 completed (loss: 0.004798159934580326, acc: 1.0)
[2025-02-13 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:30][root][INFO] - Training Epoch: 2/2, step 5735/7134 completed (loss: 0.022930199280381203, acc: 0.9964157938957214)
[2025-02-13 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:30][root][INFO] - Training Epoch: 2/2, step 5736/7134 completed (loss: 0.018872104585170746, acc: 0.9969924688339233)
[2025-02-13 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31][root][INFO] - Training Epoch: 2/2, step 5737/7134 completed (loss: 0.019187070429325104, acc: 0.9939576983451843)
[2025-02-13 04:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31][root][INFO] - Training Epoch: 2/2, step 5738/7134 completed (loss: 0.01272287406027317, acc: 0.9975816011428833)
[2025-02-13 04:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:32][root][INFO] - Training Epoch: 2/2, step 5739/7134 completed (loss: 0.030314242467284203, acc: 0.9915789365768433)
[2025-02-13 04:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:32][root][INFO] - Training Epoch: 2/2, step 5740/7134 completed (loss: 0.022421743720769882, acc: 0.9897698163986206)
[2025-02-13 04:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:32][root][INFO] - Training Epoch: 2/2, step 5741/7134 completed (loss: 0.02848104201257229, acc: 0.9935815334320068)
[2025-02-13 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:33][root][INFO] - Training Epoch: 2/2, step 5742/7134 completed (loss: 0.02353380061686039, acc: 0.9934297204017639)
[2025-02-13 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:33][root][INFO] - Training Epoch: 2/2, step 5743/7134 completed (loss: 0.007019846234470606, acc: 0.9975489974021912)
[2025-02-13 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34][root][INFO] - Training Epoch: 2/2, step 5744/7134 completed (loss: 0.006960710044950247, acc: 1.0)
[2025-02-13 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34][root][INFO] - Training Epoch: 2/2, step 5745/7134 completed (loss: 0.023756127804517746, acc: 0.9896373152732849)
[2025-02-13 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35][root][INFO] - Training Epoch: 2/2, step 5746/7134 completed (loss: 0.039244018495082855, acc: 0.9926470518112183)
[2025-02-13 04:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35][root][INFO] - Training Epoch: 2/2, step 5747/7134 completed (loss: 0.019354376941919327, acc: 0.9944598078727722)
[2025-02-13 04:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35][root][INFO] - Training Epoch: 2/2, step 5748/7134 completed (loss: 0.008298111148178577, acc: 0.998171865940094)
[2025-02-13 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36][root][INFO] - Training Epoch: 2/2, step 5749/7134 completed (loss: 0.018128277733922005, acc: 0.996073305606842)
[2025-02-13 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36][root][INFO] - Training Epoch: 2/2, step 5750/7134 completed (loss: 0.030121106654405594, acc: 0.9876237511634827)
[2025-02-13 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:37][root][INFO] - Training Epoch: 2/2, step 5751/7134 completed (loss: 0.022178947925567627, acc: 0.9910141229629517)
[2025-02-13 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:37][root][INFO] - Training Epoch: 2/2, step 5752/7134 completed (loss: 0.06485844403505325, acc: 0.9814385175704956)
[2025-02-13 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38][root][INFO] - Training Epoch: 2/2, step 5753/7134 completed (loss: 0.04338967055082321, acc: 0.9859374761581421)
[2025-02-13 04:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38][root][INFO] - Training Epoch: 2/2, step 5754/7134 completed (loss: 0.040065355598926544, acc: 0.987261176109314)
[2025-02-13 04:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39][root][INFO] - Training Epoch: 2/2, step 5755/7134 completed (loss: 0.030358675867319107, acc: 0.9912152290344238)
[2025-02-13 04:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39][root][INFO] - Training Epoch: 2/2, step 5756/7134 completed (loss: 0.020767372101545334, acc: 0.9946808218955994)
[2025-02-13 04:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39][root][INFO] - Training Epoch: 2/2, step 5757/7134 completed (loss: 0.026113415136933327, acc: 0.9885057210922241)
[2025-02-13 04:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40][root][INFO] - Training Epoch: 2/2, step 5758/7134 completed (loss: 0.02218829281628132, acc: 0.9955882430076599)
[2025-02-13 04:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40][root][INFO] - Training Epoch: 2/2, step 5759/7134 completed (loss: 0.027826854959130287, acc: 0.9892183542251587)
[2025-02-13 04:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:41][root][INFO] - Training Epoch: 2/2, step 5760/7134 completed (loss: 0.014492173679172993, acc: 0.9954198598861694)
[2025-02-13 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:41][root][INFO] - Training Epoch: 2/2, step 5761/7134 completed (loss: 0.013092603534460068, acc: 0.9945054650306702)
[2025-02-13 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42][root][INFO] - Training Epoch: 2/2, step 5762/7134 completed (loss: 0.03610887750983238, acc: 0.989051103591919)
[2025-02-13 04:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42][root][INFO] - Training Epoch: 2/2, step 5763/7134 completed (loss: 0.021942393854260445, acc: 0.9949811697006226)
[2025-02-13 04:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42][root][INFO] - Training Epoch: 2/2, step 5764/7134 completed (loss: 0.011174811981618404, acc: 0.9965811967849731)
[2025-02-13 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43][root][INFO] - Training Epoch: 2/2, step 5765/7134 completed (loss: 0.010750199668109417, acc: 0.9957982897758484)
[2025-02-13 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43][root][INFO] - Training Epoch: 2/2, step 5766/7134 completed (loss: 0.01297780778259039, acc: 0.9985052347183228)
[2025-02-13 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:44][root][INFO] - Training Epoch: 2/2, step 5767/7134 completed (loss: 0.005218899343162775, acc: 0.998609185218811)
[2025-02-13 04:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:44][root][INFO] - Training Epoch: 2/2, step 5768/7134 completed (loss: 0.009147840552031994, acc: 0.9970059990882874)
[2025-02-13 04:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:45][root][INFO] - Training Epoch: 2/2, step 5769/7134 completed (loss: 0.0013752576196566224, acc: 1.0)
[2025-02-13 04:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:45][root][INFO] - Training Epoch: 2/2, step 5770/7134 completed (loss: 0.030946925282478333, acc: 0.9914320707321167)
[2025-02-13 04:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46][root][INFO] - Training Epoch: 2/2, step 5771/7134 completed (loss: 0.027005678042769432, acc: 0.9932065010070801)
[2025-02-13 04:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46][root][INFO] - Training Epoch: 2/2, step 5772/7134 completed (loss: 0.05756513029336929, acc: 0.9878261089324951)
[2025-02-13 04:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46][root][INFO] - Training Epoch: 2/2, step 5773/7134 completed (loss: 0.016744980588555336, acc: 0.9949832558631897)
[2025-02-13 04:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:47][root][INFO] - Training Epoch: 2/2, step 5774/7134 completed (loss: 0.004978797864168882, acc: 0.998516321182251)
[2025-02-13 04:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:47][root][INFO] - Training Epoch: 2/2, step 5775/7134 completed (loss: 0.009666376747190952, acc: 0.9983525276184082)
[2025-02-13 04:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:48][root][INFO] - Training Epoch: 2/2, step 5776/7134 completed (loss: 0.013368151150643826, acc: 0.9957864880561829)
[2025-02-13 04:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:48][root][INFO] - Training Epoch: 2/2, step 5777/7134 completed (loss: 0.03530446067452431, acc: 0.9950494766235352)
[2025-02-13 04:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49][root][INFO] - Training Epoch: 2/2, step 5778/7134 completed (loss: 0.04867992177605629, acc: 0.9855769276618958)
[2025-02-13 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49][root][INFO] - Training Epoch: 2/2, step 5779/7134 completed (loss: 0.04040955379605293, acc: 0.9903714060783386)
[2025-02-13 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:50][root][INFO] - Training Epoch: 2/2, step 5780/7134 completed (loss: 0.013095183297991753, acc: 0.9952267408370972)
[2025-02-13 04:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:50][root][INFO] - Training Epoch: 2/2, step 5781/7134 completed (loss: 0.023059291765093803, acc: 0.9917126893997192)
[2025-02-13 04:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:50][root][INFO] - Training Epoch: 2/2, step 5782/7134 completed (loss: 0.009630875661969185, acc: 0.9974093437194824)
[2025-02-13 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:51][root][INFO] - Training Epoch: 2/2, step 5783/7134 completed (loss: 0.00851333886384964, acc: 0.9986720085144043)
[2025-02-13 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:51][root][INFO] - Training Epoch: 2/2, step 5784/7134 completed (loss: 0.016667086631059647, acc: 0.9946164488792419)
[2025-02-13 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52][root][INFO] - Training Epoch: 2/2, step 5785/7134 completed (loss: 0.02419513650238514, acc: 0.9919137358665466)
[2025-02-13 04:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52][root][INFO] - Training Epoch: 2/2, step 5786/7134 completed (loss: 0.009994257241487503, acc: 0.9979507923126221)
[2025-02-13 04:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:53][root][INFO] - Training Epoch: 2/2, step 5787/7134 completed (loss: 0.019242674112319946, acc: 0.9950248599052429)
[2025-02-13 04:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:53][root][INFO] - Training Epoch: 2/2, step 5788/7134 completed (loss: 0.007651998661458492, acc: 0.9969230890274048)
[2025-02-13 04:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54][root][INFO] - Training Epoch: 2/2, step 5789/7134 completed (loss: 0.021150849759578705, acc: 0.9900744557380676)
[2025-02-13 04:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54][root][INFO] - Training Epoch: 2/2, step 5790/7134 completed (loss: 0.020552178844809532, acc: 0.9903846383094788)
[2025-02-13 04:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54][root][INFO] - Training Epoch: 2/2, step 5791/7134 completed (loss: 0.020583946257829666, acc: 0.9948453903198242)
[2025-02-13 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:55][root][INFO] - Training Epoch: 2/2, step 5792/7134 completed (loss: 0.05360414832830429, acc: 0.9881154298782349)
[2025-02-13 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:55][root][INFO] - Training Epoch: 2/2, step 5793/7134 completed (loss: 0.04572953283786774, acc: 0.9858490824699402)
[2025-02-13 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56][root][INFO] - Training Epoch: 2/2, step 5794/7134 completed (loss: 0.02508203126490116, acc: 0.9878787994384766)
[2025-02-13 04:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56][root][INFO] - Training Epoch: 2/2, step 5795/7134 completed (loss: 0.07157032191753387, acc: 0.9852941036224365)
[2025-02-13 04:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57][root][INFO] - Training Epoch: 2/2, step 5796/7134 completed (loss: 0.015019095502793789, acc: 0.9966158866882324)
[2025-02-13 04:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57][root][INFO] - Training Epoch: 2/2, step 5797/7134 completed (loss: 0.015299275517463684, acc: 0.995199978351593)
[2025-02-13 04:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57][root][INFO] - Training Epoch: 2/2, step 5798/7134 completed (loss: 0.1517125368118286, acc: 0.9686956405639648)
[2025-02-13 04:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:58][root][INFO] - Training Epoch: 2/2, step 5799/7134 completed (loss: 0.04090365767478943, acc: 0.9881656765937805)
[2025-02-13 04:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:58][root][INFO] - Training Epoch: 2/2, step 5800/7134 completed (loss: 0.037110745906829834, acc: 0.9834558963775635)
[2025-02-13 04:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59][root][INFO] - Training Epoch: 2/2, step 5801/7134 completed (loss: 0.028237726539373398, acc: 0.9908536672592163)
[2025-02-13 04:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59][root][INFO] - Training Epoch: 2/2, step 5802/7134 completed (loss: 0.016588998958468437, acc: 0.9944547414779663)
[2025-02-13 04:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:00][root][INFO] - Training Epoch: 2/2, step 5803/7134 completed (loss: 0.013829830102622509, acc: 0.9961734414100647)
[2025-02-13 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:00][root][INFO] - Training Epoch: 2/2, step 5804/7134 completed (loss: 0.05387363210320473, acc: 0.9824086427688599)
[2025-02-13 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:00][root][INFO] - Training Epoch: 2/2, step 5805/7134 completed (loss: 0.04015519097447395, acc: 0.9913669228553772)
[2025-02-13 04:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:01][root][INFO] - Training Epoch: 2/2, step 5806/7134 completed (loss: 0.015683569014072418, acc: 0.994535505771637)
[2025-02-13 04:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:01][root][INFO] - Training Epoch: 2/2, step 5807/7134 completed (loss: 0.03920488432049751, acc: 0.9909228682518005)
[2025-02-13 04:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02][root][INFO] - Training Epoch: 2/2, step 5808/7134 completed (loss: 0.015499910339713097, acc: 0.9943740963935852)
[2025-02-13 04:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02][root][INFO] - Training Epoch: 2/2, step 5809/7134 completed (loss: 0.026583785191178322, acc: 0.9956140518188477)
[2025-02-13 04:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:03][root][INFO] - Training Epoch: 2/2, step 5810/7134 completed (loss: 0.014484059996902943, acc: 0.993914783000946)
[2025-02-13 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:03][root][INFO] - Training Epoch: 2/2, step 5811/7134 completed (loss: 0.022020844742655754, acc: 0.9903614521026611)
[2025-02-13 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:03][root][INFO] - Training Epoch: 2/2, step 5812/7134 completed (loss: 0.009275971911847591, acc: 0.9983498454093933)
[2025-02-13 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:04][root][INFO] - Training Epoch: 2/2, step 5813/7134 completed (loss: 0.015018881298601627, acc: 0.994339644908905)
[2025-02-13 04:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:04][root][INFO] - Training Epoch: 2/2, step 5814/7134 completed (loss: 0.007818596437573433, acc: 1.0)
[2025-02-13 04:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05][root][INFO] - Training Epoch: 2/2, step 5815/7134 completed (loss: 0.013592607341706753, acc: 0.994350254535675)
[2025-02-13 04:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05][root][INFO] - Training Epoch: 2/2, step 5816/7134 completed (loss: 0.0038204132579267025, acc: 0.9987531304359436)
[2025-02-13 04:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06][root][INFO] - Training Epoch: 2/2, step 5817/7134 completed (loss: 0.0760657787322998, acc: 0.984886646270752)
[2025-02-13 04:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06][root][INFO] - Training Epoch: 2/2, step 5818/7134 completed (loss: 0.017239732667803764, acc: 0.9925925731658936)
[2025-02-13 04:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06][root][INFO] - Training Epoch: 2/2, step 5819/7134 completed (loss: 0.020761949941515923, acc: 0.9913793206214905)
[2025-02-13 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07][root][INFO] - Training Epoch: 2/2, step 5820/7134 completed (loss: 0.038487423211336136, acc: 0.9896694421768188)
[2025-02-13 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07][root][INFO] - Training Epoch: 2/2, step 5821/7134 completed (loss: 0.006368803326040506, acc: 0.9969183206558228)
[2025-02-13 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:08][root][INFO] - Training Epoch: 2/2, step 5822/7134 completed (loss: 0.011067792773246765, acc: 0.9967741966247559)
[2025-02-13 04:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:08][root][INFO] - Training Epoch: 2/2, step 5823/7134 completed (loss: 0.015919683501124382, acc: 0.9953343868255615)
[2025-02-13 04:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09][root][INFO] - Training Epoch: 2/2, step 5824/7134 completed (loss: 0.006112896837294102, acc: 0.996889591217041)
[2025-02-13 04:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09][root][INFO] - Training Epoch: 2/2, step 5825/7134 completed (loss: 0.028486842289566994, acc: 0.9930796027183533)
[2025-02-13 04:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09][root][INFO] - Training Epoch: 2/2, step 5826/7134 completed (loss: 0.0029989462345838547, acc: 1.0)
[2025-02-13 04:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10][root][INFO] - Training Epoch: 2/2, step 5827/7134 completed (loss: 0.01924843154847622, acc: 0.9961538314819336)
[2025-02-13 04:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10][root][INFO] - Training Epoch: 2/2, step 5828/7134 completed (loss: 0.04300420358777046, acc: 0.9881129264831543)
[2025-02-13 04:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:11][root][INFO] - Training Epoch: 2/2, step 5829/7134 completed (loss: 0.018738722428679466, acc: 0.9947299361228943)
[2025-02-13 04:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:11][root][INFO] - Training Epoch: 2/2, step 5830/7134 completed (loss: 0.015163884498178959, acc: 0.9981096386909485)
[2025-02-13 04:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:11][root][INFO] - Training Epoch: 2/2, step 5831/7134 completed (loss: 0.008096698671579361, acc: 0.9976744055747986)
[2025-02-13 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12][root][INFO] - Training Epoch: 2/2, step 5832/7134 completed (loss: 0.03477870300412178, acc: 0.9915540814399719)
[2025-02-13 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12][root][INFO] - Training Epoch: 2/2, step 5833/7134 completed (loss: 0.02058630809187889, acc: 0.9910394549369812)
[2025-02-13 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:13][root][INFO] - Training Epoch: 2/2, step 5834/7134 completed (loss: 0.012686293572187424, acc: 0.9958333373069763)
[2025-02-13 04:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:13][root][INFO] - Training Epoch: 2/2, step 5835/7134 completed (loss: 0.008903248235583305, acc: 0.9985548853874207)
[2025-02-13 04:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14][root][INFO] - Training Epoch: 2/2, step 5836/7134 completed (loss: 0.01516114454716444, acc: 0.9956772327423096)
[2025-02-13 04:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14][root][INFO] - Training Epoch: 2/2, step 5837/7134 completed (loss: 0.0641193613409996, acc: 0.9915540814399719)
[2025-02-13 04:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14][root][INFO] - Training Epoch: 2/2, step 5838/7134 completed (loss: 0.030447134748101234, acc: 0.9950576424598694)
[2025-02-13 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:15][root][INFO] - Training Epoch: 2/2, step 5839/7134 completed (loss: 0.0352734811604023, acc: 0.987500011920929)
[2025-02-13 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:15][root][INFO] - Training Epoch: 2/2, step 5840/7134 completed (loss: 0.0062097059562802315, acc: 0.998410165309906)
[2025-02-13 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16][root][INFO] - Training Epoch: 2/2, step 5841/7134 completed (loss: 0.01445770263671875, acc: 0.9968101978302002)
[2025-02-13 04:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16][root][INFO] - Training Epoch: 2/2, step 5842/7134 completed (loss: 0.023542355746030807, acc: 0.9963768124580383)
[2025-02-13 04:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:17][root][INFO] - Training Epoch: 2/2, step 5843/7134 completed (loss: 0.03336720913648605, acc: 0.9919678568840027)
[2025-02-13 04:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:17][root][INFO] - Training Epoch: 2/2, step 5844/7134 completed (loss: 0.01185333076864481, acc: 0.9985775351524353)
[2025-02-13 04:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:17][root][INFO] - Training Epoch: 2/2, step 5845/7134 completed (loss: 0.012984625063836575, acc: 0.9971510171890259)
[2025-02-13 04:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18][root][INFO] - Training Epoch: 2/2, step 5846/7134 completed (loss: 0.01958271488547325, acc: 0.99589604139328)
[2025-02-13 04:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18][root][INFO] - Training Epoch: 2/2, step 5847/7134 completed (loss: 0.00953839160501957, acc: 0.9972527623176575)
[2025-02-13 04:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:19][root][INFO] - Training Epoch: 2/2, step 5848/7134 completed (loss: 0.019912857562303543, acc: 0.9940476417541504)
[2025-02-13 04:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:19][root][INFO] - Training Epoch: 2/2, step 5849/7134 completed (loss: 0.030700523406267166, acc: 0.9882006049156189)
[2025-02-13 04:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20][root][INFO] - Training Epoch: 2/2, step 5850/7134 completed (loss: 0.012445039115846157, acc: 0.9938931465148926)
[2025-02-13 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20][root][INFO] - Training Epoch: 2/2, step 5851/7134 completed (loss: 0.013306248001754284, acc: 0.9967105388641357)
[2025-02-13 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:21][root][INFO] - Training Epoch: 2/2, step 5852/7134 completed (loss: 0.015806738287210464, acc: 0.9941605925559998)
[2025-02-13 04:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:21][root][INFO] - Training Epoch: 2/2, step 5853/7134 completed (loss: 0.0388282872736454, acc: 0.9905277490615845)
[2025-02-13 04:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:21][root][INFO] - Training Epoch: 2/2, step 5854/7134 completed (loss: 0.012274404987692833, acc: 0.9946902394294739)
[2025-02-13 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:22][root][INFO] - Training Epoch: 2/2, step 5855/7134 completed (loss: 0.019641755148768425, acc: 0.9913644194602966)
[2025-02-13 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:22][root][INFO] - Training Epoch: 2/2, step 5856/7134 completed (loss: 0.018645891919732094, acc: 0.995468258857727)
[2025-02-13 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:23][root][INFO] - Training Epoch: 2/2, step 5857/7134 completed (loss: 0.01821000501513481, acc: 0.9936808943748474)
[2025-02-13 04:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:23][root][INFO] - Training Epoch: 2/2, step 5858/7134 completed (loss: 0.009641861543059349, acc: 0.9981516003608704)
[2025-02-13 04:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24][root][INFO] - Training Epoch: 2/2, step 5859/7134 completed (loss: 0.019432049244642258, acc: 0.990234375)
[2025-02-13 04:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24][root][INFO] - Training Epoch: 2/2, step 5860/7134 completed (loss: 0.02067325823009014, acc: 0.9917355179786682)
[2025-02-13 04:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24][root][INFO] - Training Epoch: 2/2, step 5861/7134 completed (loss: 0.01947018690407276, acc: 0.9959431886672974)
[2025-02-13 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25][root][INFO] - Training Epoch: 2/2, step 5862/7134 completed (loss: 0.011863486841320992, acc: 0.995708167552948)
[2025-02-13 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25][root][INFO] - Training Epoch: 2/2, step 5863/7134 completed (loss: 0.00925405602902174, acc: 0.997474730014801)
[2025-02-13 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:26][root][INFO] - Training Epoch: 2/2, step 5864/7134 completed (loss: 0.03547128289937973, acc: 0.9895209670066833)
[2025-02-13 04:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:26][root][INFO] - Training Epoch: 2/2, step 5865/7134 completed (loss: 0.01934792473912239, acc: 0.9936507940292358)
[2025-02-13 04:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27][root][INFO] - Training Epoch: 2/2, step 5866/7134 completed (loss: 0.01321424636989832, acc: 0.9947090148925781)
[2025-02-13 04:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27][root][INFO] - Training Epoch: 2/2, step 5867/7134 completed (loss: 0.004200726747512817, acc: 1.0)
[2025-02-13 04:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27][root][INFO] - Training Epoch: 2/2, step 5868/7134 completed (loss: 0.021785631775856018, acc: 0.9913793206214905)
[2025-02-13 04:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:28][root][INFO] - Training Epoch: 2/2, step 5869/7134 completed (loss: 0.016389025375247, acc: 0.9940298795700073)
[2025-02-13 04:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:28][root][INFO] - Training Epoch: 2/2, step 5870/7134 completed (loss: 0.02374293841421604, acc: 0.9958158731460571)
[2025-02-13 04:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29][root][INFO] - Training Epoch: 2/2, step 5871/7134 completed (loss: 0.00980374589562416, acc: 0.9957537055015564)
[2025-02-13 04:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29][root][INFO] - Training Epoch: 2/2, step 5872/7134 completed (loss: 0.008532773703336716, acc: 0.9969558715820312)
[2025-02-13 04:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:30][root][INFO] - Training Epoch: 2/2, step 5873/7134 completed (loss: 0.013207855634391308, acc: 0.9973333477973938)
[2025-02-13 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:30][root][INFO] - Training Epoch: 2/2, step 5874/7134 completed (loss: 0.026899252086877823, acc: 0.9939758777618408)
[2025-02-13 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:30][root][INFO] - Training Epoch: 2/2, step 5875/7134 completed (loss: 0.03189615160226822, acc: 0.991150438785553)
[2025-02-13 04:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:31][root][INFO] - Training Epoch: 2/2, step 5876/7134 completed (loss: 0.002382454462349415, acc: 1.0)
[2025-02-13 04:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:31][root][INFO] - Training Epoch: 2/2, step 5877/7134 completed (loss: 0.027324950322508812, acc: 0.9923858046531677)
[2025-02-13 04:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:32][root][INFO] - Training Epoch: 2/2, step 5878/7134 completed (loss: 0.017348498106002808, acc: 0.9934210777282715)
[2025-02-13 04:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:32][root][INFO] - Training Epoch: 2/2, step 5879/7134 completed (loss: 0.00632309727370739, acc: 1.0)
[2025-02-13 04:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33][root][INFO] - Training Epoch: 2/2, step 5880/7134 completed (loss: 0.00402018241584301, acc: 0.995555579662323)
[2025-02-13 04:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33][root][INFO] - Training Epoch: 2/2, step 5881/7134 completed (loss: 0.00453712185844779, acc: 0.9980806112289429)
[2025-02-13 04:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33][root][INFO] - Training Epoch: 2/2, step 5882/7134 completed (loss: 0.013676697388291359, acc: 0.9952940940856934)
[2025-02-13 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:34][root][INFO] - Training Epoch: 2/2, step 5883/7134 completed (loss: 0.02361235022544861, acc: 0.9943820238113403)
[2025-02-13 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:34][root][INFO] - Training Epoch: 2/2, step 5884/7134 completed (loss: 0.033600740134716034, acc: 0.9908758997917175)
[2025-02-13 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35][root][INFO] - Training Epoch: 2/2, step 5885/7134 completed (loss: 0.07811105996370316, acc: 0.97826087474823)
[2025-02-13 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35][root][INFO] - Training Epoch: 2/2, step 5886/7134 completed (loss: 0.008676665835082531, acc: 0.9942857027053833)
[2025-02-13 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35][root][INFO] - Training Epoch: 2/2, step 5887/7134 completed (loss: 0.02420756220817566, acc: 0.9915074110031128)
[2025-02-13 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:36][root][INFO] - Training Epoch: 2/2, step 5888/7134 completed (loss: 0.03541964292526245, acc: 0.9898648858070374)
[2025-02-13 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:36][root][INFO] - Training Epoch: 2/2, step 5889/7134 completed (loss: 0.001233869232237339, acc: 1.0)
[2025-02-13 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37][root][INFO] - Training Epoch: 2/2, step 5890/7134 completed (loss: 0.007898686453700066, acc: 0.998251736164093)
[2025-02-13 04:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37][root][INFO] - Training Epoch: 2/2, step 5891/7134 completed (loss: 0.01968146488070488, acc: 0.9944751262664795)
[2025-02-13 04:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:38][root][INFO] - Training Epoch: 2/2, step 5892/7134 completed (loss: 0.02894628420472145, acc: 0.9941691160202026)
[2025-02-13 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:38][root][INFO] - Training Epoch: 2/2, step 5893/7134 completed (loss: 0.031025348231196404, acc: 0.9921259880065918)
[2025-02-13 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:38][root][INFO] - Training Epoch: 2/2, step 5894/7134 completed (loss: 0.0035822130739688873, acc: 1.0)
[2025-02-13 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39][root][INFO] - Training Epoch: 2/2, step 5895/7134 completed (loss: 0.003452962264418602, acc: 1.0)
[2025-02-13 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39][root][INFO] - Training Epoch: 2/2, step 5896/7134 completed (loss: 0.02714209258556366, acc: 0.9931972622871399)
[2025-02-13 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40][root][INFO] - Training Epoch: 2/2, step 5897/7134 completed (loss: 0.006793103646486998, acc: 0.9986225962638855)
[2025-02-13 04:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40][root][INFO] - Training Epoch: 2/2, step 5898/7134 completed (loss: 0.006475653499364853, acc: 1.0)
[2025-02-13 04:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40][root][INFO] - Training Epoch: 2/2, step 5899/7134 completed (loss: 0.005851361900568008, acc: 1.0)
[2025-02-13 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41][root][INFO] - Training Epoch: 2/2, step 5900/7134 completed (loss: 0.00833666231483221, acc: 0.9970238208770752)
[2025-02-13 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41][root][INFO] - Training Epoch: 2/2, step 5901/7134 completed (loss: 0.03766223043203354, acc: 0.9900000095367432)
[2025-02-13 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42][root][INFO] - Training Epoch: 2/2, step 5902/7134 completed (loss: 0.01441067736595869, acc: 0.9948453903198242)
[2025-02-13 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42][root][INFO] - Training Epoch: 2/2, step 5903/7134 completed (loss: 0.03997740149497986, acc: 0.9872262477874756)
[2025-02-13 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43][root][INFO] - Training Epoch: 2/2, step 5904/7134 completed (loss: 0.01704034022986889, acc: 0.994535505771637)
[2025-02-13 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43][root][INFO] - Training Epoch: 2/2, step 5905/7134 completed (loss: 0.017014136537909508, acc: 0.9930459260940552)
[2025-02-13 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43][root][INFO] - Training Epoch: 2/2, step 5906/7134 completed (loss: 0.023758506402373314, acc: 0.9938575029373169)
[2025-02-13 04:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44][root][INFO] - Training Epoch: 2/2, step 5907/7134 completed (loss: 0.02258216217160225, acc: 0.9935317039489746)
[2025-02-13 04:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44][root][INFO] - Training Epoch: 2/2, step 5908/7134 completed (loss: 0.007186757866293192, acc: 0.9987212419509888)
[2025-02-13 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:45][root][INFO] - Training Epoch: 2/2, step 5909/7134 completed (loss: 0.025962557643651962, acc: 0.9899135231971741)
[2025-02-13 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:46][root][INFO] - Training Epoch: 2/2, step 5910/7134 completed (loss: 0.044396668672561646, acc: 0.9861303567886353)
[2025-02-13 04:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:46][root][INFO] - Training Epoch: 2/2, step 5911/7134 completed (loss: 0.02974882908165455, acc: 0.9950413107872009)
[2025-02-13 04:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:46][root][INFO] - Training Epoch: 2/2, step 5912/7134 completed (loss: 0.010083592496812344, acc: 1.0)
[2025-02-13 04:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47][root][INFO] - Training Epoch: 2/2, step 5913/7134 completed (loss: 0.04829857498407364, acc: 0.9850543737411499)
[2025-02-13 04:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47][root][INFO] - Training Epoch: 2/2, step 5914/7134 completed (loss: 0.021175963804125786, acc: 0.9935794472694397)
[2025-02-13 04:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:48][root][INFO] - Training Epoch: 2/2, step 5915/7134 completed (loss: 0.014495564624667168, acc: 0.9953488111495972)
[2025-02-13 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:48][root][INFO] - Training Epoch: 2/2, step 5916/7134 completed (loss: 0.027739055454730988, acc: 0.9921362996101379)
[2025-02-13 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:49][root][INFO] - Training Epoch: 2/2, step 5917/7134 completed (loss: 0.017862264066934586, acc: 0.9930675625801086)
[2025-02-13 04:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:49][root][INFO] - Training Epoch: 2/2, step 5918/7134 completed (loss: 0.05927308648824692, acc: 0.9811320900917053)
[2025-02-13 04:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:49][root][INFO] - Training Epoch: 2/2, step 5919/7134 completed (loss: 0.01255446020513773, acc: 0.996960461139679)
[2025-02-13 04:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50][root][INFO] - Training Epoch: 2/2, step 5920/7134 completed (loss: 0.011341760866343975, acc: 0.9966722130775452)
[2025-02-13 04:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50][root][INFO] - Training Epoch: 2/2, step 5921/7134 completed (loss: 0.009561642073094845, acc: 0.9958246350288391)
[2025-02-13 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:51][root][INFO] - Training Epoch: 2/2, step 5922/7134 completed (loss: 0.1411934345960617, acc: 0.9716312289237976)
[2025-02-13 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:51][root][INFO] - Training Epoch: 2/2, step 5923/7134 completed (loss: 0.05473264679312706, acc: 0.985358715057373)
[2025-02-13 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:52][root][INFO] - Training Epoch: 2/2, step 5924/7134 completed (loss: 0.019121356308460236, acc: 0.9904534816741943)
[2025-02-13 04:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:52][root][INFO] - Training Epoch: 2/2, step 5925/7134 completed (loss: 0.09234241396188736, acc: 0.973607063293457)
[2025-02-13 04:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53][root][INFO] - Training Epoch: 2/2, step 5926/7134 completed (loss: 0.05426964908838272, acc: 0.9850000143051147)
[2025-02-13 04:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53][root][INFO] - Training Epoch: 2/2, step 5927/7134 completed (loss: 0.038938574492931366, acc: 0.9866369962692261)
[2025-02-13 04:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53][root][INFO] - Training Epoch: 2/2, step 5928/7134 completed (loss: 0.10721476376056671, acc: 0.965753436088562)
[2025-02-13 04:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:54][root][INFO] - Training Epoch: 2/2, step 5929/7134 completed (loss: 0.02302224189043045, acc: 0.9944030046463013)
[2025-02-13 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:54][root][INFO] - Training Epoch: 2/2, step 5930/7134 completed (loss: 0.05320512875914574, acc: 0.9888641238212585)
[2025-02-13 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55][root][INFO] - Training Epoch: 2/2, step 5931/7134 completed (loss: 0.020808372646570206, acc: 0.9935204982757568)
[2025-02-13 04:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55][root][INFO] - Training Epoch: 2/2, step 5932/7134 completed (loss: 0.0619642473757267, acc: 0.979629635810852)
[2025-02-13 04:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55][root][INFO] - Training Epoch: 2/2, step 5933/7134 completed (loss: 0.09063132107257843, acc: 0.9727463126182556)
[2025-02-13 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:56][root][INFO] - Training Epoch: 2/2, step 5934/7134 completed (loss: 0.04205017536878586, acc: 0.9893428087234497)
[2025-02-13 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:56][root][INFO] - Training Epoch: 2/2, step 5935/7134 completed (loss: 0.011842655017971992, acc: 0.9961165189743042)
[2025-02-13 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57][root][INFO] - Training Epoch: 2/2, step 5936/7134 completed (loss: 0.05314267426729202, acc: 0.9831546545028687)
[2025-02-13 04:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57][root][INFO] - Training Epoch: 2/2, step 5937/7134 completed (loss: 0.017561914399266243, acc: 0.9968101978302002)
[2025-02-13 04:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57][root][INFO] - Training Epoch: 2/2, step 5938/7134 completed (loss: 0.03205833584070206, acc: 0.990439772605896)
[2025-02-13 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:58][root][INFO] - Training Epoch: 2/2, step 5939/7134 completed (loss: 0.06581693887710571, acc: 0.9819004535675049)
[2025-02-13 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:58][root][INFO] - Training Epoch: 2/2, step 5940/7134 completed (loss: 0.025608137249946594, acc: 0.991253674030304)
[2025-02-13 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:59][root][INFO] - Training Epoch: 2/2, step 5941/7134 completed (loss: 0.02724180743098259, acc: 0.9937499761581421)
[2025-02-13 04:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:59][root][INFO] - Training Epoch: 2/2, step 5942/7134 completed (loss: 0.007872500456869602, acc: 0.9961758852005005)
[2025-02-13 04:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00][root][INFO] - Training Epoch: 2/2, step 5943/7134 completed (loss: 0.019305281341075897, acc: 0.9928469061851501)
[2025-02-13 04:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00][root][INFO] - Training Epoch: 2/2, step 5944/7134 completed (loss: 0.021869821473956108, acc: 0.9921466112136841)
[2025-02-13 04:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:01][root][INFO] - Training Epoch: 2/2, step 5945/7134 completed (loss: 0.0427408292889595, acc: 0.9879879951477051)
[2025-02-13 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:01][root][INFO] - Training Epoch: 2/2, step 5946/7134 completed (loss: 0.02005923166871071, acc: 0.9927536249160767)
[2025-02-13 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:01][root][INFO] - Training Epoch: 2/2, step 5947/7134 completed (loss: 0.027060655876994133, acc: 0.9913259148597717)
[2025-02-13 04:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02][root][INFO] - Training Epoch: 2/2, step 5948/7134 completed (loss: 0.05385489761829376, acc: 0.9860000014305115)
[2025-02-13 04:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02][root][INFO] - Training Epoch: 2/2, step 5949/7134 completed (loss: 0.010371233336627483, acc: 0.9974905848503113)
[2025-02-13 04:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:03][root][INFO] - Training Epoch: 2/2, step 5950/7134 completed (loss: 0.060386188328266144, acc: 0.980997622013092)
[2025-02-13 04:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:03][root][INFO] - Training Epoch: 2/2, step 5951/7134 completed (loss: 0.04270334914326668, acc: 0.9885222315788269)
[2025-02-13 04:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04][root][INFO] - Training Epoch: 2/2, step 5952/7134 completed (loss: 0.1299774944782257, acc: 0.9750000238418579)
[2025-02-13 04:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04][root][INFO] - Training Epoch: 2/2, step 5953/7134 completed (loss: 0.030971111729741096, acc: 0.994490385055542)
[2025-02-13 04:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04][root][INFO] - Training Epoch: 2/2, step 5954/7134 completed (loss: 0.007337210234254599, acc: 1.0)
[2025-02-13 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:05][root][INFO] - Training Epoch: 2/2, step 5955/7134 completed (loss: 0.02914660982787609, acc: 0.9952380657196045)
[2025-02-13 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:05][root][INFO] - Training Epoch: 2/2, step 5956/7134 completed (loss: 0.0343969352543354, acc: 0.9921568632125854)
[2025-02-13 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06][root][INFO] - Training Epoch: 2/2, step 5957/7134 completed (loss: 0.030868593603372574, acc: 0.9925037622451782)
[2025-02-13 04:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06][root][INFO] - Training Epoch: 2/2, step 5958/7134 completed (loss: 0.025263190269470215, acc: 0.991946280002594)
[2025-02-13 04:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06][root][INFO] - Training Epoch: 2/2, step 5959/7134 completed (loss: 0.03264150395989418, acc: 0.9919893145561218)
[2025-02-13 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07][root][INFO] - Training Epoch: 2/2, step 5960/7134 completed (loss: 0.007624361664056778, acc: 0.9985954761505127)
[2025-02-13 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07][root][INFO] - Training Epoch: 2/2, step 5961/7134 completed (loss: 0.027625393122434616, acc: 0.9918032884597778)
[2025-02-13 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:08][root][INFO] - Training Epoch: 2/2, step 5962/7134 completed (loss: 0.007178624626249075, acc: 0.9983739852905273)
[2025-02-13 04:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:08][root][INFO] - Training Epoch: 2/2, step 5963/7134 completed (loss: 0.01210128702223301, acc: 0.99301677942276)
[2025-02-13 04:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09][root][INFO] - Training Epoch: 2/2, step 5964/7134 completed (loss: 0.016919992864131927, acc: 0.996303141117096)
[2025-02-13 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09][root][INFO] - Training Epoch: 2/2, step 5965/7134 completed (loss: 0.022734548896551132, acc: 0.9940029978752136)
[2025-02-13 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09][root][INFO] - Training Epoch: 2/2, step 5966/7134 completed (loss: 0.030227480456233025, acc: 0.9889042973518372)
[2025-02-13 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10][root][INFO] - Training Epoch: 2/2, step 5967/7134 completed (loss: 0.014805713668465614, acc: 0.9925373196601868)
[2025-02-13 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10][root][INFO] - Training Epoch: 2/2, step 5968/7134 completed (loss: 0.01616569422185421, acc: 0.9931507110595703)
[2025-02-13 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:11][root][INFO] - Training Epoch: 2/2, step 5969/7134 completed (loss: 0.022556636482477188, acc: 0.995356023311615)
[2025-02-13 04:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:11][root][INFO] - Training Epoch: 2/2, step 5970/7134 completed (loss: 0.03139717876911163, acc: 0.993852436542511)
[2025-02-13 04:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12][root][INFO] - Training Epoch: 2/2, step 5971/7134 completed (loss: 0.005242218263447285, acc: 0.9974937438964844)
[2025-02-13 04:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12][root][INFO] - Training Epoch: 2/2, step 5972/7134 completed (loss: 0.04595315828919411, acc: 0.9897210001945496)
[2025-02-13 04:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13][root][INFO] - Training Epoch: 2/2, step 5973/7134 completed (loss: 0.041812408715486526, acc: 0.9884892106056213)
[2025-02-13 04:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13][root][INFO] - Training Epoch: 2/2, step 5974/7134 completed (loss: 0.00611053965985775, acc: 0.9965338110923767)
[2025-02-13 04:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13][root][INFO] - Training Epoch: 2/2, step 5975/7134 completed (loss: 0.00829910859465599, acc: 0.9970501661300659)
[2025-02-13 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:14][root][INFO] - Training Epoch: 2/2, step 5976/7134 completed (loss: 0.011290179565548897, acc: 0.9969325065612793)
[2025-02-13 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:14][root][INFO] - Training Epoch: 2/2, step 5977/7134 completed (loss: 0.012386845424771309, acc: 0.9962825179100037)
[2025-02-13 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15][root][INFO] - Training Epoch: 2/2, step 5978/7134 completed (loss: 0.024079646915197372, acc: 0.9983974099159241)
[2025-02-13 04:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15][root][INFO] - Training Epoch: 2/2, step 5979/7134 completed (loss: 0.033010244369506836, acc: 0.9925925731658936)
[2025-02-13 04:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:16][root][INFO] - Training Epoch: 2/2, step 5980/7134 completed (loss: 0.01999051868915558, acc: 0.9935979247093201)
[2025-02-13 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:16][root][INFO] - Training Epoch: 2/2, step 5981/7134 completed (loss: 0.018445292487740517, acc: 0.9953756928443909)
[2025-02-13 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17][root][INFO] - Training Epoch: 2/2, step 5982/7134 completed (loss: 0.016392430290579796, acc: 0.9960238337516785)
[2025-02-13 04:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17][root][INFO] - Training Epoch: 2/2, step 5983/7134 completed (loss: 0.02216864749789238, acc: 0.9930555820465088)
[2025-02-13 04:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17][root][INFO] - Training Epoch: 2/2, step 5984/7134 completed (loss: 0.010785986669361591, acc: 0.9957447052001953)
[2025-02-13 04:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18][root][INFO] - Training Epoch: 2/2, step 5985/7134 completed (loss: 0.012888616882264614, acc: 0.9974968433380127)
[2025-02-13 04:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18][root][INFO] - Training Epoch: 2/2, step 5986/7134 completed (loss: 0.02156342752277851, acc: 0.9910827875137329)
[2025-02-13 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:19][root][INFO] - Training Epoch: 2/2, step 5987/7134 completed (loss: 0.03760799393057823, acc: 0.9860140085220337)
[2025-02-13 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:19][root][INFO] - Training Epoch: 2/2, step 5988/7134 completed (loss: 0.0022910432890057564, acc: 1.0)
[2025-02-13 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20][root][INFO] - Training Epoch: 2/2, step 5989/7134 completed (loss: 0.021670376881957054, acc: 0.990774929523468)
[2025-02-13 04:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20][root][INFO] - Training Epoch: 2/2, step 5990/7134 completed (loss: 0.01658080704510212, acc: 0.9958904385566711)
[2025-02-13 04:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:21][root][INFO] - Training Epoch: 2/2, step 5991/7134 completed (loss: 0.06621348112821579, acc: 0.9832402467727661)
[2025-02-13 04:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:21][root][INFO] - Training Epoch: 2/2, step 5992/7134 completed (loss: 0.023730164393782616, acc: 0.9883856177330017)
[2025-02-13 04:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:22][root][INFO] - Training Epoch: 2/2, step 5993/7134 completed (loss: 0.02021602727472782, acc: 0.9930651783943176)
[2025-02-13 04:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:22][root][INFO] - Training Epoch: 2/2, step 5994/7134 completed (loss: 0.005217978730797768, acc: 0.9980988502502441)
[2025-02-13 04:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:22][root][INFO] - Training Epoch: 2/2, step 5995/7134 completed (loss: 0.006365537643432617, acc: 0.9985915422439575)
[2025-02-13 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23][root][INFO] - Training Epoch: 2/2, step 5996/7134 completed (loss: 0.022931180894374847, acc: 0.9906103014945984)
[2025-02-13 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23][root][INFO] - Training Epoch: 2/2, step 5997/7134 completed (loss: 0.022125618532299995, acc: 0.9973297715187073)
[2025-02-13 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24][root][INFO] - Training Epoch: 2/2, step 5998/7134 completed (loss: 0.022462157532572746, acc: 0.9887780547142029)
[2025-02-13 04:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24][root][INFO] - Training Epoch: 2/2, step 5999/7134 completed (loss: 0.01757911778986454, acc: 0.9925650358200073)
[2025-02-13 04:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25][root][INFO] - Training Epoch: 2/2, step 6000/7134 completed (loss: 0.018058354035019875, acc: 0.9933444261550903)
[2025-02-13 04:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25][root][INFO] - Training Epoch: 2/2, step 6001/7134 completed (loss: 0.0045927176252007484, acc: 1.0)
[2025-02-13 04:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25][root][INFO] - Training Epoch: 2/2, step 6002/7134 completed (loss: 0.03871629387140274, acc: 0.988304078578949)
[2025-02-13 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26][root][INFO] - Training Epoch: 2/2, step 6003/7134 completed (loss: 0.01749785989522934, acc: 0.9941262602806091)
[2025-02-13 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26][root][INFO] - Training Epoch: 2/2, step 6004/7134 completed (loss: 0.016554949805140495, acc: 0.9958275556564331)
[2025-02-13 04:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:27][root][INFO] - Training Epoch: 2/2, step 6005/7134 completed (loss: 0.021153824403882027, acc: 0.9954338073730469)
[2025-02-13 04:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:27][root][INFO] - Training Epoch: 2/2, step 6006/7134 completed (loss: 0.030689766630530357, acc: 0.990328848361969)
[2025-02-13 04:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28][root][INFO] - Training Epoch: 2/2, step 6007/7134 completed (loss: 0.008628061041235924, acc: 0.9983305335044861)
[2025-02-13 04:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28][root][INFO] - Training Epoch: 2/2, step 6008/7134 completed (loss: 0.018895477056503296, acc: 0.9925816059112549)
[2025-02-13 04:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29][root][INFO] - Training Epoch: 2/2, step 6009/7134 completed (loss: 0.03176432475447655, acc: 0.9906832575798035)
[2025-02-13 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29][root][INFO] - Training Epoch: 2/2, step 6010/7134 completed (loss: 0.045601628720760345, acc: 0.9850948452949524)
[2025-02-13 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29][root][INFO] - Training Epoch: 2/2, step 6011/7134 completed (loss: 0.02531014382839203, acc: 0.9935232996940613)
[2025-02-13 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30][root][INFO] - Training Epoch: 2/2, step 6012/7134 completed (loss: 0.02081741765141487, acc: 0.9940298795700073)
[2025-02-13 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30][root][INFO] - Training Epoch: 2/2, step 6013/7134 completed (loss: 0.04263155907392502, acc: 0.9866310358047485)
[2025-02-13 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:31][root][INFO] - Training Epoch: 2/2, step 6014/7134 completed (loss: 0.00923520140349865, acc: 1.0)
[2025-02-13 04:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:31][root][INFO] - Training Epoch: 2/2, step 6015/7134 completed (loss: 0.04746357351541519, acc: 0.9896694421768188)
[2025-02-13 04:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32][root][INFO] - Training Epoch: 2/2, step 6016/7134 completed (loss: 0.003936552908271551, acc: 1.0)
[2025-02-13 04:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32][root][INFO] - Training Epoch: 2/2, step 6017/7134 completed (loss: 0.020733479410409927, acc: 0.9950494766235352)
[2025-02-13 04:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32][root][INFO] - Training Epoch: 2/2, step 6018/7134 completed (loss: 0.0070868805050849915, acc: 1.0)
[2025-02-13 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33][root][INFO] - Training Epoch: 2/2, step 6019/7134 completed (loss: 0.010019687935709953, acc: 1.0)
[2025-02-13 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33][root][INFO] - Training Epoch: 2/2, step 6020/7134 completed (loss: 0.0017339495243504643, acc: 1.0)
[2025-02-13 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34][root][INFO] - Training Epoch: 2/2, step 6021/7134 completed (loss: 0.003313243854790926, acc: 1.0)
[2025-02-13 04:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34][root][INFO] - Training Epoch: 2/2, step 6022/7134 completed (loss: 0.012797290459275246, acc: 0.9926199316978455)
[2025-02-13 04:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34][root][INFO] - Training Epoch: 2/2, step 6023/7134 completed (loss: 0.012396851554512978, acc: 0.9945799708366394)
[2025-02-13 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35][root][INFO] - Training Epoch: 2/2, step 6024/7134 completed (loss: 0.0022102533839643, acc: 1.0)
[2025-02-13 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35][root][INFO] - Training Epoch: 2/2, step 6025/7134 completed (loss: 0.007675452623516321, acc: 0.9979079365730286)
[2025-02-13 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36][root][INFO] - Training Epoch: 2/2, step 6026/7134 completed (loss: 0.07115015387535095, acc: 0.9942857027053833)
[2025-02-13 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36][root][INFO] - Training Epoch: 2/2, step 6027/7134 completed (loss: 0.004159694071859121, acc: 0.9974293112754822)
[2025-02-13 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36][root][INFO] - Training Epoch: 2/2, step 6028/7134 completed (loss: 0.0036521076690405607, acc: 1.0)
[2025-02-13 04:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:37][root][INFO] - Training Epoch: 2/2, step 6029/7134 completed (loss: 0.007362989708781242, acc: 0.9963898658752441)
[2025-02-13 04:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:37][root][INFO] - Training Epoch: 2/2, step 6030/7134 completed (loss: 0.010659760795533657, acc: 0.9961685538291931)
[2025-02-13 04:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38][root][INFO] - Training Epoch: 2/2, step 6031/7134 completed (loss: 0.01377785298973322, acc: 0.9941520690917969)
[2025-02-13 04:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38][root][INFO] - Training Epoch: 2/2, step 6032/7134 completed (loss: 0.016238488256931305, acc: 0.9974683523178101)
[2025-02-13 04:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:39][root][INFO] - Training Epoch: 2/2, step 6033/7134 completed (loss: 0.042975690215826035, acc: 0.9877049326896667)
[2025-02-13 04:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:39][root][INFO] - Training Epoch: 2/2, step 6034/7134 completed (loss: 0.00957304798066616, acc: 1.0)
[2025-02-13 04:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:39][root][INFO] - Training Epoch: 2/2, step 6035/7134 completed (loss: 0.0607333742082119, acc: 0.9812865257263184)
[2025-02-13 04:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40][root][INFO] - Training Epoch: 2/2, step 6036/7134 completed (loss: 0.04499902203679085, acc: 0.9916083812713623)
[2025-02-13 04:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40][root][INFO] - Training Epoch: 2/2, step 6037/7134 completed (loss: 0.01849026419222355, acc: 0.993220329284668)
[2025-02-13 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41][root][INFO] - Training Epoch: 2/2, step 6038/7134 completed (loss: 0.021754750981926918, acc: 0.9955257177352905)
[2025-02-13 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41][root][INFO] - Training Epoch: 2/2, step 6039/7134 completed (loss: 0.015346124768257141, acc: 0.9952550530433655)
[2025-02-13 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:42][root][INFO] - Training Epoch: 2/2, step 6040/7134 completed (loss: 0.03959722816944122, acc: 0.9906445145606995)
[2025-02-13 04:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:42][root][INFO] - Training Epoch: 2/2, step 6041/7134 completed (loss: 0.021922171115875244, acc: 0.993446946144104)
[2025-02-13 04:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43][root][INFO] - Training Epoch: 2/2, step 6042/7134 completed (loss: 0.012812855653464794, acc: 0.996999979019165)
[2025-02-13 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43][root][INFO] - Training Epoch: 2/2, step 6043/7134 completed (loss: 0.0206595566123724, acc: 0.9952038526535034)
[2025-02-13 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44][root][INFO] - Training Epoch: 2/2, step 6044/7134 completed (loss: 0.029935598373413086, acc: 0.990123450756073)
[2025-02-13 04:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44][root][INFO] - Training Epoch: 2/2, step 6045/7134 completed (loss: 0.023873494938015938, acc: 0.9905325174331665)
[2025-02-13 04:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45][root][INFO] - Training Epoch: 2/2, step 6046/7134 completed (loss: 0.035682179033756256, acc: 0.9930675625801086)
[2025-02-13 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45][root][INFO] - Training Epoch: 2/2, step 6047/7134 completed (loss: 0.08037997782230377, acc: 0.979763925075531)
[2025-02-13 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45][root][INFO] - Training Epoch: 2/2, step 6048/7134 completed (loss: 0.035574957728385925, acc: 0.9899799823760986)
[2025-02-13 04:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:46][root][INFO] - Training Epoch: 2/2, step 6049/7134 completed (loss: 0.010690106078982353, acc: 0.9982935190200806)
[2025-02-13 04:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:46][root][INFO] - Training Epoch: 2/2, step 6050/7134 completed (loss: 0.044555746018886566, acc: 0.9876881241798401)
[2025-02-13 04:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47][root][INFO] - Training Epoch: 2/2, step 6051/7134 completed (loss: 0.062105823308229446, acc: 0.9807976484298706)
[2025-02-13 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47][root][INFO] - Training Epoch: 2/2, step 6052/7134 completed (loss: 0.05591980367898941, acc: 0.9858712553977966)
[2025-02-13 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48][root][INFO] - Training Epoch: 2/2, step 6053/7134 completed (loss: 0.046831585466861725, acc: 0.984649121761322)
[2025-02-13 04:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48][root][INFO] - Training Epoch: 2/2, step 6054/7134 completed (loss: 0.03118150867521763, acc: 0.9915459156036377)
[2025-02-13 04:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49][root][INFO] - Training Epoch: 2/2, step 6055/7134 completed (loss: 0.023745110258460045, acc: 0.9936808943748474)
[2025-02-13 04:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49][root][INFO] - Training Epoch: 2/2, step 6056/7134 completed (loss: 0.020877134054899216, acc: 0.9930875301361084)
[2025-02-13 04:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:50][root][INFO] - Training Epoch: 2/2, step 6057/7134 completed (loss: 0.015416046604514122, acc: 0.9934210777282715)
[2025-02-13 04:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:50][root][INFO] - Training Epoch: 2/2, step 6058/7134 completed (loss: 0.011133079417049885, acc: 0.9954954981803894)
[2025-02-13 04:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:50][root][INFO] - Training Epoch: 2/2, step 6059/7134 completed (loss: 0.02112487517297268, acc: 0.9929328560829163)
[2025-02-13 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51][root][INFO] - Training Epoch: 2/2, step 6060/7134 completed (loss: 0.01689997874200344, acc: 0.9924812316894531)
[2025-02-13 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51][root][INFO] - Training Epoch: 2/2, step 6061/7134 completed (loss: 0.03702519088983536, acc: 0.9892141819000244)
[2025-02-13 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:52][root][INFO] - Training Epoch: 2/2, step 6062/7134 completed (loss: 0.04997725412249565, acc: 0.9832041263580322)
[2025-02-13 04:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:52][root][INFO] - Training Epoch: 2/2, step 6063/7134 completed (loss: 0.04576271399855614, acc: 0.9909583926200867)
[2025-02-13 04:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53][root][INFO] - Training Epoch: 2/2, step 6064/7134 completed (loss: 0.020618604496121407, acc: 0.9942113161087036)
[2025-02-13 04:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53][root][INFO] - Training Epoch: 2/2, step 6065/7134 completed (loss: 0.03287981078028679, acc: 0.9914945363998413)
[2025-02-13 04:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54][root][INFO] - Training Epoch: 2/2, step 6066/7134 completed (loss: 0.054983846843242645, acc: 0.9785832166671753)
[2025-02-13 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54][root][INFO] - Training Epoch: 2/2, step 6067/7134 completed (loss: 0.0518002025783062, acc: 0.9797979593276978)
[2025-02-13 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54][root][INFO] - Training Epoch: 2/2, step 6068/7134 completed (loss: 0.07187250256538391, acc: 0.9824780821800232)
[2025-02-13 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55][root][INFO] - Training Epoch: 2/2, step 6069/7134 completed (loss: 0.04666592553257942, acc: 0.985401451587677)
[2025-02-13 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55][root][INFO] - Training Epoch: 2/2, step 6070/7134 completed (loss: 0.04964735358953476, acc: 0.9826897382736206)
[2025-02-13 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:56][root][INFO] - Training Epoch: 2/2, step 6071/7134 completed (loss: 0.044102270156145096, acc: 0.9865410327911377)
[2025-02-13 04:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:56][root][INFO] - Training Epoch: 2/2, step 6072/7134 completed (loss: 0.09124832600355148, acc: 0.9768907427787781)
[2025-02-13 04:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57][root][INFO] - Training Epoch: 2/2, step 6073/7134 completed (loss: 0.031801316887140274, acc: 0.9902098178863525)
[2025-02-13 04:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57][root][INFO] - Training Epoch: 2/2, step 6074/7134 completed (loss: 0.04791786149144173, acc: 0.9897058606147766)
[2025-02-13 04:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57][root][INFO] - Training Epoch: 2/2, step 6075/7134 completed (loss: 0.03650158271193504, acc: 0.9916067123413086)
[2025-02-13 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58][root][INFO] - Training Epoch: 2/2, step 6076/7134 completed (loss: 0.05997265502810478, acc: 0.9784482717514038)
[2025-02-13 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58][root][INFO] - Training Epoch: 2/2, step 6077/7134 completed (loss: 0.04328668490052223, acc: 0.9888178706169128)
[2025-02-13 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59][root][INFO] - Training Epoch: 2/2, step 6078/7134 completed (loss: 0.04185411334037781, acc: 0.9901408553123474)
[2025-02-13 04:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59][root][INFO] - Training Epoch: 2/2, step 6079/7134 completed (loss: 0.019082989543676376, acc: 0.9927007555961609)
[2025-02-13 04:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:00][root][INFO] - Training Epoch: 2/2, step 6080/7134 completed (loss: 0.024181852117180824, acc: 0.9929278492927551)
[2025-02-13 04:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:00][root][INFO] - Training Epoch: 2/2, step 6081/7134 completed (loss: 0.025301335379481316, acc: 0.9923664331436157)
[2025-02-13 04:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01][root][INFO] - Training Epoch: 2/2, step 6082/7134 completed (loss: 0.018437497317790985, acc: 0.993630588054657)
[2025-02-13 04:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01][root][INFO] - Training Epoch: 2/2, step 6083/7134 completed (loss: 0.008344190195202827, acc: 0.996314525604248)
[2025-02-13 04:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01][root][INFO] - Training Epoch: 2/2, step 6084/7134 completed (loss: 0.027185866609215736, acc: 0.9952830076217651)
[2025-02-13 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02][root][INFO] - Training Epoch: 2/2, step 6085/7134 completed (loss: 0.019215527921915054, acc: 0.9928443431854248)
[2025-02-13 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02][root][INFO] - Training Epoch: 2/2, step 6086/7134 completed (loss: 0.0046885451301932335, acc: 0.9985954761505127)
[2025-02-13 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03][root][INFO] - Training Epoch: 2/2, step 6087/7134 completed (loss: 0.0411774106323719, acc: 0.9874213933944702)
[2025-02-13 04:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03][root][INFO] - Training Epoch: 2/2, step 6088/7134 completed (loss: 0.01861432194709778, acc: 0.9946380853652954)
[2025-02-13 04:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:04][root][INFO] - Training Epoch: 2/2, step 6089/7134 completed (loss: 0.01874336041510105, acc: 0.9931034445762634)
[2025-02-13 04:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:04][root][INFO] - Training Epoch: 2/2, step 6090/7134 completed (loss: 0.013544739224016666, acc: 0.9958847761154175)
[2025-02-13 04:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05][root][INFO] - Training Epoch: 2/2, step 6091/7134 completed (loss: 0.020204149186611176, acc: 0.993630588054657)
[2025-02-13 04:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05][root][INFO] - Training Epoch: 2/2, step 6092/7134 completed (loss: 0.01449529267847538, acc: 0.9984126687049866)
[2025-02-13 04:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05][root][INFO] - Training Epoch: 2/2, step 6093/7134 completed (loss: 0.04078910872340202, acc: 0.9907235503196716)
[2025-02-13 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06][root][INFO] - Training Epoch: 2/2, step 6094/7134 completed (loss: 0.027101190760731697, acc: 0.9901960492134094)
[2025-02-13 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06][root][INFO] - Training Epoch: 2/2, step 6095/7134 completed (loss: 0.023163288831710815, acc: 0.9913793206214905)
[2025-02-13 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:07][root][INFO] - Training Epoch: 2/2, step 6096/7134 completed (loss: 0.03207054361701012, acc: 0.9892601370811462)
[2025-02-13 04:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:07][root][INFO] - Training Epoch: 2/2, step 6097/7134 completed (loss: 0.02296806499361992, acc: 0.9916897416114807)
[2025-02-13 04:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08][root][INFO] - Training Epoch: 2/2, step 6098/7134 completed (loss: 0.02122350037097931, acc: 0.992277979850769)
[2025-02-13 04:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08][root][INFO] - Training Epoch: 2/2, step 6099/7134 completed (loss: 0.050922494381666183, acc: 0.9892966151237488)
[2025-02-13 04:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09][root][INFO] - Training Epoch: 2/2, step 6100/7134 completed (loss: 0.03639794513583183, acc: 0.9908536672592163)
[2025-02-13 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09][root][INFO] - Training Epoch: 2/2, step 6101/7134 completed (loss: 0.007672886364161968, acc: 0.9968051314353943)
[2025-02-13 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09][root][INFO] - Training Epoch: 2/2, step 6102/7134 completed (loss: 0.024349067360162735, acc: 0.9947183132171631)
[2025-02-13 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:10][root][INFO] - Training Epoch: 2/2, step 6103/7134 completed (loss: 0.01619831472635269, acc: 0.9925925731658936)
[2025-02-13 04:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:10][root][INFO] - Training Epoch: 2/2, step 6104/7134 completed (loss: 0.04884248599410057, acc: 0.9822221994400024)
[2025-02-13 04:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11][root][INFO] - Training Epoch: 2/2, step 6105/7134 completed (loss: 0.046978216618299484, acc: 0.988095223903656)
[2025-02-13 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11][root][INFO] - Training Epoch: 2/2, step 6106/7134 completed (loss: 0.026309749111533165, acc: 0.9874776601791382)
[2025-02-13 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12][root][INFO] - Training Epoch: 2/2, step 6107/7134 completed (loss: 0.043685346841812134, acc: 0.9873949289321899)
[2025-02-13 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12][root][INFO] - Training Epoch: 2/2, step 6108/7134 completed (loss: 0.04190855473279953, acc: 0.9890710115432739)
[2025-02-13 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12][root][INFO] - Training Epoch: 2/2, step 6109/7134 completed (loss: 0.01748322881758213, acc: 0.991525411605835)
[2025-02-13 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13][root][INFO] - Training Epoch: 2/2, step 6110/7134 completed (loss: 0.06095718592405319, acc: 0.9743589758872986)
[2025-02-13 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13][root][INFO] - Training Epoch: 2/2, step 6111/7134 completed (loss: 0.038989584892988205, acc: 0.9865951538085938)
[2025-02-13 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:14][root][INFO] - Training Epoch: 2/2, step 6112/7134 completed (loss: 0.01964433304965496, acc: 0.9949832558631897)
[2025-02-13 04:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:14][root][INFO] - Training Epoch: 2/2, step 6113/7134 completed (loss: 0.013666764833033085, acc: 0.9954476356506348)
[2025-02-13 04:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:14][root][INFO] - Training Epoch: 2/2, step 6114/7134 completed (loss: 0.05321332439780235, acc: 0.9900826215744019)
[2025-02-13 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15][root][INFO] - Training Epoch: 2/2, step 6115/7134 completed (loss: 0.023323871195316315, acc: 0.99042147397995)
[2025-02-13 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15][root][INFO] - Training Epoch: 2/2, step 6116/7134 completed (loss: 0.04168090969324112, acc: 0.992094874382019)
[2025-02-13 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16][root][INFO] - Training Epoch: 2/2, step 6117/7134 completed (loss: 0.05925282835960388, acc: 0.9849726557731628)
[2025-02-13 04:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16][root][INFO] - Training Epoch: 2/2, step 6118/7134 completed (loss: 0.03293418511748314, acc: 0.9944853186607361)
[2025-02-13 04:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:17][root][INFO] - Training Epoch: 2/2, step 6119/7134 completed (loss: 0.028142990544438362, acc: 0.9927971363067627)
[2025-02-13 04:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:17][root][INFO] - Training Epoch: 2/2, step 6120/7134 completed (loss: 0.029193853959441185, acc: 0.9946332573890686)
[2025-02-13 04:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18][root][INFO] - Training Epoch: 2/2, step 6121/7134 completed (loss: 0.023097699508070946, acc: 0.9928057789802551)
[2025-02-13 04:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18][root][INFO] - Training Epoch: 2/2, step 6122/7134 completed (loss: 0.021499289199709892, acc: 0.994339644908905)
[2025-02-13 04:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18][root][INFO] - Training Epoch: 2/2, step 6123/7134 completed (loss: 0.05917789787054062, acc: 0.9854469895362854)
[2025-02-13 04:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19][root][INFO] - Training Epoch: 2/2, step 6124/7134 completed (loss: 0.012587600387632847, acc: 0.9923076629638672)
[2025-02-13 04:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19][root][INFO] - Training Epoch: 2/2, step 6125/7134 completed (loss: 0.09062516689300537, acc: 0.9759036302566528)
[2025-02-13 04:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20][root][INFO] - Training Epoch: 2/2, step 6126/7134 completed (loss: 0.06429412215948105, acc: 0.9843260049819946)
[2025-02-13 04:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20][root][INFO] - Training Epoch: 2/2, step 6127/7134 completed (loss: 0.01876545324921608, acc: 0.9904761910438538)
[2025-02-13 04:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21][root][INFO] - Training Epoch: 2/2, step 6128/7134 completed (loss: 0.018318545073270798, acc: 0.9959514141082764)
[2025-02-13 04:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21][root][INFO] - Training Epoch: 2/2, step 6129/7134 completed (loss: 0.023490307852625847, acc: 0.9929676651954651)
[2025-02-13 04:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21][root][INFO] - Training Epoch: 2/2, step 6130/7134 completed (loss: 0.018892807886004448, acc: 0.9946236610412598)
[2025-02-13 04:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:22][root][INFO] - Training Epoch: 2/2, step 6131/7134 completed (loss: 0.0365605354309082, acc: 0.9880239367485046)
[2025-02-13 04:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:22][root][INFO] - Training Epoch: 2/2, step 6132/7134 completed (loss: 0.0272621251642704, acc: 0.9906250238418579)
[2025-02-13 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23][root][INFO] - Training Epoch: 2/2, step 6133/7134 completed (loss: 0.03921079635620117, acc: 0.9890776872634888)
[2025-02-13 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23][root][INFO] - Training Epoch: 2/2, step 6134/7134 completed (loss: 0.06755220144987106, acc: 0.9800918698310852)
[2025-02-13 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:24][root][INFO] - Training Epoch: 2/2, step 6135/7134 completed (loss: 0.011549044400453568, acc: 0.9982578158378601)
[2025-02-13 04:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:24][root][INFO] - Training Epoch: 2/2, step 6136/7134 completed (loss: 0.047373346984386444, acc: 0.9929971694946289)
[2025-02-13 04:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25][root][INFO] - Training Epoch: 2/2, step 6137/7134 completed (loss: 0.05241190642118454, acc: 0.9921996593475342)
[2025-02-13 04:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25][root][INFO] - Training Epoch: 2/2, step 6138/7134 completed (loss: 0.03672486171126366, acc: 0.9877111911773682)
[2025-02-13 04:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25][root][INFO] - Training Epoch: 2/2, step 6139/7134 completed (loss: 0.04135973006486893, acc: 0.9870370626449585)
[2025-02-13 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26][root][INFO] - Training Epoch: 2/2, step 6140/7134 completed (loss: 0.026920722797513008, acc: 0.9964664578437805)
[2025-02-13 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26][root][INFO] - Training Epoch: 2/2, step 6141/7134 completed (loss: 0.048881374299526215, acc: 0.9908257126808167)
[2025-02-13 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27][root][INFO] - Training Epoch: 2/2, step 6142/7134 completed (loss: 0.02875020168721676, acc: 0.9913669228553772)
[2025-02-13 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27][root][INFO] - Training Epoch: 2/2, step 6143/7134 completed (loss: 0.021945590153336525, acc: 0.9945945739746094)
[2025-02-13 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28][root][INFO] - Training Epoch: 2/2, step 6144/7134 completed (loss: 0.024354105815291405, acc: 0.9915966391563416)
[2025-02-13 04:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28][root][INFO] - Training Epoch: 2/2, step 6145/7134 completed (loss: 0.04351404681801796, acc: 0.9907529950141907)
[2025-02-13 04:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29][root][INFO] - Training Epoch: 2/2, step 6146/7134 completed (loss: 0.04115275666117668, acc: 0.9879336357116699)
[2025-02-13 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29][root][INFO] - Training Epoch: 2/2, step 6147/7134 completed (loss: 0.038592517375946045, acc: 0.9861111044883728)
[2025-02-13 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29][root][INFO] - Training Epoch: 2/2, step 6148/7134 completed (loss: 0.04041609540581703, acc: 0.9849246144294739)
[2025-02-13 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30][root][INFO] - Training Epoch: 2/2, step 6149/7134 completed (loss: 0.03541870042681694, acc: 0.9887459874153137)
[2025-02-13 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30][root][INFO] - Training Epoch: 2/2, step 6150/7134 completed (loss: 0.017575114965438843, acc: 0.9947368502616882)
[2025-02-13 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:31][root][INFO] - Training Epoch: 2/2, step 6151/7134 completed (loss: 0.022306794300675392, acc: 0.9937205910682678)
[2025-02-13 04:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:31][root][INFO] - Training Epoch: 2/2, step 6152/7134 completed (loss: 0.030101384967565536, acc: 0.9894419312477112)
[2025-02-13 04:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32][root][INFO] - Training Epoch: 2/2, step 6153/7134 completed (loss: 0.014214002527296543, acc: 0.9971056580543518)
[2025-02-13 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32][root][INFO] - Training Epoch: 2/2, step 6154/7134 completed (loss: 0.03522130101919174, acc: 0.9902371168136597)
[2025-02-13 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32][root][INFO] - Training Epoch: 2/2, step 6155/7134 completed (loss: 0.024112483486533165, acc: 0.9940652847290039)
[2025-02-13 04:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33][root][INFO] - Training Epoch: 2/2, step 6156/7134 completed (loss: 0.014520741999149323, acc: 0.9936608672142029)
[2025-02-13 04:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33][root][INFO] - Training Epoch: 2/2, step 6157/7134 completed (loss: 0.015932327136397362, acc: 0.9968652129173279)
[2025-02-13 04:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34][root][INFO] - Training Epoch: 2/2, step 6158/7134 completed (loss: 0.052081819623708725, acc: 0.9891501069068909)
[2025-02-13 04:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34][root][INFO] - Training Epoch: 2/2, step 6159/7134 completed (loss: 0.03185423091053963, acc: 0.9903498291969299)
[2025-02-13 04:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35][root][INFO] - Training Epoch: 2/2, step 6160/7134 completed (loss: 0.055497922003269196, acc: 0.9863636493682861)
[2025-02-13 04:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35][root][INFO] - Training Epoch: 2/2, step 6161/7134 completed (loss: 0.04728887975215912, acc: 0.9876977205276489)
[2025-02-13 04:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35][root][INFO] - Training Epoch: 2/2, step 6162/7134 completed (loss: 0.03888962045311928, acc: 0.9901800155639648)
[2025-02-13 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36][root][INFO] - Training Epoch: 2/2, step 6163/7134 completed (loss: 0.07472715526819229, acc: 0.9821428656578064)
[2025-02-13 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36][root][INFO] - Training Epoch: 2/2, step 6164/7134 completed (loss: 0.0596345029771328, acc: 0.9815863966941833)
[2025-02-13 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37][root][INFO] - Training Epoch: 2/2, step 6165/7134 completed (loss: 0.03670723736286163, acc: 0.9852398633956909)
[2025-02-13 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37][root][INFO] - Training Epoch: 2/2, step 6166/7134 completed (loss: 0.027654705569148064, acc: 0.9964850544929504)
[2025-02-13 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:38][root][INFO] - Training Epoch: 2/2, step 6167/7134 completed (loss: 0.026786087080836296, acc: 0.9929278492927551)
[2025-02-13 04:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:38][root][INFO] - Training Epoch: 2/2, step 6168/7134 completed (loss: 0.026034066453576088, acc: 0.992277979850769)
[2025-02-13 04:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:38][root][INFO] - Training Epoch: 2/2, step 6169/7134 completed (loss: 0.03328557312488556, acc: 0.9897959232330322)
[2025-02-13 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39][root][INFO] - Training Epoch: 2/2, step 6170/7134 completed (loss: 0.0191644299775362, acc: 0.9943289160728455)
[2025-02-13 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39][root][INFO] - Training Epoch: 2/2, step 6171/7134 completed (loss: 0.010629073716700077, acc: 0.9947437644004822)
[2025-02-13 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40][root][INFO] - Training Epoch: 2/2, step 6172/7134 completed (loss: 0.01804329827427864, acc: 0.9959920048713684)
[2025-02-13 04:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40][root][INFO] - Training Epoch: 2/2, step 6173/7134 completed (loss: 0.006645603571087122, acc: 0.9985272288322449)
[2025-02-13 04:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40][root][INFO] - Training Epoch: 2/2, step 6174/7134 completed (loss: 0.003544972511008382, acc: 0.9985337257385254)
[2025-02-13 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:41][root][INFO] - Training Epoch: 2/2, step 6175/7134 completed (loss: 0.02043973281979561, acc: 0.9955621361732483)
[2025-02-13 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:41][root][INFO] - Training Epoch: 2/2, step 6176/7134 completed (loss: 0.02172563597559929, acc: 0.9898697733879089)
[2025-02-13 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42][root][INFO] - Training Epoch: 2/2, step 6177/7134 completed (loss: 0.01507094968110323, acc: 0.9961636662483215)
[2025-02-13 04:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42][root][INFO] - Training Epoch: 2/2, step 6178/7134 completed (loss: 0.010321656242012978, acc: 0.9970501661300659)
[2025-02-13 04:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43][root][INFO] - Training Epoch: 2/2, step 6179/7134 completed (loss: 0.03272091597318649, acc: 0.9953415989875793)
[2025-02-13 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43][root][INFO] - Training Epoch: 2/2, step 6180/7134 completed (loss: 0.08376601338386536, acc: 0.9713375568389893)
[2025-02-13 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43][root][INFO] - Training Epoch: 2/2, step 6181/7134 completed (loss: 0.1498851329088211, acc: 0.9545454382896423)
[2025-02-13 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:44][root][INFO] - Training Epoch: 2/2, step 6182/7134 completed (loss: 0.03366388753056526, acc: 0.9870129823684692)
[2025-02-13 04:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:44][root][INFO] - Training Epoch: 2/2, step 6183/7134 completed (loss: 0.04372428357601166, acc: 0.9846860766410828)
[2025-02-13 04:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45][root][INFO] - Training Epoch: 2/2, step 6184/7134 completed (loss: 0.08319350332021713, acc: 0.9778226017951965)
[2025-02-13 04:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45][root][INFO] - Training Epoch: 2/2, step 6185/7134 completed (loss: 0.019523220136761665, acc: 0.9963235259056091)
[2025-02-13 04:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45][root][INFO] - Training Epoch: 2/2, step 6186/7134 completed (loss: 0.031869757920503616, acc: 0.9895104765892029)
[2025-02-13 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:46][root][INFO] - Training Epoch: 2/2, step 6187/7134 completed (loss: 0.021135084331035614, acc: 0.9933862686157227)
[2025-02-13 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:46][root][INFO] - Training Epoch: 2/2, step 6188/7134 completed (loss: 0.03964097052812576, acc: 0.9901269674301147)
[2025-02-13 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47][root][INFO] - Training Epoch: 2/2, step 6189/7134 completed (loss: 0.04638758674263954, acc: 0.9884560108184814)
[2025-02-13 04:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47][root][INFO] - Training Epoch: 2/2, step 6190/7134 completed (loss: 0.04468914121389389, acc: 0.9900332093238831)
[2025-02-13 04:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48][root][INFO] - Training Epoch: 2/2, step 6191/7134 completed (loss: 0.031003355979919434, acc: 0.9930070042610168)
[2025-02-13 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48][root][INFO] - Training Epoch: 2/2, step 6192/7134 completed (loss: 0.01615193672478199, acc: 0.9957127571105957)
[2025-02-13 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49][root][INFO] - Training Epoch: 2/2, step 6193/7134 completed (loss: 0.031346745789051056, acc: 0.9916765689849854)
[2025-02-13 04:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49][root][INFO] - Training Epoch: 2/2, step 6194/7134 completed (loss: 0.020351573824882507, acc: 0.994163453578949)
[2025-02-13 04:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49][root][INFO] - Training Epoch: 2/2, step 6195/7134 completed (loss: 0.015192781575024128, acc: 0.9952152967453003)
[2025-02-13 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50][root][INFO] - Training Epoch: 2/2, step 6196/7134 completed (loss: 0.01257147453725338, acc: 0.994490385055542)
[2025-02-13 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50][root][INFO] - Training Epoch: 2/2, step 6197/7134 completed (loss: 0.04130157455801964, acc: 0.9931034445762634)
[2025-02-13 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51][root][INFO] - Training Epoch: 2/2, step 6198/7134 completed (loss: 0.020629145205020905, acc: 0.9917012453079224)
[2025-02-13 04:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51][root][INFO] - Training Epoch: 2/2, step 6199/7134 completed (loss: 0.03885567933320999, acc: 0.9907192587852478)
[2025-02-13 04:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:52][root][INFO] - Training Epoch: 2/2, step 6200/7134 completed (loss: 0.01492343284189701, acc: 0.997560977935791)
[2025-02-13 04:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:52][root][INFO] - Training Epoch: 2/2, step 6201/7134 completed (loss: 0.012267847545444965, acc: 0.9966555237770081)
[2025-02-13 04:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:53][root][INFO] - Training Epoch: 2/2, step 6202/7134 completed (loss: 0.033586226403713226, acc: 0.9913232326507568)
[2025-02-13 04:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:53][root][INFO] - Training Epoch: 2/2, step 6203/7134 completed (loss: 0.013213137164711952, acc: 0.9961734414100647)
[2025-02-13 04:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:53][root][INFO] - Training Epoch: 2/2, step 6204/7134 completed (loss: 0.013519917614758015, acc: 0.9980353713035583)
[2025-02-13 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54][root][INFO] - Training Epoch: 2/2, step 6205/7134 completed (loss: 0.01690468192100525, acc: 0.9914675951004028)
[2025-02-13 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54][root][INFO] - Training Epoch: 2/2, step 6206/7134 completed (loss: 0.016182206571102142, acc: 0.9956584572792053)
[2025-02-13 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55][root][INFO] - Training Epoch: 2/2, step 6207/7134 completed (loss: 0.10159915685653687, acc: 0.9746328592300415)
[2025-02-13 04:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55][root][INFO] - Training Epoch: 2/2, step 6208/7134 completed (loss: 0.028013987466692924, acc: 0.9907407164573669)
[2025-02-13 04:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:56][root][INFO] - Training Epoch: 2/2, step 6209/7134 completed (loss: 0.03236798197031021, acc: 0.9935317039489746)
[2025-02-13 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:56][root][INFO] - Training Epoch: 2/2, step 6210/7134 completed (loss: 0.021005403250455856, acc: 0.9901098608970642)
[2025-02-13 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57][root][INFO] - Training Epoch: 2/2, step 6211/7134 completed (loss: 0.039090365171432495, acc: 0.9864681959152222)
[2025-02-13 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57][root][INFO] - Training Epoch: 2/2, step 6212/7134 completed (loss: 0.010909859091043472, acc: 0.9964912533760071)
[2025-02-13 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57][root][INFO] - Training Epoch: 2/2, step 6213/7134 completed (loss: 0.0033268213737756014, acc: 1.0)
[2025-02-13 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58][root][INFO] - Training Epoch: 2/2, step 6214/7134 completed (loss: 0.009324533864855766, acc: 0.9974325895309448)
[2025-02-13 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58][root][INFO] - Training Epoch: 2/2, step 6215/7134 completed (loss: 0.0175627488642931, acc: 0.9965437650680542)
[2025-02-13 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59][root][INFO] - Training Epoch: 2/2, step 6216/7134 completed (loss: 0.010529880411922932, acc: 0.9978723526000977)
[2025-02-13 04:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59][root][INFO] - Training Epoch: 2/2, step 6217/7134 completed (loss: 0.013924605213105679, acc: 0.9973154067993164)
[2025-02-13 04:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59][root][INFO] - Training Epoch: 2/2, step 6218/7134 completed (loss: 0.03503287583589554, acc: 0.9923780560493469)
[2025-02-13 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00][root][INFO] - Training Epoch: 2/2, step 6219/7134 completed (loss: 0.023845050483942032, acc: 0.9915611743927002)
[2025-02-13 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00][root][INFO] - Training Epoch: 2/2, step 6220/7134 completed (loss: 0.02688267081975937, acc: 0.9857142567634583)
[2025-02-13 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:01][root][INFO] - Training Epoch: 2/2, step 6221/7134 completed (loss: 0.005847802851349115, acc: 0.9982455968856812)
[2025-02-13 04:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:01][root][INFO] - Training Epoch: 2/2, step 6222/7134 completed (loss: 0.043589185923337936, acc: 0.9918032884597778)
[2025-02-13 04:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:02][root][INFO] - Training Epoch: 2/2, step 6223/7134 completed (loss: 0.032632384449243546, acc: 0.9890109896659851)
[2025-02-13 04:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:02][root][INFO] - Training Epoch: 2/2, step 6224/7134 completed (loss: 0.0029498585499823093, acc: 1.0)
[2025-02-13 04:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:02][root][INFO] - Training Epoch: 2/2, step 6225/7134 completed (loss: 0.03043351322412491, acc: 0.9923469424247742)
[2025-02-13 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03][root][INFO] - Training Epoch: 2/2, step 6226/7134 completed (loss: 0.026189543306827545, acc: 0.9908257126808167)
[2025-02-13 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03][root][INFO] - Training Epoch: 2/2, step 6227/7134 completed (loss: 0.003631517756730318, acc: 1.0)
[2025-02-13 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:04][root][INFO] - Training Epoch: 2/2, step 6228/7134 completed (loss: 0.020939338952302933, acc: 0.9901719689369202)
[2025-02-13 04:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:04][root][INFO] - Training Epoch: 2/2, step 6229/7134 completed (loss: 0.04469458758831024, acc: 0.9872390031814575)
[2025-02-13 04:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05][root][INFO] - Training Epoch: 2/2, step 6230/7134 completed (loss: 0.021649470552802086, acc: 0.9937655925750732)
[2025-02-13 04:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05][root][INFO] - Training Epoch: 2/2, step 6231/7134 completed (loss: 0.008697889745235443, acc: 0.9974160194396973)
[2025-02-13 04:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05][root][INFO] - Training Epoch: 2/2, step 6232/7134 completed (loss: 0.027951842173933983, acc: 0.993686854839325)
[2025-02-13 04:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:06][root][INFO] - Training Epoch: 2/2, step 6233/7134 completed (loss: 0.022043079137802124, acc: 0.9932975769042969)
[2025-02-13 04:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:06][root][INFO] - Training Epoch: 2/2, step 6234/7134 completed (loss: 0.04661470651626587, acc: 0.9852941036224365)
[2025-02-13 04:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:07][root][INFO] - Training Epoch: 2/2, step 6235/7134 completed (loss: 0.018738478422164917, acc: 0.9960886836051941)
[2025-02-13 04:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:07][root][INFO] - Training Epoch: 2/2, step 6236/7134 completed (loss: 0.013491473160684109, acc: 0.9959072470664978)
[2025-02-13 04:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:08][root][INFO] - Training Epoch: 2/2, step 6237/7134 completed (loss: 0.024842824786901474, acc: 0.9892966151237488)
[2025-02-13 04:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:08][root][INFO] - Training Epoch: 2/2, step 6238/7134 completed (loss: 0.008824740536510944, acc: 0.9967690110206604)
[2025-02-13 04:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09][root][INFO] - Training Epoch: 2/2, step 6239/7134 completed (loss: 0.03984459489583969, acc: 0.9928315281867981)
[2025-02-13 04:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09][root][INFO] - Training Epoch: 2/2, step 6240/7134 completed (loss: 0.0031486600637435913, acc: 0.9985895752906799)
[2025-02-13 04:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09][root][INFO] - Training Epoch: 2/2, step 6241/7134 completed (loss: 0.005226926412433386, acc: 1.0)
[2025-02-13 04:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:10][root][INFO] - Training Epoch: 2/2, step 6242/7134 completed (loss: 0.010494660586118698, acc: 0.9982993006706238)
[2025-02-13 04:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:10][root][INFO] - Training Epoch: 2/2, step 6243/7134 completed (loss: 0.015846576541662216, acc: 0.9952229261398315)
[2025-02-13 04:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:11][root][INFO] - Training Epoch: 2/2, step 6244/7134 completed (loss: 0.01586831733584404, acc: 0.9945130348205566)
[2025-02-13 04:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:11][root][INFO] - Training Epoch: 2/2, step 6245/7134 completed (loss: 0.013114314526319504, acc: 0.99589604139328)
[2025-02-13 04:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12][root][INFO] - Training Epoch: 2/2, step 6246/7134 completed (loss: 0.013759610243141651, acc: 0.9934123754501343)
[2025-02-13 04:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12][root][INFO] - Training Epoch: 2/2, step 6247/7134 completed (loss: 0.028210284188389778, acc: 0.9879518151283264)
[2025-02-13 04:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12][root][INFO] - Training Epoch: 2/2, step 6248/7134 completed (loss: 0.04335522651672363, acc: 0.9875389337539673)
[2025-02-13 04:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:13][root][INFO] - Training Epoch: 2/2, step 6249/7134 completed (loss: 0.009977396577596664, acc: 0.9968553185462952)
[2025-02-13 04:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:13][root][INFO] - Training Epoch: 2/2, step 6250/7134 completed (loss: 0.003872578265145421, acc: 1.0)
[2025-02-13 04:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14][root][INFO] - Training Epoch: 2/2, step 6251/7134 completed (loss: 0.020423395559191704, acc: 0.995398759841919)
[2025-02-13 04:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14][root][INFO] - Training Epoch: 2/2, step 6252/7134 completed (loss: 0.007720609195530415, acc: 0.9967637658119202)
[2025-02-13 04:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15][root][INFO] - Training Epoch: 2/2, step 6253/7134 completed (loss: 0.01235999446362257, acc: 0.998275876045227)
[2025-02-13 04:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15][root][INFO] - Training Epoch: 2/2, step 6254/7134 completed (loss: 0.01664031110703945, acc: 0.9965397715568542)
[2025-02-13 04:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15][root][INFO] - Training Epoch: 2/2, step 6255/7134 completed (loss: 0.014395668171346188, acc: 0.9945799708366394)
[2025-02-13 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:16][root][INFO] - Training Epoch: 2/2, step 6256/7134 completed (loss: 0.009011146612465382, acc: 0.9957447052001953)
[2025-02-13 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:16][root][INFO] - Training Epoch: 2/2, step 6257/7134 completed (loss: 0.01010641548782587, acc: 0.9942445755004883)
[2025-02-13 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17][root][INFO] - Training Epoch: 2/2, step 6258/7134 completed (loss: 0.008325591683387756, acc: 0.9983999729156494)
[2025-02-13 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17][root][INFO] - Training Epoch: 2/2, step 6259/7134 completed (loss: 0.009427917189896107, acc: 0.9969969987869263)
[2025-02-13 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:18][root][INFO] - Training Epoch: 2/2, step 6260/7134 completed (loss: 0.01550733670592308, acc: 0.9966443181037903)
[2025-02-13 04:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:18][root][INFO] - Training Epoch: 2/2, step 6261/7134 completed (loss: 0.01563592068850994, acc: 0.9969512224197388)
[2025-02-13 04:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:18][root][INFO] - Training Epoch: 2/2, step 6262/7134 completed (loss: 0.004777163732796907, acc: 0.9985652565956116)
[2025-02-13 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19][root][INFO] - Training Epoch: 2/2, step 6263/7134 completed (loss: 0.002651992253959179, acc: 1.0)
[2025-02-13 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19][root][INFO] - Training Epoch: 2/2, step 6264/7134 completed (loss: 0.00714982021600008, acc: 0.9967159032821655)
[2025-02-13 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:20][root][INFO] - Training Epoch: 2/2, step 6265/7134 completed (loss: 0.009635542519390583, acc: 0.9970414042472839)
[2025-02-13 04:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:20][root][INFO] - Training Epoch: 2/2, step 6266/7134 completed (loss: 0.0041615962982177734, acc: 0.998251736164093)
[2025-02-13 04:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21][root][INFO] - Training Epoch: 2/2, step 6267/7134 completed (loss: 0.011760031804442406, acc: 0.9983713626861572)
[2025-02-13 04:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21][root][INFO] - Training Epoch: 2/2, step 6268/7134 completed (loss: 0.013840471394360065, acc: 0.9985119104385376)
[2025-02-13 04:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21][root][INFO] - Training Epoch: 2/2, step 6269/7134 completed (loss: 0.01092985924333334, acc: 0.9978678226470947)
[2025-02-13 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22][root][INFO] - Training Epoch: 2/2, step 6270/7134 completed (loss: 0.005051439628005028, acc: 0.9984662532806396)
[2025-02-13 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22][root][INFO] - Training Epoch: 2/2, step 6271/7134 completed (loss: 0.021555913612246513, acc: 0.9933155179023743)
[2025-02-13 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23][root][INFO] - Training Epoch: 2/2, step 6272/7134 completed (loss: 0.0008584980387240648, acc: 1.0)
[2025-02-13 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23][root][INFO] - Training Epoch: 2/2, step 6273/7134 completed (loss: 0.006930951960384846, acc: 0.99863201379776)
[2025-02-13 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24][root][INFO] - Training Epoch: 2/2, step 6274/7134 completed (loss: 0.009947975166141987, acc: 0.9985734820365906)
[2025-02-13 04:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24][root][INFO] - Training Epoch: 2/2, step 6275/7134 completed (loss: 0.016347600147128105, acc: 0.9939117431640625)
[2025-02-13 04:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24][root][INFO] - Training Epoch: 2/2, step 6276/7134 completed (loss: 0.017080187797546387, acc: 0.9947460889816284)
[2025-02-13 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25][root][INFO] - Training Epoch: 2/2, step 6277/7134 completed (loss: 0.034943848848342896, acc: 0.9932126402854919)
[2025-02-13 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25][root][INFO] - Training Epoch: 2/2, step 6278/7134 completed (loss: 0.00799943134188652, acc: 0.9950658082962036)
[2025-02-13 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:26][root][INFO] - Training Epoch: 2/2, step 6279/7134 completed (loss: 0.003470666240900755, acc: 1.0)
[2025-02-13 04:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:26][root][INFO] - Training Epoch: 2/2, step 6280/7134 completed (loss: 0.006849600933492184, acc: 0.9982547760009766)
[2025-02-13 04:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:26][root][INFO] - Training Epoch: 2/2, step 6281/7134 completed (loss: 0.012709063477814198, acc: 0.9965338110923767)
[2025-02-13 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27][root][INFO] - Training Epoch: 2/2, step 6282/7134 completed (loss: 0.004775386303663254, acc: 0.99842768907547)
[2025-02-13 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27][root][INFO] - Training Epoch: 2/2, step 6283/7134 completed (loss: 0.00960860401391983, acc: 0.9978166222572327)
[2025-02-13 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:28][root][INFO] - Training Epoch: 2/2, step 6284/7134 completed (loss: 0.019228629767894745, acc: 0.9890590906143188)
[2025-02-13 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:28][root][INFO] - Training Epoch: 2/2, step 6285/7134 completed (loss: 0.009569521062076092, acc: 0.9947506785392761)
[2025-02-13 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29][root][INFO] - Training Epoch: 2/2, step 6286/7134 completed (loss: 0.005895507521927357, acc: 0.9967105388641357)
[2025-02-13 04:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29][root][INFO] - Training Epoch: 2/2, step 6287/7134 completed (loss: 0.026895953342318535, acc: 0.9926470518112183)
[2025-02-13 04:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29][root][INFO] - Training Epoch: 2/2, step 6288/7134 completed (loss: 0.009334799833595753, acc: 0.9967319965362549)
[2025-02-13 04:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:30][root][INFO] - Training Epoch: 2/2, step 6289/7134 completed (loss: 0.019177017733454704, acc: 0.9945945739746094)
[2025-02-13 04:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:30][root][INFO] - Training Epoch: 2/2, step 6290/7134 completed (loss: 0.013679992407560349, acc: 0.995199978351593)
[2025-02-13 04:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:31][root][INFO] - Training Epoch: 2/2, step 6291/7134 completed (loss: 0.004048836417496204, acc: 0.9981818199157715)
[2025-02-13 04:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:31][root][INFO] - Training Epoch: 2/2, step 6292/7134 completed (loss: 0.017903296276926994, acc: 0.9954441785812378)
[2025-02-13 04:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:31][root][INFO] - Training Epoch: 2/2, step 6293/7134 completed (loss: 0.029719404876232147, acc: 0.9897611141204834)
[2025-02-13 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32][root][INFO] - Training Epoch: 2/2, step 6294/7134 completed (loss: 0.011395915411412716, acc: 0.9951377511024475)
[2025-02-13 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32][root][INFO] - Training Epoch: 2/2, step 6295/7134 completed (loss: 0.009953755885362625, acc: 0.9963833689689636)
[2025-02-13 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33][root][INFO] - Training Epoch: 2/2, step 6296/7134 completed (loss: 0.046262290328741074, acc: 0.9862204790115356)
[2025-02-13 04:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33][root][INFO] - Training Epoch: 2/2, step 6297/7134 completed (loss: 0.02226816676557064, acc: 0.9918032884597778)
[2025-02-13 04:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33][root][INFO] - Training Epoch: 2/2, step 6298/7134 completed (loss: 0.03861640766263008, acc: 0.9862542748451233)
[2025-02-13 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:34][root][INFO] - Training Epoch: 2/2, step 6299/7134 completed (loss: 0.057973701506853104, acc: 0.9840255379676819)
[2025-02-13 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:34][root][INFO] - Training Epoch: 2/2, step 6300/7134 completed (loss: 0.035120535641908646, acc: 0.9899497628211975)
[2025-02-13 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35][root][INFO] - Training Epoch: 2/2, step 6301/7134 completed (loss: 0.009291648864746094, acc: 0.9977011680603027)
[2025-02-13 04:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35][root][INFO] - Training Epoch: 2/2, step 6302/7134 completed (loss: 0.006124019157141447, acc: 0.9986187815666199)
[2025-02-13 04:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36][root][INFO] - Training Epoch: 2/2, step 6303/7134 completed (loss: 0.004801546223461628, acc: 0.9978700876235962)
[2025-02-13 04:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36][root][INFO] - Training Epoch: 2/2, step 6304/7134 completed (loss: 0.010906528681516647, acc: 0.9972972869873047)
[2025-02-13 04:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:37][root][INFO] - Training Epoch: 2/2, step 6305/7134 completed (loss: 0.005452316254377365, acc: 1.0)
[2025-02-13 04:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:37][root][INFO] - Training Epoch: 2/2, step 6306/7134 completed (loss: 0.028327573090791702, acc: 0.9915966391563416)
[2025-02-13 04:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:38][root][INFO] - Training Epoch: 2/2, step 6307/7134 completed (loss: 0.030866697430610657, acc: 0.9947090148925781)
[2025-02-13 04:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:38][root][INFO] - Training Epoch: 2/2, step 6308/7134 completed (loss: 0.014388271607458591, acc: 0.9975429773330688)
[2025-02-13 04:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:38][root][INFO] - Training Epoch: 2/2, step 6309/7134 completed (loss: 0.008533026091754436, acc: 0.9983999729156494)
[2025-02-13 04:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:39][root][INFO] - Training Epoch: 2/2, step 6310/7134 completed (loss: 0.019886115565896034, acc: 0.9912087917327881)
[2025-02-13 04:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:39][root][INFO] - Training Epoch: 2/2, step 6311/7134 completed (loss: 0.017268378287553787, acc: 0.9938499331474304)
[2025-02-13 04:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:40][root][INFO] - Training Epoch: 2/2, step 6312/7134 completed (loss: 0.02451958693563938, acc: 0.9968553185462952)
[2025-02-13 04:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:40][root][INFO] - Training Epoch: 2/2, step 6313/7134 completed (loss: 0.014086795970797539, acc: 0.9963189959526062)
[2025-02-13 04:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41][root][INFO] - Training Epoch: 2/2, step 6314/7134 completed (loss: 0.010743244551122189, acc: 0.9974779486656189)
[2025-02-13 04:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41][root][INFO] - Training Epoch: 2/2, step 6315/7134 completed (loss: 0.032867006957530975, acc: 0.9918032884597778)
[2025-02-13 04:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42][root][INFO] - Training Epoch: 2/2, step 6316/7134 completed (loss: 0.01836678571999073, acc: 0.9935170412063599)
[2025-02-13 04:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42][root][INFO] - Training Epoch: 2/2, step 6317/7134 completed (loss: 0.006104517728090286, acc: 0.9983844757080078)
[2025-02-13 04:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42][root][INFO] - Training Epoch: 2/2, step 6318/7134 completed (loss: 0.005338624585419893, acc: 0.998410165309906)
[2025-02-13 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43][root][INFO] - Training Epoch: 2/2, step 6319/7134 completed (loss: 0.003949892707169056, acc: 1.0)
[2025-02-13 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43][root][INFO] - Training Epoch: 2/2, step 6320/7134 completed (loss: 0.009039269760251045, acc: 0.9968404173851013)
[2025-02-13 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44][root][INFO] - Training Epoch: 2/2, step 6321/7134 completed (loss: 0.006471061613410711, acc: 0.9977678656578064)
[2025-02-13 04:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44][root][INFO] - Training Epoch: 2/2, step 6322/7134 completed (loss: 0.02629946544766426, acc: 0.9912790656089783)
[2025-02-13 04:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45][root][INFO] - Training Epoch: 2/2, step 6323/7134 completed (loss: 0.017197448760271072, acc: 0.9939939975738525)
[2025-02-13 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45][root][INFO] - Training Epoch: 2/2, step 6324/7134 completed (loss: 0.014001838862895966, acc: 0.9946737885475159)
[2025-02-13 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46][root][INFO] - Training Epoch: 2/2, step 6325/7134 completed (loss: 0.020079156383872032, acc: 0.99245285987854)
[2025-02-13 04:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46][root][INFO] - Training Epoch: 2/2, step 6326/7134 completed (loss: 0.028744827955961227, acc: 0.9871588945388794)
[2025-02-13 04:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46][root][INFO] - Training Epoch: 2/2, step 6327/7134 completed (loss: 0.0035128898452967405, acc: 1.0)
[2025-02-13 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47][root][INFO] - Training Epoch: 2/2, step 6328/7134 completed (loss: 0.008209503255784512, acc: 0.9984050989151001)
[2025-02-13 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47][root][INFO] - Training Epoch: 2/2, step 6329/7134 completed (loss: 0.016632765531539917, acc: 0.9939393997192383)
[2025-02-13 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48][root][INFO] - Training Epoch: 2/2, step 6330/7134 completed (loss: 0.0149172218516469, acc: 0.9925558567047119)
[2025-02-13 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48][root][INFO] - Training Epoch: 2/2, step 6331/7134 completed (loss: 0.006258013192564249, acc: 0.9982331991195679)
[2025-02-13 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49][root][INFO] - Training Epoch: 2/2, step 6332/7134 completed (loss: 0.03636975213885307, acc: 0.9814126491546631)
[2025-02-13 04:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49][root][INFO] - Training Epoch: 2/2, step 6333/7134 completed (loss: 0.02558217942714691, acc: 0.9904761910438538)
[2025-02-13 04:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49][root][INFO] - Training Epoch: 2/2, step 6334/7134 completed (loss: 0.007630216423422098, acc: 0.998106062412262)
[2025-02-13 04:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:50][root][INFO] - Training Epoch: 2/2, step 6335/7134 completed (loss: 0.009484793059527874, acc: 0.994966447353363)
[2025-02-13 04:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:50][root][INFO] - Training Epoch: 2/2, step 6336/7134 completed (loss: 0.013776808977127075, acc: 0.9954128265380859)
[2025-02-13 04:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51][root][INFO] - Training Epoch: 2/2, step 6337/7134 completed (loss: 0.004722108133137226, acc: 1.0)
[2025-02-13 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51][root][INFO] - Training Epoch: 2/2, step 6338/7134 completed (loss: 0.017794013023376465, acc: 0.9905213117599487)
[2025-02-13 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52][root][INFO] - Training Epoch: 2/2, step 6339/7134 completed (loss: 0.016957677900791168, acc: 0.9930796027183533)
[2025-02-13 04:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52][root][INFO] - Training Epoch: 2/2, step 6340/7134 completed (loss: 0.006399191915988922, acc: 0.9985337257385254)
[2025-02-13 04:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52][root][INFO] - Training Epoch: 2/2, step 6341/7134 completed (loss: 0.0064663090743124485, acc: 1.0)
[2025-02-13 04:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53][root][INFO] - Training Epoch: 2/2, step 6342/7134 completed (loss: 0.018698399886488914, acc: 0.9946164488792419)
[2025-02-13 04:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53][root][INFO] - Training Epoch: 2/2, step 6343/7134 completed (loss: 0.021115554496645927, acc: 0.9931153059005737)
[2025-02-13 04:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:54][root][INFO] - Training Epoch: 2/2, step 6344/7134 completed (loss: 0.011677158996462822, acc: 0.99726402759552)
[2025-02-13 04:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:54][root][INFO] - Training Epoch: 2/2, step 6345/7134 completed (loss: 0.012391026131808758, acc: 0.994535505771637)
[2025-02-13 04:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55][root][INFO] - Training Epoch: 2/2, step 6346/7134 completed (loss: 0.05554681643843651, acc: 0.9845201373100281)
[2025-02-13 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55][root][INFO] - Training Epoch: 2/2, step 6347/7134 completed (loss: 0.022524336352944374, acc: 0.9956709742546082)
[2025-02-13 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55][root][INFO] - Training Epoch: 2/2, step 6348/7134 completed (loss: 0.02450737915933132, acc: 0.9947916865348816)
[2025-02-13 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56][root][INFO] - Training Epoch: 2/2, step 6349/7134 completed (loss: 0.011396904475986958, acc: 0.994535505771637)
[2025-02-13 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56][root][INFO] - Training Epoch: 2/2, step 6350/7134 completed (loss: 0.0258125402033329, acc: 0.9931972622871399)
[2025-02-13 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57][root][INFO] - Training Epoch: 2/2, step 6351/7134 completed (loss: 0.038665980100631714, acc: 0.9895522594451904)
[2025-02-13 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57][root][INFO] - Training Epoch: 2/2, step 6352/7134 completed (loss: 0.10203785449266434, acc: 0.97919762134552)
[2025-02-13 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58][root][INFO] - Training Epoch: 2/2, step 6353/7134 completed (loss: 0.010950547643005848, acc: 0.9956076145172119)
[2025-02-13 04:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58][root][INFO] - Training Epoch: 2/2, step 6354/7134 completed (loss: 0.005619085859507322, acc: 0.9986149668693542)
[2025-02-13 04:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59][root][INFO] - Training Epoch: 2/2, step 6355/7134 completed (loss: 0.002917917910963297, acc: 1.0)
[2025-02-13 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59][root][INFO] - Training Epoch: 2/2, step 6356/7134 completed (loss: 0.0047709220089018345, acc: 1.0)
[2025-02-13 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59][root][INFO] - Training Epoch: 2/2, step 6357/7134 completed (loss: 0.008659949526190758, acc: 0.9972222447395325)
[2025-02-13 04:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:00][root][INFO] - Training Epoch: 2/2, step 6358/7134 completed (loss: 0.01251726783812046, acc: 0.997183084487915)
[2025-02-13 04:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:00][root][INFO] - Training Epoch: 2/2, step 6359/7134 completed (loss: 0.031188368797302246, acc: 0.9929971694946289)
[2025-02-13 04:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01][root][INFO] - Training Epoch: 2/2, step 6360/7134 completed (loss: 0.013750435784459114, acc: 0.9952229261398315)
[2025-02-13 04:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01][root][INFO] - Training Epoch: 2/2, step 6361/7134 completed (loss: 0.023273780941963196, acc: 0.9945255517959595)
[2025-02-13 04:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02][root][INFO] - Training Epoch: 2/2, step 6362/7134 completed (loss: 0.016820169985294342, acc: 0.9939024448394775)
[2025-02-13 04:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02][root][INFO] - Training Epoch: 2/2, step 6363/7134 completed (loss: 0.021775513887405396, acc: 0.9941349029541016)
[2025-02-13 04:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:03][root][INFO] - Training Epoch: 2/2, step 6364/7134 completed (loss: 0.005338382441550493, acc: 1.0)
[2025-02-13 04:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:03][root][INFO] - Training Epoch: 2/2, step 6365/7134 completed (loss: 0.011102627962827682, acc: 0.9918699264526367)
[2025-02-13 04:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:03][root][INFO] - Training Epoch: 2/2, step 6366/7134 completed (loss: 0.01750337891280651, acc: 0.997357964515686)
[2025-02-13 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04][root][INFO] - Training Epoch: 2/2, step 6367/7134 completed (loss: 0.03367827460169792, acc: 0.9867674708366394)
[2025-02-13 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04][root][INFO] - Training Epoch: 2/2, step 6368/7134 completed (loss: 0.04648149758577347, acc: 0.991525411605835)
[2025-02-13 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05][root][INFO] - Training Epoch: 2/2, step 6369/7134 completed (loss: 0.029347866773605347, acc: 0.9908015727996826)
[2025-02-13 04:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05][root][INFO] - Training Epoch: 2/2, step 6370/7134 completed (loss: 0.07385977357625961, acc: 0.9683377146720886)
[2025-02-13 04:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06][root][INFO] - Training Epoch: 2/2, step 6371/7134 completed (loss: 0.037439778447151184, acc: 0.9889196753501892)
[2025-02-13 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06][root][INFO] - Training Epoch: 2/2, step 6372/7134 completed (loss: 0.08586800843477249, acc: 0.9750480055809021)
[2025-02-13 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:07][root][INFO] - Training Epoch: 2/2, step 6373/7134 completed (loss: 0.014865909703075886, acc: 1.0)
[2025-02-13 04:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:07][root][INFO] - Training Epoch: 2/2, step 6374/7134 completed (loss: 0.011815372854471207, acc: 0.9979252815246582)
[2025-02-13 04:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:07][root][INFO] - Training Epoch: 2/2, step 6375/7134 completed (loss: 0.03448369726538658, acc: 0.9833119511604309)
[2025-02-13 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08][root][INFO] - Training Epoch: 2/2, step 6376/7134 completed (loss: 0.016089878976345062, acc: 0.9944649338722229)
[2025-02-13 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08][root][INFO] - Training Epoch: 2/2, step 6377/7134 completed (loss: 0.01605830527842045, acc: 0.9945054650306702)
[2025-02-13 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:09][root][INFO] - Training Epoch: 2/2, step 6378/7134 completed (loss: 0.03028879314661026, acc: 0.986066460609436)
[2025-02-13 04:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:09][root][INFO] - Training Epoch: 2/2, step 6379/7134 completed (loss: 0.018471114337444305, acc: 0.9961928725242615)
[2025-02-13 04:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10][root][INFO] - Training Epoch: 2/2, step 6380/7134 completed (loss: 0.04154616966843605, acc: 0.992514967918396)
[2025-02-13 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10][root][INFO] - Training Epoch: 2/2, step 6381/7134 completed (loss: 0.008413524366915226, acc: 0.9984732866287231)
[2025-02-13 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11][root][INFO] - Training Epoch: 2/2, step 6382/7134 completed (loss: 0.0013955808244645596, acc: 1.0)
[2025-02-13 04:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11][root][INFO] - Training Epoch: 2/2, step 6383/7134 completed (loss: 0.03585083410143852, acc: 0.9959294199943542)
[2025-02-13 04:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11][root][INFO] - Training Epoch: 2/2, step 6384/7134 completed (loss: 0.014058861881494522, acc: 0.9970104694366455)
[2025-02-13 04:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:12][root][INFO] - Training Epoch: 2/2, step 6385/7134 completed (loss: 0.04543791338801384, acc: 0.9928143620491028)
[2025-02-13 04:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:12][root][INFO] - Training Epoch: 2/2, step 6386/7134 completed (loss: 0.007306629326194525, acc: 0.9963369965553284)
[2025-02-13 04:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13][root][INFO] - Training Epoch: 2/2, step 6387/7134 completed (loss: 0.030763842165470123, acc: 0.9903225898742676)
[2025-02-13 04:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13][root][INFO] - Training Epoch: 2/2, step 6388/7134 completed (loss: 0.025888977572321892, acc: 0.9940119981765747)
[2025-02-13 04:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14][root][INFO] - Training Epoch: 2/2, step 6389/7134 completed (loss: 0.03602668642997742, acc: 0.990231990814209)
[2025-02-13 04:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14][root][INFO] - Training Epoch: 2/2, step 6390/7134 completed (loss: 0.042369574308395386, acc: 0.9878542423248291)
[2025-02-13 04:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15][root][INFO] - Training Epoch: 2/2, step 6391/7134 completed (loss: 0.04487223923206329, acc: 0.9923175573348999)
[2025-02-13 04:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15][root][INFO] - Training Epoch: 2/2, step 6392/7134 completed (loss: 0.03735635429620743, acc: 0.9887179732322693)
[2025-02-13 04:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16][root][INFO] - Training Epoch: 2/2, step 6393/7134 completed (loss: 0.020913641899824142, acc: 0.9943116903305054)
[2025-02-13 04:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16][root][INFO] - Training Epoch: 2/2, step 6394/7134 completed (loss: 0.04910345748066902, acc: 0.9874826073646545)
[2025-02-13 04:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16][root][INFO] - Training Epoch: 2/2, step 6395/7134 completed (loss: 0.028816023841500282, acc: 0.9938499331474304)
[2025-02-13 04:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17][root][INFO] - Training Epoch: 2/2, step 6396/7134 completed (loss: 0.02969362400472164, acc: 0.9942660331726074)
[2025-02-13 04:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17][root][INFO] - Training Epoch: 2/2, step 6397/7134 completed (loss: 0.01371216494590044, acc: 0.9913151264190674)
[2025-02-13 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18][root][INFO] - Training Epoch: 2/2, step 6398/7134 completed (loss: 0.008334586396813393, acc: 0.997187077999115)
[2025-02-13 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18][root][INFO] - Training Epoch: 2/2, step 6399/7134 completed (loss: 0.0384419783949852, acc: 0.9874686598777771)
[2025-02-13 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:19][root][INFO] - Training Epoch: 2/2, step 6400/7134 completed (loss: 0.016651805490255356, acc: 0.994301974773407)
[2025-02-13 04:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:19][root][INFO] - Training Epoch: 2/2, step 6401/7134 completed (loss: 0.03089132346212864, acc: 0.9913138151168823)
[2025-02-13 04:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20][root][INFO] - Training Epoch: 2/2, step 6402/7134 completed (loss: 0.03336457908153534, acc: 0.9920544624328613)
[2025-02-13 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20][root][INFO] - Training Epoch: 2/2, step 6403/7134 completed (loss: 0.0154853705316782, acc: 0.9923469424247742)
[2025-02-13 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:21][root][INFO] - Training Epoch: 2/2, step 6404/7134 completed (loss: 0.015447360463440418, acc: 0.9958275556564331)
[2025-02-13 04:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:21][root][INFO] - Training Epoch: 2/2, step 6405/7134 completed (loss: 0.013473601080477238, acc: 0.9956989288330078)
[2025-02-13 04:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22][root][INFO] - Training Epoch: 2/2, step 6406/7134 completed (loss: 0.0269564650952816, acc: 0.9920182228088379)
[2025-02-13 04:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22][root][INFO] - Training Epoch: 2/2, step 6407/7134 completed (loss: 0.013408671133220196, acc: 0.9960578083992004)
[2025-02-13 04:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22][root][INFO] - Training Epoch: 2/2, step 6408/7134 completed (loss: 0.04755096137523651, acc: 0.9911373853683472)
[2025-02-13 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23][root][INFO] - Training Epoch: 2/2, step 6409/7134 completed (loss: 0.023717304691672325, acc: 0.9947368502616882)
[2025-02-13 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23][root][INFO] - Training Epoch: 2/2, step 6410/7134 completed (loss: 0.017823629081249237, acc: 0.9926470518112183)
[2025-02-13 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:24][root][INFO] - Training Epoch: 2/2, step 6411/7134 completed (loss: 0.025447087362408638, acc: 0.9949748516082764)
[2025-02-13 04:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:24][root][INFO] - Training Epoch: 2/2, step 6412/7134 completed (loss: 0.0069817472249269485, acc: 0.9973333477973938)
[2025-02-13 04:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25][root][INFO] - Training Epoch: 2/2, step 6413/7134 completed (loss: 0.03336924687027931, acc: 0.9880136847496033)
[2025-02-13 04:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25][root][INFO] - Training Epoch: 2/2, step 6414/7134 completed (loss: 0.006150073371827602, acc: 0.9984894394874573)
[2025-02-13 04:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25][root][INFO] - Training Epoch: 2/2, step 6415/7134 completed (loss: 0.008774872869253159, acc: 0.9974325895309448)
[2025-02-13 04:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26][root][INFO] - Training Epoch: 2/2, step 6416/7134 completed (loss: 0.02626102603971958, acc: 0.9867172837257385)
[2025-02-13 04:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26][root][INFO] - Training Epoch: 2/2, step 6417/7134 completed (loss: 0.04674160107970238, acc: 0.9912126660346985)
[2025-02-13 04:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:27][root][INFO] - Training Epoch: 2/2, step 6418/7134 completed (loss: 0.0583626925945282, acc: 0.9873595237731934)
[2025-02-13 04:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:27][root][INFO] - Training Epoch: 2/2, step 6419/7134 completed (loss: 0.042380984872579575, acc: 0.985318124294281)
[2025-02-13 04:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28][root][INFO] - Training Epoch: 2/2, step 6420/7134 completed (loss: 0.013782303780317307, acc: 0.9956772327423096)
[2025-02-13 04:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28][root][INFO] - Training Epoch: 2/2, step 6421/7134 completed (loss: 0.027802713215351105, acc: 0.9925705790519714)
[2025-02-13 04:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29][root][INFO] - Training Epoch: 2/2, step 6422/7134 completed (loss: 0.03349744528532028, acc: 0.9876881241798401)
[2025-02-13 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29][root][INFO] - Training Epoch: 2/2, step 6423/7134 completed (loss: 0.046146735548973083, acc: 0.9912023544311523)
[2025-02-13 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29][root][INFO] - Training Epoch: 2/2, step 6424/7134 completed (loss: 0.027681931853294373, acc: 0.9911727905273438)
[2025-02-13 04:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:30][root][INFO] - Training Epoch: 2/2, step 6425/7134 completed (loss: 0.020495902746915817, acc: 0.9934810996055603)
[2025-02-13 04:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:30][root][INFO] - Training Epoch: 2/2, step 6426/7134 completed (loss: 0.024212786927819252, acc: 0.9907651543617249)
[2025-02-13 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:31][root][INFO] - Training Epoch: 2/2, step 6427/7134 completed (loss: 0.027110010385513306, acc: 0.9928977489471436)
[2025-02-13 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:31][root][INFO] - Training Epoch: 2/2, step 6428/7134 completed (loss: 0.012811106629669666, acc: 0.9972066879272461)
[2025-02-13 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32][root][INFO] - Training Epoch: 2/2, step 6429/7134 completed (loss: 0.011284499429166317, acc: 0.9969696998596191)
[2025-02-13 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32][root][INFO] - Training Epoch: 2/2, step 6430/7134 completed (loss: 0.041558653116226196, acc: 0.9899280667304993)
[2025-02-13 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33][root][INFO] - Training Epoch: 2/2, step 6431/7134 completed (loss: 0.04438643157482147, acc: 0.9907894730567932)
[2025-02-13 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33][root][INFO] - Training Epoch: 2/2, step 6432/7134 completed (loss: 0.1577564775943756, acc: 0.9579124450683594)
[2025-02-13 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33][root][INFO] - Training Epoch: 2/2, step 6433/7134 completed (loss: 0.029556354507803917, acc: 0.9918032884597778)
[2025-02-13 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:34][root][INFO] - Training Epoch: 2/2, step 6434/7134 completed (loss: 0.004994284827262163, acc: 1.0)
[2025-02-13 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:34][root][INFO] - Training Epoch: 2/2, step 6435/7134 completed (loss: 0.03596768528223038, acc: 0.9878683090209961)
[2025-02-13 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35][root][INFO] - Training Epoch: 2/2, step 6436/7134 completed (loss: 0.006628899369388819, acc: 1.0)
[2025-02-13 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35][root][INFO] - Training Epoch: 2/2, step 6437/7134 completed (loss: 0.03170968219637871, acc: 0.9950000047683716)
[2025-02-13 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36][root][INFO] - Training Epoch: 2/2, step 6438/7134 completed (loss: 0.02625083178281784, acc: 0.9925834536552429)
[2025-02-13 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36][root][INFO] - Training Epoch: 2/2, step 6439/7134 completed (loss: 0.03007178194820881, acc: 0.9905511736869812)
[2025-02-13 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:37][root][INFO] - Training Epoch: 2/2, step 6440/7134 completed (loss: 0.018956458196043968, acc: 0.9952210187911987)
[2025-02-13 04:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:37][root][INFO] - Training Epoch: 2/2, step 6441/7134 completed (loss: 0.01639803685247898, acc: 0.9970458149909973)
[2025-02-13 04:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38][root][INFO] - Training Epoch: 2/2, step 6442/7134 completed (loss: 0.019620485603809357, acc: 0.9954596757888794)
[2025-02-13 04:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38][root][INFO] - Training Epoch: 2/2, step 6443/7134 completed (loss: 0.014747056178748608, acc: 0.9950494766235352)
[2025-02-13 04:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38][root][INFO] - Training Epoch: 2/2, step 6444/7134 completed (loss: 0.020305797457695007, acc: 0.994413435459137)
[2025-02-13 04:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39][root][INFO] - Training Epoch: 2/2, step 6445/7134 completed (loss: 0.04240019619464874, acc: 0.9901356101036072)
[2025-02-13 04:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39][root][INFO] - Training Epoch: 2/2, step 6446/7134 completed (loss: 0.012125365436077118, acc: 0.9952324032783508)
[2025-02-13 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:40][root][INFO] - Training Epoch: 2/2, step 6447/7134 completed (loss: 0.024792464450001717, acc: 0.994246244430542)
[2025-02-13 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:40][root][INFO] - Training Epoch: 2/2, step 6448/7134 completed (loss: 0.05997093766927719, acc: 0.9844236969947815)
[2025-02-13 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41][root][INFO] - Training Epoch: 2/2, step 6449/7134 completed (loss: 0.020837703719735146, acc: 0.9942029118537903)
[2025-02-13 04:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41][root][INFO] - Training Epoch: 2/2, step 6450/7134 completed (loss: 0.02026181109249592, acc: 0.992559552192688)
[2025-02-13 04:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42][root][INFO] - Training Epoch: 2/2, step 6451/7134 completed (loss: 0.03239798918366432, acc: 0.990205705165863)
[2025-02-13 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42][root][INFO] - Training Epoch: 2/2, step 6452/7134 completed (loss: 0.020776621997356415, acc: 0.9966555237770081)
[2025-02-13 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:43][root][INFO] - Training Epoch: 2/2, step 6453/7134 completed (loss: 0.01031313557177782, acc: 0.9959677457809448)
[2025-02-13 04:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:43][root][INFO] - Training Epoch: 2/2, step 6454/7134 completed (loss: 0.018176620826125145, acc: 0.9952885508537292)
[2025-02-13 04:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44][root][INFO] - Training Epoch: 2/2, step 6455/7134 completed (loss: 0.012478250078856945, acc: 0.9966044425964355)
[2025-02-13 04:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44][root][INFO] - Training Epoch: 2/2, step 6456/7134 completed (loss: 0.018282724544405937, acc: 0.9969325065612793)
[2025-02-13 04:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45][root][INFO] - Training Epoch: 2/2, step 6457/7134 completed (loss: 0.006025581154972315, acc: 1.0)
[2025-02-13 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45][root][INFO] - Training Epoch: 2/2, step 6458/7134 completed (loss: 0.02851632982492447, acc: 0.9915966391563416)
[2025-02-13 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45][root][INFO] - Training Epoch: 2/2, step 6459/7134 completed (loss: 0.0014265823410823941, acc: 1.0)
[2025-02-13 04:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:46][root][INFO] - Training Epoch: 2/2, step 6460/7134 completed (loss: 0.004090090747922659, acc: 0.9988193511962891)
[2025-02-13 04:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:46][root][INFO] - Training Epoch: 2/2, step 6461/7134 completed (loss: 0.010201944969594479, acc: 0.9960578083992004)
[2025-02-13 04:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47][root][INFO] - Training Epoch: 2/2, step 6462/7134 completed (loss: 0.010280275717377663, acc: 0.9963099360466003)
[2025-02-13 04:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47][root][INFO] - Training Epoch: 2/2, step 6463/7134 completed (loss: 0.006414799485355616, acc: 0.9988610744476318)
[2025-02-13 04:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:48][root][INFO] - Training Epoch: 2/2, step 6464/7134 completed (loss: 0.020342882722616196, acc: 0.9934980273246765)
[2025-02-13 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:48][root][INFO] - Training Epoch: 2/2, step 6465/7134 completed (loss: 0.007537083234637976, acc: 0.9974293112754822)
[2025-02-13 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49][root][INFO] - Training Epoch: 2/2, step 6466/7134 completed (loss: 0.010200741700828075, acc: 0.9960370063781738)
[2025-02-13 04:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49][root][INFO] - Training Epoch: 2/2, step 6467/7134 completed (loss: 0.01615966483950615, acc: 0.9954198598861694)
[2025-02-13 04:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49][root][INFO] - Training Epoch: 2/2, step 6468/7134 completed (loss: 0.010668719187378883, acc: 0.998123824596405)
[2025-02-13 04:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:50][root][INFO] - Training Epoch: 2/2, step 6469/7134 completed (loss: 0.021298328414559364, acc: 0.9951279163360596)
[2025-02-13 04:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:50][root][INFO] - Training Epoch: 2/2, step 6470/7134 completed (loss: 0.01949426159262657, acc: 0.995110034942627)
[2025-02-13 04:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51][root][INFO] - Training Epoch: 2/2, step 6471/7134 completed (loss: 0.004879117477685213, acc: 0.9983713626861572)
[2025-02-13 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51][root][INFO] - Training Epoch: 2/2, step 6472/7134 completed (loss: 0.03523490950465202, acc: 0.9952718615531921)
[2025-02-13 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52][root][INFO] - Training Epoch: 2/2, step 6473/7134 completed (loss: 0.005171211902052164, acc: 0.9986594915390015)
[2025-02-13 04:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52][root][INFO] - Training Epoch: 2/2, step 6474/7134 completed (loss: 0.0058547137305140495, acc: 0.9986541271209717)
[2025-02-13 04:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:53][root][INFO] - Training Epoch: 2/2, step 6475/7134 completed (loss: 0.022939730435609818, acc: 0.9933333396911621)
[2025-02-13 04:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:53][root][INFO] - Training Epoch: 2/2, step 6476/7134 completed (loss: 0.036028601229190826, acc: 0.9917491674423218)
[2025-02-13 04:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:53][root][INFO] - Training Epoch: 2/2, step 6477/7134 completed (loss: 0.010074472054839134, acc: 0.9957627058029175)
[2025-02-13 04:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54][root][INFO] - Training Epoch: 2/2, step 6478/7134 completed (loss: 0.014092101715505123, acc: 0.9944649338722229)
[2025-02-13 04:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54][root][INFO] - Training Epoch: 2/2, step 6479/7134 completed (loss: 0.007107485551387072, acc: 0.9983249306678772)
[2025-02-13 04:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55][root][INFO] - Training Epoch: 2/2, step 6480/7134 completed (loss: 0.025236740708351135, acc: 0.9879724979400635)
[2025-02-13 04:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55][root][INFO] - Training Epoch: 2/2, step 6481/7134 completed (loss: 0.006615909282118082, acc: 0.9982993006706238)
[2025-02-13 04:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56][root][INFO] - Training Epoch: 2/2, step 6482/7134 completed (loss: 0.011121846735477448, acc: 0.9975000023841858)
[2025-02-13 04:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56][root][INFO] - Training Epoch: 2/2, step 6483/7134 completed (loss: 0.04524907469749451, acc: 0.9909747242927551)
[2025-02-13 04:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56][root][INFO] - Training Epoch: 2/2, step 6484/7134 completed (loss: 0.02027423307299614, acc: 0.9941349029541016)
[2025-02-13 04:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:57][root][INFO] - Training Epoch: 2/2, step 6485/7134 completed (loss: 0.004829657729715109, acc: 0.9980236887931824)
[2025-02-13 04:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:57][root][INFO] - Training Epoch: 2/2, step 6486/7134 completed (loss: 0.06169317662715912, acc: 0.9836363792419434)
[2025-02-13 04:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:58][root][INFO] - Training Epoch: 2/2, step 6487/7134 completed (loss: 0.08515387028455734, acc: 0.9736841917037964)
[2025-02-13 04:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:58][root][INFO] - Training Epoch: 2/2, step 6488/7134 completed (loss: 0.03662443533539772, acc: 0.9839034080505371)
[2025-02-13 04:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59][root][INFO] - Training Epoch: 2/2, step 6489/7134 completed (loss: 0.07256797701120377, acc: 0.986940324306488)
[2025-02-13 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59][root][INFO] - Training Epoch: 2/2, step 6490/7134 completed (loss: 0.04303149878978729, acc: 0.9931389093399048)
[2025-02-13 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59][root][INFO] - Training Epoch: 2/2, step 6491/7134 completed (loss: 0.04655371233820915, acc: 0.9892037510871887)
[2025-02-13 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:00][root][INFO] - Training Epoch: 2/2, step 6492/7134 completed (loss: 0.011370629072189331, acc: 0.9961190223693848)
[2025-02-13 04:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:00][root][INFO] - Training Epoch: 2/2, step 6493/7134 completed (loss: 0.04273756220936775, acc: 0.9885495901107788)
[2025-02-13 04:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01][root][INFO] - Training Epoch: 2/2, step 6494/7134 completed (loss: 0.0251341350376606, acc: 0.9926199316978455)
[2025-02-13 04:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01][root][INFO] - Training Epoch: 2/2, step 6495/7134 completed (loss: 0.03519927337765694, acc: 0.9887797832489014)
[2025-02-13 04:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:02][root][INFO] - Training Epoch: 2/2, step 6496/7134 completed (loss: 0.022288596257567406, acc: 0.992443323135376)
[2025-02-13 04:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:02][root][INFO] - Training Epoch: 2/2, step 6497/7134 completed (loss: 0.04274773225188255, acc: 0.9860031008720398)
[2025-02-13 04:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:02][root][INFO] - Training Epoch: 2/2, step 6498/7134 completed (loss: 0.01224552746862173, acc: 0.995121955871582)
[2025-02-13 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03][root][INFO] - Training Epoch: 2/2, step 6499/7134 completed (loss: 0.02113390527665615, acc: 0.993773341178894)
[2025-02-13 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03][root][INFO] - Training Epoch: 2/2, step 6500/7134 completed (loss: 0.039011746644973755, acc: 0.9901960492134094)
[2025-02-13 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:04][root][INFO] - Training Epoch: 2/2, step 6501/7134 completed (loss: 0.09829166531562805, acc: 0.9769821166992188)
[2025-02-13 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:05][root][INFO] - Training Epoch: 2/2, step 6502/7134 completed (loss: 0.016016928479075432, acc: 0.9952493906021118)
[2025-02-13 04:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:05][root][INFO] - Training Epoch: 2/2, step 6503/7134 completed (loss: 0.02877793088555336, acc: 0.992668628692627)
[2025-02-13 04:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:05][root][INFO] - Training Epoch: 2/2, step 6504/7134 completed (loss: 0.024676084518432617, acc: 0.9905213117599487)
[2025-02-13 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06][root][INFO] - Training Epoch: 2/2, step 6505/7134 completed (loss: 0.05224591866135597, acc: 0.9826086759567261)
[2025-02-13 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06][root][INFO] - Training Epoch: 2/2, step 6506/7134 completed (loss: 0.038406189531087875, acc: 0.988056480884552)
[2025-02-13 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07][root][INFO] - Training Epoch: 2/2, step 6507/7134 completed (loss: 0.05626139044761658, acc: 0.9833585619926453)
[2025-02-13 04:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07][root][INFO] - Training Epoch: 2/2, step 6508/7134 completed (loss: 0.040987882763147354, acc: 0.9929577708244324)
[2025-02-13 04:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08][root][INFO] - Training Epoch: 2/2, step 6509/7134 completed (loss: 0.017922861501574516, acc: 0.9944629073143005)
[2025-02-13 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08][root][INFO] - Training Epoch: 2/2, step 6510/7134 completed (loss: 0.023589881137013435, acc: 0.9894291758537292)
[2025-02-13 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09][root][INFO] - Training Epoch: 2/2, step 6511/7134 completed (loss: 0.045152321457862854, acc: 0.9894737005233765)
[2025-02-13 04:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09][root][INFO] - Training Epoch: 2/2, step 6512/7134 completed (loss: 0.010935521684587002, acc: 1.0)
[2025-02-13 04:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:10][root][INFO] - Training Epoch: 2/2, step 6513/7134 completed (loss: 0.01819075644016266, acc: 0.9922839403152466)
[2025-02-13 04:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:10][root][INFO] - Training Epoch: 2/2, step 6514/7134 completed (loss: 0.04369981959462166, acc: 0.9872159361839294)
[2025-02-13 04:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:10][root][INFO] - Training Epoch: 2/2, step 6515/7134 completed (loss: 0.01736261323094368, acc: 0.9936548471450806)
[2025-02-13 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11][root][INFO] - Training Epoch: 2/2, step 6516/7134 completed (loss: 0.05528777837753296, acc: 0.988063633441925)
[2025-02-13 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11][root][INFO] - Training Epoch: 2/2, step 6517/7134 completed (loss: 0.013666239567101002, acc: 0.9973683953285217)
[2025-02-13 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12][root][INFO] - Training Epoch: 2/2, step 6518/7134 completed (loss: 0.013941504061222076, acc: 0.9958620667457581)
[2025-02-13 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12][root][INFO] - Training Epoch: 2/2, step 6519/7134 completed (loss: 0.03008297272026539, acc: 0.9929078221321106)
[2025-02-13 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13][root][INFO] - Training Epoch: 2/2, step 6520/7134 completed (loss: 0.012718606740236282, acc: 0.996889591217041)
[2025-02-13 04:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13][root][INFO] - Training Epoch: 2/2, step 6521/7134 completed (loss: 0.009183314628899097, acc: 0.9947916865348816)
[2025-02-13 04:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14][root][INFO] - Training Epoch: 2/2, step 6522/7134 completed (loss: 0.03284250944852829, acc: 0.9931389093399048)
[2025-02-13 04:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14][root][INFO] - Training Epoch: 2/2, step 6523/7134 completed (loss: 0.0594264417886734, acc: 0.9854809641838074)
[2025-02-13 04:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14][root][INFO] - Training Epoch: 2/2, step 6524/7134 completed (loss: 0.06290177255868912, acc: 0.9910025596618652)
[2025-02-13 04:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15][root][INFO] - Training Epoch: 2/2, step 6525/7134 completed (loss: 0.01225374173372984, acc: 0.9919678568840027)
[2025-02-13 04:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15][root][INFO] - Training Epoch: 2/2, step 6526/7134 completed (loss: 0.0480474978685379, acc: 0.991037130355835)
[2025-02-13 04:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:16][root][INFO] - Training Epoch: 2/2, step 6527/7134 completed (loss: 0.02396349236369133, acc: 0.9940652847290039)
[2025-02-13 04:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:16][root][INFO] - Training Epoch: 2/2, step 6528/7134 completed (loss: 0.04580990970134735, acc: 0.987364649772644)
[2025-02-13 04:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:16][root][INFO] - Training Epoch: 2/2, step 6529/7134 completed (loss: 0.05019507557153702, acc: 0.9887217879295349)
[2025-02-13 04:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:17][root][INFO] - Training Epoch: 2/2, step 6530/7134 completed (loss: 0.01307622715830803, acc: 0.9928673505783081)
[2025-02-13 04:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:17][root][INFO] - Training Epoch: 2/2, step 6531/7134 completed (loss: 0.008435155265033245, acc: 0.9985548853874207)
[2025-02-13 04:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:18][root][INFO] - Training Epoch: 2/2, step 6532/7134 completed (loss: 0.03743254020810127, acc: 0.991239070892334)
[2025-02-13 04:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:18][root][INFO] - Training Epoch: 2/2, step 6533/7134 completed (loss: 0.007200562860816717, acc: 0.9970149397850037)
[2025-02-13 04:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19][root][INFO] - Training Epoch: 2/2, step 6534/7134 completed (loss: 0.046488698571920395, acc: 0.9919571280479431)
[2025-02-13 04:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19][root][INFO] - Training Epoch: 2/2, step 6535/7134 completed (loss: 0.009760498069226742, acc: 0.9976580739021301)
[2025-02-13 04:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19][root][INFO] - Training Epoch: 2/2, step 6536/7134 completed (loss: 0.010173510760068893, acc: 0.9980158805847168)
[2025-02-13 04:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:20][root][INFO] - Training Epoch: 2/2, step 6537/7134 completed (loss: 0.04092855378985405, acc: 0.982300877571106)
[2025-02-13 04:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:20][root][INFO] - Training Epoch: 2/2, step 6538/7134 completed (loss: 0.0587308369576931, acc: 0.9857904314994812)
[2025-02-13 04:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:21][root][INFO] - Training Epoch: 2/2, step 6539/7134 completed (loss: 0.004796319641172886, acc: 1.0)
[2025-02-13 04:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:21][root][INFO] - Training Epoch: 2/2, step 6540/7134 completed (loss: 0.16060559451580048, acc: 0.9562841653823853)
[2025-02-13 04:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22][root][INFO] - Training Epoch: 2/2, step 6541/7134 completed (loss: 0.019429663196206093, acc: 0.9950980544090271)
[2025-02-13 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22][root][INFO] - Training Epoch: 2/2, step 6542/7134 completed (loss: 0.01790793053805828, acc: 0.996610164642334)
[2025-02-13 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22][root][INFO] - Training Epoch: 2/2, step 6543/7134 completed (loss: 0.0039104497991502285, acc: 1.0)
[2025-02-13 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23][root][INFO] - Training Epoch: 2/2, step 6544/7134 completed (loss: 0.03747902438044548, acc: 0.9900596141815186)
[2025-02-13 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23][root][INFO] - Training Epoch: 2/2, step 6545/7134 completed (loss: 0.01575195975601673, acc: 0.9962825179100037)
[2025-02-13 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:24][root][INFO] - Training Epoch: 2/2, step 6546/7134 completed (loss: 0.024136586114764214, acc: 0.9951140284538269)
[2025-02-13 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:24][root][INFO] - Training Epoch: 2/2, step 6547/7134 completed (loss: 0.07037553191184998, acc: 0.9812606573104858)
[2025-02-13 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25][root][INFO] - Training Epoch: 2/2, step 6548/7134 completed (loss: 0.0060995789244771, acc: 0.9980158805847168)
[2025-02-13 04:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25][root][INFO] - Training Epoch: 2/2, step 6549/7134 completed (loss: 0.059633783996105194, acc: 0.9883494973182678)
[2025-02-13 04:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25][root][INFO] - Training Epoch: 2/2, step 6550/7134 completed (loss: 0.054405394941568375, acc: 0.9861538410186768)
[2025-02-13 04:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:26][root][INFO] - Training Epoch: 2/2, step 6551/7134 completed (loss: 0.01650489866733551, acc: 0.9954441785812378)
[2025-02-13 04:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:26][root][INFO] - Training Epoch: 2/2, step 6552/7134 completed (loss: 0.01874917931854725, acc: 0.9960159659385681)
[2025-02-13 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27][root][INFO] - Training Epoch: 2/2, step 6553/7134 completed (loss: 0.01753552071750164, acc: 0.9936386942863464)
[2025-02-13 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27][root][INFO] - Training Epoch: 2/2, step 6554/7134 completed (loss: 0.02895646169781685, acc: 0.9956756830215454)
[2025-02-13 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28][root][INFO] - Training Epoch: 2/2, step 6555/7134 completed (loss: 0.03505762293934822, acc: 0.9899396300315857)
[2025-02-13 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28][root][INFO] - Training Epoch: 2/2, step 6556/7134 completed (loss: 0.033057115972042084, acc: 0.9915789365768433)
[2025-02-13 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:29][root][INFO] - Training Epoch: 2/2, step 6557/7134 completed (loss: 0.022784724831581116, acc: 0.992943525314331)
[2025-02-13 04:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:29][root][INFO] - Training Epoch: 2/2, step 6558/7134 completed (loss: 0.02954670786857605, acc: 0.991304337978363)
[2025-02-13 04:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30][root][INFO] - Training Epoch: 2/2, step 6559/7134 completed (loss: 0.015888722613453865, acc: 0.994452178478241)
[2025-02-13 04:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30][root][INFO] - Training Epoch: 2/2, step 6560/7134 completed (loss: 0.02136383019387722, acc: 0.9928656220436096)
[2025-02-13 04:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31][root][INFO] - Training Epoch: 2/2, step 6561/7134 completed (loss: 0.00837370753288269, acc: 0.9964912533760071)
[2025-02-13 04:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31][root][INFO] - Training Epoch: 2/2, step 6562/7134 completed (loss: 0.020452488213777542, acc: 0.9895366430282593)
[2025-02-13 04:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:32][root][INFO] - Training Epoch: 2/2, step 6563/7134 completed (loss: 0.021819673478603363, acc: 0.9930151104927063)
[2025-02-13 04:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:32][root][INFO] - Training Epoch: 2/2, step 6564/7134 completed (loss: 0.03546784073114395, acc: 0.9919028282165527)
[2025-02-13 04:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33][root][INFO] - Training Epoch: 2/2, step 6565/7134 completed (loss: 0.03457231819629669, acc: 0.9894291758537292)
[2025-02-13 04:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33][root][INFO] - Training Epoch: 2/2, step 6566/7134 completed (loss: 0.017413580790162086, acc: 0.9933523535728455)
[2025-02-13 04:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34][root][INFO] - Training Epoch: 2/2, step 6567/7134 completed (loss: 0.00958293303847313, acc: 0.996960461139679)
[2025-02-13 04:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34][root][INFO] - Training Epoch: 2/2, step 6568/7134 completed (loss: 0.010950826108455658, acc: 0.994955837726593)
[2025-02-13 04:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35][root][INFO] - Training Epoch: 2/2, step 6569/7134 completed (loss: 0.011071290820837021, acc: 0.9967032670974731)
[2025-02-13 04:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35][root][INFO] - Training Epoch: 2/2, step 6570/7134 completed (loss: 0.011671635322272778, acc: 0.996515691280365)
[2025-02-13 04:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35][root][INFO] - Training Epoch: 2/2, step 6571/7134 completed (loss: 0.0072465743869543076, acc: 0.9986910820007324)
[2025-02-13 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:36][root][INFO] - Training Epoch: 2/2, step 6572/7134 completed (loss: 0.032253678888082504, acc: 0.9900221824645996)
[2025-02-13 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:36][root][INFO] - Training Epoch: 2/2, step 6573/7134 completed (loss: 0.014544998295605183, acc: 0.993228018283844)
[2025-02-13 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:37][root][INFO] - Training Epoch: 2/2, step 6574/7134 completed (loss: 0.008114959113299847, acc: 0.9969325065612793)
[2025-02-13 04:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:37][root][INFO] - Training Epoch: 2/2, step 6575/7134 completed (loss: 0.017871921882033348, acc: 0.9922651648521423)
[2025-02-13 04:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:38][root][INFO] - Training Epoch: 2/2, step 6576/7134 completed (loss: 0.014689761213958263, acc: 0.9947478771209717)
[2025-02-13 04:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:38][root][INFO] - Training Epoch: 2/2, step 6577/7134 completed (loss: 0.041776180267333984, acc: 0.9873949289321899)
[2025-02-13 04:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:39][root][INFO] - Training Epoch: 2/2, step 6578/7134 completed (loss: 0.006846961565315723, acc: 0.9991103410720825)
[2025-02-13 04:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:39][root][INFO] - Training Epoch: 2/2, step 6579/7134 completed (loss: 0.01897723786532879, acc: 0.9927404522895813)
[2025-02-13 04:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:40][root][INFO] - Training Epoch: 2/2, step 6580/7134 completed (loss: 0.010337663814425468, acc: 0.99647057056427)
[2025-02-13 04:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:40][root][INFO] - Training Epoch: 2/2, step 6581/7134 completed (loss: 0.016549425199627876, acc: 0.9950799345970154)
[2025-02-13 04:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:41][root][INFO] - Training Epoch: 2/2, step 6582/7134 completed (loss: 0.02677731402218342, acc: 0.9913669228553772)
[2025-02-13 04:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:41][root][INFO] - Training Epoch: 2/2, step 6583/7134 completed (loss: 0.013998553156852722, acc: 0.9942792057991028)
[2025-02-13 04:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:42][root][INFO] - Training Epoch: 2/2, step 6584/7134 completed (loss: 0.033468976616859436, acc: 0.9914320707321167)
[2025-02-13 04:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:42][root][INFO] - Training Epoch: 2/2, step 6585/7134 completed (loss: 0.017885014414787292, acc: 0.9901315569877625)
[2025-02-13 04:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:42][root][INFO] - Training Epoch: 2/2, step 6586/7134 completed (loss: 0.020207446068525314, acc: 0.9961140155792236)
[2025-02-13 04:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:43][root][INFO] - Training Epoch: 2/2, step 6587/7134 completed (loss: 0.05610062927007675, acc: 0.9882075190544128)
[2025-02-13 04:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:43][root][INFO] - Training Epoch: 2/2, step 6588/7134 completed (loss: 0.04208965599536896, acc: 0.9911949634552002)
[2025-02-13 04:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:44][root][INFO] - Training Epoch: 2/2, step 6589/7134 completed (loss: 0.023516999557614326, acc: 0.9903181195259094)
[2025-02-13 04:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:44][root][INFO] - Training Epoch: 2/2, step 6590/7134 completed (loss: 0.029252510517835617, acc: 0.9918509721755981)
[2025-02-13 04:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:45][root][INFO] - Training Epoch: 2/2, step 6591/7134 completed (loss: 0.020101971924304962, acc: 0.9938042163848877)
[2025-02-13 04:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:45][root][INFO] - Training Epoch: 2/2, step 6592/7134 completed (loss: 0.019387776032090187, acc: 0.9920544624328613)
[2025-02-13 04:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:46][root][INFO] - Training Epoch: 2/2, step 6593/7134 completed (loss: 0.03911074250936508, acc: 0.9923896789550781)
[2025-02-13 04:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:46][root][INFO] - Training Epoch: 2/2, step 6594/7134 completed (loss: 0.021369969472289085, acc: 0.9941314458847046)
[2025-02-13 04:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:47][root][INFO] - Training Epoch: 2/2, step 6595/7134 completed (loss: 0.03817025572061539, acc: 0.9925816059112549)
[2025-02-13 04:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:47][root][INFO] - Training Epoch: 2/2, step 6596/7134 completed (loss: 0.020151587203145027, acc: 0.9908952713012695)
[2025-02-13 04:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:47][root][INFO] - Training Epoch: 2/2, step 6597/7134 completed (loss: 0.02188783511519432, acc: 0.9941037893295288)
[2025-02-13 04:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:48][root][INFO] - Training Epoch: 2/2, step 6598/7134 completed (loss: 0.024402759969234467, acc: 0.990604043006897)
[2025-02-13 04:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:48][root][INFO] - Training Epoch: 2/2, step 6599/7134 completed (loss: 0.034867748618125916, acc: 0.987922728061676)
[2025-02-13 04:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:49][root][INFO] - Training Epoch: 2/2, step 6600/7134 completed (loss: 0.024953795596957207, acc: 0.9920760989189148)
[2025-02-13 04:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:49][root][INFO] - Training Epoch: 2/2, step 6601/7134 completed (loss: 0.036085765808820724, acc: 0.9917840361595154)
[2025-02-13 04:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:50][root][INFO] - Training Epoch: 2/2, step 6602/7134 completed (loss: 0.010583150200545788, acc: 0.9963898658752441)
[2025-02-13 04:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:50][root][INFO] - Training Epoch: 2/2, step 6603/7134 completed (loss: 0.011032612062990665, acc: 0.9943898916244507)
[2025-02-13 04:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:51][root][INFO] - Training Epoch: 2/2, step 6604/7134 completed (loss: 0.024413922801613808, acc: 0.9971264600753784)
[2025-02-13 04:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:51][root][INFO] - Training Epoch: 2/2, step 6605/7134 completed (loss: 0.08229079097509384, acc: 0.977869987487793)
[2025-02-13 04:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:51][root][INFO] - Training Epoch: 2/2, step 6606/7134 completed (loss: 0.03450543060898781, acc: 0.9924242496490479)
[2025-02-13 04:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:52][root][INFO] - Training Epoch: 2/2, step 6607/7134 completed (loss: 0.05754592642188072, acc: 0.9867021441459656)
[2025-02-13 04:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:52][root][INFO] - Training Epoch: 2/2, step 6608/7134 completed (loss: 0.015173510648310184, acc: 0.9933599233627319)
[2025-02-13 04:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:53][root][INFO] - Training Epoch: 2/2, step 6609/7134 completed (loss: 0.027582425624132156, acc: 0.9902200698852539)
[2025-02-13 04:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:53][root][INFO] - Training Epoch: 2/2, step 6610/7134 completed (loss: 0.053553320467472076, acc: 0.986522912979126)
[2025-02-13 04:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:54][root][INFO] - Training Epoch: 2/2, step 6611/7134 completed (loss: 0.06932578235864639, acc: 0.9800000190734863)
[2025-02-13 04:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:54][root][INFO] - Training Epoch: 2/2, step 6612/7134 completed (loss: 0.037708960473537445, acc: 0.9918319582939148)
[2025-02-13 04:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:55][root][INFO] - Training Epoch: 2/2, step 6613/7134 completed (loss: 0.031757958233356476, acc: 0.9895833134651184)
[2025-02-13 04:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:55][root][INFO] - Training Epoch: 2/2, step 6614/7134 completed (loss: 0.047100357711315155, acc: 0.9900990128517151)
[2025-02-13 04:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:56][root][INFO] - Training Epoch: 2/2, step 6615/7134 completed (loss: 0.016126293689012527, acc: 0.9943422675132751)
[2025-02-13 04:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:56][root][INFO] - Training Epoch: 2/2, step 6616/7134 completed (loss: 0.021052878350019455, acc: 0.9906103014945984)
[2025-02-13 04:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:56][root][INFO] - Training Epoch: 2/2, step 6617/7134 completed (loss: 0.023455874994397163, acc: 0.9956772327423096)
[2025-02-13 04:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:57][root][INFO] - Training Epoch: 2/2, step 6618/7134 completed (loss: 0.011368968524038792, acc: 0.9955423474311829)
[2025-02-13 04:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:57][root][INFO] - Training Epoch: 2/2, step 6619/7134 completed (loss: 0.017324555665254593, acc: 0.9943946003913879)
[2025-02-13 04:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:58][root][INFO] - Training Epoch: 2/2, step 6620/7134 completed (loss: 0.0206596702337265, acc: 0.993122398853302)
[2025-02-13 04:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:58][root][INFO] - Training Epoch: 2/2, step 6621/7134 completed (loss: 0.02865629829466343, acc: 0.99190753698349)
[2025-02-13 04:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:59][root][INFO] - Training Epoch: 2/2, step 6622/7134 completed (loss: 0.027408922091126442, acc: 0.9890710115432739)
[2025-02-13 04:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:59][root][INFO] - Training Epoch: 2/2, step 6623/7134 completed (loss: 0.012296359986066818, acc: 0.99622642993927)
[2025-02-13 04:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:00][root][INFO] - Training Epoch: 2/2, step 6624/7134 completed (loss: 0.0156831257045269, acc: 0.9947916865348816)
[2025-02-13 04:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:00][root][INFO] - Training Epoch: 2/2, step 6625/7134 completed (loss: 0.019855918362736702, acc: 0.9917469024658203)
[2025-02-13 04:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:01][root][INFO] - Training Epoch: 2/2, step 6626/7134 completed (loss: 0.006288087461143732, acc: 0.9975757598876953)
[2025-02-13 04:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:01][root][INFO] - Training Epoch: 2/2, step 6627/7134 completed (loss: 0.017570318654179573, acc: 0.9924324154853821)
[2025-02-13 04:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:01][root][INFO] - Training Epoch: 2/2, step 6628/7134 completed (loss: 0.033970754593610764, acc: 0.9917159676551819)
[2025-02-13 04:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:02][root][INFO] - Training Epoch: 2/2, step 6629/7134 completed (loss: 0.051205042749643326, acc: 0.9884467124938965)
[2025-02-13 04:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:02][root][INFO] - Training Epoch: 2/2, step 6630/7134 completed (loss: 0.02983630821108818, acc: 0.9899665713310242)
[2025-02-13 04:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:03][root][INFO] - Training Epoch: 2/2, step 6631/7134 completed (loss: 0.015822559595108032, acc: 0.993446946144104)
[2025-02-13 04:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:03][root][INFO] - Training Epoch: 2/2, step 6632/7134 completed (loss: 0.024505428969860077, acc: 0.9943757057189941)
[2025-02-13 04:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:04][root][INFO] - Training Epoch: 2/2, step 6633/7134 completed (loss: 0.034531787037849426, acc: 0.9902439117431641)
[2025-02-13 04:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:04][root][INFO] - Training Epoch: 2/2, step 6634/7134 completed (loss: 0.023372061550617218, acc: 0.989266574382782)
[2025-02-13 04:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:05][root][INFO] - Training Epoch: 2/2, step 6635/7134 completed (loss: 0.06215471774339676, acc: 0.9846547245979309)
[2025-02-13 04:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:05][root][INFO] - Training Epoch: 2/2, step 6636/7134 completed (loss: 0.025665374472737312, acc: 0.9885844588279724)
[2025-02-13 04:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:06][root][INFO] - Training Epoch: 2/2, step 6637/7134 completed (loss: 0.017448177561163902, acc: 0.9956331849098206)
[2025-02-13 04:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:06][root][INFO] - Training Epoch: 2/2, step 6638/7134 completed (loss: 0.022354356944561005, acc: 0.9949173927307129)
[2025-02-13 04:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:06][root][INFO] - Training Epoch: 2/2, step 6639/7134 completed (loss: 0.011490113101899624, acc: 0.997019350528717)
[2025-02-13 04:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:07][root][INFO] - Training Epoch: 2/2, step 6640/7134 completed (loss: 0.012348680756986141, acc: 0.9954057931900024)
[2025-02-13 04:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:07][root][INFO] - Training Epoch: 2/2, step 6641/7134 completed (loss: 0.04598456248641014, acc: 0.9824150204658508)
[2025-02-13 04:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:08][root][INFO] - Training Epoch: 2/2, step 6642/7134 completed (loss: 0.0203259214758873, acc: 0.9939758777618408)
[2025-02-13 04:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:08][root][INFO] - Training Epoch: 2/2, step 6643/7134 completed (loss: 0.010764360427856445, acc: 0.997357964515686)
[2025-02-13 04:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:09][root][INFO] - Training Epoch: 2/2, step 6644/7134 completed (loss: 0.0332426019012928, acc: 0.9949135184288025)
[2025-02-13 04:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:09][root][INFO] - Training Epoch: 2/2, step 6645/7134 completed (loss: 0.013252107426524162, acc: 0.996314525604248)
[2025-02-13 04:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:10][root][INFO] - Training Epoch: 2/2, step 6646/7134 completed (loss: 0.022373242303729057, acc: 0.9944933652877808)
[2025-02-13 04:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:10][root][INFO] - Training Epoch: 2/2, step 6647/7134 completed (loss: 0.018955718725919724, acc: 0.9945651888847351)
[2025-02-13 04:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:11][root][INFO] - Training Epoch: 2/2, step 6648/7134 completed (loss: 0.0529758557677269, acc: 0.9807999730110168)
[2025-02-13 04:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:11][root][INFO] - Training Epoch: 2/2, step 6649/7134 completed (loss: 0.02952967956662178, acc: 0.990554928779602)
[2025-02-13 04:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:12][root][INFO] - Training Epoch: 2/2, step 6650/7134 completed (loss: 0.009846636094152927, acc: 0.9933035969734192)
[2025-02-13 04:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:12][root][INFO] - Training Epoch: 2/2, step 6651/7134 completed (loss: 0.022733112797141075, acc: 0.9959072470664978)
[2025-02-13 04:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:12][root][INFO] - Training Epoch: 2/2, step 6652/7134 completed (loss: 0.019813286140561104, acc: 0.9963768124580383)
[2025-02-13 04:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:13][root][INFO] - Training Epoch: 2/2, step 6653/7134 completed (loss: 0.03310384228825569, acc: 0.9901424050331116)
[2025-02-13 04:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:13][root][INFO] - Training Epoch: 2/2, step 6654/7134 completed (loss: 0.05719354376196861, acc: 0.9891473054885864)
[2025-02-13 04:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:14][root][INFO] - Training Epoch: 2/2, step 6655/7134 completed (loss: 0.0206710584461689, acc: 0.9925558567047119)
[2025-02-13 04:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:14][root][INFO] - Training Epoch: 2/2, step 6656/7134 completed (loss: 0.026455437764525414, acc: 0.9936386942863464)
[2025-02-13 04:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:15][root][INFO] - Training Epoch: 2/2, step 6657/7134 completed (loss: 0.027776090428233147, acc: 0.9932249188423157)
[2025-02-13 04:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:15][root][INFO] - Training Epoch: 2/2, step 6658/7134 completed (loss: 0.009563704952597618, acc: 0.9987096786499023)
[2025-02-13 04:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:16][root][INFO] - Training Epoch: 2/2, step 6659/7134 completed (loss: 0.010349114425480366, acc: 0.9973958134651184)
[2025-02-13 04:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:16][root][INFO] - Training Epoch: 2/2, step 6660/7134 completed (loss: 0.008793980814516544, acc: 0.9990079402923584)
[2025-02-13 04:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:17][root][INFO] - Training Epoch: 2/2, step 6661/7134 completed (loss: 0.018332820385694504, acc: 0.9925742745399475)
[2025-02-13 04:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:17][root][INFO] - Training Epoch: 2/2, step 6662/7134 completed (loss: 0.05362265557050705, acc: 0.9828326106071472)
[2025-02-13 04:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:18][root][INFO] - Training Epoch: 2/2, step 6663/7134 completed (loss: 0.027540650218725204, acc: 0.9923497438430786)
[2025-02-13 04:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:18][root][INFO] - Training Epoch: 2/2, step 6664/7134 completed (loss: 0.02439074218273163, acc: 0.990774929523468)
[2025-02-13 04:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:18][root][INFO] - Training Epoch: 2/2, step 6665/7134 completed (loss: 0.04355434700846672, acc: 0.9837662577629089)
[2025-02-13 04:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:19][root][INFO] - Training Epoch: 2/2, step 6666/7134 completed (loss: 0.016598211601376534, acc: 0.9961928725242615)
[2025-02-13 04:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:19][root][INFO] - Training Epoch: 2/2, step 6667/7134 completed (loss: 0.03721607103943825, acc: 0.9876325130462646)
[2025-02-13 04:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:20][root][INFO] - Training Epoch: 2/2, step 6668/7134 completed (loss: 0.004253899212926626, acc: 1.0)
[2025-02-13 04:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:20][root][INFO] - Training Epoch: 2/2, step 6669/7134 completed (loss: 0.010968147777020931, acc: 0.9932885766029358)
[2025-02-13 04:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:20][root][INFO] - Training Epoch: 2/2, step 6670/7134 completed (loss: 0.008995609357953072, acc: 0.9983277320861816)
[2025-02-13 04:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:21][root][INFO] - Training Epoch: 2/2, step 6671/7134 completed (loss: 0.006923569831997156, acc: 0.9985337257385254)
[2025-02-13 04:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:21][root][INFO] - Training Epoch: 2/2, step 6672/7134 completed (loss: 0.014317109249532223, acc: 0.9943714737892151)
[2025-02-13 04:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:22][root][INFO] - Training Epoch: 2/2, step 6673/7134 completed (loss: 0.015935396775603294, acc: 0.9955947399139404)
[2025-02-13 04:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:22][root][INFO] - Training Epoch: 2/2, step 6674/7134 completed (loss: 0.005555083975195885, acc: 1.0)
[2025-02-13 04:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:23][root][INFO] - Training Epoch: 2/2, step 6675/7134 completed (loss: 0.010426288470625877, acc: 0.9975308775901794)
[2025-02-13 04:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:23][root][INFO] - Training Epoch: 2/2, step 6676/7134 completed (loss: 0.008111100643873215, acc: 0.9952267408370972)
[2025-02-13 04:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:23][root][INFO] - Training Epoch: 2/2, step 6677/7134 completed (loss: 0.055572912096977234, acc: 0.9847328066825867)
[2025-02-13 04:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:24][root][INFO] - Training Epoch: 2/2, step 6678/7134 completed (loss: 0.011691521853208542, acc: 0.9964285492897034)
[2025-02-13 04:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:24][root][INFO] - Training Epoch: 2/2, step 6679/7134 completed (loss: 0.013534171506762505, acc: 0.9956331849098206)
[2025-02-13 04:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:25][root][INFO] - Training Epoch: 2/2, step 6680/7134 completed (loss: 0.0019980694632977247, acc: 1.0)
[2025-02-13 04:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:25][root][INFO] - Training Epoch: 2/2, step 6681/7134 completed (loss: 0.0026919820811599493, acc: 1.0)
[2025-02-13 04:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:26][root][INFO] - Training Epoch: 2/2, step 6682/7134 completed (loss: 0.04037892445921898, acc: 0.9929906725883484)
[2025-02-13 04:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:26][root][INFO] - Training Epoch: 2/2, step 6683/7134 completed (loss: 0.013785197399556637, acc: 0.9972527623176575)
[2025-02-13 04:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:26][root][INFO] - Training Epoch: 2/2, step 6684/7134 completed (loss: 0.027629978954792023, acc: 0.9910714030265808)
[2025-02-13 04:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:27][root][INFO] - Training Epoch: 2/2, step 6685/7134 completed (loss: 0.034442465752363205, acc: 0.9923664331436157)
[2025-02-13 04:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:27][root][INFO] - Training Epoch: 2/2, step 6686/7134 completed (loss: 0.05698840320110321, acc: 0.9876033067703247)
[2025-02-13 04:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:28][root][INFO] - Training Epoch: 2/2, step 6687/7134 completed (loss: 0.027388112619519234, acc: 0.9876543283462524)
[2025-02-13 04:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:28][root][INFO] - Training Epoch: 2/2, step 6688/7134 completed (loss: 0.03482711687684059, acc: 0.9879724979400635)
[2025-02-13 04:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:28][root][INFO] - Training Epoch: 2/2, step 6689/7134 completed (loss: 0.01187826320528984, acc: 0.9920791983604431)
[2025-02-13 04:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:29][root][INFO] - Training Epoch: 2/2, step 6690/7134 completed (loss: 0.03389209881424904, acc: 0.9878934621810913)
[2025-02-13 04:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:29][root][INFO] - Training Epoch: 2/2, step 6691/7134 completed (loss: 0.010409090667963028, acc: 0.9967266917228699)
[2025-02-13 04:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:30][root][INFO] - Training Epoch: 2/2, step 6692/7134 completed (loss: 0.016132183372974396, acc: 0.991525411605835)
[2025-02-13 04:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:30][root][INFO] - Training Epoch: 2/2, step 6693/7134 completed (loss: 0.012829367071390152, acc: 0.9966996908187866)
[2025-02-13 04:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:31][root][INFO] - Training Epoch: 2/2, step 6694/7134 completed (loss: 0.014960382133722305, acc: 0.9969419240951538)
[2025-02-13 04:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:31][root][INFO] - Training Epoch: 2/2, step 6695/7134 completed (loss: 0.0251547209918499, acc: 0.9931600689888)
[2025-02-13 04:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:31][root][INFO] - Training Epoch: 2/2, step 6696/7134 completed (loss: 0.045010119676589966, acc: 0.9845201373100281)
[2025-02-13 04:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:32][root][INFO] - Training Epoch: 2/2, step 6697/7134 completed (loss: 0.04917619004845619, acc: 0.9852458834648132)
[2025-02-13 04:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:32][root][INFO] - Training Epoch: 2/2, step 6698/7134 completed (loss: 0.010678837075829506, acc: 0.9955489635467529)
[2025-02-13 04:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:33][root][INFO] - Training Epoch: 2/2, step 6699/7134 completed (loss: 0.018211454153060913, acc: 0.9954441785812378)
[2025-02-13 04:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:33][root][INFO] - Training Epoch: 2/2, step 6700/7134 completed (loss: 0.04999440163373947, acc: 0.9834558963775635)
[2025-02-13 04:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:34][root][INFO] - Training Epoch: 2/2, step 6701/7134 completed (loss: 0.07108284533023834, acc: 0.9801587462425232)
[2025-02-13 04:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:34][root][INFO] - Training Epoch: 2/2, step 6702/7134 completed (loss: 0.02077344059944153, acc: 0.994106113910675)
[2025-02-13 04:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:34][root][INFO] - Training Epoch: 2/2, step 6703/7134 completed (loss: 0.04867970198392868, acc: 0.9901315569877625)
[2025-02-13 04:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:35][root][INFO] - Training Epoch: 2/2, step 6704/7134 completed (loss: 0.05951200798153877, acc: 0.9844760894775391)
[2025-02-13 04:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:35][root][INFO] - Training Epoch: 2/2, step 6705/7134 completed (loss: 0.02957070991396904, acc: 0.9908972978591919)
[2025-02-13 04:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:36][root][INFO] - Training Epoch: 2/2, step 6706/7134 completed (loss: 0.015448550693690777, acc: 0.9938016533851624)
[2025-02-13 04:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:36][root][INFO] - Training Epoch: 2/2, step 6707/7134 completed (loss: 0.04078835994005203, acc: 0.9861932992935181)
[2025-02-13 04:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:37][root][INFO] - Training Epoch: 2/2, step 6708/7134 completed (loss: 0.017285283654928207, acc: 0.9966996908187866)
[2025-02-13 04:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:37][root][INFO] - Training Epoch: 2/2, step 6709/7134 completed (loss: 0.015333369374275208, acc: 0.9940119981765747)
[2025-02-13 04:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:37][root][INFO] - Training Epoch: 2/2, step 6710/7134 completed (loss: 0.03982049971818924, acc: 0.9890310764312744)
[2025-02-13 04:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:38][root][INFO] - Training Epoch: 2/2, step 6711/7134 completed (loss: 0.00881213415414095, acc: 0.9984126687049866)
[2025-02-13 04:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:38][root][INFO] - Training Epoch: 2/2, step 6712/7134 completed (loss: 0.040701236575841904, acc: 0.9909228682518005)
[2025-02-13 04:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:39][root][INFO] - Training Epoch: 2/2, step 6713/7134 completed (loss: 0.03712054714560509, acc: 0.9884225726127625)
[2025-02-13 04:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:39][root][INFO] - Training Epoch: 2/2, step 6714/7134 completed (loss: 0.020974837243556976, acc: 0.9925705790519714)
[2025-02-13 04:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:40][root][INFO] - Training Epoch: 2/2, step 6715/7134 completed (loss: 0.04846009612083435, acc: 0.9843013882637024)
[2025-02-13 04:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:40][root][INFO] - Training Epoch: 2/2, step 6716/7134 completed (loss: 0.009834456257522106, acc: 0.9970059990882874)
[2025-02-13 04:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:41][root][INFO] - Training Epoch: 2/2, step 6717/7134 completed (loss: 0.021146418526768684, acc: 0.990338146686554)
[2025-02-13 04:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:41][root][INFO] - Training Epoch: 2/2, step 6718/7134 completed (loss: 0.03431607410311699, acc: 0.989393949508667)
[2025-02-13 04:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:41][root][INFO] - Training Epoch: 2/2, step 6719/7134 completed (loss: 0.014364532195031643, acc: 0.9950980544090271)
[2025-02-13 04:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:42][root][INFO] - Training Epoch: 2/2, step 6720/7134 completed (loss: 0.04726974293589592, acc: 0.9874301552772522)
[2025-02-13 04:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:42][root][INFO] - Training Epoch: 2/2, step 6721/7134 completed (loss: 0.060293201357126236, acc: 0.9843993782997131)
[2025-02-13 04:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:43][root][INFO] - Training Epoch: 2/2, step 6722/7134 completed (loss: 0.05056092143058777, acc: 0.9841827750205994)
[2025-02-13 04:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:43][root][INFO] - Training Epoch: 2/2, step 6723/7134 completed (loss: 0.019276758655905724, acc: 0.9958333373069763)
[2025-02-13 04:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:44][root][INFO] - Training Epoch: 2/2, step 6724/7134 completed (loss: 0.026402335613965988, acc: 0.9926793575286865)
[2025-02-13 04:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:44][root][INFO] - Training Epoch: 2/2, step 6725/7134 completed (loss: 0.019477371126413345, acc: 0.9914675951004028)
[2025-02-13 04:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:44][root][INFO] - Training Epoch: 2/2, step 6726/7134 completed (loss: 0.03135696053504944, acc: 0.9919678568840027)
[2025-02-13 04:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:45][root][INFO] - Training Epoch: 2/2, step 6727/7134 completed (loss: 0.0310995951294899, acc: 0.9911971688270569)
[2025-02-13 04:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:45][root][INFO] - Training Epoch: 2/2, step 6728/7134 completed (loss: 0.041883111000061035, acc: 0.9923195242881775)
[2025-02-13 04:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:46][root][INFO] - Training Epoch: 2/2, step 6729/7134 completed (loss: 0.01662929728627205, acc: 0.9959239363670349)
[2025-02-13 04:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:46][root][INFO] - Training Epoch: 2/2, step 6730/7134 completed (loss: 0.03214883431792259, acc: 0.9890109896659851)
[2025-02-13 04:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:47][root][INFO] - Training Epoch: 2/2, step 6731/7134 completed (loss: 0.018270304426550865, acc: 0.9954751133918762)
[2025-02-13 04:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:47][root][INFO] - Training Epoch: 2/2, step 6732/7134 completed (loss: 0.017775580286979675, acc: 0.9921383857727051)
[2025-02-13 04:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:47][root][INFO] - Training Epoch: 2/2, step 6733/7134 completed (loss: 0.027259977534413338, acc: 0.9886147975921631)
[2025-02-13 04:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:48][root][INFO] - Training Epoch: 2/2, step 6734/7134 completed (loss: 0.04000399634242058, acc: 0.9906542301177979)
[2025-02-13 04:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:48][root][INFO] - Training Epoch: 2/2, step 6735/7134 completed (loss: 0.03612486273050308, acc: 0.9892473220825195)
[2025-02-13 04:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:49][root][INFO] - Training Epoch: 2/2, step 6736/7134 completed (loss: 0.04039737209677696, acc: 0.9888579249382019)
[2025-02-13 04:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:49][root][INFO] - Training Epoch: 2/2, step 6737/7134 completed (loss: 0.047643110156059265, acc: 0.9854133129119873)
[2025-02-13 04:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:50][root][INFO] - Training Epoch: 2/2, step 6738/7134 completed (loss: 0.026635240763425827, acc: 0.9906716346740723)
[2025-02-13 04:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:50][root][INFO] - Training Epoch: 2/2, step 6739/7134 completed (loss: 0.02269255556166172, acc: 0.9926470518112183)
[2025-02-13 04:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:50][root][INFO] - Training Epoch: 2/2, step 6740/7134 completed (loss: 0.04329778254032135, acc: 0.9915110468864441)
[2025-02-13 04:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:51][root][INFO] - Training Epoch: 2/2, step 6741/7134 completed (loss: 0.015708915889263153, acc: 0.9912917017936707)
[2025-02-13 04:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:51][root][INFO] - Training Epoch: 2/2, step 6742/7134 completed (loss: 0.016362255439162254, acc: 0.9942280054092407)
[2025-02-13 04:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:52][root][INFO] - Training Epoch: 2/2, step 6743/7134 completed (loss: 0.008932952769100666, acc: 0.9972752332687378)
[2025-02-13 04:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:52][root][INFO] - Training Epoch: 2/2, step 6744/7134 completed (loss: 0.018634501844644547, acc: 0.9935897588729858)
[2025-02-13 04:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:53][root][INFO] - Training Epoch: 2/2, step 6745/7134 completed (loss: 0.007415862753987312, acc: 0.9986245036125183)
[2025-02-13 04:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:53][root][INFO] - Training Epoch: 2/2, step 6746/7134 completed (loss: 0.006149013992398977, acc: 1.0)
[2025-02-13 04:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:54][root][INFO] - Training Epoch: 2/2, step 6747/7134 completed (loss: 0.024050911888480186, acc: 0.9975154995918274)
[2025-02-13 04:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:54][root][INFO] - Training Epoch: 2/2, step 6748/7134 completed (loss: 0.01891346275806427, acc: 0.9954904317855835)
[2025-02-13 04:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:55][root][INFO] - Training Epoch: 2/2, step 6749/7134 completed (loss: 0.009512776508927345, acc: 0.9956427216529846)
[2025-02-13 04:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:55][root][INFO] - Training Epoch: 2/2, step 6750/7134 completed (loss: 0.014516961760818958, acc: 0.9965397715568542)
[2025-02-13 04:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:56][root][INFO] - Training Epoch: 2/2, step 6751/7134 completed (loss: 0.01119405496865511, acc: 0.9977169036865234)
[2025-02-13 04:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:56][root][INFO] - Training Epoch: 2/2, step 6752/7134 completed (loss: 0.015012847259640694, acc: 0.9970972537994385)
[2025-02-13 04:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:56][root][INFO] - Training Epoch: 2/2, step 6753/7134 completed (loss: 0.012023833580315113, acc: 0.997706413269043)
[2025-02-13 04:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:57][root][INFO] - Training Epoch: 2/2, step 6754/7134 completed (loss: 0.0038720739539712667, acc: 0.9987951517105103)
[2025-02-13 04:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:57][root][INFO] - Training Epoch: 2/2, step 6755/7134 completed (loss: 0.006611274555325508, acc: 0.9975369572639465)
[2025-02-13 04:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:58][root][INFO] - Training Epoch: 2/2, step 6756/7134 completed (loss: 0.002148794010281563, acc: 1.0)
[2025-02-13 04:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:58][root][INFO] - Training Epoch: 2/2, step 6757/7134 completed (loss: 0.004394332878291607, acc: 0.9984756112098694)
[2025-02-13 04:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:59][root][INFO] - Training Epoch: 2/2, step 6758/7134 completed (loss: 0.008095288649201393, acc: 0.9961190223693848)
[2025-02-13 04:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:59][root][INFO] - Training Epoch: 2/2, step 6759/7134 completed (loss: 0.019933681935071945, acc: 0.9944071769714355)
[2025-02-13 04:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:00][root][INFO] - Training Epoch: 2/2, step 6760/7134 completed (loss: 0.012379257008433342, acc: 0.9947506785392761)
[2025-02-13 04:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:00][root][INFO] - Training Epoch: 2/2, step 6761/7134 completed (loss: 0.033655013889074326, acc: 0.9891566038131714)
[2025-02-13 04:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:00][root][INFO] - Training Epoch: 2/2, step 6762/7134 completed (loss: 0.02561276964843273, acc: 0.9914215803146362)
[2025-02-13 04:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:01][root][INFO] - Training Epoch: 2/2, step 6763/7134 completed (loss: 0.00917841400951147, acc: 0.9974905848503113)
[2025-02-13 04:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:01][root][INFO] - Training Epoch: 2/2, step 6764/7134 completed (loss: 0.011866738088428974, acc: 0.9956896305084229)
[2025-02-13 04:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:02][root][INFO] - Training Epoch: 2/2, step 6765/7134 completed (loss: 0.012931838631629944, acc: 0.9933422207832336)
[2025-02-13 04:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:02][root][INFO] - Training Epoch: 2/2, step 6766/7134 completed (loss: 0.007717496249824762, acc: 0.9986206889152527)
[2025-02-13 04:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:03][root][INFO] - Training Epoch: 2/2, step 6767/7134 completed (loss: 0.032862339168787, acc: 0.9909909963607788)
[2025-02-13 04:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:03][root][INFO] - Training Epoch: 2/2, step 6768/7134 completed (loss: 0.007949944585561752, acc: 0.9980988502502441)
[2025-02-13 04:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:04][root][INFO] - Training Epoch: 2/2, step 6769/7134 completed (loss: 0.026847869157791138, acc: 0.9953703880310059)
[2025-02-13 04:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:04][root][INFO] - Training Epoch: 2/2, step 6770/7134 completed (loss: 0.02430506981909275, acc: 0.991946280002594)
[2025-02-13 04:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:04][root][INFO] - Training Epoch: 2/2, step 6771/7134 completed (loss: 0.018091564998030663, acc: 0.9968454241752625)
[2025-02-13 04:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:05][root][INFO] - Training Epoch: 2/2, step 6772/7134 completed (loss: 0.058515191078186035, acc: 0.9839572310447693)
[2025-02-13 04:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:05][root][INFO] - Training Epoch: 2/2, step 6773/7134 completed (loss: 0.016619261354207993, acc: 0.9936708807945251)
[2025-02-13 04:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:06][root][INFO] - Training Epoch: 2/2, step 6774/7134 completed (loss: 0.018558379262685776, acc: 0.993630588054657)
[2025-02-13 04:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:06][root][INFO] - Training Epoch: 2/2, step 6775/7134 completed (loss: 0.013017799705266953, acc: 0.9943289160728455)
[2025-02-13 04:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:07][root][INFO] - Training Epoch: 2/2, step 6776/7134 completed (loss: 0.025679048150777817, acc: 0.9893617033958435)
[2025-02-13 04:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:07][root][INFO] - Training Epoch: 2/2, step 6777/7134 completed (loss: 0.016100695356726646, acc: 0.9973154067993164)
[2025-02-13 04:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:08][root][INFO] - Training Epoch: 2/2, step 6778/7134 completed (loss: 0.025142034515738487, acc: 0.9925705790519714)
[2025-02-13 04:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:08][root][INFO] - Training Epoch: 2/2, step 6779/7134 completed (loss: 0.008154451847076416, acc: 0.9970149397850037)
[2025-02-13 04:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:08][root][INFO] - Training Epoch: 2/2, step 6780/7134 completed (loss: 0.019169988110661507, acc: 0.9970104694366455)
[2025-02-13 04:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:09][root][INFO] - Training Epoch: 2/2, step 6781/7134 completed (loss: 0.031097451224923134, acc: 0.992514967918396)
[2025-02-13 04:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:09][root][INFO] - Training Epoch: 2/2, step 6782/7134 completed (loss: 0.017853649333119392, acc: 0.9947916865348816)
[2025-02-13 04:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:10][root][INFO] - Training Epoch: 2/2, step 6783/7134 completed (loss: 0.03176668658852577, acc: 0.9884836673736572)
[2025-02-13 04:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:10][root][INFO] - Training Epoch: 2/2, step 6784/7134 completed (loss: 0.012084499932825565, acc: 0.9950082898139954)
[2025-02-13 04:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:11][root][INFO] - Training Epoch: 2/2, step 6785/7134 completed (loss: 0.007110825274139643, acc: 0.99842768907547)
[2025-02-13 04:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:11][root][INFO] - Training Epoch: 2/2, step 6786/7134 completed (loss: 0.0033311983570456505, acc: 0.9985611438751221)
[2025-02-13 04:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:11][root][INFO] - Training Epoch: 2/2, step 6787/7134 completed (loss: 0.013234368525445461, acc: 0.9947368502616882)
[2025-02-13 04:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:12][root][INFO] - Training Epoch: 2/2, step 6788/7134 completed (loss: 0.0073706479743123055, acc: 0.9982014298439026)
[2025-02-13 04:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:12][root][INFO] - Training Epoch: 2/2, step 6789/7134 completed (loss: 0.012925134971737862, acc: 0.9980545043945312)
[2025-02-13 04:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:13][root][INFO] - Training Epoch: 2/2, step 6790/7134 completed (loss: 0.030504046007990837, acc: 0.9942528605461121)
[2025-02-13 04:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:13][root][INFO] - Training Epoch: 2/2, step 6791/7134 completed (loss: 0.01928219385445118, acc: 0.9931856989860535)
[2025-02-13 04:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:14][root][INFO] - Training Epoch: 2/2, step 6792/7134 completed (loss: 0.01111891120672226, acc: 0.9981651306152344)
[2025-02-13 04:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:14][root][INFO] - Training Epoch: 2/2, step 6793/7134 completed (loss: 0.0049397824332118034, acc: 0.997890293598175)
[2025-02-13 04:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:15][root][INFO] - Training Epoch: 2/2, step 6794/7134 completed (loss: 0.016646478325128555, acc: 0.9983579516410828)
[2025-02-13 04:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:15][root][INFO] - Training Epoch: 2/2, step 6795/7134 completed (loss: 0.00943498034030199, acc: 0.998701274394989)
[2025-02-13 04:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:15][root][INFO] - Training Epoch: 2/2, step 6796/7134 completed (loss: 0.005695111583918333, acc: 0.9985337257385254)
[2025-02-13 04:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:16][root][INFO] - Training Epoch: 2/2, step 6797/7134 completed (loss: 0.022430678829550743, acc: 0.9927219748497009)
[2025-02-13 04:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:16][root][INFO] - Training Epoch: 2/2, step 6798/7134 completed (loss: 0.015575571916997433, acc: 0.9958677887916565)
[2025-02-13 04:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:17][root][INFO] - Training Epoch: 2/2, step 6799/7134 completed (loss: 0.0077143884263932705, acc: 0.99863201379776)
[2025-02-13 04:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:17][root][INFO] - Training Epoch: 2/2, step 6800/7134 completed (loss: 0.01660153642296791, acc: 0.9958563446998596)
[2025-02-13 04:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:18][root][INFO] - Training Epoch: 2/2, step 6801/7134 completed (loss: 0.0307108536362648, acc: 0.9921259880065918)
[2025-02-13 04:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:18][root][INFO] - Training Epoch: 2/2, step 6802/7134 completed (loss: 0.010639145039021969, acc: 0.9985358715057373)
[2025-02-13 04:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:18][root][INFO] - Training Epoch: 2/2, step 6803/7134 completed (loss: 0.005145041272044182, acc: 0.9987421631813049)
[2025-02-13 04:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:19][root][INFO] - Training Epoch: 2/2, step 6804/7134 completed (loss: 0.02563934586942196, acc: 0.996874988079071)
[2025-02-13 04:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:19][root][INFO] - Training Epoch: 2/2, step 6805/7134 completed (loss: 0.02847360633313656, acc: 0.9919871687889099)
[2025-02-13 04:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:20][root][INFO] - Training Epoch: 2/2, step 6806/7134 completed (loss: 0.032368045300245285, acc: 0.9936708807945251)
[2025-02-13 04:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:20][root][INFO] - Training Epoch: 2/2, step 6807/7134 completed (loss: 0.012135413475334644, acc: 0.9971264600753784)
[2025-02-13 04:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:21][root][INFO] - Training Epoch: 2/2, step 6808/7134 completed (loss: 0.01956111565232277, acc: 0.9952152967453003)
[2025-02-13 04:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:21][root][INFO] - Training Epoch: 2/2, step 6809/7134 completed (loss: 0.017324766144156456, acc: 0.9945799708366394)
[2025-02-13 04:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:22][root][INFO] - Training Epoch: 2/2, step 6810/7134 completed (loss: 0.011530333198606968, acc: 0.9973683953285217)
[2025-02-13 04:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:22][root][INFO] - Training Epoch: 2/2, step 6811/7134 completed (loss: 0.029661893844604492, acc: 0.9921362996101379)
[2025-02-13 04:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:22][root][INFO] - Training Epoch: 2/2, step 6812/7134 completed (loss: 0.009676375426352024, acc: 0.9983792304992676)
[2025-02-13 04:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:23][root][INFO] - Training Epoch: 2/2, step 6813/7134 completed (loss: 0.003632431384176016, acc: 1.0)
[2025-02-13 04:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:23][root][INFO] - Training Epoch: 2/2, step 6814/7134 completed (loss: 0.009852619841694832, acc: 0.9986559152603149)
[2025-02-13 04:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:24][root][INFO] - Training Epoch: 2/2, step 6815/7134 completed (loss: 0.013015243224799633, acc: 0.9973718523979187)
[2025-02-13 04:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:24][root][INFO] - Training Epoch: 2/2, step 6816/7134 completed (loss: 0.0194159634411335, acc: 0.9966555237770081)
[2025-02-13 04:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:25][root][INFO] - Training Epoch: 2/2, step 6817/7134 completed (loss: 0.03846155107021332, acc: 0.9863247871398926)
[2025-02-13 04:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:25][root][INFO] - Training Epoch: 2/2, step 6818/7134 completed (loss: 0.008600655011832714, acc: 0.995502233505249)
[2025-02-13 04:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:26][root][INFO] - Training Epoch: 2/2, step 6819/7134 completed (loss: 0.022185588255524635, acc: 0.9928057789802551)
[2025-02-13 04:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:26][root][INFO] - Training Epoch: 2/2, step 6820/7134 completed (loss: 0.010815664194524288, acc: 0.9981982111930847)
[2025-02-13 04:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:26][root][INFO] - Training Epoch: 2/2, step 6821/7134 completed (loss: 0.00510203605517745, acc: 0.9979798197746277)
[2025-02-13 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:27][root][INFO] - Training Epoch: 2/2, step 6822/7134 completed (loss: 0.03046576678752899, acc: 0.9930192232131958)
[2025-02-13 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:27][root][INFO] - Training Epoch: 2/2, step 6823/7134 completed (loss: 0.024352416396141052, acc: 0.9899857044219971)
[2025-02-13 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:28][root][INFO] - Training Epoch: 2/2, step 6824/7134 completed (loss: 0.04693644866347313, acc: 0.9818548560142517)
[2025-02-13 04:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:28][root][INFO] - Training Epoch: 2/2, step 6825/7134 completed (loss: 0.003987409640103579, acc: 1.0)
[2025-02-13 04:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:28][root][INFO] - Training Epoch: 2/2, step 6826/7134 completed (loss: 0.02135716937482357, acc: 0.9898989796638489)
[2025-02-13 04:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:29][root][INFO] - Training Epoch: 2/2, step 6827/7134 completed (loss: 0.09765303134918213, acc: 0.9755154848098755)
[2025-02-13 04:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:29][root][INFO] - Training Epoch: 2/2, step 6828/7134 completed (loss: 0.032359346747398376, acc: 0.989051103591919)
[2025-02-13 04:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:30][root][INFO] - Training Epoch: 2/2, step 6829/7134 completed (loss: 0.026712778955698013, acc: 0.9910979270935059)
[2025-02-13 04:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:30][root][INFO] - Training Epoch: 2/2, step 6830/7134 completed (loss: 0.018585622310638428, acc: 0.9934102296829224)
[2025-02-13 04:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:31][root][INFO] - Training Epoch: 2/2, step 6831/7134 completed (loss: 0.03122011013329029, acc: 0.9892473220825195)
[2025-02-13 04:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:31][root][INFO] - Training Epoch: 2/2, step 6832/7134 completed (loss: 0.01780090294778347, acc: 0.9944547414779663)
[2025-02-13 04:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:31][root][INFO] - Training Epoch: 2/2, step 6833/7134 completed (loss: 0.01963266171514988, acc: 0.9887459874153137)
[2025-02-13 04:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:32][root][INFO] - Training Epoch: 2/2, step 6834/7134 completed (loss: 0.018829233944416046, acc: 0.9931856989860535)
[2025-02-13 04:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:32][root][INFO] - Training Epoch: 2/2, step 6835/7134 completed (loss: 0.009689011611044407, acc: 0.9951298832893372)
[2025-02-13 04:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:33][root][INFO] - Training Epoch: 2/2, step 6836/7134 completed (loss: 0.023209577426314354, acc: 0.9899497628211975)
[2025-02-13 04:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:33][root][INFO] - Training Epoch: 2/2, step 6837/7134 completed (loss: 0.017866406589746475, acc: 0.9926380515098572)
[2025-02-13 04:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:34][root][INFO] - Training Epoch: 2/2, step 6838/7134 completed (loss: 0.057809796184301376, acc: 0.9829059839248657)
[2025-02-13 04:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:34][root][INFO] - Training Epoch: 2/2, step 6839/7134 completed (loss: 0.0681590661406517, acc: 0.9768977165222168)
[2025-02-13 04:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:34][root][INFO] - Training Epoch: 2/2, step 6840/7134 completed (loss: 0.06289904564619064, acc: 0.9799426794052124)
[2025-02-13 04:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:35][root][INFO] - Training Epoch: 2/2, step 6841/7134 completed (loss: 0.021330147981643677, acc: 0.9942280054092407)
[2025-02-13 04:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:35][root][INFO] - Training Epoch: 2/2, step 6842/7134 completed (loss: 0.01994359865784645, acc: 0.9937343597412109)
[2025-02-13 04:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:36][root][INFO] - Training Epoch: 2/2, step 6843/7134 completed (loss: 0.03056376799941063, acc: 0.9901356101036072)
[2025-02-13 04:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:36][root][INFO] - Training Epoch: 2/2, step 6844/7134 completed (loss: 0.017979120835661888, acc: 0.9930915236473083)
[2025-02-13 04:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:37][root][INFO] - Training Epoch: 2/2, step 6845/7134 completed (loss: 0.008288759738206863, acc: 0.9982993006706238)
[2025-02-13 04:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:37][root][INFO] - Training Epoch: 2/2, step 6846/7134 completed (loss: 0.01569933257997036, acc: 0.9957020282745361)
[2025-02-13 04:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:38][root][INFO] - Training Epoch: 2/2, step 6847/7134 completed (loss: 0.02430858090519905, acc: 0.9924242496490479)
[2025-02-13 04:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:38][root][INFO] - Training Epoch: 2/2, step 6848/7134 completed (loss: 0.00609933864325285, acc: 0.9972972869873047)
[2025-02-13 04:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:38][root][INFO] - Training Epoch: 2/2, step 6849/7134 completed (loss: 0.021925657987594604, acc: 0.9965635538101196)
[2025-02-13 04:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:39][root][INFO] - Training Epoch: 2/2, step 6850/7134 completed (loss: 0.01766751892864704, acc: 0.9954233169555664)
[2025-02-13 04:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:39][root][INFO] - Training Epoch: 2/2, step 6851/7134 completed (loss: 0.039887119084596634, acc: 0.9887164831161499)
[2025-02-13 04:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:40][root][INFO] - Training Epoch: 2/2, step 6852/7134 completed (loss: 0.011428611353039742, acc: 0.996503472328186)
[2025-02-13 04:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:40][root][INFO] - Training Epoch: 2/2, step 6853/7134 completed (loss: 0.029345862567424774, acc: 0.9930843710899353)
[2025-02-13 04:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:41][root][INFO] - Training Epoch: 2/2, step 6854/7134 completed (loss: 0.015640215948224068, acc: 0.9956616163253784)
[2025-02-13 04:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:41][root][INFO] - Training Epoch: 2/2, step 6855/7134 completed (loss: 0.00568510452285409, acc: 1.0)
[2025-02-13 04:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:41][root][INFO] - Training Epoch: 2/2, step 6856/7134 completed (loss: 0.001839157659560442, acc: 1.0)
[2025-02-13 04:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:42][root][INFO] - Training Epoch: 2/2, step 6857/7134 completed (loss: 0.011977327056229115, acc: 0.9941349029541016)
[2025-02-13 04:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:42][root][INFO] - Training Epoch: 2/2, step 6858/7134 completed (loss: 0.000737101654522121, acc: 1.0)
[2025-02-13 04:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:43][root][INFO] - Training Epoch: 2/2, step 6859/7134 completed (loss: 0.0034841245505958796, acc: 0.9987195730209351)
[2025-02-13 04:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:43][root][INFO] - Training Epoch: 2/2, step 6860/7134 completed (loss: 0.009937562048435211, acc: 0.9982269406318665)
[2025-02-13 04:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:44][root][INFO] - Training Epoch: 2/2, step 6861/7134 completed (loss: 0.009048635140061378, acc: 0.9965397715568542)
[2025-02-13 04:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:44][root][INFO] - Training Epoch: 2/2, step 6862/7134 completed (loss: 0.031213095411658287, acc: 0.9923809766769409)
[2025-02-13 04:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:44][root][INFO] - Training Epoch: 2/2, step 6863/7134 completed (loss: 0.008653650060296059, acc: 0.9971751570701599)
[2025-02-13 04:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:45][root][INFO] - Training Epoch: 2/2, step 6864/7134 completed (loss: 0.004831599071621895, acc: 0.9983360767364502)
[2025-02-13 04:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:45][root][INFO] - Training Epoch: 2/2, step 6865/7134 completed (loss: 0.017593368887901306, acc: 0.9938144087791443)
[2025-02-13 04:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:46][root][INFO] - Training Epoch: 2/2, step 6866/7134 completed (loss: 0.0005914538051001728, acc: 1.0)
[2025-02-13 04:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:46][root][INFO] - Training Epoch: 2/2, step 6867/7134 completed (loss: 0.00608537532389164, acc: 0.9981481432914734)
[2025-02-13 04:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:46][root][INFO] - Training Epoch: 2/2, step 6868/7134 completed (loss: 0.002104911021888256, acc: 1.0)
[2025-02-13 04:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:47][root][INFO] - Training Epoch: 2/2, step 6869/7134 completed (loss: 0.0037929352838546038, acc: 1.0)
[2025-02-13 04:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:47][root][INFO] - Training Epoch: 2/2, step 6870/7134 completed (loss: 0.016984721645712852, acc: 0.9967426657676697)
[2025-02-13 04:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:48][root][INFO] - Training Epoch: 2/2, step 6871/7134 completed (loss: 0.0007956160698086023, acc: 1.0)
[2025-02-13 04:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:48][root][INFO] - Training Epoch: 2/2, step 6872/7134 completed (loss: 0.0065053533762693405, acc: 0.9986149668693542)
[2025-02-13 04:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:49][root][INFO] - Training Epoch: 2/2, step 6873/7134 completed (loss: 0.009472274221479893, acc: 0.9971988797187805)
[2025-02-13 04:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:49][root][INFO] - Training Epoch: 2/2, step 6874/7134 completed (loss: 0.00999914389103651, acc: 0.9982269406318665)
[2025-02-13 04:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:49][root][INFO] - Training Epoch: 2/2, step 6875/7134 completed (loss: 0.005037033464759588, acc: 0.9984543919563293)
[2025-02-13 04:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:50][root][INFO] - Training Epoch: 2/2, step 6876/7134 completed (loss: 0.012176960706710815, acc: 0.9949238300323486)
[2025-02-13 04:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:50][root][INFO] - Training Epoch: 2/2, step 6877/7134 completed (loss: 0.014017871581017971, acc: 0.9957864880561829)
[2025-02-13 04:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:51][root][INFO] - Training Epoch: 2/2, step 6878/7134 completed (loss: 0.008032969199120998, acc: 0.9970015287399292)
[2025-02-13 04:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:51][root][INFO] - Training Epoch: 2/2, step 6879/7134 completed (loss: 0.006742272060364485, acc: 0.9979079365730286)
[2025-02-13 04:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:52][root][INFO] - Training Epoch: 2/2, step 6880/7134 completed (loss: 0.006568159908056259, acc: 0.9965075850486755)
[2025-02-13 04:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:52][root][INFO] - Training Epoch: 2/2, step 6881/7134 completed (loss: 0.03175047039985657, acc: 0.9927095770835876)
[2025-02-13 04:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:53][root][INFO] - Training Epoch: 2/2, step 6882/7134 completed (loss: 0.026975268498063087, acc: 0.9924812316894531)
[2025-02-13 04:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:53][root][INFO] - Training Epoch: 2/2, step 6883/7134 completed (loss: 0.005771486088633537, acc: 0.9983498454093933)
[2025-02-13 04:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:53][root][INFO] - Training Epoch: 2/2, step 6884/7134 completed (loss: 0.0343918614089489, acc: 0.996221661567688)
[2025-02-13 04:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:54][root][INFO] - Training Epoch: 2/2, step 6885/7134 completed (loss: 0.02598881907761097, acc: 0.9933035969734192)
[2025-02-13 04:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:54][root][INFO] - Training Epoch: 2/2, step 6886/7134 completed (loss: 0.014820756390690804, acc: 0.9936608672142029)
[2025-02-13 04:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:55][root][INFO] - Training Epoch: 2/2, step 6887/7134 completed (loss: 0.021723518148064613, acc: 0.9938398599624634)
[2025-02-13 04:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:55][root][INFO] - Training Epoch: 2/2, step 6888/7134 completed (loss: 0.01013175304979086, acc: 0.9933554530143738)
[2025-02-13 04:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:56][root][INFO] - Training Epoch: 2/2, step 6889/7134 completed (loss: 0.0036163264885544777, acc: 1.0)
[2025-02-13 04:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:56][root][INFO] - Training Epoch: 2/2, step 6890/7134 completed (loss: 0.013300280086696148, acc: 0.9947159886360168)
[2025-02-13 04:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:56][root][INFO] - Training Epoch: 2/2, step 6891/7134 completed (loss: 0.014064252376556396, acc: 0.995555579662323)
[2025-02-13 04:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:57][root][INFO] - Training Epoch: 2/2, step 6892/7134 completed (loss: 0.014896892011165619, acc: 0.9921875)
[2025-02-13 04:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:57][root][INFO] - Training Epoch: 2/2, step 6893/7134 completed (loss: 0.02571137435734272, acc: 0.9906542301177979)
[2025-02-13 04:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:58][root][INFO] - Training Epoch: 2/2, step 6894/7134 completed (loss: 0.025213045999407768, acc: 0.9907894730567932)
[2025-02-13 04:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:58][root][INFO] - Training Epoch: 2/2, step 6895/7134 completed (loss: 0.023747682571411133, acc: 0.9916067123413086)
[2025-02-13 04:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:59][root][INFO] - Training Epoch: 2/2, step 6896/7134 completed (loss: 0.014084145426750183, acc: 0.9942029118537903)
[2025-02-13 04:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:59][root][INFO] - Training Epoch: 2/2, step 6897/7134 completed (loss: 0.0163858812302351, acc: 0.9938650131225586)
[2025-02-13 04:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:59][root][INFO] - Training Epoch: 2/2, step 6898/7134 completed (loss: 0.006443875841796398, acc: 0.9986263513565063)
[2025-02-13 04:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:00][root][INFO] - Training Epoch: 2/2, step 6899/7134 completed (loss: 0.02078191749751568, acc: 0.9937185645103455)
[2025-02-13 04:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:00][root][INFO] - Training Epoch: 2/2, step 6900/7134 completed (loss: 0.026391368359327316, acc: 0.9929278492927551)
[2025-02-13 04:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:01][root][INFO] - Training Epoch: 2/2, step 6901/7134 completed (loss: 0.024381838738918304, acc: 0.9920739531517029)
[2025-02-13 04:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:01][root][INFO] - Training Epoch: 2/2, step 6902/7134 completed (loss: 0.020817676559090614, acc: 0.994452178478241)
[2025-02-13 04:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:02][root][INFO] - Training Epoch: 2/2, step 6903/7134 completed (loss: 0.012203915044665337, acc: 0.9974651336669922)
[2025-02-13 04:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:02][root][INFO] - Training Epoch: 2/2, step 6904/7134 completed (loss: 0.010001285001635551, acc: 0.9950248599052429)
[2025-02-13 04:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:03][root][INFO] - Training Epoch: 2/2, step 6905/7134 completed (loss: 0.01807379350066185, acc: 0.9948849081993103)
[2025-02-13 04:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:03][root][INFO] - Training Epoch: 2/2, step 6906/7134 completed (loss: 0.02318253181874752, acc: 0.9976047873497009)
[2025-02-13 04:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:04][root][INFO] - Training Epoch: 2/2, step 6907/7134 completed (loss: 0.02156321331858635, acc: 0.996277928352356)
[2025-02-13 04:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:04][root][INFO] - Training Epoch: 2/2, step 6908/7134 completed (loss: 0.015116266906261444, acc: 0.9948186278343201)
[2025-02-13 04:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:04][root][INFO] - Training Epoch: 2/2, step 6909/7134 completed (loss: 0.01533654984086752, acc: 0.996052622795105)
[2025-02-13 04:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:05][root][INFO] - Training Epoch: 2/2, step 6910/7134 completed (loss: 0.004159609787166119, acc: 1.0)
[2025-02-13 04:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:05][root][INFO] - Training Epoch: 2/2, step 6911/7134 completed (loss: 0.028529508039355278, acc: 0.9941860437393188)
[2025-02-13 04:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:06][root][INFO] - Training Epoch: 2/2, step 6912/7134 completed (loss: 0.023265479132533073, acc: 0.9975031018257141)
[2025-02-13 04:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:06][root][INFO] - Training Epoch: 2/2, step 6913/7134 completed (loss: 0.019139626994729042, acc: 0.9939098954200745)
[2025-02-13 04:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:07][root][INFO] - Training Epoch: 2/2, step 6914/7134 completed (loss: 0.05663894861936569, acc: 0.9882155060768127)
[2025-02-13 04:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:07][root][INFO] - Training Epoch: 2/2, step 6915/7134 completed (loss: 0.012019677087664604, acc: 0.9987046718597412)
[2025-02-13 04:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:08][root][INFO] - Training Epoch: 2/2, step 6916/7134 completed (loss: 0.0258505679666996, acc: 0.9950920343399048)
[2025-02-13 04:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:08][root][INFO] - Training Epoch: 2/2, step 6917/7134 completed (loss: 0.02665584720671177, acc: 0.9924242496490479)
[2025-02-13 04:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:08][root][INFO] - Training Epoch: 2/2, step 6918/7134 completed (loss: 0.009055874310433865, acc: 0.9976580739021301)
[2025-02-13 04:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:09][root][INFO] - Training Epoch: 2/2, step 6919/7134 completed (loss: 0.019004039466381073, acc: 0.9965397715568542)
[2025-02-13 04:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:09][root][INFO] - Training Epoch: 2/2, step 6920/7134 completed (loss: 0.0066038197837769985, acc: 0.9971510171890259)
[2025-02-13 04:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:10][root][INFO] - Training Epoch: 2/2, step 6921/7134 completed (loss: 0.040018171072006226, acc: 0.9907621145248413)
[2025-02-13 04:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:10][root][INFO] - Training Epoch: 2/2, step 6922/7134 completed (loss: 0.0322759635746479, acc: 0.9890795350074768)
[2025-02-13 04:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:11][root][INFO] - Training Epoch: 2/2, step 6923/7134 completed (loss: 0.03863770142197609, acc: 0.994163453578949)
[2025-02-13 04:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:11][root][INFO] - Training Epoch: 2/2, step 6924/7134 completed (loss: 0.01090206392109394, acc: 0.9959127902984619)
[2025-02-13 04:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:12][root][INFO] - Training Epoch: 2/2, step 6925/7134 completed (loss: 0.00437623867765069, acc: 1.0)
[2025-02-13 04:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:12][root][INFO] - Training Epoch: 2/2, step 6926/7134 completed (loss: 0.01244240440428257, acc: 0.9963833689689636)
[2025-02-13 04:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:13][root][INFO] - Training Epoch: 2/2, step 6927/7134 completed (loss: 0.028914103284478188, acc: 0.9940047860145569)
[2025-02-13 04:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:13][root][INFO] - Training Epoch: 2/2, step 6928/7134 completed (loss: 0.03103482536971569, acc: 0.9902152419090271)
[2025-02-13 04:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:13][root][INFO] - Training Epoch: 2/2, step 6929/7134 completed (loss: 0.018052933737635612, acc: 0.994350254535675)
[2025-02-13 04:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:14][root][INFO] - Training Epoch: 2/2, step 6930/7134 completed (loss: 0.018640238791704178, acc: 0.9900744557380676)
[2025-02-13 04:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:14][root][INFO] - Training Epoch: 2/2, step 6931/7134 completed (loss: 0.026608077809214592, acc: 0.9902439117431641)
[2025-02-13 04:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:15][root][INFO] - Training Epoch: 2/2, step 6932/7134 completed (loss: 0.029809674248099327, acc: 0.9911764860153198)
[2025-02-13 04:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:15][root][INFO] - Training Epoch: 2/2, step 6933/7134 completed (loss: 0.05705980584025383, acc: 0.9896507263183594)
[2025-02-13 04:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:16][root][INFO] - Training Epoch: 2/2, step 6934/7134 completed (loss: 0.0421961285173893, acc: 0.9903692007064819)
[2025-02-13 04:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:16][root][INFO] - Training Epoch: 2/2, step 6935/7134 completed (loss: 0.03518296033143997, acc: 0.9921466112136841)
[2025-02-13 04:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:17][root][INFO] - Training Epoch: 2/2, step 6936/7134 completed (loss: 0.009294137358665466, acc: 0.9959999918937683)
[2025-02-13 04:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:17][root][INFO] - Training Epoch: 2/2, step 6937/7134 completed (loss: 0.0879146158695221, acc: 0.9789842367172241)
[2025-02-13 04:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:17][root][INFO] - Training Epoch: 2/2, step 6938/7134 completed (loss: 0.07947130501270294, acc: 0.9765625)
[2025-02-13 04:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:18][root][INFO] - Training Epoch: 2/2, step 6939/7134 completed (loss: 0.05683081969618797, acc: 0.9902642369270325)
[2025-02-13 04:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:18][root][INFO] - Training Epoch: 2/2, step 6940/7134 completed (loss: 0.04536653310060501, acc: 0.9863013625144958)
[2025-02-13 04:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:19][root][INFO] - Training Epoch: 2/2, step 6941/7134 completed (loss: 0.0217296052724123, acc: 0.9965277910232544)
[2025-02-13 04:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:19][root][INFO] - Training Epoch: 2/2, step 6942/7134 completed (loss: 0.036530956625938416, acc: 0.9917808175086975)
[2025-02-13 04:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:20][root][INFO] - Training Epoch: 2/2, step 6943/7134 completed (loss: 0.03264434635639191, acc: 0.9895397424697876)
[2025-02-13 04:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:20][root][INFO] - Training Epoch: 2/2, step 6944/7134 completed (loss: 0.018428849056363106, acc: 0.9968152642250061)
[2025-02-13 04:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:21][root][INFO] - Training Epoch: 2/2, step 6945/7134 completed (loss: 0.04582180827856064, acc: 0.9912739992141724)
[2025-02-13 04:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:21][root][INFO] - Training Epoch: 2/2, step 6946/7134 completed (loss: 0.01602010428905487, acc: 0.9942693114280701)
[2025-02-13 04:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:21][root][INFO] - Training Epoch: 2/2, step 6947/7134 completed (loss: 0.022784708067774773, acc: 0.9956521987915039)
[2025-02-13 04:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:22][root][INFO] - Training Epoch: 2/2, step 6948/7134 completed (loss: 0.02729034423828125, acc: 0.9932998418807983)
[2025-02-13 04:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:22][root][INFO] - Training Epoch: 2/2, step 6949/7134 completed (loss: 0.023173334077000618, acc: 0.9935897588729858)
[2025-02-13 04:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:23][root][INFO] - Training Epoch: 2/2, step 6950/7134 completed (loss: 0.013584231957793236, acc: 0.9974489808082581)
[2025-02-13 04:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:23][root][INFO] - Training Epoch: 2/2, step 6951/7134 completed (loss: 0.011460378766059875, acc: 0.996610164642334)
[2025-02-13 04:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:23][root][INFO] - Training Epoch: 2/2, step 6952/7134 completed (loss: 0.013508244417607784, acc: 0.9929906725883484)
[2025-02-13 04:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:24][root][INFO] - Training Epoch: 2/2, step 6953/7134 completed (loss: 0.01080687902867794, acc: 0.9967213273048401)
[2025-02-13 04:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:24][root][INFO] - Training Epoch: 2/2, step 6954/7134 completed (loss: 0.04566197842359543, acc: 0.9936034083366394)
[2025-02-13 04:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:25][root][INFO] - Training Epoch: 2/2, step 6955/7134 completed (loss: 0.017497526481747627, acc: 0.9939024448394775)
[2025-02-13 04:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:25][root][INFO] - Training Epoch: 2/2, step 6956/7134 completed (loss: 0.00970485806465149, acc: 1.0)
[2025-02-13 04:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:26][root][INFO] - Training Epoch: 2/2, step 6957/7134 completed (loss: 0.011286299675703049, acc: 0.9960861206054688)
[2025-02-13 04:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:26][root][INFO] - Training Epoch: 2/2, step 6958/7134 completed (loss: 0.038888946175575256, acc: 0.988304078578949)
[2025-02-13 04:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:26][root][INFO] - Training Epoch: 2/2, step 6959/7134 completed (loss: 0.03980717435479164, acc: 0.9866962432861328)
[2025-02-13 04:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:27][root][INFO] - Training Epoch: 2/2, step 6960/7134 completed (loss: 0.03130842745304108, acc: 0.9902439117431641)
[2025-02-13 04:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:27][root][INFO] - Training Epoch: 2/2, step 6961/7134 completed (loss: 0.029977263882756233, acc: 0.9898989796638489)
[2025-02-13 04:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:28][root][INFO] - Training Epoch: 2/2, step 6962/7134 completed (loss: 0.028916891664266586, acc: 0.9947826266288757)
[2025-02-13 04:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:28][root][INFO] - Training Epoch: 2/2, step 6963/7134 completed (loss: 0.011238077655434608, acc: 0.9949495196342468)
[2025-02-13 04:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:29][root][INFO] - Training Epoch: 2/2, step 6964/7134 completed (loss: 0.00969709549099207, acc: 0.9981949329376221)
[2025-02-13 04:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:29][root][INFO] - Training Epoch: 2/2, step 6965/7134 completed (loss: 0.019556183367967606, acc: 0.9926739931106567)
[2025-02-13 04:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:29][root][INFO] - Training Epoch: 2/2, step 6966/7134 completed (loss: 0.02705121412873268, acc: 0.9900249242782593)
[2025-02-13 04:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:30][root][INFO] - Training Epoch: 2/2, step 6967/7134 completed (loss: 0.012372566387057304, acc: 0.9924585223197937)
[2025-02-13 04:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:30][root][INFO] - Training Epoch: 2/2, step 6968/7134 completed (loss: 0.018176227807998657, acc: 0.9931034445762634)
[2025-02-13 04:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:31][root][INFO] - Training Epoch: 2/2, step 6969/7134 completed (loss: 0.06568632274866104, acc: 0.9854469895362854)
[2025-02-13 04:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:31][root][INFO] - Training Epoch: 2/2, step 6970/7134 completed (loss: 0.03735042363405228, acc: 0.9936842322349548)
[2025-02-13 04:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:32][root][INFO] - Training Epoch: 2/2, step 6971/7134 completed (loss: 0.04227370768785477, acc: 0.9861830472946167)
[2025-02-13 04:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:32][root][INFO] - Training Epoch: 2/2, step 6972/7134 completed (loss: 0.011642200872302055, acc: 0.9964349269866943)
[2025-02-13 04:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:32][root][INFO] - Training Epoch: 2/2, step 6973/7134 completed (loss: 0.031238244846463203, acc: 0.9955947399139404)
[2025-02-13 04:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:33][root][INFO] - Training Epoch: 2/2, step 6974/7134 completed (loss: 0.02599475160241127, acc: 0.9931034445762634)
[2025-02-13 04:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:33][root][INFO] - Training Epoch: 2/2, step 6975/7134 completed (loss: 0.02538859099149704, acc: 0.9956616163253784)
[2025-02-13 04:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:34][root][INFO] - Training Epoch: 2/2, step 6976/7134 completed (loss: 0.031579501926898956, acc: 0.9896013736724854)
[2025-02-13 04:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:34][root][INFO] - Training Epoch: 2/2, step 6977/7134 completed (loss: 0.022362137213349342, acc: 0.9943609237670898)
[2025-02-13 04:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:34][root][INFO] - Training Epoch: 2/2, step 6978/7134 completed (loss: 0.015970442444086075, acc: 0.9967479705810547)
[2025-02-13 04:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:35][root][INFO] - Training Epoch: 2/2, step 6979/7134 completed (loss: 0.029257042333483696, acc: 0.9878048896789551)
[2025-02-13 04:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:35][root][INFO] - Training Epoch: 2/2, step 6980/7134 completed (loss: 0.07744229584932327, acc: 0.9773585200309753)
[2025-02-13 04:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:36][root][INFO] - Training Epoch: 2/2, step 6981/7134 completed (loss: 0.05482365936040878, acc: 0.9795657992362976)
[2025-02-13 04:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:36][root][INFO] - Training Epoch: 2/2, step 6982/7134 completed (loss: 0.053330451250076294, acc: 0.9878197312355042)
[2025-02-13 04:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:37][root][INFO] - Training Epoch: 2/2, step 6983/7134 completed (loss: 0.07163599878549576, acc: 0.9821958541870117)
[2025-02-13 04:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:37][root][INFO] - Training Epoch: 2/2, step 6984/7134 completed (loss: 0.02785203419625759, acc: 0.9946409463882446)
[2025-02-13 04:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:38][root][INFO] - Training Epoch: 2/2, step 6985/7134 completed (loss: 0.07292566448450089, acc: 0.983589768409729)
[2025-02-13 04:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:38][root][INFO] - Training Epoch: 2/2, step 6986/7134 completed (loss: 0.020487945526838303, acc: 0.9925705790519714)
[2025-02-13 04:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:39][root][INFO] - Training Epoch: 2/2, step 6987/7134 completed (loss: 0.034934889525175095, acc: 0.9878787994384766)
[2025-02-13 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:39][root][INFO] - Training Epoch: 2/2, step 6988/7134 completed (loss: 0.014066677540540695, acc: 0.9980119466781616)
[2025-02-13 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:39][root][INFO] - Training Epoch: 2/2, step 6989/7134 completed (loss: 0.02428256906569004, acc: 0.9890909194946289)
[2025-02-13 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:40][root][INFO] - Training Epoch: 2/2, step 6990/7134 completed (loss: 0.0026172322686761618, acc: 1.0)
[2025-02-13 04:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:40][root][INFO] - Training Epoch: 2/2, step 6991/7134 completed (loss: 0.026814023032784462, acc: 0.9924812316894531)
[2025-02-13 04:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:41][root][INFO] - Training Epoch: 2/2, step 6992/7134 completed (loss: 0.038765449076890945, acc: 0.9873684048652649)
[2025-02-13 04:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:41][root][INFO] - Training Epoch: 2/2, step 6993/7134 completed (loss: 0.0669390857219696, acc: 0.9819004535675049)
[2025-02-13 04:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:41][root][INFO] - Training Epoch: 2/2, step 6994/7134 completed (loss: 0.07493411749601364, acc: 0.9823434948921204)
[2025-02-13 04:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:42][root][INFO] - Training Epoch: 2/2, step 6995/7134 completed (loss: 0.02153441496193409, acc: 0.9934554696083069)
[2025-02-13 04:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:42][root][INFO] - Training Epoch: 2/2, step 6996/7134 completed (loss: 0.011396662332117558, acc: 0.9983766078948975)
[2025-02-13 04:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:43][root][INFO] - Training Epoch: 2/2, step 6997/7134 completed (loss: 0.023023497313261032, acc: 0.9906542301177979)
[2025-02-13 04:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:43][root][INFO] - Training Epoch: 2/2, step 6998/7134 completed (loss: 0.058289092034101486, acc: 0.9877451062202454)
[2025-02-13 04:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:44][root][INFO] - Training Epoch: 2/2, step 6999/7134 completed (loss: 0.01364264264702797, acc: 0.9947712421417236)
[2025-02-13 04:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:44][root][INFO] - Training Epoch: 2/2, step 7000/7134 completed (loss: 0.010855535976588726, acc: 0.9978813529014587)
[2025-02-13 04:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:44][root][INFO] - Training Epoch: 2/2, step 7001/7134 completed (loss: 0.06873191148042679, acc: 0.9864457845687866)
[2025-02-13 04:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:45][root][INFO] - Training Epoch: 2/2, step 7002/7134 completed (loss: 0.006851304788142443, acc: 0.9959431886672974)
[2025-02-13 04:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:45][root][INFO] - Training Epoch: 2/2, step 7003/7134 completed (loss: 0.01071644015610218, acc: 0.9956188201904297)
[2025-02-13 04:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:46][root][INFO] - Training Epoch: 2/2, step 7004/7134 completed (loss: 0.06642165035009384, acc: 0.9895012974739075)
[2025-02-13 04:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:46][root][INFO] - Training Epoch: 2/2, step 7005/7134 completed (loss: 0.032183937728405, acc: 0.9878542423248291)
[2025-02-13 04:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:47][root][INFO] - Training Epoch: 2/2, step 7006/7134 completed (loss: 0.017565416172146797, acc: 0.9931600689888)
[2025-02-13 04:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:47][root][INFO] - Training Epoch: 2/2, step 7007/7134 completed (loss: 0.018034448847174644, acc: 0.9955089688301086)
[2025-02-13 04:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:48][root][INFO] - Training Epoch: 2/2, step 7008/7134 completed (loss: 0.019645357504487038, acc: 0.995067834854126)
[2025-02-13 04:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:48][root][INFO] - Training Epoch: 2/2, step 7009/7134 completed (loss: 0.020651595667004585, acc: 0.9942129850387573)
[2025-02-13 04:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:49][root][INFO] - Training Epoch: 2/2, step 7010/7134 completed (loss: 0.016803976148366928, acc: 0.9969183206558228)
[2025-02-13 04:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:49][root][INFO] - Training Epoch: 2/2, step 7011/7134 completed (loss: 0.01772293820977211, acc: 0.9927884340286255)
[2025-02-13 04:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:50][root][INFO] - Training Epoch: 2/2, step 7012/7134 completed (loss: 0.011686775833368301, acc: 0.9970845580101013)
[2025-02-13 04:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:50][root][INFO] - Training Epoch: 2/2, step 7013/7134 completed (loss: 0.00636957585811615, acc: 0.9985590577125549)
[2025-02-13 04:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:50][root][INFO] - Training Epoch: 2/2, step 7014/7134 completed (loss: 0.012925581075251102, acc: 0.9967793822288513)
[2025-02-13 04:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:51][root][INFO] - Training Epoch: 2/2, step 7015/7134 completed (loss: 0.017992980778217316, acc: 0.9928443431854248)
[2025-02-13 04:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:51][root][INFO] - Training Epoch: 2/2, step 7016/7134 completed (loss: 0.03158218041062355, acc: 0.9855491518974304)
[2025-02-13 04:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:52][root][INFO] - Training Epoch: 2/2, step 7017/7134 completed (loss: 0.0321783572435379, acc: 0.9896265268325806)
[2025-02-13 04:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:52][root][INFO] - Training Epoch: 2/2, step 7018/7134 completed (loss: 0.036994483321905136, acc: 0.9953488111495972)
[2025-02-13 04:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:53][root][INFO] - Training Epoch: 2/2, step 7019/7134 completed (loss: 0.08665280044078827, acc: 0.9824561476707458)
[2025-02-13 04:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:53][root][INFO] - Training Epoch: 2/2, step 7020/7134 completed (loss: 0.016405025497078896, acc: 0.995768666267395)
[2025-02-13 04:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:53][root][INFO] - Training Epoch: 2/2, step 7021/7134 completed (loss: 0.016827896237373352, acc: 0.9965928196907043)
[2025-02-13 04:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:54][root][INFO] - Training Epoch: 2/2, step 7022/7134 completed (loss: 0.06489759683609009, acc: 0.9845678806304932)
[2025-02-13 04:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:54][root][INFO] - Training Epoch: 2/2, step 7023/7134 completed (loss: 0.05834389105439186, acc: 0.9809069037437439)
[2025-02-13 04:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:55][root][INFO] - Training Epoch: 2/2, step 7024/7134 completed (loss: 0.021534813567996025, acc: 0.993630588054657)
[2025-02-13 04:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:55][root][INFO] - Training Epoch: 2/2, step 7025/7134 completed (loss: 0.06491069495677948, acc: 0.9747126698493958)
[2025-02-13 04:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:55][root][INFO] - Training Epoch: 2/2, step 7026/7134 completed (loss: 0.051672909408807755, acc: 0.9845559597015381)
[2025-02-13 04:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:56][root][INFO] - Training Epoch: 2/2, step 7027/7134 completed (loss: 0.04780285432934761, acc: 0.9847561120986938)
[2025-02-13 04:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:56][root][INFO] - Training Epoch: 2/2, step 7028/7134 completed (loss: 0.016980545595288277, acc: 0.9927361011505127)
[2025-02-13 04:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:56][root][INFO] - Training Epoch: 2/2, step 7029/7134 completed (loss: 0.010072982870042324, acc: 0.9976798295974731)
[2025-02-13 04:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:57][root][INFO] - Training Epoch: 2/2, step 7030/7134 completed (loss: 0.02196674421429634, acc: 0.9952606558799744)
[2025-02-13 04:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:57][root][INFO] - Training Epoch: 2/2, step 7031/7134 completed (loss: 0.03374140337109566, acc: 0.9873096346855164)
[2025-02-13 04:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:58][root][INFO] - Training Epoch: 2/2, step 7032/7134 completed (loss: 0.03859533742070198, acc: 0.9918699264526367)
[2025-02-13 04:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:58][root][INFO] - Training Epoch: 2/2, step 7033/7134 completed (loss: 0.03074457310140133, acc: 0.9882698059082031)
[2025-02-13 04:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:58][root][INFO] - Training Epoch: 2/2, step 7034/7134 completed (loss: 0.05213163048028946, acc: 0.9858657121658325)
[2025-02-13 04:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:59][root][INFO] - Training Epoch: 2/2, step 7035/7134 completed (loss: 0.06473434716463089, acc: 0.9722222089767456)
[2025-02-13 04:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:59][root][INFO] - Training Epoch: 2/2, step 7036/7134 completed (loss: 0.01741643436253071, acc: 0.991769552230835)
[2025-02-13 04:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:00][root][INFO] - Training Epoch: 2/2, step 7037/7134 completed (loss: 0.022863803431391716, acc: 0.98828125)
[2025-02-13 04:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:00][root][INFO] - Training Epoch: 2/2, step 7038/7134 completed (loss: 0.09750130027532578, acc: 0.9762532711029053)
[2025-02-13 04:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:00][root][INFO] - Training Epoch: 2/2, step 7039/7134 completed (loss: 0.021866271272301674, acc: 0.9951691031455994)
[2025-02-13 04:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:01][root][INFO] - Training Epoch: 2/2, step 7040/7134 completed (loss: 0.07139905542135239, acc: 0.9683908224105835)
[2025-02-13 04:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:01][root][INFO] - Training Epoch: 2/2, step 7041/7134 completed (loss: 0.05216614156961441, acc: 0.9811320900917053)
[2025-02-13 04:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:01][root][INFO] - Training Epoch: 2/2, step 7042/7134 completed (loss: 0.024259045720100403, acc: 0.9876922965049744)
[2025-02-13 04:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:02][root][INFO] - Training Epoch: 2/2, step 7043/7134 completed (loss: 0.05085819587111473, acc: 0.981333315372467)
[2025-02-13 04:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:02][root][INFO] - Training Epoch: 2/2, step 7044/7134 completed (loss: 0.05629272386431694, acc: 0.9785932898521423)
[2025-02-13 04:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:03][root][INFO] - Training Epoch: 2/2, step 7045/7134 completed (loss: 0.021078400313854218, acc: 0.9931972622871399)
[2025-02-13 04:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:03][root][INFO] - Training Epoch: 2/2, step 7046/7134 completed (loss: 0.04390259459614754, acc: 0.9877049326896667)
[2025-02-13 04:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:03][root][INFO] - Training Epoch: 2/2, step 7047/7134 completed (loss: 0.04439903050661087, acc: 0.9821428656578064)
[2025-02-13 04:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:04][root][INFO] - Training Epoch: 2/2, step 7048/7134 completed (loss: 0.0017411446897312999, acc: 1.0)
[2025-02-13 04:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:04][root][INFO] - Training Epoch: 2/2, step 7049/7134 completed (loss: 0.011449005454778671, acc: 0.9973261952400208)
[2025-02-13 04:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:05][root][INFO] - Training Epoch: 2/2, step 7050/7134 completed (loss: 0.027350623160600662, acc: 0.9935232996940613)
[2025-02-13 04:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:05][root][INFO] - Training Epoch: 2/2, step 7051/7134 completed (loss: 0.01488061435520649, acc: 0.9962073564529419)
[2025-02-13 04:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:06][root][INFO] - Training Epoch: 2/2, step 7052/7134 completed (loss: 0.025746220722794533, acc: 0.9933422207832336)
[2025-02-13 04:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:06][root][INFO] - Training Epoch: 2/2, step 7053/7134 completed (loss: 0.01189847756177187, acc: 0.9939098954200745)
[2025-02-13 04:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:07][root][INFO] - Training Epoch: 2/2, step 7054/7134 completed (loss: 0.037262700498104095, acc: 0.9889240264892578)
[2025-02-13 04:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:07][root][INFO] - Training Epoch: 2/2, step 7055/7134 completed (loss: 0.033182453364133835, acc: 0.9885203838348389)
[2025-02-13 04:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:07][root][INFO] - Training Epoch: 2/2, step 7056/7134 completed (loss: 0.029185328632593155, acc: 0.9915966391563416)
[2025-02-13 04:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:08][root][INFO] - Training Epoch: 2/2, step 7057/7134 completed (loss: 0.07355944812297821, acc: 0.9863013625144958)
[2025-02-13 04:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:08][root][INFO] - Training Epoch: 2/2, step 7058/7134 completed (loss: 0.040727101266384125, acc: 0.9901574850082397)
[2025-02-13 04:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:09][root][INFO] - Training Epoch: 2/2, step 7059/7134 completed (loss: 0.018905170261859894, acc: 0.994413435459137)
[2025-02-13 04:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:09][root][INFO] - Training Epoch: 2/2, step 7060/7134 completed (loss: 0.04502638801932335, acc: 0.9903225898742676)
[2025-02-13 04:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:10][root][INFO] - Training Epoch: 2/2, step 7061/7134 completed (loss: 0.028364071622490883, acc: 0.9947916865348816)
[2025-02-13 04:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:10][root][INFO] - Training Epoch: 2/2, step 7062/7134 completed (loss: 0.05461544543504715, acc: 0.9844444394111633)
[2025-02-13 04:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:11][root][INFO] - Training Epoch: 2/2, step 7063/7134 completed (loss: 0.06303951889276505, acc: 0.9895424842834473)
[2025-02-13 04:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:11][root][INFO] - Training Epoch: 2/2, step 7064/7134 completed (loss: 0.01666659116744995, acc: 0.9940652847290039)
[2025-02-13 04:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:12][root][INFO] - Training Epoch: 2/2, step 7065/7134 completed (loss: 0.006791712250560522, acc: 0.9965237379074097)
[2025-02-13 04:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:12][root][INFO] - Training Epoch: 2/2, step 7066/7134 completed (loss: 0.016090260818600655, acc: 0.9952437281608582)
[2025-02-13 04:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:12][root][INFO] - Training Epoch: 2/2, step 7067/7134 completed (loss: 0.014408860355615616, acc: 0.993630588054657)
[2025-02-13 04:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:13][root][INFO] - Training Epoch: 2/2, step 7068/7134 completed (loss: 0.01906801201403141, acc: 0.9927623867988586)
[2025-02-13 04:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:13][root][INFO] - Training Epoch: 2/2, step 7069/7134 completed (loss: 0.01832447201013565, acc: 0.9950690269470215)
[2025-02-13 04:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:14][root][INFO] - Training Epoch: 2/2, step 7070/7134 completed (loss: 0.020289454609155655, acc: 0.9958847761154175)
[2025-02-13 04:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:14][root][INFO] - Training Epoch: 2/2, step 7071/7134 completed (loss: 0.02928316406905651, acc: 0.9962406158447266)
[2025-02-13 04:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:15][root][INFO] - Training Epoch: 2/2, step 7072/7134 completed (loss: 0.01592344418168068, acc: 0.996363639831543)
[2025-02-13 04:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:15][root][INFO] - Training Epoch: 2/2, step 7073/7134 completed (loss: 0.025674227625131607, acc: 0.9920364022254944)
[2025-02-13 04:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:16][root][INFO] - Training Epoch: 2/2, step 7074/7134 completed (loss: 0.024642912670969963, acc: 0.9980879426002502)
[2025-02-13 04:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:16][root][INFO] - Training Epoch: 2/2, step 7075/7134 completed (loss: 0.02439873479306698, acc: 0.9925261735916138)
[2025-02-13 04:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:16][root][INFO] - Training Epoch: 2/2, step 7076/7134 completed (loss: 0.04045168310403824, acc: 0.9886040091514587)
[2025-02-13 04:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:17][root][INFO] - Training Epoch: 2/2, step 7077/7134 completed (loss: 0.03657592087984085, acc: 0.9868852496147156)
[2025-02-13 04:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:17][root][INFO] - Training Epoch: 2/2, step 7078/7134 completed (loss: 0.022269170731306076, acc: 0.995192289352417)
[2025-02-13 04:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:18][root][INFO] - Training Epoch: 2/2, step 7079/7134 completed (loss: 0.020518774166703224, acc: 0.9934318661689758)
[2025-02-13 04:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:18][root][INFO] - Training Epoch: 2/2, step 7080/7134 completed (loss: 0.019335197284817696, acc: 0.9916943311691284)
[2025-02-13 04:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:18][root][INFO] - Training Epoch: 2/2, step 7081/7134 completed (loss: 0.019064726307988167, acc: 0.991584837436676)
[2025-02-13 04:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:19][root][INFO] - Training Epoch: 2/2, step 7082/7134 completed (loss: 0.03554605692625046, acc: 0.9903069734573364)
[2025-02-13 04:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:19][root][INFO] - Training Epoch: 2/2, step 7083/7134 completed (loss: 0.009837858378887177, acc: 0.9916387796401978)
[2025-02-13 04:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:20][root][INFO] - Training Epoch: 2/2, step 7084/7134 completed (loss: 0.027316903695464134, acc: 0.9912917017936707)
[2025-02-13 04:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:20][root][INFO] - Training Epoch: 2/2, step 7085/7134 completed (loss: 0.0326894074678421, acc: 0.9922839403152466)
[2025-02-13 04:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:21][root][INFO] - Training Epoch: 2/2, step 7086/7134 completed (loss: 0.024296315386891365, acc: 0.99452805519104)
[2025-02-13 04:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:21][root][INFO] - Training Epoch: 2/2, step 7087/7134 completed (loss: 0.03710401803255081, acc: 0.9907578825950623)
[2025-02-13 04:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:21][root][INFO] - Training Epoch: 2/2, step 7088/7134 completed (loss: 0.02872318960726261, acc: 0.9900662302970886)
[2025-02-13 04:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:22][root][INFO] - Training Epoch: 2/2, step 7089/7134 completed (loss: 0.09743206202983856, acc: 0.9808917045593262)
[2025-02-13 04:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:22][root][INFO] - Training Epoch: 2/2, step 7090/7134 completed (loss: 0.014563807286322117, acc: 0.998645007610321)
[2025-02-13 04:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:23][root][INFO] - Training Epoch: 2/2, step 7091/7134 completed (loss: 0.0036101716104894876, acc: 0.9986072182655334)
[2025-02-13 04:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:23][root][INFO] - Training Epoch: 2/2, step 7092/7134 completed (loss: 0.008979085832834244, acc: 0.9965694546699524)
[2025-02-13 04:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:24][root][INFO] - Training Epoch: 2/2, step 7093/7134 completed (loss: 0.03806820139288902, acc: 0.9821428656578064)
[2025-02-13 04:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:24][root][INFO] - Training Epoch: 2/2, step 7094/7134 completed (loss: 0.004557328764349222, acc: 0.9976958632469177)
[2025-02-13 04:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:24][root][INFO] - Training Epoch: 2/2, step 7095/7134 completed (loss: 0.03411021828651428, acc: 0.9958677887916565)
[2025-02-13 04:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:25][root][INFO] - Training Epoch: 2/2, step 7096/7134 completed (loss: 0.03165445849299431, acc: 0.9932773113250732)
[2025-02-13 04:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:25][root][INFO] - Training Epoch: 2/2, step 7097/7134 completed (loss: 0.023080160841345787, acc: 0.9927954077720642)
[2025-02-13 04:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:26][root][INFO] - Training Epoch: 2/2, step 7098/7134 completed (loss: 0.015903038904070854, acc: 0.995184600353241)
[2025-02-13 04:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:26][root][INFO] - Training Epoch: 2/2, step 7099/7134 completed (loss: 0.023318693041801453, acc: 0.9931623935699463)
[2025-02-13 04:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:26][root][INFO] - Training Epoch: 2/2, step 7100/7134 completed (loss: 0.038298290222883224, acc: 0.9829457402229309)
[2025-02-13 04:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:27][root][INFO] - Training Epoch: 2/2, step 7101/7134 completed (loss: 0.03628740832209587, acc: 0.9874776601791382)
[2025-02-13 04:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:27][root][INFO] - Training Epoch: 2/2, step 7102/7134 completed (loss: 0.009677105583250523, acc: 0.9941860437393188)
[2025-02-13 04:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:28][root][INFO] - Training Epoch: 2/2, step 7103/7134 completed (loss: 0.015270876698195934, acc: 0.9959677457809448)
[2025-02-13 04:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:28][root][INFO] - Training Epoch: 2/2, step 7104/7134 completed (loss: 0.005831830203533173, acc: 0.9979423880577087)
[2025-02-13 04:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:29][root][INFO] - Training Epoch: 2/2, step 7105/7134 completed (loss: 0.02711123414337635, acc: 0.9950000047683716)
[2025-02-13 04:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:29][root][INFO] - Training Epoch: 2/2, step 7106/7134 completed (loss: 0.0056549301370978355, acc: 1.0)
[2025-02-13 04:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:29][root][INFO] - Training Epoch: 2/2, step 7107/7134 completed (loss: 0.01319092232733965, acc: 0.9953415989875793)
[2025-02-13 04:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:30][root][INFO] - Training Epoch: 2/2, step 7108/7134 completed (loss: 0.017052195966243744, acc: 0.9950000047683716)
[2025-02-13 04:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:30][root][INFO] - Training Epoch: 2/2, step 7109/7134 completed (loss: 0.024732904508709908, acc: 0.9906191229820251)
[2025-02-13 04:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:31][root][INFO] - Training Epoch: 2/2, step 7110/7134 completed (loss: 0.012630405835807323, acc: 0.996610164642334)
[2025-02-13 04:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:31][root][INFO] - Training Epoch: 2/2, step 7111/7134 completed (loss: 0.027651742100715637, acc: 0.9911308288574219)
[2025-02-13 04:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:31][root][INFO] - Training Epoch: 2/2, step 7112/7134 completed (loss: 0.013072936795651913, acc: 0.9958847761154175)
[2025-02-13 04:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:32][root][INFO] - Training Epoch: 2/2, step 7113/7134 completed (loss: 0.04404110461473465, acc: 0.9826498627662659)
[2025-02-13 04:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:32][root][INFO] - Training Epoch: 2/2, step 7114/7134 completed (loss: 0.014794304966926575, acc: 0.9980879426002502)
[2025-02-13 04:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:33][root][INFO] - Training Epoch: 2/2, step 7115/7134 completed (loss: 0.013839836232364178, acc: 0.9964726567268372)
[2025-02-13 04:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:33][root][INFO] - Training Epoch: 2/2, step 7116/7134 completed (loss: 0.016746796667575836, acc: 0.9929701089859009)
[2025-02-13 04:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:33][root][INFO] - Training Epoch: 2/2, step 7117/7134 completed (loss: 0.012450975365936756, acc: 0.996666669845581)
[2025-02-13 04:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:34][root][INFO] - Training Epoch: 2/2, step 7118/7134 completed (loss: 0.03664351999759674, acc: 0.992732584476471)
[2025-02-13 04:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:34][root][INFO] - Training Epoch: 2/2, step 7119/7134 completed (loss: 0.017173457890748978, acc: 0.9918166995048523)
[2025-02-13 04:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:35][root][INFO] - Training Epoch: 2/2, step 7120/7134 completed (loss: 0.027870384976267815, acc: 0.9906542301177979)
[2025-02-13 04:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:35][root][INFO] - Training Epoch: 2/2, step 7121/7134 completed (loss: 0.011812735348939896, acc: 0.9947183132171631)
[2025-02-13 04:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:35][root][INFO] - Training Epoch: 2/2, step 7122/7134 completed (loss: 0.015697142109274864, acc: 0.9946523904800415)
[2025-02-13 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:36][root][INFO] - Training Epoch: 2/2, step 7123/7134 completed (loss: 0.020397713407874107, acc: 0.992682933807373)
[2025-02-13 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:36][root][INFO] - Training Epoch: 2/2, step 7124/7134 completed (loss: 0.023128826171159744, acc: 0.9924952983856201)
[2025-02-13 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:37][root][INFO] - Training Epoch: 2/2, step 7125/7134 completed (loss: 0.016029300168156624, acc: 0.9917080998420715)
[2025-02-13 04:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:37][root][INFO] - Training Epoch: 2/2, step 7126/7134 completed (loss: 0.03930554911494255, acc: 0.9831365942955017)
[2025-02-13 04:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:37][root][INFO] - Training Epoch: 2/2, step 7127/7134 completed (loss: 0.015165950171649456, acc: 0.9968152642250061)
[2025-02-13 04:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:38][root][INFO] - Training Epoch: 2/2, step 7128/7134 completed (loss: 0.032711613923311234, acc: 0.9902439117431641)
[2025-02-13 04:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:38][root][INFO] - Training Epoch: 2/2, step 7129/7134 completed (loss: 0.041020192205905914, acc: 0.9798657894134521)
[2025-02-13 04:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:55][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0463, device='cuda:0') eval_epoch_loss=tensor(0.0453, device='cuda:0') eval_epoch_acc=tensor(0.9886, device='cuda:0')
[2025-02-13 04:44:55][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:44:55][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:44:55][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_7130_loss_0.04527689144015312/model.pt
[2025-02-13 04:44:55][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:44:55][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9885826110839844
[2025-02-13 04:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:56][root][INFO] - Training Epoch: 2/2, step 7130/7134 completed (loss: 0.062029074877500534, acc: 0.9767857193946838)
[2025-02-13 04:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:56][root][INFO] - Training Epoch: 2/2, step 7131/7134 completed (loss: 0.008177405223250389, acc: 0.9976798295974731)
[2025-02-13 04:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:57][root][INFO] - Training Epoch: 2/2, step 7132/7134 completed (loss: 0.01368760596960783, acc: 0.996632993221283)
[2025-02-13 04:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:57][root][INFO] - Training Epoch: 2/2, step 7133/7134 completed (loss: 0.006638930179178715, acc: 0.9981516003608704)
[2025-02-13 04:44:57][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.0299, train_epoch_loss=0.0294, epoch time 4191.977940818295s
[2025-02-13 04:44:57][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2025-02-13 04:44:57][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 30 GB
[2025-02-13 04:44:57][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2025-02-13 04:44:57][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2025-02-13 04:44:57][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 8 GB
[2025-02-13 04:44:57][root][INFO] - Key: avg_train_prep, Value: 1.0528074502944946
[2025-02-13 04:44:57][root][INFO] - Key: avg_train_loss, Value: 0.05122274532914162
[2025-02-13 04:44:57][root][INFO] - Key: avg_train_acc, Value: 0.9870235919952393
[2025-02-13 04:44:57][root][INFO] - Key: avg_eval_prep, Value: 1.049648404121399
[2025-02-13 04:44:57][root][INFO] - Key: avg_eval_loss, Value: 0.04844176024198532
[2025-02-13 04:44:57][root][INFO] - Key: avg_eval_acc, Value: 0.9870224595069885
[2025-02-13 04:44:57][root][INFO] - Key: avg_epoch_time, Value: 4205.1605284055695
[2025-02-13 04:44:57][root][INFO] - Key: avg_checkpoint_time, Value: 0.3444020722527057
Selected lowest loss checkpoint: asr_epoch_2_step_3564_loss_0.041885703802108765
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.041885703802108765/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.041885703802108765
[2025-02-13 04:45:32][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-13 04:45:32][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 04:45:32][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme'}
[2025-02-13 04:45:34][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 04:45:40][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 04:45:40][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 04:45:40][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 04:45:40][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 04:45:44][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 04:45:44][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 04:45:44][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 04:45:44][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 04:45:44][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 04:45:44][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 04:45:44][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 04:45:44][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.041885703802108765/model.pt
[2025-02-13 04:45:45][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 04:45:45][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 04:45:47][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 04:45:48][root][INFO] - --> Training Set Length = 2620
[2025-02-13 04:45:48][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/repetition_penalty.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-02-13 04:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:35][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_pred_20250213_044548
[2025-02-13 05:36:35][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_gt_20250213_044548
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_gt_20250213_044548
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_pred_20250213_044548
Combined WER: 0.0927979474848101

Filtering repeated words...

Found 0 repeated lines in total.
Filtered Combined WER: 0.0927979474848101
