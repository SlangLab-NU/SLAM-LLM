/work/van-speech-nlp/jindaznb/slamenv/bin/python



---
Training Configuration:
Task: all
Config File: wavlm-mono
Epochs: 2
Batch Size: 4
Data Folder: librispeech-100_phoneme
Use PEFT: true
LLM Name: llama32_1b
Freeze Encoder: true
speech encoder name: wavlm
speech encoder path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt
speech encoder2 name: 
speech encoder2 path: 
llm_path: 
use_peft: true
use_fp16: 
Final identifier: librispeech-100_phoneme_wavlm_llama32_1b_linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.050975121557712555



----- Transfer Learning Information -----
Resume Epoch: 1
Resume Step: 0
Train Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl
Validation Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl
Test Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl
Identifier: librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
Output Directory: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
----------------------------------------
Resume epoch: 1
Resume step: 0
[2025-01-02 00:38:11][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-01-02 00:38:11][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-02 00:38:11][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-02 00:38:11][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-01-02_00-38-11.txt', 'log_interval': 5}
[2025-01-02 00:38:31][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-02 00:38:37][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:38:37][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-02 00:38:37][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:38:37][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-02 00:38:43][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:38:43][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-02 00:38:43][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-01-02 00:38:43][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:38:43][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-02 00:38:43][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-02 00:38:43][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-02 00:38:43][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_7130_loss_0.050975121557712555/model.pt
[2025-01-02 00:38:43][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-02 00:38:43][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-02 00:38:45][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-02 00:38:46][root][INFO] - --> Training Set Length = 2298
[2025-01-02 00:38:46][root][INFO] - --> Validation Set Length = 341
[2025-01-02 00:38:46][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-02 00:38:46][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-02 00:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:48][root][INFO] - Training Epoch: 1/2, step 0/574 completed (loss: 4.565918922424316, acc: 0.2222222238779068)
[2025-01-02 00:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:48][root][INFO] - Training Epoch: 1/2, step 1/574 completed (loss: 3.6701340675354004, acc: 0.2800000011920929)
[2025-01-02 00:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:49][root][INFO] - Training Epoch: 1/2, step 2/574 completed (loss: 3.0726566314697266, acc: 0.4054054021835327)
[2025-01-02 00:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:49][root][INFO] - Training Epoch: 1/2, step 3/574 completed (loss: 4.179104328155518, acc: 0.2368421107530594)
[2025-01-02 00:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:50][root][INFO] - Training Epoch: 1/2, step 4/574 completed (loss: 4.1991047859191895, acc: 0.21621622145175934)
[2025-01-02 00:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:50][root][INFO] - Training Epoch: 1/2, step 5/574 completed (loss: 3.3390657901763916, acc: 0.3928571343421936)
[2025-01-02 00:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:51][root][INFO] - Training Epoch: 1/2, step 6/574 completed (loss: 4.611793518066406, acc: 0.30612245202064514)
[2025-01-02 00:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:51][root][INFO] - Training Epoch: 1/2, step 7/574 completed (loss: 3.025235414505005, acc: 0.46666666865348816)
[2025-01-02 00:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:51][root][INFO] - Training Epoch: 1/2, step 8/574 completed (loss: 3.5062613487243652, acc: 0.5)
[2025-01-02 00:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:52][root][INFO] - Training Epoch: 1/2, step 9/574 completed (loss: 1.9377830028533936, acc: 0.692307710647583)
[2025-01-02 00:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:52][root][INFO] - Training Epoch: 1/2, step 10/574 completed (loss: 1.4856634140014648, acc: 0.6666666865348816)
[2025-01-02 00:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:53][root][INFO] - Training Epoch: 1/2, step 11/574 completed (loss: 3.778326988220215, acc: 0.3076923191547394)
[2025-01-02 00:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:53][root][INFO] - Training Epoch: 1/2, step 12/574 completed (loss: 3.122279405593872, acc: 0.4545454680919647)
[2025-01-02 00:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:53][root][INFO] - Training Epoch: 1/2, step 13/574 completed (loss: 3.3837459087371826, acc: 0.3695652186870575)
[2025-01-02 00:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:54][root][INFO] - Training Epoch: 1/2, step 14/574 completed (loss: 3.736240863800049, acc: 0.47058823704719543)
[2025-01-02 00:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:54][root][INFO] - Training Epoch: 1/2, step 15/574 completed (loss: 2.7348246574401855, acc: 0.5102040767669678)
[2025-01-02 00:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:55][root][INFO] - Training Epoch: 1/2, step 16/574 completed (loss: 3.640578031539917, acc: 0.42105263471603394)
[2025-01-02 00:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:55][root][INFO] - Training Epoch: 1/2, step 17/574 completed (loss: 2.8711702823638916, acc: 0.4583333432674408)
[2025-01-02 00:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:55][root][INFO] - Training Epoch: 1/2, step 18/574 completed (loss: 4.022885322570801, acc: 0.3611111044883728)
[2025-01-02 00:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:56][root][INFO] - Training Epoch: 1/2, step 19/574 completed (loss: 3.6987786293029785, acc: 0.5263158082962036)
[2025-01-02 00:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:56][root][INFO] - Training Epoch: 1/2, step 20/574 completed (loss: 2.517129421234131, acc: 0.5769230723381042)
[2025-01-02 00:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:57][root][INFO] - Training Epoch: 1/2, step 21/574 completed (loss: 2.9080355167388916, acc: 0.48275861144065857)
[2025-01-02 00:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:57][root][INFO] - Training Epoch: 1/2, step 22/574 completed (loss: 4.26425313949585, acc: 0.3199999928474426)
[2025-01-02 00:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:57][root][INFO] - Training Epoch: 1/2, step 23/574 completed (loss: 2.4392616748809814, acc: 0.6666666865348816)
[2025-01-02 00:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:58][root][INFO] - Training Epoch: 1/2, step 24/574 completed (loss: 2.8660073280334473, acc: 0.5625)
[2025-01-02 00:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:58][root][INFO] - Training Epoch: 1/2, step 25/574 completed (loss: 3.410203456878662, acc: 0.4150943458080292)
[2025-01-02 00:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:38:59][root][INFO] - Training Epoch: 1/2, step 26/574 completed (loss: 3.3793797492980957, acc: 0.27397260069847107)
[2025-01-02 00:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:00][root][INFO] - Training Epoch: 1/2, step 27/574 completed (loss: 3.235335111618042, acc: 0.3596837818622589)
[2025-01-02 00:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:00][root][INFO] - Training Epoch: 1/2, step 28/574 completed (loss: 3.8038437366485596, acc: 0.3488371968269348)
[2025-01-02 00:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:01][root][INFO] - Training Epoch: 1/2, step 29/574 completed (loss: 3.4300684928894043, acc: 0.40963855385780334)
[2025-01-02 00:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:01][root][INFO] - Training Epoch: 1/2, step 30/574 completed (loss: 3.1179370880126953, acc: 0.43209877610206604)
[2025-01-02 00:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:02][root][INFO] - Training Epoch: 1/2, step 31/574 completed (loss: 3.7029573917388916, acc: 0.3928571343421936)
[2025-01-02 00:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:02][root][INFO] - Training Epoch: 1/2, step 32/574 completed (loss: 2.630068778991699, acc: 0.5185185074806213)
[2025-01-02 00:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:02][root][INFO] - Training Epoch: 1/2, step 33/574 completed (loss: 2.9637179374694824, acc: 0.6086956262588501)
[2025-01-02 00:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:03][root][INFO] - Training Epoch: 1/2, step 34/574 completed (loss: 2.693861722946167, acc: 0.45378151535987854)
[2025-01-02 00:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:03][root][INFO] - Training Epoch: 1/2, step 35/574 completed (loss: 2.7415852546691895, acc: 0.5409836173057556)
[2025-01-02 00:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:04][root][INFO] - Training Epoch: 1/2, step 36/574 completed (loss: 2.9526925086975098, acc: 0.4285714328289032)
[2025-01-02 00:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:04][root][INFO] - Training Epoch: 1/2, step 37/574 completed (loss: 2.871122121810913, acc: 0.5423728823661804)
[2025-01-02 00:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:05][root][INFO] - Training Epoch: 1/2, step 38/574 completed (loss: 2.828233003616333, acc: 0.5632184147834778)
[2025-01-02 00:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:05][root][INFO] - Training Epoch: 1/2, step 39/574 completed (loss: 4.379754066467285, acc: 0.2380952388048172)
[2025-01-02 00:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:05][root][INFO] - Training Epoch: 1/2, step 40/574 completed (loss: 2.794499635696411, acc: 0.6538461446762085)
[2025-01-02 00:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:06][root][INFO] - Training Epoch: 1/2, step 41/574 completed (loss: 2.3172364234924316, acc: 0.5945945978164673)
[2025-01-02 00:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:06][root][INFO] - Training Epoch: 1/2, step 42/574 completed (loss: 3.5864038467407227, acc: 0.3692307770252228)
[2025-01-02 00:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:07][root][INFO] - Training Epoch: 1/2, step 43/574 completed (loss: 3.4926998615264893, acc: 0.3232323229312897)
[2025-01-02 00:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:07][root][INFO] - Training Epoch: 1/2, step 44/574 completed (loss: 2.6371116638183594, acc: 0.5670102834701538)
[2025-01-02 00:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:07][root][INFO] - Training Epoch: 1/2, step 45/574 completed (loss: 3.284898519515991, acc: 0.40441176295280457)
[2025-01-02 00:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:08][root][INFO] - Training Epoch: 1/2, step 46/574 completed (loss: 3.328519582748413, acc: 0.38461539149284363)
[2025-01-02 00:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:08][root][INFO] - Training Epoch: 1/2, step 47/574 completed (loss: 1.991088628768921, acc: 0.5925925970077515)
[2025-01-02 00:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:08][root][INFO] - Training Epoch: 1/2, step 48/574 completed (loss: 2.2159512042999268, acc: 0.5357142686843872)
[2025-01-02 00:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:09][root][INFO] - Training Epoch: 1/2, step 49/574 completed (loss: 2.1327288150787354, acc: 0.5833333134651184)
[2025-01-02 00:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:09][root][INFO] - Training Epoch: 1/2, step 50/574 completed (loss: 3.500363349914551, acc: 0.4736842215061188)
[2025-01-02 00:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:10][root][INFO] - Training Epoch: 1/2, step 51/574 completed (loss: 3.3304097652435303, acc: 0.460317462682724)
[2025-01-02 00:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:10][root][INFO] - Training Epoch: 1/2, step 52/574 completed (loss: 4.112359523773193, acc: 0.3661971688270569)
[2025-01-02 00:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:11][root][INFO] - Training Epoch: 1/2, step 53/574 completed (loss: 3.9678523540496826, acc: 0.2866666615009308)
[2025-01-02 00:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:11][root][INFO] - Training Epoch: 1/2, step 54/574 completed (loss: 4.3024091720581055, acc: 0.21621622145175934)
[2025-01-02 00:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:12][root][INFO] - Training Epoch: 1/2, step 55/574 completed (loss: 1.8599413633346558, acc: 0.5384615659713745)
[2025-01-02 00:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:15][root][INFO] - Training Epoch: 1/2, step 56/574 completed (loss: 3.06178617477417, acc: 0.33788394927978516)
[2025-01-02 00:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:16][root][INFO] - Training Epoch: 1/2, step 57/574 completed (loss: 3.0808749198913574, acc: 0.35729846358299255)
[2025-01-02 00:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:17][root][INFO] - Training Epoch: 1/2, step 58/574 completed (loss: 3.3392577171325684, acc: 0.3693181872367859)
[2025-01-02 00:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:17][root][INFO] - Training Epoch: 1/2, step 59/574 completed (loss: 2.519524574279785, acc: 0.4485294222831726)
[2025-01-02 00:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:18][root][INFO] - Training Epoch: 1/2, step 60/574 completed (loss: 2.8573834896087646, acc: 0.4057970941066742)
[2025-01-02 00:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:19][root][INFO] - Training Epoch: 1/2, step 61/574 completed (loss: 2.907433271408081, acc: 0.38749998807907104)
[2025-01-02 00:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:19][root][INFO] - Training Epoch: 1/2, step 62/574 completed (loss: 1.6543489694595337, acc: 0.529411792755127)
[2025-01-02 00:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:19][root][INFO] - Training Epoch: 1/2, step 63/574 completed (loss: 2.6071159839630127, acc: 0.5277777910232544)
[2025-01-02 00:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:20][root][INFO] - Training Epoch: 1/2, step 64/574 completed (loss: 1.9234144687652588, acc: 0.640625)
[2025-01-02 00:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:20][root][INFO] - Training Epoch: 1/2, step 65/574 completed (loss: 1.4698727130889893, acc: 0.6206896305084229)
[2025-01-02 00:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:20][root][INFO] - Training Epoch: 1/2, step 66/574 completed (loss: 3.459401845932007, acc: 0.3392857015132904)
[2025-01-02 00:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:21][root][INFO] - Training Epoch: 1/2, step 67/574 completed (loss: 2.7412240505218506, acc: 0.4000000059604645)
[2025-01-02 00:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:21][root][INFO] - Training Epoch: 1/2, step 68/574 completed (loss: 1.187901258468628, acc: 0.7200000286102295)
[2025-01-02 00:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:22][root][INFO] - Training Epoch: 1/2, step 69/574 completed (loss: 2.24293851852417, acc: 0.4166666567325592)
[2025-01-02 00:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:22][root][INFO] - Training Epoch: 1/2, step 70/574 completed (loss: 3.6772358417510986, acc: 0.3636363744735718)
[2025-01-02 00:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:22][root][INFO] - Training Epoch: 1/2, step 71/574 completed (loss: 2.6245367527008057, acc: 0.40441176295280457)
[2025-01-02 00:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:23][root][INFO] - Training Epoch: 1/2, step 72/574 completed (loss: 1.8136476278305054, acc: 0.5555555820465088)
[2025-01-02 00:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:23][root][INFO] - Training Epoch: 1/2, step 73/574 completed (loss: 2.6639866828918457, acc: 0.3692307770252228)
[2025-01-02 00:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:24][root][INFO] - Training Epoch: 1/2, step 74/574 completed (loss: 3.5617454051971436, acc: 0.30612245202064514)
[2025-01-02 00:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:24][root][INFO] - Training Epoch: 1/2, step 75/574 completed (loss: 2.5667800903320312, acc: 0.3805970251560211)
[2025-01-02 00:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:24][root][INFO] - Training Epoch: 1/2, step 76/574 completed (loss: 3.0736048221588135, acc: 0.3540146052837372)
[2025-01-02 00:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:25][root][INFO] - Training Epoch: 1/2, step 77/574 completed (loss: 1.0581291913986206, acc: 0.7142857313156128)
[2025-01-02 00:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:25][root][INFO] - Training Epoch: 1/2, step 78/574 completed (loss: 1.4690192937850952, acc: 0.625)
[2025-01-02 00:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:26][root][INFO] - Training Epoch: 1/2, step 79/574 completed (loss: 1.2073217630386353, acc: 0.6666666865348816)
[2025-01-02 00:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:26][root][INFO] - Training Epoch: 1/2, step 80/574 completed (loss: 1.9726632833480835, acc: 0.692307710647583)
[2025-01-02 00:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:26][root][INFO] - Training Epoch: 1/2, step 81/574 completed (loss: 2.6414265632629395, acc: 0.557692289352417)
[2025-01-02 00:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:27][root][INFO] - Training Epoch: 1/2, step 82/574 completed (loss: 2.7956719398498535, acc: 0.4423076808452606)
[2025-01-02 00:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:27][root][INFO] - Training Epoch: 1/2, step 83/574 completed (loss: 0.8084146976470947, acc: 0.84375)
[2025-01-02 00:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:27][root][INFO] - Training Epoch: 1/2, step 84/574 completed (loss: 1.9850724935531616, acc: 0.5942028760910034)
[2025-01-02 00:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:28][root][INFO] - Training Epoch: 1/2, step 85/574 completed (loss: 2.5030436515808105, acc: 0.5600000023841858)
[2025-01-02 00:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:28][root][INFO] - Training Epoch: 1/2, step 86/574 completed (loss: 0.9830278158187866, acc: 0.695652186870575)
[2025-01-02 00:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:29][root][INFO] - Training Epoch: 1/2, step 87/574 completed (loss: 2.8882803916931152, acc: 0.4000000059604645)
[2025-01-02 00:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:29][root][INFO] - Training Epoch: 1/2, step 88/574 completed (loss: 3.065053939819336, acc: 0.3883495032787323)
[2025-01-02 00:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:30][root][INFO] - Training Epoch: 1/2, step 89/574 completed (loss: 2.4016566276550293, acc: 0.49514561891555786)
[2025-01-02 00:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:31][root][INFO] - Training Epoch: 1/2, step 90/574 completed (loss: 2.8966856002807617, acc: 0.42473119497299194)
[2025-01-02 00:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:32][root][INFO] - Training Epoch: 1/2, step 91/574 completed (loss: 2.4038703441619873, acc: 0.5)
[2025-01-02 00:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:33][root][INFO] - Training Epoch: 1/2, step 92/574 completed (loss: 2.4089112281799316, acc: 0.5263158082962036)
[2025-01-02 00:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:34][root][INFO] - Training Epoch: 1/2, step 93/574 completed (loss: 3.3138771057128906, acc: 0.2673267424106598)
[2025-01-02 00:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:34][root][INFO] - Training Epoch: 1/2, step 94/574 completed (loss: 2.3489127159118652, acc: 0.4838709533214569)
[2025-01-02 00:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:34][root][INFO] - Training Epoch: 1/2, step 95/574 completed (loss: 2.417255401611328, acc: 0.3478260934352875)
[2025-01-02 00:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:35][root][INFO] - Training Epoch: 1/2, step 96/574 completed (loss: 3.222446918487549, acc: 0.32773110270500183)
[2025-01-02 00:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:35][root][INFO] - Training Epoch: 1/2, step 97/574 completed (loss: 3.275271415710449, acc: 0.32692307233810425)
[2025-01-02 00:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:36][root][INFO] - Training Epoch: 1/2, step 98/574 completed (loss: 3.3138062953948975, acc: 0.30656933784484863)
[2025-01-02 00:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:36][root][INFO] - Training Epoch: 1/2, step 99/574 completed (loss: 3.423379898071289, acc: 0.31343284249305725)
[2025-01-02 00:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:36][root][INFO] - Training Epoch: 1/2, step 100/574 completed (loss: 2.4203052520751953, acc: 0.550000011920929)
[2025-01-02 00:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:37][root][INFO] - Training Epoch: 1/2, step 101/574 completed (loss: 1.3619434833526611, acc: 0.7272727489471436)
[2025-01-02 00:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:37][root][INFO] - Training Epoch: 1/2, step 102/574 completed (loss: 0.7013345956802368, acc: 0.739130437374115)
[2025-01-02 00:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:37][root][INFO] - Training Epoch: 1/2, step 103/574 completed (loss: 0.9561676383018494, acc: 0.7727272510528564)
[2025-01-02 00:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:38][root][INFO] - Training Epoch: 1/2, step 104/574 completed (loss: 1.580527901649475, acc: 0.6206896305084229)
[2025-01-02 00:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:38][root][INFO] - Training Epoch: 1/2, step 105/574 completed (loss: 1.3592196702957153, acc: 0.6976743936538696)
[2025-01-02 00:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:39][root][INFO] - Training Epoch: 1/2, step 106/574 completed (loss: 0.9877244830131531, acc: 0.7599999904632568)
[2025-01-02 00:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:39][root][INFO] - Training Epoch: 1/2, step 107/574 completed (loss: 0.9230515956878662, acc: 0.8235294222831726)
[2025-01-02 00:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:39][root][INFO] - Training Epoch: 1/2, step 108/574 completed (loss: 0.8038516640663147, acc: 0.8461538553237915)
[2025-01-02 00:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:40][root][INFO] - Training Epoch: 1/2, step 109/574 completed (loss: 0.6703882217407227, acc: 0.8571428656578064)
[2025-01-02 00:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:40][root][INFO] - Training Epoch: 1/2, step 110/574 completed (loss: 1.8341482877731323, acc: 0.6615384817123413)
[2025-01-02 00:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:41][root][INFO] - Training Epoch: 1/2, step 111/574 completed (loss: 1.5186504125595093, acc: 0.6315789222717285)
[2025-01-02 00:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:41][root][INFO] - Training Epoch: 1/2, step 112/574 completed (loss: 2.771782875061035, acc: 0.4736842215061188)
[2025-01-02 00:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:41][root][INFO] - Training Epoch: 1/2, step 113/574 completed (loss: 1.5293385982513428, acc: 0.5897436141967773)
[2025-01-02 00:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:42][root][INFO] - Training Epoch: 1/2, step 114/574 completed (loss: 1.5130350589752197, acc: 0.6530612111091614)
[2025-01-02 00:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:42][root][INFO] - Training Epoch: 1/2, step 115/574 completed (loss: 1.3971327543258667, acc: 0.7272727489471436)
[2025-01-02 00:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:43][root][INFO] - Training Epoch: 1/2, step 116/574 completed (loss: 1.3162338733673096, acc: 0.6190476417541504)
[2025-01-02 00:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:43][root][INFO] - Training Epoch: 1/2, step 117/574 completed (loss: 1.486054539680481, acc: 0.6341463327407837)
[2025-01-02 00:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:43][root][INFO] - Training Epoch: 1/2, step 118/574 completed (loss: 1.597548484802246, acc: 0.7096773982048035)
[2025-01-02 00:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:44][root][INFO] - Training Epoch: 1/2, step 119/574 completed (loss: 1.993386149406433, acc: 0.5399239659309387)
[2025-01-02 00:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:45][root][INFO] - Training Epoch: 1/2, step 120/574 completed (loss: 1.396101713180542, acc: 0.6933333277702332)
[2025-01-02 00:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:45][root][INFO] - Training Epoch: 1/2, step 121/574 completed (loss: 2.0678627490997314, acc: 0.5961538553237915)
[2025-01-02 00:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:46][root][INFO] - Training Epoch: 1/2, step 122/574 completed (loss: 1.1890355348587036, acc: 0.6666666865348816)
[2025-01-02 00:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:46][root][INFO] - Training Epoch: 1/2, step 123/574 completed (loss: 1.0188530683517456, acc: 0.6842105388641357)
[2025-01-02 00:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:46][root][INFO] - Training Epoch: 1/2, step 124/574 completed (loss: 2.0181427001953125, acc: 0.5521472096443176)
[2025-01-02 00:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:47][root][INFO] - Training Epoch: 1/2, step 125/574 completed (loss: 2.0524840354919434, acc: 0.5)
[2025-01-02 00:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:47][root][INFO] - Training Epoch: 1/2, step 126/574 completed (loss: 1.9213632345199585, acc: 0.5)
[2025-01-02 00:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:48][root][INFO] - Training Epoch: 1/2, step 127/574 completed (loss: 1.6306703090667725, acc: 0.5833333134651184)
[2025-01-02 00:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:48][root][INFO] - Training Epoch: 1/2, step 128/574 completed (loss: 1.7955976724624634, acc: 0.5897436141967773)
[2025-01-02 00:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:48][root][INFO] - Training Epoch: 1/2, step 129/574 completed (loss: 1.710350751876831, acc: 0.5441176295280457)
[2025-01-02 00:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:49][root][INFO] - Training Epoch: 1/2, step 130/574 completed (loss: 2.664008140563965, acc: 0.3461538553237915)
[2025-01-02 00:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:49][root][INFO] - Training Epoch: 1/2, step 131/574 completed (loss: 2.4736194610595703, acc: 0.43478259444236755)
[2025-01-02 00:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:49][root][INFO] - Training Epoch: 1/2, step 132/574 completed (loss: 2.0687029361724854, acc: 0.5)
[2025-01-02 00:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:50][root][INFO] - Training Epoch: 1/2, step 133/574 completed (loss: 2.3381268978118896, acc: 0.3913043439388275)
[2025-01-02 00:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:50][root][INFO] - Training Epoch: 1/2, step 134/574 completed (loss: 1.697987675666809, acc: 0.5714285969734192)
[2025-01-02 00:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:51][root][INFO] - Training Epoch: 1/2, step 135/574 completed (loss: 1.8390274047851562, acc: 0.5)
[2025-01-02 00:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:51][root][INFO] - Training Epoch: 1/2, step 136/574 completed (loss: 1.8226277828216553, acc: 0.5952380895614624)
[2025-01-02 00:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:51][root][INFO] - Training Epoch: 1/2, step 137/574 completed (loss: 2.2117486000061035, acc: 0.36666667461395264)
[2025-01-02 00:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:52][root][INFO] - Training Epoch: 1/2, step 138/574 completed (loss: 1.7167556285858154, acc: 0.6521739363670349)
[2025-01-02 00:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:52][root][INFO] - Training Epoch: 1/2, step 139/574 completed (loss: 0.6512330770492554, acc: 0.8571428656578064)
[2025-01-02 00:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:52][root][INFO] - Training Epoch: 1/2, step 140/574 completed (loss: 0.7566673755645752, acc: 0.7692307829856873)
[2025-01-02 00:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:53][root][INFO] - Training Epoch: 1/2, step 141/574 completed (loss: 1.3652455806732178, acc: 0.6451612710952759)
[2025-01-02 00:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:53][root][INFO] - Training Epoch: 1/2, step 142/574 completed (loss: 1.5792851448059082, acc: 0.5945945978164673)
[2025-01-02 00:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:26][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.4350, device='cuda:0') eval_epoch_loss=tensor(1.2340, device='cuda:0') eval_epoch_acc=tensor(0.7119, device='cuda:0')
[2025-01-02 00:40:26][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:40:26][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:40:26][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_143_loss_1.2340302467346191/model.pt
[2025-01-02 00:40:26][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:40:26][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.2340302467346191
[2025-01-02 00:40:26][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7118833065032959
[2025-01-02 00:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:26][root][INFO] - Training Epoch: 1/2, step 143/574 completed (loss: 1.9555134773254395, acc: 0.5526315569877625)
[2025-01-02 00:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:27][root][INFO] - Training Epoch: 1/2, step 144/574 completed (loss: 1.4470372200012207, acc: 0.6865671873092651)
[2025-01-02 00:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:27][root][INFO] - Training Epoch: 1/2, step 145/574 completed (loss: 1.6237736940383911, acc: 0.5510203838348389)
[2025-01-02 00:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:28][root][INFO] - Training Epoch: 1/2, step 146/574 completed (loss: 1.9318411350250244, acc: 0.478723406791687)
[2025-01-02 00:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:28][root][INFO] - Training Epoch: 1/2, step 147/574 completed (loss: 2.225682497024536, acc: 0.4714285731315613)
[2025-01-02 00:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:29][root][INFO] - Training Epoch: 1/2, step 148/574 completed (loss: 1.9076851606369019, acc: 0.4642857015132904)
[2025-01-02 00:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:29][root][INFO] - Training Epoch: 1/2, step 149/574 completed (loss: 1.9024313688278198, acc: 0.52173912525177)
[2025-01-02 00:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:29][root][INFO] - Training Epoch: 1/2, step 150/574 completed (loss: 1.4507282972335815, acc: 0.5862069129943848)
[2025-01-02 00:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:30][root][INFO] - Training Epoch: 1/2, step 151/574 completed (loss: 2.0053417682647705, acc: 0.5652173757553101)
[2025-01-02 00:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:30][root][INFO] - Training Epoch: 1/2, step 152/574 completed (loss: 1.2923908233642578, acc: 0.6779661178588867)
[2025-01-02 00:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:30][root][INFO] - Training Epoch: 1/2, step 153/574 completed (loss: 1.47339928150177, acc: 0.6140350699424744)
[2025-01-02 00:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:31][root][INFO] - Training Epoch: 1/2, step 154/574 completed (loss: 1.68242609500885, acc: 0.6486486196517944)
[2025-01-02 00:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:31][root][INFO] - Training Epoch: 1/2, step 155/574 completed (loss: 1.2317341566085815, acc: 0.7857142686843872)
[2025-01-02 00:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:31][root][INFO] - Training Epoch: 1/2, step 156/574 completed (loss: 1.2030788660049438, acc: 0.6521739363670349)
[2025-01-02 00:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:32][root][INFO] - Training Epoch: 1/2, step 157/574 completed (loss: 2.9596621990203857, acc: 0.3684210479259491)
[2025-01-02 00:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:34][root][INFO] - Training Epoch: 1/2, step 158/574 completed (loss: 3.4603662490844727, acc: 0.3918918967247009)
[2025-01-02 00:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:34][root][INFO] - Training Epoch: 1/2, step 159/574 completed (loss: 2.5724847316741943, acc: 0.40740740299224854)
[2025-01-02 00:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:34][root][INFO] - Training Epoch: 1/2, step 160/574 completed (loss: 2.9554572105407715, acc: 0.3720930218696594)
[2025-01-02 00:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:35][root][INFO] - Training Epoch: 1/2, step 161/574 completed (loss: 3.0566723346710205, acc: 0.3529411852359772)
[2025-01-02 00:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:36][root][INFO] - Training Epoch: 1/2, step 162/574 completed (loss: 3.194429636001587, acc: 0.33707866072654724)
[2025-01-02 00:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:36][root][INFO] - Training Epoch: 1/2, step 163/574 completed (loss: 1.60767662525177, acc: 0.7045454382896423)
[2025-01-02 00:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:36][root][INFO] - Training Epoch: 1/2, step 164/574 completed (loss: 0.9492246508598328, acc: 0.7142857313156128)
[2025-01-02 00:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:37][root][INFO] - Training Epoch: 1/2, step 165/574 completed (loss: 1.6065897941589355, acc: 0.5517241358757019)
[2025-01-02 00:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:37][root][INFO] - Training Epoch: 1/2, step 166/574 completed (loss: 0.8231494426727295, acc: 0.8367347121238708)
[2025-01-02 00:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:37][root][INFO] - Training Epoch: 1/2, step 167/574 completed (loss: 0.7496836185455322, acc: 0.8199999928474426)
[2025-01-02 00:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:38][root][INFO] - Training Epoch: 1/2, step 168/574 completed (loss: 1.4847321510314941, acc: 0.7222222089767456)
[2025-01-02 00:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:38][root][INFO] - Training Epoch: 1/2, step 169/574 completed (loss: 1.3755122423171997, acc: 0.6470588445663452)
[2025-01-02 00:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:39][root][INFO] - Training Epoch: 1/2, step 170/574 completed (loss: 2.2570252418518066, acc: 0.5068492889404297)
[2025-01-02 00:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:40][root][INFO] - Training Epoch: 1/2, step 171/574 completed (loss: 1.0501433610916138, acc: 0.6666666865348816)
[2025-01-02 00:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:40][root][INFO] - Training Epoch: 1/2, step 172/574 completed (loss: 1.6915427446365356, acc: 0.5925925970077515)
[2025-01-02 00:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:40][root][INFO] - Training Epoch: 1/2, step 173/574 completed (loss: 2.112375497817993, acc: 0.4642857015132904)
[2025-01-02 00:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:41][root][INFO] - Training Epoch: 1/2, step 174/574 completed (loss: 1.4983453750610352, acc: 0.6371681690216064)
[2025-01-02 00:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:41][root][INFO] - Training Epoch: 1/2, step 175/574 completed (loss: 1.578614354133606, acc: 0.6376811861991882)
[2025-01-02 00:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:41][root][INFO] - Training Epoch: 1/2, step 176/574 completed (loss: 1.2939586639404297, acc: 0.6477272510528564)
[2025-01-02 00:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:42][root][INFO] - Training Epoch: 1/2, step 177/574 completed (loss: 2.226735830307007, acc: 0.49618321657180786)
[2025-01-02 00:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:43][root][INFO] - Training Epoch: 1/2, step 178/574 completed (loss: 2.3262643814086914, acc: 0.4888888895511627)
[2025-01-02 00:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:43][root][INFO] - Training Epoch: 1/2, step 179/574 completed (loss: 1.2655489444732666, acc: 0.6721311211585999)
[2025-01-02 00:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:44][root][INFO] - Training Epoch: 1/2, step 180/574 completed (loss: 0.6177307963371277, acc: 0.8333333134651184)
[2025-01-02 00:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:44][root][INFO] - Training Epoch: 1/2, step 181/574 completed (loss: 0.10565561056137085, acc: 1.0)
[2025-01-02 00:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:44][root][INFO] - Training Epoch: 1/2, step 182/574 completed (loss: 0.7209062576293945, acc: 0.7857142686843872)
[2025-01-02 00:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:45][root][INFO] - Training Epoch: 1/2, step 183/574 completed (loss: 0.8879206776618958, acc: 0.792682945728302)
[2025-01-02 00:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:45][root][INFO] - Training Epoch: 1/2, step 184/574 completed (loss: 1.1788359880447388, acc: 0.7673715949058533)
[2025-01-02 00:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:45][root][INFO] - Training Epoch: 1/2, step 185/574 completed (loss: 0.9879315495491028, acc: 0.7694524526596069)
[2025-01-02 00:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:46][root][INFO] - Training Epoch: 1/2, step 186/574 completed (loss: 1.0306320190429688, acc: 0.7437499761581421)
[2025-01-02 00:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:46][root][INFO] - Training Epoch: 1/2, step 187/574 completed (loss: 0.9307247400283813, acc: 0.7786116600036621)
[2025-01-02 00:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:47][root][INFO] - Training Epoch: 1/2, step 188/574 completed (loss: 1.147809624671936, acc: 0.6975088715553284)
[2025-01-02 00:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:47][root][INFO] - Training Epoch: 1/2, step 189/574 completed (loss: 1.1194149255752563, acc: 0.6800000071525574)
[2025-01-02 00:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:48][root][INFO] - Training Epoch: 1/2, step 190/574 completed (loss: 1.9097312688827515, acc: 0.5348837375640869)
[2025-01-02 00:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:49][root][INFO] - Training Epoch: 1/2, step 191/574 completed (loss: 2.7567241191864014, acc: 0.4126984179019928)
[2025-01-02 00:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:50][root][INFO] - Training Epoch: 1/2, step 192/574 completed (loss: 2.402129888534546, acc: 0.4545454680919647)
[2025-01-02 00:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:50][root][INFO] - Training Epoch: 1/2, step 193/574 completed (loss: 2.0187549591064453, acc: 0.5176470875740051)
[2025-01-02 00:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:51][root][INFO] - Training Epoch: 1/2, step 194/574 completed (loss: 2.287086248397827, acc: 0.42592594027519226)
[2025-01-02 00:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:52][root][INFO] - Training Epoch: 1/2, step 195/574 completed (loss: 2.0336925983428955, acc: 0.5161290168762207)
[2025-01-02 00:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:53][root][INFO] - Training Epoch: 1/2, step 196/574 completed (loss: 0.8821660280227661, acc: 0.6785714030265808)
[2025-01-02 00:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:53][root][INFO] - Training Epoch: 1/2, step 197/574 completed (loss: 2.126171588897705, acc: 0.550000011920929)
[2025-01-02 00:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:53][root][INFO] - Training Epoch: 1/2, step 198/574 completed (loss: 1.7755298614501953, acc: 0.6470588445663452)
[2025-01-02 00:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:54][root][INFO] - Training Epoch: 1/2, step 199/574 completed (loss: 1.7423195838928223, acc: 0.654411792755127)
[2025-01-02 00:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:54][root][INFO] - Training Epoch: 1/2, step 200/574 completed (loss: 1.3644949197769165, acc: 0.6440678238868713)
[2025-01-02 00:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:55][root][INFO] - Training Epoch: 1/2, step 201/574 completed (loss: 1.791461706161499, acc: 0.5820895433425903)
[2025-01-02 00:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:55][root][INFO] - Training Epoch: 1/2, step 202/574 completed (loss: 2.061650037765503, acc: 0.5436893105506897)
[2025-01-02 00:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:55][root][INFO] - Training Epoch: 1/2, step 203/574 completed (loss: 1.5478763580322266, acc: 0.60317462682724)
[2025-01-02 00:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:56][root][INFO] - Training Epoch: 1/2, step 204/574 completed (loss: 0.4092247784137726, acc: 0.901098906993866)
[2025-01-02 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:56][root][INFO] - Training Epoch: 1/2, step 205/574 completed (loss: 0.8969773650169373, acc: 0.7982062697410583)
[2025-01-02 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:56][root][INFO] - Training Epoch: 1/2, step 206/574 completed (loss: 0.8912990093231201, acc: 0.7795275449752808)
[2025-01-02 00:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:57][root][INFO] - Training Epoch: 1/2, step 207/574 completed (loss: 1.0292813777923584, acc: 0.7887930870056152)
[2025-01-02 00:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:57][root][INFO] - Training Epoch: 1/2, step 208/574 completed (loss: 0.8289788961410522, acc: 0.804347813129425)
[2025-01-02 00:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:57][root][INFO] - Training Epoch: 1/2, step 209/574 completed (loss: 1.0197430849075317, acc: 0.774319052696228)
[2025-01-02 00:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:58][root][INFO] - Training Epoch: 1/2, step 210/574 completed (loss: 0.783724308013916, acc: 0.804347813129425)
[2025-01-02 00:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:58][root][INFO] - Training Epoch: 1/2, step 211/574 completed (loss: 0.912301778793335, acc: 0.782608687877655)
[2025-01-02 00:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:58][root][INFO] - Training Epoch: 1/2, step 212/574 completed (loss: 0.45795607566833496, acc: 0.8571428656578064)
[2025-01-02 00:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:59][root][INFO] - Training Epoch: 1/2, step 213/574 completed (loss: 0.9606814384460449, acc: 0.8510638475418091)
[2025-01-02 00:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:40:59][root][INFO] - Training Epoch: 1/2, step 214/574 completed (loss: 1.0464251041412354, acc: 0.807692289352417)
[2025-01-02 00:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:00][root][INFO] - Training Epoch: 1/2, step 215/574 completed (loss: 0.722058892250061, acc: 0.8108108043670654)
[2025-01-02 00:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:00][root][INFO] - Training Epoch: 1/2, step 216/574 completed (loss: 0.8762537240982056, acc: 0.8372092843055725)
[2025-01-02 00:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:01][root][INFO] - Training Epoch: 1/2, step 217/574 completed (loss: 0.7745137810707092, acc: 0.837837815284729)
[2025-01-02 00:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:01][root][INFO] - Training Epoch: 1/2, step 218/574 completed (loss: 0.6508837938308716, acc: 0.8666666746139526)
[2025-01-02 00:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:01][root][INFO] - Training Epoch: 1/2, step 219/574 completed (loss: 0.6241779923439026, acc: 0.8484848737716675)
[2025-01-02 00:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:02][root][INFO] - Training Epoch: 1/2, step 220/574 completed (loss: 0.8419529795646667, acc: 0.7407407164573669)
[2025-01-02 00:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:02][root][INFO] - Training Epoch: 1/2, step 221/574 completed (loss: 0.4149685204029083, acc: 0.8799999952316284)
[2025-01-02 00:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:02][root][INFO] - Training Epoch: 1/2, step 222/574 completed (loss: 1.4953892230987549, acc: 0.6538461446762085)
[2025-01-02 00:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:03][root][INFO] - Training Epoch: 1/2, step 223/574 completed (loss: 1.245303750038147, acc: 0.7336956262588501)
[2025-01-02 00:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:04][root][INFO] - Training Epoch: 1/2, step 224/574 completed (loss: 1.2625855207443237, acc: 0.7102272510528564)
[2025-01-02 00:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:04][root][INFO] - Training Epoch: 1/2, step 225/574 completed (loss: 1.3112919330596924, acc: 0.6489361524581909)
[2025-01-02 00:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:05][root][INFO] - Training Epoch: 1/2, step 226/574 completed (loss: 1.9369306564331055, acc: 0.6037735939025879)
[2025-01-02 00:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:05][root][INFO] - Training Epoch: 1/2, step 227/574 completed (loss: 1.1363794803619385, acc: 0.6000000238418579)
[2025-01-02 00:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:05][root][INFO] - Training Epoch: 1/2, step 228/574 completed (loss: 1.4001473188400269, acc: 0.6279069781303406)
[2025-01-02 00:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:06][root][INFO] - Training Epoch: 1/2, step 229/574 completed (loss: 2.627236843109131, acc: 0.4333333373069763)
[2025-01-02 00:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:06][root][INFO] - Training Epoch: 1/2, step 230/574 completed (loss: 2.9859490394592285, acc: 0.3052631616592407)
[2025-01-02 00:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:07][root][INFO] - Training Epoch: 1/2, step 231/574 completed (loss: 2.3620665073394775, acc: 0.41111111640930176)
[2025-01-02 00:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:07][root][INFO] - Training Epoch: 1/2, step 232/574 completed (loss: 2.2843105792999268, acc: 0.4555555582046509)
[2025-01-02 00:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:08][root][INFO] - Training Epoch: 1/2, step 233/574 completed (loss: 2.4693572521209717, acc: 0.4082568883895874)
[2025-01-02 00:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:08][root][INFO] - Training Epoch: 1/2, step 234/574 completed (loss: 2.4532127380371094, acc: 0.4384615421295166)
[2025-01-02 00:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:08][root][INFO] - Training Epoch: 1/2, step 235/574 completed (loss: 1.0207843780517578, acc: 0.7368420958518982)
[2025-01-02 00:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:09][root][INFO] - Training Epoch: 1/2, step 236/574 completed (loss: 1.0650547742843628, acc: 0.7083333134651184)
[2025-01-02 00:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:09][root][INFO] - Training Epoch: 1/2, step 237/574 completed (loss: 1.4459179639816284, acc: 0.5909090638160706)
[2025-01-02 00:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:09][root][INFO] - Training Epoch: 1/2, step 238/574 completed (loss: 1.5181602239608765, acc: 0.5925925970077515)
[2025-01-02 00:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:10][root][INFO] - Training Epoch: 1/2, step 239/574 completed (loss: 1.3962138891220093, acc: 0.6285714507102966)
[2025-01-02 00:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:10][root][INFO] - Training Epoch: 1/2, step 240/574 completed (loss: 1.6877745389938354, acc: 0.5909090638160706)
[2025-01-02 00:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:11][root][INFO] - Training Epoch: 1/2, step 241/574 completed (loss: 1.236732006072998, acc: 0.7045454382896423)
[2025-01-02 00:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:11][root][INFO] - Training Epoch: 1/2, step 242/574 completed (loss: 2.297238349914551, acc: 0.4354838728904724)
[2025-01-02 00:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:12][root][INFO] - Training Epoch: 1/2, step 243/574 completed (loss: 1.9893345832824707, acc: 0.47727271914482117)
[2025-01-02 00:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:12][root][INFO] - Training Epoch: 1/2, step 244/574 completed (loss: 0.6570329666137695, acc: 0.8571428656578064)
[2025-01-02 00:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:12][root][INFO] - Training Epoch: 1/2, step 245/574 completed (loss: 0.9229353666305542, acc: 0.7307692170143127)
[2025-01-02 00:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:13][root][INFO] - Training Epoch: 1/2, step 246/574 completed (loss: 0.6210612654685974, acc: 0.8064516186714172)
[2025-01-02 00:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:13][root][INFO] - Training Epoch: 1/2, step 247/574 completed (loss: 0.9196876287460327, acc: 0.6000000238418579)
[2025-01-02 00:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:13][root][INFO] - Training Epoch: 1/2, step 248/574 completed (loss: 1.427586317062378, acc: 0.7027027010917664)
[2025-01-02 00:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:14][root][INFO] - Training Epoch: 1/2, step 249/574 completed (loss: 0.8502328395843506, acc: 0.7837837934494019)
[2025-01-02 00:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:14][root][INFO] - Training Epoch: 1/2, step 250/574 completed (loss: 0.9125193953514099, acc: 0.837837815284729)
[2025-01-02 00:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:15][root][INFO] - Training Epoch: 1/2, step 251/574 completed (loss: 0.7753826379776001, acc: 0.8088235259056091)
[2025-01-02 00:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:15][root][INFO] - Training Epoch: 1/2, step 252/574 completed (loss: 0.7566378116607666, acc: 0.8048780560493469)
[2025-01-02 00:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:15][root][INFO] - Training Epoch: 1/2, step 253/574 completed (loss: 0.726594090461731, acc: 0.7599999904632568)
[2025-01-02 00:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:16][root][INFO] - Training Epoch: 1/2, step 254/574 completed (loss: 0.24227012693881989, acc: 0.9599999785423279)
[2025-01-02 00:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:16][root][INFO] - Training Epoch: 1/2, step 255/574 completed (loss: 0.7476482391357422, acc: 0.774193525314331)
[2025-01-02 00:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:16][root][INFO] - Training Epoch: 1/2, step 256/574 completed (loss: 0.8070595264434814, acc: 0.859649121761322)
[2025-01-02 00:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:17][root][INFO] - Training Epoch: 1/2, step 257/574 completed (loss: 0.5401820540428162, acc: 0.8571428656578064)
[2025-01-02 00:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:17][root][INFO] - Training Epoch: 1/2, step 258/574 completed (loss: 0.45181363821029663, acc: 0.8947368264198303)
[2025-01-02 00:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:18][root][INFO] - Training Epoch: 1/2, step 259/574 completed (loss: 0.8321058750152588, acc: 0.7641509175300598)
[2025-01-02 00:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:18][root][INFO] - Training Epoch: 1/2, step 260/574 completed (loss: 0.6876882314682007, acc: 0.7916666865348816)
[2025-01-02 00:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:19][root][INFO] - Training Epoch: 1/2, step 261/574 completed (loss: 0.823515772819519, acc: 0.7777777910232544)
[2025-01-02 00:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:19][root][INFO] - Training Epoch: 1/2, step 262/574 completed (loss: 1.3888685703277588, acc: 0.6451612710952759)
[2025-01-02 00:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:19][root][INFO] - Training Epoch: 1/2, step 263/574 completed (loss: 1.763527274131775, acc: 0.6133333444595337)
[2025-01-02 00:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:20][root][INFO] - Training Epoch: 1/2, step 264/574 completed (loss: 1.2386854887008667, acc: 0.625)
[2025-01-02 00:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:21][root][INFO] - Training Epoch: 1/2, step 265/574 completed (loss: 2.057518720626831, acc: 0.4399999976158142)
[2025-01-02 00:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:21][root][INFO] - Training Epoch: 1/2, step 266/574 completed (loss: 1.9850060939788818, acc: 0.550561785697937)
[2025-01-02 00:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:22][root][INFO] - Training Epoch: 1/2, step 267/574 completed (loss: 1.7892911434173584, acc: 0.4864864945411682)
[2025-01-02 00:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:22][root][INFO] - Training Epoch: 1/2, step 268/574 completed (loss: 1.3424876928329468, acc: 0.568965494632721)
[2025-01-02 00:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:22][root][INFO] - Training Epoch: 1/2, step 269/574 completed (loss: 0.46009567379951477, acc: 0.8636363744735718)
[2025-01-02 00:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:23][root][INFO] - Training Epoch: 1/2, step 270/574 completed (loss: 0.3801385760307312, acc: 0.8636363744735718)
[2025-01-02 00:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:23][root][INFO] - Training Epoch: 1/2, step 271/574 completed (loss: 0.539517343044281, acc: 0.875)
[2025-01-02 00:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:23][root][INFO] - Training Epoch: 1/2, step 272/574 completed (loss: 0.2677713632583618, acc: 0.9333333373069763)
[2025-01-02 00:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:24][root][INFO] - Training Epoch: 1/2, step 273/574 completed (loss: 0.6041055917739868, acc: 0.9166666865348816)
[2025-01-02 00:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:24][root][INFO] - Training Epoch: 1/2, step 274/574 completed (loss: 0.7101876735687256, acc: 0.78125)
[2025-01-02 00:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:25][root][INFO] - Training Epoch: 1/2, step 275/574 completed (loss: 0.8153854012489319, acc: 0.8666666746139526)
[2025-01-02 00:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:25][root][INFO] - Training Epoch: 1/2, step 276/574 completed (loss: 0.8267799615859985, acc: 0.7931034564971924)
[2025-01-02 00:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:25][root][INFO] - Training Epoch: 1/2, step 277/574 completed (loss: 0.5826603770256042, acc: 0.8399999737739563)
[2025-01-02 00:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:26][root][INFO] - Training Epoch: 1/2, step 278/574 completed (loss: 1.188112735748291, acc: 0.7446808218955994)
[2025-01-02 00:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:26][root][INFO] - Training Epoch: 1/2, step 279/574 completed (loss: 0.8831205368041992, acc: 0.7708333134651184)
[2025-01-02 00:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:26][root][INFO] - Training Epoch: 1/2, step 280/574 completed (loss: 0.5034599900245667, acc: 0.8636363744735718)
[2025-01-02 00:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:27][root][INFO] - Training Epoch: 1/2, step 281/574 completed (loss: 1.427101731300354, acc: 0.6265060305595398)
[2025-01-02 00:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:27][root][INFO] - Training Epoch: 1/2, step 282/574 completed (loss: 1.5384513139724731, acc: 0.6018518805503845)
[2025-01-02 00:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:28][root][INFO] - Training Epoch: 1/2, step 283/574 completed (loss: 0.47995051741600037, acc: 0.8684210777282715)
[2025-01-02 00:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:28][root][INFO] - Training Epoch: 1/2, step 284/574 completed (loss: 0.8291048407554626, acc: 0.6764705777168274)
[2025-01-02 00:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:28][root][INFO] - Training Epoch: 1/2, step 285/574 completed (loss: 0.5241010785102844, acc: 0.8999999761581421)
[2025-01-02 00:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:58][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4781, device='cuda:0') eval_epoch_loss=tensor(0.9075, device='cuda:0') eval_epoch_acc=tensor(0.7641, device='cuda:0')
[2025-01-02 00:41:58][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:41:58][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:41:58][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_286_loss_0.9074847102165222/model.pt
[2025-01-02 00:41:58][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:41:58][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.9074847102165222
[2025-01-02 00:41:58][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7641414403915405
[2025-01-02 00:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:58][root][INFO] - Training Epoch: 1/2, step 286/574 completed (loss: 0.8005144596099854, acc: 0.8046875)
[2025-01-02 00:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:59][root][INFO] - Training Epoch: 1/2, step 287/574 completed (loss: 1.2074525356292725, acc: 0.6880000233650208)
[2025-01-02 00:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:59][root][INFO] - Training Epoch: 1/2, step 288/574 completed (loss: 1.182656168937683, acc: 0.7472527623176575)
[2025-01-02 00:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:41:59][root][INFO] - Training Epoch: 1/2, step 289/574 completed (loss: 1.168505311012268, acc: 0.7453415989875793)
[2025-01-02 00:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:00][root][INFO] - Training Epoch: 1/2, step 290/574 completed (loss: 1.132481336593628, acc: 0.7474226951599121)
[2025-01-02 00:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:00][root][INFO] - Training Epoch: 1/2, step 291/574 completed (loss: 0.6021957993507385, acc: 0.7727272510528564)
[2025-01-02 00:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:00][root][INFO] - Training Epoch: 1/2, step 292/574 completed (loss: 1.284606695175171, acc: 0.6428571343421936)
[2025-01-02 00:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:01][root][INFO] - Training Epoch: 1/2, step 293/574 completed (loss: 0.9799959659576416, acc: 0.7758620977401733)
[2025-01-02 00:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:01][root][INFO] - Training Epoch: 1/2, step 294/574 completed (loss: 0.900489866733551, acc: 0.6909090876579285)
[2025-01-02 00:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:02][root][INFO] - Training Epoch: 1/2, step 295/574 completed (loss: 0.9168203473091125, acc: 0.7525773048400879)
[2025-01-02 00:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:02][root][INFO] - Training Epoch: 1/2, step 296/574 completed (loss: 1.0030148029327393, acc: 0.7413793206214905)
[2025-01-02 00:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:03][root][INFO] - Training Epoch: 1/2, step 297/574 completed (loss: 0.6173954606056213, acc: 0.8148148059844971)
[2025-01-02 00:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:03][root][INFO] - Training Epoch: 1/2, step 298/574 completed (loss: 1.1562235355377197, acc: 0.6842105388641357)
[2025-01-02 00:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:03][root][INFO] - Training Epoch: 1/2, step 299/574 completed (loss: 0.45966798067092896, acc: 0.875)
[2025-01-02 00:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:04][root][INFO] - Training Epoch: 1/2, step 300/574 completed (loss: 0.3240152895450592, acc: 0.875)
[2025-01-02 00:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:04][root][INFO] - Training Epoch: 1/2, step 301/574 completed (loss: 0.8151790499687195, acc: 0.7735849022865295)
[2025-01-02 00:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:04][root][INFO] - Training Epoch: 1/2, step 302/574 completed (loss: 0.44566676020622253, acc: 0.9056603908538818)
[2025-01-02 00:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:05][root][INFO] - Training Epoch: 1/2, step 303/574 completed (loss: 0.2540920376777649, acc: 0.9117646813392639)
[2025-01-02 00:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:05][root][INFO] - Training Epoch: 1/2, step 304/574 completed (loss: 0.6150187253952026, acc: 0.75)
[2025-01-02 00:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:05][root][INFO] - Training Epoch: 1/2, step 305/574 completed (loss: 0.9109933972358704, acc: 0.7540983557701111)
[2025-01-02 00:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:06][root][INFO] - Training Epoch: 1/2, step 306/574 completed (loss: 0.9612130522727966, acc: 0.800000011920929)
[2025-01-02 00:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:06][root][INFO] - Training Epoch: 1/2, step 307/574 completed (loss: 0.4966517984867096, acc: 0.9473684430122375)
[2025-01-02 00:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:06][root][INFO] - Training Epoch: 1/2, step 308/574 completed (loss: 0.8949815630912781, acc: 0.7246376872062683)
[2025-01-02 00:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:07][root][INFO] - Training Epoch: 1/2, step 309/574 completed (loss: 0.955723762512207, acc: 0.75)
[2025-01-02 00:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:07][root][INFO] - Training Epoch: 1/2, step 310/574 completed (loss: 0.9806077480316162, acc: 0.7228915691375732)
[2025-01-02 00:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:07][root][INFO] - Training Epoch: 1/2, step 311/574 completed (loss: 0.8520811796188354, acc: 0.7564102411270142)
[2025-01-02 00:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:08][root][INFO] - Training Epoch: 1/2, step 312/574 completed (loss: 0.4738319218158722, acc: 0.8877550959587097)
[2025-01-02 00:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:08][root][INFO] - Training Epoch: 1/2, step 313/574 completed (loss: 0.27475616335868835, acc: 0.8333333134651184)
[2025-01-02 00:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:08][root][INFO] - Training Epoch: 1/2, step 314/574 completed (loss: 0.29481959342956543, acc: 0.8333333134651184)
[2025-01-02 00:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:09][root][INFO] - Training Epoch: 1/2, step 315/574 completed (loss: 0.41185203194618225, acc: 0.9032257795333862)
[2025-01-02 00:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:09][root][INFO] - Training Epoch: 1/2, step 316/574 completed (loss: 1.513351321220398, acc: 0.7096773982048035)
[2025-01-02 00:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:09][root][INFO] - Training Epoch: 1/2, step 317/574 completed (loss: 0.7793877124786377, acc: 0.8358209133148193)
[2025-01-02 00:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:10][root][INFO] - Training Epoch: 1/2, step 318/574 completed (loss: 0.2858263850212097, acc: 0.932692289352417)
[2025-01-02 00:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:10][root][INFO] - Training Epoch: 1/2, step 319/574 completed (loss: 0.4514736533164978, acc: 0.8444444537162781)
[2025-01-02 00:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:10][root][INFO] - Training Epoch: 1/2, step 320/574 completed (loss: 0.41369956731796265, acc: 0.9032257795333862)
[2025-01-02 00:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:11][root][INFO] - Training Epoch: 1/2, step 321/574 completed (loss: 0.35343509912490845, acc: 0.9399999976158142)
[2025-01-02 00:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:11][root][INFO] - Training Epoch: 1/2, step 322/574 completed (loss: 1.908878207206726, acc: 0.48148149251937866)
[2025-01-02 00:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:11][root][INFO] - Training Epoch: 1/2, step 323/574 completed (loss: 2.4788553714752197, acc: 0.37142857909202576)
[2025-01-02 00:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:12][root][INFO] - Training Epoch: 1/2, step 324/574 completed (loss: 2.5572173595428467, acc: 0.4615384638309479)
[2025-01-02 00:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:12][root][INFO] - Training Epoch: 1/2, step 325/574 completed (loss: 2.0950887203216553, acc: 0.46341463923454285)
[2025-01-02 00:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:12][root][INFO] - Training Epoch: 1/2, step 326/574 completed (loss: 2.217607021331787, acc: 0.3947368562221527)
[2025-01-02 00:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:13][root][INFO] - Training Epoch: 1/2, step 327/574 completed (loss: 0.9925418496131897, acc: 0.7368420958518982)
[2025-01-02 00:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:13][root][INFO] - Training Epoch: 1/2, step 328/574 completed (loss: 0.4225345551967621, acc: 0.8928571343421936)
[2025-01-02 00:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:13][root][INFO] - Training Epoch: 1/2, step 329/574 completed (loss: 0.494242399930954, acc: 0.8888888955116272)
[2025-01-02 00:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:14][root][INFO] - Training Epoch: 1/2, step 330/574 completed (loss: 0.2772162854671478, acc: 0.90625)
[2025-01-02 00:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:14][root][INFO] - Training Epoch: 1/2, step 331/574 completed (loss: 0.5302110314369202, acc: 0.8709677457809448)
[2025-01-02 00:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:14][root][INFO] - Training Epoch: 1/2, step 332/574 completed (loss: 0.5551416277885437, acc: 0.8771929740905762)
[2025-01-02 00:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:15][root][INFO] - Training Epoch: 1/2, step 333/574 completed (loss: 0.725619912147522, acc: 0.78125)
[2025-01-02 00:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:15][root][INFO] - Training Epoch: 1/2, step 334/574 completed (loss: 0.4008267819881439, acc: 0.8999999761581421)
[2025-01-02 00:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:15][root][INFO] - Training Epoch: 1/2, step 335/574 completed (loss: 0.9926006197929382, acc: 0.7368420958518982)
[2025-01-02 00:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:16][root][INFO] - Training Epoch: 1/2, step 336/574 completed (loss: 1.3239264488220215, acc: 0.6399999856948853)
[2025-01-02 00:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:16][root][INFO] - Training Epoch: 1/2, step 337/574 completed (loss: 1.7606555223464966, acc: 0.5632184147834778)
[2025-01-02 00:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:17][root][INFO] - Training Epoch: 1/2, step 338/574 completed (loss: 1.781680941581726, acc: 0.5106382966041565)
[2025-01-02 00:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:17][root][INFO] - Training Epoch: 1/2, step 339/574 completed (loss: 1.7319209575653076, acc: 0.5542168617248535)
[2025-01-02 00:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:17][root][INFO] - Training Epoch: 1/2, step 340/574 completed (loss: 0.4994173049926758, acc: 0.8260869383811951)
[2025-01-02 00:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:17][root][INFO] - Training Epoch: 1/2, step 341/574 completed (loss: 1.0782265663146973, acc: 0.7948718070983887)
[2025-01-02 00:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:18][root][INFO] - Training Epoch: 1/2, step 342/574 completed (loss: 0.9577940702438354, acc: 0.7228915691375732)
[2025-01-02 00:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:18][root][INFO] - Training Epoch: 1/2, step 343/574 completed (loss: 1.107060194015503, acc: 0.698113203048706)
[2025-01-02 00:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:19][root][INFO] - Training Epoch: 1/2, step 344/574 completed (loss: 0.40772533416748047, acc: 0.8860759735107422)
[2025-01-02 00:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:19][root][INFO] - Training Epoch: 1/2, step 345/574 completed (loss: 0.2990310788154602, acc: 0.9215686321258545)
[2025-01-02 00:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:19][root][INFO] - Training Epoch: 1/2, step 346/574 completed (loss: 1.2377433776855469, acc: 0.6865671873092651)
[2025-01-02 00:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:20][root][INFO] - Training Epoch: 1/2, step 347/574 completed (loss: 0.4397578835487366, acc: 0.8999999761581421)
[2025-01-02 00:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:20][root][INFO] - Training Epoch: 1/2, step 348/574 completed (loss: 1.0745346546173096, acc: 0.7200000286102295)
[2025-01-02 00:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:20][root][INFO] - Training Epoch: 1/2, step 349/574 completed (loss: 1.332379698753357, acc: 0.7222222089767456)
[2025-01-02 00:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:21][root][INFO] - Training Epoch: 1/2, step 350/574 completed (loss: 1.5012632608413696, acc: 0.5116279125213623)
[2025-01-02 00:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:21][root][INFO] - Training Epoch: 1/2, step 351/574 completed (loss: 0.9802061319351196, acc: 0.7179487347602844)
[2025-01-02 00:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:21][root][INFO] - Training Epoch: 1/2, step 352/574 completed (loss: 2.3192999362945557, acc: 0.3777777850627899)
[2025-01-02 00:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:22][root][INFO] - Training Epoch: 1/2, step 353/574 completed (loss: 0.3310868740081787, acc: 0.95652174949646)
[2025-01-02 00:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:22][root][INFO] - Training Epoch: 1/2, step 354/574 completed (loss: 1.0462113618850708, acc: 0.7307692170143127)
[2025-01-02 00:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:22][root][INFO] - Training Epoch: 1/2, step 355/574 completed (loss: 1.4438124895095825, acc: 0.5824176073074341)
[2025-01-02 00:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:23][root][INFO] - Training Epoch: 1/2, step 356/574 completed (loss: 1.3042254447937012, acc: 0.626086950302124)
[2025-01-02 00:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:23][root][INFO] - Training Epoch: 1/2, step 357/574 completed (loss: 1.1986453533172607, acc: 0.6521739363670349)
[2025-01-02 00:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:24][root][INFO] - Training Epoch: 1/2, step 358/574 completed (loss: 1.0386818647384644, acc: 0.6938775777816772)
[2025-01-02 00:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:24][root][INFO] - Training Epoch: 1/2, step 359/574 completed (loss: 0.1598534733057022, acc: 0.9583333134651184)
[2025-01-02 00:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:24][root][INFO] - Training Epoch: 1/2, step 360/574 completed (loss: 0.6345915198326111, acc: 0.7692307829856873)
[2025-01-02 00:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:24][root][INFO] - Training Epoch: 1/2, step 361/574 completed (loss: 1.1907578706741333, acc: 0.6585366129875183)
[2025-01-02 00:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:25][root][INFO] - Training Epoch: 1/2, step 362/574 completed (loss: 0.8904244303703308, acc: 0.8444444537162781)
[2025-01-02 00:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:25][root][INFO] - Training Epoch: 1/2, step 363/574 completed (loss: 0.6551095843315125, acc: 0.8421052694320679)
[2025-01-02 00:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:26][root][INFO] - Training Epoch: 1/2, step 364/574 completed (loss: 0.45005717873573303, acc: 0.9024389982223511)
[2025-01-02 00:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:26][root][INFO] - Training Epoch: 1/2, step 365/574 completed (loss: 0.6369144320487976, acc: 0.7575757503509521)
[2025-01-02 00:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:26][root][INFO] - Training Epoch: 1/2, step 366/574 completed (loss: 0.1336660236120224, acc: 0.9583333134651184)
[2025-01-02 00:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:27][root][INFO] - Training Epoch: 1/2, step 367/574 completed (loss: 0.7411879897117615, acc: 0.739130437374115)
[2025-01-02 00:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:27][root][INFO] - Training Epoch: 1/2, step 368/574 completed (loss: 0.42222243547439575, acc: 0.9285714030265808)
[2025-01-02 00:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:27][root][INFO] - Training Epoch: 1/2, step 369/574 completed (loss: 1.1520321369171143, acc: 0.78125)
[2025-01-02 00:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:28][root][INFO] - Training Epoch: 1/2, step 370/574 completed (loss: 1.2339346408843994, acc: 0.6424242258071899)
[2025-01-02 00:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:29][root][INFO] - Training Epoch: 1/2, step 371/574 completed (loss: 0.9755131006240845, acc: 0.7547169923782349)
[2025-01-02 00:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:29][root][INFO] - Training Epoch: 1/2, step 372/574 completed (loss: 0.4555671811103821, acc: 0.8999999761581421)
[2025-01-02 00:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:29][root][INFO] - Training Epoch: 1/2, step 373/574 completed (loss: 0.674355685710907, acc: 0.8928571343421936)
[2025-01-02 00:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:30][root][INFO] - Training Epoch: 1/2, step 374/574 completed (loss: 0.7297677993774414, acc: 0.8571428656578064)
[2025-01-02 00:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:30][root][INFO] - Training Epoch: 1/2, step 375/574 completed (loss: 0.06856150925159454, acc: 1.0)
[2025-01-02 00:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:30][root][INFO] - Training Epoch: 1/2, step 376/574 completed (loss: 0.3051396608352661, acc: 0.8695651888847351)
[2025-01-02 00:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:31][root][INFO] - Training Epoch: 1/2, step 377/574 completed (loss: 0.5195842385292053, acc: 0.8958333134651184)
[2025-01-02 00:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:31][root][INFO] - Training Epoch: 1/2, step 378/574 completed (loss: 0.35453152656555176, acc: 0.9052631855010986)
[2025-01-02 00:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:32][root][INFO] - Training Epoch: 1/2, step 379/574 completed (loss: 0.6176060438156128, acc: 0.8502994179725647)
[2025-01-02 00:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:32][root][INFO] - Training Epoch: 1/2, step 380/574 completed (loss: 0.7490099668502808, acc: 0.8045112490653992)
[2025-01-02 00:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:33][root][INFO] - Training Epoch: 1/2, step 381/574 completed (loss: 1.1628879308700562, acc: 0.6951871514320374)
[2025-01-02 00:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:34][root][INFO] - Training Epoch: 1/2, step 382/574 completed (loss: 0.5626320838928223, acc: 0.8738738894462585)
[2025-01-02 00:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:34][root][INFO] - Training Epoch: 1/2, step 383/574 completed (loss: 0.9197145104408264, acc: 0.7857142686843872)
[2025-01-02 00:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:35][root][INFO] - Training Epoch: 1/2, step 384/574 completed (loss: 0.13058039546012878, acc: 1.0)
[2025-01-02 00:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:35][root][INFO] - Training Epoch: 1/2, step 385/574 completed (loss: 0.39230021834373474, acc: 0.90625)
[2025-01-02 00:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:35][root][INFO] - Training Epoch: 1/2, step 386/574 completed (loss: 0.34171852469444275, acc: 0.9444444179534912)
[2025-01-02 00:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:36][root][INFO] - Training Epoch: 1/2, step 387/574 completed (loss: 0.1385039985179901, acc: 0.9736841917037964)
[2025-01-02 00:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:36][root][INFO] - Training Epoch: 1/2, step 388/574 completed (loss: 0.22423739731311798, acc: 0.9090909361839294)
[2025-01-02 00:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:36][root][INFO] - Training Epoch: 1/2, step 389/574 completed (loss: 0.16927513480186462, acc: 0.949999988079071)
[2025-01-02 00:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:37][root][INFO] - Training Epoch: 1/2, step 390/574 completed (loss: 0.9747900366783142, acc: 0.8095238208770752)
[2025-01-02 00:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:37][root][INFO] - Training Epoch: 1/2, step 391/574 completed (loss: 1.4042572975158691, acc: 0.5555555820465088)
[2025-01-02 00:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:38][root][INFO] - Training Epoch: 1/2, step 392/574 completed (loss: 1.3455485105514526, acc: 0.6796116232872009)
[2025-01-02 00:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:38][root][INFO] - Training Epoch: 1/2, step 393/574 completed (loss: 1.3183287382125854, acc: 0.7132353186607361)
[2025-01-02 00:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:39][root][INFO] - Training Epoch: 1/2, step 394/574 completed (loss: 1.4589347839355469, acc: 0.6200000047683716)
[2025-01-02 00:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:39][root][INFO] - Training Epoch: 1/2, step 395/574 completed (loss: 1.242250680923462, acc: 0.6666666865348816)
[2025-01-02 00:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:39][root][INFO] - Training Epoch: 1/2, step 396/574 completed (loss: 0.8797982931137085, acc: 0.7674418687820435)
[2025-01-02 00:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:40][root][INFO] - Training Epoch: 1/2, step 397/574 completed (loss: 0.5163269639015198, acc: 0.875)
[2025-01-02 00:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:40][root][INFO] - Training Epoch: 1/2, step 398/574 completed (loss: 0.6819582581520081, acc: 0.7674418687820435)
[2025-01-02 00:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:41][root][INFO] - Training Epoch: 1/2, step 399/574 completed (loss: 0.47976425290107727, acc: 0.8799999952316284)
[2025-01-02 00:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:41][root][INFO] - Training Epoch: 1/2, step 400/574 completed (loss: 0.6823372840881348, acc: 0.7941176295280457)
[2025-01-02 00:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:42][root][INFO] - Training Epoch: 1/2, step 401/574 completed (loss: 0.6368151903152466, acc: 0.800000011920929)
[2025-01-02 00:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:42][root][INFO] - Training Epoch: 1/2, step 402/574 completed (loss: 1.2284868955612183, acc: 0.6969696879386902)
[2025-01-02 00:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:42][root][INFO] - Training Epoch: 1/2, step 403/574 completed (loss: 0.5938553214073181, acc: 0.7575757503509521)
[2025-01-02 00:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:43][root][INFO] - Training Epoch: 1/2, step 404/574 completed (loss: 1.0687816143035889, acc: 0.7419354915618896)
[2025-01-02 00:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:43][root][INFO] - Training Epoch: 1/2, step 405/574 completed (loss: 0.2773459851741791, acc: 0.8518518805503845)
[2025-01-02 00:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:43][root][INFO] - Training Epoch: 1/2, step 406/574 completed (loss: 0.5483890771865845, acc: 0.8399999737739563)
[2025-01-02 00:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:43][root][INFO] - Training Epoch: 1/2, step 407/574 completed (loss: 0.32297712564468384, acc: 0.8888888955116272)
[2025-01-02 00:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:44][root][INFO] - Training Epoch: 1/2, step 408/574 completed (loss: 0.4743301272392273, acc: 0.8888888955116272)
[2025-01-02 00:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:44][root][INFO] - Training Epoch: 1/2, step 409/574 completed (loss: 0.4568483531475067, acc: 0.8846153616905212)
[2025-01-02 00:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:45][root][INFO] - Training Epoch: 1/2, step 410/574 completed (loss: 0.6482757329940796, acc: 0.8620689511299133)
[2025-01-02 00:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:45][root][INFO] - Training Epoch: 1/2, step 411/574 completed (loss: 0.15669049322605133, acc: 1.0)
[2025-01-02 00:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:45][root][INFO] - Training Epoch: 1/2, step 412/574 completed (loss: 0.42868563532829285, acc: 0.8666666746139526)
[2025-01-02 00:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:46][root][INFO] - Training Epoch: 1/2, step 413/574 completed (loss: 0.6229144334793091, acc: 0.8484848737716675)
[2025-01-02 00:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:46][root][INFO] - Training Epoch: 1/2, step 414/574 completed (loss: 0.6508092284202576, acc: 0.8636363744735718)
[2025-01-02 00:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:46][root][INFO] - Training Epoch: 1/2, step 415/574 completed (loss: 0.7787695527076721, acc: 0.7647058963775635)
[2025-01-02 00:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:47][root][INFO] - Training Epoch: 1/2, step 416/574 completed (loss: 0.6841326355934143, acc: 0.807692289352417)
[2025-01-02 00:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:47][root][INFO] - Training Epoch: 1/2, step 417/574 completed (loss: 0.781464695930481, acc: 0.7777777910232544)
[2025-01-02 00:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:48][root][INFO] - Training Epoch: 1/2, step 418/574 completed (loss: 0.7042449712753296, acc: 0.800000011920929)
[2025-01-02 00:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:48][root][INFO] - Training Epoch: 1/2, step 419/574 completed (loss: 0.9401046633720398, acc: 0.8500000238418579)
[2025-01-02 00:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:48][root][INFO] - Training Epoch: 1/2, step 420/574 completed (loss: 0.4138031005859375, acc: 0.8571428656578064)
[2025-01-02 00:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:49][root][INFO] - Training Epoch: 1/2, step 421/574 completed (loss: 0.636038064956665, acc: 0.8333333134651184)
[2025-01-02 00:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:49][root][INFO] - Training Epoch: 1/2, step 422/574 completed (loss: 0.7556023597717285, acc: 0.75)
[2025-01-02 00:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:50][root][INFO] - Training Epoch: 1/2, step 423/574 completed (loss: 1.1674339771270752, acc: 0.7222222089767456)
[2025-01-02 00:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:50][root][INFO] - Training Epoch: 1/2, step 424/574 completed (loss: 0.9309764504432678, acc: 0.8888888955116272)
[2025-01-02 00:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:50][root][INFO] - Training Epoch: 1/2, step 425/574 completed (loss: 0.5981173515319824, acc: 0.9090909361839294)
[2025-01-02 00:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:51][root][INFO] - Training Epoch: 1/2, step 426/574 completed (loss: 0.30733224749565125, acc: 0.8695651888847351)
[2025-01-02 00:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:51][root][INFO] - Training Epoch: 1/2, step 427/574 completed (loss: 0.45895782113075256, acc: 0.8918918967247009)
[2025-01-02 00:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:51][root][INFO] - Training Epoch: 1/2, step 428/574 completed (loss: 0.5391839146614075, acc: 0.8888888955116272)
[2025-01-02 00:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:20][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2726, device='cuda:0') eval_epoch_loss=tensor(0.8209, device='cuda:0') eval_epoch_acc=tensor(0.7820, device='cuda:0')
[2025-01-02 00:43:20][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:43:20][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:43:20][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_429_loss_0.8209146857261658/model.pt
[2025-01-02 00:43:20][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:43:20][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.8209146857261658
[2025-01-02 00:43:20][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7819832563400269
[2025-01-02 00:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:21][root][INFO] - Training Epoch: 1/2, step 429/574 completed (loss: 0.6097924113273621, acc: 0.8695651888847351)
[2025-01-02 00:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:21][root][INFO] - Training Epoch: 1/2, step 430/574 completed (loss: 0.05837055668234825, acc: 1.0)
[2025-01-02 00:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:21][root][INFO] - Training Epoch: 1/2, step 431/574 completed (loss: 0.30894240736961365, acc: 0.9259259104728699)
[2025-01-02 00:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:21][root][INFO] - Training Epoch: 1/2, step 432/574 completed (loss: 0.9460218548774719, acc: 0.782608687877655)
[2025-01-02 00:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:22][root][INFO] - Training Epoch: 1/2, step 433/574 completed (loss: 0.49343428015708923, acc: 0.8888888955116272)
[2025-01-02 00:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:22][root][INFO] - Training Epoch: 1/2, step 434/574 completed (loss: 0.013566466979682446, acc: 1.0)
[2025-01-02 00:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:23][root][INFO] - Training Epoch: 1/2, step 435/574 completed (loss: 0.19751609861850739, acc: 0.939393937587738)
[2025-01-02 00:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:23][root][INFO] - Training Epoch: 1/2, step 436/574 completed (loss: 0.5692617297172546, acc: 0.8611111044883728)
[2025-01-02 00:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:23][root][INFO] - Training Epoch: 1/2, step 437/574 completed (loss: 0.4383975863456726, acc: 0.8863636255264282)
[2025-01-02 00:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:24][root][INFO] - Training Epoch: 1/2, step 438/574 completed (loss: 0.20260637998580933, acc: 0.9523809552192688)
[2025-01-02 00:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:24][root][INFO] - Training Epoch: 1/2, step 439/574 completed (loss: 0.8319169878959656, acc: 0.8205128312110901)
[2025-01-02 00:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:24][root][INFO] - Training Epoch: 1/2, step 440/574 completed (loss: 0.9786441922187805, acc: 0.6969696879386902)
[2025-01-02 00:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:25][root][INFO] - Training Epoch: 1/2, step 441/574 completed (loss: 1.3853442668914795, acc: 0.6159999966621399)
[2025-01-02 00:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:26][root][INFO] - Training Epoch: 1/2, step 442/574 completed (loss: 1.2926418781280518, acc: 0.6451612710952759)
[2025-01-02 00:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:26][root][INFO] - Training Epoch: 1/2, step 443/574 completed (loss: 0.9762853980064392, acc: 0.7910447716712952)
[2025-01-02 00:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:27][root][INFO] - Training Epoch: 1/2, step 444/574 completed (loss: 0.6217185258865356, acc: 0.7924528121948242)
[2025-01-02 00:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:27][root][INFO] - Training Epoch: 1/2, step 445/574 completed (loss: 0.5872686505317688, acc: 0.8636363744735718)
[2025-01-02 00:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:27][root][INFO] - Training Epoch: 1/2, step 446/574 completed (loss: 0.7223207354545593, acc: 0.8260869383811951)
[2025-01-02 00:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:28][root][INFO] - Training Epoch: 1/2, step 447/574 completed (loss: 1.161546230316162, acc: 0.7692307829856873)
[2025-01-02 00:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:28][root][INFO] - Training Epoch: 1/2, step 448/574 completed (loss: 0.38916832208633423, acc: 0.9285714030265808)
[2025-01-02 00:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:28][root][INFO] - Training Epoch: 1/2, step 449/574 completed (loss: 0.5996367931365967, acc: 0.8656716346740723)
[2025-01-02 00:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:29][root][INFO] - Training Epoch: 1/2, step 450/574 completed (loss: 0.28945234417915344, acc: 0.9305555820465088)
[2025-01-02 00:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:29][root][INFO] - Training Epoch: 1/2, step 451/574 completed (loss: 0.2510492205619812, acc: 0.9347826242446899)
[2025-01-02 00:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:29][root][INFO] - Training Epoch: 1/2, step 452/574 completed (loss: 0.5720747113227844, acc: 0.8333333134651184)
[2025-01-02 00:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:30][root][INFO] - Training Epoch: 1/2, step 453/574 completed (loss: 0.9047011137008667, acc: 0.8421052694320679)
[2025-01-02 00:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:30][root][INFO] - Training Epoch: 1/2, step 454/574 completed (loss: 0.5115208029747009, acc: 0.8571428656578064)
[2025-01-02 00:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:30][root][INFO] - Training Epoch: 1/2, step 455/574 completed (loss: 0.5470913052558899, acc: 0.8787878751754761)
[2025-01-02 00:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:31][root][INFO] - Training Epoch: 1/2, step 456/574 completed (loss: 0.7634338736534119, acc: 0.8247422575950623)
[2025-01-02 00:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:31][root][INFO] - Training Epoch: 1/2, step 457/574 completed (loss: 0.2791040539741516, acc: 0.9428571462631226)
[2025-01-02 00:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:31][root][INFO] - Training Epoch: 1/2, step 458/574 completed (loss: 0.9676839113235474, acc: 0.7558139562606812)
[2025-01-02 00:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:32][root][INFO] - Training Epoch: 1/2, step 459/574 completed (loss: 0.21790345013141632, acc: 0.9464285969734192)
[2025-01-02 00:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:32][root][INFO] - Training Epoch: 1/2, step 460/574 completed (loss: 0.5495805144309998, acc: 0.8395061492919922)
[2025-01-02 00:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:32][root][INFO] - Training Epoch: 1/2, step 461/574 completed (loss: 0.9010792970657349, acc: 0.7222222089767456)
[2025-01-02 00:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:33][root][INFO] - Training Epoch: 1/2, step 462/574 completed (loss: 0.4345313608646393, acc: 0.875)
[2025-01-02 00:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:33][root][INFO] - Training Epoch: 1/2, step 463/574 completed (loss: 0.5494575500488281, acc: 0.8846153616905212)
[2025-01-02 00:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:33][root][INFO] - Training Epoch: 1/2, step 464/574 completed (loss: 0.5392590165138245, acc: 0.782608687877655)
[2025-01-02 00:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:34][root][INFO] - Training Epoch: 1/2, step 465/574 completed (loss: 0.7298398613929749, acc: 0.773809552192688)
[2025-01-02 00:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:34][root][INFO] - Training Epoch: 1/2, step 466/574 completed (loss: 0.8615332245826721, acc: 0.7831325531005859)
[2025-01-02 00:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:34][root][INFO] - Training Epoch: 1/2, step 467/574 completed (loss: 0.5326200723648071, acc: 0.8648648858070374)
[2025-01-02 00:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:35][root][INFO] - Training Epoch: 1/2, step 468/574 completed (loss: 1.2524218559265137, acc: 0.708737850189209)
[2025-01-02 00:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:35][root][INFO] - Training Epoch: 1/2, step 469/574 completed (loss: 0.8584423661231995, acc: 0.7642276287078857)
[2025-01-02 00:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:36][root][INFO] - Training Epoch: 1/2, step 470/574 completed (loss: 0.5410823225975037, acc: 0.8333333134651184)
[2025-01-02 00:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:36][root][INFO] - Training Epoch: 1/2, step 471/574 completed (loss: 0.8305784463882446, acc: 0.75)
[2025-01-02 00:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:36][root][INFO] - Training Epoch: 1/2, step 472/574 completed (loss: 1.2987197637557983, acc: 0.6078431606292725)
[2025-01-02 00:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:37][root][INFO] - Training Epoch: 1/2, step 473/574 completed (loss: 1.0241113901138306, acc: 0.7379912734031677)
[2025-01-02 00:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:37][root][INFO] - Training Epoch: 1/2, step 474/574 completed (loss: 0.9436690211296082, acc: 0.7395833134651184)
[2025-01-02 00:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:37][root][INFO] - Training Epoch: 1/2, step 475/574 completed (loss: 0.6169830560684204, acc: 0.8098159432411194)
[2025-01-02 00:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:38][root][INFO] - Training Epoch: 1/2, step 476/574 completed (loss: 0.6622976064682007, acc: 0.8201438784599304)
[2025-01-02 00:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:38][root][INFO] - Training Epoch: 1/2, step 477/574 completed (loss: 1.1834863424301147, acc: 0.6733668446540833)
[2025-01-02 00:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:38][root][INFO] - Training Epoch: 1/2, step 478/574 completed (loss: 0.9866683483123779, acc: 0.75)
[2025-01-02 00:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:39][root][INFO] - Training Epoch: 1/2, step 479/574 completed (loss: 1.2987689971923828, acc: 0.6363636255264282)
[2025-01-02 00:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:39][root][INFO] - Training Epoch: 1/2, step 480/574 completed (loss: 1.076332688331604, acc: 0.7037037014961243)
[2025-01-02 00:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:39][root][INFO] - Training Epoch: 1/2, step 481/574 completed (loss: 1.0995655059814453, acc: 0.699999988079071)
[2025-01-02 00:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:40][root][INFO] - Training Epoch: 1/2, step 482/574 completed (loss: 1.38528311252594, acc: 0.6499999761581421)
[2025-01-02 00:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:40][root][INFO] - Training Epoch: 1/2, step 483/574 completed (loss: 1.3727370500564575, acc: 0.517241358757019)
[2025-01-02 00:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:40][root][INFO] - Training Epoch: 1/2, step 484/574 completed (loss: 0.3402647376060486, acc: 0.9032257795333862)
[2025-01-02 00:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:41][root][INFO] - Training Epoch: 1/2, step 485/574 completed (loss: 1.0696340799331665, acc: 0.7894737124443054)
[2025-01-02 00:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:41][root][INFO] - Training Epoch: 1/2, step 486/574 completed (loss: 1.9880784749984741, acc: 0.37037035822868347)
[2025-01-02 00:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:41][root][INFO] - Training Epoch: 1/2, step 487/574 completed (loss: 0.9465441703796387, acc: 0.5714285969734192)
[2025-01-02 00:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:42][root][INFO] - Training Epoch: 1/2, step 488/574 completed (loss: 1.3652230501174927, acc: 0.7272727489471436)
[2025-01-02 00:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:42][root][INFO] - Training Epoch: 1/2, step 489/574 completed (loss: 1.3812777996063232, acc: 0.6307692527770996)
[2025-01-02 00:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:42][root][INFO] - Training Epoch: 1/2, step 490/574 completed (loss: 0.7470589280128479, acc: 0.8666666746139526)
[2025-01-02 00:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:43][root][INFO] - Training Epoch: 1/2, step 491/574 completed (loss: 0.845294177532196, acc: 0.8275862336158752)
[2025-01-02 00:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:43][root][INFO] - Training Epoch: 1/2, step 492/574 completed (loss: 0.8346754312515259, acc: 0.7058823704719543)
[2025-01-02 00:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:43][root][INFO] - Training Epoch: 1/2, step 493/574 completed (loss: 0.7304230332374573, acc: 0.7931034564971924)
[2025-01-02 00:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:44][root][INFO] - Training Epoch: 1/2, step 494/574 completed (loss: 0.8748641610145569, acc: 0.7894737124443054)
[2025-01-02 00:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:44][root][INFO] - Training Epoch: 1/2, step 495/574 completed (loss: 1.0833407640457153, acc: 0.7368420958518982)
[2025-01-02 00:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:45][root][INFO] - Training Epoch: 1/2, step 496/574 completed (loss: 1.0350700616836548, acc: 0.7232142686843872)
[2025-01-02 00:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:45][root][INFO] - Training Epoch: 1/2, step 497/574 completed (loss: 0.7698575258255005, acc: 0.7977527976036072)
[2025-01-02 00:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:45][root][INFO] - Training Epoch: 1/2, step 498/574 completed (loss: 1.0847077369689941, acc: 0.6853932738304138)
[2025-01-02 00:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:46][root][INFO] - Training Epoch: 1/2, step 499/574 completed (loss: 1.7180838584899902, acc: 0.5390070676803589)
[2025-01-02 00:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:46][root][INFO] - Training Epoch: 1/2, step 500/574 completed (loss: 1.256935477256775, acc: 0.739130437374115)
[2025-01-02 00:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:46][root][INFO] - Training Epoch: 1/2, step 501/574 completed (loss: 0.14654380083084106, acc: 1.0)
[2025-01-02 00:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:47][root][INFO] - Training Epoch: 1/2, step 502/574 completed (loss: 0.3405143916606903, acc: 0.8846153616905212)
[2025-01-02 00:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:47][root][INFO] - Training Epoch: 1/2, step 503/574 completed (loss: 0.6445003747940063, acc: 0.7777777910232544)
[2025-01-02 00:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:47][root][INFO] - Training Epoch: 1/2, step 504/574 completed (loss: 0.4984731376171112, acc: 0.8518518805503845)
[2025-01-02 00:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:48][root][INFO] - Training Epoch: 1/2, step 505/574 completed (loss: 0.8644101619720459, acc: 0.8301886916160583)
[2025-01-02 00:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:48][root][INFO] - Training Epoch: 1/2, step 506/574 completed (loss: 0.6563904881477356, acc: 0.8620689511299133)
[2025-01-02 00:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:49][root][INFO] - Training Epoch: 1/2, step 507/574 completed (loss: 1.6459354162216187, acc: 0.5495495200157166)
[2025-01-02 00:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:49][root][INFO] - Training Epoch: 1/2, step 508/574 completed (loss: 1.0780097246170044, acc: 0.7605633735656738)
[2025-01-02 00:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:49][root][INFO] - Training Epoch: 1/2, step 509/574 completed (loss: 0.4208000600337982, acc: 0.8500000238418579)
[2025-01-02 00:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:50][root][INFO] - Training Epoch: 1/2, step 510/574 completed (loss: 0.7068901062011719, acc: 0.7333333492279053)
[2025-01-02 00:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:50][root][INFO] - Training Epoch: 1/2, step 511/574 completed (loss: 0.9096481800079346, acc: 0.7307692170143127)
[2025-01-02 00:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:53][root][INFO] - Training Epoch: 1/2, step 512/574 completed (loss: 1.7806445360183716, acc: 0.5714285969734192)
[2025-01-02 00:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:54][root][INFO] - Training Epoch: 1/2, step 513/574 completed (loss: 0.7447519302368164, acc: 0.8333333134651184)
[2025-01-02 00:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:54][root][INFO] - Training Epoch: 1/2, step 514/574 completed (loss: 0.9827247858047485, acc: 0.7857142686843872)
[2025-01-02 00:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:54][root][INFO] - Training Epoch: 1/2, step 515/574 completed (loss: 0.32734590768814087, acc: 0.8999999761581421)
[2025-01-02 00:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:55][root][INFO] - Training Epoch: 1/2, step 516/574 completed (loss: 0.8480120897293091, acc: 0.7916666865348816)
[2025-01-02 00:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:55][root][INFO] - Training Epoch: 1/2, step 517/574 completed (loss: 0.08848810940980911, acc: 0.9615384340286255)
[2025-01-02 00:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:56][root][INFO] - Training Epoch: 1/2, step 518/574 completed (loss: 0.4141835570335388, acc: 0.8387096524238586)
[2025-01-02 00:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:56][root][INFO] - Training Epoch: 1/2, step 519/574 completed (loss: 0.5748397707939148, acc: 0.800000011920929)
[2025-01-02 00:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:56][root][INFO] - Training Epoch: 1/2, step 520/574 completed (loss: 0.6282604336738586, acc: 0.8148148059844971)
[2025-01-02 00:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:57][root][INFO] - Training Epoch: 1/2, step 521/574 completed (loss: 0.9966042637825012, acc: 0.7372881174087524)
[2025-01-02 00:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:58][root][INFO] - Training Epoch: 1/2, step 522/574 completed (loss: 0.5037257075309753, acc: 0.8507462739944458)
[2025-01-02 00:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:58][root][INFO] - Training Epoch: 1/2, step 523/574 completed (loss: 0.5584475994110107, acc: 0.8029196858406067)
[2025-01-02 00:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:59][root][INFO] - Training Epoch: 1/2, step 524/574 completed (loss: 1.0392217636108398, acc: 0.6899999976158142)
[2025-01-02 00:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:43:59][root][INFO] - Training Epoch: 1/2, step 525/574 completed (loss: 0.08316779136657715, acc: 1.0)
[2025-01-02 00:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:00][root][INFO] - Training Epoch: 1/2, step 526/574 completed (loss: 0.34885284304618835, acc: 0.8846153616905212)
[2025-01-02 00:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:00][root][INFO] - Training Epoch: 1/2, step 527/574 completed (loss: 0.648557186126709, acc: 0.8571428656578064)
[2025-01-02 00:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:00][root][INFO] - Training Epoch: 1/2, step 528/574 completed (loss: 2.3381805419921875, acc: 0.4262295067310333)
[2025-01-02 00:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:01][root][INFO] - Training Epoch: 1/2, step 529/574 completed (loss: 0.5051990151405334, acc: 0.7966101765632629)
[2025-01-02 00:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:01][root][INFO] - Training Epoch: 1/2, step 530/574 completed (loss: 1.8218848705291748, acc: 0.5581395626068115)
[2025-01-02 00:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:01][root][INFO] - Training Epoch: 1/2, step 531/574 completed (loss: 1.4684650897979736, acc: 0.6590909361839294)
[2025-01-02 00:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:02][root][INFO] - Training Epoch: 1/2, step 532/574 completed (loss: 1.5438965559005737, acc: 0.6037735939025879)
[2025-01-02 00:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:02][root][INFO] - Training Epoch: 1/2, step 533/574 completed (loss: 1.3665688037872314, acc: 0.6818181872367859)
[2025-01-02 00:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:03][root][INFO] - Training Epoch: 1/2, step 534/574 completed (loss: 1.002338171005249, acc: 0.7599999904632568)
[2025-01-02 00:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:03][root][INFO] - Training Epoch: 1/2, step 535/574 completed (loss: 0.8902385830879211, acc: 0.800000011920929)
[2025-01-02 00:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:03][root][INFO] - Training Epoch: 1/2, step 536/574 completed (loss: 0.546348512172699, acc: 0.8181818127632141)
[2025-01-02 00:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:04][root][INFO] - Training Epoch: 1/2, step 537/574 completed (loss: 0.9673441648483276, acc: 0.7692307829856873)
[2025-01-02 00:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:04][root][INFO] - Training Epoch: 1/2, step 538/574 completed (loss: 0.9858657121658325, acc: 0.75)
[2025-01-02 00:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:05][root][INFO] - Training Epoch: 1/2, step 539/574 completed (loss: 0.8467003107070923, acc: 0.75)
[2025-01-02 00:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:05][root][INFO] - Training Epoch: 1/2, step 540/574 completed (loss: 1.208911657333374, acc: 0.6666666865348816)
[2025-01-02 00:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:05][root][INFO] - Training Epoch: 1/2, step 541/574 completed (loss: 0.7867726683616638, acc: 0.6875)
[2025-01-02 00:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:06][root][INFO] - Training Epoch: 1/2, step 542/574 completed (loss: 0.1864745169878006, acc: 0.9354838728904724)
[2025-01-02 00:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:06][root][INFO] - Training Epoch: 1/2, step 543/574 completed (loss: 0.32991379499435425, acc: 0.95652174949646)
[2025-01-02 00:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:06][root][INFO] - Training Epoch: 1/2, step 544/574 completed (loss: 0.38899409770965576, acc: 0.8999999761581421)
[2025-01-02 00:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:07][root][INFO] - Training Epoch: 1/2, step 545/574 completed (loss: 0.30949610471725464, acc: 0.8780487775802612)
[2025-01-02 00:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:07][root][INFO] - Training Epoch: 1/2, step 546/574 completed (loss: 0.047699496150016785, acc: 0.9714285731315613)
[2025-01-02 00:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:08][root][INFO] - Training Epoch: 1/2, step 547/574 completed (loss: 0.10420898348093033, acc: 0.9736841917037964)
[2025-01-02 00:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:08][root][INFO] - Training Epoch: 1/2, step 548/574 completed (loss: 0.7057033777236938, acc: 0.774193525314331)
[2025-01-02 00:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:08][root][INFO] - Training Epoch: 1/2, step 549/574 completed (loss: 0.0734517052769661, acc: 1.0)
[2025-01-02 00:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:09][root][INFO] - Training Epoch: 1/2, step 550/574 completed (loss: 0.5114112496376038, acc: 0.9090909361839294)
[2025-01-02 00:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:09][root][INFO] - Training Epoch: 1/2, step 551/574 completed (loss: 0.34905555844306946, acc: 0.8999999761581421)
[2025-01-02 00:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:09][root][INFO] - Training Epoch: 1/2, step 552/574 completed (loss: 0.4086940288543701, acc: 0.8714285492897034)
[2025-01-02 00:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:10][root][INFO] - Training Epoch: 1/2, step 553/574 completed (loss: 0.6664745211601257, acc: 0.8321167826652527)
[2025-01-02 00:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:10][root][INFO] - Training Epoch: 1/2, step 554/574 completed (loss: 0.5600289106369019, acc: 0.8413792848587036)
[2025-01-02 00:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:11][root][INFO] - Training Epoch: 1/2, step 555/574 completed (loss: 0.6275069713592529, acc: 0.8428571224212646)
[2025-01-02 00:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:11][root][INFO] - Training Epoch: 1/2, step 556/574 completed (loss: 0.6385287046432495, acc: 0.8344370722770691)
[2025-01-02 00:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:11][root][INFO] - Training Epoch: 1/2, step 557/574 completed (loss: 0.5510947108268738, acc: 0.8803418874740601)
[2025-01-02 00:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:12][root][INFO] - Training Epoch: 1/2, step 558/574 completed (loss: 0.3278893530368805, acc: 0.8799999952316284)
[2025-01-02 00:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:12][root][INFO] - Training Epoch: 1/2, step 559/574 completed (loss: 0.6940935850143433, acc: 0.8461538553237915)
[2025-01-02 00:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:12][root][INFO] - Training Epoch: 1/2, step 560/574 completed (loss: 0.2221429944038391, acc: 0.9230769276618958)
[2025-01-02 00:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:13][root][INFO] - Training Epoch: 1/2, step 561/574 completed (loss: 0.28364527225494385, acc: 0.9487179517745972)
[2025-01-02 00:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:13][root][INFO] - Training Epoch: 1/2, step 562/574 completed (loss: 0.7907505631446838, acc: 0.8333333134651184)
[2025-01-02 00:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:13][root][INFO] - Training Epoch: 1/2, step 563/574 completed (loss: 0.4979042410850525, acc: 0.8831169009208679)
[2025-01-02 00:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:14][root][INFO] - Training Epoch: 1/2, step 564/574 completed (loss: 0.68310546875, acc: 0.8125)
[2025-01-02 00:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:14][root][INFO] - Training Epoch: 1/2, step 565/574 completed (loss: 0.4215903878211975, acc: 0.8448275923728943)
[2025-01-02 00:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:14][root][INFO] - Training Epoch: 1/2, step 566/574 completed (loss: 0.6061614155769348, acc: 0.8809523582458496)
[2025-01-02 00:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:15][root][INFO] - Training Epoch: 1/2, step 567/574 completed (loss: 0.08418630808591843, acc: 0.9736841917037964)
[2025-01-02 00:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:15][root][INFO] - Training Epoch: 1/2, step 568/574 completed (loss: 0.23149630427360535, acc: 0.8888888955116272)
[2025-01-02 00:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:16][root][INFO] - Training Epoch: 1/2, step 569/574 completed (loss: 0.4037863314151764, acc: 0.8823529481887817)
[2025-01-02 00:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:16][root][INFO] - Training Epoch: 1/2, step 570/574 completed (loss: 0.07807048410177231, acc: 0.9677419066429138)
[2025-01-02 00:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:16][root][INFO] - Training Epoch: 1/2, step 571/574 completed (loss: 0.6014483571052551, acc: 0.8547008633613586)
[2025-01-02 00:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:45][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9955, device='cuda:0') eval_epoch_loss=tensor(0.6909, device='cuda:0') eval_epoch_acc=tensor(0.8103, device='cuda:0')
[2025-01-02 00:44:45][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:44:45][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:44:46][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_572_loss_0.6909047961235046/model.pt
[2025-01-02 00:44:46][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:44:46][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6909047961235046
[2025-01-02 00:44:46][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8103269934654236
[2025-01-02 00:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:46][root][INFO] - Training Epoch: 1/2, step 572/574 completed (loss: 0.5608293414115906, acc: 0.8367347121238708)
[2025-01-02 00:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:46][root][INFO] - Training Epoch: 1/2, step 573/574 completed (loss: 0.5298458337783813, acc: 0.849056601524353)
[2025-01-02 00:44:47][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=3.8628, train_epoch_loss=1.3514, epoch time 361.4659474529326s
[2025-01-02 00:44:47][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-02 00:44:47][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-02 00:44:47][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-02 00:44:47][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2025-01-02 00:44:47][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-02 00:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:48][root][INFO] - Training Epoch: 2/2, step 0/574 completed (loss: 0.9124844670295715, acc: 0.7037037014961243)
[2025-01-02 00:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:48][root][INFO] - Training Epoch: 2/2, step 1/574 completed (loss: 0.6774526834487915, acc: 0.800000011920929)
[2025-01-02 00:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:48][root][INFO] - Training Epoch: 2/2, step 2/574 completed (loss: 0.9458051323890686, acc: 0.7297297120094299)
[2025-01-02 00:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:49][root][INFO] - Training Epoch: 2/2, step 3/574 completed (loss: 0.8646947145462036, acc: 0.7894737124443054)
[2025-01-02 00:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:49][root][INFO] - Training Epoch: 2/2, step 4/574 completed (loss: 1.0276401042938232, acc: 0.7837837934494019)
[2025-01-02 00:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:50][root][INFO] - Training Epoch: 2/2, step 5/574 completed (loss: 0.9392654299736023, acc: 0.75)
[2025-01-02 00:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:50][root][INFO] - Training Epoch: 2/2, step 6/574 completed (loss: 1.4259871244430542, acc: 0.5714285969734192)
[2025-01-02 00:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:50][root][INFO] - Training Epoch: 2/2, step 7/574 completed (loss: 0.9141570329666138, acc: 0.800000011920929)
[2025-01-02 00:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:51][root][INFO] - Training Epoch: 2/2, step 8/574 completed (loss: 0.3185177445411682, acc: 0.8636363744735718)
[2025-01-02 00:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:51][root][INFO] - Training Epoch: 2/2, step 9/574 completed (loss: 0.15636955201625824, acc: 0.9615384340286255)
[2025-01-02 00:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:52][root][INFO] - Training Epoch: 2/2, step 10/574 completed (loss: 0.28775763511657715, acc: 0.9259259104728699)
[2025-01-02 00:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:52][root][INFO] - Training Epoch: 2/2, step 11/574 completed (loss: 0.6571784615516663, acc: 0.8461538553237915)
[2025-01-02 00:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:52][root][INFO] - Training Epoch: 2/2, step 12/574 completed (loss: 0.20439140498638153, acc: 0.939393937587738)
[2025-01-02 00:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:53][root][INFO] - Training Epoch: 2/2, step 13/574 completed (loss: 0.44879627227783203, acc: 0.804347813129425)
[2025-01-02 00:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:53][root][INFO] - Training Epoch: 2/2, step 14/574 completed (loss: 0.4021383225917816, acc: 0.8823529481887817)
[2025-01-02 00:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:53][root][INFO] - Training Epoch: 2/2, step 15/574 completed (loss: 0.6364148855209351, acc: 0.8571428656578064)
[2025-01-02 00:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:54][root][INFO] - Training Epoch: 2/2, step 16/574 completed (loss: 0.3361192047595978, acc: 0.8947368264198303)
[2025-01-02 00:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:54][root][INFO] - Training Epoch: 2/2, step 17/574 completed (loss: 0.883822500705719, acc: 0.75)
[2025-01-02 00:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:54][root][INFO] - Training Epoch: 2/2, step 18/574 completed (loss: 1.226986289024353, acc: 0.6944444179534912)
[2025-01-02 00:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:55][root][INFO] - Training Epoch: 2/2, step 19/574 completed (loss: 0.7229117155075073, acc: 0.8421052694320679)
[2025-01-02 00:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:55][root][INFO] - Training Epoch: 2/2, step 20/574 completed (loss: 0.5479753017425537, acc: 0.8461538553237915)
[2025-01-02 00:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:55][root][INFO] - Training Epoch: 2/2, step 21/574 completed (loss: 0.8600219488143921, acc: 0.7931034564971924)
[2025-01-02 00:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:56][root][INFO] - Training Epoch: 2/2, step 22/574 completed (loss: 1.3219943046569824, acc: 0.5199999809265137)
[2025-01-02 00:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:56][root][INFO] - Training Epoch: 2/2, step 23/574 completed (loss: 1.049065113067627, acc: 0.761904776096344)
[2025-01-02 00:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:57][root][INFO] - Training Epoch: 2/2, step 24/574 completed (loss: 0.24452653527259827, acc: 0.9375)
[2025-01-02 00:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:57][root][INFO] - Training Epoch: 2/2, step 25/574 completed (loss: 1.0662915706634521, acc: 0.7735849022865295)
[2025-01-02 00:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:57][root][INFO] - Training Epoch: 2/2, step 26/574 completed (loss: 1.1471112966537476, acc: 0.6712328791618347)
[2025-01-02 00:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:59][root][INFO] - Training Epoch: 2/2, step 27/574 completed (loss: 1.2401103973388672, acc: 0.6679841876029968)
[2025-01-02 00:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:59][root][INFO] - Training Epoch: 2/2, step 28/574 completed (loss: 0.7237554788589478, acc: 0.7209302186965942)
[2025-01-02 00:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:44:59][root][INFO] - Training Epoch: 2/2, step 29/574 completed (loss: 0.8927239775657654, acc: 0.6867470145225525)
[2025-01-02 00:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:00][root][INFO] - Training Epoch: 2/2, step 30/574 completed (loss: 1.1135969161987305, acc: 0.7037037014961243)
[2025-01-02 00:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:00][root][INFO] - Training Epoch: 2/2, step 31/574 completed (loss: 1.1999682188034058, acc: 0.6785714030265808)
[2025-01-02 00:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:00][root][INFO] - Training Epoch: 2/2, step 32/574 completed (loss: 0.7350943088531494, acc: 0.7777777910232544)
[2025-01-02 00:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:01][root][INFO] - Training Epoch: 2/2, step 33/574 completed (loss: 0.22624905407428741, acc: 0.95652174949646)
[2025-01-02 00:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:01][root][INFO] - Training Epoch: 2/2, step 34/574 completed (loss: 0.6019973158836365, acc: 0.7983193397521973)
[2025-01-02 00:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:01][root][INFO] - Training Epoch: 2/2, step 35/574 completed (loss: 0.5825110673904419, acc: 0.8524590134620667)
[2025-01-02 00:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:02][root][INFO] - Training Epoch: 2/2, step 36/574 completed (loss: 0.8271243572235107, acc: 0.8253968358039856)
[2025-01-02 00:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:02][root][INFO] - Training Epoch: 2/2, step 37/574 completed (loss: 0.7776505351066589, acc: 0.8474576473236084)
[2025-01-02 00:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:02][root][INFO] - Training Epoch: 2/2, step 38/574 completed (loss: 0.5135668516159058, acc: 0.8850574493408203)
[2025-01-02 00:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:03][root][INFO] - Training Epoch: 2/2, step 39/574 completed (loss: 0.6415225267410278, acc: 0.8095238208770752)
[2025-01-02 00:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:03][root][INFO] - Training Epoch: 2/2, step 40/574 completed (loss: 0.8192837834358215, acc: 0.7692307829856873)
[2025-01-02 00:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:04][root][INFO] - Training Epoch: 2/2, step 41/574 completed (loss: 0.547429621219635, acc: 0.837837815284729)
[2025-01-02 00:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:04][root][INFO] - Training Epoch: 2/2, step 42/574 completed (loss: 1.0855330228805542, acc: 0.692307710647583)
[2025-01-02 00:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:04][root][INFO] - Training Epoch: 2/2, step 43/574 completed (loss: 0.9188240766525269, acc: 0.7777777910232544)
[2025-01-02 00:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:05][root][INFO] - Training Epoch: 2/2, step 44/574 completed (loss: 0.6045804023742676, acc: 0.8350515365600586)
[2025-01-02 00:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:05][root][INFO] - Training Epoch: 2/2, step 45/574 completed (loss: 0.6241723299026489, acc: 0.8308823704719543)
[2025-01-02 00:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:06][root][INFO] - Training Epoch: 2/2, step 46/574 completed (loss: 0.7339200377464294, acc: 0.7692307829856873)
[2025-01-02 00:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:06][root][INFO] - Training Epoch: 2/2, step 47/574 completed (loss: 0.35399889945983887, acc: 0.9629629850387573)
[2025-01-02 00:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:06][root][INFO] - Training Epoch: 2/2, step 48/574 completed (loss: 0.8259038925170898, acc: 0.7857142686843872)
[2025-01-02 00:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:07][root][INFO] - Training Epoch: 2/2, step 49/574 completed (loss: 0.4473608434200287, acc: 0.8333333134651184)
[2025-01-02 00:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:07][root][INFO] - Training Epoch: 2/2, step 50/574 completed (loss: 1.021304726600647, acc: 0.7543859481811523)
[2025-01-02 00:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:07][root][INFO] - Training Epoch: 2/2, step 51/574 completed (loss: 1.0099763870239258, acc: 0.6984127163887024)
[2025-01-02 00:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:08][root][INFO] - Training Epoch: 2/2, step 52/574 completed (loss: 1.2451261281967163, acc: 0.6901408433914185)
[2025-01-02 00:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:08][root][INFO] - Training Epoch: 2/2, step 53/574 completed (loss: 1.7022897005081177, acc: 0.5266666412353516)
[2025-01-02 00:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:09][root][INFO] - Training Epoch: 2/2, step 54/574 completed (loss: 1.6704847812652588, acc: 0.6216216087341309)
[2025-01-02 00:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:09][root][INFO] - Training Epoch: 2/2, step 55/574 completed (loss: 0.27382469177246094, acc: 0.8846153616905212)
[2025-01-02 00:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:12][root][INFO] - Training Epoch: 2/2, step 56/574 completed (loss: 1.4419469833374023, acc: 0.6109215021133423)
[2025-01-02 00:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:13][root][INFO] - Training Epoch: 2/2, step 57/574 completed (loss: 1.4146056175231934, acc: 0.6165577173233032)
[2025-01-02 00:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:14][root][INFO] - Training Epoch: 2/2, step 58/574 completed (loss: 1.1291139125823975, acc: 0.6761363744735718)
[2025-01-02 00:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:14][root][INFO] - Training Epoch: 2/2, step 59/574 completed (loss: 0.5901892781257629, acc: 0.8308823704719543)
[2025-01-02 00:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:15][root][INFO] - Training Epoch: 2/2, step 60/574 completed (loss: 1.1555426120758057, acc: 0.695652186870575)
[2025-01-02 00:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:15][root][INFO] - Training Epoch: 2/2, step 61/574 completed (loss: 1.2733081579208374, acc: 0.625)
[2025-01-02 00:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:16][root][INFO] - Training Epoch: 2/2, step 62/574 completed (loss: 0.5915985703468323, acc: 0.8529411554336548)
[2025-01-02 00:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:16][root][INFO] - Training Epoch: 2/2, step 63/574 completed (loss: 0.5845121145248413, acc: 0.8055555820465088)
[2025-01-02 00:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:16][root][INFO] - Training Epoch: 2/2, step 64/574 completed (loss: 0.376249223947525, acc: 0.890625)
[2025-01-02 00:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:17][root][INFO] - Training Epoch: 2/2, step 65/574 completed (loss: 0.3331270217895508, acc: 0.8965517282485962)
[2025-01-02 00:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:17][root][INFO] - Training Epoch: 2/2, step 66/574 completed (loss: 1.2523125410079956, acc: 0.6785714030265808)
[2025-01-02 00:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:18][root][INFO] - Training Epoch: 2/2, step 67/574 completed (loss: 0.5362033247947693, acc: 0.8500000238418579)
[2025-01-02 00:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:18][root][INFO] - Training Epoch: 2/2, step 68/574 completed (loss: 0.1256266087293625, acc: 0.9599999785423279)
[2025-01-02 00:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:18][root][INFO] - Training Epoch: 2/2, step 69/574 completed (loss: 1.196810245513916, acc: 0.6388888955116272)
[2025-01-02 00:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:19][root][INFO] - Training Epoch: 2/2, step 70/574 completed (loss: 1.3453692197799683, acc: 0.5757575631141663)
[2025-01-02 00:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:19][root][INFO] - Training Epoch: 2/2, step 71/574 completed (loss: 1.2151399850845337, acc: 0.6397058963775635)
[2025-01-02 00:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:19][root][INFO] - Training Epoch: 2/2, step 72/574 completed (loss: 0.9216346144676208, acc: 0.7539682388305664)
[2025-01-02 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:20][root][INFO] - Training Epoch: 2/2, step 73/574 completed (loss: 1.6395318508148193, acc: 0.5692307949066162)
[2025-01-02 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:20][root][INFO] - Training Epoch: 2/2, step 74/574 completed (loss: 1.415593147277832, acc: 0.6428571343421936)
[2025-01-02 00:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:20][root][INFO] - Training Epoch: 2/2, step 75/574 completed (loss: 1.4690258502960205, acc: 0.5970149040222168)
[2025-01-02 00:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:21][root][INFO] - Training Epoch: 2/2, step 76/574 completed (loss: 1.6021902561187744, acc: 0.5729926824569702)
[2025-01-02 00:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:21][root][INFO] - Training Epoch: 2/2, step 77/574 completed (loss: 0.08229812234640121, acc: 1.0)
[2025-01-02 00:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:22][root][INFO] - Training Epoch: 2/2, step 78/574 completed (loss: 0.3998325765132904, acc: 0.9166666865348816)
[2025-01-02 00:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:22][root][INFO] - Training Epoch: 2/2, step 79/574 completed (loss: 0.2946641743183136, acc: 0.8787878751754761)
[2025-01-02 00:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:22][root][INFO] - Training Epoch: 2/2, step 80/574 completed (loss: 0.572117269039154, acc: 0.8461538553237915)
[2025-01-02 00:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:23][root][INFO] - Training Epoch: 2/2, step 81/574 completed (loss: 0.906161367893219, acc: 0.7692307829856873)
[2025-01-02 00:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:23][root][INFO] - Training Epoch: 2/2, step 82/574 completed (loss: 0.9909586310386658, acc: 0.7692307829856873)
[2025-01-02 00:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:23][root][INFO] - Training Epoch: 2/2, step 83/574 completed (loss: 0.46997761726379395, acc: 0.875)
[2025-01-02 00:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:24][root][INFO] - Training Epoch: 2/2, step 84/574 completed (loss: 0.6538348197937012, acc: 0.8260869383811951)
[2025-01-02 00:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:24][root][INFO] - Training Epoch: 2/2, step 85/574 completed (loss: 0.961769163608551, acc: 0.7200000286102295)
[2025-01-02 00:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:24][root][INFO] - Training Epoch: 2/2, step 86/574 completed (loss: 0.5245355367660522, acc: 0.8695651888847351)
[2025-01-02 00:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:25][root][INFO] - Training Epoch: 2/2, step 87/574 completed (loss: 1.259476661682129, acc: 0.6000000238418579)
[2025-01-02 00:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:25][root][INFO] - Training Epoch: 2/2, step 88/574 completed (loss: 1.2209806442260742, acc: 0.6796116232872009)
[2025-01-02 00:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:26][root][INFO] - Training Epoch: 2/2, step 89/574 completed (loss: 1.1009974479675293, acc: 0.708737850189209)
[2025-01-02 00:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:27][root][INFO] - Training Epoch: 2/2, step 90/574 completed (loss: 1.4801056385040283, acc: 0.6129032373428345)
[2025-01-02 00:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:28][root][INFO] - Training Epoch: 2/2, step 91/574 completed (loss: 1.221411943435669, acc: 0.6637930870056152)
[2025-01-02 00:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:29][root][INFO] - Training Epoch: 2/2, step 92/574 completed (loss: 0.9035508036613464, acc: 0.7263157963752747)
[2025-01-02 00:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:30][root][INFO] - Training Epoch: 2/2, step 93/574 completed (loss: 1.809672236442566, acc: 0.48514851927757263)
[2025-01-02 00:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:30][root][INFO] - Training Epoch: 2/2, step 94/574 completed (loss: 1.4718034267425537, acc: 0.5483871102333069)
[2025-01-02 00:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:30][root][INFO] - Training Epoch: 2/2, step 95/574 completed (loss: 1.2238073348999023, acc: 0.6376811861991882)
[2025-01-02 00:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:31][root][INFO] - Training Epoch: 2/2, step 96/574 completed (loss: 1.4892754554748535, acc: 0.5546218752861023)
[2025-01-02 00:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:31][root][INFO] - Training Epoch: 2/2, step 97/574 completed (loss: 1.7006657123565674, acc: 0.5384615659713745)
[2025-01-02 00:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:31][root][INFO] - Training Epoch: 2/2, step 98/574 completed (loss: 1.6561508178710938, acc: 0.525547444820404)
[2025-01-02 00:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:32][root][INFO] - Training Epoch: 2/2, step 99/574 completed (loss: 1.7892506122589111, acc: 0.5223880410194397)
[2025-01-02 00:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:32][root][INFO] - Training Epoch: 2/2, step 100/574 completed (loss: 0.6476337313652039, acc: 0.800000011920929)
[2025-01-02 00:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:32][root][INFO] - Training Epoch: 2/2, step 101/574 completed (loss: 0.11194231361150742, acc: 1.0)
[2025-01-02 00:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:33][root][INFO] - Training Epoch: 2/2, step 102/574 completed (loss: 0.1558501273393631, acc: 1.0)
[2025-01-02 00:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:33][root][INFO] - Training Epoch: 2/2, step 103/574 completed (loss: 0.276003360748291, acc: 0.8863636255264282)
[2025-01-02 00:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:33][root][INFO] - Training Epoch: 2/2, step 104/574 completed (loss: 0.7620695233345032, acc: 0.7931034564971924)
[2025-01-02 00:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:34][root][INFO] - Training Epoch: 2/2, step 105/574 completed (loss: 0.46034660935401917, acc: 0.8604651093482971)
[2025-01-02 00:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:34][root][INFO] - Training Epoch: 2/2, step 106/574 completed (loss: 0.4543876647949219, acc: 0.8799999952316284)
[2025-01-02 00:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:34][root][INFO] - Training Epoch: 2/2, step 107/574 completed (loss: 0.07582376152276993, acc: 1.0)
[2025-01-02 00:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:35][root][INFO] - Training Epoch: 2/2, step 108/574 completed (loss: 0.14180204272270203, acc: 0.9615384340286255)
[2025-01-02 00:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:35][root][INFO] - Training Epoch: 2/2, step 109/574 completed (loss: 0.187638059258461, acc: 0.9047619104385376)
[2025-01-02 00:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:35][root][INFO] - Training Epoch: 2/2, step 110/574 completed (loss: 0.20384575426578522, acc: 0.9538461565971375)
[2025-01-02 00:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:36][root][INFO] - Training Epoch: 2/2, step 111/574 completed (loss: 0.7443204522132874, acc: 0.7894737124443054)
[2025-01-02 00:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:36][root][INFO] - Training Epoch: 2/2, step 112/574 completed (loss: 1.3024191856384277, acc: 0.6491228342056274)
[2025-01-02 00:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:36][root][INFO] - Training Epoch: 2/2, step 113/574 completed (loss: 0.8238357901573181, acc: 0.7948718070983887)
[2025-01-02 00:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:37][root][INFO] - Training Epoch: 2/2, step 114/574 completed (loss: 0.5462307333946228, acc: 0.8367347121238708)
[2025-01-02 00:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:37][root][INFO] - Training Epoch: 2/2, step 115/574 completed (loss: 0.23140357434749603, acc: 0.9545454382896423)
[2025-01-02 00:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:38][root][INFO] - Training Epoch: 2/2, step 116/574 completed (loss: 0.6766969561576843, acc: 0.8095238208770752)
[2025-01-02 00:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:38][root][INFO] - Training Epoch: 2/2, step 117/574 completed (loss: 0.6669535636901855, acc: 0.8292682766914368)
[2025-01-02 00:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:38][root][INFO] - Training Epoch: 2/2, step 118/574 completed (loss: 0.40581393241882324, acc: 0.9032257795333862)
[2025-01-02 00:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:39][root][INFO] - Training Epoch: 2/2, step 119/574 completed (loss: 0.9073807001113892, acc: 0.7756654024124146)
[2025-01-02 00:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:39][root][INFO] - Training Epoch: 2/2, step 120/574 completed (loss: 0.5184493660926819, acc: 0.8399999737739563)
[2025-01-02 00:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:40][root][INFO] - Training Epoch: 2/2, step 121/574 completed (loss: 0.6274763345718384, acc: 0.8653846383094788)
[2025-01-02 00:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:40][root][INFO] - Training Epoch: 2/2, step 122/574 completed (loss: 0.4319418668746948, acc: 0.7916666865348816)
[2025-01-02 00:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:40][root][INFO] - Training Epoch: 2/2, step 123/574 completed (loss: 0.4832645356655121, acc: 0.8421052694320679)
[2025-01-02 00:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:41][root][INFO] - Training Epoch: 2/2, step 124/574 completed (loss: 1.3103513717651367, acc: 0.6441717743873596)
[2025-01-02 00:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:41][root][INFO] - Training Epoch: 2/2, step 125/574 completed (loss: 1.3808741569519043, acc: 0.6180555820465088)
[2025-01-02 00:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:41][root][INFO] - Training Epoch: 2/2, step 126/574 completed (loss: 1.4481858015060425, acc: 0.6000000238418579)
[2025-01-02 00:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:42][root][INFO] - Training Epoch: 2/2, step 127/574 completed (loss: 1.0194635391235352, acc: 0.6964285969734192)
[2025-01-02 00:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:42][root][INFO] - Training Epoch: 2/2, step 128/574 completed (loss: 1.0719903707504272, acc: 0.7076923251152039)
[2025-01-02 00:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:42][root][INFO] - Training Epoch: 2/2, step 129/574 completed (loss: 1.1706552505493164, acc: 0.6691176295280457)
[2025-01-02 00:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:43][root][INFO] - Training Epoch: 2/2, step 130/574 completed (loss: 1.1137207746505737, acc: 0.692307710647583)
[2025-01-02 00:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:43][root][INFO] - Training Epoch: 2/2, step 131/574 completed (loss: 0.8669174313545227, acc: 0.6086956262588501)
[2025-01-02 00:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:43][root][INFO] - Training Epoch: 2/2, step 132/574 completed (loss: 1.395291805267334, acc: 0.59375)
[2025-01-02 00:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:44][root][INFO] - Training Epoch: 2/2, step 133/574 completed (loss: 1.6353415250778198, acc: 0.5652173757553101)
[2025-01-02 00:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:44][root][INFO] - Training Epoch: 2/2, step 134/574 completed (loss: 1.0082969665527344, acc: 0.6285714507102966)
[2025-01-02 00:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:44][root][INFO] - Training Epoch: 2/2, step 135/574 completed (loss: 1.1564621925354004, acc: 0.7307692170143127)
[2025-01-02 00:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:45][root][INFO] - Training Epoch: 2/2, step 136/574 completed (loss: 0.9973049163818359, acc: 0.7142857313156128)
[2025-01-02 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:45][root][INFO] - Training Epoch: 2/2, step 137/574 completed (loss: 1.4318007230758667, acc: 0.5)
[2025-01-02 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:45][root][INFO] - Training Epoch: 2/2, step 138/574 completed (loss: 1.1971009969711304, acc: 0.695652186870575)
[2025-01-02 00:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:46][root][INFO] - Training Epoch: 2/2, step 139/574 completed (loss: 0.4443656802177429, acc: 0.8571428656578064)
[2025-01-02 00:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:46][root][INFO] - Training Epoch: 2/2, step 140/574 completed (loss: 0.5591672658920288, acc: 0.7692307829856873)
[2025-01-02 00:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:16][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0195, device='cuda:0') eval_epoch_loss=tensor(0.7028, device='cuda:0') eval_epoch_acc=tensor(0.8059, device='cuda:0')
[2025-01-02 00:46:16][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:46:16][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:46:16][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_141_loss_0.7028403282165527/model.pt
[2025-01-02 00:46:16][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:16][root][INFO] - Training Epoch: 2/2, step 141/574 completed (loss: 1.111975908279419, acc: 0.7096773982048035)
[2025-01-02 00:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:17][root][INFO] - Training Epoch: 2/2, step 142/574 completed (loss: 1.3321881294250488, acc: 0.6486486196517944)
[2025-01-02 00:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:17][root][INFO] - Training Epoch: 2/2, step 143/574 completed (loss: 1.1621216535568237, acc: 0.6666666865348816)
[2025-01-02 00:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:18][root][INFO] - Training Epoch: 2/2, step 144/574 completed (loss: 0.9174838662147522, acc: 0.7835820913314819)
[2025-01-02 00:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:18][root][INFO] - Training Epoch: 2/2, step 145/574 completed (loss: 0.9219920039176941, acc: 0.6938775777816772)
[2025-01-02 00:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:18][root][INFO] - Training Epoch: 2/2, step 146/574 completed (loss: 1.3442537784576416, acc: 0.5744680762290955)
[2025-01-02 00:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:19][root][INFO] - Training Epoch: 2/2, step 147/574 completed (loss: 1.151896357536316, acc: 0.6714285612106323)
[2025-01-02 00:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:19][root][INFO] - Training Epoch: 2/2, step 148/574 completed (loss: 1.2521506547927856, acc: 0.6428571343421936)
[2025-01-02 00:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:19][root][INFO] - Training Epoch: 2/2, step 149/574 completed (loss: 1.0773855447769165, acc: 0.6521739363670349)
[2025-01-02 00:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:20][root][INFO] - Training Epoch: 2/2, step 150/574 completed (loss: 0.9225040078163147, acc: 0.7241379022598267)
[2025-01-02 00:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:20][root][INFO] - Training Epoch: 2/2, step 151/574 completed (loss: 1.4070367813110352, acc: 0.6521739363670349)
[2025-01-02 00:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:21][root][INFO] - Training Epoch: 2/2, step 152/574 completed (loss: 0.8809707760810852, acc: 0.7796609997749329)
[2025-01-02 00:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:21][root][INFO] - Training Epoch: 2/2, step 153/574 completed (loss: 1.1554696559906006, acc: 0.6666666865348816)
[2025-01-02 00:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:21][root][INFO] - Training Epoch: 2/2, step 154/574 completed (loss: 0.9410272836685181, acc: 0.7567567825317383)
[2025-01-02 00:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:22][root][INFO] - Training Epoch: 2/2, step 155/574 completed (loss: 0.4600163400173187, acc: 0.8214285969734192)
[2025-01-02 00:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:22][root][INFO] - Training Epoch: 2/2, step 156/574 completed (loss: 0.6059880256652832, acc: 0.8260869383811951)
[2025-01-02 00:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:22][root][INFO] - Training Epoch: 2/2, step 157/574 completed (loss: 2.2107927799224854, acc: 0.3684210479259491)
[2025-01-02 00:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:24][root][INFO] - Training Epoch: 2/2, step 158/574 completed (loss: 1.3966407775878906, acc: 0.5675675868988037)
[2025-01-02 00:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:24][root][INFO] - Training Epoch: 2/2, step 159/574 completed (loss: 1.5614553689956665, acc: 0.5)
[2025-01-02 00:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:25][root][INFO] - Training Epoch: 2/2, step 160/574 completed (loss: 1.5636985301971436, acc: 0.569767415523529)
[2025-01-02 00:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:25][root][INFO] - Training Epoch: 2/2, step 161/574 completed (loss: 1.8473049402236938, acc: 0.4588235318660736)
[2025-01-02 00:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:26][root][INFO] - Training Epoch: 2/2, step 162/574 completed (loss: 1.8602302074432373, acc: 0.550561785697937)
[2025-01-02 00:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:26][root][INFO] - Training Epoch: 2/2, step 163/574 completed (loss: 0.6748567819595337, acc: 0.8863636255264282)
[2025-01-02 00:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:27][root][INFO] - Training Epoch: 2/2, step 164/574 completed (loss: 0.634529173374176, acc: 0.9047619104385376)
[2025-01-02 00:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:27][root][INFO] - Training Epoch: 2/2, step 165/574 completed (loss: 1.2255070209503174, acc: 0.6551724076271057)
[2025-01-02 00:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:27][root][INFO] - Training Epoch: 2/2, step 166/574 completed (loss: 0.2856191396713257, acc: 0.8775510191917419)
[2025-01-02 00:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:28][root][INFO] - Training Epoch: 2/2, step 167/574 completed (loss: 0.38573840260505676, acc: 0.8399999737739563)
[2025-01-02 00:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:28][root][INFO] - Training Epoch: 2/2, step 168/574 completed (loss: 0.7384030818939209, acc: 0.7916666865348816)
[2025-01-02 00:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:28][root][INFO] - Training Epoch: 2/2, step 169/574 completed (loss: 1.1500128507614136, acc: 0.7450980544090271)
[2025-01-02 00:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:29][root][INFO] - Training Epoch: 2/2, step 170/574 completed (loss: 1.1921526193618774, acc: 0.6780821681022644)
[2025-01-02 00:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:30][root][INFO] - Training Epoch: 2/2, step 171/574 completed (loss: 0.19833029806613922, acc: 0.9583333134651184)
[2025-01-02 00:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:30][root][INFO] - Training Epoch: 2/2, step 172/574 completed (loss: 0.6238297820091248, acc: 0.7777777910232544)
[2025-01-02 00:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:30][root][INFO] - Training Epoch: 2/2, step 173/574 completed (loss: 1.1669389009475708, acc: 0.6785714030265808)
[2025-01-02 00:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:31][root][INFO] - Training Epoch: 2/2, step 174/574 completed (loss: 1.1923125982284546, acc: 0.7168141603469849)
[2025-01-02 00:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:31][root][INFO] - Training Epoch: 2/2, step 175/574 completed (loss: 0.9634955525398254, acc: 0.7681159377098083)
[2025-01-02 00:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:32][root][INFO] - Training Epoch: 2/2, step 176/574 completed (loss: 0.746195375919342, acc: 0.7840909361839294)
[2025-01-02 00:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:33][root][INFO] - Training Epoch: 2/2, step 177/574 completed (loss: 1.4408495426177979, acc: 0.572519063949585)
[2025-01-02 00:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:33][root][INFO] - Training Epoch: 2/2, step 178/574 completed (loss: 1.3520551919937134, acc: 0.6222222447395325)
[2025-01-02 00:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:34][root][INFO] - Training Epoch: 2/2, step 179/574 completed (loss: 0.7153638005256653, acc: 0.8032786846160889)
[2025-01-02 00:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:34][root][INFO] - Training Epoch: 2/2, step 180/574 completed (loss: 0.09580855816602707, acc: 0.9583333134651184)
[2025-01-02 00:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:34][root][INFO] - Training Epoch: 2/2, step 181/574 completed (loss: 0.14568859338760376, acc: 0.9200000166893005)
[2025-01-02 00:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:35][root][INFO] - Training Epoch: 2/2, step 182/574 completed (loss: 0.3413195312023163, acc: 0.8928571343421936)
[2025-01-02 00:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:35][root][INFO] - Training Epoch: 2/2, step 183/574 completed (loss: 0.4495880901813507, acc: 0.8780487775802612)
[2025-01-02 00:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:35][root][INFO] - Training Epoch: 2/2, step 184/574 completed (loss: 0.553597092628479, acc: 0.8580060601234436)
[2025-01-02 00:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:36][root][INFO] - Training Epoch: 2/2, step 185/574 completed (loss: 0.6147966384887695, acc: 0.8443803787231445)
[2025-01-02 00:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:36][root][INFO] - Training Epoch: 2/2, step 186/574 completed (loss: 0.5313649773597717, acc: 0.8031250238418579)
[2025-01-02 00:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:37][root][INFO] - Training Epoch: 2/2, step 187/574 completed (loss: 0.5826783180236816, acc: 0.8386491537094116)
[2025-01-02 00:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:37][root][INFO] - Training Epoch: 2/2, step 188/574 completed (loss: 0.6797831058502197, acc: 0.7971529960632324)
[2025-01-02 00:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:38][root][INFO] - Training Epoch: 2/2, step 189/574 completed (loss: 0.6838026642799377, acc: 0.7599999904632568)
[2025-01-02 00:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:38][root][INFO] - Training Epoch: 2/2, step 190/574 completed (loss: 1.2994730472564697, acc: 0.604651153087616)
[2025-01-02 00:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:39][root][INFO] - Training Epoch: 2/2, step 191/574 completed (loss: 1.8537015914916992, acc: 0.5079365372657776)
[2025-01-02 00:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:40][root][INFO] - Training Epoch: 2/2, step 192/574 completed (loss: 1.4746090173721313, acc: 0.5757575631141663)
[2025-01-02 00:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:41][root][INFO] - Training Epoch: 2/2, step 193/574 completed (loss: 1.168418288230896, acc: 0.6352941393852234)
[2025-01-02 00:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:42][root][INFO] - Training Epoch: 2/2, step 194/574 completed (loss: 1.310115098953247, acc: 0.604938268661499)
[2025-01-02 00:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:43][root][INFO] - Training Epoch: 2/2, step 195/574 completed (loss: 0.82551109790802, acc: 0.725806474685669)
[2025-01-02 00:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:43][root][INFO] - Training Epoch: 2/2, step 196/574 completed (loss: 0.39548900723457336, acc: 0.8928571343421936)
[2025-01-02 00:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:43][root][INFO] - Training Epoch: 2/2, step 197/574 completed (loss: 1.2718628644943237, acc: 0.625)
[2025-01-02 00:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:44][root][INFO] - Training Epoch: 2/2, step 198/574 completed (loss: 1.120703101158142, acc: 0.6911764740943909)
[2025-01-02 00:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:44][root][INFO] - Training Epoch: 2/2, step 199/574 completed (loss: 1.2111268043518066, acc: 0.6985294222831726)
[2025-01-02 00:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:45][root][INFO] - Training Epoch: 2/2, step 200/574 completed (loss: 1.0008800029754639, acc: 0.6864407062530518)
[2025-01-02 00:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:45][root][INFO] - Training Epoch: 2/2, step 201/574 completed (loss: 1.1266902685165405, acc: 0.7089552283287048)
[2025-01-02 00:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:45][root][INFO] - Training Epoch: 2/2, step 202/574 completed (loss: 1.1111516952514648, acc: 0.7281553149223328)
[2025-01-02 00:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:46][root][INFO] - Training Epoch: 2/2, step 203/574 completed (loss: 0.9769495129585266, acc: 0.7142857313156128)
[2025-01-02 00:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:46][root][INFO] - Training Epoch: 2/2, step 204/574 completed (loss: 0.2022113800048828, acc: 0.9560439586639404)
[2025-01-02 00:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:46][root][INFO] - Training Epoch: 2/2, step 205/574 completed (loss: 0.44021832942962646, acc: 0.8834080696105957)
[2025-01-02 00:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:47][root][INFO] - Training Epoch: 2/2, step 206/574 completed (loss: 0.5160427093505859, acc: 0.8543307185173035)
[2025-01-02 00:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:47][root][INFO] - Training Epoch: 2/2, step 207/574 completed (loss: 0.5217982530593872, acc: 0.8663793206214905)
[2025-01-02 00:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:47][root][INFO] - Training Epoch: 2/2, step 208/574 completed (loss: 0.5748584866523743, acc: 0.8695651888847351)
[2025-01-02 00:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:48][root][INFO] - Training Epoch: 2/2, step 209/574 completed (loss: 0.5150178074836731, acc: 0.8638132214546204)
[2025-01-02 00:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:48][root][INFO] - Training Epoch: 2/2, step 210/574 completed (loss: 0.45107510685920715, acc: 0.9130434989929199)
[2025-01-02 00:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:48][root][INFO] - Training Epoch: 2/2, step 211/574 completed (loss: 0.5547838807106018, acc: 0.8260869383811951)
[2025-01-02 00:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:49][root][INFO] - Training Epoch: 2/2, step 212/574 completed (loss: 0.24880042672157288, acc: 0.9285714030265808)
[2025-01-02 00:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:49][root][INFO] - Training Epoch: 2/2, step 213/574 completed (loss: 0.3117772936820984, acc: 0.914893627166748)
[2025-01-02 00:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:50][root][INFO] - Training Epoch: 2/2, step 214/574 completed (loss: 0.33827418088912964, acc: 0.9153845906257629)
[2025-01-02 00:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:50][root][INFO] - Training Epoch: 2/2, step 215/574 completed (loss: 0.37115585803985596, acc: 0.8918918967247009)
[2025-01-02 00:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:51][root][INFO] - Training Epoch: 2/2, step 216/574 completed (loss: 0.34776297211647034, acc: 0.895348846912384)
[2025-01-02 00:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:51][root][INFO] - Training Epoch: 2/2, step 217/574 completed (loss: 0.4420880973339081, acc: 0.9009009003639221)
[2025-01-02 00:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:52][root][INFO] - Training Epoch: 2/2, step 218/574 completed (loss: 0.24332386255264282, acc: 0.9333333373069763)
[2025-01-02 00:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:52][root][INFO] - Training Epoch: 2/2, step 219/574 completed (loss: 0.2996911406517029, acc: 0.9090909361839294)
[2025-01-02 00:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:52][root][INFO] - Training Epoch: 2/2, step 220/574 completed (loss: 0.2599581182003021, acc: 0.8518518805503845)
[2025-01-02 00:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:53][root][INFO] - Training Epoch: 2/2, step 221/574 completed (loss: 0.3187073767185211, acc: 0.8399999737739563)
[2025-01-02 00:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:53][root][INFO] - Training Epoch: 2/2, step 222/574 completed (loss: 1.0362471342086792, acc: 0.692307710647583)
[2025-01-02 00:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:54][root][INFO] - Training Epoch: 2/2, step 223/574 completed (loss: 0.5044193863868713, acc: 0.8804348111152649)
[2025-01-02 00:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:54][root][INFO] - Training Epoch: 2/2, step 224/574 completed (loss: 0.7234319448471069, acc: 0.7613636255264282)
[2025-01-02 00:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:55][root][INFO] - Training Epoch: 2/2, step 225/574 completed (loss: 1.0311483144760132, acc: 0.7659574747085571)
[2025-01-02 00:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:55][root][INFO] - Training Epoch: 2/2, step 226/574 completed (loss: 0.8231806755065918, acc: 0.7735849022865295)
[2025-01-02 00:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:56][root][INFO] - Training Epoch: 2/2, step 227/574 completed (loss: 0.516089677810669, acc: 0.800000011920929)
[2025-01-02 00:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:56][root][INFO] - Training Epoch: 2/2, step 228/574 completed (loss: 0.4088277518749237, acc: 0.8604651093482971)
[2025-01-02 00:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:56][root][INFO] - Training Epoch: 2/2, step 229/574 completed (loss: 1.7844713926315308, acc: 0.46666666865348816)
[2025-01-02 00:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:57][root][INFO] - Training Epoch: 2/2, step 230/574 completed (loss: 2.1617705821990967, acc: 0.46315789222717285)
[2025-01-02 00:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:57][root][INFO] - Training Epoch: 2/2, step 231/574 completed (loss: 1.6447198390960693, acc: 0.5777778029441833)
[2025-01-02 00:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:57][root][INFO] - Training Epoch: 2/2, step 232/574 completed (loss: 1.6778175830841064, acc: 0.5222222208976746)
[2025-01-02 00:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:58][root][INFO] - Training Epoch: 2/2, step 233/574 completed (loss: 1.9350014925003052, acc: 0.47706422209739685)
[2025-01-02 00:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:58][root][INFO] - Training Epoch: 2/2, step 234/574 completed (loss: 1.6489176750183105, acc: 0.5230769515037537)
[2025-01-02 00:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:59][root][INFO] - Training Epoch: 2/2, step 235/574 completed (loss: 0.6037936806678772, acc: 0.8421052694320679)
[2025-01-02 00:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:59][root][INFO] - Training Epoch: 2/2, step 236/574 completed (loss: 0.7434808611869812, acc: 0.75)
[2025-01-02 00:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:46:59][root][INFO] - Training Epoch: 2/2, step 237/574 completed (loss: 1.4272652864456177, acc: 0.6363636255264282)
[2025-01-02 00:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:00][root][INFO] - Training Epoch: 2/2, step 238/574 completed (loss: 0.83221435546875, acc: 0.7777777910232544)
[2025-01-02 00:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:00][root][INFO] - Training Epoch: 2/2, step 239/574 completed (loss: 1.033820390701294, acc: 0.7142857313156128)
[2025-01-02 00:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:01][root][INFO] - Training Epoch: 2/2, step 240/574 completed (loss: 1.2613255977630615, acc: 0.6590909361839294)
[2025-01-02 00:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:01][root][INFO] - Training Epoch: 2/2, step 241/574 completed (loss: 0.8955293297767639, acc: 0.7272727489471436)
[2025-01-02 00:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:02][root][INFO] - Training Epoch: 2/2, step 242/574 completed (loss: 1.4194068908691406, acc: 0.5645161271095276)
[2025-01-02 00:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:02][root][INFO] - Training Epoch: 2/2, step 243/574 completed (loss: 1.4863208532333374, acc: 0.5909090638160706)
[2025-01-02 00:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:02][root][INFO] - Training Epoch: 2/2, step 244/574 completed (loss: 0.16377563774585724, acc: 0.9523809552192688)
[2025-01-02 00:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:03][root][INFO] - Training Epoch: 2/2, step 245/574 completed (loss: 0.674846887588501, acc: 0.7692307829856873)
[2025-01-02 00:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:03][root][INFO] - Training Epoch: 2/2, step 246/574 completed (loss: 0.484453409910202, acc: 0.8709677457809448)
[2025-01-02 00:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:03][root][INFO] - Training Epoch: 2/2, step 247/574 completed (loss: 0.47655004262924194, acc: 0.800000011920929)
[2025-01-02 00:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:04][root][INFO] - Training Epoch: 2/2, step 248/574 completed (loss: 0.4059503674507141, acc: 0.9189189076423645)
[2025-01-02 00:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:04][root][INFO] - Training Epoch: 2/2, step 249/574 completed (loss: 0.5715776085853577, acc: 0.8648648858070374)
[2025-01-02 00:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:05][root][INFO] - Training Epoch: 2/2, step 250/574 completed (loss: 0.15919199585914612, acc: 1.0)
[2025-01-02 00:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:05][root][INFO] - Training Epoch: 2/2, step 251/574 completed (loss: 0.470597505569458, acc: 0.8382353186607361)
[2025-01-02 00:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:05][root][INFO] - Training Epoch: 2/2, step 252/574 completed (loss: 0.2181006371974945, acc: 0.9268292784690857)
[2025-01-02 00:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:06][root][INFO] - Training Epoch: 2/2, step 253/574 completed (loss: 0.15158426761627197, acc: 0.9200000166893005)
[2025-01-02 00:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:06][root][INFO] - Training Epoch: 2/2, step 254/574 completed (loss: 0.06938344985246658, acc: 0.9599999785423279)
[2025-01-02 00:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:06][root][INFO] - Training Epoch: 2/2, step 255/574 completed (loss: 0.30988022685050964, acc: 0.8709677457809448)
[2025-01-02 00:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:07][root][INFO] - Training Epoch: 2/2, step 256/574 completed (loss: 0.35594311356544495, acc: 0.9122806787490845)
[2025-01-02 00:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:07][root][INFO] - Training Epoch: 2/2, step 257/574 completed (loss: 0.2923228144645691, acc: 0.8999999761581421)
[2025-01-02 00:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:07][root][INFO] - Training Epoch: 2/2, step 258/574 completed (loss: 0.23984374105930328, acc: 0.9210526347160339)
[2025-01-02 00:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:08][root][INFO] - Training Epoch: 2/2, step 259/574 completed (loss: 0.5169633030891418, acc: 0.8867924809455872)
[2025-01-02 00:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:09][root][INFO] - Training Epoch: 2/2, step 260/574 completed (loss: 0.48412826657295227, acc: 0.8833333253860474)
[2025-01-02 00:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:09][root][INFO] - Training Epoch: 2/2, step 261/574 completed (loss: 0.2616880536079407, acc: 0.9166666865348816)
[2025-01-02 00:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:09][root][INFO] - Training Epoch: 2/2, step 262/574 completed (loss: 0.898044228553772, acc: 0.7419354915618896)
[2025-01-02 00:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:10][root][INFO] - Training Epoch: 2/2, step 263/574 completed (loss: 1.6264326572418213, acc: 0.653333306312561)
[2025-01-02 00:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:10][root][INFO] - Training Epoch: 2/2, step 264/574 completed (loss: 0.9267535209655762, acc: 0.6458333134651184)
[2025-01-02 00:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:11][root][INFO] - Training Epoch: 2/2, step 265/574 completed (loss: 1.5329816341400146, acc: 0.5759999752044678)
[2025-01-02 00:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:11][root][INFO] - Training Epoch: 2/2, step 266/574 completed (loss: 1.7422881126403809, acc: 0.584269642829895)
[2025-01-02 00:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:12][root][INFO] - Training Epoch: 2/2, step 267/574 completed (loss: 1.332161545753479, acc: 0.5810810923576355)
[2025-01-02 00:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:12][root][INFO] - Training Epoch: 2/2, step 268/574 completed (loss: 0.9834010601043701, acc: 0.7068965435028076)
[2025-01-02 00:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:12][root][INFO] - Training Epoch: 2/2, step 269/574 completed (loss: 0.27745354175567627, acc: 0.9090909361839294)
[2025-01-02 00:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:13][root][INFO] - Training Epoch: 2/2, step 270/574 completed (loss: 0.2696075141429901, acc: 0.9090909361839294)
[2025-01-02 00:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:13][root][INFO] - Training Epoch: 2/2, step 271/574 completed (loss: 0.32025620341300964, acc: 0.90625)
[2025-01-02 00:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:14][root][INFO] - Training Epoch: 2/2, step 272/574 completed (loss: 0.2315036505460739, acc: 0.9666666388511658)
[2025-01-02 00:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:14][root][INFO] - Training Epoch: 2/2, step 273/574 completed (loss: 0.44091513752937317, acc: 0.9166666865348816)
[2025-01-02 00:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:14][root][INFO] - Training Epoch: 2/2, step 274/574 completed (loss: 0.47320517897605896, acc: 0.84375)
[2025-01-02 00:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:15][root][INFO] - Training Epoch: 2/2, step 275/574 completed (loss: 0.3550277650356293, acc: 0.9333333373069763)
[2025-01-02 00:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:15][root][INFO] - Training Epoch: 2/2, step 276/574 completed (loss: 0.5499384999275208, acc: 0.8965517282485962)
[2025-01-02 00:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:15][root][INFO] - Training Epoch: 2/2, step 277/574 completed (loss: 0.2802356779575348, acc: 0.9599999785423279)
[2025-01-02 00:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:16][root][INFO] - Training Epoch: 2/2, step 278/574 completed (loss: 0.7449029088020325, acc: 0.8085106611251831)
[2025-01-02 00:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:16][root][INFO] - Training Epoch: 2/2, step 279/574 completed (loss: 0.6255424618721008, acc: 0.8541666865348816)
[2025-01-02 00:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:16][root][INFO] - Training Epoch: 2/2, step 280/574 completed (loss: 0.2743220925331116, acc: 0.9318181872367859)
[2025-01-02 00:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:17][root][INFO] - Training Epoch: 2/2, step 281/574 completed (loss: 0.994485080242157, acc: 0.7228915691375732)
[2025-01-02 00:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:17][root][INFO] - Training Epoch: 2/2, step 282/574 completed (loss: 1.1246556043624878, acc: 0.7129629850387573)
[2025-01-02 00:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:18][root][INFO] - Training Epoch: 2/2, step 283/574 completed (loss: 0.27558279037475586, acc: 0.9736841917037964)
[2025-01-02 00:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:47][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9307, device='cuda:0') eval_epoch_loss=tensor(0.6579, device='cuda:0') eval_epoch_acc=tensor(0.8161, device='cuda:0')
[2025-01-02 00:47:47][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:47:47][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:47:47][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.6578754186630249/model.pt
[2025-01-02 00:47:47][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:47:47][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6578754186630249
[2025-01-02 00:47:47][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8160508275032043
[2025-01-02 00:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:48][root][INFO] - Training Epoch: 2/2, step 284/574 completed (loss: 0.5990118980407715, acc: 0.8235294222831726)
[2025-01-02 00:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:48][root][INFO] - Training Epoch: 2/2, step 285/574 completed (loss: 0.5418704748153687, acc: 0.8999999761581421)
[2025-01-02 00:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:48][root][INFO] - Training Epoch: 2/2, step 286/574 completed (loss: 0.6615339517593384, acc: 0.8203125)
[2025-01-02 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:49][root][INFO] - Training Epoch: 2/2, step 287/574 completed (loss: 0.6842729449272156, acc: 0.8159999847412109)
[2025-01-02 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:49][root][INFO] - Training Epoch: 2/2, step 288/574 completed (loss: 0.6449192762374878, acc: 0.8131868243217468)
[2025-01-02 00:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:49][root][INFO] - Training Epoch: 2/2, step 289/574 completed (loss: 0.6195071935653687, acc: 0.8260869383811951)
[2025-01-02 00:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:50][root][INFO] - Training Epoch: 2/2, step 290/574 completed (loss: 0.6437070965766907, acc: 0.8195876479148865)
[2025-01-02 00:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:50][root][INFO] - Training Epoch: 2/2, step 291/574 completed (loss: 0.1234099343419075, acc: 0.9545454382896423)
[2025-01-02 00:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:50][root][INFO] - Training Epoch: 2/2, step 292/574 completed (loss: 0.7898426055908203, acc: 0.8095238208770752)
[2025-01-02 00:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:51][root][INFO] - Training Epoch: 2/2, step 293/574 completed (loss: 0.294440895318985, acc: 0.931034505367279)
[2025-01-02 00:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:51][root][INFO] - Training Epoch: 2/2, step 294/574 completed (loss: 0.6962355375289917, acc: 0.800000011920929)
[2025-01-02 00:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:52][root][INFO] - Training Epoch: 2/2, step 295/574 completed (loss: 0.7055964469909668, acc: 0.8144329786300659)
[2025-01-02 00:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:52][root][INFO] - Training Epoch: 2/2, step 296/574 completed (loss: 0.6044155359268188, acc: 0.8103448152542114)
[2025-01-02 00:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:53][root][INFO] - Training Epoch: 2/2, step 297/574 completed (loss: 0.19098952412605286, acc: 0.9629629850387573)
[2025-01-02 00:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:53][root][INFO] - Training Epoch: 2/2, step 298/574 completed (loss: 0.6317788362503052, acc: 0.7368420958518982)
[2025-01-02 00:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:53][root][INFO] - Training Epoch: 2/2, step 299/574 completed (loss: 0.2689908742904663, acc: 0.9285714030265808)
[2025-01-02 00:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:54][root][INFO] - Training Epoch: 2/2, step 300/574 completed (loss: 0.22139519453048706, acc: 0.875)
[2025-01-02 00:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:54][root][INFO] - Training Epoch: 2/2, step 301/574 completed (loss: 0.5770386457443237, acc: 0.849056601524353)
[2025-01-02 00:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:54][root][INFO] - Training Epoch: 2/2, step 302/574 completed (loss: 0.07491473108530045, acc: 0.9811320900917053)
[2025-01-02 00:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:55][root][INFO] - Training Epoch: 2/2, step 303/574 completed (loss: 0.13929474353790283, acc: 0.9411764740943909)
[2025-01-02 00:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:55][root][INFO] - Training Epoch: 2/2, step 304/574 completed (loss: 0.30400726199150085, acc: 0.90625)
[2025-01-02 00:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:55][root][INFO] - Training Epoch: 2/2, step 305/574 completed (loss: 0.5692434310913086, acc: 0.8360655903816223)
[2025-01-02 00:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:56][root][INFO] - Training Epoch: 2/2, step 306/574 completed (loss: 0.1702529639005661, acc: 0.9333333373069763)
[2025-01-02 00:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:56][root][INFO] - Training Epoch: 2/2, step 307/574 completed (loss: 0.03772460296750069, acc: 1.0)
[2025-01-02 00:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:56][root][INFO] - Training Epoch: 2/2, step 308/574 completed (loss: 0.468142569065094, acc: 0.8695651888847351)
[2025-01-02 00:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:57][root][INFO] - Training Epoch: 2/2, step 309/574 completed (loss: 0.5173702836036682, acc: 0.8888888955116272)
[2025-01-02 00:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:57][root][INFO] - Training Epoch: 2/2, step 310/574 completed (loss: 0.38378578424453735, acc: 0.8674699068069458)
[2025-01-02 00:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:58][root][INFO] - Training Epoch: 2/2, step 311/574 completed (loss: 0.5052756071090698, acc: 0.807692289352417)
[2025-01-02 00:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:58][root][INFO] - Training Epoch: 2/2, step 312/574 completed (loss: 0.16356372833251953, acc: 0.9591836929321289)
[2025-01-02 00:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:58][root][INFO] - Training Epoch: 2/2, step 313/574 completed (loss: 0.030792566016316414, acc: 1.0)
[2025-01-02 00:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:59][root][INFO] - Training Epoch: 2/2, step 314/574 completed (loss: 0.21877919137477875, acc: 0.9166666865348816)
[2025-01-02 00:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:59][root][INFO] - Training Epoch: 2/2, step 315/574 completed (loss: 0.301364928483963, acc: 0.9677419066429138)
[2025-01-02 00:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:47:59][root][INFO] - Training Epoch: 2/2, step 316/574 completed (loss: 0.734933614730835, acc: 0.8064516186714172)
[2025-01-02 00:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:00][root][INFO] - Training Epoch: 2/2, step 317/574 completed (loss: 0.35470154881477356, acc: 0.8805969953536987)
[2025-01-02 00:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:00][root][INFO] - Training Epoch: 2/2, step 318/574 completed (loss: 0.12660542130470276, acc: 0.9615384340286255)
[2025-01-02 00:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:00][root][INFO] - Training Epoch: 2/2, step 319/574 completed (loss: 0.3009259104728699, acc: 0.9333333373069763)
[2025-01-02 00:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:01][root][INFO] - Training Epoch: 2/2, step 320/574 completed (loss: 0.29672273993492126, acc: 0.9193548560142517)
[2025-01-02 00:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:01][root][INFO] - Training Epoch: 2/2, step 321/574 completed (loss: 0.02773737907409668, acc: 1.0)
[2025-01-02 00:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:01][root][INFO] - Training Epoch: 2/2, step 322/574 completed (loss: 1.5351834297180176, acc: 0.5925925970077515)
[2025-01-02 00:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:02][root][INFO] - Training Epoch: 2/2, step 323/574 completed (loss: 2.12319016456604, acc: 0.4285714328289032)
[2025-01-02 00:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:02][root][INFO] - Training Epoch: 2/2, step 324/574 completed (loss: 1.637305736541748, acc: 0.6153846383094788)
[2025-01-02 00:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:02][root][INFO] - Training Epoch: 2/2, step 325/574 completed (loss: 1.6535040140151978, acc: 0.5609756112098694)
[2025-01-02 00:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:03][root][INFO] - Training Epoch: 2/2, step 326/574 completed (loss: 1.499983787536621, acc: 0.6052631735801697)
[2025-01-02 00:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:03][root][INFO] - Training Epoch: 2/2, step 327/574 completed (loss: 0.5348008275032043, acc: 0.8947368264198303)
[2025-01-02 00:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:03][root][INFO] - Training Epoch: 2/2, step 328/574 completed (loss: 0.12355675548315048, acc: 0.9642857313156128)
[2025-01-02 00:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:04][root][INFO] - Training Epoch: 2/2, step 329/574 completed (loss: 0.2643274664878845, acc: 0.8518518805503845)
[2025-01-02 00:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:04][root][INFO] - Training Epoch: 2/2, step 330/574 completed (loss: 0.0752735286951065, acc: 0.96875)
[2025-01-02 00:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:05][root][INFO] - Training Epoch: 2/2, step 331/574 completed (loss: 0.3570118844509125, acc: 0.8870967626571655)
[2025-01-02 00:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:05][root][INFO] - Training Epoch: 2/2, step 332/574 completed (loss: 0.2022656351327896, acc: 0.9473684430122375)
[2025-01-02 00:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:05][root][INFO] - Training Epoch: 2/2, step 333/574 completed (loss: 0.5564901232719421, acc: 0.875)
[2025-01-02 00:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:06][root][INFO] - Training Epoch: 2/2, step 334/574 completed (loss: 0.2682091295719147, acc: 0.9333333373069763)
[2025-01-02 00:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:06][root][INFO] - Training Epoch: 2/2, step 335/574 completed (loss: 0.5474388599395752, acc: 0.8947368264198303)
[2025-01-02 00:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:06][root][INFO] - Training Epoch: 2/2, step 336/574 completed (loss: 1.0090289115905762, acc: 0.6800000071525574)
[2025-01-02 00:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:07][root][INFO] - Training Epoch: 2/2, step 337/574 completed (loss: 1.4837937355041504, acc: 0.5977011322975159)
[2025-01-02 00:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:07][root][INFO] - Training Epoch: 2/2, step 338/574 completed (loss: 1.3380897045135498, acc: 0.6063829660415649)
[2025-01-02 00:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:07][root][INFO] - Training Epoch: 2/2, step 339/574 completed (loss: 1.5413577556610107, acc: 0.6144578456878662)
[2025-01-02 00:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:08][root][INFO] - Training Epoch: 2/2, step 340/574 completed (loss: 0.08624375611543655, acc: 1.0)
[2025-01-02 00:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:08][root][INFO] - Training Epoch: 2/2, step 341/574 completed (loss: 0.7511066794395447, acc: 0.7948718070983887)
[2025-01-02 00:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:09][root][INFO] - Training Epoch: 2/2, step 342/574 completed (loss: 0.5061196684837341, acc: 0.8554216623306274)
[2025-01-02 00:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:09][root][INFO] - Training Epoch: 2/2, step 343/574 completed (loss: 0.8414302468299866, acc: 0.7358490824699402)
[2025-01-02 00:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:09][root][INFO] - Training Epoch: 2/2, step 344/574 completed (loss: 0.17948202788829803, acc: 0.949367105960846)
[2025-01-02 00:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:10][root][INFO] - Training Epoch: 2/2, step 345/574 completed (loss: 0.1154036745429039, acc: 0.9803921580314636)
[2025-01-02 00:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:10][root][INFO] - Training Epoch: 2/2, step 346/574 completed (loss: 0.5462613105773926, acc: 0.8656716346740723)
[2025-01-02 00:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:10][root][INFO] - Training Epoch: 2/2, step 347/574 completed (loss: 0.14793702960014343, acc: 0.949999988079071)
[2025-01-02 00:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:11][root][INFO] - Training Epoch: 2/2, step 348/574 completed (loss: 0.2330361008644104, acc: 0.9599999785423279)
[2025-01-02 00:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:11][root][INFO] - Training Epoch: 2/2, step 349/574 completed (loss: 0.9722051620483398, acc: 0.75)
[2025-01-02 00:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:12][root][INFO] - Training Epoch: 2/2, step 350/574 completed (loss: 0.9408183693885803, acc: 0.7441860437393188)
[2025-01-02 00:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:12][root][INFO] - Training Epoch: 2/2, step 351/574 completed (loss: 0.41816744208335876, acc: 0.8717948794364929)
[2025-01-02 00:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:12][root][INFO] - Training Epoch: 2/2, step 352/574 completed (loss: 1.469020128250122, acc: 0.5555555820465088)
[2025-01-02 00:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:13][root][INFO] - Training Epoch: 2/2, step 353/574 completed (loss: 0.23143640160560608, acc: 0.95652174949646)
[2025-01-02 00:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:13][root][INFO] - Training Epoch: 2/2, step 354/574 completed (loss: 0.5376570224761963, acc: 0.7692307829856873)
[2025-01-02 00:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:13][root][INFO] - Training Epoch: 2/2, step 355/574 completed (loss: 1.009989857673645, acc: 0.692307710647583)
[2025-01-02 00:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:14][root][INFO] - Training Epoch: 2/2, step 356/574 completed (loss: 0.7981839179992676, acc: 0.747826099395752)
[2025-01-02 00:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:14][root][INFO] - Training Epoch: 2/2, step 357/574 completed (loss: 0.615811288356781, acc: 0.8260869383811951)
[2025-01-02 00:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:15][root][INFO] - Training Epoch: 2/2, step 358/574 completed (loss: 0.7782459855079651, acc: 0.7551020383834839)
[2025-01-02 00:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:15][root][INFO] - Training Epoch: 2/2, step 359/574 completed (loss: 0.010184361599385738, acc: 1.0)
[2025-01-02 00:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:15][root][INFO] - Training Epoch: 2/2, step 360/574 completed (loss: 0.3157027065753937, acc: 0.9230769276618958)
[2025-01-02 00:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:16][root][INFO] - Training Epoch: 2/2, step 361/574 completed (loss: 0.6263071298599243, acc: 0.8048780560493469)
[2025-01-02 00:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:16][root][INFO] - Training Epoch: 2/2, step 362/574 completed (loss: 0.5536567568778992, acc: 0.8666666746139526)
[2025-01-02 00:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:16][root][INFO] - Training Epoch: 2/2, step 363/574 completed (loss: 0.27019283175468445, acc: 0.9078947305679321)
[2025-01-02 00:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:17][root][INFO] - Training Epoch: 2/2, step 364/574 completed (loss: 0.1735139638185501, acc: 0.9268292784690857)
[2025-01-02 00:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:17][root][INFO] - Training Epoch: 2/2, step 365/574 completed (loss: 0.19815589487552643, acc: 0.9696969985961914)
[2025-01-02 00:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:17][root][INFO] - Training Epoch: 2/2, step 366/574 completed (loss: 0.010017733089625835, acc: 1.0)
[2025-01-02 00:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:18][root][INFO] - Training Epoch: 2/2, step 367/574 completed (loss: 0.08799762278795242, acc: 0.95652174949646)
[2025-01-02 00:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:18][root][INFO] - Training Epoch: 2/2, step 368/574 completed (loss: 0.13878293335437775, acc: 0.9642857313156128)
[2025-01-02 00:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:19][root][INFO] - Training Epoch: 2/2, step 369/574 completed (loss: 0.4520649313926697, acc: 0.875)
[2025-01-02 00:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:19][root][INFO] - Training Epoch: 2/2, step 370/574 completed (loss: 0.6813608407974243, acc: 0.8121212124824524)
[2025-01-02 00:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:20][root][INFO] - Training Epoch: 2/2, step 371/574 completed (loss: 0.4775567650794983, acc: 0.8773584961891174)
[2025-01-02 00:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:20][root][INFO] - Training Epoch: 2/2, step 372/574 completed (loss: 0.26357516646385193, acc: 0.9222221970558167)
[2025-01-02 00:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:21][root][INFO] - Training Epoch: 2/2, step 373/574 completed (loss: 0.3044445514678955, acc: 0.9642857313156128)
[2025-01-02 00:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:21][root][INFO] - Training Epoch: 2/2, step 374/574 completed (loss: 0.2739754021167755, acc: 0.9142857193946838)
[2025-01-02 00:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:21][root][INFO] - Training Epoch: 2/2, step 375/574 completed (loss: 0.008376657031476498, acc: 1.0)
[2025-01-02 00:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:22][root][INFO] - Training Epoch: 2/2, step 376/574 completed (loss: 0.11987566947937012, acc: 0.9130434989929199)
[2025-01-02 00:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:22][root][INFO] - Training Epoch: 2/2, step 377/574 completed (loss: 0.18246476352214813, acc: 0.8958333134651184)
[2025-01-02 00:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:22][root][INFO] - Training Epoch: 2/2, step 378/574 completed (loss: 0.0995660275220871, acc: 0.9684210419654846)
[2025-01-02 00:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:23][root][INFO] - Training Epoch: 2/2, step 379/574 completed (loss: 0.35844436287879944, acc: 0.8982036113739014)
[2025-01-02 00:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:23][root][INFO] - Training Epoch: 2/2, step 380/574 completed (loss: 0.44310882687568665, acc: 0.9097744226455688)
[2025-01-02 00:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:25][root][INFO] - Training Epoch: 2/2, step 381/574 completed (loss: 0.7215206623077393, acc: 0.8235294222831726)
[2025-01-02 00:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:25][root][INFO] - Training Epoch: 2/2, step 382/574 completed (loss: 0.16626258194446564, acc: 0.9459459185600281)
[2025-01-02 00:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:25][root][INFO] - Training Epoch: 2/2, step 383/574 completed (loss: 0.5390798449516296, acc: 0.8571428656578064)
[2025-01-02 00:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:26][root][INFO] - Training Epoch: 2/2, step 384/574 completed (loss: 0.05125012993812561, acc: 1.0)
[2025-01-02 00:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:26][root][INFO] - Training Epoch: 2/2, step 385/574 completed (loss: 0.08312999457120895, acc: 0.96875)
[2025-01-02 00:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:26][root][INFO] - Training Epoch: 2/2, step 386/574 completed (loss: 0.023185541853308678, acc: 1.0)
[2025-01-02 00:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:27][root][INFO] - Training Epoch: 2/2, step 387/574 completed (loss: 0.04627525433897972, acc: 0.9736841917037964)
[2025-01-02 00:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:27][root][INFO] - Training Epoch: 2/2, step 388/574 completed (loss: 0.08834069222211838, acc: 0.9545454382896423)
[2025-01-02 00:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:27][root][INFO] - Training Epoch: 2/2, step 389/574 completed (loss: 0.014071579091250896, acc: 1.0)
[2025-01-02 00:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:28][root][INFO] - Training Epoch: 2/2, step 390/574 completed (loss: 0.30430588126182556, acc: 0.8571428656578064)
[2025-01-02 00:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:28][root][INFO] - Training Epoch: 2/2, step 391/574 completed (loss: 1.3259576559066772, acc: 0.6666666865348816)
[2025-01-02 00:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:28][root][INFO] - Training Epoch: 2/2, step 392/574 completed (loss: 1.198205590248108, acc: 0.6893203854560852)
[2025-01-02 00:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:29][root][INFO] - Training Epoch: 2/2, step 393/574 completed (loss: 1.1195297241210938, acc: 0.720588207244873)
[2025-01-02 00:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:29][root][INFO] - Training Epoch: 2/2, step 394/574 completed (loss: 1.058861255645752, acc: 0.6666666865348816)
[2025-01-02 00:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:30][root][INFO] - Training Epoch: 2/2, step 395/574 completed (loss: 1.0146396160125732, acc: 0.7569444179534912)
[2025-01-02 00:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:30][root][INFO] - Training Epoch: 2/2, step 396/574 completed (loss: 0.6774406433105469, acc: 0.7906976938247681)
[2025-01-02 00:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:30][root][INFO] - Training Epoch: 2/2, step 397/574 completed (loss: 0.2759086787700653, acc: 0.9166666865348816)
[2025-01-02 00:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:31][root][INFO] - Training Epoch: 2/2, step 398/574 completed (loss: 0.33219078183174133, acc: 0.8837209343910217)
[2025-01-02 00:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:31][root][INFO] - Training Epoch: 2/2, step 399/574 completed (loss: 0.2790674567222595, acc: 0.9200000166893005)
[2025-01-02 00:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:31][root][INFO] - Training Epoch: 2/2, step 400/574 completed (loss: 0.36517009139060974, acc: 0.8970588445663452)
[2025-01-02 00:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:32][root][INFO] - Training Epoch: 2/2, step 401/574 completed (loss: 0.5438112616539001, acc: 0.8266666531562805)
[2025-01-02 00:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:32][root][INFO] - Training Epoch: 2/2, step 402/574 completed (loss: 0.735771119594574, acc: 0.8787878751754761)
[2025-01-02 00:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:32][root][INFO] - Training Epoch: 2/2, step 403/574 completed (loss: 0.37137579917907715, acc: 0.9090909361839294)
[2025-01-02 00:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:33][root][INFO] - Training Epoch: 2/2, step 404/574 completed (loss: 0.40346068143844604, acc: 0.8709677457809448)
[2025-01-02 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:33][root][INFO] - Training Epoch: 2/2, step 405/574 completed (loss: 0.15819033980369568, acc: 0.9259259104728699)
[2025-01-02 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:33][root][INFO] - Training Epoch: 2/2, step 406/574 completed (loss: 0.21722137928009033, acc: 0.9599999785423279)
[2025-01-02 00:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:34][root][INFO] - Training Epoch: 2/2, step 407/574 completed (loss: 0.08935532718896866, acc: 0.9722222089767456)
[2025-01-02 00:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:34][root][INFO] - Training Epoch: 2/2, step 408/574 completed (loss: 0.25564122200012207, acc: 0.9259259104728699)
[2025-01-02 00:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:34][root][INFO] - Training Epoch: 2/2, step 409/574 completed (loss: 0.19020342826843262, acc: 0.9615384340286255)
[2025-01-02 00:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:35][root][INFO] - Training Epoch: 2/2, step 410/574 completed (loss: 0.21168409287929535, acc: 0.9655172228813171)
[2025-01-02 00:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:35][root][INFO] - Training Epoch: 2/2, step 411/574 completed (loss: 0.05376775190234184, acc: 1.0)
[2025-01-02 00:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:35][root][INFO] - Training Epoch: 2/2, step 412/574 completed (loss: 0.18381769955158234, acc: 0.9666666388511658)
[2025-01-02 00:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:36][root][INFO] - Training Epoch: 2/2, step 413/574 completed (loss: 0.2924279272556305, acc: 0.939393937587738)
[2025-01-02 00:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:36][root][INFO] - Training Epoch: 2/2, step 414/574 completed (loss: 0.12082414329051971, acc: 0.9545454382896423)
[2025-01-02 00:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:36][root][INFO] - Training Epoch: 2/2, step 415/574 completed (loss: 0.604336678981781, acc: 0.8235294222831726)
[2025-01-02 00:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37][root][INFO] - Training Epoch: 2/2, step 416/574 completed (loss: 0.3830519914627075, acc: 0.8846153616905212)
[2025-01-02 00:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37][root][INFO] - Training Epoch: 2/2, step 417/574 completed (loss: 0.34063395857810974, acc: 0.9444444179534912)
[2025-01-02 00:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37][root][INFO] - Training Epoch: 2/2, step 418/574 completed (loss: 0.46317917108535767, acc: 0.8999999761581421)
[2025-01-02 00:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:37][root][INFO] - Training Epoch: 2/2, step 419/574 completed (loss: 0.4908730387687683, acc: 0.8999999761581421)
[2025-01-02 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:38][root][INFO] - Training Epoch: 2/2, step 420/574 completed (loss: 0.04957888275384903, acc: 1.0)
[2025-01-02 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:38][root][INFO] - Training Epoch: 2/2, step 421/574 completed (loss: 0.3043033480644226, acc: 0.9333333373069763)
[2025-01-02 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:38][root][INFO] - Training Epoch: 2/2, step 422/574 completed (loss: 0.3333136737346649, acc: 0.90625)
[2025-01-02 00:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:39][root][INFO] - Training Epoch: 2/2, step 423/574 completed (loss: 0.566709578037262, acc: 0.8611111044883728)
[2025-01-02 00:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:39][root][INFO] - Training Epoch: 2/2, step 424/574 completed (loss: 0.4973582923412323, acc: 0.9259259104728699)
[2025-01-02 00:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:39][root][INFO] - Training Epoch: 2/2, step 425/574 completed (loss: 0.17641279101371765, acc: 0.939393937587738)
[2025-01-02 00:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:40][root][INFO] - Training Epoch: 2/2, step 426/574 completed (loss: 0.022014213725924492, acc: 1.0)
[2025-01-02 00:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:09][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8662, device='cuda:0') eval_epoch_loss=tensor(0.6239, device='cuda:0') eval_epoch_acc=tensor(0.8262, device='cuda:0')
[2025-01-02 00:49:09][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:49:09][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:49:09][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.6238870024681091/model.pt
[2025-01-02 00:49:09][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:49:09][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6238870024681091
[2025-01-02 00:49:09][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.826225757598877
[2025-01-02 00:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:10][root][INFO] - Training Epoch: 2/2, step 427/574 completed (loss: 0.24313436448574066, acc: 0.9189189076423645)
[2025-01-02 00:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:10][root][INFO] - Training Epoch: 2/2, step 428/574 completed (loss: 0.0341838076710701, acc: 1.0)
[2025-01-02 00:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11][root][INFO] - Training Epoch: 2/2, step 429/574 completed (loss: 0.3209170401096344, acc: 0.9130434989929199)
[2025-01-02 00:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11][root][INFO] - Training Epoch: 2/2, step 430/574 completed (loss: 0.005056640598922968, acc: 1.0)
[2025-01-02 00:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11][root][INFO] - Training Epoch: 2/2, step 431/574 completed (loss: 0.01781943254172802, acc: 1.0)
[2025-01-02 00:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:11][root][INFO] - Training Epoch: 2/2, step 432/574 completed (loss: 0.31653374433517456, acc: 0.9130434989929199)
[2025-01-02 00:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:12][root][INFO] - Training Epoch: 2/2, step 433/574 completed (loss: 0.26259666681289673, acc: 0.9166666865348816)
[2025-01-02 00:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:12][root][INFO] - Training Epoch: 2/2, step 434/574 completed (loss: 0.002150279004126787, acc: 1.0)
[2025-01-02 00:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:12][root][INFO] - Training Epoch: 2/2, step 435/574 completed (loss: 0.04477909579873085, acc: 0.9696969985961914)
[2025-01-02 00:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:13][root][INFO] - Training Epoch: 2/2, step 436/574 completed (loss: 0.5629217624664307, acc: 0.8333333134651184)
[2025-01-02 00:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:13][root][INFO] - Training Epoch: 2/2, step 437/574 completed (loss: 0.07971729338169098, acc: 0.9545454382896423)
[2025-01-02 00:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:13][root][INFO] - Training Epoch: 2/2, step 438/574 completed (loss: 0.020442456007003784, acc: 1.0)
[2025-01-02 00:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:14][root][INFO] - Training Epoch: 2/2, step 439/574 completed (loss: 0.47963690757751465, acc: 0.8717948794364929)
[2025-01-02 00:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:14][root][INFO] - Training Epoch: 2/2, step 440/574 completed (loss: 0.524680495262146, acc: 0.8484848737716675)
[2025-01-02 00:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:15][root][INFO] - Training Epoch: 2/2, step 441/574 completed (loss: 1.026268482208252, acc: 0.6959999799728394)
[2025-01-02 00:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:15][root][INFO] - Training Epoch: 2/2, step 442/574 completed (loss: 1.030521035194397, acc: 0.75)
[2025-01-02 00:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:16][root][INFO] - Training Epoch: 2/2, step 443/574 completed (loss: 0.6317675113677979, acc: 0.8557214140892029)
[2025-01-02 00:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:16][root][INFO] - Training Epoch: 2/2, step 444/574 completed (loss: 0.35535332560539246, acc: 0.8679245114326477)
[2025-01-02 00:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:17][root][INFO] - Training Epoch: 2/2, step 445/574 completed (loss: 0.237130269408226, acc: 0.9090909361839294)
[2025-01-02 00:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:17][root][INFO] - Training Epoch: 2/2, step 446/574 completed (loss: 0.39897337555885315, acc: 0.8260869383811951)
[2025-01-02 00:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:17][root][INFO] - Training Epoch: 2/2, step 447/574 completed (loss: 0.7137216329574585, acc: 0.807692289352417)
[2025-01-02 00:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:18][root][INFO] - Training Epoch: 2/2, step 448/574 completed (loss: 0.2620560824871063, acc: 0.9642857313156128)
[2025-01-02 00:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:18][root][INFO] - Training Epoch: 2/2, step 449/574 completed (loss: 0.2192775160074234, acc: 0.9253731369972229)
[2025-01-02 00:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:18][root][INFO] - Training Epoch: 2/2, step 450/574 completed (loss: 0.07067989557981491, acc: 1.0)
[2025-01-02 00:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:19][root][INFO] - Training Epoch: 2/2, step 451/574 completed (loss: 0.11219379305839539, acc: 0.95652174949646)
[2025-01-02 00:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:19][root][INFO] - Training Epoch: 2/2, step 452/574 completed (loss: 0.3601292669773102, acc: 0.8974359035491943)
[2025-01-02 00:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:19][root][INFO] - Training Epoch: 2/2, step 453/574 completed (loss: 0.43413102626800537, acc: 0.8552631735801697)
[2025-01-02 00:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20][root][INFO] - Training Epoch: 2/2, step 454/574 completed (loss: 0.24114200472831726, acc: 0.9591836929321289)
[2025-01-02 00:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20][root][INFO] - Training Epoch: 2/2, step 455/574 completed (loss: 0.34109461307525635, acc: 0.9090909361839294)
[2025-01-02 00:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20][root][INFO] - Training Epoch: 2/2, step 456/574 completed (loss: 0.6768763661384583, acc: 0.8350515365600586)
[2025-01-02 00:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:20][root][INFO] - Training Epoch: 2/2, step 457/574 completed (loss: 0.07024888694286346, acc: 0.9714285731315613)
[2025-01-02 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:21][root][INFO] - Training Epoch: 2/2, step 458/574 completed (loss: 0.5429477691650391, acc: 0.8720930218696594)
[2025-01-02 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:21][root][INFO] - Training Epoch: 2/2, step 459/574 completed (loss: 0.12649479508399963, acc: 0.9285714030265808)
[2025-01-02 00:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:21][root][INFO] - Training Epoch: 2/2, step 460/574 completed (loss: 0.3227265179157257, acc: 0.9382715821266174)
[2025-01-02 00:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:22][root][INFO] - Training Epoch: 2/2, step 461/574 completed (loss: 0.3538317084312439, acc: 0.8888888955116272)
[2025-01-02 00:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:22][root][INFO] - Training Epoch: 2/2, step 462/574 completed (loss: 0.11893121898174286, acc: 0.96875)
[2025-01-02 00:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:22][root][INFO] - Training Epoch: 2/2, step 463/574 completed (loss: 0.5683345198631287, acc: 0.8846153616905212)
[2025-01-02 00:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:23][root][INFO] - Training Epoch: 2/2, step 464/574 completed (loss: 0.32817986607551575, acc: 0.9130434989929199)
[2025-01-02 00:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:23][root][INFO] - Training Epoch: 2/2, step 465/574 completed (loss: 0.4404368996620178, acc: 0.8809523582458496)
[2025-01-02 00:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:23][root][INFO] - Training Epoch: 2/2, step 466/574 completed (loss: 0.6607702374458313, acc: 0.8554216623306274)
[2025-01-02 00:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:24][root][INFO] - Training Epoch: 2/2, step 467/574 completed (loss: 0.31962403655052185, acc: 0.8828828930854797)
[2025-01-02 00:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:24][root][INFO] - Training Epoch: 2/2, step 468/574 completed (loss: 0.8685567378997803, acc: 0.7961165308952332)
[2025-01-02 00:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:24][root][INFO] - Training Epoch: 2/2, step 469/574 completed (loss: 0.6878997087478638, acc: 0.8211382031440735)
[2025-01-02 00:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:25][root][INFO] - Training Epoch: 2/2, step 470/574 completed (loss: 0.18888740241527557, acc: 0.9166666865348816)
[2025-01-02 00:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:25][root][INFO] - Training Epoch: 2/2, step 471/574 completed (loss: 0.4810551106929779, acc: 0.8571428656578064)
[2025-01-02 00:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:25][root][INFO] - Training Epoch: 2/2, step 472/574 completed (loss: 0.7223342657089233, acc: 0.7941176295280457)
[2025-01-02 00:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:26][root][INFO] - Training Epoch: 2/2, step 473/574 completed (loss: 0.9303083419799805, acc: 0.7685589790344238)
[2025-01-02 00:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:26][root][INFO] - Training Epoch: 2/2, step 474/574 completed (loss: 0.6669576168060303, acc: 0.7708333134651184)
[2025-01-02 00:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:26][root][INFO] - Training Epoch: 2/2, step 475/574 completed (loss: 0.44827303290367126, acc: 0.8588957190513611)
[2025-01-02 00:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:27][root][INFO] - Training Epoch: 2/2, step 476/574 completed (loss: 0.5434756875038147, acc: 0.8417266011238098)
[2025-01-02 00:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:27][root][INFO] - Training Epoch: 2/2, step 477/574 completed (loss: 0.9014036059379578, acc: 0.7487437129020691)
[2025-01-02 00:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:27][root][INFO] - Training Epoch: 2/2, step 478/574 completed (loss: 0.6013895273208618, acc: 0.8055555820465088)
[2025-01-02 00:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:28][root][INFO] - Training Epoch: 2/2, step 479/574 completed (loss: 0.656624972820282, acc: 0.7878788113594055)
[2025-01-02 00:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:28][root][INFO] - Training Epoch: 2/2, step 480/574 completed (loss: 0.33533188700675964, acc: 0.8888888955116272)
[2025-01-02 00:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:28][root][INFO] - Training Epoch: 2/2, step 481/574 completed (loss: 0.504776120185852, acc: 0.8500000238418579)
[2025-01-02 00:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:29][root][INFO] - Training Epoch: 2/2, step 482/574 completed (loss: 0.6639146208763123, acc: 0.75)
[2025-01-02 00:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:29][root][INFO] - Training Epoch: 2/2, step 483/574 completed (loss: 0.8638572096824646, acc: 0.6724137663841248)
[2025-01-02 00:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:29][root][INFO] - Training Epoch: 2/2, step 484/574 completed (loss: 0.15865513682365417, acc: 0.8709677457809448)
[2025-01-02 00:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:30][root][INFO] - Training Epoch: 2/2, step 485/574 completed (loss: 0.4599360525608063, acc: 0.8421052694320679)
[2025-01-02 00:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:30][root][INFO] - Training Epoch: 2/2, step 486/574 completed (loss: 0.9993571043014526, acc: 0.6296296119689941)
[2025-01-02 00:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:30][root][INFO] - Training Epoch: 2/2, step 487/574 completed (loss: 0.6365112066268921, acc: 0.8095238208770752)
[2025-01-02 00:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:31][root][INFO] - Training Epoch: 2/2, step 488/574 completed (loss: 0.6564621329307556, acc: 0.7272727489471436)
[2025-01-02 00:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:31][root][INFO] - Training Epoch: 2/2, step 489/574 completed (loss: 1.0504417419433594, acc: 0.7076923251152039)
[2025-01-02 00:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:31][root][INFO] - Training Epoch: 2/2, step 490/574 completed (loss: 0.28852999210357666, acc: 0.8999999761581421)
[2025-01-02 00:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:32][root][INFO] - Training Epoch: 2/2, step 491/574 completed (loss: 0.5997211337089539, acc: 0.7931034564971924)
[2025-01-02 00:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:32][root][INFO] - Training Epoch: 2/2, step 492/574 completed (loss: 0.5793434381484985, acc: 0.7647058963775635)
[2025-01-02 00:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:32][root][INFO] - Training Epoch: 2/2, step 493/574 completed (loss: 0.6038863658905029, acc: 0.7931034564971924)
[2025-01-02 00:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:33][root][INFO] - Training Epoch: 2/2, step 494/574 completed (loss: 0.6000206470489502, acc: 0.8947368264198303)
[2025-01-02 00:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:33][root][INFO] - Training Epoch: 2/2, step 495/574 completed (loss: 0.7998450994491577, acc: 0.7368420958518982)
[2025-01-02 00:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:33][root][INFO] - Training Epoch: 2/2, step 496/574 completed (loss: 0.8080825805664062, acc: 0.7946428656578064)
[2025-01-02 00:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:34][root][INFO] - Training Epoch: 2/2, step 497/574 completed (loss: 0.5385922193527222, acc: 0.8202247023582458)
[2025-01-02 00:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:34][root][INFO] - Training Epoch: 2/2, step 498/574 completed (loss: 0.8185279369354248, acc: 0.7528089880943298)
[2025-01-02 00:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:35][root][INFO] - Training Epoch: 2/2, step 499/574 completed (loss: 1.4562774896621704, acc: 0.6028369069099426)
[2025-01-02 00:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:35][root][INFO] - Training Epoch: 2/2, step 500/574 completed (loss: 0.9814066886901855, acc: 0.75)
[2025-01-02 00:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:35][root][INFO] - Training Epoch: 2/2, step 501/574 completed (loss: 0.05237557739019394, acc: 1.0)
[2025-01-02 00:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:36][root][INFO] - Training Epoch: 2/2, step 502/574 completed (loss: 0.08437420427799225, acc: 0.9615384340286255)
[2025-01-02 00:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:36][root][INFO] - Training Epoch: 2/2, step 503/574 completed (loss: 0.18780425190925598, acc: 0.9629629850387573)
[2025-01-02 00:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:36][root][INFO] - Training Epoch: 2/2, step 504/574 completed (loss: 0.1256549060344696, acc: 0.9629629850387573)
[2025-01-02 00:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:37][root][INFO] - Training Epoch: 2/2, step 505/574 completed (loss: 0.7737933397293091, acc: 0.849056601524353)
[2025-01-02 00:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:37][root][INFO] - Training Epoch: 2/2, step 506/574 completed (loss: 0.8231225609779358, acc: 0.7586206793785095)
[2025-01-02 00:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:38][root][INFO] - Training Epoch: 2/2, step 507/574 completed (loss: 1.299957036972046, acc: 0.6126126050949097)
[2025-01-02 00:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:38][root][INFO] - Training Epoch: 2/2, step 508/574 completed (loss: 1.0083539485931396, acc: 0.7323943376541138)
[2025-01-02 00:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:38][root][INFO] - Training Epoch: 2/2, step 509/574 completed (loss: 0.1796262562274933, acc: 0.949999988079071)
[2025-01-02 00:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:39][root][INFO] - Training Epoch: 2/2, step 510/574 completed (loss: 0.34507811069488525, acc: 0.8666666746139526)
[2025-01-02 00:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:39][root][INFO] - Training Epoch: 2/2, step 511/574 completed (loss: 0.4521266520023346, acc: 0.807692289352417)
[2025-01-02 00:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:42][root][INFO] - Training Epoch: 2/2, step 512/574 completed (loss: 1.2971861362457275, acc: 0.6428571343421936)
[2025-01-02 00:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:42][root][INFO] - Training Epoch: 2/2, step 513/574 completed (loss: 0.2639964520931244, acc: 0.9126983880996704)
[2025-01-02 00:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:43][root][INFO] - Training Epoch: 2/2, step 514/574 completed (loss: 0.7563806772232056, acc: 0.75)
[2025-01-02 00:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:43][root][INFO] - Training Epoch: 2/2, step 515/574 completed (loss: 0.16663450002670288, acc: 0.9666666388511658)
[2025-01-02 00:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:44][root][INFO] - Training Epoch: 2/2, step 516/574 completed (loss: 0.6835288405418396, acc: 0.8472222089767456)
[2025-01-02 00:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:44][root][INFO] - Training Epoch: 2/2, step 517/574 completed (loss: 0.01423941645771265, acc: 1.0)
[2025-01-02 00:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:44][root][INFO] - Training Epoch: 2/2, step 518/574 completed (loss: 0.1810530424118042, acc: 0.9354838728904724)
[2025-01-02 00:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:45][root][INFO] - Training Epoch: 2/2, step 519/574 completed (loss: 0.3367961049079895, acc: 0.949999988079071)
[2025-01-02 00:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:45][root][INFO] - Training Epoch: 2/2, step 520/574 completed (loss: 0.6514290571212769, acc: 0.8518518805503845)
[2025-01-02 00:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:46][root][INFO] - Training Epoch: 2/2, step 521/574 completed (loss: 0.7655441164970398, acc: 0.7796609997749329)
[2025-01-02 00:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:46][root][INFO] - Training Epoch: 2/2, step 522/574 completed (loss: 0.362483948469162, acc: 0.8656716346740723)
[2025-01-02 00:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:47][root][INFO] - Training Epoch: 2/2, step 523/574 completed (loss: 0.4630281925201416, acc: 0.8394160866737366)
[2025-01-02 00:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:47][root][INFO] - Training Epoch: 2/2, step 524/574 completed (loss: 0.7992565035820007, acc: 0.8050000071525574)
[2025-01-02 00:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:48][root][INFO] - Training Epoch: 2/2, step 525/574 completed (loss: 0.06882359087467194, acc: 0.9814814925193787)
[2025-01-02 00:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:48][root][INFO] - Training Epoch: 2/2, step 526/574 completed (loss: 0.2605605125427246, acc: 0.9230769276618958)
[2025-01-02 00:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:48][root][INFO] - Training Epoch: 2/2, step 527/574 completed (loss: 0.4544468820095062, acc: 0.8571428656578064)
[2025-01-02 00:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:49][root][INFO] - Training Epoch: 2/2, step 528/574 completed (loss: 1.9160934686660767, acc: 0.5409836173057556)
[2025-01-02 00:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:49][root][INFO] - Training Epoch: 2/2, step 529/574 completed (loss: 0.34217530488967896, acc: 0.9152542352676392)
[2025-01-02 00:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:49][root][INFO] - Training Epoch: 2/2, step 530/574 completed (loss: 1.5099186897277832, acc: 0.5116279125213623)
[2025-01-02 00:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:50][root][INFO] - Training Epoch: 2/2, step 531/574 completed (loss: 0.9787791967391968, acc: 0.7727272510528564)
[2025-01-02 00:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:50][root][INFO] - Training Epoch: 2/2, step 532/574 completed (loss: 1.1922868490219116, acc: 0.7358490824699402)
[2025-01-02 00:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51][root][INFO] - Training Epoch: 2/2, step 533/574 completed (loss: 0.9506902098655701, acc: 0.75)
[2025-01-02 00:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51][root][INFO] - Training Epoch: 2/2, step 534/574 completed (loss: 0.6536093950271606, acc: 0.800000011920929)
[2025-01-02 00:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51][root][INFO] - Training Epoch: 2/2, step 535/574 completed (loss: 0.527251660823822, acc: 0.8999999761581421)
[2025-01-02 00:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:51][root][INFO] - Training Epoch: 2/2, step 536/574 completed (loss: 0.31275948882102966, acc: 0.9090909361839294)
[2025-01-02 00:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:52][root][INFO] - Training Epoch: 2/2, step 537/574 completed (loss: 0.7740687727928162, acc: 0.800000011920929)
[2025-01-02 00:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:52][root][INFO] - Training Epoch: 2/2, step 538/574 completed (loss: 0.5767817497253418, acc: 0.8125)
[2025-01-02 00:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:53][root][INFO] - Training Epoch: 2/2, step 539/574 completed (loss: 0.5255203247070312, acc: 0.90625)
[2025-01-02 00:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:53][root][INFO] - Training Epoch: 2/2, step 540/574 completed (loss: 0.7679576873779297, acc: 0.7575757503509521)
[2025-01-02 00:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:53][root][INFO] - Training Epoch: 2/2, step 541/574 completed (loss: 0.22283513844013214, acc: 0.9375)
[2025-01-02 00:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54][root][INFO] - Training Epoch: 2/2, step 542/574 completed (loss: 0.06865264475345612, acc: 1.0)
[2025-01-02 00:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54][root][INFO] - Training Epoch: 2/2, step 543/574 completed (loss: 0.0910535603761673, acc: 0.95652174949646)
[2025-01-02 00:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:54][root][INFO] - Training Epoch: 2/2, step 544/574 completed (loss: 0.1324910968542099, acc: 0.9666666388511658)
[2025-01-02 00:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:55][root][INFO] - Training Epoch: 2/2, step 545/574 completed (loss: 0.13618206977844238, acc: 0.9512194991111755)
[2025-01-02 00:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:55][root][INFO] - Training Epoch: 2/2, step 546/574 completed (loss: 0.030324792489409447, acc: 1.0)
[2025-01-02 00:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:55][root][INFO] - Training Epoch: 2/2, step 547/574 completed (loss: 0.0279703289270401, acc: 1.0)
[2025-01-02 00:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:56][root][INFO] - Training Epoch: 2/2, step 548/574 completed (loss: 0.30661678314208984, acc: 0.9032257795333862)
[2025-01-02 00:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:56][root][INFO] - Training Epoch: 2/2, step 549/574 completed (loss: 0.0166938453912735, acc: 1.0)
[2025-01-02 00:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:57][root][INFO] - Training Epoch: 2/2, step 550/574 completed (loss: 0.3882424831390381, acc: 0.8787878751754761)
[2025-01-02 00:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:57][root][INFO] - Training Epoch: 2/2, step 551/574 completed (loss: 0.16426554322242737, acc: 0.949999988079071)
[2025-01-02 00:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:57][root][INFO] - Training Epoch: 2/2, step 552/574 completed (loss: 0.18707819283008575, acc: 0.9285714030265808)
[2025-01-02 00:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:58][root][INFO] - Training Epoch: 2/2, step 553/574 completed (loss: 0.5609815120697021, acc: 0.8467153310775757)
[2025-01-02 00:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:58][root][INFO] - Training Epoch: 2/2, step 554/574 completed (loss: 0.23998534679412842, acc: 0.9172413945198059)
[2025-01-02 00:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:58][root][INFO] - Training Epoch: 2/2, step 555/574 completed (loss: 0.4237699508666992, acc: 0.8642857074737549)
[2025-01-02 00:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:59][root][INFO] - Training Epoch: 2/2, step 556/574 completed (loss: 0.43038409948349, acc: 0.8741722106933594)
[2025-01-02 00:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:59][root][INFO] - Training Epoch: 2/2, step 557/574 completed (loss: 0.266554594039917, acc: 0.9145299196243286)
[2025-01-02 00:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:49:59][root][INFO] - Training Epoch: 2/2, step 558/574 completed (loss: 0.10524069517850876, acc: 0.9599999785423279)
[2025-01-02 00:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:00][root][INFO] - Training Epoch: 2/2, step 559/574 completed (loss: 0.37969568371772766, acc: 0.9230769276618958)
[2025-01-02 00:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:00][root][INFO] - Training Epoch: 2/2, step 560/574 completed (loss: 0.02612951397895813, acc: 1.0)
[2025-01-02 00:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:00][root][INFO] - Training Epoch: 2/2, step 561/574 completed (loss: 0.0711427554488182, acc: 0.9743589758872986)
[2025-01-02 00:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:01][root][INFO] - Training Epoch: 2/2, step 562/574 completed (loss: 0.6171661615371704, acc: 0.8666666746139526)
[2025-01-02 00:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:01][root][INFO] - Training Epoch: 2/2, step 563/574 completed (loss: 0.5039026737213135, acc: 0.8831169009208679)
[2025-01-02 00:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:02][root][INFO] - Training Epoch: 2/2, step 564/574 completed (loss: 0.2661310136318207, acc: 0.875)
[2025-01-02 00:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:02][root][INFO] - Training Epoch: 2/2, step 565/574 completed (loss: 0.22732657194137573, acc: 0.9137930870056152)
[2025-01-02 00:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:02][root][INFO] - Training Epoch: 2/2, step 566/574 completed (loss: 0.37475913763046265, acc: 0.9285714030265808)
[2025-01-02 00:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:03][root][INFO] - Training Epoch: 2/2, step 567/574 completed (loss: 0.0371011421084404, acc: 1.0)
[2025-01-02 00:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:03][root][INFO] - Training Epoch: 2/2, step 568/574 completed (loss: 0.07090317457914352, acc: 0.9629629850387573)
[2025-01-02 00:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:03][root][INFO] - Training Epoch: 2/2, step 569/574 completed (loss: 0.22911430895328522, acc: 0.9358288645744324)
[2025-01-02 00:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9333, device='cuda:0') eval_epoch_loss=tensor(0.6592, device='cuda:0') eval_epoch_acc=tensor(0.8267, device='cuda:0')
[2025-01-02 00:50:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-02 00:50:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-02 00:50:33][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_570_loss_0.6592080593109131/model.pt
[2025-01-02 00:50:33][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-02 00:50:33][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8267472386360168
[2025-01-02 00:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:34][root][INFO] - Training Epoch: 2/2, step 570/574 completed (loss: 0.055733900517225266, acc: 0.9838709831237793)
[2025-01-02 00:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:34][root][INFO] - Training Epoch: 2/2, step 571/574 completed (loss: 0.3753429353237152, acc: 0.9145299196243286)
[2025-01-02 00:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:34][root][INFO] - Training Epoch: 2/2, step 572/574 completed (loss: 0.43216946721076965, acc: 0.8520408272743225)
[2025-01-02 00:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:50:35][root][INFO] - Training Epoch: 2/2, step 573/574 completed (loss: 0.3863102197647095, acc: 0.893081784248352)
[2025-01-02 00:50:35][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.9167, train_epoch_loss=0.6506, epoch time 347.9451165497303s
[2025-01-02 00:50:35][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-02 00:50:35][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-02 00:50:35][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-02 00:50:35][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-01-02 00:50:35][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-02 00:50:35][root][INFO] - Key: avg_train_prep, Value: 2.88970947265625
[2025-01-02 00:50:35][root][INFO] - Key: avg_train_loss, Value: 1.0009819269180298
[2025-01-02 00:50:35][root][INFO] - Key: avg_train_acc, Value: 0.7553256750106812
[2025-01-02 00:50:35][root][INFO] - Key: avg_eval_prep, Value: 2.2413525581359863
[2025-01-02 00:50:35][root][INFO] - Key: avg_eval_loss, Value: 0.7871432304382324
[2025-01-02 00:50:35][root][INFO] - Key: avg_eval_acc, Value: 0.7929084897041321
[2025-01-02 00:50:35][root][INFO] - Key: avg_epoch_time, Value: 354.70553200133145
[2025-01-02 00:50:35][root][INFO] - Key: avg_checkpoint_time, Value: 0.2541820714250207
Selected lowest loss checkpoint: asr_epoch_2_step_427_loss_0.6238870024681091
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.6238870024681091/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.6238870024681091
[2025-01-02 00:50:54][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-01-02 00:50:54][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-02 00:50:54][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-02 00:50:56][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-02 00:51:02][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:51:02][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-02 00:51:02][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-02 00:51:02][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-02 00:51:06][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:51:06][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-02 00:51:06][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-01-02 00:51:06][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-02 00:51:06][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-02 00:51:06][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-02 00:51:06][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-02 00:51:06][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.6238870024681091/model.pt
[2025-01-02 00:51:06][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-02 00:51:06][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-02 00:51:08][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-02 00:51:08][root][INFO] - --> Training Set Length = 652
[2025-01-02 00:51:08][root][INFO] - =====================================
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-01-02 00:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-02 00:55:08][slam_llm.models.slam_model][INFO] - modality encoder
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/librispeech-100_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
Missing GT or PRED file.
