/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
encoder_config: wavlm-mono
num_epochs: 2
batch_size_training: 4
train_data_folder: aphasia_phoneme
test_data_folder: aphasia_phoneme
use_peft: true
seed: 
llm_name: llama32_1b
debug: 
Is test_run? 
freeze_encoder: true
encoder_projector: q-former
encoder_projector_ds_rate: 5
Is save_embedding? false
projector_transfer_learning: false
transfer_data_folder: 
----------
----------
Final identifier: aphasia_phoneme_wavlm_llama32_1b_q-former_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_1.2942701578140259
Resume epoch: 2
Resume step: 17875
[2025-02-05 13:38:45][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 17875, 'resume_epoch': 2, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-05 13:38:45][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-05 13:38:45][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'q-former', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_phoneme_wavlm_llama32_1b_q-former_peft'}
[2025-02-05 13:38:45][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_phoneme_wavlm_llama32_1b_q-former_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-05_13-38-45.txt', 'log_interval': 5}
[2025-02-05 13:39:10][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-05 13:39:15][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-05 13:39:15][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-05 13:39:15][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-05 13:39:15][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-05 13:39:23][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-05 13:39:23][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-05 13:39:23][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-05 13:39:23][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-05 13:39:23][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-05 13:39:24][slam_llm.utils.train_utils][INFO] - --> Module q-former
[2025-02-05 13:39:24][slam_llm.utils.train_utils][INFO] - --> q-former has 69.361152 Million params

[2025-02-05 13:39:24][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_1.2942701578140259/model.pt
[2025-02-05 13:39:25][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-05 13:39:25][slam_llm.utils.train_utils][INFO] - --> asr has 74.997248 Million params

[2025-02-05 13:39:27][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-05 13:39:29][root][INFO] - --> Training Set Length = 95353
[2025-02-05 13:39:30][root][INFO] - --> Validation Set Length = 13162
[2025-02-05 13:39:30][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-05 13:39:30][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-05 13:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:33][root][INFO] - Training Epoch: 2/2, step 17875/23838 completed (loss: 0.9598489999771118, acc: 0.8048780560493469)
[2025-02-05 13:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:34][root][INFO] - Training Epoch: 2/2, step 17876/23838 completed (loss: 1.2339316606521606, acc: 0.6984127163887024)
[2025-02-05 13:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:34][root][INFO] - Training Epoch: 2/2, step 17877/23838 completed (loss: 0.8260186314582825, acc: 0.746268630027771)
[2025-02-05 13:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:35][root][INFO] - Training Epoch: 2/2, step 17878/23838 completed (loss: 1.3506567478179932, acc: 0.5909090638160706)
[2025-02-05 13:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:35][root][INFO] - Training Epoch: 2/2, step 17879/23838 completed (loss: 1.1039973497390747, acc: 0.656862735748291)
[2025-02-05 13:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:36][root][INFO] - Training Epoch: 2/2, step 17880/23838 completed (loss: 1.3130627870559692, acc: 0.653333306312561)
[2025-02-05 13:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:36][root][INFO] - Training Epoch: 2/2, step 17881/23838 completed (loss: 1.4465663433074951, acc: 0.5865384340286255)
[2025-02-05 13:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:37][root][INFO] - Training Epoch: 2/2, step 17882/23838 completed (loss: 1.375523328781128, acc: 0.5968992114067078)
[2025-02-05 13:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:38][root][INFO] - Training Epoch: 2/2, step 17883/23838 completed (loss: 1.371055245399475, acc: 0.6147540807723999)
[2025-02-05 13:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:38][root][INFO] - Training Epoch: 2/2, step 17884/23838 completed (loss: 1.2374334335327148, acc: 0.6296296119689941)
[2025-02-05 13:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:38][root][INFO] - Training Epoch: 2/2, step 17885/23838 completed (loss: 1.3792288303375244, acc: 0.6000000238418579)
[2025-02-05 13:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:39][root][INFO] - Training Epoch: 2/2, step 17886/23838 completed (loss: 1.1354292631149292, acc: 0.6545454263687134)
[2025-02-05 13:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:39][root][INFO] - Training Epoch: 2/2, step 17887/23838 completed (loss: 0.7066895365715027, acc: 0.7714285850524902)
[2025-02-05 13:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:40][root][INFO] - Training Epoch: 2/2, step 17888/23838 completed (loss: 0.8121238946914673, acc: 0.7419354915618896)
[2025-02-05 13:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:40][root][INFO] - Training Epoch: 2/2, step 17889/23838 completed (loss: 0.9491462707519531, acc: 0.699999988079071)
[2025-02-05 13:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:41][root][INFO] - Training Epoch: 2/2, step 17890/23838 completed (loss: 1.1445786952972412, acc: 0.6973684430122375)
[2025-02-05 13:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:41][root][INFO] - Training Epoch: 2/2, step 17891/23838 completed (loss: 1.1380468606948853, acc: 0.6122449040412903)
[2025-02-05 13:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:41][root][INFO] - Training Epoch: 2/2, step 17892/23838 completed (loss: 0.5481058359146118, acc: 0.8421052694320679)
[2025-02-05 13:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:42][root][INFO] - Training Epoch: 2/2, step 17893/23838 completed (loss: 0.9170490503311157, acc: 0.7313432693481445)
[2025-02-05 13:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:42][root][INFO] - Training Epoch: 2/2, step 17894/23838 completed (loss: 1.262283444404602, acc: 0.65625)
[2025-02-05 13:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:43][root][INFO] - Training Epoch: 2/2, step 17895/23838 completed (loss: 1.0711404085159302, acc: 0.6753246784210205)
[2025-02-05 13:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:43][root][INFO] - Training Epoch: 2/2, step 17896/23838 completed (loss: 1.5678399801254272, acc: 0.6000000238418579)
[2025-02-05 13:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:43][root][INFO] - Training Epoch: 2/2, step 17897/23838 completed (loss: 1.029536485671997, acc: 0.699999988079071)
[2025-02-05 13:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:44][root][INFO] - Training Epoch: 2/2, step 17898/23838 completed (loss: 1.1484683752059937, acc: 0.6630434989929199)
[2025-02-05 13:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:44][root][INFO] - Training Epoch: 2/2, step 17899/23838 completed (loss: 1.2661839723587036, acc: 0.6296296119689941)
[2025-02-05 13:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:45][root][INFO] - Training Epoch: 2/2, step 17900/23838 completed (loss: 0.8830103278160095, acc: 0.7160493731498718)
[2025-02-05 13:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:45][root][INFO] - Training Epoch: 2/2, step 17901/23838 completed (loss: 0.8952543139457703, acc: 0.75)
[2025-02-05 13:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:46][root][INFO] - Training Epoch: 2/2, step 17902/23838 completed (loss: 1.0108921527862549, acc: 0.7142857313156128)
[2025-02-05 13:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:46][root][INFO] - Training Epoch: 2/2, step 17903/23838 completed (loss: 1.0663210153579712, acc: 0.699999988079071)
[2025-02-05 13:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:47][root][INFO] - Training Epoch: 2/2, step 17904/23838 completed (loss: 0.8039563298225403, acc: 0.7972972989082336)
[2025-02-05 13:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:47][root][INFO] - Training Epoch: 2/2, step 17905/23838 completed (loss: 1.3993544578552246, acc: 0.6538461446762085)
[2025-02-05 13:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:48][root][INFO] - Training Epoch: 2/2, step 17906/23838 completed (loss: 0.995136022567749, acc: 0.800000011920929)
[2025-02-05 13:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:48][root][INFO] - Training Epoch: 2/2, step 17907/23838 completed (loss: 1.4564871788024902, acc: 0.5333333611488342)
[2025-02-05 13:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:49][root][INFO] - Training Epoch: 2/2, step 17908/23838 completed (loss: 1.3403302431106567, acc: 0.5357142686843872)
[2025-02-05 13:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:49][root][INFO] - Training Epoch: 2/2, step 17909/23838 completed (loss: 0.8623285293579102, acc: 0.7666666507720947)
[2025-02-05 13:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:49][root][INFO] - Training Epoch: 2/2, step 17910/23838 completed (loss: 1.360590934753418, acc: 0.6315789222717285)
[2025-02-05 13:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:50][root][INFO] - Training Epoch: 2/2, step 17911/23838 completed (loss: 1.8849903345108032, acc: 0.47999998927116394)
[2025-02-05 13:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:51][root][INFO] - Training Epoch: 2/2, step 17912/23838 completed (loss: 2.1760923862457275, acc: 0.44186046719551086)
[2025-02-05 13:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:51][root][INFO] - Training Epoch: 2/2, step 17913/23838 completed (loss: 1.7277981042861938, acc: 0.5882353186607361)
[2025-02-05 13:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:52][root][INFO] - Training Epoch: 2/2, step 17914/23838 completed (loss: 0.24665117263793945, acc: 0.9166666865348816)
[2025-02-05 13:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:52][root][INFO] - Training Epoch: 2/2, step 17915/23838 completed (loss: 0.16525913774967194, acc: 1.0)
[2025-02-05 13:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:52][root][INFO] - Training Epoch: 2/2, step 17916/23838 completed (loss: 1.3875741958618164, acc: 0.699999988079071)
[2025-02-05 13:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:53][root][INFO] - Training Epoch: 2/2, step 17917/23838 completed (loss: 1.3156163692474365, acc: 0.523809552192688)
[2025-02-05 13:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:53][root][INFO] - Training Epoch: 2/2, step 17918/23838 completed (loss: 1.2088918685913086, acc: 0.6578947305679321)
[2025-02-05 13:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:54][root][INFO] - Training Epoch: 2/2, step 17919/23838 completed (loss: 1.2160940170288086, acc: 0.6499999761581421)
[2025-02-05 13:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:54][root][INFO] - Training Epoch: 2/2, step 17920/23838 completed (loss: 1.1347577571868896, acc: 0.6727272868156433)
[2025-02-05 13:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:54][root][INFO] - Training Epoch: 2/2, step 17921/23838 completed (loss: 0.9759006500244141, acc: 0.692307710647583)
[2025-02-05 13:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:55][root][INFO] - Training Epoch: 2/2, step 17922/23838 completed (loss: 1.0289206504821777, acc: 0.7037037014961243)
[2025-02-05 13:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:55][root][INFO] - Training Epoch: 2/2, step 17923/23838 completed (loss: 0.5887700915336609, acc: 0.8679245114326477)
[2025-02-05 13:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:56][root][INFO] - Training Epoch: 2/2, step 17924/23838 completed (loss: 1.519759178161621, acc: 0.6237623691558838)
[2025-02-05 13:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:56][root][INFO] - Training Epoch: 2/2, step 17925/23838 completed (loss: 1.1305615901947021, acc: 0.6666666865348816)
[2025-02-05 13:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:57][root][INFO] - Training Epoch: 2/2, step 17926/23838 completed (loss: 1.0519367456436157, acc: 0.6959459185600281)
[2025-02-05 13:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:57][root][INFO] - Training Epoch: 2/2, step 17927/23838 completed (loss: 1.090319275856018, acc: 0.6666666865348816)
[2025-02-05 13:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:57][root][INFO] - Training Epoch: 2/2, step 17928/23838 completed (loss: 1.6371158361434937, acc: 0.5432098507881165)
[2025-02-05 13:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:58][root][INFO] - Training Epoch: 2/2, step 17929/23838 completed (loss: 0.8063626885414124, acc: 0.7272727489471436)
[2025-02-05 13:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:58][root][INFO] - Training Epoch: 2/2, step 17930/23838 completed (loss: 1.0352470874786377, acc: 0.7169811129570007)
[2025-02-05 13:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:59][root][INFO] - Training Epoch: 2/2, step 17931/23838 completed (loss: 1.0817404985427856, acc: 0.6607142686843872)
[2025-02-05 13:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:39:59][root][INFO] - Training Epoch: 2/2, step 17932/23838 completed (loss: 0.944452702999115, acc: 0.6902654767036438)
[2025-02-05 13:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:00][root][INFO] - Training Epoch: 2/2, step 17933/23838 completed (loss: 1.0557938814163208, acc: 0.6888889074325562)
[2025-02-05 13:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:00][root][INFO] - Training Epoch: 2/2, step 17934/23838 completed (loss: 1.0519393682479858, acc: 0.7216494679450989)
[2025-02-05 13:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:01][root][INFO] - Training Epoch: 2/2, step 17935/23838 completed (loss: 0.9749711155891418, acc: 0.6916666626930237)
[2025-02-05 13:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:01][root][INFO] - Training Epoch: 2/2, step 17936/23838 completed (loss: 0.9924506545066833, acc: 0.7168141603469849)
[2025-02-05 13:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:02][root][INFO] - Training Epoch: 2/2, step 17937/23838 completed (loss: 1.0579863786697388, acc: 0.6373626589775085)
[2025-02-05 13:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:02][root][INFO] - Training Epoch: 2/2, step 17938/23838 completed (loss: 1.328478455543518, acc: 0.6396396160125732)
[2025-02-05 13:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:02][root][INFO] - Training Epoch: 2/2, step 17939/23838 completed (loss: 1.2462493181228638, acc: 0.6338028311729431)
[2025-02-05 13:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:03][root][INFO] - Training Epoch: 2/2, step 17940/23838 completed (loss: 1.0577784776687622, acc: 0.6854838728904724)
[2025-02-05 13:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:03][root][INFO] - Training Epoch: 2/2, step 17941/23838 completed (loss: 0.47664710879325867, acc: 0.8727272748947144)
[2025-02-05 13:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:04][root][INFO] - Training Epoch: 2/2, step 17942/23838 completed (loss: 0.7696688771247864, acc: 0.7572815418243408)
[2025-02-05 13:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:04][root][INFO] - Training Epoch: 2/2, step 17943/23838 completed (loss: 0.81064772605896, acc: 0.7916666865348816)
[2025-02-05 13:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:05][root][INFO] - Training Epoch: 2/2, step 17944/23838 completed (loss: 0.540901780128479, acc: 0.8461538553237915)
[2025-02-05 13:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:05][root][INFO] - Training Epoch: 2/2, step 17945/23838 completed (loss: 1.1078507900238037, acc: 0.6904761791229248)
[2025-02-05 13:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:05][root][INFO] - Training Epoch: 2/2, step 17946/23838 completed (loss: 1.109275221824646, acc: 0.7428571581840515)
[2025-02-05 13:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:06][root][INFO] - Training Epoch: 2/2, step 17947/23838 completed (loss: 2.2386512756347656, acc: 0.53125)
[2025-02-05 13:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:06][root][INFO] - Training Epoch: 2/2, step 17948/23838 completed (loss: 1.4102215766906738, acc: 0.5869565010070801)
[2025-02-05 13:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:07][root][INFO] - Training Epoch: 2/2, step 17949/23838 completed (loss: 1.0443466901779175, acc: 0.7307692170143127)
[2025-02-05 13:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:07][root][INFO] - Training Epoch: 2/2, step 17950/23838 completed (loss: 1.224599838256836, acc: 0.7200000286102295)
[2025-02-05 13:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:08][root][INFO] - Training Epoch: 2/2, step 17951/23838 completed (loss: 0.7483741044998169, acc: 0.782608687877655)
[2025-02-05 13:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:08][root][INFO] - Training Epoch: 2/2, step 17952/23838 completed (loss: 1.3221582174301147, acc: 0.5932203531265259)
[2025-02-05 13:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:09][root][INFO] - Training Epoch: 2/2, step 17953/23838 completed (loss: 0.9904003143310547, acc: 0.7241379022598267)
[2025-02-05 13:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:09][root][INFO] - Training Epoch: 2/2, step 17954/23838 completed (loss: 0.8599056601524353, acc: 0.7777777910232544)
[2025-02-05 13:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:09][root][INFO] - Training Epoch: 2/2, step 17955/23838 completed (loss: 0.9619923233985901, acc: 0.7428571581840515)
[2025-02-05 13:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:10][root][INFO] - Training Epoch: 2/2, step 17956/23838 completed (loss: 0.42519327998161316, acc: 0.8421052694320679)
[2025-02-05 13:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:10][root][INFO] - Training Epoch: 2/2, step 17957/23838 completed (loss: 0.6650124788284302, acc: 0.875)
[2025-02-05 13:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:11][root][INFO] - Training Epoch: 2/2, step 17958/23838 completed (loss: 0.95245361328125, acc: 0.6904761791229248)
[2025-02-05 13:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:11][root][INFO] - Training Epoch: 2/2, step 17959/23838 completed (loss: 0.6887382864952087, acc: 0.841269850730896)
[2025-02-05 13:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:12][root][INFO] - Training Epoch: 2/2, step 17960/23838 completed (loss: 0.981846272945404, acc: 0.7446808218955994)
[2025-02-05 13:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:12][root][INFO] - Training Epoch: 2/2, step 17961/23838 completed (loss: 0.8055252432823181, acc: 0.75)
[2025-02-05 13:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:12][root][INFO] - Training Epoch: 2/2, step 17962/23838 completed (loss: 1.12621009349823, acc: 0.7014925479888916)
[2025-02-05 13:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:13][root][INFO] - Training Epoch: 2/2, step 17963/23838 completed (loss: 0.5439653992652893, acc: 0.8478260636329651)
[2025-02-05 13:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:13][root][INFO] - Training Epoch: 2/2, step 17964/23838 completed (loss: 1.6516928672790527, acc: 0.5384615659713745)
[2025-02-05 13:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:14][root][INFO] - Training Epoch: 2/2, step 17965/23838 completed (loss: 1.507378339767456, acc: 0.577464759349823)
[2025-02-05 13:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:14][root][INFO] - Training Epoch: 2/2, step 17966/23838 completed (loss: 0.7783440947532654, acc: 0.75789475440979)
[2025-02-05 13:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:14][root][INFO] - Training Epoch: 2/2, step 17967/23838 completed (loss: 0.6637161374092102, acc: 0.8461538553237915)
[2025-02-05 13:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:15][root][INFO] - Training Epoch: 2/2, step 17968/23838 completed (loss: 0.751621663570404, acc: 0.8023256063461304)
[2025-02-05 13:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:15][root][INFO] - Training Epoch: 2/2, step 17969/23838 completed (loss: 0.6651771068572998, acc: 0.8414633870124817)
[2025-02-05 13:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:16][root][INFO] - Training Epoch: 2/2, step 17970/23838 completed (loss: 0.5205432176589966, acc: 0.8888888955116272)
[2025-02-05 13:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:16][root][INFO] - Training Epoch: 2/2, step 17971/23838 completed (loss: 0.2827615737915039, acc: 0.875)
[2025-02-05 13:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:17][root][INFO] - Training Epoch: 2/2, step 17972/23838 completed (loss: 0.27310699224472046, acc: 0.9599999785423279)
[2025-02-05 13:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:17][root][INFO] - Training Epoch: 2/2, step 17973/23838 completed (loss: 1.1698086261749268, acc: 0.6969696879386902)
[2025-02-05 13:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:18][root][INFO] - Training Epoch: 2/2, step 17974/23838 completed (loss: 0.2618182897567749, acc: 0.9473684430122375)
[2025-02-05 13:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:18][root][INFO] - Training Epoch: 2/2, step 17975/23838 completed (loss: 0.7326314449310303, acc: 0.7647058963775635)
[2025-02-05 13:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:19][root][INFO] - Training Epoch: 2/2, step 17976/23838 completed (loss: 0.9042659997940063, acc: 0.7555555701255798)
[2025-02-05 13:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:19][root][INFO] - Training Epoch: 2/2, step 17977/23838 completed (loss: 0.6783917546272278, acc: 0.7272727489471436)
[2025-02-05 13:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:19][root][INFO] - Training Epoch: 2/2, step 17978/23838 completed (loss: 0.8114719986915588, acc: 0.7200000286102295)
[2025-02-05 13:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:20][root][INFO] - Training Epoch: 2/2, step 17979/23838 completed (loss: 0.427354097366333, acc: 0.8260869383811951)
[2025-02-05 13:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:20][root][INFO] - Training Epoch: 2/2, step 17980/23838 completed (loss: 0.7802175879478455, acc: 0.7941176295280457)
[2025-02-05 13:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:20][root][INFO] - Training Epoch: 2/2, step 17981/23838 completed (loss: 0.4904441237449646, acc: 0.8484848737716675)
[2025-02-05 13:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:21][root][INFO] - Training Epoch: 2/2, step 17982/23838 completed (loss: 0.5582682490348816, acc: 0.8666666746139526)
[2025-02-05 13:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:21][root][INFO] - Training Epoch: 2/2, step 17983/23838 completed (loss: 0.30449071526527405, acc: 0.8888888955116272)
[2025-02-05 13:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:22][root][INFO] - Training Epoch: 2/2, step 17984/23838 completed (loss: 0.8457123041152954, acc: 0.7058823704719543)
[2025-02-05 13:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:22][root][INFO] - Training Epoch: 2/2, step 17985/23838 completed (loss: 0.8255159854888916, acc: 0.8064516186714172)
[2025-02-05 13:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:22][root][INFO] - Training Epoch: 2/2, step 17986/23838 completed (loss: 0.4633568823337555, acc: 0.9166666865348816)
[2025-02-05 13:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:23][root][INFO] - Training Epoch: 2/2, step 17987/23838 completed (loss: 1.0002567768096924, acc: 0.6551724076271057)
[2025-02-05 13:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:23][root][INFO] - Training Epoch: 2/2, step 17988/23838 completed (loss: 0.5419286489486694, acc: 0.7916666865348816)
[2025-02-05 13:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:24][root][INFO] - Training Epoch: 2/2, step 17989/23838 completed (loss: 0.9228122234344482, acc: 0.7777777910232544)
[2025-02-05 13:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:24][root][INFO] - Training Epoch: 2/2, step 17990/23838 completed (loss: 1.6091359853744507, acc: 0.508474588394165)
[2025-02-05 13:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:24][root][INFO] - Training Epoch: 2/2, step 17991/23838 completed (loss: 1.3505737781524658, acc: 0.5753424763679504)
[2025-02-05 13:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:25][root][INFO] - Training Epoch: 2/2, step 17992/23838 completed (loss: 1.557478427886963, acc: 0.5833333134651184)
[2025-02-05 13:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:25][root][INFO] - Training Epoch: 2/2, step 17993/23838 completed (loss: 1.3635973930358887, acc: 0.5882353186607361)
[2025-02-05 13:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:26][root][INFO] - Training Epoch: 2/2, step 17994/23838 completed (loss: 1.741182804107666, acc: 0.4680851101875305)
[2025-02-05 13:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:26][root][INFO] - Training Epoch: 2/2, step 17995/23838 completed (loss: 1.7325736284255981, acc: 0.40425533056259155)
[2025-02-05 13:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:26][root][INFO] - Training Epoch: 2/2, step 17996/23838 completed (loss: 1.4296108484268188, acc: 0.6097561120986938)
[2025-02-05 13:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:27][root][INFO] - Training Epoch: 2/2, step 17997/23838 completed (loss: 1.333034873008728, acc: 0.5714285969734192)
[2025-02-05 13:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:27][root][INFO] - Training Epoch: 2/2, step 17998/23838 completed (loss: 0.9311381578445435, acc: 0.6333333253860474)
[2025-02-05 13:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:28][root][INFO] - Training Epoch: 2/2, step 17999/23838 completed (loss: 1.120742678642273, acc: 0.7250000238418579)
[2025-02-05 13:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:28][root][INFO] - Training Epoch: 2/2, step 18000/23838 completed (loss: 1.1326920986175537, acc: 0.6351351141929626)
[2025-02-05 13:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:28][root][INFO] - Training Epoch: 2/2, step 18001/23838 completed (loss: 1.4325311183929443, acc: 0.6190476417541504)
[2025-02-05 13:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:29][root][INFO] - Training Epoch: 2/2, step 18002/23838 completed (loss: 1.1875461339950562, acc: 0.6399999856948853)
[2025-02-05 13:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:29][root][INFO] - Training Epoch: 2/2, step 18003/23838 completed (loss: 1.8044160604476929, acc: 0.54666668176651)
[2025-02-05 13:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:30][root][INFO] - Training Epoch: 2/2, step 18004/23838 completed (loss: 1.4018710851669312, acc: 0.5652173757553101)
[2025-02-05 13:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:30][root][INFO] - Training Epoch: 2/2, step 18005/23838 completed (loss: 1.2756680250167847, acc: 0.6034482717514038)
[2025-02-05 13:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:30][root][INFO] - Training Epoch: 2/2, step 18006/23838 completed (loss: 1.3397862911224365, acc: 0.6071428656578064)
[2025-02-05 13:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:31][root][INFO] - Training Epoch: 2/2, step 18007/23838 completed (loss: 1.3941797018051147, acc: 0.59375)
[2025-02-05 13:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:31][root][INFO] - Training Epoch: 2/2, step 18008/23838 completed (loss: 1.2673912048339844, acc: 0.6388888955116272)
[2025-02-05 13:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:31][root][INFO] - Training Epoch: 2/2, step 18009/23838 completed (loss: 1.0824888944625854, acc: 0.6666666865348816)
[2025-02-05 13:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:32][root][INFO] - Training Epoch: 2/2, step 18010/23838 completed (loss: 1.2052228450775146, acc: 0.5641025900840759)
[2025-02-05 13:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:32][root][INFO] - Training Epoch: 2/2, step 18011/23838 completed (loss: 1.4342581033706665, acc: 0.5813953280448914)
[2025-02-05 13:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:33][root][INFO] - Training Epoch: 2/2, step 18012/23838 completed (loss: 1.2758150100708008, acc: 0.6349206566810608)
[2025-02-05 13:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:33][root][INFO] - Training Epoch: 2/2, step 18013/23838 completed (loss: 1.1886111497879028, acc: 0.6341463327407837)
[2025-02-05 13:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:33][root][INFO] - Training Epoch: 2/2, step 18014/23838 completed (loss: 1.1155965328216553, acc: 0.695652186870575)
[2025-02-05 13:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:34][root][INFO] - Training Epoch: 2/2, step 18015/23838 completed (loss: 1.6375675201416016, acc: 0.4901960790157318)
[2025-02-05 13:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:34][root][INFO] - Training Epoch: 2/2, step 18016/23838 completed (loss: 1.3353374004364014, acc: 0.6000000238418579)
[2025-02-05 13:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:35][root][INFO] - Training Epoch: 2/2, step 18017/23838 completed (loss: 1.4350950717926025, acc: 0.5799999833106995)
[2025-02-05 13:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:35][root][INFO] - Training Epoch: 2/2, step 18018/23838 completed (loss: 1.2295570373535156, acc: 0.6078431606292725)
[2025-02-05 13:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:35][root][INFO] - Training Epoch: 2/2, step 18019/23838 completed (loss: 0.7526066303253174, acc: 0.7222222089767456)
[2025-02-05 13:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:36][root][INFO] - Training Epoch: 2/2, step 18020/23838 completed (loss: 1.240322470664978, acc: 0.6600000262260437)
[2025-02-05 13:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:36][root][INFO] - Training Epoch: 2/2, step 18021/23838 completed (loss: 1.3667949438095093, acc: 0.5789473652839661)
[2025-02-05 13:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:37][root][INFO] - Training Epoch: 2/2, step 18022/23838 completed (loss: 1.8766980171203613, acc: 0.5185185074806213)
[2025-02-05 13:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:37][root][INFO] - Training Epoch: 2/2, step 18023/23838 completed (loss: 1.3061012029647827, acc: 0.6304348111152649)
[2025-02-05 13:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:37][root][INFO] - Training Epoch: 2/2, step 18024/23838 completed (loss: 1.049190878868103, acc: 0.625)
[2025-02-05 13:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:38][root][INFO] - Training Epoch: 2/2, step 18025/23838 completed (loss: 1.7610279321670532, acc: 0.5625)
[2025-02-05 13:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:38][root][INFO] - Training Epoch: 2/2, step 18026/23838 completed (loss: 1.46891450881958, acc: 0.5694444179534912)
[2025-02-05 13:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:39][root][INFO] - Training Epoch: 2/2, step 18027/23838 completed (loss: 1.548751950263977, acc: 0.5405405163764954)
[2025-02-05 13:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:39][root][INFO] - Training Epoch: 2/2, step 18028/23838 completed (loss: 1.1006635427474976, acc: 0.5609756112098694)
[2025-02-05 13:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:39][root][INFO] - Training Epoch: 2/2, step 18029/23838 completed (loss: 1.0940245389938354, acc: 0.7333333492279053)
[2025-02-05 13:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:40][root][INFO] - Training Epoch: 2/2, step 18030/23838 completed (loss: 1.3372217416763306, acc: 0.6025640964508057)
[2025-02-05 13:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:40][root][INFO] - Training Epoch: 2/2, step 18031/23838 completed (loss: 1.2447419166564941, acc: 0.6111111044883728)
[2025-02-05 13:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:40][root][INFO] - Training Epoch: 2/2, step 18032/23838 completed (loss: 1.6294130086898804, acc: 0.5)
[2025-02-05 13:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:41][root][INFO] - Training Epoch: 2/2, step 18033/23838 completed (loss: 0.8786928057670593, acc: 0.6969696879386902)
[2025-02-05 13:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:41][root][INFO] - Training Epoch: 2/2, step 18034/23838 completed (loss: 1.3530157804489136, acc: 0.6388888955116272)
[2025-02-05 13:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:42][root][INFO] - Training Epoch: 2/2, step 18035/23838 completed (loss: 1.664973497390747, acc: 0.6551724076271057)
[2025-02-05 13:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:42][root][INFO] - Training Epoch: 2/2, step 18036/23838 completed (loss: 1.3493273258209229, acc: 0.6000000238418579)
[2025-02-05 13:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:42][root][INFO] - Training Epoch: 2/2, step 18037/23838 completed (loss: 1.3184361457824707, acc: 0.6153846383094788)
[2025-02-05 13:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:43][root][INFO] - Training Epoch: 2/2, step 18038/23838 completed (loss: 1.1564009189605713, acc: 0.6376811861991882)
[2025-02-05 13:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:43][root][INFO] - Training Epoch: 2/2, step 18039/23838 completed (loss: 1.4596868753433228, acc: 0.5490196347236633)
[2025-02-05 13:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:44][root][INFO] - Training Epoch: 2/2, step 18040/23838 completed (loss: 1.4160664081573486, acc: 0.5633803009986877)
[2025-02-05 13:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:44][root][INFO] - Training Epoch: 2/2, step 18041/23838 completed (loss: 1.8070334196090698, acc: 0.4821428656578064)
[2025-02-05 13:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:44][root][INFO] - Training Epoch: 2/2, step 18042/23838 completed (loss: 1.6838113069534302, acc: 0.5046728849411011)
[2025-02-05 13:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:45][root][INFO] - Training Epoch: 2/2, step 18043/23838 completed (loss: 1.064819097518921, acc: 0.6935483813285828)
[2025-02-05 13:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:45][root][INFO] - Training Epoch: 2/2, step 18044/23838 completed (loss: 0.8244898319244385, acc: 0.739130437374115)
[2025-02-05 13:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:46][root][INFO] - Training Epoch: 2/2, step 18045/23838 completed (loss: 1.192667841911316, acc: 0.6557376980781555)
[2025-02-05 13:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:46][root][INFO] - Training Epoch: 2/2, step 18046/23838 completed (loss: 1.3045698404312134, acc: 0.6938775777816772)
[2025-02-05 13:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:46][root][INFO] - Training Epoch: 2/2, step 18047/23838 completed (loss: 0.7779386639595032, acc: 0.75)
[2025-02-05 13:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:47][root][INFO] - Training Epoch: 2/2, step 18048/23838 completed (loss: 1.2398232221603394, acc: 0.6666666865348816)
[2025-02-05 13:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:47][root][INFO] - Training Epoch: 2/2, step 18049/23838 completed (loss: 1.2943094968795776, acc: 0.5454545617103577)
[2025-02-05 13:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:48][root][INFO] - Training Epoch: 2/2, step 18050/23838 completed (loss: 1.4014290571212769, acc: 0.577464759349823)
[2025-02-05 13:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:48][root][INFO] - Training Epoch: 2/2, step 18051/23838 completed (loss: 0.9466762542724609, acc: 0.746835470199585)
[2025-02-05 13:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:48][root][INFO] - Training Epoch: 2/2, step 18052/23838 completed (loss: 0.9449529647827148, acc: 0.7551020383834839)
[2025-02-05 13:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:49][root][INFO] - Training Epoch: 2/2, step 18053/23838 completed (loss: 1.0916082859039307, acc: 0.625)
[2025-02-05 13:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:49][root][INFO] - Training Epoch: 2/2, step 18054/23838 completed (loss: 1.175133466720581, acc: 0.6808510422706604)
[2025-02-05 13:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:50][root][INFO] - Training Epoch: 2/2, step 18055/23838 completed (loss: 1.0912251472473145, acc: 0.625)
[2025-02-05 13:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:50][root][INFO] - Training Epoch: 2/2, step 18056/23838 completed (loss: 0.881834089756012, acc: 0.7540983557701111)
[2025-02-05 13:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:51][root][INFO] - Training Epoch: 2/2, step 18057/23838 completed (loss: 1.4811729192733765, acc: 0.625)
[2025-02-05 13:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:51][root][INFO] - Training Epoch: 2/2, step 18058/23838 completed (loss: 1.243754267692566, acc: 0.6499999761581421)
[2025-02-05 13:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:51][root][INFO] - Training Epoch: 2/2, step 18059/23838 completed (loss: 1.2819201946258545, acc: 0.6296296119689941)
[2025-02-05 13:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:52][root][INFO] - Training Epoch: 2/2, step 18060/23838 completed (loss: 1.325927972793579, acc: 0.644444465637207)
[2025-02-05 13:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:52][root][INFO] - Training Epoch: 2/2, step 18061/23838 completed (loss: 1.2816485166549683, acc: 0.6393442749977112)
[2025-02-05 13:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:53][root][INFO] - Training Epoch: 2/2, step 18062/23838 completed (loss: 1.4724175930023193, acc: 0.637499988079071)
[2025-02-05 13:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:53][root][INFO] - Training Epoch: 2/2, step 18063/23838 completed (loss: 1.1570465564727783, acc: 0.7029703259468079)
[2025-02-05 13:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:53][root][INFO] - Training Epoch: 2/2, step 18064/23838 completed (loss: 1.018905520439148, acc: 0.6822429895401001)
[2025-02-05 13:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:54][root][INFO] - Training Epoch: 2/2, step 18065/23838 completed (loss: 0.9909043908119202, acc: 0.7341772317886353)
[2025-02-05 13:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:54][root][INFO] - Training Epoch: 2/2, step 18066/23838 completed (loss: 1.0950003862380981, acc: 0.6774193644523621)
[2025-02-05 13:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:55][root][INFO] - Training Epoch: 2/2, step 18067/23838 completed (loss: 1.1354725360870361, acc: 0.6486486196517944)
[2025-02-05 13:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:55][root][INFO] - Training Epoch: 2/2, step 18068/23838 completed (loss: 1.210273265838623, acc: 0.6491228342056274)
[2025-02-05 13:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:56][root][INFO] - Training Epoch: 2/2, step 18069/23838 completed (loss: 0.8184317946434021, acc: 0.7142857313156128)
[2025-02-05 13:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:56][root][INFO] - Training Epoch: 2/2, step 18070/23838 completed (loss: 1.0735889673233032, acc: 0.6355140209197998)
[2025-02-05 13:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:57][root][INFO] - Training Epoch: 2/2, step 18071/23838 completed (loss: 1.2733476161956787, acc: 0.6309523582458496)
[2025-02-05 13:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:57][root][INFO] - Training Epoch: 2/2, step 18072/23838 completed (loss: 1.2499818801879883, acc: 0.6296296119689941)
[2025-02-05 13:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:57][root][INFO] - Training Epoch: 2/2, step 18073/23838 completed (loss: 1.4257405996322632, acc: 0.6486486196517944)
[2025-02-05 13:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:58][root][INFO] - Training Epoch: 2/2, step 18074/23838 completed (loss: 1.3546115159988403, acc: 0.5)
[2025-02-05 13:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:58][root][INFO] - Training Epoch: 2/2, step 18075/23838 completed (loss: 0.9887535572052002, acc: 0.6666666865348816)
[2025-02-05 13:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:59][root][INFO] - Training Epoch: 2/2, step 18076/23838 completed (loss: 1.1144226789474487, acc: 0.6666666865348816)
[2025-02-05 13:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:59][root][INFO] - Training Epoch: 2/2, step 18077/23838 completed (loss: 1.2971664667129517, acc: 0.59375)
[2025-02-05 13:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:40:59][root][INFO] - Training Epoch: 2/2, step 18078/23838 completed (loss: 0.8789184093475342, acc: 0.78125)
[2025-02-05 13:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:00][root][INFO] - Training Epoch: 2/2, step 18079/23838 completed (loss: 1.4981838464736938, acc: 0.6363636255264282)
[2025-02-05 13:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:00][root][INFO] - Training Epoch: 2/2, step 18080/23838 completed (loss: 1.2477829456329346, acc: 0.675000011920929)
[2025-02-05 13:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:01][root][INFO] - Training Epoch: 2/2, step 18081/23838 completed (loss: 1.2279856204986572, acc: 0.8399999737739563)
[2025-02-05 13:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:01][root][INFO] - Training Epoch: 2/2, step 18082/23838 completed (loss: 1.8063846826553345, acc: 0.4545454680919647)
[2025-02-05 13:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:01][root][INFO] - Training Epoch: 2/2, step 18083/23838 completed (loss: 1.0181095600128174, acc: 0.6744186282157898)
[2025-02-05 13:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:02][root][INFO] - Training Epoch: 2/2, step 18084/23838 completed (loss: 0.8740111589431763, acc: 0.7692307829856873)
[2025-02-05 13:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:02][root][INFO] - Training Epoch: 2/2, step 18085/23838 completed (loss: 1.0357799530029297, acc: 0.692307710647583)
[2025-02-05 13:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:02][root][INFO] - Training Epoch: 2/2, step 18086/23838 completed (loss: 1.0435402393341064, acc: 0.695652186870575)
[2025-02-05 13:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:03][root][INFO] - Training Epoch: 2/2, step 18087/23838 completed (loss: 1.5665504932403564, acc: 0.5714285969734192)
[2025-02-05 13:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:03][root][INFO] - Training Epoch: 2/2, step 18088/23838 completed (loss: 0.5943778157234192, acc: 0.8947368264198303)
[2025-02-05 13:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:04][root][INFO] - Training Epoch: 2/2, step 18089/23838 completed (loss: 1.3551108837127686, acc: 0.6071428656578064)
[2025-02-05 13:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:04][root][INFO] - Training Epoch: 2/2, step 18090/23838 completed (loss: 1.2424207925796509, acc: 0.5925925970077515)
[2025-02-05 13:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:04][root][INFO] - Training Epoch: 2/2, step 18091/23838 completed (loss: 0.9404352307319641, acc: 0.6774193644523621)
[2025-02-05 13:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:05][root][INFO] - Training Epoch: 2/2, step 18092/23838 completed (loss: 1.6115933656692505, acc: 0.5)
[2025-02-05 13:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:05][root][INFO] - Training Epoch: 2/2, step 18093/23838 completed (loss: 1.088392972946167, acc: 0.800000011920929)
[2025-02-05 13:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:06][root][INFO] - Training Epoch: 2/2, step 18094/23838 completed (loss: 0.9939022064208984, acc: 0.75)
[2025-02-05 13:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:06][root][INFO] - Training Epoch: 2/2, step 18095/23838 completed (loss: 1.3807921409606934, acc: 0.5769230723381042)
[2025-02-05 13:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:06][root][INFO] - Training Epoch: 2/2, step 18096/23838 completed (loss: 1.5142379999160767, acc: 0.5483871102333069)
[2025-02-05 13:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:07][root][INFO] - Training Epoch: 2/2, step 18097/23838 completed (loss: 1.2450222969055176, acc: 0.6857143044471741)
[2025-02-05 13:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:07][root][INFO] - Training Epoch: 2/2, step 18098/23838 completed (loss: 1.182483434677124, acc: 0.7142857313156128)
[2025-02-05 13:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:07][root][INFO] - Training Epoch: 2/2, step 18099/23838 completed (loss: 1.4900457859039307, acc: 0.604651153087616)
[2025-02-05 13:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:08][root][INFO] - Training Epoch: 2/2, step 18100/23838 completed (loss: 1.413732647895813, acc: 0.5945945978164673)
[2025-02-05 13:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:08][root][INFO] - Training Epoch: 2/2, step 18101/23838 completed (loss: 1.0509614944458008, acc: 0.695652186870575)
[2025-02-05 13:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:09][root][INFO] - Training Epoch: 2/2, step 18102/23838 completed (loss: 0.979360044002533, acc: 0.6875)
[2025-02-05 13:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:09][root][INFO] - Training Epoch: 2/2, step 18103/23838 completed (loss: 1.6201776266098022, acc: 0.5333333611488342)
[2025-02-05 13:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:09][root][INFO] - Training Epoch: 2/2, step 18104/23838 completed (loss: 1.5499485731124878, acc: 0.6060606241226196)
[2025-02-05 13:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:10][root][INFO] - Training Epoch: 2/2, step 18105/23838 completed (loss: 0.9692915081977844, acc: 0.7083333134651184)
[2025-02-05 13:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:10][root][INFO] - Training Epoch: 2/2, step 18106/23838 completed (loss: 0.8590667247772217, acc: 0.75)
[2025-02-05 13:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:10][root][INFO] - Training Epoch: 2/2, step 18107/23838 completed (loss: 0.5764591097831726, acc: 0.8571428656578064)
[2025-02-05 13:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:11][root][INFO] - Training Epoch: 2/2, step 18108/23838 completed (loss: 1.5008536577224731, acc: 0.6842105388641357)
[2025-02-05 13:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:11][root][INFO] - Training Epoch: 2/2, step 18109/23838 completed (loss: 0.6642811894416809, acc: 0.8333333134651184)
[2025-02-05 13:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:12][root][INFO] - Training Epoch: 2/2, step 18110/23838 completed (loss: 1.7432135343551636, acc: 0.5151515007019043)
[2025-02-05 13:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:12][root][INFO] - Training Epoch: 2/2, step 18111/23838 completed (loss: 1.1960846185684204, acc: 0.625)
[2025-02-05 13:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:12][root][INFO] - Training Epoch: 2/2, step 18112/23838 completed (loss: 1.0161675214767456, acc: 0.6399999856948853)
[2025-02-05 13:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:13][root][INFO] - Training Epoch: 2/2, step 18113/23838 completed (loss: 1.1951138973236084, acc: 0.6200000047683716)
[2025-02-05 13:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:13][root][INFO] - Training Epoch: 2/2, step 18114/23838 completed (loss: 1.1314762830734253, acc: 0.6458333134651184)
[2025-02-05 13:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:14][root][INFO] - Training Epoch: 2/2, step 18115/23838 completed (loss: 0.3437524139881134, acc: 0.9285714030265808)
[2025-02-05 13:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:14][root][INFO] - Training Epoch: 2/2, step 18116/23838 completed (loss: 1.1252483129501343, acc: 0.6595744490623474)
[2025-02-05 13:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:14][root][INFO] - Training Epoch: 2/2, step 18117/23838 completed (loss: 1.3376092910766602, acc: 0.6052631735801697)
[2025-02-05 13:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:15][root][INFO] - Training Epoch: 2/2, step 18118/23838 completed (loss: 0.8477911949157715, acc: 0.695652186870575)
[2025-02-05 13:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:15][root][INFO] - Training Epoch: 2/2, step 18119/23838 completed (loss: 1.4147464036941528, acc: 0.7407407164573669)
[2025-02-05 13:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:16][root][INFO] - Training Epoch: 2/2, step 18120/23838 completed (loss: 1.541741132736206, acc: 0.6428571343421936)
[2025-02-05 13:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:16][root][INFO] - Training Epoch: 2/2, step 18121/23838 completed (loss: 1.047653079032898, acc: 0.7352941036224365)
[2025-02-05 13:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:16][root][INFO] - Training Epoch: 2/2, step 18122/23838 completed (loss: 1.1463321447372437, acc: 0.7083333134651184)
[2025-02-05 13:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:17][root][INFO] - Training Epoch: 2/2, step 18123/23838 completed (loss: 1.1645482778549194, acc: 0.6111111044883728)
[2025-02-05 13:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:17][root][INFO] - Training Epoch: 2/2, step 18124/23838 completed (loss: 1.3501489162445068, acc: 0.6279069781303406)
[2025-02-05 13:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:17][root][INFO] - Training Epoch: 2/2, step 18125/23838 completed (loss: 1.5466527938842773, acc: 0.5483871102333069)
[2025-02-05 13:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:18][root][INFO] - Training Epoch: 2/2, step 18126/23838 completed (loss: 0.9655866026878357, acc: 0.7560975551605225)
[2025-02-05 13:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:18][root][INFO] - Training Epoch: 2/2, step 18127/23838 completed (loss: 1.3248907327651978, acc: 0.6153846383094788)
[2025-02-05 13:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:19][root][INFO] - Training Epoch: 2/2, step 18128/23838 completed (loss: 1.1100471019744873, acc: 0.6399999856948853)
[2025-02-05 13:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:19][root][INFO] - Training Epoch: 2/2, step 18129/23838 completed (loss: 1.3320066928863525, acc: 0.6000000238418579)
[2025-02-05 13:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:19][root][INFO] - Training Epoch: 2/2, step 18130/23838 completed (loss: 0.2565825581550598, acc: 0.9230769276618958)
[2025-02-05 13:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:20][root][INFO] - Training Epoch: 2/2, step 18131/23838 completed (loss: 0.858590304851532, acc: 0.875)
[2025-02-05 13:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:20][root][INFO] - Training Epoch: 2/2, step 18132/23838 completed (loss: 1.046378254890442, acc: 0.8260869383811951)
[2025-02-05 13:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:20][root][INFO] - Training Epoch: 2/2, step 18133/23838 completed (loss: 1.3128021955490112, acc: 0.6486486196517944)
[2025-02-05 13:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:21][root][INFO] - Training Epoch: 2/2, step 18134/23838 completed (loss: 0.9590068459510803, acc: 0.6792452931404114)
[2025-02-05 13:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:21][root][INFO] - Training Epoch: 2/2, step 18135/23838 completed (loss: 1.1910690069198608, acc: 0.649350643157959)
[2025-02-05 13:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:21][root][INFO] - Training Epoch: 2/2, step 18136/23838 completed (loss: 1.293381690979004, acc: 0.6619718074798584)
[2025-02-05 13:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:22][root][INFO] - Training Epoch: 2/2, step 18137/23838 completed (loss: 1.0768606662750244, acc: 0.6530612111091614)
[2025-02-05 13:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:22][root][INFO] - Training Epoch: 2/2, step 18138/23838 completed (loss: 1.5516868829727173, acc: 0.625)
[2025-02-05 13:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:23][root][INFO] - Training Epoch: 2/2, step 18139/23838 completed (loss: 2.2641305923461914, acc: 0.4000000059604645)
[2025-02-05 13:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:23][root][INFO] - Training Epoch: 2/2, step 18140/23838 completed (loss: 1.2820947170257568, acc: 0.6477272510528564)
[2025-02-05 13:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:23][root][INFO] - Training Epoch: 2/2, step 18141/23838 completed (loss: 1.4048686027526855, acc: 0.6382978558540344)
[2025-02-05 13:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:24][root][INFO] - Training Epoch: 2/2, step 18142/23838 completed (loss: 1.0009527206420898, acc: 0.6969696879386902)
[2025-02-05 13:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:24][root][INFO] - Training Epoch: 2/2, step 18143/23838 completed (loss: 0.9267738461494446, acc: 0.6976743936538696)
[2025-02-05 13:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:24][root][INFO] - Training Epoch: 2/2, step 18144/23838 completed (loss: 1.2182495594024658, acc: 0.699999988079071)
[2025-02-05 13:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:25][root][INFO] - Training Epoch: 2/2, step 18145/23838 completed (loss: 1.097446322441101, acc: 0.6666666865348816)
[2025-02-05 13:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:25][root][INFO] - Training Epoch: 2/2, step 18146/23838 completed (loss: 1.107286810874939, acc: 0.699999988079071)
[2025-02-05 13:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:26][root][INFO] - Training Epoch: 2/2, step 18147/23838 completed (loss: 1.0331709384918213, acc: 0.6779661178588867)
[2025-02-05 13:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:26][root][INFO] - Training Epoch: 2/2, step 18148/23838 completed (loss: 0.7598605155944824, acc: 0.7407407164573669)
[2025-02-05 13:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:26][root][INFO] - Training Epoch: 2/2, step 18149/23838 completed (loss: 0.6906428933143616, acc: 0.8108108043670654)
[2025-02-05 13:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:27][root][INFO] - Training Epoch: 2/2, step 18150/23838 completed (loss: 1.3859117031097412, acc: 0.5882353186607361)
[2025-02-05 13:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:27][root][INFO] - Training Epoch: 2/2, step 18151/23838 completed (loss: 0.617746889591217, acc: 0.8399999737739563)
[2025-02-05 13:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:28][root][INFO] - Training Epoch: 2/2, step 18152/23838 completed (loss: 0.31517472863197327, acc: 0.9473684430122375)
[2025-02-05 13:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:28][root][INFO] - Training Epoch: 2/2, step 18153/23838 completed (loss: 1.1080939769744873, acc: 0.6899999976158142)
[2025-02-05 13:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:28][root][INFO] - Training Epoch: 2/2, step 18154/23838 completed (loss: 0.7665885090827942, acc: 0.7792207598686218)
[2025-02-05 13:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:29][root][INFO] - Training Epoch: 2/2, step 18155/23838 completed (loss: 0.9731787443161011, acc: 0.699999988079071)
[2025-02-05 13:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:29][root][INFO] - Training Epoch: 2/2, step 18156/23838 completed (loss: 1.0549025535583496, acc: 0.6960784196853638)
[2025-02-05 13:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:30][root][INFO] - Training Epoch: 2/2, step 18157/23838 completed (loss: 0.7677900195121765, acc: 0.7954545617103577)
[2025-02-05 13:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:30][root][INFO] - Training Epoch: 2/2, step 18158/23838 completed (loss: 0.7936325669288635, acc: 0.75)
[2025-02-05 13:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:30][root][INFO] - Training Epoch: 2/2, step 18159/23838 completed (loss: 0.7980000376701355, acc: 0.7962962985038757)
[2025-02-05 13:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:31][root][INFO] - Training Epoch: 2/2, step 18160/23838 completed (loss: 0.9570907354354858, acc: 0.7454545497894287)
[2025-02-05 13:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:31][root][INFO] - Training Epoch: 2/2, step 18161/23838 completed (loss: 0.7362306118011475, acc: 0.7931034564971924)
[2025-02-05 13:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:31][root][INFO] - Training Epoch: 2/2, step 18162/23838 completed (loss: 1.2116159200668335, acc: 0.6382978558540344)
[2025-02-05 13:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:32][root][INFO] - Training Epoch: 2/2, step 18163/23838 completed (loss: 1.0518707036972046, acc: 0.6790123581886292)
[2025-02-05 13:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:32][root][INFO] - Training Epoch: 2/2, step 18164/23838 completed (loss: 1.1533929109573364, acc: 0.6136363744735718)
[2025-02-05 13:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:32][root][INFO] - Training Epoch: 2/2, step 18165/23838 completed (loss: 1.2529882192611694, acc: 0.6172839403152466)
[2025-02-05 13:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:33][root][INFO] - Training Epoch: 2/2, step 18166/23838 completed (loss: 1.146188497543335, acc: 0.6511628031730652)
[2025-02-05 13:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:33][root][INFO] - Training Epoch: 2/2, step 18167/23838 completed (loss: 1.1773216724395752, acc: 0.6849315166473389)
[2025-02-05 13:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:34][root][INFO] - Training Epoch: 2/2, step 18168/23838 completed (loss: 0.8157459497451782, acc: 0.8035714030265808)
[2025-02-05 13:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:34][root][INFO] - Training Epoch: 2/2, step 18169/23838 completed (loss: 1.583113431930542, acc: 0.5242718458175659)
[2025-02-05 13:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:34][root][INFO] - Training Epoch: 2/2, step 18170/23838 completed (loss: 1.1429011821746826, acc: 0.6597937941551208)
[2025-02-05 13:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:35][root][INFO] - Training Epoch: 2/2, step 18171/23838 completed (loss: 1.3137435913085938, acc: 0.6415094137191772)
[2025-02-05 13:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:35][root][INFO] - Training Epoch: 2/2, step 18172/23838 completed (loss: 1.4028562307357788, acc: 0.593406617641449)
[2025-02-05 13:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:36][root][INFO] - Training Epoch: 2/2, step 18173/23838 completed (loss: 1.624977469444275, acc: 0.5909090638160706)
[2025-02-05 13:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:36][root][INFO] - Training Epoch: 2/2, step 18174/23838 completed (loss: 1.4539072513580322, acc: 0.633093535900116)
[2025-02-05 13:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:37][root][INFO] - Training Epoch: 2/2, step 18175/23838 completed (loss: 1.3149560689926147, acc: 0.5982906222343445)
[2025-02-05 13:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:37][root][INFO] - Training Epoch: 2/2, step 18176/23838 completed (loss: 1.5023242235183716, acc: 0.6461538672447205)
[2025-02-05 13:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:37][root][INFO] - Training Epoch: 2/2, step 18177/23838 completed (loss: 1.467081069946289, acc: 0.6145833134651184)
[2025-02-05 13:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:38][root][INFO] - Training Epoch: 2/2, step 18178/23838 completed (loss: 1.526820182800293, acc: 0.5797101259231567)
[2025-02-05 13:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:38][root][INFO] - Training Epoch: 2/2, step 18179/23838 completed (loss: 1.489152431488037, acc: 0.5903614163398743)
[2025-02-05 13:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:38][root][INFO] - Training Epoch: 2/2, step 18180/23838 completed (loss: 1.4523587226867676, acc: 0.5555555820465088)
[2025-02-05 13:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:39][root][INFO] - Training Epoch: 2/2, step 18181/23838 completed (loss: 1.4703900814056396, acc: 0.574999988079071)
[2025-02-05 13:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:39][root][INFO] - Training Epoch: 2/2, step 18182/23838 completed (loss: 1.197885513305664, acc: 0.6976743936538696)
[2025-02-05 13:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:40][root][INFO] - Training Epoch: 2/2, step 18183/23838 completed (loss: 1.2963018417358398, acc: 0.6363636255264282)
[2025-02-05 13:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:40][root][INFO] - Training Epoch: 2/2, step 18184/23838 completed (loss: 1.3186150789260864, acc: 0.5692307949066162)
[2025-02-05 13:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:40][root][INFO] - Training Epoch: 2/2, step 18185/23838 completed (loss: 1.2626543045043945, acc: 0.6454545259475708)
[2025-02-05 13:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:41][root][INFO] - Training Epoch: 2/2, step 18186/23838 completed (loss: 1.5749627351760864, acc: 0.5111111402511597)
[2025-02-05 13:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:41][root][INFO] - Training Epoch: 2/2, step 18187/23838 completed (loss: 1.3957512378692627, acc: 0.603960394859314)
[2025-02-05 13:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:42][root][INFO] - Training Epoch: 2/2, step 18188/23838 completed (loss: 1.3178160190582275, acc: 0.6000000238418579)
[2025-02-05 13:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:42][root][INFO] - Training Epoch: 2/2, step 18189/23838 completed (loss: 1.8044583797454834, acc: 0.44999998807907104)
[2025-02-05 13:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:42][root][INFO] - Training Epoch: 2/2, step 18190/23838 completed (loss: 1.3006843328475952, acc: 0.5822784900665283)
[2025-02-05 13:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:43][root][INFO] - Training Epoch: 2/2, step 18191/23838 completed (loss: 1.5783798694610596, acc: 0.5476190447807312)
[2025-02-05 13:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:43][root][INFO] - Training Epoch: 2/2, step 18192/23838 completed (loss: 1.3567781448364258, acc: 0.625)
[2025-02-05 13:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:44][root][INFO] - Training Epoch: 2/2, step 18193/23838 completed (loss: 1.2731220722198486, acc: 0.6399999856948853)
[2025-02-05 13:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:44][root][INFO] - Training Epoch: 2/2, step 18194/23838 completed (loss: 1.2135683298110962, acc: 0.6129032373428345)
[2025-02-05 13:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:44][root][INFO] - Training Epoch: 2/2, step 18195/23838 completed (loss: 0.9897986650466919, acc: 0.6774193644523621)
[2025-02-05 13:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:45][root][INFO] - Training Epoch: 2/2, step 18196/23838 completed (loss: 1.2775938510894775, acc: 0.5806451439857483)
[2025-02-05 13:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:45][root][INFO] - Training Epoch: 2/2, step 18197/23838 completed (loss: 0.7984916567802429, acc: 0.7674418687820435)
[2025-02-05 13:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:46][root][INFO] - Training Epoch: 2/2, step 18198/23838 completed (loss: 1.3620933294296265, acc: 0.6279069781303406)
[2025-02-05 13:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:46][root][INFO] - Training Epoch: 2/2, step 18199/23838 completed (loss: 1.2767611742019653, acc: 0.5909090638160706)
[2025-02-05 13:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:46][root][INFO] - Training Epoch: 2/2, step 18200/23838 completed (loss: 1.2519762516021729, acc: 0.6607142686843872)
[2025-02-05 13:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:47][root][INFO] - Training Epoch: 2/2, step 18201/23838 completed (loss: 0.9340531826019287, acc: 0.78125)
[2025-02-05 13:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:47][root][INFO] - Training Epoch: 2/2, step 18202/23838 completed (loss: 1.216461181640625, acc: 0.6071428656578064)
[2025-02-05 13:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:47][root][INFO] - Training Epoch: 2/2, step 18203/23838 completed (loss: 1.1838531494140625, acc: 0.6226415038108826)
[2025-02-05 13:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:48][root][INFO] - Training Epoch: 2/2, step 18204/23838 completed (loss: 1.2957282066345215, acc: 0.6617646813392639)
[2025-02-05 13:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:48][root][INFO] - Training Epoch: 2/2, step 18205/23838 completed (loss: 1.2247861623764038, acc: 0.6224489808082581)
[2025-02-05 13:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:49][root][INFO] - Training Epoch: 2/2, step 18206/23838 completed (loss: 1.0896912813186646, acc: 0.6363636255264282)
[2025-02-05 13:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:49][root][INFO] - Training Epoch: 2/2, step 18207/23838 completed (loss: 1.5066359043121338, acc: 0.5384615659713745)
[2025-02-05 13:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:49][root][INFO] - Training Epoch: 2/2, step 18208/23838 completed (loss: 1.2380218505859375, acc: 0.6428571343421936)
[2025-02-05 13:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:50][root][INFO] - Training Epoch: 2/2, step 18209/23838 completed (loss: 1.4962048530578613, acc: 0.5151515007019043)
[2025-02-05 13:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:50][root][INFO] - Training Epoch: 2/2, step 18210/23838 completed (loss: 1.5846501588821411, acc: 0.5)
[2025-02-05 13:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:51][root][INFO] - Training Epoch: 2/2, step 18211/23838 completed (loss: 1.2336002588272095, acc: 0.625)
[2025-02-05 13:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:51][root][INFO] - Training Epoch: 2/2, step 18212/23838 completed (loss: 1.157714605331421, acc: 0.6666666865348816)
[2025-02-05 13:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:51][root][INFO] - Training Epoch: 2/2, step 18213/23838 completed (loss: 1.5507946014404297, acc: 0.5487805008888245)
[2025-02-05 13:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:52][root][INFO] - Training Epoch: 2/2, step 18214/23838 completed (loss: 1.1666889190673828, acc: 0.6699029207229614)
[2025-02-05 13:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:52][root][INFO] - Training Epoch: 2/2, step 18215/23838 completed (loss: 1.0771141052246094, acc: 0.7037037014961243)
[2025-02-05 13:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:53][root][INFO] - Training Epoch: 2/2, step 18216/23838 completed (loss: 0.9230066537857056, acc: 0.699999988079071)
[2025-02-05 13:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:53][root][INFO] - Training Epoch: 2/2, step 18217/23838 completed (loss: 1.17180335521698, acc: 0.6336633563041687)
[2025-02-05 13:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:53][root][INFO] - Training Epoch: 2/2, step 18218/23838 completed (loss: 1.2713450193405151, acc: 0.6240000128746033)
[2025-02-05 13:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:54][root][INFO] - Training Epoch: 2/2, step 18219/23838 completed (loss: 1.0739175081253052, acc: 0.6458333134651184)
[2025-02-05 13:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:54][root][INFO] - Training Epoch: 2/2, step 18220/23838 completed (loss: 1.4834736585617065, acc: 0.5409836173057556)
[2025-02-05 13:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:55][root][INFO] - Training Epoch: 2/2, step 18221/23838 completed (loss: 1.5128830671310425, acc: 0.569767415523529)
[2025-02-05 13:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:55][root][INFO] - Training Epoch: 2/2, step 18222/23838 completed (loss: 1.5786727666854858, acc: 0.54666668176651)
[2025-02-05 13:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:55][root][INFO] - Training Epoch: 2/2, step 18223/23838 completed (loss: 1.2135239839553833, acc: 0.6842105388641357)
[2025-02-05 13:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:56][root][INFO] - Training Epoch: 2/2, step 18224/23838 completed (loss: 1.5014809370040894, acc: 0.540229856967926)
[2025-02-05 13:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:56][root][INFO] - Training Epoch: 2/2, step 18225/23838 completed (loss: 1.1499894857406616, acc: 0.6200000047683716)
[2025-02-05 13:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:57][root][INFO] - Training Epoch: 2/2, step 18226/23838 completed (loss: 1.3331396579742432, acc: 0.59375)
[2025-02-05 13:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:57][root][INFO] - Training Epoch: 2/2, step 18227/23838 completed (loss: 1.5060521364212036, acc: 0.6111111044883728)
[2025-02-05 13:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:57][root][INFO] - Training Epoch: 2/2, step 18228/23838 completed (loss: 1.4650940895080566, acc: 0.5740740895271301)
[2025-02-05 13:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:58][root][INFO] - Training Epoch: 2/2, step 18229/23838 completed (loss: 1.3384625911712646, acc: 0.5947712659835815)
[2025-02-05 13:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:58][root][INFO] - Training Epoch: 2/2, step 18230/23838 completed (loss: 1.3597792387008667, acc: 0.6000000238418579)
[2025-02-05 13:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:59][root][INFO] - Training Epoch: 2/2, step 18231/23838 completed (loss: 1.477324366569519, acc: 0.5802469253540039)
[2025-02-05 13:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:59][root][INFO] - Training Epoch: 2/2, step 18232/23838 completed (loss: 1.154911756515503, acc: 0.6666666865348816)
[2025-02-05 13:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:41:59][root][INFO] - Training Epoch: 2/2, step 18233/23838 completed (loss: 1.5211693048477173, acc: 0.5370370149612427)
[2025-02-05 13:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:00][root][INFO] - Training Epoch: 2/2, step 18234/23838 completed (loss: 1.5178943872451782, acc: 0.5616438388824463)
[2025-02-05 13:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:00][root][INFO] - Training Epoch: 2/2, step 18235/23838 completed (loss: 1.1555283069610596, acc: 0.6530612111091614)
[2025-02-05 13:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:01][root][INFO] - Training Epoch: 2/2, step 18236/23838 completed (loss: 0.965086817741394, acc: 0.6666666865348816)
[2025-02-05 13:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:01][root][INFO] - Training Epoch: 2/2, step 18237/23838 completed (loss: 1.040464997291565, acc: 0.6666666865348816)
[2025-02-05 13:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:02][root][INFO] - Training Epoch: 2/2, step 18238/23838 completed (loss: 1.199607253074646, acc: 0.6086956262588501)
[2025-02-05 13:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:02][root][INFO] - Training Epoch: 2/2, step 18239/23838 completed (loss: 1.4369689226150513, acc: 0.5490196347236633)
[2025-02-05 13:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:02][root][INFO] - Training Epoch: 2/2, step 18240/23838 completed (loss: 0.9351246953010559, acc: 0.7567567825317383)
[2025-02-05 13:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:03][root][INFO] - Training Epoch: 2/2, step 18241/23838 completed (loss: 1.5620397329330444, acc: 0.5423728823661804)
[2025-02-05 13:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:03][root][INFO] - Training Epoch: 2/2, step 18242/23838 completed (loss: 1.300693154335022, acc: 0.6000000238418579)
[2025-02-05 13:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:03][root][INFO] - Training Epoch: 2/2, step 18243/23838 completed (loss: 1.59771728515625, acc: 0.546875)
[2025-02-05 13:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:04][root][INFO] - Training Epoch: 2/2, step 18244/23838 completed (loss: 1.192762851715088, acc: 0.6951219439506531)
[2025-02-05 13:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:04][root][INFO] - Training Epoch: 2/2, step 18245/23838 completed (loss: 0.9917177557945251, acc: 0.7162162065505981)
[2025-02-05 13:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:05][root][INFO] - Training Epoch: 2/2, step 18246/23838 completed (loss: 1.1020454168319702, acc: 0.6893203854560852)
[2025-02-05 13:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:05][root][INFO] - Training Epoch: 2/2, step 18247/23838 completed (loss: 1.2293404340744019, acc: 0.6521739363670349)
[2025-02-05 13:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:05][root][INFO] - Training Epoch: 2/2, step 18248/23838 completed (loss: 1.217670202255249, acc: 0.6576576828956604)
[2025-02-05 13:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:06][root][INFO] - Training Epoch: 2/2, step 18249/23838 completed (loss: 1.2284053564071655, acc: 0.6551724076271057)
[2025-02-05 13:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:06][root][INFO] - Training Epoch: 2/2, step 18250/23838 completed (loss: 1.2576957941055298, acc: 0.6666666865348816)
[2025-02-05 13:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:06][root][INFO] - Training Epoch: 2/2, step 18251/23838 completed (loss: 0.9831104874610901, acc: 0.694656491279602)
[2025-02-05 13:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:07][root][INFO] - Training Epoch: 2/2, step 18252/23838 completed (loss: 1.2026631832122803, acc: 0.6328125)
[2025-02-05 13:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:07][root][INFO] - Training Epoch: 2/2, step 18253/23838 completed (loss: 1.2926528453826904, acc: 0.6333333253860474)
[2025-02-05 13:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:08][root][INFO] - Training Epoch: 2/2, step 18254/23838 completed (loss: 1.3233519792556763, acc: 0.6036036014556885)
[2025-02-05 13:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:08][root][INFO] - Training Epoch: 2/2, step 18255/23838 completed (loss: 1.4506264925003052, acc: 0.5666666626930237)
[2025-02-05 13:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:09][root][INFO] - Training Epoch: 2/2, step 18256/23838 completed (loss: 1.6045351028442383, acc: 0.5609756112098694)
[2025-02-05 13:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:09][root][INFO] - Training Epoch: 2/2, step 18257/23838 completed (loss: 1.286290168762207, acc: 0.6603773832321167)
[2025-02-05 13:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:09][root][INFO] - Training Epoch: 2/2, step 18258/23838 completed (loss: 1.3413044214248657, acc: 0.5799999833106995)
[2025-02-05 13:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:10][root][INFO] - Training Epoch: 2/2, step 18259/23838 completed (loss: 1.2639007568359375, acc: 0.6384615302085876)
[2025-02-05 13:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:10][root][INFO] - Training Epoch: 2/2, step 18260/23838 completed (loss: 0.9744744896888733, acc: 0.7654321193695068)
[2025-02-05 13:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:11][root][INFO] - Training Epoch: 2/2, step 18261/23838 completed (loss: 1.2219198942184448, acc: 0.6393442749977112)
[2025-02-05 13:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:11][root][INFO] - Training Epoch: 2/2, step 18262/23838 completed (loss: 1.3502520322799683, acc: 0.6548672318458557)
[2025-02-05 13:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:11][root][INFO] - Training Epoch: 2/2, step 18263/23838 completed (loss: 1.1512019634246826, acc: 0.686274528503418)
[2025-02-05 13:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:12][root][INFO] - Training Epoch: 2/2, step 18264/23838 completed (loss: 1.1098122596740723, acc: 0.695652186870575)
[2025-02-05 13:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:12][root][INFO] - Training Epoch: 2/2, step 18265/23838 completed (loss: 1.2977349758148193, acc: 0.5799999833106995)
[2025-02-05 13:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:13][root][INFO] - Training Epoch: 2/2, step 18266/23838 completed (loss: 0.7838228344917297, acc: 0.7760000228881836)
[2025-02-05 13:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:13][root][INFO] - Training Epoch: 2/2, step 18267/23838 completed (loss: 1.1647769212722778, acc: 0.7142857313156128)
[2025-02-05 13:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:13][root][INFO] - Training Epoch: 2/2, step 18268/23838 completed (loss: 0.8925451040267944, acc: 0.7611940503120422)
[2025-02-05 13:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:14][root][INFO] - Training Epoch: 2/2, step 18269/23838 completed (loss: 1.082947015762329, acc: 0.6760563254356384)
[2025-02-05 13:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:14][root][INFO] - Training Epoch: 2/2, step 18270/23838 completed (loss: 1.1691118478775024, acc: 0.6695652008056641)
[2025-02-05 13:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:14][root][INFO] - Training Epoch: 2/2, step 18271/23838 completed (loss: 1.1104246377944946, acc: 0.6499999761581421)
[2025-02-05 13:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:15][root][INFO] - Training Epoch: 2/2, step 18272/23838 completed (loss: 1.1564346551895142, acc: 0.6781609058380127)
[2025-02-05 13:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:15][root][INFO] - Training Epoch: 2/2, step 18273/23838 completed (loss: 1.1997497081756592, acc: 0.6571428775787354)
[2025-02-05 13:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:16][root][INFO] - Training Epoch: 2/2, step 18274/23838 completed (loss: 1.0936813354492188, acc: 0.6842105388641357)
[2025-02-05 13:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:16][root][INFO] - Training Epoch: 2/2, step 18275/23838 completed (loss: 1.2031190395355225, acc: 0.6666666865348816)
[2025-02-05 13:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:17][root][INFO] - Training Epoch: 2/2, step 18276/23838 completed (loss: 1.282252311706543, acc: 0.6091370582580566)
[2025-02-05 13:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:17][root][INFO] - Training Epoch: 2/2, step 18277/23838 completed (loss: 1.1934192180633545, acc: 0.6413043737411499)
[2025-02-05 13:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:17][root][INFO] - Training Epoch: 2/2, step 18278/23838 completed (loss: 1.1503574848175049, acc: 0.695652186870575)
[2025-02-05 13:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:18][root][INFO] - Training Epoch: 2/2, step 18279/23838 completed (loss: 1.042611002922058, acc: 0.7384615540504456)
[2025-02-05 13:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:18][root][INFO] - Training Epoch: 2/2, step 18280/23838 completed (loss: 0.83218914270401, acc: 0.7887324094772339)
[2025-02-05 13:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:19][root][INFO] - Training Epoch: 2/2, step 18281/23838 completed (loss: 1.3481687307357788, acc: 0.6578947305679321)
[2025-02-05 13:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:19][root][INFO] - Training Epoch: 2/2, step 18282/23838 completed (loss: 1.4341732263565063, acc: 0.6666666865348816)
[2025-02-05 13:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:19][root][INFO] - Training Epoch: 2/2, step 18283/23838 completed (loss: 1.56831955909729, acc: 0.6470588445663452)
[2025-02-05 13:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:20][root][INFO] - Training Epoch: 2/2, step 18284/23838 completed (loss: 0.959983229637146, acc: 0.6875)
[2025-02-05 13:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:20][root][INFO] - Training Epoch: 2/2, step 18285/23838 completed (loss: 1.4668182134628296, acc: 0.6666666865348816)
[2025-02-05 13:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:20][root][INFO] - Training Epoch: 2/2, step 18286/23838 completed (loss: 1.2367888689041138, acc: 0.5510203838348389)
[2025-02-05 13:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:21][root][INFO] - Training Epoch: 2/2, step 18287/23838 completed (loss: 0.8115313053131104, acc: 0.800000011920929)
[2025-02-05 13:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:21][root][INFO] - Training Epoch: 2/2, step 18288/23838 completed (loss: 1.3091100454330444, acc: 0.7142857313156128)
[2025-02-05 13:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:22][root][INFO] - Training Epoch: 2/2, step 18289/23838 completed (loss: 1.1007490158081055, acc: 0.7222222089767456)
[2025-02-05 13:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:22][root][INFO] - Training Epoch: 2/2, step 18290/23838 completed (loss: 0.9155082702636719, acc: 0.7428571581840515)
[2025-02-05 13:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:23][root][INFO] - Training Epoch: 2/2, step 18291/23838 completed (loss: 1.3725029230117798, acc: 0.6000000238418579)
[2025-02-05 13:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:23][root][INFO] - Training Epoch: 2/2, step 18292/23838 completed (loss: 1.2500567436218262, acc: 0.6399999856948853)
[2025-02-05 13:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:23][root][INFO] - Training Epoch: 2/2, step 18293/23838 completed (loss: 1.2244398593902588, acc: 0.5833333134651184)
[2025-02-05 13:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:24][root][INFO] - Training Epoch: 2/2, step 18294/23838 completed (loss: 1.1861860752105713, acc: 0.6285714507102966)
[2025-02-05 13:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:24][root][INFO] - Training Epoch: 2/2, step 18295/23838 completed (loss: 1.3279016017913818, acc: 0.5853658318519592)
[2025-02-05 13:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:24][root][INFO] - Training Epoch: 2/2, step 18296/23838 completed (loss: 1.1975994110107422, acc: 0.6666666865348816)
[2025-02-05 13:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:25][root][INFO] - Training Epoch: 2/2, step 18297/23838 completed (loss: 1.2670471668243408, acc: 0.625)
[2025-02-05 13:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:25][root][INFO] - Training Epoch: 2/2, step 18298/23838 completed (loss: 1.4960209131240845, acc: 0.5783132314682007)
[2025-02-05 13:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:26][root][INFO] - Training Epoch: 2/2, step 18299/23838 completed (loss: 1.3722301721572876, acc: 0.5967742204666138)
[2025-02-05 13:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:26][root][INFO] - Training Epoch: 2/2, step 18300/23838 completed (loss: 1.2072274684906006, acc: 0.7333333492279053)
[2025-02-05 13:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:26][root][INFO] - Training Epoch: 2/2, step 18301/23838 completed (loss: 3.0235140323638916, acc: 0.0)
[2025-02-05 13:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:27][root][INFO] - Training Epoch: 2/2, step 18302/23838 completed (loss: 1.0843501091003418, acc: 0.6111111044883728)
[2025-02-05 13:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:27][root][INFO] - Training Epoch: 2/2, step 18303/23838 completed (loss: 0.9864851832389832, acc: 0.75)
[2025-02-05 13:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:28][root][INFO] - Training Epoch: 2/2, step 18304/23838 completed (loss: 1.6216034889221191, acc: 0.4545454680919647)
[2025-02-05 13:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:28][root][INFO] - Training Epoch: 2/2, step 18305/23838 completed (loss: 0.935741126537323, acc: 0.7272727489471436)
[2025-02-05 13:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:28][root][INFO] - Training Epoch: 2/2, step 18306/23838 completed (loss: 1.1270155906677246, acc: 0.6966292262077332)
[2025-02-05 13:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:29][root][INFO] - Training Epoch: 2/2, step 18307/23838 completed (loss: 1.3862947225570679, acc: 0.6181818246841431)
[2025-02-05 13:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:29][root][INFO] - Training Epoch: 2/2, step 18308/23838 completed (loss: 1.1415997743606567, acc: 0.6370370388031006)
[2025-02-05 13:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:30][root][INFO] - Training Epoch: 2/2, step 18309/23838 completed (loss: 1.1812496185302734, acc: 0.6470588445663452)
[2025-02-05 13:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:30][root][INFO] - Training Epoch: 2/2, step 18310/23838 completed (loss: 1.1314146518707275, acc: 0.7076923251152039)
[2025-02-05 13:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:30][root][INFO] - Training Epoch: 2/2, step 18311/23838 completed (loss: 1.1745905876159668, acc: 0.6951219439506531)
[2025-02-05 13:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:31][root][INFO] - Training Epoch: 2/2, step 18312/23838 completed (loss: 1.2983452081680298, acc: 0.643478274345398)
[2025-02-05 13:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:31][root][INFO] - Training Epoch: 2/2, step 18313/23838 completed (loss: 1.3762362003326416, acc: 0.6465517282485962)
[2025-02-05 13:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:31][root][INFO] - Training Epoch: 2/2, step 18314/23838 completed (loss: 1.0193558931350708, acc: 0.7142857313156128)
[2025-02-05 13:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:32][root][INFO] - Training Epoch: 2/2, step 18315/23838 completed (loss: 1.1013214588165283, acc: 0.682539701461792)
[2025-02-05 13:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:32][root][INFO] - Training Epoch: 2/2, step 18316/23838 completed (loss: 1.5136884450912476, acc: 0.5869565010070801)
[2025-02-05 13:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:33][root][INFO] - Training Epoch: 2/2, step 18317/23838 completed (loss: 0.7638689875602722, acc: 0.7611940503120422)
[2025-02-05 13:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:33][root][INFO] - Training Epoch: 2/2, step 18318/23838 completed (loss: 0.8279760479927063, acc: 0.7272727489471436)
[2025-02-05 13:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:33][root][INFO] - Training Epoch: 2/2, step 18319/23838 completed (loss: 1.4249039888381958, acc: 0.5974025726318359)
[2025-02-05 13:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:34][root][INFO] - Training Epoch: 2/2, step 18320/23838 completed (loss: 1.7830959558486938, acc: 0.5)
[2025-02-05 13:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:34][root][INFO] - Training Epoch: 2/2, step 18321/23838 completed (loss: 1.4187718629837036, acc: 0.5726495981216431)
[2025-02-05 13:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:35][root][INFO] - Training Epoch: 2/2, step 18322/23838 completed (loss: 1.0233070850372314, acc: 0.6818181872367859)
[2025-02-05 13:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:35][root][INFO] - Training Epoch: 2/2, step 18323/23838 completed (loss: 1.2692091464996338, acc: 0.6022727489471436)
[2025-02-05 13:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:35][root][INFO] - Training Epoch: 2/2, step 18324/23838 completed (loss: 1.1894363164901733, acc: 0.6200000047683716)
[2025-02-05 13:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:36][root][INFO] - Training Epoch: 2/2, step 18325/23838 completed (loss: 1.2162202596664429, acc: 0.6299999952316284)
[2025-02-05 13:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:36][root][INFO] - Training Epoch: 2/2, step 18326/23838 completed (loss: 1.2038642168045044, acc: 0.6063829660415649)
[2025-02-05 13:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:37][root][INFO] - Training Epoch: 2/2, step 18327/23838 completed (loss: 1.1509429216384888, acc: 0.6481481194496155)
[2025-02-05 13:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:37][root][INFO] - Training Epoch: 2/2, step 18328/23838 completed (loss: 1.155063271522522, acc: 0.640625)
[2025-02-05 13:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:37][root][INFO] - Training Epoch: 2/2, step 18329/23838 completed (loss: 1.3561005592346191, acc: 0.6436170339584351)
[2025-02-05 13:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:38][root][INFO] - Training Epoch: 2/2, step 18330/23838 completed (loss: 1.3521337509155273, acc: 0.6097561120986938)
[2025-02-05 13:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:38][root][INFO] - Training Epoch: 2/2, step 18331/23838 completed (loss: 1.1616218090057373, acc: 0.686956524848938)
[2025-02-05 13:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:39][root][INFO] - Training Epoch: 2/2, step 18332/23838 completed (loss: 1.1560598611831665, acc: 0.6639344096183777)
[2025-02-05 13:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:39][root][INFO] - Training Epoch: 2/2, step 18333/23838 completed (loss: 1.400117039680481, acc: 0.6335877776145935)
[2025-02-05 13:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:39][root][INFO] - Training Epoch: 2/2, step 18334/23838 completed (loss: 1.3498826026916504, acc: 0.6344085931777954)
[2025-02-05 13:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:40][root][INFO] - Training Epoch: 2/2, step 18335/23838 completed (loss: 1.117713451385498, acc: 0.7472527623176575)
[2025-02-05 13:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:40][root][INFO] - Training Epoch: 2/2, step 18336/23838 completed (loss: 1.2835723161697388, acc: 0.6132075190544128)
[2025-02-05 13:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:41][root][INFO] - Training Epoch: 2/2, step 18337/23838 completed (loss: 1.3680598735809326, acc: 0.642201840877533)
[2025-02-05 13:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:41][root][INFO] - Training Epoch: 2/2, step 18338/23838 completed (loss: 1.1389868259429932, acc: 0.686274528503418)
[2025-02-05 13:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:42][root][INFO] - Training Epoch: 2/2, step 18339/23838 completed (loss: 1.2411510944366455, acc: 0.6666666865348816)
[2025-02-05 13:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:42][root][INFO] - Training Epoch: 2/2, step 18340/23838 completed (loss: 1.4041650295257568, acc: 0.5859375)
[2025-02-05 13:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:42][root][INFO] - Training Epoch: 2/2, step 18341/23838 completed (loss: 1.3802675008773804, acc: 0.5967742204666138)
[2025-02-05 13:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:43][root][INFO] - Training Epoch: 2/2, step 18342/23838 completed (loss: 1.263696312904358, acc: 0.5966386795043945)
[2025-02-05 13:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:43][root][INFO] - Training Epoch: 2/2, step 18343/23838 completed (loss: 1.4674280881881714, acc: 0.5584415793418884)
[2025-02-05 13:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:43][root][INFO] - Training Epoch: 2/2, step 18344/23838 completed (loss: 0.8554176688194275, acc: 0.6857143044471741)
[2025-02-05 13:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:44][root][INFO] - Training Epoch: 2/2, step 18345/23838 completed (loss: 0.9861403703689575, acc: 0.6741573214530945)
[2025-02-05 13:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:44][root][INFO] - Training Epoch: 2/2, step 18346/23838 completed (loss: 0.8320457339286804, acc: 0.7924528121948242)
[2025-02-05 13:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:45][root][INFO] - Training Epoch: 2/2, step 18347/23838 completed (loss: 1.1806104183197021, acc: 0.6698113083839417)
[2025-02-05 13:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:45][root][INFO] - Training Epoch: 2/2, step 18348/23838 completed (loss: 1.1487129926681519, acc: 0.6052631735801697)
[2025-02-05 13:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:45][root][INFO] - Training Epoch: 2/2, step 18349/23838 completed (loss: 1.234042763710022, acc: 0.6190476417541504)
[2025-02-05 13:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:46][root][INFO] - Training Epoch: 2/2, step 18350/23838 completed (loss: 1.5558074712753296, acc: 0.6074766516685486)
[2025-02-05 13:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:46][root][INFO] - Training Epoch: 2/2, step 18351/23838 completed (loss: 1.3778704404830933, acc: 0.5843373537063599)
[2025-02-05 13:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:47][root][INFO] - Training Epoch: 2/2, step 18352/23838 completed (loss: 0.8227952122688293, acc: 0.7669903039932251)
[2025-02-05 13:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:47][root][INFO] - Training Epoch: 2/2, step 18353/23838 completed (loss: 1.0350288152694702, acc: 0.6771653294563293)
[2025-02-05 13:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:48][root][INFO] - Training Epoch: 2/2, step 18354/23838 completed (loss: 0.9726403951644897, acc: 0.7142857313156128)
[2025-02-05 13:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:48][root][INFO] - Training Epoch: 2/2, step 18355/23838 completed (loss: 0.7639592289924622, acc: 0.7984496355056763)
[2025-02-05 13:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:48][root][INFO] - Training Epoch: 2/2, step 18356/23838 completed (loss: 0.854843020439148, acc: 0.7027027010917664)
[2025-02-05 13:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:49][root][INFO] - Training Epoch: 2/2, step 18357/23838 completed (loss: 1.3780372142791748, acc: 0.5416666865348816)
[2025-02-05 13:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:49][root][INFO] - Training Epoch: 2/2, step 18358/23838 completed (loss: 0.8903377056121826, acc: 0.7538461685180664)
[2025-02-05 13:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:50][root][INFO] - Training Epoch: 2/2, step 18359/23838 completed (loss: 1.1890126466751099, acc: 0.6896551847457886)
[2025-02-05 13:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:50][root][INFO] - Training Epoch: 2/2, step 18360/23838 completed (loss: 1.1604949235916138, acc: 0.6229507923126221)
[2025-02-05 13:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:50][root][INFO] - Training Epoch: 2/2, step 18361/23838 completed (loss: 0.671343207359314, acc: 0.7611940503120422)
[2025-02-05 13:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:51][root][INFO] - Training Epoch: 2/2, step 18362/23838 completed (loss: 1.1818078756332397, acc: 0.7105262875556946)
[2025-02-05 13:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:51][root][INFO] - Training Epoch: 2/2, step 18363/23838 completed (loss: 0.9344250559806824, acc: 0.7887324094772339)
[2025-02-05 13:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:52][root][INFO] - Training Epoch: 2/2, step 18364/23838 completed (loss: 1.0235458612442017, acc: 0.7142857313156128)
[2025-02-05 13:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:52][root][INFO] - Training Epoch: 2/2, step 18365/23838 completed (loss: 1.0948604345321655, acc: 0.6753246784210205)
[2025-02-05 13:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:52][root][INFO] - Training Epoch: 2/2, step 18366/23838 completed (loss: 1.2082825899124146, acc: 0.6666666865348816)
[2025-02-05 13:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:53][root][INFO] - Training Epoch: 2/2, step 18367/23838 completed (loss: 1.0222351551055908, acc: 0.7109375)
[2025-02-05 13:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:53][root][INFO] - Training Epoch: 2/2, step 18368/23838 completed (loss: 1.0507148504257202, acc: 0.7230769395828247)
[2025-02-05 13:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:53][root][INFO] - Training Epoch: 2/2, step 18369/23838 completed (loss: 1.1836599111557007, acc: 0.6800000071525574)
[2025-02-05 13:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:54][root][INFO] - Training Epoch: 2/2, step 18370/23838 completed (loss: 1.309901237487793, acc: 0.6323529481887817)
[2025-02-05 13:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:54][root][INFO] - Training Epoch: 2/2, step 18371/23838 completed (loss: 1.0376839637756348, acc: 0.6388888955116272)
[2025-02-05 13:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:55][root][INFO] - Training Epoch: 2/2, step 18372/23838 completed (loss: 1.2145041227340698, acc: 0.6190476417541504)
[2025-02-05 13:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:55][root][INFO] - Training Epoch: 2/2, step 18373/23838 completed (loss: 1.7485138177871704, acc: 0.4333333373069763)
[2025-02-05 13:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:56][root][INFO] - Training Epoch: 2/2, step 18374/23838 completed (loss: 1.2547845840454102, acc: 0.642276406288147)
[2025-02-05 13:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:56][root][INFO] - Training Epoch: 2/2, step 18375/23838 completed (loss: 1.3104603290557861, acc: 0.6160714030265808)
[2025-02-05 13:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:57][root][INFO] - Training Epoch: 2/2, step 18376/23838 completed (loss: 0.9273667335510254, acc: 0.675000011920929)
[2025-02-05 13:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:57][root][INFO] - Training Epoch: 2/2, step 18377/23838 completed (loss: 1.0241330862045288, acc: 0.6907216310501099)
[2025-02-05 13:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:57][root][INFO] - Training Epoch: 2/2, step 18378/23838 completed (loss: 1.1291505098342896, acc: 0.694915235042572)
[2025-02-05 13:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:58][root][INFO] - Training Epoch: 2/2, step 18379/23838 completed (loss: 1.3006560802459717, acc: 0.6190476417541504)
[2025-02-05 13:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:58][root][INFO] - Training Epoch: 2/2, step 18380/23838 completed (loss: 1.2303659915924072, acc: 0.6724137663841248)
[2025-02-05 13:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:59][root][INFO] - Training Epoch: 2/2, step 18381/23838 completed (loss: 1.1663455963134766, acc: 0.6779661178588867)
[2025-02-05 13:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:59][root][INFO] - Training Epoch: 2/2, step 18382/23838 completed (loss: 1.0201935768127441, acc: 0.6511628031730652)
[2025-02-05 13:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:42:59][root][INFO] - Training Epoch: 2/2, step 18383/23838 completed (loss: 1.206505537033081, acc: 0.6842105388641357)
[2025-02-05 13:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:00][root][INFO] - Training Epoch: 2/2, step 18384/23838 completed (loss: 1.066689372062683, acc: 0.7203390002250671)
[2025-02-05 13:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:00][root][INFO] - Training Epoch: 2/2, step 18385/23838 completed (loss: 1.2305423021316528, acc: 0.641791045665741)
[2025-02-05 13:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:00][root][INFO] - Training Epoch: 2/2, step 18386/23838 completed (loss: 0.9484949707984924, acc: 0.692307710647583)
[2025-02-05 13:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:01][root][INFO] - Training Epoch: 2/2, step 18387/23838 completed (loss: 0.7777095437049866, acc: 0.7777777910232544)
[2025-02-05 13:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:01][root][INFO] - Training Epoch: 2/2, step 18388/23838 completed (loss: 1.3427796363830566, acc: 0.6388888955116272)
[2025-02-05 13:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:02][root][INFO] - Training Epoch: 2/2, step 18389/23838 completed (loss: 0.9865834712982178, acc: 0.6851851940155029)
[2025-02-05 13:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:02][root][INFO] - Training Epoch: 2/2, step 18390/23838 completed (loss: 1.4477717876434326, acc: 0.6166666746139526)
[2025-02-05 13:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:03][root][INFO] - Training Epoch: 2/2, step 18391/23838 completed (loss: 1.0254297256469727, acc: 0.7108433842658997)
[2025-02-05 13:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:03][root][INFO] - Training Epoch: 2/2, step 18392/23838 completed (loss: 0.8431695699691772, acc: 0.7857142686843872)
[2025-02-05 13:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:03][root][INFO] - Training Epoch: 2/2, step 18393/23838 completed (loss: 1.2999107837677002, acc: 0.7086614370346069)
[2025-02-05 13:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:04][root][INFO] - Training Epoch: 2/2, step 18394/23838 completed (loss: 1.0050764083862305, acc: 0.6973684430122375)
[2025-02-05 13:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:04][root][INFO] - Training Epoch: 2/2, step 18395/23838 completed (loss: 1.2716587781906128, acc: 0.656000018119812)
[2025-02-05 13:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:05][root][INFO] - Training Epoch: 2/2, step 18396/23838 completed (loss: 1.0144599676132202, acc: 0.6607142686843872)
[2025-02-05 13:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:05][root][INFO] - Training Epoch: 2/2, step 18397/23838 completed (loss: 0.9322847723960876, acc: 0.7530864477157593)
[2025-02-05 13:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:06][root][INFO] - Training Epoch: 2/2, step 18398/23838 completed (loss: 1.0683151483535767, acc: 0.6928104758262634)
[2025-02-05 13:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:06][root][INFO] - Training Epoch: 2/2, step 18399/23838 completed (loss: 1.0048612356185913, acc: 0.6666666865348816)
[2025-02-05 13:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:06][root][INFO] - Training Epoch: 2/2, step 18400/23838 completed (loss: 1.1719988584518433, acc: 0.6875)
[2025-02-05 13:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:07][root][INFO] - Training Epoch: 2/2, step 18401/23838 completed (loss: 1.0126780271530151, acc: 0.7196261882781982)
[2025-02-05 13:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:07][root][INFO] - Training Epoch: 2/2, step 18402/23838 completed (loss: 0.8264707922935486, acc: 0.7400000095367432)
[2025-02-05 13:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:08][root][INFO] - Training Epoch: 2/2, step 18403/23838 completed (loss: 0.8579607605934143, acc: 0.7678571343421936)
[2025-02-05 13:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:08][root][INFO] - Training Epoch: 2/2, step 18404/23838 completed (loss: 1.136236548423767, acc: 0.6614173054695129)
[2025-02-05 13:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:08][root][INFO] - Training Epoch: 2/2, step 18405/23838 completed (loss: 1.0467625856399536, acc: 0.7532467246055603)
[2025-02-05 13:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:09][root][INFO] - Training Epoch: 2/2, step 18406/23838 completed (loss: 0.9565544724464417, acc: 0.7333333492279053)
[2025-02-05 13:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:09][root][INFO] - Training Epoch: 2/2, step 18407/23838 completed (loss: 1.240228295326233, acc: 0.6071428656578064)
[2025-02-05 13:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:10][root][INFO] - Training Epoch: 2/2, step 18408/23838 completed (loss: 1.0338560342788696, acc: 0.7457627058029175)
[2025-02-05 13:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:10][root][INFO] - Training Epoch: 2/2, step 18409/23838 completed (loss: 1.0033224821090698, acc: 0.732758641242981)
[2025-02-05 13:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:10][root][INFO] - Training Epoch: 2/2, step 18410/23838 completed (loss: 1.1054331064224243, acc: 0.6938775777816772)
[2025-02-05 13:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:11][root][INFO] - Training Epoch: 2/2, step 18411/23838 completed (loss: 1.2853978872299194, acc: 0.6363636255264282)
[2025-02-05 13:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:11][root][INFO] - Training Epoch: 2/2, step 18412/23838 completed (loss: 1.1093778610229492, acc: 0.6879432797431946)
[2025-02-05 13:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:12][root][INFO] - Training Epoch: 2/2, step 18413/23838 completed (loss: 1.3584436178207397, acc: 0.6285714507102966)
[2025-02-05 13:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:12][root][INFO] - Training Epoch: 2/2, step 18414/23838 completed (loss: 0.9903566241264343, acc: 0.6976743936538696)
[2025-02-05 13:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:12][root][INFO] - Training Epoch: 2/2, step 18415/23838 completed (loss: 0.8637010455131531, acc: 0.7545454502105713)
[2025-02-05 13:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:13][root][INFO] - Training Epoch: 2/2, step 18416/23838 completed (loss: 1.0197192430496216, acc: 0.6891891956329346)
[2025-02-05 13:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:13][root][INFO] - Training Epoch: 2/2, step 18417/23838 completed (loss: 1.0108951330184937, acc: 0.746268630027771)
[2025-02-05 13:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:14][root][INFO] - Training Epoch: 2/2, step 18418/23838 completed (loss: 1.019487977027893, acc: 0.7627118825912476)
[2025-02-05 13:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:14][root][INFO] - Training Epoch: 2/2, step 18419/23838 completed (loss: 0.9258853793144226, acc: 0.7472527623176575)
[2025-02-05 13:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:15][root][INFO] - Training Epoch: 2/2, step 18420/23838 completed (loss: 0.8224629759788513, acc: 0.75)
[2025-02-05 13:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:15][root][INFO] - Training Epoch: 2/2, step 18421/23838 completed (loss: 0.9930109977722168, acc: 0.7142857313156128)
[2025-02-05 13:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:15][root][INFO] - Training Epoch: 2/2, step 18422/23838 completed (loss: 1.0189846754074097, acc: 0.7263157963752747)
[2025-02-05 13:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:16][root][INFO] - Training Epoch: 2/2, step 18423/23838 completed (loss: 1.0289101600646973, acc: 0.707317054271698)
[2025-02-05 13:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:16][root][INFO] - Training Epoch: 2/2, step 18424/23838 completed (loss: 1.342226505279541, acc: 0.6282051205635071)
[2025-02-05 13:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:17][root][INFO] - Training Epoch: 2/2, step 18425/23838 completed (loss: 1.0389564037322998, acc: 0.7250000238418579)
[2025-02-05 13:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:17][root][INFO] - Training Epoch: 2/2, step 18426/23838 completed (loss: 1.2238599061965942, acc: 0.6704545617103577)
[2025-02-05 13:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:17][root][INFO] - Training Epoch: 2/2, step 18427/23838 completed (loss: 1.169384241104126, acc: 0.75)
[2025-02-05 13:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:18][root][INFO] - Training Epoch: 2/2, step 18428/23838 completed (loss: 1.0662964582443237, acc: 0.7333333492279053)
[2025-02-05 13:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:18][root][INFO] - Training Epoch: 2/2, step 18429/23838 completed (loss: 0.6522859930992126, acc: 0.8070175647735596)
[2025-02-05 13:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:18][root][INFO] - Training Epoch: 2/2, step 18430/23838 completed (loss: 0.8835844397544861, acc: 0.7457627058029175)
[2025-02-05 13:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:19][root][INFO] - Training Epoch: 2/2, step 18431/23838 completed (loss: 0.8713038563728333, acc: 0.7432432174682617)
[2025-02-05 13:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:19][root][INFO] - Training Epoch: 2/2, step 18432/23838 completed (loss: 1.0553631782531738, acc: 0.7151898741722107)
[2025-02-05 13:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:20][root][INFO] - Training Epoch: 2/2, step 18433/23838 completed (loss: 0.953286349773407, acc: 0.71875)
[2025-02-05 13:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:20][root][INFO] - Training Epoch: 2/2, step 18434/23838 completed (loss: 1.0740232467651367, acc: 0.719298243522644)
[2025-02-05 13:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:20][root][INFO] - Training Epoch: 2/2, step 18435/23838 completed (loss: 1.0078704357147217, acc: 0.703125)
[2025-02-05 13:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:21][root][INFO] - Training Epoch: 2/2, step 18436/23838 completed (loss: 1.2729110717773438, acc: 0.6111111044883728)
[2025-02-05 13:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:21][root][INFO] - Training Epoch: 2/2, step 18437/23838 completed (loss: 1.2722333669662476, acc: 0.6712328791618347)
[2025-02-05 13:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:22][root][INFO] - Training Epoch: 2/2, step 18438/23838 completed (loss: 1.0180268287658691, acc: 0.719298243522644)
[2025-02-05 13:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:22][root][INFO] - Training Epoch: 2/2, step 18439/23838 completed (loss: 0.8150936365127563, acc: 0.7857142686843872)
[2025-02-05 13:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:22][root][INFO] - Training Epoch: 2/2, step 18440/23838 completed (loss: 0.8237579464912415, acc: 0.7567567825317383)
[2025-02-05 13:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:23][root][INFO] - Training Epoch: 2/2, step 18441/23838 completed (loss: 1.0882854461669922, acc: 0.7346938848495483)
[2025-02-05 13:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:23][root][INFO] - Training Epoch: 2/2, step 18442/23838 completed (loss: 1.1909375190734863, acc: 0.6851851940155029)
[2025-02-05 13:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:24][root][INFO] - Training Epoch: 2/2, step 18443/23838 completed (loss: 1.3287638425827026, acc: 0.581632673740387)
[2025-02-05 13:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:24][root][INFO] - Training Epoch: 2/2, step 18444/23838 completed (loss: 1.2937793731689453, acc: 0.653333306312561)
[2025-02-05 13:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:24][root][INFO] - Training Epoch: 2/2, step 18445/23838 completed (loss: 1.3335479497909546, acc: 0.6222222447395325)
[2025-02-05 13:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:25][root][INFO] - Training Epoch: 2/2, step 18446/23838 completed (loss: 0.9170952439308167, acc: 0.739130437374115)
[2025-02-05 13:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:25][root][INFO] - Training Epoch: 2/2, step 18447/23838 completed (loss: 0.9977260231971741, acc: 0.6808510422706604)
[2025-02-05 13:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:26][root][INFO] - Training Epoch: 2/2, step 18448/23838 completed (loss: 1.3121739625930786, acc: 0.6037735939025879)
[2025-02-05 13:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:26][root][INFO] - Training Epoch: 2/2, step 18449/23838 completed (loss: 1.3352118730545044, acc: 0.6000000238418579)
[2025-02-05 13:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:26][root][INFO] - Training Epoch: 2/2, step 18450/23838 completed (loss: 1.4113643169403076, acc: 0.5272727012634277)
[2025-02-05 13:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:27][root][INFO] - Training Epoch: 2/2, step 18451/23838 completed (loss: 0.9185670018196106, acc: 0.8148148059844971)
[2025-02-05 13:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:27][root][INFO] - Training Epoch: 2/2, step 18452/23838 completed (loss: 1.0588897466659546, acc: 0.7096773982048035)
[2025-02-05 13:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:28][root][INFO] - Training Epoch: 2/2, step 18453/23838 completed (loss: 1.1256394386291504, acc: 0.71875)
[2025-02-05 13:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:28][root][INFO] - Training Epoch: 2/2, step 18454/23838 completed (loss: 1.25921630859375, acc: 0.6000000238418579)
[2025-02-05 13:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:28][root][INFO] - Training Epoch: 2/2, step 18455/23838 completed (loss: 1.2538089752197266, acc: 0.6274510025978088)
[2025-02-05 13:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:29][root][INFO] - Training Epoch: 2/2, step 18456/23838 completed (loss: 1.1390471458435059, acc: 0.5862069129943848)
[2025-02-05 13:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:29][root][INFO] - Training Epoch: 2/2, step 18457/23838 completed (loss: 1.25473952293396, acc: 0.6388888955116272)
[2025-02-05 13:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:29][root][INFO] - Training Epoch: 2/2, step 18458/23838 completed (loss: 0.8447950482368469, acc: 0.675000011920929)
[2025-02-05 13:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:30][root][INFO] - Training Epoch: 2/2, step 18459/23838 completed (loss: 1.3274587392807007, acc: 0.5609756112098694)
[2025-02-05 13:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:30][root][INFO] - Training Epoch: 2/2, step 18460/23838 completed (loss: 1.1635380983352661, acc: 0.75)
[2025-02-05 13:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:30][root][INFO] - Training Epoch: 2/2, step 18461/23838 completed (loss: 1.078805923461914, acc: 0.6349206566810608)
[2025-02-05 13:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:31][root][INFO] - Training Epoch: 2/2, step 18462/23838 completed (loss: 1.1006015539169312, acc: 0.7090908885002136)
[2025-02-05 13:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:31][root][INFO] - Training Epoch: 2/2, step 18463/23838 completed (loss: 1.4311378002166748, acc: 0.609375)
[2025-02-05 13:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:32][root][INFO] - Training Epoch: 2/2, step 18464/23838 completed (loss: 0.8966248035430908, acc: 0.7096773982048035)
[2025-02-05 13:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:32][root][INFO] - Training Epoch: 2/2, step 18465/23838 completed (loss: 1.2844654321670532, acc: 0.6499999761581421)
[2025-02-05 13:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:33][root][INFO] - Training Epoch: 2/2, step 18466/23838 completed (loss: 1.1241482496261597, acc: 0.6506849527359009)
[2025-02-05 13:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:33][root][INFO] - Training Epoch: 2/2, step 18467/23838 completed (loss: 0.9208422899246216, acc: 0.7209302186965942)
[2025-02-05 13:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:33][root][INFO] - Training Epoch: 2/2, step 18468/23838 completed (loss: 1.4026076793670654, acc: 0.6052631735801697)
[2025-02-05 13:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:34][root][INFO] - Training Epoch: 2/2, step 18469/23838 completed (loss: 1.2831546068191528, acc: 0.6068376302719116)
[2025-02-05 13:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:34][root][INFO] - Training Epoch: 2/2, step 18470/23838 completed (loss: 1.1734226942062378, acc: 0.6704545617103577)
[2025-02-05 13:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:35][root][INFO] - Training Epoch: 2/2, step 18471/23838 completed (loss: 1.1993097066879272, acc: 0.6781609058380127)
[2025-02-05 13:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:35][root][INFO] - Training Epoch: 2/2, step 18472/23838 completed (loss: 1.1905982494354248, acc: 0.65625)
[2025-02-05 13:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:36][root][INFO] - Training Epoch: 2/2, step 18473/23838 completed (loss: 1.1680619716644287, acc: 0.6499999761581421)
[2025-02-05 13:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:36][root][INFO] - Training Epoch: 2/2, step 18474/23838 completed (loss: 1.3259774446487427, acc: 0.6333333253860474)
[2025-02-05 13:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:36][root][INFO] - Training Epoch: 2/2, step 18475/23838 completed (loss: 1.1220406293869019, acc: 0.6736842393875122)
[2025-02-05 13:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:37][root][INFO] - Training Epoch: 2/2, step 18476/23838 completed (loss: 1.0232234001159668, acc: 0.6875)
[2025-02-05 13:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:37][root][INFO] - Training Epoch: 2/2, step 18477/23838 completed (loss: 1.1076325178146362, acc: 0.6761904954910278)
[2025-02-05 13:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:37][root][INFO] - Training Epoch: 2/2, step 18478/23838 completed (loss: 0.8634009957313538, acc: 0.7441860437393188)
[2025-02-05 13:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:38][root][INFO] - Training Epoch: 2/2, step 18479/23838 completed (loss: 1.2641693353652954, acc: 0.6582278609275818)
[2025-02-05 13:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:38][root][INFO] - Training Epoch: 2/2, step 18480/23838 completed (loss: 1.2256921529769897, acc: 0.6647727489471436)
[2025-02-05 13:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:39][root][INFO] - Training Epoch: 2/2, step 18481/23838 completed (loss: 1.3076027631759644, acc: 0.625)
[2025-02-05 13:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:39][root][INFO] - Training Epoch: 2/2, step 18482/23838 completed (loss: 0.8944427371025085, acc: 0.7111111283302307)
[2025-02-05 13:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:40][root][INFO] - Training Epoch: 2/2, step 18483/23838 completed (loss: 1.199051856994629, acc: 0.6339285969734192)
[2025-02-05 13:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:40][root][INFO] - Training Epoch: 2/2, step 18484/23838 completed (loss: 1.2688102722167969, acc: 0.6321839094161987)
[2025-02-05 13:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:40][root][INFO] - Training Epoch: 2/2, step 18485/23838 completed (loss: 1.1537765264511108, acc: 0.643410861492157)
[2025-02-05 13:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:41][root][INFO] - Training Epoch: 2/2, step 18486/23838 completed (loss: 1.0105617046356201, acc: 0.7142857313156128)
[2025-02-05 13:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:41][root][INFO] - Training Epoch: 2/2, step 18487/23838 completed (loss: 0.6318570971488953, acc: 0.7924528121948242)
[2025-02-05 13:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:41][root][INFO] - Training Epoch: 2/2, step 18488/23838 completed (loss: 1.0181710720062256, acc: 0.6941176652908325)
[2025-02-05 13:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:42][root][INFO] - Training Epoch: 2/2, step 18489/23838 completed (loss: 1.2604726552963257, acc: 0.658823549747467)
[2025-02-05 13:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:42][root][INFO] - Training Epoch: 2/2, step 18490/23838 completed (loss: 1.3895857334136963, acc: 0.6068376302719116)
[2025-02-05 13:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:43][root][INFO] - Training Epoch: 2/2, step 18491/23838 completed (loss: 0.9803234338760376, acc: 0.6818181872367859)
[2025-02-05 13:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:43][root][INFO] - Training Epoch: 2/2, step 18492/23838 completed (loss: 0.9687397480010986, acc: 0.7368420958518982)
[2025-02-05 13:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:44][root][INFO] - Training Epoch: 2/2, step 18493/23838 completed (loss: 1.330411672592163, acc: 0.6000000238418579)
[2025-02-05 13:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:44][root][INFO] - Training Epoch: 2/2, step 18494/23838 completed (loss: 1.3257743120193481, acc: 0.6399999856948853)
[2025-02-05 13:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:44][root][INFO] - Training Epoch: 2/2, step 18495/23838 completed (loss: 1.246672511100769, acc: 0.6770833134651184)
[2025-02-05 13:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:45][root][INFO] - Training Epoch: 2/2, step 18496/23838 completed (loss: 0.9696552753448486, acc: 0.6808510422706604)
[2025-02-05 13:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:45][root][INFO] - Training Epoch: 2/2, step 18497/23838 completed (loss: 1.197383999824524, acc: 0.6727272868156433)
[2025-02-05 13:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:46][root][INFO] - Training Epoch: 2/2, step 18498/23838 completed (loss: 1.1860034465789795, acc: 0.6959459185600281)
[2025-02-05 13:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:46][root][INFO] - Training Epoch: 2/2, step 18499/23838 completed (loss: 0.9195376038551331, acc: 0.7755101919174194)
[2025-02-05 13:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:47][root][INFO] - Training Epoch: 2/2, step 18500/23838 completed (loss: 1.2134473323822021, acc: 0.7169811129570007)
[2025-02-05 13:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:47][root][INFO] - Training Epoch: 2/2, step 18501/23838 completed (loss: 1.134191870689392, acc: 0.6744186282157898)
[2025-02-05 13:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:47][root][INFO] - Training Epoch: 2/2, step 18502/23838 completed (loss: 1.1128339767456055, acc: 0.6811594367027283)
[2025-02-05 13:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:48][root][INFO] - Training Epoch: 2/2, step 18503/23838 completed (loss: 1.2556177377700806, acc: 0.7021276354789734)
[2025-02-05 13:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:48][root][INFO] - Training Epoch: 2/2, step 18504/23838 completed (loss: 1.3981879949569702, acc: 0.5753424763679504)
[2025-02-05 13:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:48][root][INFO] - Training Epoch: 2/2, step 18505/23838 completed (loss: 1.6098228693008423, acc: 0.5087719559669495)
[2025-02-05 13:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:49][root][INFO] - Training Epoch: 2/2, step 18506/23838 completed (loss: 0.7831928730010986, acc: 0.7058823704719543)
[2025-02-05 13:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:49][root][INFO] - Training Epoch: 2/2, step 18507/23838 completed (loss: 0.9069454669952393, acc: 0.699999988079071)
[2025-02-05 13:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:50][root][INFO] - Training Epoch: 2/2, step 18508/23838 completed (loss: 0.7922497987747192, acc: 0.8058252334594727)
[2025-02-05 13:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:50][root][INFO] - Training Epoch: 2/2, step 18509/23838 completed (loss: 0.8176792860031128, acc: 0.7543859481811523)
[2025-02-05 13:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:50][root][INFO] - Training Epoch: 2/2, step 18510/23838 completed (loss: 0.8211848139762878, acc: 0.7575757503509521)
[2025-02-05 13:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:51][root][INFO] - Training Epoch: 2/2, step 18511/23838 completed (loss: 1.291437029838562, acc: 0.6349206566810608)
[2025-02-05 13:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:51][root][INFO] - Training Epoch: 2/2, step 18512/23838 completed (loss: 0.9023904204368591, acc: 0.7272727489471436)
[2025-02-05 13:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:52][root][INFO] - Training Epoch: 2/2, step 18513/23838 completed (loss: 0.8205062747001648, acc: 0.695652186870575)
[2025-02-05 13:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:52][root][INFO] - Training Epoch: 2/2, step 18514/23838 completed (loss: 1.001954436302185, acc: 0.6987951993942261)
[2025-02-05 13:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:52][root][INFO] - Training Epoch: 2/2, step 18515/23838 completed (loss: 0.7092466950416565, acc: 0.7808219194412231)
[2025-02-05 13:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:53][root][INFO] - Training Epoch: 2/2, step 18516/23838 completed (loss: 0.8897338509559631, acc: 0.7428571581840515)
[2025-02-05 13:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:53][root][INFO] - Training Epoch: 2/2, step 18517/23838 completed (loss: 1.0804431438446045, acc: 0.6338028311729431)
[2025-02-05 13:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:54][root][INFO] - Training Epoch: 2/2, step 18518/23838 completed (loss: 0.93181973695755, acc: 0.7049180269241333)
[2025-02-05 13:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:54][root][INFO] - Training Epoch: 2/2, step 18519/23838 completed (loss: 1.1871826648712158, acc: 0.6837607026100159)
[2025-02-05 13:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:54][root][INFO] - Training Epoch: 2/2, step 18520/23838 completed (loss: 1.15127432346344, acc: 0.6896551847457886)
[2025-02-05 13:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:55][root][INFO] - Training Epoch: 2/2, step 18521/23838 completed (loss: 0.8418086171150208, acc: 0.7142857313156128)
[2025-02-05 13:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:55][root][INFO] - Training Epoch: 2/2, step 18522/23838 completed (loss: 1.1181696653366089, acc: 0.6492537260055542)
[2025-02-05 13:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:56][root][INFO] - Training Epoch: 2/2, step 18523/23838 completed (loss: 1.024877905845642, acc: 0.7066666483879089)
[2025-02-05 13:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:56][root][INFO] - Training Epoch: 2/2, step 18524/23838 completed (loss: 1.1944833993911743, acc: 0.6034482717514038)
[2025-02-05 13:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:56][root][INFO] - Training Epoch: 2/2, step 18525/23838 completed (loss: 1.1840068101882935, acc: 0.6168830990791321)
[2025-02-05 13:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:57][root][INFO] - Training Epoch: 2/2, step 18526/23838 completed (loss: 1.0555976629257202, acc: 0.7142857313156128)
[2025-02-05 13:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:57][root][INFO] - Training Epoch: 2/2, step 18527/23838 completed (loss: 1.4465504884719849, acc: 0.6090225577354431)
[2025-02-05 13:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:58][root][INFO] - Training Epoch: 2/2, step 18528/23838 completed (loss: 1.0946149826049805, acc: 0.6875)
[2025-02-05 13:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:58][root][INFO] - Training Epoch: 2/2, step 18529/23838 completed (loss: 1.058670163154602, acc: 0.7209302186965942)
[2025-02-05 13:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:58][root][INFO] - Training Epoch: 2/2, step 18530/23838 completed (loss: 0.8566148281097412, acc: 0.7438016533851624)
[2025-02-05 13:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:59][root][INFO] - Training Epoch: 2/2, step 18531/23838 completed (loss: 0.5630349516868591, acc: 0.8421052694320679)
[2025-02-05 13:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:43:59][root][INFO] - Training Epoch: 2/2, step 18532/23838 completed (loss: 1.0780199766159058, acc: 0.6704545617103577)
[2025-02-05 13:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:00][root][INFO] - Training Epoch: 2/2, step 18533/23838 completed (loss: 0.9478640556335449, acc: 0.7358490824699402)
[2025-02-05 13:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:00][root][INFO] - Training Epoch: 2/2, step 18534/23838 completed (loss: 0.9520269632339478, acc: 0.692307710647583)
[2025-02-05 13:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:00][root][INFO] - Training Epoch: 2/2, step 18535/23838 completed (loss: 1.067541241645813, acc: 0.675000011920929)
[2025-02-05 13:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:01][root][INFO] - Training Epoch: 2/2, step 18536/23838 completed (loss: 1.1380259990692139, acc: 0.650943398475647)
[2025-02-05 13:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:01][root][INFO] - Training Epoch: 2/2, step 18537/23838 completed (loss: 1.2549723386764526, acc: 0.6373626589775085)
[2025-02-05 13:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:02][root][INFO] - Training Epoch: 2/2, step 18538/23838 completed (loss: 1.097249984741211, acc: 0.6494252681732178)
[2025-02-05 13:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:02][root][INFO] - Training Epoch: 2/2, step 18539/23838 completed (loss: 1.223753809928894, acc: 0.625)
[2025-02-05 13:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:02][root][INFO] - Training Epoch: 2/2, step 18540/23838 completed (loss: 1.228087306022644, acc: 0.6319018602371216)
[2025-02-05 13:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:03][root][INFO] - Training Epoch: 2/2, step 18541/23838 completed (loss: 0.9371687173843384, acc: 0.7207207083702087)
[2025-02-05 13:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:03][root][INFO] - Training Epoch: 2/2, step 18542/23838 completed (loss: 0.989804744720459, acc: 0.7818182110786438)
[2025-02-05 13:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:04][root][INFO] - Training Epoch: 2/2, step 18543/23838 completed (loss: 1.0932594537734985, acc: 0.6470588445663452)
[2025-02-05 13:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:04][root][INFO] - Training Epoch: 2/2, step 18544/23838 completed (loss: 1.1829159259796143, acc: 0.6466666460037231)
[2025-02-05 13:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:04][root][INFO] - Training Epoch: 2/2, step 18545/23838 completed (loss: 0.8077290654182434, acc: 0.7352941036224365)
[2025-02-05 13:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:05][root][INFO] - Training Epoch: 2/2, step 18546/23838 completed (loss: 1.1621588468551636, acc: 0.6700000166893005)
[2025-02-05 13:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:05][root][INFO] - Training Epoch: 2/2, step 18547/23838 completed (loss: 0.9307078123092651, acc: 0.7129629850387573)
[2025-02-05 13:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:05][root][INFO] - Training Epoch: 2/2, step 18548/23838 completed (loss: 1.0517687797546387, acc: 0.6893203854560852)
[2025-02-05 13:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:06][root][INFO] - Training Epoch: 2/2, step 18549/23838 completed (loss: 0.9612971544265747, acc: 0.717391312122345)
[2025-02-05 13:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:06][root][INFO] - Training Epoch: 2/2, step 18550/23838 completed (loss: 1.1894348859786987, acc: 0.6725663542747498)
[2025-02-05 13:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:07][root][INFO] - Training Epoch: 2/2, step 18551/23838 completed (loss: 0.6416263580322266, acc: 0.800000011920929)
[2025-02-05 13:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:07][root][INFO] - Training Epoch: 2/2, step 18552/23838 completed (loss: 1.2988704442977905, acc: 0.603960394859314)
[2025-02-05 13:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:07][root][INFO] - Training Epoch: 2/2, step 18553/23838 completed (loss: 0.9484662413597107, acc: 0.739130437374115)
[2025-02-05 13:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:08][root][INFO] - Training Epoch: 2/2, step 18554/23838 completed (loss: 0.8452298641204834, acc: 0.7230769395828247)
[2025-02-05 13:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:08][root][INFO] - Training Epoch: 2/2, step 18555/23838 completed (loss: 0.8473080396652222, acc: 0.7799999713897705)
[2025-02-05 13:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:08][root][INFO] - Training Epoch: 2/2, step 18556/23838 completed (loss: 1.178778886795044, acc: 0.6605504751205444)
[2025-02-05 13:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:09][root][INFO] - Training Epoch: 2/2, step 18557/23838 completed (loss: 1.4009020328521729, acc: 0.6136363744735718)
[2025-02-05 13:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:09][root][INFO] - Training Epoch: 2/2, step 18558/23838 completed (loss: 1.0383410453796387, acc: 0.7266187071800232)
[2025-02-05 13:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:10][root][INFO] - Training Epoch: 2/2, step 18559/23838 completed (loss: 1.2389203310012817, acc: 0.6342856884002686)
[2025-02-05 13:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:10][root][INFO] - Training Epoch: 2/2, step 18560/23838 completed (loss: 1.285564661026001, acc: 0.6404494643211365)
[2025-02-05 13:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:11][root][INFO] - Training Epoch: 2/2, step 18561/23838 completed (loss: 1.179657220840454, acc: 0.6699029207229614)
[2025-02-05 13:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:11][root][INFO] - Training Epoch: 2/2, step 18562/23838 completed (loss: 1.1473666429519653, acc: 0.6535432934761047)
[2025-02-05 13:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:11][root][INFO] - Training Epoch: 2/2, step 18563/23838 completed (loss: 1.0493998527526855, acc: 0.7413793206214905)
[2025-02-05 13:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:12][root][INFO] - Training Epoch: 2/2, step 18564/23838 completed (loss: 0.7133537530899048, acc: 0.7837837934494019)
[2025-02-05 13:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:12][root][INFO] - Training Epoch: 2/2, step 18565/23838 completed (loss: 1.174954891204834, acc: 0.6181818246841431)
[2025-02-05 13:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:13][root][INFO] - Training Epoch: 2/2, step 18566/23838 completed (loss: 1.2171242237091064, acc: 0.6212121248245239)
[2025-02-05 13:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:13][root][INFO] - Training Epoch: 2/2, step 18567/23838 completed (loss: 1.3270233869552612, acc: 0.6276595592498779)
[2025-02-05 13:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:13][root][INFO] - Training Epoch: 2/2, step 18568/23838 completed (loss: 0.7265360355377197, acc: 0.8199999928474426)
[2025-02-05 13:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:14][root][INFO] - Training Epoch: 2/2, step 18569/23838 completed (loss: 0.9788815975189209, acc: 0.699999988079071)
[2025-02-05 13:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:14][root][INFO] - Training Epoch: 2/2, step 18570/23838 completed (loss: 1.146456003189087, acc: 0.7227723002433777)
[2025-02-05 13:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:14][root][INFO] - Training Epoch: 2/2, step 18571/23838 completed (loss: 1.2409762144088745, acc: 0.653333306312561)
[2025-02-05 13:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:15][root][INFO] - Training Epoch: 2/2, step 18572/23838 completed (loss: 1.0847080945968628, acc: 0.7272727489471436)
[2025-02-05 13:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:15][root][INFO] - Training Epoch: 2/2, step 18573/23838 completed (loss: 0.9470870494842529, acc: 0.7014925479888916)
[2025-02-05 13:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:16][root][INFO] - Training Epoch: 2/2, step 18574/23838 completed (loss: 1.0390828847885132, acc: 0.7068965435028076)
[2025-02-05 13:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:16][root][INFO] - Training Epoch: 2/2, step 18575/23838 completed (loss: 1.1271286010742188, acc: 0.6947368383407593)
[2025-02-05 13:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:16][root][INFO] - Training Epoch: 2/2, step 18576/23838 completed (loss: 1.179581642150879, acc: 0.6767676472663879)
[2025-02-05 13:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:17][root][INFO] - Training Epoch: 2/2, step 18577/23838 completed (loss: 0.7122998237609863, acc: 0.8405796885490417)
[2025-02-05 13:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:17][root][INFO] - Training Epoch: 2/2, step 18578/23838 completed (loss: 1.1053684949874878, acc: 0.6213592290878296)
[2025-02-05 13:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:18][root][INFO] - Training Epoch: 2/2, step 18579/23838 completed (loss: 0.8086097240447998, acc: 0.6875)
[2025-02-05 13:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:18][root][INFO] - Training Epoch: 2/2, step 18580/23838 completed (loss: 0.7366053462028503, acc: 0.7543859481811523)
[2025-02-05 13:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:18][root][INFO] - Training Epoch: 2/2, step 18581/23838 completed (loss: 1.004847764968872, acc: 0.7441860437393188)
[2025-02-05 13:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:19][root][INFO] - Training Epoch: 2/2, step 18582/23838 completed (loss: 1.0745528936386108, acc: 0.7301587462425232)
[2025-02-05 13:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:19][root][INFO] - Training Epoch: 2/2, step 18583/23838 completed (loss: 1.2067328691482544, acc: 0.6944444179534912)
[2025-02-05 13:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:20][root][INFO] - Training Epoch: 2/2, step 18584/23838 completed (loss: 1.2881559133529663, acc: 0.6725663542747498)
[2025-02-05 13:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:20][root][INFO] - Training Epoch: 2/2, step 18585/23838 completed (loss: 1.1559553146362305, acc: 0.6811594367027283)
[2025-02-05 13:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:20][root][INFO] - Training Epoch: 2/2, step 18586/23838 completed (loss: 1.0684677362442017, acc: 0.6865671873092651)
[2025-02-05 13:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:21][root][INFO] - Training Epoch: 2/2, step 18587/23838 completed (loss: 1.004908561706543, acc: 0.7291666865348816)
[2025-02-05 13:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:21][root][INFO] - Training Epoch: 2/2, step 18588/23838 completed (loss: 1.043771743774414, acc: 0.6111111044883728)
[2025-02-05 13:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:21][root][INFO] - Training Epoch: 2/2, step 18589/23838 completed (loss: 1.0124483108520508, acc: 0.6597937941551208)
[2025-02-05 13:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:22][root][INFO] - Training Epoch: 2/2, step 18590/23838 completed (loss: 0.7119479775428772, acc: 0.8108108043670654)
[2025-02-05 13:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:22][root][INFO] - Training Epoch: 2/2, step 18591/23838 completed (loss: 0.9961957931518555, acc: 0.644859790802002)
[2025-02-05 13:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:23][root][INFO] - Training Epoch: 2/2, step 18592/23838 completed (loss: 1.039259433746338, acc: 0.699999988079071)
[2025-02-05 13:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:23][root][INFO] - Training Epoch: 2/2, step 18593/23838 completed (loss: 0.8466259837150574, acc: 0.761904776096344)
[2025-02-05 13:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:24][root][INFO] - Training Epoch: 2/2, step 18594/23838 completed (loss: 0.6122704148292542, acc: 0.8709677457809448)
[2025-02-05 13:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:24][root][INFO] - Training Epoch: 2/2, step 18595/23838 completed (loss: 0.8352327942848206, acc: 0.7636363506317139)
[2025-02-05 13:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:25][root][INFO] - Training Epoch: 2/2, step 18596/23838 completed (loss: 0.8834547996520996, acc: 0.75)
[2025-02-05 13:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:25][root][INFO] - Training Epoch: 2/2, step 18597/23838 completed (loss: 0.7607728838920593, acc: 0.8181818127632141)
[2025-02-05 13:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:25][root][INFO] - Training Epoch: 2/2, step 18598/23838 completed (loss: 0.8050440549850464, acc: 0.7848101258277893)
[2025-02-05 13:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:26][root][INFO] - Training Epoch: 2/2, step 18599/23838 completed (loss: 0.7195292115211487, acc: 0.7692307829856873)
[2025-02-05 13:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:26][root][INFO] - Training Epoch: 2/2, step 18600/23838 completed (loss: 1.102370262145996, acc: 0.6351351141929626)
[2025-02-05 13:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:27][root][INFO] - Training Epoch: 2/2, step 18601/23838 completed (loss: 0.9934741258621216, acc: 0.6744186282157898)
[2025-02-05 13:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:27][root][INFO] - Training Epoch: 2/2, step 18602/23838 completed (loss: 0.9019854068756104, acc: 0.728723406791687)
[2025-02-05 13:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:28][root][INFO] - Training Epoch: 2/2, step 18603/23838 completed (loss: 1.1011815071105957, acc: 0.695364236831665)
[2025-02-05 13:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:28][root][INFO] - Training Epoch: 2/2, step 18604/23838 completed (loss: 0.664730429649353, acc: 0.804347813129425)
[2025-02-05 13:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:28][root][INFO] - Training Epoch: 2/2, step 18605/23838 completed (loss: 1.4258010387420654, acc: 0.6481481194496155)
[2025-02-05 13:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:29][root][INFO] - Training Epoch: 2/2, step 18606/23838 completed (loss: 1.1231589317321777, acc: 0.6309523582458496)
[2025-02-05 13:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:29][root][INFO] - Training Epoch: 2/2, step 18607/23838 completed (loss: 1.1555522680282593, acc: 0.6198347210884094)
[2025-02-05 13:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:30][root][INFO] - Training Epoch: 2/2, step 18608/23838 completed (loss: 1.3164772987365723, acc: 0.6530612111091614)
[2025-02-05 13:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:30][root][INFO] - Training Epoch: 2/2, step 18609/23838 completed (loss: 1.8052009344100952, acc: 0.5070422291755676)
[2025-02-05 13:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:31][root][INFO] - Training Epoch: 2/2, step 18610/23838 completed (loss: 1.4574713706970215, acc: 0.5384615659713745)
[2025-02-05 13:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:31][root][INFO] - Training Epoch: 2/2, step 18611/23838 completed (loss: 1.2793467044830322, acc: 0.6326530575752258)
[2025-02-05 13:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:32][root][INFO] - Training Epoch: 2/2, step 18612/23838 completed (loss: 0.7399101853370667, acc: 0.7954545617103577)
[2025-02-05 13:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:32][root][INFO] - Training Epoch: 2/2, step 18613/23838 completed (loss: 1.1249051094055176, acc: 0.6666666865348816)
[2025-02-05 13:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:33][root][INFO] - Training Epoch: 2/2, step 18614/23838 completed (loss: 1.1732017993927002, acc: 0.6605504751205444)
[2025-02-05 13:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:33][root][INFO] - Training Epoch: 2/2, step 18615/23838 completed (loss: 1.071618914604187, acc: 0.6875)
[2025-02-05 13:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:34][root][INFO] - Training Epoch: 2/2, step 18616/23838 completed (loss: 0.8665250539779663, acc: 0.7613636255264282)
[2025-02-05 13:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:34][root][INFO] - Training Epoch: 2/2, step 18617/23838 completed (loss: 1.2405511140823364, acc: 0.6666666865348816)
[2025-02-05 13:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:34][root][INFO] - Training Epoch: 2/2, step 18618/23838 completed (loss: 0.9564359188079834, acc: 0.699999988079071)
[2025-02-05 13:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:35][root][INFO] - Training Epoch: 2/2, step 18619/23838 completed (loss: 1.0872336626052856, acc: 0.7215189933776855)
[2025-02-05 13:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:35][root][INFO] - Training Epoch: 2/2, step 18620/23838 completed (loss: 1.0314987897872925, acc: 0.7079645991325378)
[2025-02-05 13:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:36][root][INFO] - Training Epoch: 2/2, step 18621/23838 completed (loss: 1.2757493257522583, acc: 0.6666666865348816)
[2025-02-05 13:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:36][root][INFO] - Training Epoch: 2/2, step 18622/23838 completed (loss: 1.2251224517822266, acc: 0.672897219657898)
[2025-02-05 13:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:37][root][INFO] - Training Epoch: 2/2, step 18623/23838 completed (loss: 1.2031329870224, acc: 0.6449275612831116)
[2025-02-05 13:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:37][root][INFO] - Training Epoch: 2/2, step 18624/23838 completed (loss: 1.3361542224884033, acc: 0.6279069781303406)
[2025-02-05 13:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:38][root][INFO] - Training Epoch: 2/2, step 18625/23838 completed (loss: 1.2517598867416382, acc: 0.607692301273346)
[2025-02-05 13:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:38][root][INFO] - Training Epoch: 2/2, step 18626/23838 completed (loss: 0.9782825112342834, acc: 0.7356321811676025)
[2025-02-05 13:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:39][root][INFO] - Training Epoch: 2/2, step 18627/23838 completed (loss: 0.9452937245368958, acc: 0.6594203114509583)
[2025-02-05 13:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:40][root][INFO] - Training Epoch: 2/2, step 18628/23838 completed (loss: 1.1559911966323853, acc: 0.6410256624221802)
[2025-02-05 13:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:40][root][INFO] - Training Epoch: 2/2, step 18629/23838 completed (loss: 1.2641441822052002, acc: 0.5564516186714172)
[2025-02-05 13:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:41][root][INFO] - Training Epoch: 2/2, step 18630/23838 completed (loss: 1.289928674697876, acc: 0.5809524059295654)
[2025-02-05 13:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:41][root][INFO] - Training Epoch: 2/2, step 18631/23838 completed (loss: 1.2756452560424805, acc: 0.6288659572601318)
[2025-02-05 13:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:42][root][INFO] - Training Epoch: 2/2, step 18632/23838 completed (loss: 0.8511311411857605, acc: 0.7676056623458862)
[2025-02-05 13:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:42][root][INFO] - Training Epoch: 2/2, step 18633/23838 completed (loss: 1.0017406940460205, acc: 0.7124999761581421)
[2025-02-05 13:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:42][root][INFO] - Training Epoch: 2/2, step 18634/23838 completed (loss: 1.6561393737792969, acc: 0.5154638886451721)
[2025-02-05 13:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:43][root][INFO] - Training Epoch: 2/2, step 18635/23838 completed (loss: 1.1246215105056763, acc: 0.6666666865348816)
[2025-02-05 13:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:43][root][INFO] - Training Epoch: 2/2, step 18636/23838 completed (loss: 1.1006964445114136, acc: 0.6666666865348816)
[2025-02-05 13:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:44][root][INFO] - Training Epoch: 2/2, step 18637/23838 completed (loss: 0.7145853638648987, acc: 0.7922077775001526)
[2025-02-05 13:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:44][root][INFO] - Training Epoch: 2/2, step 18638/23838 completed (loss: 1.035616159439087, acc: 0.7142857313156128)
[2025-02-05 13:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:45][root][INFO] - Training Epoch: 2/2, step 18639/23838 completed (loss: 0.9588304162025452, acc: 0.7272727489471436)
[2025-02-05 13:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:45][root][INFO] - Training Epoch: 2/2, step 18640/23838 completed (loss: 1.1536506414413452, acc: 0.607594907283783)
[2025-02-05 13:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:46][root][INFO] - Training Epoch: 2/2, step 18641/23838 completed (loss: 1.046702265739441, acc: 0.7115384340286255)
[2025-02-05 13:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:46][root][INFO] - Training Epoch: 2/2, step 18642/23838 completed (loss: 0.9970489740371704, acc: 0.7373737096786499)
[2025-02-05 13:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:47][root][INFO] - Training Epoch: 2/2, step 18643/23838 completed (loss: 0.9896872043609619, acc: 0.6790123581886292)
[2025-02-05 13:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:47][root][INFO] - Training Epoch: 2/2, step 18644/23838 completed (loss: 1.5097299814224243, acc: 0.5454545617103577)
[2025-02-05 13:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:48][root][INFO] - Training Epoch: 2/2, step 18645/23838 completed (loss: 0.8698172569274902, acc: 0.7413793206214905)
[2025-02-05 13:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:48][root][INFO] - Training Epoch: 2/2, step 18646/23838 completed (loss: 1.0975463390350342, acc: 0.6438356041908264)
[2025-02-05 13:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:48][root][INFO] - Training Epoch: 2/2, step 18647/23838 completed (loss: 0.8519056439399719, acc: 0.7083333134651184)
[2025-02-05 13:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:49][root][INFO] - Training Epoch: 2/2, step 18648/23838 completed (loss: 1.0573534965515137, acc: 0.7037037014961243)
[2025-02-05 13:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:49][root][INFO] - Training Epoch: 2/2, step 18649/23838 completed (loss: 1.0665301084518433, acc: 0.6451612710952759)
[2025-02-05 13:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:50][root][INFO] - Training Epoch: 2/2, step 18650/23838 completed (loss: 0.8066867589950562, acc: 0.7941176295280457)
[2025-02-05 13:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:50][root][INFO] - Training Epoch: 2/2, step 18651/23838 completed (loss: 1.1285344362258911, acc: 0.6607142686843872)
[2025-02-05 13:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:50][root][INFO] - Training Epoch: 2/2, step 18652/23838 completed (loss: 1.0532745122909546, acc: 0.7047619223594666)
[2025-02-05 13:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:51][root][INFO] - Training Epoch: 2/2, step 18653/23838 completed (loss: 1.2224069833755493, acc: 0.6129032373428345)
[2025-02-05 13:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:51][root][INFO] - Training Epoch: 2/2, step 18654/23838 completed (loss: 0.7412399053573608, acc: 0.7692307829856873)
[2025-02-05 13:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:52][root][INFO] - Training Epoch: 2/2, step 18655/23838 completed (loss: 0.5957197546958923, acc: 0.7916666865348816)
[2025-02-05 13:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:52][root][INFO] - Training Epoch: 2/2, step 18656/23838 completed (loss: 0.9923656582832336, acc: 0.6808510422706604)
[2025-02-05 13:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:53][root][INFO] - Training Epoch: 2/2, step 18657/23838 completed (loss: 0.6439969539642334, acc: 0.8518518805503845)
[2025-02-05 13:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:53][root][INFO] - Training Epoch: 2/2, step 18658/23838 completed (loss: 0.8533638119697571, acc: 0.7037037014961243)
[2025-02-05 13:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:53][root][INFO] - Training Epoch: 2/2, step 18659/23838 completed (loss: 1.1709901094436646, acc: 0.686274528503418)
[2025-02-05 13:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:54][root][INFO] - Training Epoch: 2/2, step 18660/23838 completed (loss: 1.1421302556991577, acc: 0.7142857313156128)
[2025-02-05 13:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:54][root][INFO] - Training Epoch: 2/2, step 18661/23838 completed (loss: 1.132718563079834, acc: 0.6136363744735718)
[2025-02-05 13:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:55][root][INFO] - Training Epoch: 2/2, step 18662/23838 completed (loss: 0.9474843144416809, acc: 0.6388888955116272)
[2025-02-05 13:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:55][root][INFO] - Training Epoch: 2/2, step 18663/23838 completed (loss: 1.0188167095184326, acc: 0.6727272868156433)
[2025-02-05 13:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:55][root][INFO] - Training Epoch: 2/2, step 18664/23838 completed (loss: 1.3155763149261475, acc: 0.5714285969734192)
[2025-02-05 13:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:56][root][INFO] - Training Epoch: 2/2, step 18665/23838 completed (loss: 0.4469940960407257, acc: 0.7272727489471436)
[2025-02-05 13:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:56][root][INFO] - Training Epoch: 2/2, step 18666/23838 completed (loss: 0.2968791425228119, acc: 0.8333333134651184)
[2025-02-05 13:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:56][root][INFO] - Training Epoch: 2/2, step 18667/23838 completed (loss: 0.606573760509491, acc: 0.7666666507720947)
[2025-02-05 13:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:57][root][INFO] - Training Epoch: 2/2, step 18668/23838 completed (loss: 1.1386150121688843, acc: 0.7333333492279053)
[2025-02-05 13:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:57][root][INFO] - Training Epoch: 2/2, step 18669/23838 completed (loss: 0.6245837211608887, acc: 0.8064516186714172)
[2025-02-05 13:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:58][root][INFO] - Training Epoch: 2/2, step 18670/23838 completed (loss: 0.2704829275608063, acc: 0.9473684430122375)
[2025-02-05 13:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:58][root][INFO] - Training Epoch: 2/2, step 18671/23838 completed (loss: 0.9381654858589172, acc: 0.7045454382896423)
[2025-02-05 13:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:58][root][INFO] - Training Epoch: 2/2, step 18672/23838 completed (loss: 0.9485944509506226, acc: 0.6499999761581421)
[2025-02-05 13:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:59][root][INFO] - Training Epoch: 2/2, step 18673/23838 completed (loss: 0.7443608045578003, acc: 0.8333333134651184)
[2025-02-05 13:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:44:59][root][INFO] - Training Epoch: 2/2, step 18674/23838 completed (loss: 1.1552540063858032, acc: 0.692307710647583)
[2025-02-05 13:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:00][root][INFO] - Training Epoch: 2/2, step 18675/23838 completed (loss: 0.6776221394538879, acc: 0.8421052694320679)
[2025-02-05 13:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:00][root][INFO] - Training Epoch: 2/2, step 18676/23838 completed (loss: 1.2499034404754639, acc: 0.6551724076271057)
[2025-02-05 13:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:00][root][INFO] - Training Epoch: 2/2, step 18677/23838 completed (loss: 0.9638282060623169, acc: 0.695652186870575)
[2025-02-05 13:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:01][root][INFO] - Training Epoch: 2/2, step 18678/23838 completed (loss: 1.1624771356582642, acc: 0.6153846383094788)
[2025-02-05 13:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:01][root][INFO] - Training Epoch: 2/2, step 18679/23838 completed (loss: 0.7177368998527527, acc: 0.8529411554336548)
[2025-02-05 13:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:01][root][INFO] - Training Epoch: 2/2, step 18680/23838 completed (loss: 0.6955150365829468, acc: 0.75)
[2025-02-05 13:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:02][root][INFO] - Training Epoch: 2/2, step 18681/23838 completed (loss: 0.8338669538497925, acc: 0.8260869383811951)
[2025-02-05 13:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:02][root][INFO] - Training Epoch: 2/2, step 18682/23838 completed (loss: 0.9007292985916138, acc: 0.8333333134651184)
[2025-02-05 13:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:02][root][INFO] - Training Epoch: 2/2, step 18683/23838 completed (loss: 1.3420000076293945, acc: 0.6000000238418579)
[2025-02-05 13:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:03][root][INFO] - Training Epoch: 2/2, step 18684/23838 completed (loss: 1.1359386444091797, acc: 0.6197183132171631)
[2025-02-05 13:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:03][root][INFO] - Training Epoch: 2/2, step 18685/23838 completed (loss: 1.4055657386779785, acc: 0.6233766078948975)
[2025-02-05 13:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:04][root][INFO] - Training Epoch: 2/2, step 18686/23838 completed (loss: 1.0209742784500122, acc: 0.7083333134651184)
[2025-02-05 13:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:04][root][INFO] - Training Epoch: 2/2, step 18687/23838 completed (loss: 1.1187951564788818, acc: 0.6666666865348816)
[2025-02-05 13:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:04][root][INFO] - Training Epoch: 2/2, step 18688/23838 completed (loss: 1.2210267782211304, acc: 0.6393442749977112)
[2025-02-05 13:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:05][root][INFO] - Training Epoch: 2/2, step 18689/23838 completed (loss: 1.5455069541931152, acc: 0.5270270109176636)
[2025-02-05 13:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:05][root][INFO] - Training Epoch: 2/2, step 18690/23838 completed (loss: 1.3439804315567017, acc: 0.6494252681732178)
[2025-02-05 13:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:06][root][INFO] - Training Epoch: 2/2, step 18691/23838 completed (loss: 1.4386073350906372, acc: 0.5802469253540039)
[2025-02-05 13:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:06][root][INFO] - Training Epoch: 2/2, step 18692/23838 completed (loss: 1.1843520402908325, acc: 0.6953125)
[2025-02-05 13:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:06][root][INFO] - Training Epoch: 2/2, step 18693/23838 completed (loss: 1.0094722509384155, acc: 0.7252747416496277)
[2025-02-05 13:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:07][root][INFO] - Training Epoch: 2/2, step 18694/23838 completed (loss: 1.0714315176010132, acc: 0.6888889074325562)
[2025-02-05 13:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:07][root][INFO] - Training Epoch: 2/2, step 18695/23838 completed (loss: 1.050628900527954, acc: 0.6880000233650208)
[2025-02-05 13:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:08][root][INFO] - Training Epoch: 2/2, step 18696/23838 completed (loss: 1.6686162948608398, acc: 0.5327102541923523)
[2025-02-05 13:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:08][root][INFO] - Training Epoch: 2/2, step 18697/23838 completed (loss: 1.1076431274414062, acc: 0.682170569896698)
[2025-02-05 13:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:08][root][INFO] - Training Epoch: 2/2, step 18698/23838 completed (loss: 1.3355821371078491, acc: 0.5742574334144592)
[2025-02-05 13:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:09][root][INFO] - Training Epoch: 2/2, step 18699/23838 completed (loss: 1.0586646795272827, acc: 0.7051281929016113)
[2025-02-05 13:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:09][root][INFO] - Training Epoch: 2/2, step 18700/23838 completed (loss: 1.0253548622131348, acc: 0.6341463327407837)
[2025-02-05 13:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:10][root][INFO] - Training Epoch: 2/2, step 18701/23838 completed (loss: 1.2436070442199707, acc: 0.650602400302887)
[2025-02-05 13:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:10][root][INFO] - Training Epoch: 2/2, step 18702/23838 completed (loss: 1.0362366437911987, acc: 0.6966292262077332)
[2025-02-05 13:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:11][root][INFO] - Training Epoch: 2/2, step 18703/23838 completed (loss: 1.0936542749404907, acc: 0.6722689270973206)
[2025-02-05 13:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:11][root][INFO] - Training Epoch: 2/2, step 18704/23838 completed (loss: 1.084475040435791, acc: 0.6865671873092651)
[2025-02-05 13:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:11][root][INFO] - Training Epoch: 2/2, step 18705/23838 completed (loss: 1.2550395727157593, acc: 0.6233766078948975)
[2025-02-05 13:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:12][root][INFO] - Training Epoch: 2/2, step 18706/23838 completed (loss: 1.0511105060577393, acc: 0.6382978558540344)
[2025-02-05 13:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:12][root][INFO] - Training Epoch: 2/2, step 18707/23838 completed (loss: 1.0634268522262573, acc: 0.682692289352417)
[2025-02-05 13:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:13][root][INFO] - Training Epoch: 2/2, step 18708/23838 completed (loss: 0.9948484301567078, acc: 0.7045454382896423)
[2025-02-05 13:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:13][root][INFO] - Training Epoch: 2/2, step 18709/23838 completed (loss: 1.0496255159378052, acc: 0.7088607549667358)
[2025-02-05 13:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:14][root][INFO] - Training Epoch: 2/2, step 18710/23838 completed (loss: 1.0248140096664429, acc: 0.682539701461792)
[2025-02-05 13:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:14][root][INFO] - Training Epoch: 2/2, step 18711/23838 completed (loss: 1.2331961393356323, acc: 0.6122449040412903)
[2025-02-05 13:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:14][root][INFO] - Training Epoch: 2/2, step 18712/23838 completed (loss: 1.1600415706634521, acc: 0.609375)
[2025-02-05 13:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:15][root][INFO] - Training Epoch: 2/2, step 18713/23838 completed (loss: 1.202573299407959, acc: 0.6319444179534912)
[2025-02-05 13:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:15][root][INFO] - Training Epoch: 2/2, step 18714/23838 completed (loss: 1.0511583089828491, acc: 0.7654321193695068)
[2025-02-05 13:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:16][root][INFO] - Training Epoch: 2/2, step 18715/23838 completed (loss: 1.2607920169830322, acc: 0.6319444179534912)
[2025-02-05 13:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:16][root][INFO] - Training Epoch: 2/2, step 18716/23838 completed (loss: 0.8895024061203003, acc: 0.7142857313156128)
[2025-02-05 13:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:16][root][INFO] - Training Epoch: 2/2, step 18717/23838 completed (loss: 1.0687448978424072, acc: 0.6744186282157898)
[2025-02-05 13:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:17][root][INFO] - Training Epoch: 2/2, step 18718/23838 completed (loss: 1.0453510284423828, acc: 0.652482271194458)
[2025-02-05 13:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:17][root][INFO] - Training Epoch: 2/2, step 18719/23838 completed (loss: 0.9520381689071655, acc: 0.7613636255264282)
[2025-02-05 13:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:18][root][INFO] - Training Epoch: 2/2, step 18720/23838 completed (loss: 0.944248616695404, acc: 0.7227723002433777)
[2025-02-05 13:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:18][root][INFO] - Training Epoch: 2/2, step 18721/23838 completed (loss: 1.0118335485458374, acc: 0.7551020383834839)
[2025-02-05 13:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:18][root][INFO] - Training Epoch: 2/2, step 18722/23838 completed (loss: 1.1800440549850464, acc: 0.5945945978164673)
[2025-02-05 13:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:19][root][INFO] - Training Epoch: 2/2, step 18723/23838 completed (loss: 1.2374759912490845, acc: 0.5909090638160706)
[2025-02-05 13:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:19][root][INFO] - Training Epoch: 2/2, step 18724/23838 completed (loss: 0.6301107406616211, acc: 0.7941176295280457)
[2025-02-05 13:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:20][root][INFO] - Training Epoch: 2/2, step 18725/23838 completed (loss: 0.8809895515441895, acc: 0.7142857313156128)
[2025-02-05 13:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:20][root][INFO] - Training Epoch: 2/2, step 18726/23838 completed (loss: 0.8625144362449646, acc: 0.6969696879386902)
[2025-02-05 13:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:20][root][INFO] - Training Epoch: 2/2, step 18727/23838 completed (loss: 1.504101037979126, acc: 0.5555555820465088)
[2025-02-05 13:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:21][root][INFO] - Training Epoch: 2/2, step 18728/23838 completed (loss: 0.9725139737129211, acc: 0.5925925970077515)
[2025-02-05 13:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:21][root][INFO] - Training Epoch: 2/2, step 18729/23838 completed (loss: 0.10456471145153046, acc: 1.0)
[2025-02-05 13:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:22][root][INFO] - Training Epoch: 2/2, step 18730/23838 completed (loss: 0.45355311036109924, acc: 0.9200000166893005)
[2025-02-05 13:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:22][root][INFO] - Training Epoch: 2/2, step 18731/23838 completed (loss: 0.6234729290008545, acc: 0.7878788113594055)
[2025-02-05 13:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:22][root][INFO] - Training Epoch: 2/2, step 18732/23838 completed (loss: 0.6382079124450684, acc: 0.8695651888847351)
[2025-02-05 13:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:23][root][INFO] - Training Epoch: 2/2, step 18733/23838 completed (loss: 0.8151189684867859, acc: 0.7894737124443054)
[2025-02-05 13:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:23][root][INFO] - Training Epoch: 2/2, step 18734/23838 completed (loss: 0.5179951190948486, acc: 0.8399999737739563)
[2025-02-05 13:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:24][root][INFO] - Training Epoch: 2/2, step 18735/23838 completed (loss: 1.1718312501907349, acc: 0.6666666865348816)
[2025-02-05 13:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:24][root][INFO] - Training Epoch: 2/2, step 18736/23838 completed (loss: 1.0447865724563599, acc: 0.75)
[2025-02-05 13:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:25][root][INFO] - Training Epoch: 2/2, step 18737/23838 completed (loss: 0.8798658847808838, acc: 0.7111111283302307)
[2025-02-05 13:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:25][root][INFO] - Training Epoch: 2/2, step 18738/23838 completed (loss: 0.8038475513458252, acc: 0.6666666865348816)
[2025-02-05 13:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:25][root][INFO] - Training Epoch: 2/2, step 18739/23838 completed (loss: 1.0098283290863037, acc: 0.6285714507102966)
[2025-02-05 13:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:26][root][INFO] - Training Epoch: 2/2, step 18740/23838 completed (loss: 0.6676682233810425, acc: 0.8055555820465088)
[2025-02-05 13:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:26][root][INFO] - Training Epoch: 2/2, step 18741/23838 completed (loss: 1.1019372940063477, acc: 0.782608687877655)
[2025-02-05 13:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:27][root][INFO] - Training Epoch: 2/2, step 18742/23838 completed (loss: 0.7347606420516968, acc: 0.782608687877655)
[2025-02-05 13:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:27][root][INFO] - Training Epoch: 2/2, step 18743/23838 completed (loss: 0.9719703793525696, acc: 0.6470588445663452)
[2025-02-05 13:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:27][root][INFO] - Training Epoch: 2/2, step 18744/23838 completed (loss: 0.9926531910896301, acc: 0.7419354915618896)
[2025-02-05 13:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:28][root][INFO] - Training Epoch: 2/2, step 18745/23838 completed (loss: 0.4981766641139984, acc: 0.6666666865348816)
[2025-02-05 13:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:28][root][INFO] - Training Epoch: 2/2, step 18746/23838 completed (loss: 0.37560635805130005, acc: 0.9444444179534912)
[2025-02-05 13:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:28][root][INFO] - Training Epoch: 2/2, step 18747/23838 completed (loss: 0.08191495388746262, acc: 1.0)
[2025-02-05 13:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:29][root][INFO] - Training Epoch: 2/2, step 18748/23838 completed (loss: 0.9279972314834595, acc: 0.7777777910232544)
[2025-02-05 13:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:29][root][INFO] - Training Epoch: 2/2, step 18749/23838 completed (loss: 0.6749517917633057, acc: 0.8235294222831726)
[2025-02-05 13:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:30][root][INFO] - Training Epoch: 2/2, step 18750/23838 completed (loss: 0.20866809785366058, acc: 1.0)
[2025-02-05 13:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:30][root][INFO] - Training Epoch: 2/2, step 18751/23838 completed (loss: 0.4699164032936096, acc: 0.9444444179534912)
[2025-02-05 13:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:30][root][INFO] - Training Epoch: 2/2, step 18752/23838 completed (loss: 0.9909529089927673, acc: 0.6800000071525574)
[2025-02-05 13:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:31][root][INFO] - Training Epoch: 2/2, step 18753/23838 completed (loss: 0.5302996039390564, acc: 0.8846153616905212)
[2025-02-05 13:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:31][root][INFO] - Training Epoch: 2/2, step 18754/23838 completed (loss: 0.6587908267974854, acc: 0.800000011920929)
[2025-02-05 13:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:32][root][INFO] - Training Epoch: 2/2, step 18755/23838 completed (loss: 0.28171396255493164, acc: 0.9142857193946838)
[2025-02-05 13:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:32][root][INFO] - Training Epoch: 2/2, step 18756/23838 completed (loss: 0.809685230255127, acc: 0.75)
[2025-02-05 13:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:33][root][INFO] - Training Epoch: 2/2, step 18757/23838 completed (loss: 0.5180506706237793, acc: 0.8484848737716675)
[2025-02-05 13:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:33][root][INFO] - Training Epoch: 2/2, step 18758/23838 completed (loss: 0.13143892586231232, acc: 0.95652174949646)
[2025-02-05 13:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:33][root][INFO] - Training Epoch: 2/2, step 18759/23838 completed (loss: 0.8731740713119507, acc: 0.699999988079071)
[2025-02-05 13:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:34][root][INFO] - Training Epoch: 2/2, step 18760/23838 completed (loss: 0.6045773029327393, acc: 0.8333333134651184)
[2025-02-05 13:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:34][root][INFO] - Training Epoch: 2/2, step 18761/23838 completed (loss: 1.2660107612609863, acc: 0.6666666865348816)
[2025-02-05 13:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:35][root][INFO] - Training Epoch: 2/2, step 18762/23838 completed (loss: 0.5662975907325745, acc: 0.8181818127632141)
[2025-02-05 13:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:35][root][INFO] - Training Epoch: 2/2, step 18763/23838 completed (loss: 1.0172693729400635, acc: 0.7272727489471436)
[2025-02-05 13:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:35][root][INFO] - Training Epoch: 2/2, step 18764/23838 completed (loss: 0.31615158915519714, acc: 0.9166666865348816)
[2025-02-05 13:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:36][root][INFO] - Training Epoch: 2/2, step 18765/23838 completed (loss: 0.5561919212341309, acc: 0.7878788113594055)
[2025-02-05 13:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:36][root][INFO] - Training Epoch: 2/2, step 18766/23838 completed (loss: 0.8049079775810242, acc: 0.6764705777168274)
[2025-02-05 13:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:37][root][INFO] - Training Epoch: 2/2, step 18767/23838 completed (loss: 1.2006571292877197, acc: 0.6760563254356384)
[2025-02-05 13:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:37][root][INFO] - Training Epoch: 2/2, step 18768/23838 completed (loss: 0.3518424332141876, acc: 0.8888888955116272)
[2025-02-05 13:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:37][root][INFO] - Training Epoch: 2/2, step 18769/23838 completed (loss: 0.7465967535972595, acc: 0.75)
[2025-02-05 13:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:38][root][INFO] - Training Epoch: 2/2, step 18770/23838 completed (loss: 0.6618143320083618, acc: 0.800000011920929)
[2025-02-05 13:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:38][root][INFO] - Training Epoch: 2/2, step 18771/23838 completed (loss: 0.6934041380882263, acc: 0.7666666507720947)
[2025-02-05 13:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:39][root][INFO] - Training Epoch: 2/2, step 18772/23838 completed (loss: 0.10989444702863693, acc: 0.9259259104728699)
[2025-02-05 13:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:39][root][INFO] - Training Epoch: 2/2, step 18773/23838 completed (loss: 0.42205917835235596, acc: 0.8695651888847351)
[2025-02-05 13:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:39][root][INFO] - Training Epoch: 2/2, step 18774/23838 completed (loss: 0.8201555013656616, acc: 0.7941176295280457)
[2025-02-05 13:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:40][root][INFO] - Training Epoch: 2/2, step 18775/23838 completed (loss: 0.32560184597969055, acc: 0.8620689511299133)
[2025-02-05 13:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:40][root][INFO] - Training Epoch: 2/2, step 18776/23838 completed (loss: 0.7799674272537231, acc: 0.6896551847457886)
[2025-02-05 13:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:40][root][INFO] - Training Epoch: 2/2, step 18777/23838 completed (loss: 0.9280909895896912, acc: 0.7272727489471436)
[2025-02-05 13:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:41][root][INFO] - Training Epoch: 2/2, step 18778/23838 completed (loss: 0.3235742449760437, acc: 0.9210526347160339)
[2025-02-05 13:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:41][root][INFO] - Training Epoch: 2/2, step 18779/23838 completed (loss: 0.7168124914169312, acc: 0.7567567825317383)
[2025-02-05 13:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:42][root][INFO] - Training Epoch: 2/2, step 18780/23838 completed (loss: 0.7050570845603943, acc: 0.7906976938247681)
[2025-02-05 13:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:42][root][INFO] - Training Epoch: 2/2, step 18781/23838 completed (loss: 0.9374525547027588, acc: 0.6944444179534912)
[2025-02-05 13:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:42][root][INFO] - Training Epoch: 2/2, step 18782/23838 completed (loss: 1.0547243356704712, acc: 0.6595744490623474)
[2025-02-05 13:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:43][root][INFO] - Training Epoch: 2/2, step 18783/23838 completed (loss: 0.39380431175231934, acc: 0.8399999737739563)
[2025-02-05 13:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:43][root][INFO] - Training Epoch: 2/2, step 18784/23838 completed (loss: 0.9597005248069763, acc: 0.6538461446762085)
[2025-02-05 13:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:43][root][INFO] - Training Epoch: 2/2, step 18785/23838 completed (loss: 0.6904115676879883, acc: 0.7666666507720947)
[2025-02-05 13:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:44][root][INFO] - Training Epoch: 2/2, step 18786/23838 completed (loss: 1.081347942352295, acc: 0.7037037014961243)
[2025-02-05 13:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:44][root][INFO] - Training Epoch: 2/2, step 18787/23838 completed (loss: 0.8359492421150208, acc: 0.7407407164573669)
[2025-02-05 13:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:45][root][INFO] - Training Epoch: 2/2, step 18788/23838 completed (loss: 0.6256837248802185, acc: 0.8095238208770752)
[2025-02-05 13:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:45][root][INFO] - Training Epoch: 2/2, step 18789/23838 completed (loss: 0.7085604071617126, acc: 0.807692289352417)
[2025-02-05 13:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:45][root][INFO] - Training Epoch: 2/2, step 18790/23838 completed (loss: 0.7882415056228638, acc: 0.7777777910232544)
[2025-02-05 13:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:46][root][INFO] - Training Epoch: 2/2, step 18791/23838 completed (loss: 1.0471510887145996, acc: 0.699999988079071)
[2025-02-05 13:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:46][root][INFO] - Training Epoch: 2/2, step 18792/23838 completed (loss: 1.0300676822662354, acc: 0.7368420958518982)
[2025-02-05 13:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:46][root][INFO] - Training Epoch: 2/2, step 18793/23838 completed (loss: 1.0135228633880615, acc: 0.7200000286102295)
[2025-02-05 13:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:47][root][INFO] - Training Epoch: 2/2, step 18794/23838 completed (loss: 0.5425289869308472, acc: 0.8333333134651184)
[2025-02-05 13:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:47][root][INFO] - Training Epoch: 2/2, step 18795/23838 completed (loss: 0.8639603853225708, acc: 0.7222222089767456)
[2025-02-05 13:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:48][root][INFO] - Training Epoch: 2/2, step 18796/23838 completed (loss: 0.6207541227340698, acc: 0.8235294222831726)
[2025-02-05 13:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:48][root][INFO] - Training Epoch: 2/2, step 18797/23838 completed (loss: 1.3641698360443115, acc: 0.6818181872367859)
[2025-02-05 13:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:48][root][INFO] - Training Epoch: 2/2, step 18798/23838 completed (loss: 0.629562497138977, acc: 0.8235294222831726)
[2025-02-05 13:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:49][root][INFO] - Training Epoch: 2/2, step 18799/23838 completed (loss: 0.4612811505794525, acc: 0.8695651888847351)
[2025-02-05 13:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:49][root][INFO] - Training Epoch: 2/2, step 18800/23838 completed (loss: 0.7695277333259583, acc: 0.699999988079071)
[2025-02-05 13:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:49][root][INFO] - Training Epoch: 2/2, step 18801/23838 completed (loss: 0.2586476802825928, acc: 0.9333333373069763)
[2025-02-05 13:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:50][root][INFO] - Training Epoch: 2/2, step 18802/23838 completed (loss: 0.9057736396789551, acc: 0.6842105388641357)
[2025-02-05 13:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:50][root][INFO] - Training Epoch: 2/2, step 18803/23838 completed (loss: 0.4749666154384613, acc: 0.84375)
[2025-02-05 13:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:51][root][INFO] - Training Epoch: 2/2, step 18804/23838 completed (loss: 0.8450900316238403, acc: 0.8518518805503845)
[2025-02-05 13:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:51][root][INFO] - Training Epoch: 2/2, step 18805/23838 completed (loss: 0.5346699357032776, acc: 0.8888888955116272)
[2025-02-05 13:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:51][root][INFO] - Training Epoch: 2/2, step 18806/23838 completed (loss: 0.5959399342536926, acc: 0.8399999737739563)
[2025-02-05 13:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:52][root][INFO] - Training Epoch: 2/2, step 18807/23838 completed (loss: 0.5188815593719482, acc: 0.7931034564971924)
[2025-02-05 13:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:52][root][INFO] - Training Epoch: 2/2, step 18808/23838 completed (loss: 0.5693381428718567, acc: 0.800000011920929)
[2025-02-05 13:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:52][root][INFO] - Training Epoch: 2/2, step 18809/23838 completed (loss: 0.24842000007629395, acc: 0.9166666865348816)
[2025-02-05 13:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:53][root][INFO] - Training Epoch: 2/2, step 18810/23838 completed (loss: 0.6212984323501587, acc: 0.8139534592628479)
[2025-02-05 13:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:53][root][INFO] - Training Epoch: 2/2, step 18811/23838 completed (loss: 0.7685328125953674, acc: 0.75)
[2025-02-05 13:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:53][root][INFO] - Training Epoch: 2/2, step 18812/23838 completed (loss: 0.8468652963638306, acc: 0.75)
[2025-02-05 13:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:54][root][INFO] - Training Epoch: 2/2, step 18813/23838 completed (loss: 0.2838383913040161, acc: 0.9545454382896423)
[2025-02-05 13:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:54][root][INFO] - Training Epoch: 2/2, step 18814/23838 completed (loss: 1.3693052530288696, acc: 0.52173912525177)
[2025-02-05 13:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:55][root][INFO] - Training Epoch: 2/2, step 18815/23838 completed (loss: 1.32020103931427, acc: 0.640625)
[2025-02-05 13:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:55][root][INFO] - Training Epoch: 2/2, step 18816/23838 completed (loss: 1.7558414936065674, acc: 0.5445544719696045)
[2025-02-05 13:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:55][root][INFO] - Training Epoch: 2/2, step 18817/23838 completed (loss: 1.9165455102920532, acc: 0.4545454680919647)
[2025-02-05 13:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:56][root][INFO] - Training Epoch: 2/2, step 18818/23838 completed (loss: 1.5077604055404663, acc: 0.6048387289047241)
[2025-02-05 13:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:56][root][INFO] - Training Epoch: 2/2, step 18819/23838 completed (loss: 0.9967615008354187, acc: 0.7799999713897705)
[2025-02-05 13:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:57][root][INFO] - Training Epoch: 2/2, step 18820/23838 completed (loss: 1.1534621715545654, acc: 0.70652174949646)
[2025-02-05 13:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:57][root][INFO] - Training Epoch: 2/2, step 18821/23838 completed (loss: 1.5938605070114136, acc: 0.5625)
[2025-02-05 13:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:57][root][INFO] - Training Epoch: 2/2, step 18822/23838 completed (loss: 1.1097997426986694, acc: 0.7142857313156128)
[2025-02-05 13:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:58][root][INFO] - Training Epoch: 2/2, step 18823/23838 completed (loss: 1.334444284439087, acc: 0.5833333134651184)
[2025-02-05 13:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:58][root][INFO] - Training Epoch: 2/2, step 18824/23838 completed (loss: 1.4139888286590576, acc: 0.5600000023841858)
[2025-02-05 13:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:58][root][INFO] - Training Epoch: 2/2, step 18825/23838 completed (loss: 1.3639298677444458, acc: 0.6521739363670349)
[2025-02-05 13:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:59][root][INFO] - Training Epoch: 2/2, step 18826/23838 completed (loss: 1.5333399772644043, acc: 0.625)
[2025-02-05 13:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:45:59][root][INFO] - Training Epoch: 2/2, step 18827/23838 completed (loss: 1.1391990184783936, acc: 0.7127659320831299)
[2025-02-05 13:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:00][root][INFO] - Training Epoch: 2/2, step 18828/23838 completed (loss: 1.2486032247543335, acc: 0.6363636255264282)
[2025-02-05 13:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:00][root][INFO] - Training Epoch: 2/2, step 18829/23838 completed (loss: 1.1064422130584717, acc: 0.7213114500045776)
[2025-02-05 13:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:00][root][INFO] - Training Epoch: 2/2, step 18830/23838 completed (loss: 1.434256672859192, acc: 0.6212121248245239)
[2025-02-05 13:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:01][root][INFO] - Training Epoch: 2/2, step 18831/23838 completed (loss: 1.493510365486145, acc: 0.5714285969734192)
[2025-02-05 13:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:01][root][INFO] - Training Epoch: 2/2, step 18832/23838 completed (loss: 1.178613305091858, acc: 0.6781609058380127)
[2025-02-05 13:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:02][root][INFO] - Training Epoch: 2/2, step 18833/23838 completed (loss: 1.2236874103546143, acc: 0.6666666865348816)
[2025-02-05 13:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:02][root][INFO] - Training Epoch: 2/2, step 18834/23838 completed (loss: 0.8231196403503418, acc: 0.7540983557701111)
[2025-02-05 13:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:02][root][INFO] - Training Epoch: 2/2, step 18835/23838 completed (loss: 1.3953344821929932, acc: 0.5961538553237915)
[2025-02-05 13:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:03][root][INFO] - Training Epoch: 2/2, step 18836/23838 completed (loss: 1.282064437866211, acc: 0.569767415523529)
[2025-02-05 13:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:03][root][INFO] - Training Epoch: 2/2, step 18837/23838 completed (loss: 1.0811554193496704, acc: 0.6756756901741028)
[2025-02-05 13:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:04][root][INFO] - Training Epoch: 2/2, step 18838/23838 completed (loss: 1.2604120969772339, acc: 0.6521739363670349)
[2025-02-05 13:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:04][root][INFO] - Training Epoch: 2/2, step 18839/23838 completed (loss: 1.0990580320358276, acc: 0.692307710647583)
[2025-02-05 13:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:04][root][INFO] - Training Epoch: 2/2, step 18840/23838 completed (loss: 1.5294092893600464, acc: 0.6111111044883728)
[2025-02-05 13:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:05][root][INFO] - Training Epoch: 2/2, step 18841/23838 completed (loss: 1.2579599618911743, acc: 0.6344085931777954)
[2025-02-05 13:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:05][root][INFO] - Training Epoch: 2/2, step 18842/23838 completed (loss: 1.4142197370529175, acc: 0.5600000023841858)
[2025-02-05 13:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:05][root][INFO] - Training Epoch: 2/2, step 18843/23838 completed (loss: 1.2851728200912476, acc: 0.5714285969734192)
[2025-02-05 13:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:06][root][INFO] - Training Epoch: 2/2, step 18844/23838 completed (loss: 1.6686012744903564, acc: 0.5454545617103577)
[2025-02-05 13:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:06][root][INFO] - Training Epoch: 2/2, step 18845/23838 completed (loss: 1.4440007209777832, acc: 0.6050955653190613)
[2025-02-05 13:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:07][root][INFO] - Training Epoch: 2/2, step 18846/23838 completed (loss: 1.3994618654251099, acc: 0.6285714507102966)
[2025-02-05 13:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:07][root][INFO] - Training Epoch: 2/2, step 18847/23838 completed (loss: 1.2371633052825928, acc: 0.6277372241020203)
[2025-02-05 13:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:07][root][INFO] - Training Epoch: 2/2, step 18848/23838 completed (loss: 0.9927257895469666, acc: 0.7160493731498718)
[2025-02-05 13:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:08][root][INFO] - Training Epoch: 2/2, step 18849/23838 completed (loss: 1.3146284818649292, acc: 0.6451612710952759)
[2025-02-05 13:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:08][root][INFO] - Training Epoch: 2/2, step 18850/23838 completed (loss: 1.0762559175491333, acc: 0.6603773832321167)
[2025-02-05 13:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:08][root][INFO] - Training Epoch: 2/2, step 18851/23838 completed (loss: 1.335463523864746, acc: 0.6666666865348816)
[2025-02-05 13:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:09][root][INFO] - Training Epoch: 2/2, step 18852/23838 completed (loss: 1.3627113103866577, acc: 0.6018518805503845)
[2025-02-05 13:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:09][root][INFO] - Training Epoch: 2/2, step 18853/23838 completed (loss: 1.3604001998901367, acc: 0.621052622795105)
[2025-02-05 13:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:10][root][INFO] - Training Epoch: 2/2, step 18854/23838 completed (loss: 1.5714365243911743, acc: 0.582524299621582)
[2025-02-05 13:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:10][root][INFO] - Training Epoch: 2/2, step 18855/23838 completed (loss: 1.7481114864349365, acc: 0.4642857015132904)
[2025-02-05 13:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:10][root][INFO] - Training Epoch: 2/2, step 18856/23838 completed (loss: 1.9152690172195435, acc: 0.4492753744125366)
[2025-02-05 13:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:11][root][INFO] - Training Epoch: 2/2, step 18857/23838 completed (loss: 0.9948624968528748, acc: 0.7200000286102295)
[2025-02-05 13:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:11][root][INFO] - Training Epoch: 2/2, step 18858/23838 completed (loss: 1.3197861909866333, acc: 0.686274528503418)
[2025-02-05 13:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:11][root][INFO] - Training Epoch: 2/2, step 18859/23838 completed (loss: 1.6058800220489502, acc: 0.5324675440788269)
[2025-02-05 13:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:12][root][INFO] - Training Epoch: 2/2, step 18860/23838 completed (loss: 1.0344624519348145, acc: 0.7599999904632568)
[2025-02-05 13:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:12][root][INFO] - Training Epoch: 2/2, step 18861/23838 completed (loss: 1.0208256244659424, acc: 0.7272727489471436)
[2025-02-05 13:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:13][root][INFO] - Training Epoch: 2/2, step 18862/23838 completed (loss: 1.3298413753509521, acc: 0.6285714507102966)
[2025-02-05 13:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:13][root][INFO] - Training Epoch: 2/2, step 18863/23838 completed (loss: 1.0556254386901855, acc: 0.7160493731498718)
[2025-02-05 13:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:13][root][INFO] - Training Epoch: 2/2, step 18864/23838 completed (loss: 1.257557988166809, acc: 0.671875)
[2025-02-05 13:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:14][root][INFO] - Training Epoch: 2/2, step 18865/23838 completed (loss: 1.17957603931427, acc: 0.6666666865348816)
[2025-02-05 13:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:14][root][INFO] - Training Epoch: 2/2, step 18866/23838 completed (loss: 1.178890347480774, acc: 0.6279069781303406)
[2025-02-05 13:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:15][root][INFO] - Training Epoch: 2/2, step 18867/23838 completed (loss: 1.278332233428955, acc: 0.6521739363670349)
[2025-02-05 13:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:15][root][INFO] - Training Epoch: 2/2, step 18868/23838 completed (loss: 1.2286003828048706, acc: 0.6551724076271057)
[2025-02-05 13:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:15][root][INFO] - Training Epoch: 2/2, step 18869/23838 completed (loss: 1.073630452156067, acc: 0.6818181872367859)
[2025-02-05 13:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:16][root][INFO] - Training Epoch: 2/2, step 18870/23838 completed (loss: 1.1989394426345825, acc: 0.6415094137191772)
[2025-02-05 13:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:16][root][INFO] - Training Epoch: 2/2, step 18871/23838 completed (loss: 1.0131233930587769, acc: 0.7289719581604004)
[2025-02-05 13:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:17][root][INFO] - Training Epoch: 2/2, step 18872/23838 completed (loss: 0.7928406596183777, acc: 0.800000011920929)
[2025-02-05 13:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:17][root][INFO] - Training Epoch: 2/2, step 18873/23838 completed (loss: 1.0380775928497314, acc: 0.7068965435028076)
[2025-02-05 13:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:17][root][INFO] - Training Epoch: 2/2, step 18874/23838 completed (loss: 1.0146647691726685, acc: 0.7068965435028076)
[2025-02-05 13:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:18][root][INFO] - Training Epoch: 2/2, step 18875/23838 completed (loss: 0.9915544390678406, acc: 0.71875)
[2025-02-05 13:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:18][root][INFO] - Training Epoch: 2/2, step 18876/23838 completed (loss: 1.1793572902679443, acc: 0.6436781883239746)
[2025-02-05 13:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:19][root][INFO] - Training Epoch: 2/2, step 18877/23838 completed (loss: 1.181810736656189, acc: 0.607594907283783)
[2025-02-05 13:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:19][root][INFO] - Training Epoch: 2/2, step 18878/23838 completed (loss: 1.469675064086914, acc: 0.6082473993301392)
[2025-02-05 13:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:19][root][INFO] - Training Epoch: 2/2, step 18879/23838 completed (loss: 1.2321100234985352, acc: 0.6867470145225525)
[2025-02-05 13:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:20][root][INFO] - Training Epoch: 2/2, step 18880/23838 completed (loss: 1.495711326599121, acc: 0.5394737124443054)
[2025-02-05 13:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:20][root][INFO] - Training Epoch: 2/2, step 18881/23838 completed (loss: 1.4555085897445679, acc: 0.5934959053993225)
[2025-02-05 13:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:21][root][INFO] - Training Epoch: 2/2, step 18882/23838 completed (loss: 1.2067004442214966, acc: 0.6477272510528564)
[2025-02-05 13:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:21][root][INFO] - Training Epoch: 2/2, step 18883/23838 completed (loss: 1.2476447820663452, acc: 0.6458333134651184)
[2025-02-05 13:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:21][root][INFO] - Training Epoch: 2/2, step 18884/23838 completed (loss: 0.9728348851203918, acc: 0.7021276354789734)
[2025-02-05 13:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:22][root][INFO] - Training Epoch: 2/2, step 18885/23838 completed (loss: 0.8969745635986328, acc: 0.75)
[2025-02-05 13:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:22][root][INFO] - Training Epoch: 2/2, step 18886/23838 completed (loss: 1.3310366868972778, acc: 0.5730336904525757)
[2025-02-05 13:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:23][root][INFO] - Training Epoch: 2/2, step 18887/23838 completed (loss: 1.5482513904571533, acc: 0.529411792755127)
[2025-02-05 13:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:23][root][INFO] - Training Epoch: 2/2, step 18888/23838 completed (loss: 0.8103854060173035, acc: 0.75)
[2025-02-05 13:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:23][root][INFO] - Training Epoch: 2/2, step 18889/23838 completed (loss: 1.227219820022583, acc: 0.6891891956329346)
[2025-02-05 13:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:24][root][INFO] - Training Epoch: 2/2, step 18890/23838 completed (loss: 1.2999234199523926, acc: 0.6744186282157898)
[2025-02-05 13:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:24][root][INFO] - Training Epoch: 2/2, step 18891/23838 completed (loss: 1.0898149013519287, acc: 0.7045454382896423)
[2025-02-05 13:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:24][root][INFO] - Training Epoch: 2/2, step 18892/23838 completed (loss: 1.006468653678894, acc: 0.6818181872367859)
[2025-02-05 13:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:25][root][INFO] - Training Epoch: 2/2, step 18893/23838 completed (loss: 1.218577265739441, acc: 0.6478873491287231)
[2025-02-05 13:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:25][root][INFO] - Training Epoch: 2/2, step 18894/23838 completed (loss: 1.210780382156372, acc: 0.6716417670249939)
[2025-02-05 13:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:26][root][INFO] - Training Epoch: 2/2, step 18895/23838 completed (loss: 1.1783523559570312, acc: 0.6666666865348816)
[2025-02-05 13:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:26][root][INFO] - Training Epoch: 2/2, step 18896/23838 completed (loss: 1.4235223531723022, acc: 0.5899999737739563)
[2025-02-05 13:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:26][root][INFO] - Training Epoch: 2/2, step 18897/23838 completed (loss: 1.0729676485061646, acc: 0.5964912176132202)
[2025-02-05 13:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:27][root][INFO] - Training Epoch: 2/2, step 18898/23838 completed (loss: 1.0903786420822144, acc: 0.5675675868988037)
[2025-02-05 13:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:27][root][INFO] - Training Epoch: 2/2, step 18899/23838 completed (loss: 1.2053014039993286, acc: 0.6201550364494324)
[2025-02-05 13:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:27][root][INFO] - Training Epoch: 2/2, step 18900/23838 completed (loss: 1.2810369729995728, acc: 0.6476190686225891)
[2025-02-05 13:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:28][root][INFO] - Training Epoch: 2/2, step 18901/23838 completed (loss: 1.149055004119873, acc: 0.6408450603485107)
[2025-02-05 13:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:28][root][INFO] - Training Epoch: 2/2, step 18902/23838 completed (loss: 1.155841588973999, acc: 0.6503496766090393)
[2025-02-05 13:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:29][root][INFO] - Training Epoch: 2/2, step 18903/23838 completed (loss: 1.499356985092163, acc: 0.5306122303009033)
[2025-02-05 13:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:29][root][INFO] - Training Epoch: 2/2, step 18904/23838 completed (loss: 1.2105169296264648, acc: 0.6744186282157898)
[2025-02-05 13:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:29][root][INFO] - Training Epoch: 2/2, step 18905/23838 completed (loss: 1.178078055381775, acc: 0.6355140209197998)
[2025-02-05 13:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:30][root][INFO] - Training Epoch: 2/2, step 18906/23838 completed (loss: 1.4483917951583862, acc: 0.5702479481697083)
[2025-02-05 13:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:30][root][INFO] - Training Epoch: 2/2, step 18907/23838 completed (loss: 1.2148512601852417, acc: 0.6331360936164856)
[2025-02-05 13:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:31][root][INFO] - Training Epoch: 2/2, step 18908/23838 completed (loss: 0.9538713097572327, acc: 0.7215189933776855)
[2025-02-05 13:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:31][root][INFO] - Training Epoch: 2/2, step 18909/23838 completed (loss: 1.3330360651016235, acc: 0.5975610017776489)
[2025-02-05 13:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:31][root][INFO] - Training Epoch: 2/2, step 18910/23838 completed (loss: 1.3069841861724854, acc: 0.625)
[2025-02-05 13:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:32][root][INFO] - Training Epoch: 2/2, step 18911/23838 completed (loss: 1.2480204105377197, acc: 0.6336633563041687)
[2025-02-05 13:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:32][root][INFO] - Training Epoch: 2/2, step 18912/23838 completed (loss: 1.123458981513977, acc: 0.695652186870575)
[2025-02-05 13:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:32][root][INFO] - Training Epoch: 2/2, step 18913/23838 completed (loss: 1.0848112106323242, acc: 0.6495726704597473)
[2025-02-05 13:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:33][root][INFO] - Training Epoch: 2/2, step 18914/23838 completed (loss: 1.2939610481262207, acc: 0.6382978558540344)
[2025-02-05 13:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:33][root][INFO] - Training Epoch: 2/2, step 18915/23838 completed (loss: 1.1484817266464233, acc: 0.6884058117866516)
[2025-02-05 13:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:34][root][INFO] - Training Epoch: 2/2, step 18916/23838 completed (loss: 1.1487354040145874, acc: 0.7133333086967468)
[2025-02-05 13:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:34][root][INFO] - Training Epoch: 2/2, step 18917/23838 completed (loss: 1.2514724731445312, acc: 0.6258992552757263)
[2025-02-05 13:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:35][root][INFO] - Training Epoch: 2/2, step 18918/23838 completed (loss: 1.3578251600265503, acc: 0.6569767594337463)
[2025-02-05 13:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:35][root][INFO] - Training Epoch: 2/2, step 18919/23838 completed (loss: 0.7836812138557434, acc: 0.746268630027771)
[2025-02-05 13:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:35][root][INFO] - Training Epoch: 2/2, step 18920/23838 completed (loss: 1.7498719692230225, acc: 0.5625)
[2025-02-05 13:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:36][root][INFO] - Training Epoch: 2/2, step 18921/23838 completed (loss: 1.0042591094970703, acc: 0.7118644118309021)
[2025-02-05 13:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:36][root][INFO] - Training Epoch: 2/2, step 18922/23838 completed (loss: 1.60896635055542, acc: 0.5666666626930237)
[2025-02-05 13:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:36][root][INFO] - Training Epoch: 2/2, step 18923/23838 completed (loss: 0.9941855072975159, acc: 0.70652174949646)
[2025-02-05 13:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:37][root][INFO] - Training Epoch: 2/2, step 18924/23838 completed (loss: 1.209669828414917, acc: 0.6181818246841431)
[2025-02-05 13:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:37][root][INFO] - Training Epoch: 2/2, step 18925/23838 completed (loss: 0.856938362121582, acc: 0.7647058963775635)
[2025-02-05 13:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:38][root][INFO] - Training Epoch: 2/2, step 18926/23838 completed (loss: 1.1451246738433838, acc: 0.6170212626457214)
[2025-02-05 13:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:38][root][INFO] - Training Epoch: 2/2, step 18927/23838 completed (loss: 1.429713249206543, acc: 0.591549277305603)
[2025-02-05 13:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:38][root][INFO] - Training Epoch: 2/2, step 18928/23838 completed (loss: 1.2279894351959229, acc: 0.6458333134651184)
[2025-02-05 13:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:39][root][INFO] - Training Epoch: 2/2, step 18929/23838 completed (loss: 1.1009284257888794, acc: 0.698113203048706)
[2025-02-05 13:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:39][root][INFO] - Training Epoch: 2/2, step 18930/23838 completed (loss: 1.1288992166519165, acc: 0.6388888955116272)
[2025-02-05 13:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:40][root][INFO] - Training Epoch: 2/2, step 18931/23838 completed (loss: 0.9297145009040833, acc: 0.698113203048706)
[2025-02-05 13:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:40][root][INFO] - Training Epoch: 2/2, step 18932/23838 completed (loss: 0.9140279293060303, acc: 0.75)
[2025-02-05 13:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:41][root][INFO] - Training Epoch: 2/2, step 18933/23838 completed (loss: 1.0441755056381226, acc: 0.6666666865348816)
[2025-02-05 13:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:41][root][INFO] - Training Epoch: 2/2, step 18934/23838 completed (loss: 0.8503214716911316, acc: 0.7851239442825317)
[2025-02-05 13:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:41][root][INFO] - Training Epoch: 2/2, step 18935/23838 completed (loss: 1.3155847787857056, acc: 0.5357142686843872)
[2025-02-05 13:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:42][root][INFO] - Training Epoch: 2/2, step 18936/23838 completed (loss: 1.2844432592391968, acc: 0.5555555820465088)
[2025-02-05 13:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:42][root][INFO] - Training Epoch: 2/2, step 18937/23838 completed (loss: 1.0796552896499634, acc: 0.6857143044471741)
[2025-02-05 13:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:43][root][INFO] - Training Epoch: 2/2, step 18938/23838 completed (loss: 1.067246913909912, acc: 0.699999988079071)
[2025-02-05 13:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:43][root][INFO] - Training Epoch: 2/2, step 18939/23838 completed (loss: 0.8656815886497498, acc: 0.7605633735656738)
[2025-02-05 13:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:43][root][INFO] - Training Epoch: 2/2, step 18940/23838 completed (loss: 1.0412706136703491, acc: 0.7155172228813171)
[2025-02-05 13:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:44][root][INFO] - Training Epoch: 2/2, step 18941/23838 completed (loss: 1.197591781616211, acc: 0.6692913174629211)
[2025-02-05 13:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:44][root][INFO] - Training Epoch: 2/2, step 18942/23838 completed (loss: 1.0147379636764526, acc: 0.7272727489471436)
[2025-02-05 13:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:44][root][INFO] - Training Epoch: 2/2, step 18943/23838 completed (loss: 1.3870233297348022, acc: 0.6557376980781555)
[2025-02-05 13:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:45][root][INFO] - Training Epoch: 2/2, step 18944/23838 completed (loss: 1.2265228033065796, acc: 0.628742516040802)
[2025-02-05 13:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:45][root][INFO] - Training Epoch: 2/2, step 18945/23838 completed (loss: 1.468287467956543, acc: 0.594936728477478)
[2025-02-05 13:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:46][root][INFO] - Training Epoch: 2/2, step 18946/23838 completed (loss: 1.1209033727645874, acc: 0.6475409865379333)
[2025-02-05 13:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:46][root][INFO] - Training Epoch: 2/2, step 18947/23838 completed (loss: 1.2256717681884766, acc: 0.6475409865379333)
[2025-02-05 13:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:46][root][INFO] - Training Epoch: 2/2, step 18948/23838 completed (loss: 1.287580966949463, acc: 0.6379310488700867)
[2025-02-05 13:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:47][root][INFO] - Training Epoch: 2/2, step 18949/23838 completed (loss: 0.9974830746650696, acc: 0.6814814805984497)
[2025-02-05 13:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:47][root][INFO] - Training Epoch: 2/2, step 18950/23838 completed (loss: 0.8604518175125122, acc: 0.6851851940155029)
[2025-02-05 13:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:48][root][INFO] - Training Epoch: 2/2, step 18951/23838 completed (loss: 1.422431230545044, acc: 0.5953757166862488)
[2025-02-05 13:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:48][root][INFO] - Training Epoch: 2/2, step 18952/23838 completed (loss: 0.9962658286094666, acc: 0.713567852973938)
[2025-02-05 13:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:48][root][INFO] - Training Epoch: 2/2, step 18953/23838 completed (loss: 1.2055933475494385, acc: 0.6526315808296204)
[2025-02-05 13:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:49][root][INFO] - Training Epoch: 2/2, step 18954/23838 completed (loss: 0.9851573705673218, acc: 0.7319587469100952)
[2025-02-05 13:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:49][root][INFO] - Training Epoch: 2/2, step 18955/23838 completed (loss: 0.7727779150009155, acc: 0.8064516186714172)
[2025-02-05 13:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:50][root][INFO] - Training Epoch: 2/2, step 18956/23838 completed (loss: 0.9621586203575134, acc: 0.6854838728904724)
[2025-02-05 13:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:50][root][INFO] - Training Epoch: 2/2, step 18957/23838 completed (loss: 1.0170315504074097, acc: 0.7323943376541138)
[2025-02-05 13:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:50][root][INFO] - Training Epoch: 2/2, step 18958/23838 completed (loss: 1.2602814435958862, acc: 0.6477272510528564)
[2025-02-05 13:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:51][root][INFO] - Training Epoch: 2/2, step 18959/23838 completed (loss: 1.2924619913101196, acc: 0.5892857313156128)
[2025-02-05 13:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:51][root][INFO] - Training Epoch: 2/2, step 18960/23838 completed (loss: 0.9379197955131531, acc: 0.738095223903656)
[2025-02-05 13:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:52][root][INFO] - Training Epoch: 2/2, step 18961/23838 completed (loss: 1.0206657648086548, acc: 0.6911764740943909)
[2025-02-05 13:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:52][root][INFO] - Training Epoch: 2/2, step 18962/23838 completed (loss: 0.8617231249809265, acc: 0.7049180269241333)
[2025-02-05 13:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:52][root][INFO] - Training Epoch: 2/2, step 18963/23838 completed (loss: 1.1851328611373901, acc: 0.6235294342041016)
[2025-02-05 13:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:53][root][INFO] - Training Epoch: 2/2, step 18964/23838 completed (loss: 1.1094034910202026, acc: 0.5961538553237915)
[2025-02-05 13:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:53][root][INFO] - Training Epoch: 2/2, step 18965/23838 completed (loss: 1.2475612163543701, acc: 0.6739130616188049)
[2025-02-05 13:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:54][root][INFO] - Training Epoch: 2/2, step 18966/23838 completed (loss: 0.8262954950332642, acc: 0.746835470199585)
[2025-02-05 13:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:54][root][INFO] - Training Epoch: 2/2, step 18967/23838 completed (loss: 1.4355839490890503, acc: 0.5925925970077515)
[2025-02-05 13:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:54][root][INFO] - Training Epoch: 2/2, step 18968/23838 completed (loss: 0.7845604419708252, acc: 0.7647058963775635)
[2025-02-05 13:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:55][root][INFO] - Training Epoch: 2/2, step 18969/23838 completed (loss: 1.7236968278884888, acc: 0.5789473652839661)
[2025-02-05 13:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:55][root][INFO] - Training Epoch: 2/2, step 18970/23838 completed (loss: 1.3357970714569092, acc: 0.6491228342056274)
[2025-02-05 13:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:55][root][INFO] - Training Epoch: 2/2, step 18971/23838 completed (loss: 1.0005518198013306, acc: 0.7179487347602844)
[2025-02-05 13:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:56][root][INFO] - Training Epoch: 2/2, step 18972/23838 completed (loss: 1.0619145631790161, acc: 0.725806474685669)
[2025-02-05 13:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:56][root][INFO] - Training Epoch: 2/2, step 18973/23838 completed (loss: 1.123075246810913, acc: 0.6734693646430969)
[2025-02-05 13:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:57][root][INFO] - Training Epoch: 2/2, step 18974/23838 completed (loss: 0.8227169513702393, acc: 0.7460317611694336)
[2025-02-05 13:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:57][root][INFO] - Training Epoch: 2/2, step 18975/23838 completed (loss: 1.1335628032684326, acc: 0.6600000262260437)
[2025-02-05 13:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:57][root][INFO] - Training Epoch: 2/2, step 18976/23838 completed (loss: 1.2341793775558472, acc: 0.6428571343421936)
[2025-02-05 13:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:58][root][INFO] - Training Epoch: 2/2, step 18977/23838 completed (loss: 0.9222427606582642, acc: 0.7142857313156128)
[2025-02-05 13:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:58][root][INFO] - Training Epoch: 2/2, step 18978/23838 completed (loss: 1.5479779243469238, acc: 0.5365853905677795)
[2025-02-05 13:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:58][root][INFO] - Training Epoch: 2/2, step 18979/23838 completed (loss: 1.0835291147232056, acc: 0.6363636255264282)
[2025-02-05 13:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:59][root][INFO] - Training Epoch: 2/2, step 18980/23838 completed (loss: 1.3747515678405762, acc: 0.5714285969734192)
[2025-02-05 13:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:59][root][INFO] - Training Epoch: 2/2, step 18981/23838 completed (loss: 1.3613249063491821, acc: 0.5833333134651184)
[2025-02-05 13:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:46:59][root][INFO] - Training Epoch: 2/2, step 18982/23838 completed (loss: 1.8010526895523071, acc: 0.4736842215061188)
[2025-02-05 13:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:00][root][INFO] - Training Epoch: 2/2, step 18983/23838 completed (loss: 0.9966779351234436, acc: 0.7567567825317383)
[2025-02-05 13:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:00][root][INFO] - Training Epoch: 2/2, step 18984/23838 completed (loss: 1.261789321899414, acc: 0.6666666865348816)
[2025-02-05 13:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:01][root][INFO] - Training Epoch: 2/2, step 18985/23838 completed (loss: 1.3058443069458008, acc: 0.6470588445663452)
[2025-02-05 13:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:01][root][INFO] - Training Epoch: 2/2, step 18986/23838 completed (loss: 0.8407062888145447, acc: 0.739130437374115)
[2025-02-05 13:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:01][root][INFO] - Training Epoch: 2/2, step 18987/23838 completed (loss: 1.6660690307617188, acc: 0.5593220591545105)
[2025-02-05 13:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:02][root][INFO] - Training Epoch: 2/2, step 18988/23838 completed (loss: 3.066267728805542, acc: 0.0)
[2025-02-05 13:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:02][root][INFO] - Training Epoch: 2/2, step 18989/23838 completed (loss: 1.0692164897918701, acc: 0.7272727489471436)
[2025-02-05 13:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:02][root][INFO] - Training Epoch: 2/2, step 18990/23838 completed (loss: 1.2550967931747437, acc: 0.6666666865348816)
[2025-02-05 13:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:03][root][INFO] - Training Epoch: 2/2, step 18991/23838 completed (loss: 1.5720161199569702, acc: 0.4838709533214569)
[2025-02-05 13:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:03][root][INFO] - Training Epoch: 2/2, step 18992/23838 completed (loss: 1.1787525415420532, acc: 0.6571428775787354)
[2025-02-05 13:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:04][root][INFO] - Training Epoch: 2/2, step 18993/23838 completed (loss: 1.4424279928207397, acc: 0.5571428537368774)
[2025-02-05 13:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:04][root][INFO] - Training Epoch: 2/2, step 18994/23838 completed (loss: 1.2988903522491455, acc: 0.5967742204666138)
[2025-02-05 13:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:04][root][INFO] - Training Epoch: 2/2, step 18995/23838 completed (loss: 1.3577426671981812, acc: 0.6233766078948975)
[2025-02-05 13:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:05][root][INFO] - Training Epoch: 2/2, step 18996/23838 completed (loss: 0.9631972908973694, acc: 0.7894737124443054)
[2025-02-05 13:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:05][root][INFO] - Training Epoch: 2/2, step 18997/23838 completed (loss: 1.161424994468689, acc: 0.6865671873092651)
[2025-02-05 13:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:05][root][INFO] - Training Epoch: 2/2, step 18998/23838 completed (loss: 1.6436508893966675, acc: 0.5443037748336792)
[2025-02-05 13:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:06][root][INFO] - Training Epoch: 2/2, step 18999/23838 completed (loss: 1.821423053741455, acc: 0.5076923370361328)
[2025-02-05 13:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:06][root][INFO] - Training Epoch: 2/2, step 19000/23838 completed (loss: 1.2125860452651978, acc: 0.6703296899795532)
[2025-02-05 13:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:07][root][INFO] - Training Epoch: 2/2, step 19001/23838 completed (loss: 1.4487422704696655, acc: 0.6486486196517944)
[2025-02-05 13:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:07][root][INFO] - Training Epoch: 2/2, step 19002/23838 completed (loss: 1.4269706010818481, acc: 0.5974025726318359)
[2025-02-05 13:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:07][root][INFO] - Training Epoch: 2/2, step 19003/23838 completed (loss: 1.446060299873352, acc: 0.52173912525177)
[2025-02-05 13:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:08][root][INFO] - Training Epoch: 2/2, step 19004/23838 completed (loss: 0.8716191649436951, acc: 0.7692307829856873)
[2025-02-05 13:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:08][root][INFO] - Training Epoch: 2/2, step 19005/23838 completed (loss: 1.3982733488082886, acc: 0.6315789222717285)
[2025-02-05 13:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:09][root][INFO] - Training Epoch: 2/2, step 19006/23838 completed (loss: 1.3834855556488037, acc: 0.6576576828956604)
[2025-02-05 13:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:09][root][INFO] - Training Epoch: 2/2, step 19007/23838 completed (loss: 1.2162728309631348, acc: 0.6666666865348816)
[2025-02-05 13:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:09][root][INFO] - Training Epoch: 2/2, step 19008/23838 completed (loss: 1.4452576637268066, acc: 0.650602400302887)
[2025-02-05 13:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:10][root][INFO] - Training Epoch: 2/2, step 19009/23838 completed (loss: 1.5079655647277832, acc: 0.5909090638160706)
[2025-02-05 13:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:10][root][INFO] - Training Epoch: 2/2, step 19010/23838 completed (loss: 1.2278800010681152, acc: 0.6388888955116272)
[2025-02-05 13:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:10][root][INFO] - Training Epoch: 2/2, step 19011/23838 completed (loss: 1.2218211889266968, acc: 0.568965494632721)
[2025-02-05 13:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:11][root][INFO] - Training Epoch: 2/2, step 19012/23838 completed (loss: 1.239963412284851, acc: 0.6122449040412903)
[2025-02-05 13:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:11][root][INFO] - Training Epoch: 2/2, step 19013/23838 completed (loss: 1.5838912725448608, acc: 0.5333333611488342)
[2025-02-05 13:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:12][root][INFO] - Training Epoch: 2/2, step 19014/23838 completed (loss: 1.3060410022735596, acc: 0.6219512224197388)
[2025-02-05 13:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:12][root][INFO] - Training Epoch: 2/2, step 19015/23838 completed (loss: 1.1442290544509888, acc: 0.7126436829566956)
[2025-02-05 13:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:12][root][INFO] - Training Epoch: 2/2, step 19016/23838 completed (loss: 1.0588293075561523, acc: 0.6000000238418579)
[2025-02-05 13:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:13][root][INFO] - Training Epoch: 2/2, step 19017/23838 completed (loss: 1.3160080909729004, acc: 0.625)
[2025-02-05 13:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:13][root][INFO] - Training Epoch: 2/2, step 19018/23838 completed (loss: 1.2825398445129395, acc: 0.6140350699424744)
[2025-02-05 13:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:14][root][INFO] - Training Epoch: 2/2, step 19019/23838 completed (loss: 0.9695419669151306, acc: 0.8039215803146362)
[2025-02-05 13:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:14][root][INFO] - Training Epoch: 2/2, step 19020/23838 completed (loss: 1.4649988412857056, acc: 0.6229507923126221)
[2025-02-05 13:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:14][root][INFO] - Training Epoch: 2/2, step 19021/23838 completed (loss: 1.1139582395553589, acc: 0.6935483813285828)
[2025-02-05 13:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:15][root][INFO] - Training Epoch: 2/2, step 19022/23838 completed (loss: 1.1206915378570557, acc: 0.7108433842658997)
[2025-02-05 13:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:15][root][INFO] - Training Epoch: 2/2, step 19023/23838 completed (loss: 1.5122212171554565, acc: 0.5975610017776489)
[2025-02-05 13:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:16][root][INFO] - Training Epoch: 2/2, step 19024/23838 completed (loss: 1.5266493558883667, acc: 0.5189873576164246)
[2025-02-05 13:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:16][root][INFO] - Training Epoch: 2/2, step 19025/23838 completed (loss: 1.5100343227386475, acc: 0.5809524059295654)
[2025-02-05 13:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:16][root][INFO] - Training Epoch: 2/2, step 19026/23838 completed (loss: 1.328778624534607, acc: 0.6530612111091614)
[2025-02-05 13:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:17][root][INFO] - Training Epoch: 2/2, step 19027/23838 completed (loss: 1.1410456895828247, acc: 0.6913580298423767)
[2025-02-05 13:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:17][root][INFO] - Training Epoch: 2/2, step 19028/23838 completed (loss: 1.4581851959228516, acc: 0.5681818127632141)
[2025-02-05 13:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:17][root][INFO] - Training Epoch: 2/2, step 19029/23838 completed (loss: 1.4831362962722778, acc: 0.6000000238418579)
[2025-02-05 13:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:18][root][INFO] - Training Epoch: 2/2, step 19030/23838 completed (loss: 1.6976584196090698, acc: 0.5324675440788269)
[2025-02-05 13:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:18][root][INFO] - Training Epoch: 2/2, step 19031/23838 completed (loss: 1.4007139205932617, acc: 0.5901639461517334)
[2025-02-05 13:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:19][root][INFO] - Training Epoch: 2/2, step 19032/23838 completed (loss: 1.5065765380859375, acc: 0.6019417643547058)
[2025-02-05 13:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:19][root][INFO] - Training Epoch: 2/2, step 19033/23838 completed (loss: 1.8010648488998413, acc: 0.5762711763381958)
[2025-02-05 13:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:20][root][INFO] - Training Epoch: 2/2, step 19034/23838 completed (loss: 1.2869930267333984, acc: 0.6276595592498779)
[2025-02-05 13:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:20][root][INFO] - Training Epoch: 2/2, step 19035/23838 completed (loss: 1.8031237125396729, acc: 0.5)
[2025-02-05 13:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:20][root][INFO] - Training Epoch: 2/2, step 19036/23838 completed (loss: 1.0999484062194824, acc: 0.6122449040412903)
[2025-02-05 13:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:21][root][INFO] - Training Epoch: 2/2, step 19037/23838 completed (loss: 1.1119078397750854, acc: 0.6705882549285889)
[2025-02-05 13:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:21][root][INFO] - Training Epoch: 2/2, step 19038/23838 completed (loss: 1.4045934677124023, acc: 0.5955055952072144)
[2025-02-05 13:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:22][root][INFO] - Training Epoch: 2/2, step 19039/23838 completed (loss: 1.6267004013061523, acc: 0.6101694703102112)
[2025-02-05 13:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:22][root][INFO] - Training Epoch: 2/2, step 19040/23838 completed (loss: 1.454277753829956, acc: 0.5111111402511597)
[2025-02-05 13:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:23][root][INFO] - Training Epoch: 2/2, step 19041/23838 completed (loss: 1.6787019968032837, acc: 0.5438596606254578)
[2025-02-05 13:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:23][root][INFO] - Training Epoch: 2/2, step 19042/23838 completed (loss: 1.1476117372512817, acc: 0.6581196784973145)
[2025-02-05 13:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:24][root][INFO] - Training Epoch: 2/2, step 19043/23838 completed (loss: 1.09290611743927, acc: 0.6442307829856873)
[2025-02-05 13:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:24][root][INFO] - Training Epoch: 2/2, step 19044/23838 completed (loss: 1.4925061464309692, acc: 0.5675675868988037)
[2025-02-05 13:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:25][root][INFO] - Training Epoch: 2/2, step 19045/23838 completed (loss: 1.7302671670913696, acc: 0.5714285969734192)
[2025-02-05 13:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:25][root][INFO] - Training Epoch: 2/2, step 19046/23838 completed (loss: 1.448879361152649, acc: 0.5984848737716675)
[2025-02-05 13:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:26][root][INFO] - Training Epoch: 2/2, step 19047/23838 completed (loss: 1.6312237977981567, acc: 0.5784313678741455)
[2025-02-05 13:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:26][root][INFO] - Training Epoch: 2/2, step 19048/23838 completed (loss: 1.7338186502456665, acc: 0.5454545617103577)
[2025-02-05 13:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:27][root][INFO] - Training Epoch: 2/2, step 19049/23838 completed (loss: 1.0297529697418213, acc: 0.6744186282157898)
[2025-02-05 13:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:27][root][INFO] - Training Epoch: 2/2, step 19050/23838 completed (loss: 1.3180058002471924, acc: 0.6761904954910278)
[2025-02-05 13:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:28][root][INFO] - Training Epoch: 2/2, step 19051/23838 completed (loss: 1.382651448249817, acc: 0.6139240264892578)
[2025-02-05 13:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:28][root][INFO] - Training Epoch: 2/2, step 19052/23838 completed (loss: 1.2631386518478394, acc: 0.6744186282157898)
[2025-02-05 13:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:28][root][INFO] - Training Epoch: 2/2, step 19053/23838 completed (loss: 1.2873855829238892, acc: 0.6285714507102966)
[2025-02-05 13:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:29][root][INFO] - Training Epoch: 2/2, step 19054/23838 completed (loss: 1.1355621814727783, acc: 0.6714285612106323)
[2025-02-05 13:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:29][root][INFO] - Training Epoch: 2/2, step 19055/23838 completed (loss: 1.4048032760620117, acc: 0.5454545617103577)
[2025-02-05 13:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:30][root][INFO] - Training Epoch: 2/2, step 19056/23838 completed (loss: 1.5710093975067139, acc: 0.6153846383094788)
[2025-02-05 13:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:30][root][INFO] - Training Epoch: 2/2, step 19057/23838 completed (loss: 1.1674646139144897, acc: 0.664383590221405)
[2025-02-05 13:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:31][root][INFO] - Training Epoch: 2/2, step 19058/23838 completed (loss: 1.504602074623108, acc: 0.5571428537368774)
[2025-02-05 13:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:31][root][INFO] - Training Epoch: 2/2, step 19059/23838 completed (loss: 1.9797348976135254, acc: 0.4954128563404083)
[2025-02-05 13:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:31][root][INFO] - Training Epoch: 2/2, step 19060/23838 completed (loss: 1.7373440265655518, acc: 0.49572649598121643)
[2025-02-05 13:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:32][root][INFO] - Training Epoch: 2/2, step 19061/23838 completed (loss: 1.448127269744873, acc: 0.557692289352417)
[2025-02-05 13:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:32][root][INFO] - Training Epoch: 2/2, step 19062/23838 completed (loss: 0.9341541528701782, acc: 0.75)
[2025-02-05 13:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:33][root][INFO] - Training Epoch: 2/2, step 19063/23838 completed (loss: 1.540283203125, acc: 0.5904762148857117)
[2025-02-05 13:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:33][root][INFO] - Training Epoch: 2/2, step 19064/23838 completed (loss: 1.1042392253875732, acc: 0.7532467246055603)
[2025-02-05 13:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:34][root][INFO] - Training Epoch: 2/2, step 19065/23838 completed (loss: 1.4668388366699219, acc: 0.5483871102333069)
[2025-02-05 13:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:34][root][INFO] - Training Epoch: 2/2, step 19066/23838 completed (loss: 1.391664743423462, acc: 0.6190476417541504)
[2025-02-05 13:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:35][root][INFO] - Training Epoch: 2/2, step 19067/23838 completed (loss: 1.5467485189437866, acc: 0.6016260385513306)
[2025-02-05 13:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:35][root][INFO] - Training Epoch: 2/2, step 19068/23838 completed (loss: 1.7099367380142212, acc: 0.5050504803657532)
[2025-02-05 13:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:36][root][INFO] - Training Epoch: 2/2, step 19069/23838 completed (loss: 1.3767223358154297, acc: 0.5909090638160706)
[2025-02-05 13:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:36][root][INFO] - Training Epoch: 2/2, step 19070/23838 completed (loss: 1.4584884643554688, acc: 0.6195651888847351)
[2025-02-05 13:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:37][root][INFO] - Training Epoch: 2/2, step 19071/23838 completed (loss: 1.3168601989746094, acc: 0.6287878751754761)
[2025-02-05 13:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:37][root][INFO] - Training Epoch: 2/2, step 19072/23838 completed (loss: 1.4943721294403076, acc: 0.5480769276618958)
[2025-02-05 13:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:37][root][INFO] - Training Epoch: 2/2, step 19073/23838 completed (loss: 0.8668066263198853, acc: 0.75)
[2025-02-05 13:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:38][root][INFO] - Training Epoch: 2/2, step 19074/23838 completed (loss: 1.4298713207244873, acc: 0.568965494632721)
[2025-02-05 13:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:38][root][INFO] - Training Epoch: 2/2, step 19075/23838 completed (loss: 1.4640679359436035, acc: 0.5699999928474426)
[2025-02-05 13:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:39][root][INFO] - Training Epoch: 2/2, step 19076/23838 completed (loss: 1.5930097103118896, acc: 0.5546875)
[2025-02-05 13:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:39][root][INFO] - Training Epoch: 2/2, step 19077/23838 completed (loss: 1.3558881282806396, acc: 0.6266666650772095)
[2025-02-05 13:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:40][root][INFO] - Training Epoch: 2/2, step 19078/23838 completed (loss: 1.2924319505691528, acc: 0.6907216310501099)
[2025-02-05 13:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:40][root][INFO] - Training Epoch: 2/2, step 19079/23838 completed (loss: 1.92776620388031, acc: 0.5)
[2025-02-05 13:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:40][root][INFO] - Training Epoch: 2/2, step 19080/23838 completed (loss: 1.3833603858947754, acc: 0.60317462682724)
[2025-02-05 13:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:41][root][INFO] - Training Epoch: 2/2, step 19081/23838 completed (loss: 1.4516890048980713, acc: 0.5666666626930237)
[2025-02-05 13:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:41][root][INFO] - Training Epoch: 2/2, step 19082/23838 completed (loss: 1.4381831884384155, acc: 0.5873016119003296)
[2025-02-05 13:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:41][root][INFO] - Training Epoch: 2/2, step 19083/23838 completed (loss: 1.5621905326843262, acc: 0.5232558250427246)
[2025-02-05 13:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:42][root][INFO] - Training Epoch: 2/2, step 19084/23838 completed (loss: 1.4849246740341187, acc: 0.5131579041481018)
[2025-02-05 13:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:42][root][INFO] - Training Epoch: 2/2, step 19085/23838 completed (loss: 1.5130598545074463, acc: 0.585106372833252)
[2025-02-05 13:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:43][root][INFO] - Training Epoch: 2/2, step 19086/23838 completed (loss: 1.5470986366271973, acc: 0.5899999737739563)
[2025-02-05 13:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:43][root][INFO] - Training Epoch: 2/2, step 19087/23838 completed (loss: 1.431599736213684, acc: 0.6330274939537048)
[2025-02-05 13:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:43][root][INFO] - Training Epoch: 2/2, step 19088/23838 completed (loss: 1.3779098987579346, acc: 0.5760869383811951)
[2025-02-05 13:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:44][root][INFO] - Training Epoch: 2/2, step 19089/23838 completed (loss: 1.5936325788497925, acc: 0.5121951103210449)
[2025-02-05 13:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:44][root][INFO] - Training Epoch: 2/2, step 19090/23838 completed (loss: 1.7749279737472534, acc: 0.46590909361839294)
[2025-02-05 13:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:45][root][INFO] - Training Epoch: 2/2, step 19091/23838 completed (loss: 1.1671037673950195, acc: 0.6617646813392639)
[2025-02-05 13:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:45][root][INFO] - Training Epoch: 2/2, step 19092/23838 completed (loss: 1.1292829513549805, acc: 0.7142857313156128)
[2025-02-05 13:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:45][root][INFO] - Training Epoch: 2/2, step 19093/23838 completed (loss: 1.24875009059906, acc: 0.5882353186607361)
[2025-02-05 13:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:46][root][INFO] - Training Epoch: 2/2, step 19094/23838 completed (loss: 1.7294080257415771, acc: 0.5211267471313477)
[2025-02-05 13:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:46][root][INFO] - Training Epoch: 2/2, step 19095/23838 completed (loss: 1.3528388738632202, acc: 0.6164383292198181)
[2025-02-05 13:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:46][root][INFO] - Training Epoch: 2/2, step 19096/23838 completed (loss: 1.2313271760940552, acc: 0.65625)
[2025-02-05 13:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:47][root][INFO] - Training Epoch: 2/2, step 19097/23838 completed (loss: 1.3751425743103027, acc: 0.5591397881507874)
[2025-02-05 13:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:47][root][INFO] - Training Epoch: 2/2, step 19098/23838 completed (loss: 1.245162844657898, acc: 0.6491228342056274)
[2025-02-05 13:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:48][root][INFO] - Training Epoch: 2/2, step 19099/23838 completed (loss: 1.556330680847168, acc: 0.5428571701049805)
[2025-02-05 13:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:48][root][INFO] - Training Epoch: 2/2, step 19100/23838 completed (loss: 1.381341576576233, acc: 0.6354166865348816)
[2025-02-05 13:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:48][root][INFO] - Training Epoch: 2/2, step 19101/23838 completed (loss: 1.2072865962982178, acc: 0.6296296119689941)
[2025-02-05 13:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:49][root][INFO] - Training Epoch: 2/2, step 19102/23838 completed (loss: 1.2069870233535767, acc: 0.6451612710952759)
[2025-02-05 13:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:49][root][INFO] - Training Epoch: 2/2, step 19103/23838 completed (loss: 1.5896178483963013, acc: 0.5471698045730591)
[2025-02-05 13:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:49][root][INFO] - Training Epoch: 2/2, step 19104/23838 completed (loss: 1.508626937866211, acc: 0.5656565427780151)
[2025-02-05 13:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:50][root][INFO] - Training Epoch: 2/2, step 19105/23838 completed (loss: 0.9908085465431213, acc: 0.7166666388511658)
[2025-02-05 13:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:50][root][INFO] - Training Epoch: 2/2, step 19106/23838 completed (loss: 1.620536208152771, acc: 0.5609756112098694)
[2025-02-05 13:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:51][root][INFO] - Training Epoch: 2/2, step 19107/23838 completed (loss: 1.418138861656189, acc: 0.6060606241226196)
[2025-02-05 13:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:51][root][INFO] - Training Epoch: 2/2, step 19108/23838 completed (loss: 1.2062479257583618, acc: 0.6351351141929626)
[2025-02-05 13:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:51][root][INFO] - Training Epoch: 2/2, step 19109/23838 completed (loss: 1.0964844226837158, acc: 0.7246376872062683)
[2025-02-05 13:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:52][root][INFO] - Training Epoch: 2/2, step 19110/23838 completed (loss: 1.3658866882324219, acc: 0.6477272510528564)
[2025-02-05 13:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:52][root][INFO] - Training Epoch: 2/2, step 19111/23838 completed (loss: 1.2274415493011475, acc: 0.6632652878761292)
[2025-02-05 13:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:53][root][INFO] - Training Epoch: 2/2, step 19112/23838 completed (loss: 1.1042243242263794, acc: 0.6849315166473389)
[2025-02-05 13:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:53][root][INFO] - Training Epoch: 2/2, step 19113/23838 completed (loss: 1.3212333917617798, acc: 0.6181818246841431)
[2025-02-05 13:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:54][root][INFO] - Training Epoch: 2/2, step 19114/23838 completed (loss: 1.120195746421814, acc: 0.6847826242446899)
[2025-02-05 13:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:54][root][INFO] - Training Epoch: 2/2, step 19115/23838 completed (loss: 1.4418374300003052, acc: 0.5899999737739563)
[2025-02-05 13:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:54][root][INFO] - Training Epoch: 2/2, step 19116/23838 completed (loss: 0.9806002974510193, acc: 0.7948718070983887)
[2025-02-05 13:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:55][root][INFO] - Training Epoch: 2/2, step 19117/23838 completed (loss: 2.111931324005127, acc: 0.47058823704719543)
[2025-02-05 13:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:55][root][INFO] - Training Epoch: 2/2, step 19118/23838 completed (loss: 0.84841388463974, acc: 0.8048780560493469)
[2025-02-05 13:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:56][root][INFO] - Training Epoch: 2/2, step 19119/23838 completed (loss: 0.9419758915901184, acc: 0.7647058963775635)
[2025-02-05 13:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:56][root][INFO] - Training Epoch: 2/2, step 19120/23838 completed (loss: 1.0878328084945679, acc: 0.6666666865348816)
[2025-02-05 13:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:56][root][INFO] - Training Epoch: 2/2, step 19121/23838 completed (loss: 1.1779474020004272, acc: 0.5357142686843872)
[2025-02-05 13:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:57][root][INFO] - Training Epoch: 2/2, step 19122/23838 completed (loss: 1.3107260465621948, acc: 0.6875)
[2025-02-05 13:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:57][root][INFO] - Training Epoch: 2/2, step 19123/23838 completed (loss: 1.097934365272522, acc: 0.7222222089767456)
[2025-02-05 13:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:57][root][INFO] - Training Epoch: 2/2, step 19124/23838 completed (loss: 0.48245322704315186, acc: 0.9230769276618958)
[2025-02-05 13:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:58][root][INFO] - Training Epoch: 2/2, step 19125/23838 completed (loss: 1.1623389720916748, acc: 0.692307710647583)
[2025-02-05 13:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:58][root][INFO] - Training Epoch: 2/2, step 19126/23838 completed (loss: 0.7255541086196899, acc: 0.8787878751754761)
[2025-02-05 13:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:58][root][INFO] - Training Epoch: 2/2, step 19127/23838 completed (loss: 1.8120132684707642, acc: 0.4903225898742676)
[2025-02-05 13:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:59][root][INFO] - Training Epoch: 2/2, step 19128/23838 completed (loss: 1.430025577545166, acc: 0.5555555820465088)
[2025-02-05 13:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:47:59][root][INFO] - Training Epoch: 2/2, step 19129/23838 completed (loss: 1.4507055282592773, acc: 0.6012658476829529)
[2025-02-05 13:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:00][root][INFO] - Training Epoch: 2/2, step 19130/23838 completed (loss: 1.7663676738739014, acc: 0.4838709533214569)
[2025-02-05 13:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:00][root][INFO] - Training Epoch: 2/2, step 19131/23838 completed (loss: 1.711818814277649, acc: 0.5641025900840759)
[2025-02-05 13:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:00][root][INFO] - Training Epoch: 2/2, step 19132/23838 completed (loss: 1.0035040378570557, acc: 0.7216494679450989)
[2025-02-05 13:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:01][root][INFO] - Training Epoch: 2/2, step 19133/23838 completed (loss: 1.3291536569595337, acc: 0.636904776096344)
[2025-02-05 13:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:01][root][INFO] - Training Epoch: 2/2, step 19134/23838 completed (loss: 1.5687382221221924, acc: 0.5362318754196167)
[2025-02-05 13:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:01][root][INFO] - Training Epoch: 2/2, step 19135/23838 completed (loss: 1.0865381956100464, acc: 0.6630434989929199)
[2025-02-05 13:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:02][root][INFO] - Training Epoch: 2/2, step 19136/23838 completed (loss: 1.2756932973861694, acc: 0.6292135119438171)
[2025-02-05 13:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:02][root][INFO] - Training Epoch: 2/2, step 19137/23838 completed (loss: 1.3044776916503906, acc: 0.5981308221817017)
[2025-02-05 13:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:02][root][INFO] - Training Epoch: 2/2, step 19138/23838 completed (loss: 1.450798511505127, acc: 0.568965494632721)
[2025-02-05 13:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:03][root][INFO] - Training Epoch: 2/2, step 19139/23838 completed (loss: 1.2268123626708984, acc: 0.692307710647583)
[2025-02-05 13:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:03][root][INFO] - Training Epoch: 2/2, step 19140/23838 completed (loss: 1.5606029033660889, acc: 0.5754716992378235)
[2025-02-05 13:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:04][root][INFO] - Training Epoch: 2/2, step 19141/23838 completed (loss: 1.1003389358520508, acc: 0.7218044996261597)
[2025-02-05 13:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:04][root][INFO] - Training Epoch: 2/2, step 19142/23838 completed (loss: 1.1652138233184814, acc: 0.6559139490127563)
[2025-02-05 13:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:04][root][INFO] - Training Epoch: 2/2, step 19143/23838 completed (loss: 1.0882070064544678, acc: 0.699999988079071)
[2025-02-05 13:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:05][root][INFO] - Training Epoch: 2/2, step 19144/23838 completed (loss: 1.4613016843795776, acc: 0.6067415475845337)
[2025-02-05 13:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:05][root][INFO] - Training Epoch: 2/2, step 19145/23838 completed (loss: 1.1240589618682861, acc: 0.6973684430122375)
[2025-02-05 13:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:05][root][INFO] - Training Epoch: 2/2, step 19146/23838 completed (loss: 1.2741551399230957, acc: 0.6293103694915771)
[2025-02-05 13:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:06][root][INFO] - Training Epoch: 2/2, step 19147/23838 completed (loss: 1.3798894882202148, acc: 0.6153846383094788)
[2025-02-05 13:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:06][root][INFO] - Training Epoch: 2/2, step 19148/23838 completed (loss: 1.4834402799606323, acc: 0.6000000238418579)
[2025-02-05 13:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:07][root][INFO] - Training Epoch: 2/2, step 19149/23838 completed (loss: 1.0872365236282349, acc: 0.6702127456665039)
[2025-02-05 13:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:07][root][INFO] - Training Epoch: 2/2, step 19150/23838 completed (loss: 1.1912026405334473, acc: 0.6415094137191772)
[2025-02-05 13:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:07][root][INFO] - Training Epoch: 2/2, step 19151/23838 completed (loss: 0.8861680030822754, acc: 0.7758620977401733)
[2025-02-05 13:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:08][root][INFO] - Training Epoch: 2/2, step 19152/23838 completed (loss: 1.1749632358551025, acc: 0.7023809552192688)
[2025-02-05 13:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:08][root][INFO] - Training Epoch: 2/2, step 19153/23838 completed (loss: 0.9868323802947998, acc: 0.6746987700462341)
[2025-02-05 13:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:09][root][INFO] - Training Epoch: 2/2, step 19154/23838 completed (loss: 0.6422309279441833, acc: 0.8095238208770752)
[2025-02-05 13:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:09][root][INFO] - Training Epoch: 2/2, step 19155/23838 completed (loss: 0.8398891091346741, acc: 0.7636363506317139)
[2025-02-05 13:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:09][root][INFO] - Training Epoch: 2/2, step 19156/23838 completed (loss: 1.173020839691162, acc: 0.692307710647583)
[2025-02-05 13:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:10][root][INFO] - Training Epoch: 2/2, step 19157/23838 completed (loss: 1.2192965745925903, acc: 0.6600000262260437)
[2025-02-05 13:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:10][root][INFO] - Training Epoch: 2/2, step 19158/23838 completed (loss: 1.406699776649475, acc: 0.5845410823822021)
[2025-02-05 13:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:11][root][INFO] - Training Epoch: 2/2, step 19159/23838 completed (loss: 1.488170862197876, acc: 0.5967742204666138)
[2025-02-05 13:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:11][root][INFO] - Training Epoch: 2/2, step 19160/23838 completed (loss: 1.256102204322815, acc: 0.6732673048973083)
[2025-02-05 13:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:11][root][INFO] - Training Epoch: 2/2, step 19161/23838 completed (loss: 1.3212414979934692, acc: 0.6496815085411072)
[2025-02-05 13:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:12][root][INFO] - Training Epoch: 2/2, step 19162/23838 completed (loss: 1.2763268947601318, acc: 0.6438356041908264)
[2025-02-05 13:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:12][root][INFO] - Training Epoch: 2/2, step 19163/23838 completed (loss: 1.3098448514938354, acc: 0.6470588445663452)
[2025-02-05 13:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:13][root][INFO] - Training Epoch: 2/2, step 19164/23838 completed (loss: 1.3425920009613037, acc: 0.6195651888847351)
[2025-02-05 13:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:13][root][INFO] - Training Epoch: 2/2, step 19165/23838 completed (loss: 1.2116492986679077, acc: 0.6283186078071594)
[2025-02-05 13:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:13][root][INFO] - Training Epoch: 2/2, step 19166/23838 completed (loss: 1.5809648036956787, acc: 0.5652173757553101)
[2025-02-05 13:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:14][root][INFO] - Training Epoch: 2/2, step 19167/23838 completed (loss: 1.3267589807510376, acc: 0.6496350169181824)
[2025-02-05 13:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:14][root][INFO] - Training Epoch: 2/2, step 19168/23838 completed (loss: 1.7556403875350952, acc: 0.5636363625526428)
[2025-02-05 13:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:15][root][INFO] - Training Epoch: 2/2, step 19169/23838 completed (loss: 1.1445504426956177, acc: 0.75)
[2025-02-05 13:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:15][root][INFO] - Training Epoch: 2/2, step 19170/23838 completed (loss: 0.970094621181488, acc: 0.7564102411270142)
[2025-02-05 13:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:15][root][INFO] - Training Epoch: 2/2, step 19171/23838 completed (loss: 1.1569691896438599, acc: 0.6790123581886292)
[2025-02-05 13:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:16][root][INFO] - Training Epoch: 2/2, step 19172/23838 completed (loss: 1.3073161840438843, acc: 0.6557376980781555)
[2025-02-05 13:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:16][root][INFO] - Training Epoch: 2/2, step 19173/23838 completed (loss: 1.2821526527404785, acc: 0.5866666436195374)
[2025-02-05 13:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:16][root][INFO] - Training Epoch: 2/2, step 19174/23838 completed (loss: 1.0141433477401733, acc: 0.717391312122345)
[2025-02-05 13:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:17][root][INFO] - Training Epoch: 2/2, step 19175/23838 completed (loss: 1.1398240327835083, acc: 0.6793892979621887)
[2025-02-05 13:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:17][root][INFO] - Training Epoch: 2/2, step 19176/23838 completed (loss: 1.5071731805801392, acc: 0.5380434989929199)
[2025-02-05 13:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:17][root][INFO] - Training Epoch: 2/2, step 19177/23838 completed (loss: 1.0669323205947876, acc: 0.6774193644523621)
[2025-02-05 13:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:18][root][INFO] - Training Epoch: 2/2, step 19178/23838 completed (loss: 1.3545329570770264, acc: 0.625)
[2025-02-05 13:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:18][root][INFO] - Training Epoch: 2/2, step 19179/23838 completed (loss: 1.2787225246429443, acc: 0.6495726704597473)
[2025-02-05 13:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:19][root][INFO] - Training Epoch: 2/2, step 19180/23838 completed (loss: 1.2557024955749512, acc: 0.6415094137191772)
[2025-02-05 13:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:19][root][INFO] - Training Epoch: 2/2, step 19181/23838 completed (loss: 1.303733468055725, acc: 0.6036036014556885)
[2025-02-05 13:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:19][root][INFO] - Training Epoch: 2/2, step 19182/23838 completed (loss: 1.4480414390563965, acc: 0.5888888835906982)
[2025-02-05 13:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:20][root][INFO] - Training Epoch: 2/2, step 19183/23838 completed (loss: 1.0507985353469849, acc: 0.6470588445663452)
[2025-02-05 13:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:20][root][INFO] - Training Epoch: 2/2, step 19184/23838 completed (loss: 1.4611139297485352, acc: 0.5669291615486145)
[2025-02-05 13:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:21][root][INFO] - Training Epoch: 2/2, step 19185/23838 completed (loss: 1.239708423614502, acc: 0.6476190686225891)
[2025-02-05 13:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:21][root][INFO] - Training Epoch: 2/2, step 19186/23838 completed (loss: 1.334215521812439, acc: 0.5569620132446289)
[2025-02-05 13:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:21][root][INFO] - Training Epoch: 2/2, step 19187/23838 completed (loss: 1.399733304977417, acc: 0.6239316463470459)
[2025-02-05 13:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:22][root][INFO] - Training Epoch: 2/2, step 19188/23838 completed (loss: 1.3150602579116821, acc: 0.5945945978164673)
[2025-02-05 13:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:22][root][INFO] - Training Epoch: 2/2, step 19189/23838 completed (loss: 0.9533112049102783, acc: 0.746835470199585)
[2025-02-05 13:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:22][root][INFO] - Training Epoch: 2/2, step 19190/23838 completed (loss: 1.2861366271972656, acc: 0.5687500238418579)
[2025-02-05 13:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:23][root][INFO] - Training Epoch: 2/2, step 19191/23838 completed (loss: 0.9484269022941589, acc: 0.6941176652908325)
[2025-02-05 13:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:23][root][INFO] - Training Epoch: 2/2, step 19192/23838 completed (loss: 1.2428224086761475, acc: 0.625)
[2025-02-05 13:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:24][root][INFO] - Training Epoch: 2/2, step 19193/23838 completed (loss: 1.2594947814941406, acc: 0.6371681690216064)
[2025-02-05 13:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:24][root][INFO] - Training Epoch: 2/2, step 19194/23838 completed (loss: 1.3265514373779297, acc: 0.5789473652839661)
[2025-02-05 13:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:24][root][INFO] - Training Epoch: 2/2, step 19195/23838 completed (loss: 1.3930917978286743, acc: 0.6000000238418579)
[2025-02-05 13:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:25][root][INFO] - Training Epoch: 2/2, step 19196/23838 completed (loss: 1.1640336513519287, acc: 0.6538461446762085)
[2025-02-05 13:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:25][root][INFO] - Training Epoch: 2/2, step 19197/23838 completed (loss: 1.188679575920105, acc: 0.6666666865348816)
[2025-02-05 13:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:26][root][INFO] - Training Epoch: 2/2, step 19198/23838 completed (loss: 1.2404321432113647, acc: 0.6595744490623474)
[2025-02-05 13:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:26][root][INFO] - Training Epoch: 2/2, step 19199/23838 completed (loss: 1.1104305982589722, acc: 0.675000011920929)
[2025-02-05 13:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:26][root][INFO] - Training Epoch: 2/2, step 19200/23838 completed (loss: 1.3308886289596558, acc: 0.5670102834701538)
[2025-02-05 13:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:27][root][INFO] - Training Epoch: 2/2, step 19201/23838 completed (loss: 1.3308106660842896, acc: 0.6354166865348816)
[2025-02-05 13:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:27][root][INFO] - Training Epoch: 2/2, step 19202/23838 completed (loss: 1.3463109731674194, acc: 0.594936728477478)
[2025-02-05 13:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:27][root][INFO] - Training Epoch: 2/2, step 19203/23838 completed (loss: 1.3138827085494995, acc: 0.6265060305595398)
[2025-02-05 13:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:28][root][INFO] - Training Epoch: 2/2, step 19204/23838 completed (loss: 1.383270025253296, acc: 0.5789473652839661)
[2025-02-05 13:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:28][root][INFO] - Training Epoch: 2/2, step 19205/23838 completed (loss: 1.0964103937149048, acc: 0.6455696225166321)
[2025-02-05 13:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:29][root][INFO] - Training Epoch: 2/2, step 19206/23838 completed (loss: 1.1589972972869873, acc: 0.7386363744735718)
[2025-02-05 13:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:29][root][INFO] - Training Epoch: 2/2, step 19207/23838 completed (loss: 1.1312729120254517, acc: 0.6551724076271057)
[2025-02-05 13:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:29][root][INFO] - Training Epoch: 2/2, step 19208/23838 completed (loss: 0.8525236248970032, acc: 0.7254902124404907)
[2025-02-05 13:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:30][root][INFO] - Training Epoch: 2/2, step 19209/23838 completed (loss: 1.29061758518219, acc: 0.5581395626068115)
[2025-02-05 13:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:30][root][INFO] - Training Epoch: 2/2, step 19210/23838 completed (loss: 0.908470630645752, acc: 0.7559055089950562)
[2025-02-05 13:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:30][root][INFO] - Training Epoch: 2/2, step 19211/23838 completed (loss: 1.1962004899978638, acc: 0.6666666865348816)
[2025-02-05 13:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:31][root][INFO] - Training Epoch: 2/2, step 19212/23838 completed (loss: 1.0342317819595337, acc: 0.6973684430122375)
[2025-02-05 13:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:31][root][INFO] - Training Epoch: 2/2, step 19213/23838 completed (loss: 1.7312697172164917, acc: 0.53125)
[2025-02-05 13:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:32][root][INFO] - Training Epoch: 2/2, step 19214/23838 completed (loss: 1.5943399667739868, acc: 0.5671641826629639)
[2025-02-05 13:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:32][root][INFO] - Training Epoch: 2/2, step 19215/23838 completed (loss: 1.5540776252746582, acc: 0.5789473652839661)
[2025-02-05 13:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:32][root][INFO] - Training Epoch: 2/2, step 19216/23838 completed (loss: 1.3709830045700073, acc: 0.6538461446762085)
[2025-02-05 13:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:33][root][INFO] - Training Epoch: 2/2, step 19217/23838 completed (loss: 0.957219660282135, acc: 0.6666666865348816)
[2025-02-05 13:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:33][root][INFO] - Training Epoch: 2/2, step 19218/23838 completed (loss: 1.2084009647369385, acc: 0.6346153616905212)
[2025-02-05 13:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:34][root][INFO] - Training Epoch: 2/2, step 19219/23838 completed (loss: 1.3261191844940186, acc: 0.6666666865348816)
[2025-02-05 13:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:34][root][INFO] - Training Epoch: 2/2, step 19220/23838 completed (loss: 0.9340357780456543, acc: 0.6764705777168274)
[2025-02-05 13:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:34][root][INFO] - Training Epoch: 2/2, step 19221/23838 completed (loss: 1.1206539869308472, acc: 0.6744186282157898)
[2025-02-05 13:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:35][root][INFO] - Training Epoch: 2/2, step 19222/23838 completed (loss: 1.4587023258209229, acc: 0.6025640964508057)
[2025-02-05 13:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:35][root][INFO] - Training Epoch: 2/2, step 19223/23838 completed (loss: 1.286353349685669, acc: 0.6111111044883728)
[2025-02-05 13:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:35][root][INFO] - Training Epoch: 2/2, step 19224/23838 completed (loss: 1.3104528188705444, acc: 0.65625)
[2025-02-05 13:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:36][root][INFO] - Training Epoch: 2/2, step 19225/23838 completed (loss: 1.6118648052215576, acc: 0.5571428537368774)
[2025-02-05 13:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:36][root][INFO] - Training Epoch: 2/2, step 19226/23838 completed (loss: 1.1698925495147705, acc: 0.6781609058380127)
[2025-02-05 13:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:37][root][INFO] - Training Epoch: 2/2, step 19227/23838 completed (loss: 1.5950257778167725, acc: 0.5540540814399719)
[2025-02-05 13:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:37][root][INFO] - Training Epoch: 2/2, step 19228/23838 completed (loss: 1.1710261106491089, acc: 0.7142857313156128)
[2025-02-05 13:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:37][root][INFO] - Training Epoch: 2/2, step 19229/23838 completed (loss: 1.090681791305542, acc: 0.6793892979621887)
[2025-02-05 13:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:38][root][INFO] - Training Epoch: 2/2, step 19230/23838 completed (loss: 0.811037003993988, acc: 0.8068181872367859)
[2025-02-05 13:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:38][root][INFO] - Training Epoch: 2/2, step 19231/23838 completed (loss: 1.4110267162322998, acc: 0.6399999856948853)
[2025-02-05 13:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:38][root][INFO] - Training Epoch: 2/2, step 19232/23838 completed (loss: 1.149112343788147, acc: 0.6399999856948853)
[2025-02-05 13:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:39][root][INFO] - Training Epoch: 2/2, step 19233/23838 completed (loss: 1.3181427717208862, acc: 0.5616438388824463)
[2025-02-05 13:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:39][root][INFO] - Training Epoch: 2/2, step 19234/23838 completed (loss: 1.4800441265106201, acc: 0.5428571701049805)
[2025-02-05 13:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:39][root][INFO] - Training Epoch: 2/2, step 19235/23838 completed (loss: 1.2604368925094604, acc: 0.6478873491287231)
[2025-02-05 13:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:40][root][INFO] - Training Epoch: 2/2, step 19236/23838 completed (loss: 1.0810052156448364, acc: 0.7272727489471436)
[2025-02-05 13:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:40][root][INFO] - Training Epoch: 2/2, step 19237/23838 completed (loss: 1.0597437620162964, acc: 0.7368420958518982)
[2025-02-05 13:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:40][root][INFO] - Training Epoch: 2/2, step 19238/23838 completed (loss: 0.735092282295227, acc: 0.7777777910232544)
[2025-02-05 13:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:41][root][INFO] - Training Epoch: 2/2, step 19239/23838 completed (loss: 1.1612224578857422, acc: 0.6666666865348816)
[2025-02-05 13:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:41][root][INFO] - Training Epoch: 2/2, step 19240/23838 completed (loss: 1.3331066370010376, acc: 0.6260162591934204)
[2025-02-05 13:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:42][root][INFO] - Training Epoch: 2/2, step 19241/23838 completed (loss: 0.9582845568656921, acc: 0.7266187071800232)
[2025-02-05 13:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:42][root][INFO] - Training Epoch: 2/2, step 19242/23838 completed (loss: 1.2009721994400024, acc: 0.6890756487846375)
[2025-02-05 13:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:42][root][INFO] - Training Epoch: 2/2, step 19243/23838 completed (loss: 0.9921849966049194, acc: 0.7078651785850525)
[2025-02-05 13:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:43][root][INFO] - Training Epoch: 2/2, step 19244/23838 completed (loss: 1.514689564704895, acc: 0.625)
[2025-02-05 13:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:43][root][INFO] - Training Epoch: 2/2, step 19245/23838 completed (loss: 1.2343966960906982, acc: 0.6836734414100647)
[2025-02-05 13:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:43][root][INFO] - Training Epoch: 2/2, step 19246/23838 completed (loss: 1.1609238386154175, acc: 0.6623376607894897)
[2025-02-05 13:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:44][root][INFO] - Training Epoch: 2/2, step 19247/23838 completed (loss: 1.0714778900146484, acc: 0.7196261882781982)
[2025-02-05 13:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:44][root][INFO] - Training Epoch: 2/2, step 19248/23838 completed (loss: 1.0006178617477417, acc: 0.70652174949646)
[2025-02-05 13:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:45][root][INFO] - Training Epoch: 2/2, step 19249/23838 completed (loss: 1.3862451314926147, acc: 0.60317462682724)
[2025-02-05 13:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:45][root][INFO] - Training Epoch: 2/2, step 19250/23838 completed (loss: 1.0915212631225586, acc: 0.641791045665741)
[2025-02-05 13:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:45][root][INFO] - Training Epoch: 2/2, step 19251/23838 completed (loss: 1.1755038499832153, acc: 0.6266666650772095)
[2025-02-05 13:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:46][root][INFO] - Training Epoch: 2/2, step 19252/23838 completed (loss: 1.231717586517334, acc: 0.7090908885002136)
[2025-02-05 13:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:46][root][INFO] - Training Epoch: 2/2, step 19253/23838 completed (loss: 1.2738091945648193, acc: 0.625)
[2025-02-05 13:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:46][root][INFO] - Training Epoch: 2/2, step 19254/23838 completed (loss: 1.1877690553665161, acc: 0.6521739363670349)
[2025-02-05 13:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:47][root][INFO] - Training Epoch: 2/2, step 19255/23838 completed (loss: 0.9401503801345825, acc: 0.7346938848495483)
[2025-02-05 13:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:47][root][INFO] - Training Epoch: 2/2, step 19256/23838 completed (loss: 1.345794916152954, acc: 0.5950413346290588)
[2025-02-05 13:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:47][root][INFO] - Training Epoch: 2/2, step 19257/23838 completed (loss: 1.5567944049835205, acc: 0.5214285850524902)
[2025-02-05 13:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:48][root][INFO] - Training Epoch: 2/2, step 19258/23838 completed (loss: 0.9952916502952576, acc: 0.7166666388511658)
[2025-02-05 13:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:48][root][INFO] - Training Epoch: 2/2, step 19259/23838 completed (loss: 1.1261974573135376, acc: 0.6829268336296082)
[2025-02-05 13:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:49][root][INFO] - Training Epoch: 2/2, step 19260/23838 completed (loss: 1.1784968376159668, acc: 0.6271186470985413)
[2025-02-05 13:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:49][root][INFO] - Training Epoch: 2/2, step 19261/23838 completed (loss: 1.1953318119049072, acc: 0.6625000238418579)
[2025-02-05 13:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:49][root][INFO] - Training Epoch: 2/2, step 19262/23838 completed (loss: 1.2814196348190308, acc: 0.6399999856948853)
[2025-02-05 13:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:50][root][INFO] - Training Epoch: 2/2, step 19263/23838 completed (loss: 1.069566011428833, acc: 0.6598639488220215)
[2025-02-05 13:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:50][root][INFO] - Training Epoch: 2/2, step 19264/23838 completed (loss: 1.058019757270813, acc: 0.6551724076271057)
[2025-02-05 13:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:51][root][INFO] - Training Epoch: 2/2, step 19265/23838 completed (loss: 1.3948975801467896, acc: 0.5833333134651184)
[2025-02-05 13:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:51][root][INFO] - Training Epoch: 2/2, step 19266/23838 completed (loss: 0.8423248529434204, acc: 0.7586206793785095)
[2025-02-05 13:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:51][root][INFO] - Training Epoch: 2/2, step 19267/23838 completed (loss: 0.9589101076126099, acc: 0.7010309100151062)
[2025-02-05 13:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:52][root][INFO] - Training Epoch: 2/2, step 19268/23838 completed (loss: 0.9888767600059509, acc: 0.6739130616188049)
[2025-02-05 13:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:52][root][INFO] - Training Epoch: 2/2, step 19269/23838 completed (loss: 1.2452623844146729, acc: 0.6410256624221802)
[2025-02-05 13:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:53][root][INFO] - Training Epoch: 2/2, step 19270/23838 completed (loss: 0.9669333696365356, acc: 0.72826087474823)
[2025-02-05 13:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:53][root][INFO] - Training Epoch: 2/2, step 19271/23838 completed (loss: 1.2833887338638306, acc: 0.625)
[2025-02-05 13:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:53][root][INFO] - Training Epoch: 2/2, step 19272/23838 completed (loss: 0.8819726705551147, acc: 0.7209302186965942)
[2025-02-05 13:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:54][root][INFO] - Training Epoch: 2/2, step 19273/23838 completed (loss: 1.4903355836868286, acc: 0.5681818127632141)
[2025-02-05 13:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:54][root][INFO] - Training Epoch: 2/2, step 19274/23838 completed (loss: 1.1779086589813232, acc: 0.6790123581886292)
[2025-02-05 13:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:55][root][INFO] - Training Epoch: 2/2, step 19275/23838 completed (loss: 0.8592607975006104, acc: 0.7263157963752747)
[2025-02-05 13:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:55][root][INFO] - Training Epoch: 2/2, step 19276/23838 completed (loss: 1.063293218612671, acc: 0.6973684430122375)
[2025-02-05 13:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:55][root][INFO] - Training Epoch: 2/2, step 19277/23838 completed (loss: 1.0551856756210327, acc: 0.6813187003135681)
[2025-02-05 13:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:56][root][INFO] - Training Epoch: 2/2, step 19278/23838 completed (loss: 1.201992392539978, acc: 0.6499999761581421)
[2025-02-05 13:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:56][root][INFO] - Training Epoch: 2/2, step 19279/23838 completed (loss: 1.0219206809997559, acc: 0.7345678806304932)
[2025-02-05 13:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:57][root][INFO] - Training Epoch: 2/2, step 19280/23838 completed (loss: 0.8978555202484131, acc: 0.7254902124404907)
[2025-02-05 13:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:57][root][INFO] - Training Epoch: 2/2, step 19281/23838 completed (loss: 1.024876594543457, acc: 0.725806474685669)
[2025-02-05 13:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:57][root][INFO] - Training Epoch: 2/2, step 19282/23838 completed (loss: 0.90321946144104, acc: 0.7317073345184326)
[2025-02-05 13:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:58][root][INFO] - Training Epoch: 2/2, step 19283/23838 completed (loss: 1.264017939567566, acc: 0.6541353464126587)
[2025-02-05 13:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:58][root][INFO] - Training Epoch: 2/2, step 19284/23838 completed (loss: 0.9903704524040222, acc: 0.6875)
[2025-02-05 13:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:59][root][INFO] - Training Epoch: 2/2, step 19285/23838 completed (loss: 1.2074867486953735, acc: 0.6754385828971863)
[2025-02-05 13:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:59][root][INFO] - Training Epoch: 2/2, step 19286/23838 completed (loss: 0.99834144115448, acc: 0.7246376872062683)
[2025-02-05 13:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:48:59][root][INFO] - Training Epoch: 2/2, step 19287/23838 completed (loss: 1.1079989671707153, acc: 0.6571428775787354)
[2025-02-05 13:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:00][root][INFO] - Training Epoch: 2/2, step 19288/23838 completed (loss: 1.04680335521698, acc: 0.6911764740943909)
[2025-02-05 13:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:00][root][INFO] - Training Epoch: 2/2, step 19289/23838 completed (loss: 1.0929348468780518, acc: 0.6610169410705566)
[2025-02-05 13:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:01][root][INFO] - Training Epoch: 2/2, step 19290/23838 completed (loss: 0.9413413405418396, acc: 0.6701030731201172)
[2025-02-05 13:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:01][root][INFO] - Training Epoch: 2/2, step 19291/23838 completed (loss: 1.2356791496276855, acc: 0.6891891956329346)
[2025-02-05 13:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:01][root][INFO] - Training Epoch: 2/2, step 19292/23838 completed (loss: 1.615514874458313, acc: 0.625)
[2025-02-05 13:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:02][root][INFO] - Training Epoch: 2/2, step 19293/23838 completed (loss: 0.8721326589584351, acc: 0.7894737124443054)
[2025-02-05 13:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:02][root][INFO] - Training Epoch: 2/2, step 19294/23838 completed (loss: 1.444454550743103, acc: 0.6170212626457214)
[2025-02-05 13:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:02][root][INFO] - Training Epoch: 2/2, step 19295/23838 completed (loss: 1.1033650636672974, acc: 0.6756756901741028)
[2025-02-05 13:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:03][root][INFO] - Training Epoch: 2/2, step 19296/23838 completed (loss: 1.3496830463409424, acc: 0.6857143044471741)
[2025-02-05 13:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:03][root][INFO] - Training Epoch: 2/2, step 19297/23838 completed (loss: 1.2655130624771118, acc: 0.6000000238418579)
[2025-02-05 13:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:04][root][INFO] - Training Epoch: 2/2, step 19298/23838 completed (loss: 1.0273404121398926, acc: 0.686274528503418)
[2025-02-05 13:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:04][root][INFO] - Training Epoch: 2/2, step 19299/23838 completed (loss: 1.3022007942199707, acc: 0.6516854166984558)
[2025-02-05 13:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:05][root][INFO] - Training Epoch: 2/2, step 19300/23838 completed (loss: 1.4057390689849854, acc: 0.6168224215507507)
[2025-02-05 13:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:05][root][INFO] - Training Epoch: 2/2, step 19301/23838 completed (loss: 1.4580014944076538, acc: 0.5571428537368774)
[2025-02-05 13:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:05][root][INFO] - Training Epoch: 2/2, step 19302/23838 completed (loss: 1.2938023805618286, acc: 0.5833333134651184)
[2025-02-05 13:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:06][root][INFO] - Training Epoch: 2/2, step 19303/23838 completed (loss: 1.1316571235656738, acc: 0.6521739363670349)
[2025-02-05 13:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:06][root][INFO] - Training Epoch: 2/2, step 19304/23838 completed (loss: 1.050166130065918, acc: 0.7096773982048035)
[2025-02-05 13:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:07][root][INFO] - Training Epoch: 2/2, step 19305/23838 completed (loss: 0.23397377133369446, acc: 0.9523809552192688)
[2025-02-05 13:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:07][root][INFO] - Training Epoch: 2/2, step 19306/23838 completed (loss: 1.106393814086914, acc: 0.6601941585540771)
[2025-02-05 13:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:07][root][INFO] - Training Epoch: 2/2, step 19307/23838 completed (loss: 0.7007231712341309, acc: 0.7653061151504517)
[2025-02-05 13:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:08][root][INFO] - Training Epoch: 2/2, step 19308/23838 completed (loss: 0.7950223684310913, acc: 0.7419354915618896)
[2025-02-05 13:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:08][root][INFO] - Training Epoch: 2/2, step 19309/23838 completed (loss: 0.9955005645751953, acc: 0.6666666865348816)
[2025-02-05 13:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:08][root][INFO] - Training Epoch: 2/2, step 19310/23838 completed (loss: 0.7962160110473633, acc: 0.8153846263885498)
[2025-02-05 13:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:09][root][INFO] - Training Epoch: 2/2, step 19311/23838 completed (loss: 0.8165684342384338, acc: 0.8064516186714172)
[2025-02-05 13:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:09][root][INFO] - Training Epoch: 2/2, step 19312/23838 completed (loss: 1.0166767835617065, acc: 0.662162184715271)
[2025-02-05 13:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:10][root][INFO] - Training Epoch: 2/2, step 19313/23838 completed (loss: 1.3160419464111328, acc: 0.6076555252075195)
[2025-02-05 13:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:10][root][INFO] - Training Epoch: 2/2, step 19314/23838 completed (loss: 1.3603631258010864, acc: 0.6315789222717285)
[2025-02-05 13:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:11][root][INFO] - Training Epoch: 2/2, step 19315/23838 completed (loss: 1.1791619062423706, acc: 0.6887417435646057)
[2025-02-05 13:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:11][root][INFO] - Training Epoch: 2/2, step 19316/23838 completed (loss: 0.881761908531189, acc: 0.7313432693481445)
[2025-02-05 13:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:11][root][INFO] - Training Epoch: 2/2, step 19317/23838 completed (loss: 1.0754203796386719, acc: 0.704081654548645)
[2025-02-05 13:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:12][root][INFO] - Training Epoch: 2/2, step 19318/23838 completed (loss: 0.7068575620651245, acc: 0.7731958627700806)
[2025-02-05 13:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:12][root][INFO] - Training Epoch: 2/2, step 19319/23838 completed (loss: 1.2276111841201782, acc: 0.6600000262260437)
[2025-02-05 13:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:12][root][INFO] - Training Epoch: 2/2, step 19320/23838 completed (loss: 1.043791651725769, acc: 0.7062937021255493)
[2025-02-05 13:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:13][root][INFO] - Training Epoch: 2/2, step 19321/23838 completed (loss: 0.8881466388702393, acc: 0.7288135886192322)
[2025-02-05 13:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:13][root][INFO] - Training Epoch: 2/2, step 19322/23838 completed (loss: 0.8037164211273193, acc: 0.7575757503509521)
[2025-02-05 13:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:14][root][INFO] - Training Epoch: 2/2, step 19323/23838 completed (loss: 0.8579937815666199, acc: 0.7636363506317139)
[2025-02-05 13:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:14][root][INFO] - Training Epoch: 2/2, step 19324/23838 completed (loss: 0.8681289553642273, acc: 0.7352941036224365)
[2025-02-05 13:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:14][root][INFO] - Training Epoch: 2/2, step 19325/23838 completed (loss: 0.9176046252250671, acc: 0.7291666865348816)
[2025-02-05 13:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:15][root][INFO] - Training Epoch: 2/2, step 19326/23838 completed (loss: 0.8944286108016968, acc: 0.78125)
[2025-02-05 13:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:15][root][INFO] - Training Epoch: 2/2, step 19327/23838 completed (loss: 1.4081989526748657, acc: 0.6326530575752258)
[2025-02-05 13:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:15][root][INFO] - Training Epoch: 2/2, step 19328/23838 completed (loss: 1.0451117753982544, acc: 0.6785714030265808)
[2025-02-05 13:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:16][root][INFO] - Training Epoch: 2/2, step 19329/23838 completed (loss: 1.6176937818527222, acc: 0.5714285969734192)
[2025-02-05 13:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:16][root][INFO] - Training Epoch: 2/2, step 19330/23838 completed (loss: 1.6033817529678345, acc: 0.5614035129547119)
[2025-02-05 13:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:17][root][INFO] - Training Epoch: 2/2, step 19331/23838 completed (loss: 1.0465012788772583, acc: 0.7307692170143127)
[2025-02-05 13:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:17][root][INFO] - Training Epoch: 2/2, step 19332/23838 completed (loss: 1.112981915473938, acc: 0.75)
[2025-02-05 13:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:17][root][INFO] - Training Epoch: 2/2, step 19333/23838 completed (loss: 1.0985761880874634, acc: 0.7272727489471436)
[2025-02-05 13:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:18][root][INFO] - Training Epoch: 2/2, step 19334/23838 completed (loss: 1.5509305000305176, acc: 0.5769230723381042)
[2025-02-05 13:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:18][root][INFO] - Training Epoch: 2/2, step 19335/23838 completed (loss: 1.334142804145813, acc: 0.52173912525177)
[2025-02-05 13:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:19][root][INFO] - Training Epoch: 2/2, step 19336/23838 completed (loss: 0.9548321962356567, acc: 0.7419354915618896)
[2025-02-05 13:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:19][root][INFO] - Training Epoch: 2/2, step 19337/23838 completed (loss: 1.1521693468093872, acc: 0.7142857313156128)
[2025-02-05 13:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:19][root][INFO] - Training Epoch: 2/2, step 19338/23838 completed (loss: 1.037333607673645, acc: 0.5952380895614624)
[2025-02-05 13:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:20][root][INFO] - Training Epoch: 2/2, step 19339/23838 completed (loss: 1.0925883054733276, acc: 0.7352941036224365)
[2025-02-05 13:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:20][root][INFO] - Training Epoch: 2/2, step 19340/23838 completed (loss: 0.992766797542572, acc: 0.7307692170143127)
[2025-02-05 13:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:21][root][INFO] - Training Epoch: 2/2, step 19341/23838 completed (loss: 1.0477374792099, acc: 0.71875)
[2025-02-05 13:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:21][root][INFO] - Training Epoch: 2/2, step 19342/23838 completed (loss: 1.8669958114624023, acc: 0.4047619104385376)
[2025-02-05 13:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:21][root][INFO] - Training Epoch: 2/2, step 19343/23838 completed (loss: 0.8778132796287537, acc: 0.7307692170143127)
[2025-02-05 13:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:22][root][INFO] - Training Epoch: 2/2, step 19344/23838 completed (loss: 1.2463196516036987, acc: 0.6842105388641357)
[2025-02-05 13:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:22][root][INFO] - Training Epoch: 2/2, step 19345/23838 completed (loss: 1.6825300455093384, acc: 0.5)
[2025-02-05 13:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:23][root][INFO] - Training Epoch: 2/2, step 19346/23838 completed (loss: 1.4755644798278809, acc: 0.6067415475845337)
[2025-02-05 13:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:23][root][INFO] - Training Epoch: 2/2, step 19347/23838 completed (loss: 1.5222585201263428, acc: 0.5555555820465088)
[2025-02-05 13:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:23][root][INFO] - Training Epoch: 2/2, step 19348/23838 completed (loss: 0.8255615234375, acc: 0.800000011920929)
[2025-02-05 13:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:24][root][INFO] - Training Epoch: 2/2, step 19349/23838 completed (loss: 0.8589866161346436, acc: 0.7307692170143127)
[2025-02-05 13:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:24][root][INFO] - Training Epoch: 2/2, step 19350/23838 completed (loss: 0.9286925196647644, acc: 0.7446808218955994)
[2025-02-05 13:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:24][root][INFO] - Training Epoch: 2/2, step 19351/23838 completed (loss: 1.221989631652832, acc: 0.6904761791229248)
[2025-02-05 13:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:25][root][INFO] - Training Epoch: 2/2, step 19352/23838 completed (loss: 1.0214877128601074, acc: 0.7254902124404907)
[2025-02-05 13:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:25][root][INFO] - Training Epoch: 2/2, step 19353/23838 completed (loss: 1.0138437747955322, acc: 0.6428571343421936)
[2025-02-05 13:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:26][root][INFO] - Training Epoch: 2/2, step 19354/23838 completed (loss: 0.908768892288208, acc: 0.6800000071525574)
[2025-02-05 13:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:26][root][INFO] - Training Epoch: 2/2, step 19355/23838 completed (loss: 1.0537184476852417, acc: 0.8181818127632141)
[2025-02-05 13:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:26][root][INFO] - Training Epoch: 2/2, step 19356/23838 completed (loss: 1.1938765048980713, acc: 0.6399999856948853)
[2025-02-05 13:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:27][root][INFO] - Training Epoch: 2/2, step 19357/23838 completed (loss: 0.6898901462554932, acc: 0.9032257795333862)
[2025-02-05 13:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:27][root][INFO] - Training Epoch: 2/2, step 19358/23838 completed (loss: 0.6150288581848145, acc: 0.8260869383811951)
[2025-02-05 13:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:27][root][INFO] - Training Epoch: 2/2, step 19359/23838 completed (loss: 0.7559255957603455, acc: 0.8421052694320679)
[2025-02-05 13:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:28][root][INFO] - Training Epoch: 2/2, step 19360/23838 completed (loss: 0.8740296959877014, acc: 0.7435897588729858)
[2025-02-05 13:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:28][root][INFO] - Training Epoch: 2/2, step 19361/23838 completed (loss: 0.9490455985069275, acc: 0.7346938848495483)
[2025-02-05 13:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:28][root][INFO] - Training Epoch: 2/2, step 19362/23838 completed (loss: 1.2177611589431763, acc: 0.6551724076271057)
[2025-02-05 13:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:29][root][INFO] - Training Epoch: 2/2, step 19363/23838 completed (loss: 1.750128984451294, acc: 0.5161290168762207)
[2025-02-05 13:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:29][root][INFO] - Training Epoch: 2/2, step 19364/23838 completed (loss: 1.2911320924758911, acc: 0.7037037014961243)
[2025-02-05 13:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:30][root][INFO] - Training Epoch: 2/2, step 19365/23838 completed (loss: 0.9381093382835388, acc: 0.8260869383811951)
[2025-02-05 13:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:30][root][INFO] - Training Epoch: 2/2, step 19366/23838 completed (loss: 0.8282491564750671, acc: 0.8333333134651184)
[2025-02-05 13:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:30][root][INFO] - Training Epoch: 2/2, step 19367/23838 completed (loss: 0.6269335150718689, acc: 0.8095238208770752)
[2025-02-05 13:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:31][root][INFO] - Training Epoch: 2/2, step 19368/23838 completed (loss: 1.0319863557815552, acc: 0.7045454382896423)
[2025-02-05 13:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:31][root][INFO] - Training Epoch: 2/2, step 19369/23838 completed (loss: 0.6837702989578247, acc: 0.8421052694320679)
[2025-02-05 13:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:31][root][INFO] - Training Epoch: 2/2, step 19370/23838 completed (loss: 0.7181008458137512, acc: 0.774193525314331)
[2025-02-05 13:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:32][root][INFO] - Training Epoch: 2/2, step 19371/23838 completed (loss: 0.49755606055259705, acc: 0.8787878751754761)
[2025-02-05 13:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:32][root][INFO] - Training Epoch: 2/2, step 19372/23838 completed (loss: 0.7320292592048645, acc: 0.7857142686843872)
[2025-02-05 13:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:32][root][INFO] - Training Epoch: 2/2, step 19373/23838 completed (loss: 0.993295431137085, acc: 0.7674418687820435)
[2025-02-05 13:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:33][root][INFO] - Training Epoch: 2/2, step 19374/23838 completed (loss: 0.7576480507850647, acc: 0.7209302186965942)
[2025-02-05 13:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:33][root][INFO] - Training Epoch: 2/2, step 19375/23838 completed (loss: 1.2281787395477295, acc: 0.6666666865348816)
[2025-02-05 13:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:33][root][INFO] - Training Epoch: 2/2, step 19376/23838 completed (loss: 0.8047864437103271, acc: 0.8260869383811951)
[2025-02-05 13:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:34][root][INFO] - Training Epoch: 2/2, step 19377/23838 completed (loss: 0.7776910066604614, acc: 0.6842105388641357)
[2025-02-05 13:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:34][root][INFO] - Training Epoch: 2/2, step 19378/23838 completed (loss: 1.4020992517471313, acc: 0.5882353186607361)
[2025-02-05 13:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:35][root][INFO] - Training Epoch: 2/2, step 19379/23838 completed (loss: 0.9717468023300171, acc: 0.6000000238418579)
[2025-02-05 13:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:35][root][INFO] - Training Epoch: 2/2, step 19380/23838 completed (loss: 0.5069365501403809, acc: 0.8571428656578064)
[2025-02-05 13:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:35][root][INFO] - Training Epoch: 2/2, step 19381/23838 completed (loss: 0.7425366044044495, acc: 0.8108108043670654)
[2025-02-05 13:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:36][root][INFO] - Training Epoch: 2/2, step 19382/23838 completed (loss: 1.1132781505584717, acc: 0.6800000071525574)
[2025-02-05 13:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:36][root][INFO] - Training Epoch: 2/2, step 19383/23838 completed (loss: 0.7734051942825317, acc: 0.774193525314331)
[2025-02-05 13:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:36][root][INFO] - Training Epoch: 2/2, step 19384/23838 completed (loss: 0.7904447913169861, acc: 0.8392857313156128)
[2025-02-05 13:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:37][root][INFO] - Training Epoch: 2/2, step 19385/23838 completed (loss: 0.39715448021888733, acc: 0.9333333373069763)
[2025-02-05 13:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:37][root][INFO] - Training Epoch: 2/2, step 19386/23838 completed (loss: 0.2556140124797821, acc: 0.9545454382896423)
[2025-02-05 13:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:37][root][INFO] - Training Epoch: 2/2, step 19387/23838 completed (loss: 0.5009788870811462, acc: 0.8571428656578064)
[2025-02-05 13:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:38][root][INFO] - Training Epoch: 2/2, step 19388/23838 completed (loss: 0.5201926231384277, acc: 0.875)
[2025-02-05 13:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:38][root][INFO] - Training Epoch: 2/2, step 19389/23838 completed (loss: 0.34397920966148376, acc: 0.9545454382896423)
[2025-02-05 13:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:38][root][INFO] - Training Epoch: 2/2, step 19390/23838 completed (loss: 0.7062707543373108, acc: 0.8333333134651184)
[2025-02-05 13:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:39][root][INFO] - Training Epoch: 2/2, step 19391/23838 completed (loss: 0.5103790760040283, acc: 0.8823529481887817)
[2025-02-05 13:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:39][root][INFO] - Training Epoch: 2/2, step 19392/23838 completed (loss: 1.0529837608337402, acc: 0.7096773982048035)
[2025-02-05 13:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:40][root][INFO] - Training Epoch: 2/2, step 19393/23838 completed (loss: 1.4539484977722168, acc: 0.557692289352417)
[2025-02-05 13:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:40][root][INFO] - Training Epoch: 2/2, step 19394/23838 completed (loss: 1.5244441032409668, acc: 0.5833333134651184)
[2025-02-05 13:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:40][root][INFO] - Training Epoch: 2/2, step 19395/23838 completed (loss: 1.1500110626220703, acc: 0.6914893388748169)
[2025-02-05 13:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:41][root][INFO] - Training Epoch: 2/2, step 19396/23838 completed (loss: 1.158493995666504, acc: 0.6600000262260437)
[2025-02-05 13:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:41][root][INFO] - Training Epoch: 2/2, step 19397/23838 completed (loss: 1.6256721019744873, acc: 0.523809552192688)
[2025-02-05 13:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:41][root][INFO] - Training Epoch: 2/2, step 19398/23838 completed (loss: 1.6426209211349487, acc: 0.6315789222717285)
[2025-02-05 13:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:42][root][INFO] - Training Epoch: 2/2, step 19399/23838 completed (loss: 1.3581160306930542, acc: 0.6097561120986938)
[2025-02-05 13:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:42][root][INFO] - Training Epoch: 2/2, step 19400/23838 completed (loss: 0.5029086470603943, acc: 0.8064516186714172)
[2025-02-05 13:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:43][root][INFO] - Training Epoch: 2/2, step 19401/23838 completed (loss: 0.9381158351898193, acc: 0.6875)
[2025-02-05 13:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:43][root][INFO] - Training Epoch: 2/2, step 19402/23838 completed (loss: 1.3245277404785156, acc: 0.6428571343421936)
[2025-02-05 13:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:43][root][INFO] - Training Epoch: 2/2, step 19403/23838 completed (loss: 0.8259945511817932, acc: 0.7215189933776855)
[2025-02-05 13:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:44][root][INFO] - Training Epoch: 2/2, step 19404/23838 completed (loss: 0.6309744119644165, acc: 0.782608687877655)
[2025-02-05 13:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:44][root][INFO] - Training Epoch: 2/2, step 19405/23838 completed (loss: 1.4338504076004028, acc: 0.5199999809265137)
[2025-02-05 13:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:44][root][INFO] - Training Epoch: 2/2, step 19406/23838 completed (loss: 0.9693027138710022, acc: 0.7368420958518982)
[2025-02-05 13:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:45][root][INFO] - Training Epoch: 2/2, step 19407/23838 completed (loss: 0.7864137887954712, acc: 0.7804877758026123)
[2025-02-05 13:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:45][root][INFO] - Training Epoch: 2/2, step 19408/23838 completed (loss: 1.0228135585784912, acc: 0.6896551847457886)
[2025-02-05 13:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:46][root][INFO] - Training Epoch: 2/2, step 19409/23838 completed (loss: 0.9051774740219116, acc: 0.8387096524238586)
[2025-02-05 13:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:46][root][INFO] - Training Epoch: 2/2, step 19410/23838 completed (loss: 1.707059383392334, acc: 0.6285714507102966)
[2025-02-05 13:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:46][root][INFO] - Training Epoch: 2/2, step 19411/23838 completed (loss: 1.8084005117416382, acc: 0.3636363744735718)
[2025-02-05 13:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:47][root][INFO] - Training Epoch: 2/2, step 19412/23838 completed (loss: 1.1083869934082031, acc: 0.7368420958518982)
[2025-02-05 13:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:47][root][INFO] - Training Epoch: 2/2, step 19413/23838 completed (loss: 0.5665729641914368, acc: 0.8518518805503845)
[2025-02-05 13:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:47][root][INFO] - Training Epoch: 2/2, step 19414/23838 completed (loss: 0.8091440796852112, acc: 0.7575757503509521)
[2025-02-05 13:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:48][root][INFO] - Training Epoch: 2/2, step 19415/23838 completed (loss: 0.9243785738945007, acc: 0.7027027010917664)
[2025-02-05 13:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:48][root][INFO] - Training Epoch: 2/2, step 19416/23838 completed (loss: 0.6414620280265808, acc: 0.7058823704719543)
[2025-02-05 13:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:49][root][INFO] - Training Epoch: 2/2, step 19417/23838 completed (loss: 0.6737239956855774, acc: 0.8035714030265808)
[2025-02-05 13:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:49][root][INFO] - Training Epoch: 2/2, step 19418/23838 completed (loss: 0.9365764856338501, acc: 0.738095223903656)
[2025-02-05 13:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:49][root][INFO] - Training Epoch: 2/2, step 19419/23838 completed (loss: 0.9015292525291443, acc: 0.7567567825317383)
[2025-02-05 13:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:50][root][INFO] - Training Epoch: 2/2, step 19420/23838 completed (loss: 1.8153678178787231, acc: 0.5204081535339355)
[2025-02-05 13:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:50][root][INFO] - Training Epoch: 2/2, step 19421/23838 completed (loss: 1.6422464847564697, acc: 0.550000011920929)
[2025-02-05 13:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:51][root][INFO] - Training Epoch: 2/2, step 19422/23838 completed (loss: 1.568703532218933, acc: 0.5728155374526978)
[2025-02-05 13:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:51][root][INFO] - Training Epoch: 2/2, step 19423/23838 completed (loss: 1.306667447090149, acc: 0.6213592290878296)
[2025-02-05 13:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:52][root][INFO] - Training Epoch: 2/2, step 19424/23838 completed (loss: 1.479777216911316, acc: 0.5520833134651184)
[2025-02-05 13:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:52][root][INFO] - Training Epoch: 2/2, step 19425/23838 completed (loss: 1.32719886302948, acc: 0.6041666865348816)
[2025-02-05 13:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:52][root][INFO] - Training Epoch: 2/2, step 19426/23838 completed (loss: 1.4084703922271729, acc: 0.6329113841056824)
[2025-02-05 13:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:53][root][INFO] - Training Epoch: 2/2, step 19427/23838 completed (loss: 1.458708643913269, acc: 0.6071428656578064)
[2025-02-05 13:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:53][root][INFO] - Training Epoch: 2/2, step 19428/23838 completed (loss: 1.6233997344970703, acc: 0.5135135054588318)
[2025-02-05 13:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:54][root][INFO] - Training Epoch: 2/2, step 19429/23838 completed (loss: 1.2206456661224365, acc: 0.6711409687995911)
[2025-02-05 13:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:54][root][INFO] - Training Epoch: 2/2, step 19430/23838 completed (loss: 1.482229232788086, acc: 0.5490196347236633)
[2025-02-05 13:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:55][root][INFO] - Training Epoch: 2/2, step 19431/23838 completed (loss: 1.1703382730484009, acc: 0.6451612710952759)
[2025-02-05 13:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:55][root][INFO] - Training Epoch: 2/2, step 19432/23838 completed (loss: 1.3300464153289795, acc: 0.5978260636329651)
[2025-02-05 13:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:55][root][INFO] - Training Epoch: 2/2, step 19433/23838 completed (loss: 2.2741165161132812, acc: 0.36206895112991333)
[2025-02-05 13:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:56][root][INFO] - Training Epoch: 2/2, step 19434/23838 completed (loss: 1.5637412071228027, acc: 0.5897436141967773)
[2025-02-05 13:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:56][root][INFO] - Training Epoch: 2/2, step 19435/23838 completed (loss: 1.230057954788208, acc: 0.6000000238418579)
[2025-02-05 13:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:56][root][INFO] - Training Epoch: 2/2, step 19436/23838 completed (loss: 1.309346079826355, acc: 0.5555555820465088)
[2025-02-05 13:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:57][root][INFO] - Training Epoch: 2/2, step 19437/23838 completed (loss: 1.879443645477295, acc: 0.5)
[2025-02-05 13:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:57][root][INFO] - Training Epoch: 2/2, step 19438/23838 completed (loss: 1.0597999095916748, acc: 0.729411780834198)
[2025-02-05 13:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:58][root][INFO] - Training Epoch: 2/2, step 19439/23838 completed (loss: 1.2612724304199219, acc: 0.6753246784210205)
[2025-02-05 13:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:58][root][INFO] - Training Epoch: 2/2, step 19440/23838 completed (loss: 1.2645270824432373, acc: 0.6428571343421936)
[2025-02-05 13:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:58][root][INFO] - Training Epoch: 2/2, step 19441/23838 completed (loss: 1.2411881685256958, acc: 0.6000000238418579)
[2025-02-05 13:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:59][root][INFO] - Training Epoch: 2/2, step 19442/23838 completed (loss: 1.3419468402862549, acc: 0.6242424249649048)
[2025-02-05 13:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:49:59][root][INFO] - Training Epoch: 2/2, step 19443/23838 completed (loss: 1.5046849250793457, acc: 0.560606062412262)
[2025-02-05 13:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:00][root][INFO] - Training Epoch: 2/2, step 19444/23838 completed (loss: 1.0172587633132935, acc: 0.7241379022598267)
[2025-02-05 13:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:00][root][INFO] - Training Epoch: 2/2, step 19445/23838 completed (loss: 1.523868203163147, acc: 0.5702479481697083)
[2025-02-05 13:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:01][root][INFO] - Training Epoch: 2/2, step 19446/23838 completed (loss: 1.1777520179748535, acc: 0.6557376980781555)
[2025-02-05 13:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:01][root][INFO] - Training Epoch: 2/2, step 19447/23838 completed (loss: 1.3486499786376953, acc: 0.626086950302124)
[2025-02-05 13:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:02][root][INFO] - Training Epoch: 2/2, step 19448/23838 completed (loss: 1.5917710065841675, acc: 0.49635037779808044)
[2025-02-05 13:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:02][root][INFO] - Training Epoch: 2/2, step 19449/23838 completed (loss: 1.3111220598220825, acc: 0.5974025726318359)
[2025-02-05 13:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:02][root][INFO] - Training Epoch: 2/2, step 19450/23838 completed (loss: 1.3884693384170532, acc: 0.6329113841056824)
[2025-02-05 13:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:03][root][INFO] - Training Epoch: 2/2, step 19451/23838 completed (loss: 1.3172435760498047, acc: 0.614814817905426)
[2025-02-05 13:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:04][root][INFO] - Training Epoch: 2/2, step 19452/23838 completed (loss: 1.40529465675354, acc: 0.6129032373428345)
[2025-02-05 13:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:04][root][INFO] - Training Epoch: 2/2, step 19453/23838 completed (loss: 1.4956259727478027, acc: 0.5882353186607361)
[2025-02-05 13:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:04][root][INFO] - Training Epoch: 2/2, step 19454/23838 completed (loss: 1.3641029596328735, acc: 0.6279069781303406)
[2025-02-05 13:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:05][root][INFO] - Training Epoch: 2/2, step 19455/23838 completed (loss: 1.2943692207336426, acc: 0.6363636255264282)
[2025-02-05 13:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:05][root][INFO] - Training Epoch: 2/2, step 19456/23838 completed (loss: 1.131041407585144, acc: 0.680672287940979)
[2025-02-05 13:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:06][root][INFO] - Training Epoch: 2/2, step 19457/23838 completed (loss: 1.1501132249832153, acc: 0.6323529481887817)
[2025-02-05 13:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:06][root][INFO] - Training Epoch: 2/2, step 19458/23838 completed (loss: 1.6918848752975464, acc: 0.375)
[2025-02-05 13:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:06][root][INFO] - Training Epoch: 2/2, step 19459/23838 completed (loss: 1.5470455884933472, acc: 0.5428571701049805)
[2025-02-05 13:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:07][root][INFO] - Training Epoch: 2/2, step 19460/23838 completed (loss: 1.2770391702651978, acc: 0.6428571343421936)
[2025-02-05 13:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:07][root][INFO] - Training Epoch: 2/2, step 19461/23838 completed (loss: 1.5293668508529663, acc: 0.5234375)
[2025-02-05 13:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:07][root][INFO] - Training Epoch: 2/2, step 19462/23838 completed (loss: 1.3377184867858887, acc: 0.6438356041908264)
[2025-02-05 13:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:08][root][INFO] - Training Epoch: 2/2, step 19463/23838 completed (loss: 1.4265098571777344, acc: 0.641791045665741)
[2025-02-05 13:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:08][root][INFO] - Training Epoch: 2/2, step 19464/23838 completed (loss: 1.3521344661712646, acc: 0.6582278609275818)
[2025-02-05 13:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:09][root][INFO] - Training Epoch: 2/2, step 19465/23838 completed (loss: 1.3314340114593506, acc: 0.6343283653259277)
[2025-02-05 13:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:09][root][INFO] - Training Epoch: 2/2, step 19466/23838 completed (loss: 1.2270338535308838, acc: 0.6666666865348816)
[2025-02-05 13:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:09][root][INFO] - Training Epoch: 2/2, step 19467/23838 completed (loss: 1.604367733001709, acc: 0.5890411138534546)
[2025-02-05 13:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:10][root][INFO] - Training Epoch: 2/2, step 19468/23838 completed (loss: 1.125501275062561, acc: 0.6262626051902771)
[2025-02-05 13:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:10][root][INFO] - Training Epoch: 2/2, step 19469/23838 completed (loss: 1.0655814409255981, acc: 0.7042253613471985)
[2025-02-05 13:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:11][root][INFO] - Training Epoch: 2/2, step 19470/23838 completed (loss: 1.5914403200149536, acc: 0.5765765905380249)
[2025-02-05 13:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:11][root][INFO] - Training Epoch: 2/2, step 19471/23838 completed (loss: 1.2593191862106323, acc: 0.6341463327407837)
[2025-02-05 13:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:11][root][INFO] - Training Epoch: 2/2, step 19472/23838 completed (loss: 1.2072019577026367, acc: 0.6521739363670349)
[2025-02-05 13:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:12][root][INFO] - Training Epoch: 2/2, step 19473/23838 completed (loss: 0.8464334607124329, acc: 0.7096773982048035)
[2025-02-05 13:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:12][root][INFO] - Training Epoch: 2/2, step 19474/23838 completed (loss: 1.1127901077270508, acc: 0.6285714507102966)
[2025-02-05 13:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:13][root][INFO] - Training Epoch: 2/2, step 19475/23838 completed (loss: 1.0316741466522217, acc: 0.6818181872367859)
[2025-02-05 13:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:13][root][INFO] - Training Epoch: 2/2, step 19476/23838 completed (loss: 1.1234831809997559, acc: 0.6290322542190552)
[2025-02-05 13:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:13][root][INFO] - Training Epoch: 2/2, step 19477/23838 completed (loss: 1.2949694395065308, acc: 0.6078431606292725)
[2025-02-05 13:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:14][root][INFO] - Training Epoch: 2/2, step 19478/23838 completed (loss: 1.2021565437316895, acc: 0.698924720287323)
[2025-02-05 13:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:14][root][INFO] - Training Epoch: 2/2, step 19479/23838 completed (loss: 1.1882685422897339, acc: 0.6724137663841248)
[2025-02-05 13:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:15][root][INFO] - Training Epoch: 2/2, step 19480/23838 completed (loss: 1.0991487503051758, acc: 0.695652186870575)
[2025-02-05 13:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:15][root][INFO] - Training Epoch: 2/2, step 19481/23838 completed (loss: 1.472531795501709, acc: 0.5199999809265137)
[2025-02-05 13:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:15][root][INFO] - Training Epoch: 2/2, step 19482/23838 completed (loss: 1.3851484060287476, acc: 0.5441176295280457)
[2025-02-05 13:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:16][root][INFO] - Training Epoch: 2/2, step 19483/23838 completed (loss: 1.6637979745864868, acc: 0.5249999761581421)
[2025-02-05 13:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:16][root][INFO] - Training Epoch: 2/2, step 19484/23838 completed (loss: 1.749131441116333, acc: 0.504273533821106)
[2025-02-05 13:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:17][root][INFO] - Training Epoch: 2/2, step 19485/23838 completed (loss: 1.3923377990722656, acc: 0.606249988079071)
[2025-02-05 13:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:17][root][INFO] - Training Epoch: 2/2, step 19486/23838 completed (loss: 1.5906020402908325, acc: 0.5481481552124023)
[2025-02-05 13:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:18][root][INFO] - Training Epoch: 2/2, step 19487/23838 completed (loss: 1.4321364164352417, acc: 0.5845070481300354)
[2025-02-05 13:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:18][root][INFO] - Training Epoch: 2/2, step 19488/23838 completed (loss: 1.1968361139297485, acc: 0.6428571343421936)
[2025-02-05 13:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:18][root][INFO] - Training Epoch: 2/2, step 19489/23838 completed (loss: 1.7888998985290527, acc: 0.5)
[2025-02-05 13:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:19][root][INFO] - Training Epoch: 2/2, step 19490/23838 completed (loss: 1.7108772993087769, acc: 0.5410959124565125)
[2025-02-05 13:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:19][root][INFO] - Training Epoch: 2/2, step 19491/23838 completed (loss: 1.3720265626907349, acc: 0.5869565010070801)
[2025-02-05 13:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:19][root][INFO] - Training Epoch: 2/2, step 19492/23838 completed (loss: 0.9076728820800781, acc: 0.7432432174682617)
[2025-02-05 13:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:20][root][INFO] - Training Epoch: 2/2, step 19493/23838 completed (loss: 1.4972103834152222, acc: 0.5362318754196167)
[2025-02-05 13:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:21][root][INFO] - Training Epoch: 2/2, step 19494/23838 completed (loss: 1.0886341333389282, acc: 0.7419354915618896)
[2025-02-05 13:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:21][root][INFO] - Training Epoch: 2/2, step 19495/23838 completed (loss: 1.092873215675354, acc: 0.6845637559890747)
[2025-02-05 13:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:22][root][INFO] - Training Epoch: 2/2, step 19496/23838 completed (loss: 0.9292498230934143, acc: 0.7171717286109924)
[2025-02-05 13:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:22][root][INFO] - Training Epoch: 2/2, step 19497/23838 completed (loss: 1.0425933599472046, acc: 0.6476190686225891)
[2025-02-05 13:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:22][root][INFO] - Training Epoch: 2/2, step 19498/23838 completed (loss: 0.9925035834312439, acc: 0.7032967209815979)
[2025-02-05 13:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:23][root][INFO] - Training Epoch: 2/2, step 19499/23838 completed (loss: 1.1861313581466675, acc: 0.6486486196517944)
[2025-02-05 13:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:23][root][INFO] - Training Epoch: 2/2, step 19500/23838 completed (loss: 1.3462468385696411, acc: 0.6344085931777954)
[2025-02-05 13:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:24][root][INFO] - Training Epoch: 2/2, step 19501/23838 completed (loss: 1.0457993745803833, acc: 0.7547169923782349)
[2025-02-05 13:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:24][root][INFO] - Training Epoch: 2/2, step 19502/23838 completed (loss: 1.0348339080810547, acc: 0.7039999961853027)
[2025-02-05 13:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:25][root][INFO] - Training Epoch: 2/2, step 19503/23838 completed (loss: 1.2627196311950684, acc: 0.6134454011917114)
[2025-02-05 13:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:25][root][INFO] - Training Epoch: 2/2, step 19504/23838 completed (loss: 0.9358506798744202, acc: 0.738095223903656)
[2025-02-05 13:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:25][root][INFO] - Training Epoch: 2/2, step 19505/23838 completed (loss: 1.043707251548767, acc: 0.725806474685669)
[2025-02-05 13:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:26][root][INFO] - Training Epoch: 2/2, step 19506/23838 completed (loss: 0.9683897495269775, acc: 0.6891891956329346)
[2025-02-05 13:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:26][root][INFO] - Training Epoch: 2/2, step 19507/23838 completed (loss: 0.9544106721878052, acc: 0.7162162065505981)
[2025-02-05 13:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:26][root][INFO] - Training Epoch: 2/2, step 19508/23838 completed (loss: 1.2716197967529297, acc: 0.6595744490623474)
[2025-02-05 13:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:27][root][INFO] - Training Epoch: 2/2, step 19509/23838 completed (loss: 0.6551100611686707, acc: 0.7878788113594055)
[2025-02-05 13:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:27][root][INFO] - Training Epoch: 2/2, step 19510/23838 completed (loss: 1.1952342987060547, acc: 0.671875)
[2025-02-05 13:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:28][root][INFO] - Training Epoch: 2/2, step 19511/23838 completed (loss: 1.164930820465088, acc: 0.6209677457809448)
[2025-02-05 13:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:28][root][INFO] - Training Epoch: 2/2, step 19512/23838 completed (loss: 1.037359595298767, acc: 0.6491228342056274)
[2025-02-05 13:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:29][root][INFO] - Training Epoch: 2/2, step 19513/23838 completed (loss: 0.8656399846076965, acc: 0.807692289352417)
[2025-02-05 13:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:29][root][INFO] - Training Epoch: 2/2, step 19514/23838 completed (loss: 1.4429198503494263, acc: 0.6153846383094788)
[2025-02-05 13:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:30][root][INFO] - Training Epoch: 2/2, step 19515/23838 completed (loss: 0.8887774348258972, acc: 0.7090908885002136)
[2025-02-05 13:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:30][root][INFO] - Training Epoch: 2/2, step 19516/23838 completed (loss: 1.8138103485107422, acc: 0.44897958636283875)
[2025-02-05 13:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:30][root][INFO] - Training Epoch: 2/2, step 19517/23838 completed (loss: 1.370339274406433, acc: 0.581818163394928)
[2025-02-05 13:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:31][root][INFO] - Training Epoch: 2/2, step 19518/23838 completed (loss: 0.9274320006370544, acc: 0.7543859481811523)
[2025-02-05 13:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:31][root][INFO] - Training Epoch: 2/2, step 19519/23838 completed (loss: 1.1939164400100708, acc: 0.6756756901741028)
[2025-02-05 13:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:31][root][INFO] - Training Epoch: 2/2, step 19520/23838 completed (loss: 1.1425379514694214, acc: 0.6705882549285889)
[2025-02-05 13:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:32][root][INFO] - Training Epoch: 2/2, step 19521/23838 completed (loss: 1.2332959175109863, acc: 0.6601307392120361)
[2025-02-05 13:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:32][root][INFO] - Training Epoch: 2/2, step 19522/23838 completed (loss: 1.0350120067596436, acc: 0.695652186870575)
[2025-02-05 13:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:33][root][INFO] - Training Epoch: 2/2, step 19523/23838 completed (loss: 1.1766778230667114, acc: 0.6603773832321167)
[2025-02-05 13:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:33][root][INFO] - Training Epoch: 2/2, step 19524/23838 completed (loss: 0.9060872197151184, acc: 0.7254902124404907)
[2025-02-05 13:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:34][root][INFO] - Training Epoch: 2/2, step 19525/23838 completed (loss: 1.119213342666626, acc: 0.6774193644523621)
[2025-02-05 13:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:34][root][INFO] - Training Epoch: 2/2, step 19526/23838 completed (loss: 1.0319862365722656, acc: 0.701298713684082)
[2025-02-05 13:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:34][root][INFO] - Training Epoch: 2/2, step 19527/23838 completed (loss: 1.1687434911727905, acc: 0.682539701461792)
[2025-02-05 13:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:35][root][INFO] - Training Epoch: 2/2, step 19528/23838 completed (loss: 1.2731611728668213, acc: 0.7083333134651184)
[2025-02-05 13:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:35][root][INFO] - Training Epoch: 2/2, step 19529/23838 completed (loss: 0.13737766444683075, acc: 1.0)
[2025-02-05 13:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:35][root][INFO] - Training Epoch: 2/2, step 19530/23838 completed (loss: 0.5744218826293945, acc: 0.8500000238418579)
[2025-02-05 13:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:36][root][INFO] - Training Epoch: 2/2, step 19531/23838 completed (loss: 1.1819981336593628, acc: 0.6115702390670776)
[2025-02-05 13:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:36][root][INFO] - Training Epoch: 2/2, step 19532/23838 completed (loss: 0.7183868885040283, acc: 0.8307692408561707)
[2025-02-05 13:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:37][root][INFO] - Training Epoch: 2/2, step 19533/23838 completed (loss: 0.5834881663322449, acc: 0.8461538553237915)
[2025-02-05 13:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:37][root][INFO] - Training Epoch: 2/2, step 19534/23838 completed (loss: 0.9789336323738098, acc: 0.6976743936538696)
[2025-02-05 13:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:37][root][INFO] - Training Epoch: 2/2, step 19535/23838 completed (loss: 0.9625198841094971, acc: 0.739130437374115)
[2025-02-05 13:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:38][root][INFO] - Training Epoch: 2/2, step 19536/23838 completed (loss: 0.9336410164833069, acc: 0.7317073345184326)
[2025-02-05 13:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:38][root][INFO] - Training Epoch: 2/2, step 19537/23838 completed (loss: 0.8535574674606323, acc: 0.7124999761581421)
[2025-02-05 13:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:39][root][INFO] - Training Epoch: 2/2, step 19538/23838 completed (loss: 0.6508021950721741, acc: 0.8133333325386047)
[2025-02-05 13:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:39][root][INFO] - Training Epoch: 2/2, step 19539/23838 completed (loss: 0.6731676459312439, acc: 0.8444444537162781)
[2025-02-05 13:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:39][root][INFO] - Training Epoch: 2/2, step 19540/23838 completed (loss: 0.6469240188598633, acc: 0.78125)
[2025-02-05 13:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:40][root][INFO] - Training Epoch: 2/2, step 19541/23838 completed (loss: 0.8039155602455139, acc: 0.7052631378173828)
[2025-02-05 13:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:40][root][INFO] - Training Epoch: 2/2, step 19542/23838 completed (loss: 0.8921183943748474, acc: 0.7659574747085571)
[2025-02-05 13:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:40][root][INFO] - Training Epoch: 2/2, step 19543/23838 completed (loss: 0.543645441532135, acc: 0.8392857313156128)
[2025-02-05 13:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:41][root][INFO] - Training Epoch: 2/2, step 19544/23838 completed (loss: 1.1798579692840576, acc: 0.6753246784210205)
[2025-02-05 13:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:41][root][INFO] - Training Epoch: 2/2, step 19545/23838 completed (loss: 0.23915155231952667, acc: 0.9473684430122375)
[2025-02-05 13:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:41][root][INFO] - Training Epoch: 2/2, step 19546/23838 completed (loss: 0.76551753282547, acc: 0.7321428656578064)
[2025-02-05 13:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:42][root][INFO] - Training Epoch: 2/2, step 19547/23838 completed (loss: 1.115987777709961, acc: 0.6742424368858337)
[2025-02-05 13:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:42][root][INFO] - Training Epoch: 2/2, step 19548/23838 completed (loss: 0.9459471702575684, acc: 0.7356321811676025)
[2025-02-05 13:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:43][root][INFO] - Training Epoch: 2/2, step 19549/23838 completed (loss: 1.0766007900238037, acc: 0.692307710647583)
[2025-02-05 13:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:43][root][INFO] - Training Epoch: 2/2, step 19550/23838 completed (loss: 1.1305044889450073, acc: 0.699999988079071)
[2025-02-05 13:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:44][root][INFO] - Training Epoch: 2/2, step 19551/23838 completed (loss: 1.5163207054138184, acc: 0.5745856165885925)
[2025-02-05 13:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:44][root][INFO] - Training Epoch: 2/2, step 19552/23838 completed (loss: 1.1352040767669678, acc: 0.6666666865348816)
[2025-02-05 13:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:45][root][INFO] - Training Epoch: 2/2, step 19553/23838 completed (loss: 1.1568011045455933, acc: 0.6442307829856873)
[2025-02-05 13:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:45][root][INFO] - Training Epoch: 2/2, step 19554/23838 completed (loss: 1.2526836395263672, acc: 0.6540540456771851)
[2025-02-05 13:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:46][root][INFO] - Training Epoch: 2/2, step 19555/23838 completed (loss: 0.9160913228988647, acc: 0.7404580116271973)
[2025-02-05 13:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:46][root][INFO] - Training Epoch: 2/2, step 19556/23838 completed (loss: 1.3645799160003662, acc: 0.5777778029441833)
[2025-02-05 13:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:47][root][INFO] - Training Epoch: 2/2, step 19557/23838 completed (loss: 1.1867293119430542, acc: 0.6619718074798584)
[2025-02-05 13:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:47][root][INFO] - Training Epoch: 2/2, step 19558/23838 completed (loss: 0.7619543075561523, acc: 0.7916666865348816)
[2025-02-05 13:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:47][root][INFO] - Training Epoch: 2/2, step 19559/23838 completed (loss: 0.7039156556129456, acc: 0.8103448152542114)
[2025-02-05 13:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:48][root][INFO] - Training Epoch: 2/2, step 19560/23838 completed (loss: 1.1080734729766846, acc: 0.6694214940071106)
[2025-02-05 13:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:48][root][INFO] - Training Epoch: 2/2, step 19561/23838 completed (loss: 1.2527259588241577, acc: 0.625)
[2025-02-05 13:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:49][root][INFO] - Training Epoch: 2/2, step 19562/23838 completed (loss: 1.2388933897018433, acc: 0.6911764740943909)
[2025-02-05 13:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:49][root][INFO] - Training Epoch: 2/2, step 19563/23838 completed (loss: 0.9883869290351868, acc: 0.6838235259056091)
[2025-02-05 13:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:50][root][INFO] - Training Epoch: 2/2, step 19564/23838 completed (loss: 1.1105438470840454, acc: 0.6608695387840271)
[2025-02-05 13:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:50][root][INFO] - Training Epoch: 2/2, step 19565/23838 completed (loss: 1.1899722814559937, acc: 0.6158940196037292)
[2025-02-05 13:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:50][root][INFO] - Training Epoch: 2/2, step 19566/23838 completed (loss: 1.2527086734771729, acc: 0.6666666865348816)
[2025-02-05 13:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:51][root][INFO] - Training Epoch: 2/2, step 19567/23838 completed (loss: 1.1846680641174316, acc: 0.7049180269241333)
[2025-02-05 13:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:51][root][INFO] - Training Epoch: 2/2, step 19568/23838 completed (loss: 1.3175270557403564, acc: 0.6000000238418579)
[2025-02-05 13:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:52][root][INFO] - Training Epoch: 2/2, step 19569/23838 completed (loss: 0.9153669476509094, acc: 0.7518796920776367)
[2025-02-05 13:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:52][root][INFO] - Training Epoch: 2/2, step 19570/23838 completed (loss: 1.1364394426345825, acc: 0.7075471878051758)
[2025-02-05 13:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:53][root][INFO] - Training Epoch: 2/2, step 19571/23838 completed (loss: 1.3722256422042847, acc: 0.5555555820465088)
[2025-02-05 13:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:53][root][INFO] - Training Epoch: 2/2, step 19572/23838 completed (loss: 1.2782362699508667, acc: 0.6082473993301392)
[2025-02-05 13:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:53][root][INFO] - Training Epoch: 2/2, step 19573/23838 completed (loss: 1.405825138092041, acc: 0.5306122303009033)
[2025-02-05 13:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:54][root][INFO] - Training Epoch: 2/2, step 19574/23838 completed (loss: 1.158340573310852, acc: 0.6043956279754639)
[2025-02-05 13:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:54][root][INFO] - Training Epoch: 2/2, step 19575/23838 completed (loss: 1.3241369724273682, acc: 0.5660377144813538)
[2025-02-05 13:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:55][root][INFO] - Training Epoch: 2/2, step 19576/23838 completed (loss: 1.16940438747406, acc: 0.6388888955116272)
[2025-02-05 13:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:55][root][INFO] - Training Epoch: 2/2, step 19577/23838 completed (loss: 1.086586594581604, acc: 0.717391312122345)
[2025-02-05 13:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:55][root][INFO] - Training Epoch: 2/2, step 19578/23838 completed (loss: 1.181994080543518, acc: 0.7121211886405945)
[2025-02-05 13:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:56][root][INFO] - Training Epoch: 2/2, step 19579/23838 completed (loss: 1.2547036409378052, acc: 0.6399999856948853)
[2025-02-05 13:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:56][root][INFO] - Training Epoch: 2/2, step 19580/23838 completed (loss: 1.1876537799835205, acc: 0.7088607549667358)
[2025-02-05 13:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:56][root][INFO] - Training Epoch: 2/2, step 19581/23838 completed (loss: 1.167926549911499, acc: 0.7058823704719543)
[2025-02-05 13:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:57][root][INFO] - Training Epoch: 2/2, step 19582/23838 completed (loss: 1.2088743448257446, acc: 0.6483516693115234)
[2025-02-05 13:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:57][root][INFO] - Training Epoch: 2/2, step 19583/23838 completed (loss: 1.247026801109314, acc: 0.6386554837226868)
[2025-02-05 13:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:58][root][INFO] - Training Epoch: 2/2, step 19584/23838 completed (loss: 1.1024143695831299, acc: 0.6666666865348816)
[2025-02-05 13:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:58][root][INFO] - Training Epoch: 2/2, step 19585/23838 completed (loss: 1.178804636001587, acc: 0.6521739363670349)
[2025-02-05 13:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:58][root][INFO] - Training Epoch: 2/2, step 19586/23838 completed (loss: 0.9591577649116516, acc: 0.7407407164573669)
[2025-02-05 13:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:59][root][INFO] - Training Epoch: 2/2, step 19587/23838 completed (loss: 1.182412028312683, acc: 0.699999988079071)
[2025-02-05 13:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:50:59][root][INFO] - Training Epoch: 2/2, step 19588/23838 completed (loss: 0.9947876334190369, acc: 0.7096773982048035)
[2025-02-05 13:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:00][root][INFO] - Training Epoch: 2/2, step 19589/23838 completed (loss: 1.4705203771591187, acc: 0.6043956279754639)
[2025-02-05 13:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:00][root][INFO] - Training Epoch: 2/2, step 19590/23838 completed (loss: 1.3429409265518188, acc: 0.6338028311729431)
[2025-02-05 13:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:00][root][INFO] - Training Epoch: 2/2, step 19591/23838 completed (loss: 1.1878242492675781, acc: 0.7058823704719543)
[2025-02-05 13:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:01][root][INFO] - Training Epoch: 2/2, step 19592/23838 completed (loss: 1.3175276517868042, acc: 0.578125)
[2025-02-05 13:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:01][root][INFO] - Training Epoch: 2/2, step 19593/23838 completed (loss: 1.2968435287475586, acc: 0.6271186470985413)
[2025-02-05 13:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:01][root][INFO] - Training Epoch: 2/2, step 19594/23838 completed (loss: 1.0410795211791992, acc: 0.7547169923782349)
[2025-02-05 13:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:02][root][INFO] - Training Epoch: 2/2, step 19595/23838 completed (loss: 1.0419621467590332, acc: 0.7586206793785095)
[2025-02-05 13:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:02][root][INFO] - Training Epoch: 2/2, step 19596/23838 completed (loss: 1.0848729610443115, acc: 0.6242774724960327)
[2025-02-05 13:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:02][root][INFO] - Training Epoch: 2/2, step 19597/23838 completed (loss: 1.3456217050552368, acc: 0.6464646458625793)
[2025-02-05 13:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:03][root][INFO] - Training Epoch: 2/2, step 19598/23838 completed (loss: 1.5804615020751953, acc: 0.560606062412262)
[2025-02-05 13:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:03][root][INFO] - Training Epoch: 2/2, step 19599/23838 completed (loss: 1.1419517993927002, acc: 0.6666666865348816)
[2025-02-05 13:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:04][root][INFO] - Training Epoch: 2/2, step 19600/23838 completed (loss: 1.396164059638977, acc: 0.5967742204666138)
[2025-02-05 13:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:04][root][INFO] - Training Epoch: 2/2, step 19601/23838 completed (loss: 0.999083936214447, acc: 0.694915235042572)
[2025-02-05 13:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:04][root][INFO] - Training Epoch: 2/2, step 19602/23838 completed (loss: 1.1945164203643799, acc: 0.6567164063453674)
[2025-02-05 13:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:05][root][INFO] - Training Epoch: 2/2, step 19603/23838 completed (loss: 1.043898105621338, acc: 0.7115384340286255)
[2025-02-05 13:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:05][root][INFO] - Training Epoch: 2/2, step 19604/23838 completed (loss: 0.9357068538665771, acc: 0.694656491279602)
[2025-02-05 13:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:05][root][INFO] - Training Epoch: 2/2, step 19605/23838 completed (loss: 1.0930507183074951, acc: 0.6899224519729614)
[2025-02-05 13:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:06][root][INFO] - Training Epoch: 2/2, step 19606/23838 completed (loss: 1.4057679176330566, acc: 0.5714285969734192)
[2025-02-05 13:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:06][root][INFO] - Training Epoch: 2/2, step 19607/23838 completed (loss: 1.171085000038147, acc: 0.7096773982048035)
[2025-02-05 13:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:06][root][INFO] - Training Epoch: 2/2, step 19608/23838 completed (loss: 1.076646327972412, acc: 0.6966292262077332)
[2025-02-05 13:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:07][root][INFO] - Training Epoch: 2/2, step 19609/23838 completed (loss: 1.331465482711792, acc: 0.6016260385513306)
[2025-02-05 13:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:07][root][INFO] - Training Epoch: 2/2, step 19610/23838 completed (loss: 0.9724863767623901, acc: 0.734375)
[2025-02-05 13:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:08][root][INFO] - Training Epoch: 2/2, step 19611/23838 completed (loss: 1.1288564205169678, acc: 0.7454545497894287)
[2025-02-05 13:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:08][root][INFO] - Training Epoch: 2/2, step 19612/23838 completed (loss: 0.9639890789985657, acc: 0.6969696879386902)
[2025-02-05 13:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:08][root][INFO] - Training Epoch: 2/2, step 19613/23838 completed (loss: 1.1539156436920166, acc: 0.6811594367027283)
[2025-02-05 13:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:09][root][INFO] - Training Epoch: 2/2, step 19614/23838 completed (loss: 1.4366438388824463, acc: 0.578125)
[2025-02-05 13:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:09][root][INFO] - Training Epoch: 2/2, step 19615/23838 completed (loss: 1.6968361139297485, acc: 0.5139442086219788)
[2025-02-05 13:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:09][root][INFO] - Training Epoch: 2/2, step 19616/23838 completed (loss: 1.3968570232391357, acc: 0.5199999809265137)
[2025-02-05 13:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:10][root][INFO] - Training Epoch: 2/2, step 19617/23838 completed (loss: 1.5174797773361206, acc: 0.5785123705863953)
[2025-02-05 13:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:10][root][INFO] - Training Epoch: 2/2, step 19618/23838 completed (loss: 1.1591517925262451, acc: 0.6145833134651184)
[2025-02-05 13:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:11][root][INFO] - Training Epoch: 2/2, step 19619/23838 completed (loss: 1.009269118309021, acc: 0.6823529601097107)
[2025-02-05 13:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:11][root][INFO] - Training Epoch: 2/2, step 19620/23838 completed (loss: 1.126044750213623, acc: 0.6363636255264282)
[2025-02-05 13:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:11][root][INFO] - Training Epoch: 2/2, step 19621/23838 completed (loss: 1.0718997716903687, acc: 0.6413043737411499)
[2025-02-05 13:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:12][root][INFO] - Training Epoch: 2/2, step 19622/23838 completed (loss: 1.2477928400039673, acc: 0.625)
[2025-02-05 13:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:12][root][INFO] - Training Epoch: 2/2, step 19623/23838 completed (loss: 1.270455002784729, acc: 0.6428571343421936)
[2025-02-05 13:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:13][root][INFO] - Training Epoch: 2/2, step 19624/23838 completed (loss: 1.274176836013794, acc: 0.6100000143051147)
[2025-02-05 13:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:13][root][INFO] - Training Epoch: 2/2, step 19625/23838 completed (loss: 1.086678385734558, acc: 0.6936936974525452)
[2025-02-05 13:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:13][root][INFO] - Training Epoch: 2/2, step 19626/23838 completed (loss: 0.9572991132736206, acc: 0.7227723002433777)
[2025-02-05 13:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:14][root][INFO] - Training Epoch: 2/2, step 19627/23838 completed (loss: 1.187616229057312, acc: 0.625)
[2025-02-05 13:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:14][root][INFO] - Training Epoch: 2/2, step 19628/23838 completed (loss: 1.3893157243728638, acc: 0.5972222089767456)
[2025-02-05 13:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:14][root][INFO] - Training Epoch: 2/2, step 19629/23838 completed (loss: 1.2445368766784668, acc: 0.6770833134651184)
[2025-02-05 13:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:15][root][INFO] - Training Epoch: 2/2, step 19630/23838 completed (loss: 1.2480592727661133, acc: 0.5966386795043945)
[2025-02-05 13:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:15][root][INFO] - Training Epoch: 2/2, step 19631/23838 completed (loss: 1.2723321914672852, acc: 0.6122449040412903)
[2025-02-05 13:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:16][root][INFO] - Training Epoch: 2/2, step 19632/23838 completed (loss: 0.9972618818283081, acc: 0.7102803587913513)
[2025-02-05 13:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:16][root][INFO] - Training Epoch: 2/2, step 19633/23838 completed (loss: 0.9458049535751343, acc: 0.7042253613471985)
[2025-02-05 13:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:16][root][INFO] - Training Epoch: 2/2, step 19634/23838 completed (loss: 1.2282723188400269, acc: 0.6960784196853638)
[2025-02-05 13:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:17][root][INFO] - Training Epoch: 2/2, step 19635/23838 completed (loss: 1.2475411891937256, acc: 0.6463414430618286)
[2025-02-05 13:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:17][root][INFO] - Training Epoch: 2/2, step 19636/23838 completed (loss: 0.8776810765266418, acc: 0.7032967209815979)
[2025-02-05 13:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:17][root][INFO] - Training Epoch: 2/2, step 19637/23838 completed (loss: 1.1606653928756714, acc: 0.6357616186141968)
[2025-02-05 13:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:18][root][INFO] - Training Epoch: 2/2, step 19638/23838 completed (loss: 1.003788948059082, acc: 0.7333333492279053)
[2025-02-05 13:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:18][root][INFO] - Training Epoch: 2/2, step 19639/23838 completed (loss: 1.0462440252304077, acc: 0.6349206566810608)
[2025-02-05 13:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:19][root][INFO] - Training Epoch: 2/2, step 19640/23838 completed (loss: 0.8286347389221191, acc: 0.7714285850524902)
[2025-02-05 13:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:19][root][INFO] - Training Epoch: 2/2, step 19641/23838 completed (loss: 0.7605921626091003, acc: 0.7899159789085388)
[2025-02-05 13:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:19][root][INFO] - Training Epoch: 2/2, step 19642/23838 completed (loss: 1.0043400526046753, acc: 0.6899999976158142)
[2025-02-05 13:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:20][root][INFO] - Training Epoch: 2/2, step 19643/23838 completed (loss: 1.1260337829589844, acc: 0.692307710647583)
[2025-02-05 13:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:20][root][INFO] - Training Epoch: 2/2, step 19644/23838 completed (loss: 0.935413122177124, acc: 0.7083333134651184)
[2025-02-05 13:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:20][root][INFO] - Training Epoch: 2/2, step 19645/23838 completed (loss: 1.2418965101242065, acc: 0.6447368264198303)
[2025-02-05 13:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:21][root][INFO] - Training Epoch: 2/2, step 19646/23838 completed (loss: 1.4769237041473389, acc: 0.5797101259231567)
[2025-02-05 13:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:21][root][INFO] - Training Epoch: 2/2, step 19647/23838 completed (loss: 0.8446633219718933, acc: 0.699999988079071)
[2025-02-05 13:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:21][root][INFO] - Training Epoch: 2/2, step 19648/23838 completed (loss: 1.1743495464324951, acc: 0.6666666865348816)
[2025-02-05 13:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:22][root][INFO] - Training Epoch: 2/2, step 19649/23838 completed (loss: 1.4860763549804688, acc: 0.6329113841056824)
[2025-02-05 13:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:22][root][INFO] - Training Epoch: 2/2, step 19650/23838 completed (loss: 1.5730026960372925, acc: 0.5802469253540039)
[2025-02-05 13:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:23][root][INFO] - Training Epoch: 2/2, step 19651/23838 completed (loss: 1.3760896921157837, acc: 0.6290322542190552)
[2025-02-05 13:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:23][root][INFO] - Training Epoch: 2/2, step 19652/23838 completed (loss: 1.1553860902786255, acc: 0.593406617641449)
[2025-02-05 13:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:23][root][INFO] - Training Epoch: 2/2, step 19653/23838 completed (loss: 0.9767293334007263, acc: 0.7300000190734863)
[2025-02-05 13:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:24][root][INFO] - Training Epoch: 2/2, step 19654/23838 completed (loss: 0.8421508073806763, acc: 0.767123281955719)
[2025-02-05 13:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:24][root][INFO] - Training Epoch: 2/2, step 19655/23838 completed (loss: 0.8938935399055481, acc: 0.7088607549667358)
[2025-02-05 13:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:25][root][INFO] - Training Epoch: 2/2, step 19656/23838 completed (loss: 1.2455613613128662, acc: 0.6412213444709778)
[2025-02-05 13:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:25][root][INFO] - Training Epoch: 2/2, step 19657/23838 completed (loss: 1.2090481519699097, acc: 0.6292135119438171)
[2025-02-05 13:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:25][root][INFO] - Training Epoch: 2/2, step 19658/23838 completed (loss: 1.1569174528121948, acc: 0.6666666865348816)
[2025-02-05 13:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:26][root][INFO] - Training Epoch: 2/2, step 19659/23838 completed (loss: 0.44223254919052124, acc: 0.7857142686843872)
[2025-02-05 13:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:26][root][INFO] - Training Epoch: 2/2, step 19660/23838 completed (loss: 0.9780480265617371, acc: 0.738095223903656)
[2025-02-05 13:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:26][root][INFO] - Training Epoch: 2/2, step 19661/23838 completed (loss: 0.9216651320457458, acc: 0.7045454382896423)
[2025-02-05 13:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:27][root][INFO] - Training Epoch: 2/2, step 19662/23838 completed (loss: 1.0206784009933472, acc: 0.7111111283302307)
[2025-02-05 13:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:27][root][INFO] - Training Epoch: 2/2, step 19663/23838 completed (loss: 1.1196651458740234, acc: 0.6477272510528564)
[2025-02-05 13:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:28][root][INFO] - Training Epoch: 2/2, step 19664/23838 completed (loss: 1.2251827716827393, acc: 0.6503496766090393)
[2025-02-05 13:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:28][root][INFO] - Training Epoch: 2/2, step 19665/23838 completed (loss: 1.5536895990371704, acc: 0.5783132314682007)
[2025-02-05 13:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:28][root][INFO] - Training Epoch: 2/2, step 19666/23838 completed (loss: 1.644094467163086, acc: 0.46666666865348816)
[2025-02-05 13:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:29][root][INFO] - Training Epoch: 2/2, step 19667/23838 completed (loss: 1.4754104614257812, acc: 0.5555555820465088)
[2025-02-05 13:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:29][root][INFO] - Training Epoch: 2/2, step 19668/23838 completed (loss: 1.091185450553894, acc: 0.6451612710952759)
[2025-02-05 13:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:30][root][INFO] - Training Epoch: 2/2, step 19669/23838 completed (loss: 1.2791646718978882, acc: 0.6410256624221802)
[2025-02-05 13:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:30][root][INFO] - Training Epoch: 2/2, step 19670/23838 completed (loss: 1.2554556131362915, acc: 0.5909090638160706)
[2025-02-05 13:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:30][root][INFO] - Training Epoch: 2/2, step 19671/23838 completed (loss: 0.8292825222015381, acc: 0.7538461685180664)
[2025-02-05 13:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:31][root][INFO] - Training Epoch: 2/2, step 19672/23838 completed (loss: 1.355627179145813, acc: 0.6173912882804871)
[2025-02-05 13:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:31][root][INFO] - Training Epoch: 2/2, step 19673/23838 completed (loss: 0.9306116104125977, acc: 0.707317054271698)
[2025-02-05 13:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:32][root][INFO] - Training Epoch: 2/2, step 19674/23838 completed (loss: 1.3961741924285889, acc: 0.6057142615318298)
[2025-02-05 13:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:32][root][INFO] - Training Epoch: 2/2, step 19675/23838 completed (loss: 1.2834571599960327, acc: 0.6012658476829529)
[2025-02-05 13:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:33][root][INFO] - Training Epoch: 2/2, step 19676/23838 completed (loss: 1.2686877250671387, acc: 0.6499999761581421)
[2025-02-05 13:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:33][root][INFO] - Training Epoch: 2/2, step 19677/23838 completed (loss: 0.5466220378875732, acc: 0.8840579986572266)
[2025-02-05 13:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:33][root][INFO] - Training Epoch: 2/2, step 19678/23838 completed (loss: 1.0111764669418335, acc: 0.6728395223617554)
[2025-02-05 13:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:34][root][INFO] - Training Epoch: 2/2, step 19679/23838 completed (loss: 1.1637701988220215, acc: 0.6704545617103577)
[2025-02-05 13:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:34][root][INFO] - Training Epoch: 2/2, step 19680/23838 completed (loss: 0.5722284913063049, acc: 0.7903226017951965)
[2025-02-05 13:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:35][root][INFO] - Training Epoch: 2/2, step 19681/23838 completed (loss: 1.0957199335098267, acc: 0.6813187003135681)
[2025-02-05 13:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:35][root][INFO] - Training Epoch: 2/2, step 19682/23838 completed (loss: 1.3341491222381592, acc: 0.5935828685760498)
[2025-02-05 13:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:35][root][INFO] - Training Epoch: 2/2, step 19683/23838 completed (loss: 1.1977019309997559, acc: 0.6702127456665039)
[2025-02-05 13:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:36][root][INFO] - Training Epoch: 2/2, step 19684/23838 completed (loss: 0.7984234690666199, acc: 0.7454545497894287)
[2025-02-05 13:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:36][root][INFO] - Training Epoch: 2/2, step 19685/23838 completed (loss: 0.7291392683982849, acc: 0.762499988079071)
[2025-02-05 13:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:36][root][INFO] - Training Epoch: 2/2, step 19686/23838 completed (loss: 0.7405795454978943, acc: 0.7837837934494019)
[2025-02-05 13:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:37][root][INFO] - Training Epoch: 2/2, step 19687/23838 completed (loss: 1.171773076057434, acc: 0.6899224519729614)
[2025-02-05 13:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:37][root][INFO] - Training Epoch: 2/2, step 19688/23838 completed (loss: 0.8511958122253418, acc: 0.7268041372299194)
[2025-02-05 13:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:38][root][INFO] - Training Epoch: 2/2, step 19689/23838 completed (loss: 0.808805525302887, acc: 0.7407407164573669)
[2025-02-05 13:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:38][root][INFO] - Training Epoch: 2/2, step 19690/23838 completed (loss: 0.8522362112998962, acc: 0.6979166865348816)
[2025-02-05 13:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:38][root][INFO] - Training Epoch: 2/2, step 19691/23838 completed (loss: 0.9761542677879333, acc: 0.7118644118309021)
[2025-02-05 13:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:39][root][INFO] - Training Epoch: 2/2, step 19692/23838 completed (loss: 0.9352080225944519, acc: 0.7169811129570007)
[2025-02-05 13:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:39][root][INFO] - Training Epoch: 2/2, step 19693/23838 completed (loss: 0.843211829662323, acc: 0.7260273694992065)
[2025-02-05 13:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:40][root][INFO] - Training Epoch: 2/2, step 19694/23838 completed (loss: 0.5680713653564453, acc: 0.8028169274330139)
[2025-02-05 13:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:40][root][INFO] - Training Epoch: 2/2, step 19695/23838 completed (loss: 0.5133764147758484, acc: 0.8700000047683716)
[2025-02-05 13:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:40][root][INFO] - Training Epoch: 2/2, step 19696/23838 completed (loss: 0.5823952555656433, acc: 0.8451613187789917)
[2025-02-05 13:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:41][root][INFO] - Training Epoch: 2/2, step 19697/23838 completed (loss: 0.9803557395935059, acc: 0.7684210538864136)
[2025-02-05 13:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:41][root][INFO] - Training Epoch: 2/2, step 19698/23838 completed (loss: 1.0714890956878662, acc: 0.65625)
[2025-02-05 13:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:41][root][INFO] - Training Epoch: 2/2, step 19699/23838 completed (loss: 1.688984990119934, acc: 0.5633803009986877)
[2025-02-05 13:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:42][root][INFO] - Training Epoch: 2/2, step 19700/23838 completed (loss: 1.2792530059814453, acc: 0.634782612323761)
[2025-02-05 13:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:42][root][INFO] - Training Epoch: 2/2, step 19701/23838 completed (loss: 1.061607837677002, acc: 0.6320754885673523)
[2025-02-05 13:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:43][root][INFO] - Training Epoch: 2/2, step 19702/23838 completed (loss: 1.08048677444458, acc: 0.6530612111091614)
[2025-02-05 13:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:43][root][INFO] - Training Epoch: 2/2, step 19703/23838 completed (loss: 1.4026362895965576, acc: 0.6499999761581421)
[2025-02-05 13:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:43][root][INFO] - Training Epoch: 2/2, step 19704/23838 completed (loss: 1.7660146951675415, acc: 0.5227272510528564)
[2025-02-05 13:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:44][root][INFO] - Training Epoch: 2/2, step 19705/23838 completed (loss: 1.416207194328308, acc: 0.6121212244033813)
[2025-02-05 13:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:44][root][INFO] - Training Epoch: 2/2, step 19706/23838 completed (loss: 1.4571106433868408, acc: 0.6000000238418579)
[2025-02-05 13:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:45][root][INFO] - Training Epoch: 2/2, step 19707/23838 completed (loss: 1.5645829439163208, acc: 0.5752212405204773)
[2025-02-05 13:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:45][root][INFO] - Training Epoch: 2/2, step 19708/23838 completed (loss: 1.5762968063354492, acc: 0.5897436141967773)
[2025-02-05 13:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:45][root][INFO] - Training Epoch: 2/2, step 19709/23838 completed (loss: 1.4534292221069336, acc: 0.6000000238418579)
[2025-02-05 13:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:46][root][INFO] - Training Epoch: 2/2, step 19710/23838 completed (loss: 1.225058674812317, acc: 0.6666666865348816)
[2025-02-05 13:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:46][root][INFO] - Training Epoch: 2/2, step 19711/23838 completed (loss: 1.520682692527771, acc: 0.5789473652839661)
[2025-02-05 13:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:46][root][INFO] - Training Epoch: 2/2, step 19712/23838 completed (loss: 1.5680127143859863, acc: 0.5625)
[2025-02-05 13:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:47][root][INFO] - Training Epoch: 2/2, step 19713/23838 completed (loss: 1.331679344177246, acc: 0.6071428656578064)
[2025-02-05 13:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:47][root][INFO] - Training Epoch: 2/2, step 19714/23838 completed (loss: 1.601585030555725, acc: 0.553398072719574)
[2025-02-05 13:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:48][root][INFO] - Training Epoch: 2/2, step 19715/23838 completed (loss: 1.545264720916748, acc: 0.6000000238418579)
[2025-02-05 13:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:48][root][INFO] - Training Epoch: 2/2, step 19716/23838 completed (loss: 1.4347918033599854, acc: 0.5726495981216431)
[2025-02-05 13:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:49][root][INFO] - Training Epoch: 2/2, step 19717/23838 completed (loss: 1.1485884189605713, acc: 0.6707317233085632)
[2025-02-05 13:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:49][root][INFO] - Training Epoch: 2/2, step 19718/23838 completed (loss: 1.3804242610931396, acc: 0.6040816307067871)
[2025-02-05 13:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:49][root][INFO] - Training Epoch: 2/2, step 19719/23838 completed (loss: 1.5093938112258911, acc: 0.5909090638160706)
[2025-02-05 13:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:50][root][INFO] - Training Epoch: 2/2, step 19720/23838 completed (loss: 0.8441516160964966, acc: 0.761904776096344)
[2025-02-05 13:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:50][root][INFO] - Training Epoch: 2/2, step 19721/23838 completed (loss: 0.927649974822998, acc: 0.6666666865348816)
[2025-02-05 13:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:50][root][INFO] - Training Epoch: 2/2, step 19722/23838 completed (loss: 1.3251540660858154, acc: 0.607594907283783)
[2025-02-05 13:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:51][root][INFO] - Training Epoch: 2/2, step 19723/23838 completed (loss: 1.2901264429092407, acc: 0.5263158082962036)
[2025-02-05 13:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:51][root][INFO] - Training Epoch: 2/2, step 19724/23838 completed (loss: 1.2144134044647217, acc: 0.6304348111152649)
[2025-02-05 13:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:51][root][INFO] - Training Epoch: 2/2, step 19725/23838 completed (loss: 1.25644850730896, acc: 0.6470588445663452)
[2025-02-05 13:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:52][root][INFO] - Training Epoch: 2/2, step 19726/23838 completed (loss: 1.5576214790344238, acc: 0.5757575631141663)
[2025-02-05 13:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:52][root][INFO] - Training Epoch: 2/2, step 19727/23838 completed (loss: 1.1267046928405762, acc: 0.6756756901741028)
[2025-02-05 13:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:53][root][INFO] - Training Epoch: 2/2, step 19728/23838 completed (loss: 1.193242073059082, acc: 0.6836734414100647)
[2025-02-05 13:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:53][root][INFO] - Training Epoch: 2/2, step 19729/23838 completed (loss: 1.2228007316589355, acc: 0.6614173054695129)
[2025-02-05 13:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:54][root][INFO] - Training Epoch: 2/2, step 19730/23838 completed (loss: 1.2793270349502563, acc: 0.6126126050949097)
[2025-02-05 13:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:54][root][INFO] - Training Epoch: 2/2, step 19731/23838 completed (loss: 1.0656074285507202, acc: 0.65625)
[2025-02-05 13:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:54][root][INFO] - Training Epoch: 2/2, step 19732/23838 completed (loss: 1.023115634918213, acc: 0.684684693813324)
[2025-02-05 13:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:55][root][INFO] - Training Epoch: 2/2, step 19733/23838 completed (loss: 1.227205753326416, acc: 0.6513158082962036)
[2025-02-05 13:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:55][root][INFO] - Training Epoch: 2/2, step 19734/23838 completed (loss: 1.2306363582611084, acc: 0.6352941393852234)
[2025-02-05 13:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:56][root][INFO] - Training Epoch: 2/2, step 19735/23838 completed (loss: 1.1149107217788696, acc: 0.6833333373069763)
[2025-02-05 13:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:56][root][INFO] - Training Epoch: 2/2, step 19736/23838 completed (loss: 0.7693496346473694, acc: 0.759036123752594)
[2025-02-05 13:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:56][root][INFO] - Training Epoch: 2/2, step 19737/23838 completed (loss: 1.367112159729004, acc: 0.6041666865348816)
[2025-02-05 13:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:57][root][INFO] - Training Epoch: 2/2, step 19738/23838 completed (loss: 1.2436513900756836, acc: 0.6666666865348816)
[2025-02-05 13:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:57][root][INFO] - Training Epoch: 2/2, step 19739/23838 completed (loss: 1.1171910762786865, acc: 0.6423357725143433)
[2025-02-05 13:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:58][root][INFO] - Training Epoch: 2/2, step 19740/23838 completed (loss: 1.0336743593215942, acc: 0.6666666865348816)
[2025-02-05 13:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:58][root][INFO] - Training Epoch: 2/2, step 19741/23838 completed (loss: 1.0262117385864258, acc: 0.695652186870575)
[2025-02-05 13:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:58][root][INFO] - Training Epoch: 2/2, step 19742/23838 completed (loss: 1.0353237390518188, acc: 0.6521739363670349)
[2025-02-05 13:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:59][root][INFO] - Training Epoch: 2/2, step 19743/23838 completed (loss: 0.6273330450057983, acc: 0.8095238208770752)
[2025-02-05 13:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:51:59][root][INFO] - Training Epoch: 2/2, step 19744/23838 completed (loss: 1.076789379119873, acc: 0.6842105388641357)
[2025-02-05 13:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:00][root][INFO] - Training Epoch: 2/2, step 19745/23838 completed (loss: 0.8635252714157104, acc: 0.7540983557701111)
[2025-02-05 13:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:00][root][INFO] - Training Epoch: 2/2, step 19746/23838 completed (loss: 0.7186551094055176, acc: 0.8082191944122314)
[2025-02-05 13:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:00][root][INFO] - Training Epoch: 2/2, step 19747/23838 completed (loss: 1.0051268339157104, acc: 0.698630154132843)
[2025-02-05 13:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:01][root][INFO] - Training Epoch: 2/2, step 19748/23838 completed (loss: 0.9534400105476379, acc: 0.75)
[2025-02-05 13:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:01][root][INFO] - Training Epoch: 2/2, step 19749/23838 completed (loss: 1.2216447591781616, acc: 0.6610169410705566)
[2025-02-05 13:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:02][root][INFO] - Training Epoch: 2/2, step 19750/23838 completed (loss: 0.8665740489959717, acc: 0.7246376872062683)
[2025-02-05 13:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:02][root][INFO] - Training Epoch: 2/2, step 19751/23838 completed (loss: 1.1306873559951782, acc: 0.6415094137191772)
[2025-02-05 13:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:02][root][INFO] - Training Epoch: 2/2, step 19752/23838 completed (loss: 0.7836225032806396, acc: 0.8181818127632141)
[2025-02-05 13:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:03][root][INFO] - Training Epoch: 2/2, step 19753/23838 completed (loss: 1.0931223630905151, acc: 0.6851851940155029)
[2025-02-05 13:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:03][root][INFO] - Training Epoch: 2/2, step 19754/23838 completed (loss: 1.1207395792007446, acc: 0.6279069781303406)
[2025-02-05 13:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:04][root][INFO] - Training Epoch: 2/2, step 19755/23838 completed (loss: 1.2253714799880981, acc: 0.675000011920929)
[2025-02-05 13:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:04][root][INFO] - Training Epoch: 2/2, step 19756/23838 completed (loss: 1.3716394901275635, acc: 0.6785714030265808)
[2025-02-05 13:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:04][root][INFO] - Training Epoch: 2/2, step 19757/23838 completed (loss: 1.3511744737625122, acc: 0.6153846383094788)
[2025-02-05 13:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:05][root][INFO] - Training Epoch: 2/2, step 19758/23838 completed (loss: 1.1785478591918945, acc: 0.6440678238868713)
[2025-02-05 13:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:05][root][INFO] - Training Epoch: 2/2, step 19759/23838 completed (loss: 1.1064479351043701, acc: 0.6764705777168274)
[2025-02-05 13:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:05][root][INFO] - Training Epoch: 2/2, step 19760/23838 completed (loss: 1.2221848964691162, acc: 0.6888889074325562)
[2025-02-05 13:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:06][root][INFO] - Training Epoch: 2/2, step 19761/23838 completed (loss: 1.3465343713760376, acc: 0.5833333134651184)
[2025-02-05 13:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:06][root][INFO] - Training Epoch: 2/2, step 19762/23838 completed (loss: 0.9421167373657227, acc: 0.7659574747085571)
[2025-02-05 13:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:07][root][INFO] - Training Epoch: 2/2, step 19763/23838 completed (loss: 1.029616117477417, acc: 0.6734693646430969)
[2025-02-05 13:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:07][root][INFO] - Training Epoch: 2/2, step 19764/23838 completed (loss: 1.3740077018737793, acc: 0.6206896305084229)
[2025-02-05 13:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:07][root][INFO] - Training Epoch: 2/2, step 19765/23838 completed (loss: 1.5624111890792847, acc: 0.5633803009986877)
[2025-02-05 13:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:08][root][INFO] - Training Epoch: 2/2, step 19766/23838 completed (loss: 1.3925310373306274, acc: 0.5641025900840759)
[2025-02-05 13:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:08][root][INFO] - Training Epoch: 2/2, step 19767/23838 completed (loss: 1.4381729364395142, acc: 0.4399999976158142)
[2025-02-05 13:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:08][root][INFO] - Training Epoch: 2/2, step 19768/23838 completed (loss: 1.2930659055709839, acc: 0.6428571343421936)
[2025-02-05 13:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:09][root][INFO] - Training Epoch: 2/2, step 19769/23838 completed (loss: 1.2958873510360718, acc: 0.6190476417541504)
[2025-02-05 13:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:09][root][INFO] - Training Epoch: 2/2, step 19770/23838 completed (loss: 1.2805016040802002, acc: 0.6511628031730652)
[2025-02-05 13:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:10][root][INFO] - Training Epoch: 2/2, step 19771/23838 completed (loss: 1.288243293762207, acc: 0.5806451439857483)
[2025-02-05 13:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:10][root][INFO] - Training Epoch: 2/2, step 19772/23838 completed (loss: 1.1542567014694214, acc: 0.625)
[2025-02-05 13:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:10][root][INFO] - Training Epoch: 2/2, step 19773/23838 completed (loss: 1.282471776008606, acc: 0.6078431606292725)
[2025-02-05 13:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:11][root][INFO] - Training Epoch: 2/2, step 19774/23838 completed (loss: 1.2196604013442993, acc: 0.6285714507102966)
[2025-02-05 13:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:11][root][INFO] - Training Epoch: 2/2, step 19775/23838 completed (loss: 1.014917016029358, acc: 0.699999988079071)
[2025-02-05 13:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:11][root][INFO] - Training Epoch: 2/2, step 19776/23838 completed (loss: 1.3320220708847046, acc: 0.6842105388641357)
[2025-02-05 13:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:12][root][INFO] - Training Epoch: 2/2, step 19777/23838 completed (loss: 1.3750834465026855, acc: 0.6388888955116272)
[2025-02-05 13:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:12][root][INFO] - Training Epoch: 2/2, step 19778/23838 completed (loss: 1.0455397367477417, acc: 0.625)
[2025-02-05 13:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:13][root][INFO] - Training Epoch: 2/2, step 19779/23838 completed (loss: 1.0562372207641602, acc: 0.6842105388641357)
[2025-02-05 13:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:13][root][INFO] - Training Epoch: 2/2, step 19780/23838 completed (loss: 1.2604916095733643, acc: 0.6029411554336548)
[2025-02-05 13:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:13][root][INFO] - Training Epoch: 2/2, step 19781/23838 completed (loss: 1.1381609439849854, acc: 0.6785714030265808)
[2025-02-05 13:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:14][root][INFO] - Training Epoch: 2/2, step 19782/23838 completed (loss: 0.9538220167160034, acc: 0.7339449524879456)
[2025-02-05 13:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:14][root][INFO] - Training Epoch: 2/2, step 19783/23838 completed (loss: 1.2275930643081665, acc: 0.699999988079071)
[2025-02-05 13:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:15][root][INFO] - Training Epoch: 2/2, step 19784/23838 completed (loss: 1.0831146240234375, acc: 0.6578947305679321)
[2025-02-05 13:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:15][root][INFO] - Training Epoch: 2/2, step 19785/23838 completed (loss: 1.2474761009216309, acc: 0.6043956279754639)
[2025-02-05 13:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:16][root][INFO] - Training Epoch: 2/2, step 19786/23838 completed (loss: 1.174186110496521, acc: 0.7090908885002136)
[2025-02-05 13:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:16][root][INFO] - Training Epoch: 2/2, step 19787/23838 completed (loss: 1.379988431930542, acc: 0.5614035129547119)
[2025-02-05 13:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:17][root][INFO] - Training Epoch: 2/2, step 19788/23838 completed (loss: 1.0644500255584717, acc: 0.695652186870575)
[2025-02-05 13:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:17][root][INFO] - Training Epoch: 2/2, step 19789/23838 completed (loss: 1.166785717010498, acc: 0.6451612710952759)
[2025-02-05 13:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:17][root][INFO] - Training Epoch: 2/2, step 19790/23838 completed (loss: 1.1148297786712646, acc: 0.6857143044471741)
[2025-02-05 13:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:18][root][INFO] - Training Epoch: 2/2, step 19791/23838 completed (loss: 1.4518557786941528, acc: 0.5492957830429077)
[2025-02-05 13:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:18][root][INFO] - Training Epoch: 2/2, step 19792/23838 completed (loss: 1.048440933227539, acc: 0.6530612111091614)
[2025-02-05 13:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:19][root][INFO] - Training Epoch: 2/2, step 19793/23838 completed (loss: 1.3720945119857788, acc: 0.6265060305595398)
[2025-02-05 13:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:19][root][INFO] - Training Epoch: 2/2, step 19794/23838 completed (loss: 0.8224880695343018, acc: 0.7291666865348816)
[2025-02-05 13:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:20][root][INFO] - Training Epoch: 2/2, step 19795/23838 completed (loss: 0.9048722982406616, acc: 0.7209302186965942)
[2025-02-05 13:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:20][root][INFO] - Training Epoch: 2/2, step 19796/23838 completed (loss: 0.9554907083511353, acc: 0.746666669845581)
[2025-02-05 13:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:20][root][INFO] - Training Epoch: 2/2, step 19797/23838 completed (loss: 1.3287054300308228, acc: 0.5492957830429077)
[2025-02-05 13:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:21][root][INFO] - Training Epoch: 2/2, step 19798/23838 completed (loss: 1.0334281921386719, acc: 0.7083333134651184)
[2025-02-05 13:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:21][root][INFO] - Training Epoch: 2/2, step 19799/23838 completed (loss: 1.0661039352416992, acc: 0.6226415038108826)
[2025-02-05 13:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:22][root][INFO] - Training Epoch: 2/2, step 19800/23838 completed (loss: 0.9095244407653809, acc: 0.699999988079071)
[2025-02-05 13:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:22][root][INFO] - Training Epoch: 2/2, step 19801/23838 completed (loss: 1.0082107782363892, acc: 0.7407407164573669)
[2025-02-05 13:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:22][root][INFO] - Training Epoch: 2/2, step 19802/23838 completed (loss: 1.110068917274475, acc: 0.6507936716079712)
[2025-02-05 13:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:23][root][INFO] - Training Epoch: 2/2, step 19803/23838 completed (loss: 0.9911938905715942, acc: 0.703125)
[2025-02-05 13:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:23][root][INFO] - Training Epoch: 2/2, step 19804/23838 completed (loss: 1.3973290920257568, acc: 0.5909090638160706)
[2025-02-05 13:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:23][root][INFO] - Training Epoch: 2/2, step 19805/23838 completed (loss: 0.923022449016571, acc: 0.7435897588729858)
[2025-02-05 13:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:24][root][INFO] - Training Epoch: 2/2, step 19806/23838 completed (loss: 0.8975996971130371, acc: 0.7333333492279053)
[2025-02-05 13:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:24][root][INFO] - Training Epoch: 2/2, step 19807/23838 completed (loss: 0.723063051700592, acc: 0.8333333134651184)
[2025-02-05 13:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:25][root][INFO] - Training Epoch: 2/2, step 19808/23838 completed (loss: 0.7484063506126404, acc: 0.7659574747085571)
[2025-02-05 13:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:25][root][INFO] - Training Epoch: 2/2, step 19809/23838 completed (loss: 1.2647347450256348, acc: 0.6571428775787354)
[2025-02-05 13:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:25][root][INFO] - Training Epoch: 2/2, step 19810/23838 completed (loss: 0.8064476251602173, acc: 0.6739130616188049)
[2025-02-05 13:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:26][root][INFO] - Training Epoch: 2/2, step 19811/23838 completed (loss: 1.167657732963562, acc: 0.644444465637207)
[2025-02-05 13:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:26][root][INFO] - Training Epoch: 2/2, step 19812/23838 completed (loss: 0.763891875743866, acc: 0.8214285969734192)
[2025-02-05 13:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:26][root][INFO] - Training Epoch: 2/2, step 19813/23838 completed (loss: 1.2666146755218506, acc: 0.65625)
[2025-02-05 13:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:27][root][INFO] - Training Epoch: 2/2, step 19814/23838 completed (loss: 0.891298234462738, acc: 0.692307710647583)
[2025-02-05 13:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:27][root][INFO] - Training Epoch: 2/2, step 19815/23838 completed (loss: 0.863145649433136, acc: 0.6470588445663452)
[2025-02-05 13:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:27][root][INFO] - Training Epoch: 2/2, step 19816/23838 completed (loss: 1.5151501893997192, acc: 0.5957446694374084)
[2025-02-05 13:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:28][root][INFO] - Training Epoch: 2/2, step 19817/23838 completed (loss: 1.7948596477508545, acc: 0.5090909004211426)
[2025-02-05 13:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:28][root][INFO] - Training Epoch: 2/2, step 19818/23838 completed (loss: 1.1343215703964233, acc: 0.6666666865348816)
[2025-02-05 13:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:29][root][INFO] - Training Epoch: 2/2, step 19819/23838 completed (loss: 1.934391736984253, acc: 0.550000011920929)
[2025-02-05 13:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:29][root][INFO] - Training Epoch: 2/2, step 19820/23838 completed (loss: 1.4491608142852783, acc: 0.523809552192688)
[2025-02-05 13:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:30][root][INFO] - Training Epoch: 2/2, step 19821/23838 completed (loss: 2.151515007019043, acc: 0.2857142984867096)
[2025-02-05 13:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:30][root][INFO] - Training Epoch: 2/2, step 19822/23838 completed (loss: 2.0104918479919434, acc: 0.3636363744735718)
[2025-02-05 13:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:30][root][INFO] - Training Epoch: 2/2, step 19823/23838 completed (loss: 1.3813090324401855, acc: 0.5454545617103577)
[2025-02-05 13:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:31][root][INFO] - Training Epoch: 2/2, step 19824/23838 completed (loss: 2.579754590988159, acc: 0.31578946113586426)
[2025-02-05 13:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:31][root][INFO] - Training Epoch: 2/2, step 19825/23838 completed (loss: 2.0799427032470703, acc: 0.5)
[2025-02-05 13:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:31][root][INFO] - Training Epoch: 2/2, step 19826/23838 completed (loss: 1.551293134689331, acc: 0.5652173757553101)
[2025-02-05 13:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:32][root][INFO] - Training Epoch: 2/2, step 19827/23838 completed (loss: 1.977463960647583, acc: 0.3125)
[2025-02-05 13:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:32][root][INFO] - Training Epoch: 2/2, step 19828/23838 completed (loss: 1.9261493682861328, acc: 0.3499999940395355)
[2025-02-05 13:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:32][root][INFO] - Training Epoch: 2/2, step 19829/23838 completed (loss: 1.4863402843475342, acc: 0.5652173757553101)
[2025-02-05 13:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:33][root][INFO] - Training Epoch: 2/2, step 19830/23838 completed (loss: 1.7531071901321411, acc: 0.46666666865348816)
[2025-02-05 13:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:33][root][INFO] - Training Epoch: 2/2, step 19831/23838 completed (loss: 1.8749008178710938, acc: 0.2631579041481018)
[2025-02-05 13:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:34][root][INFO] - Training Epoch: 2/2, step 19832/23838 completed (loss: 1.6890822649002075, acc: 0.4736842215061188)
[2025-02-05 13:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:34][root][INFO] - Training Epoch: 2/2, step 19833/23838 completed (loss: 1.3935716152191162, acc: 0.4761904776096344)
[2025-02-05 13:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:34][root][INFO] - Training Epoch: 2/2, step 19834/23838 completed (loss: 1.6324924230575562, acc: 0.5223880410194397)
[2025-02-05 13:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:35][root][INFO] - Training Epoch: 2/2, step 19835/23838 completed (loss: 2.1505279541015625, acc: 0.4324324429035187)
[2025-02-05 13:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:35][root][INFO] - Training Epoch: 2/2, step 19836/23838 completed (loss: 1.4355312585830688, acc: 0.6086956262588501)
[2025-02-05 13:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:35][root][INFO] - Training Epoch: 2/2, step 19837/23838 completed (loss: 1.014274001121521, acc: 0.6666666865348816)
[2025-02-05 13:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:36][root][INFO] - Training Epoch: 2/2, step 19838/23838 completed (loss: 1.1303414106369019, acc: 0.6666666865348816)
[2025-02-05 13:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:36][root][INFO] - Training Epoch: 2/2, step 19839/23838 completed (loss: 1.1657688617706299, acc: 0.6373626589775085)
[2025-02-05 13:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:37][root][INFO] - Training Epoch: 2/2, step 19840/23838 completed (loss: 1.2376439571380615, acc: 0.6641221642494202)
[2025-02-05 13:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:37][root][INFO] - Training Epoch: 2/2, step 19841/23838 completed (loss: 1.2104551792144775, acc: 0.699999988079071)
[2025-02-05 13:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:38][root][INFO] - Training Epoch: 2/2, step 19842/23838 completed (loss: 1.3378592729568481, acc: 0.5853658318519592)
[2025-02-05 13:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:38][root][INFO] - Training Epoch: 2/2, step 19843/23838 completed (loss: 1.541894793510437, acc: 0.6164383292198181)
[2025-02-05 13:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:39][root][INFO] - Training Epoch: 2/2, step 19844/23838 completed (loss: 1.0142557621002197, acc: 0.720588207244873)
[2025-02-05 13:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:39][root][INFO] - Training Epoch: 2/2, step 19845/23838 completed (loss: 0.33828452229499817, acc: 0.9180327653884888)
[2025-02-05 13:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:39][root][INFO] - Training Epoch: 2/2, step 19846/23838 completed (loss: 0.4358292818069458, acc: 0.876288652420044)
[2025-02-05 13:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:40][root][INFO] - Training Epoch: 2/2, step 19847/23838 completed (loss: 0.4787091016769409, acc: 0.8777777552604675)
[2025-02-05 13:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:40][root][INFO] - Training Epoch: 2/2, step 19848/23838 completed (loss: 0.7898653745651245, acc: 0.8021978139877319)
[2025-02-05 13:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:40][root][INFO] - Training Epoch: 2/2, step 19849/23838 completed (loss: 1.068386435508728, acc: 0.7123287916183472)
[2025-02-05 13:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:41][root][INFO] - Training Epoch: 2/2, step 19850/23838 completed (loss: 0.8364619612693787, acc: 0.7702702879905701)
[2025-02-05 13:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:41][root][INFO] - Training Epoch: 2/2, step 19851/23838 completed (loss: 1.1070407629013062, acc: 0.7090908885002136)
[2025-02-05 13:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:42][root][INFO] - Training Epoch: 2/2, step 19852/23838 completed (loss: 1.21924889087677, acc: 0.640625)
[2025-02-05 13:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:42][root][INFO] - Training Epoch: 2/2, step 19853/23838 completed (loss: 1.0227230787277222, acc: 0.7142857313156128)
[2025-02-05 13:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:42][root][INFO] - Training Epoch: 2/2, step 19854/23838 completed (loss: 1.397986888885498, acc: 0.7777777910232544)
[2025-02-05 13:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:43][root][INFO] - Training Epoch: 2/2, step 19855/23838 completed (loss: 0.8344100713729858, acc: 0.7857142686843872)
[2025-02-05 13:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:43][root][INFO] - Training Epoch: 2/2, step 19856/23838 completed (loss: 1.2881834506988525, acc: 0.6363636255264282)
[2025-02-05 13:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:44][root][INFO] - Training Epoch: 2/2, step 19857/23838 completed (loss: 1.1609514951705933, acc: 0.7179487347602844)
[2025-02-05 13:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:44][root][INFO] - Training Epoch: 2/2, step 19858/23838 completed (loss: 0.8273106217384338, acc: 0.8235294222831726)
[2025-02-05 13:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:44][root][INFO] - Training Epoch: 2/2, step 19859/23838 completed (loss: 1.4151641130447388, acc: 0.625)
[2025-02-05 13:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:45][root][INFO] - Training Epoch: 2/2, step 19860/23838 completed (loss: 0.7060998678207397, acc: 0.8235294222831726)
[2025-02-05 13:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:45][root][INFO] - Training Epoch: 2/2, step 19861/23838 completed (loss: 1.3048582077026367, acc: 0.6666666865348816)
[2025-02-05 13:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:45][root][INFO] - Training Epoch: 2/2, step 19862/23838 completed (loss: 0.7862993478775024, acc: 0.760869562625885)
[2025-02-05 13:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:46][root][INFO] - Training Epoch: 2/2, step 19863/23838 completed (loss: 0.5339170694351196, acc: 0.9047619104385376)
[2025-02-05 13:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:46][root][INFO] - Training Epoch: 2/2, step 19864/23838 completed (loss: 1.4161487817764282, acc: 0.5757575631141663)
[2025-02-05 13:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:47][root][INFO] - Training Epoch: 2/2, step 19865/23838 completed (loss: 0.9551090002059937, acc: 0.7333333492279053)
[2025-02-05 13:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:47][root][INFO] - Training Epoch: 2/2, step 19866/23838 completed (loss: 1.319225788116455, acc: 0.6162790656089783)
[2025-02-05 13:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:47][root][INFO] - Training Epoch: 2/2, step 19867/23838 completed (loss: 1.456233024597168, acc: 0.6419752836227417)
[2025-02-05 13:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:48][root][INFO] - Training Epoch: 2/2, step 19868/23838 completed (loss: 1.5986679792404175, acc: 0.581632673740387)
[2025-02-05 13:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:48][root][INFO] - Training Epoch: 2/2, step 19869/23838 completed (loss: 1.203216314315796, acc: 0.640816330909729)
[2025-02-05 13:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:49][root][INFO] - Training Epoch: 2/2, step 19870/23838 completed (loss: 1.5074294805526733, acc: 0.6060606241226196)
[2025-02-05 13:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:49][root][INFO] - Training Epoch: 2/2, step 19871/23838 completed (loss: 1.49530827999115, acc: 0.607594907283783)
[2025-02-05 13:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:50][root][INFO] - Training Epoch: 2/2, step 19872/23838 completed (loss: 1.1847846508026123, acc: 0.6615384817123413)
[2025-02-05 13:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:50][root][INFO] - Training Epoch: 2/2, step 19873/23838 completed (loss: 1.7110549211502075, acc: 0.5147058963775635)
[2025-02-05 13:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:50][root][INFO] - Training Epoch: 2/2, step 19874/23838 completed (loss: 1.3339333534240723, acc: 0.5698924660682678)
[2025-02-05 13:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:51][root][INFO] - Training Epoch: 2/2, step 19875/23838 completed (loss: 1.321427583694458, acc: 0.6333333253860474)
[2025-02-05 13:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:51][root][INFO] - Training Epoch: 2/2, step 19876/23838 completed (loss: 1.4936411380767822, acc: 0.5769230723381042)
[2025-02-05 13:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:52][root][INFO] - Training Epoch: 2/2, step 19877/23838 completed (loss: 0.9228547811508179, acc: 0.7123287916183472)
[2025-02-05 13:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:52][root][INFO] - Training Epoch: 2/2, step 19878/23838 completed (loss: 1.601233720779419, acc: 0.587837815284729)
[2025-02-05 13:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:52][root][INFO] - Training Epoch: 2/2, step 19879/23838 completed (loss: 1.4741485118865967, acc: 0.5909090638160706)
[2025-02-05 13:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:53][root][INFO] - Training Epoch: 2/2, step 19880/23838 completed (loss: 1.32549250125885, acc: 0.6082473993301392)
[2025-02-05 13:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:53][root][INFO] - Training Epoch: 2/2, step 19881/23838 completed (loss: 1.208240270614624, acc: 0.6442307829856873)
[2025-02-05 13:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:54][root][INFO] - Training Epoch: 2/2, step 19882/23838 completed (loss: 1.5902841091156006, acc: 0.5494505763053894)
[2025-02-05 13:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:54][root][INFO] - Training Epoch: 2/2, step 19883/23838 completed (loss: 0.966367244720459, acc: 0.7216494679450989)
[2025-02-05 13:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:54][root][INFO] - Training Epoch: 2/2, step 19884/23838 completed (loss: 1.1027315855026245, acc: 0.6293103694915771)
[2025-02-05 13:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:55][root][INFO] - Training Epoch: 2/2, step 19885/23838 completed (loss: 1.1853818893432617, acc: 0.6470588445663452)
[2025-02-05 13:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:55][root][INFO] - Training Epoch: 2/2, step 19886/23838 completed (loss: 0.9760012626647949, acc: 0.695652186870575)
[2025-02-05 13:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:55][root][INFO] - Training Epoch: 2/2, step 19887/23838 completed (loss: 1.10728919506073, acc: 0.6858974099159241)
[2025-02-05 13:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:56][root][INFO] - Training Epoch: 2/2, step 19888/23838 completed (loss: 0.9379615783691406, acc: 0.7699999809265137)
[2025-02-05 13:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:56][root][INFO] - Training Epoch: 2/2, step 19889/23838 completed (loss: 1.078534722328186, acc: 0.6691176295280457)
[2025-02-05 13:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:57][root][INFO] - Training Epoch: 2/2, step 19890/23838 completed (loss: 0.9385599493980408, acc: 0.7777777910232544)
[2025-02-05 13:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:57][root][INFO] - Training Epoch: 2/2, step 19891/23838 completed (loss: 1.0424185991287231, acc: 0.692307710647583)
[2025-02-05 13:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:57][root][INFO] - Training Epoch: 2/2, step 19892/23838 completed (loss: 1.006516933441162, acc: 0.732758641242981)
[2025-02-05 13:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:58][root][INFO] - Training Epoch: 2/2, step 19893/23838 completed (loss: 1.229162573814392, acc: 0.6283186078071594)
[2025-02-05 13:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:58][root][INFO] - Training Epoch: 2/2, step 19894/23838 completed (loss: 0.8821205496788025, acc: 0.7320261597633362)
[2025-02-05 13:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:58][root][INFO] - Training Epoch: 2/2, step 19895/23838 completed (loss: 1.0711050033569336, acc: 0.6710526347160339)
[2025-02-05 13:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:59][root][INFO] - Training Epoch: 2/2, step 19896/23838 completed (loss: 1.2143356800079346, acc: 0.621052622795105)
[2025-02-05 13:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:52:59][root][INFO] - Training Epoch: 2/2, step 19897/23838 completed (loss: 0.7944031953811646, acc: 0.8048780560493469)
[2025-02-05 13:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:00][root][INFO] - Training Epoch: 2/2, step 19898/23838 completed (loss: 0.715885579586029, acc: 0.8125)
[2025-02-05 13:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:00][root][INFO] - Training Epoch: 2/2, step 19899/23838 completed (loss: 1.3333336114883423, acc: 0.6630434989929199)
[2025-02-05 13:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:00][root][INFO] - Training Epoch: 2/2, step 19900/23838 completed (loss: 0.8809364438056946, acc: 0.7045454382896423)
[2025-02-05 13:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:01][root][INFO] - Training Epoch: 2/2, step 19901/23838 completed (loss: 0.9898982048034668, acc: 0.6808510422706604)
[2025-02-05 13:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:01][root][INFO] - Training Epoch: 2/2, step 19902/23838 completed (loss: 1.060609221458435, acc: 0.7291666865348816)
[2025-02-05 13:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:01][root][INFO] - Training Epoch: 2/2, step 19903/23838 completed (loss: 0.6213601231575012, acc: 0.7941176295280457)
[2025-02-05 13:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:02][root][INFO] - Training Epoch: 2/2, step 19904/23838 completed (loss: 1.0187665224075317, acc: 0.734375)
[2025-02-05 13:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:02][root][INFO] - Training Epoch: 2/2, step 19905/23838 completed (loss: 1.1978529691696167, acc: 0.6634615659713745)
[2025-02-05 13:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:03][root][INFO] - Training Epoch: 2/2, step 19906/23838 completed (loss: 1.1631430387496948, acc: 0.6972476840019226)
[2025-02-05 13:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:03][root][INFO] - Training Epoch: 2/2, step 19907/23838 completed (loss: 1.0963623523712158, acc: 0.6555555462837219)
[2025-02-05 13:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:03][root][INFO] - Training Epoch: 2/2, step 19908/23838 completed (loss: 1.0744130611419678, acc: 0.6935483813285828)
[2025-02-05 13:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:04][root][INFO] - Training Epoch: 2/2, step 19909/23838 completed (loss: 0.48537927865982056, acc: 0.8433734774589539)
[2025-02-05 13:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:04][root][INFO] - Training Epoch: 2/2, step 19910/23838 completed (loss: 0.4131860136985779, acc: 0.8644067645072937)
[2025-02-05 13:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:05][root][INFO] - Training Epoch: 2/2, step 19911/23838 completed (loss: 1.1572661399841309, acc: 0.6881720423698425)
[2025-02-05 13:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:05][root][INFO] - Training Epoch: 2/2, step 19912/23838 completed (loss: 0.776980996131897, acc: 0.790123462677002)
[2025-02-05 13:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:05][root][INFO] - Training Epoch: 2/2, step 19913/23838 completed (loss: 1.3670591115951538, acc: 0.6138613820075989)
[2025-02-05 13:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:06][root][INFO] - Training Epoch: 2/2, step 19914/23838 completed (loss: 1.3367704153060913, acc: 0.6578947305679321)
[2025-02-05 13:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:06][root][INFO] - Training Epoch: 2/2, step 19915/23838 completed (loss: 1.2282131910324097, acc: 0.6814814805984497)
[2025-02-05 13:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:06][root][INFO] - Training Epoch: 2/2, step 19916/23838 completed (loss: 1.5267752408981323, acc: 0.5671641826629639)
[2025-02-05 13:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:07][root][INFO] - Training Epoch: 2/2, step 19917/23838 completed (loss: 1.3926602602005005, acc: 0.6309523582458496)
[2025-02-05 13:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:07][root][INFO] - Training Epoch: 2/2, step 19918/23838 completed (loss: 0.667060375213623, acc: 0.800000011920929)
[2025-02-05 13:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:08][root][INFO] - Training Epoch: 2/2, step 19919/23838 completed (loss: 1.4556621313095093, acc: 0.6220930218696594)
[2025-02-05 13:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:08][root][INFO] - Training Epoch: 2/2, step 19920/23838 completed (loss: 1.3983047008514404, acc: 0.5675675868988037)
[2025-02-05 13:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:08][root][INFO] - Training Epoch: 2/2, step 19921/23838 completed (loss: 1.2560988664627075, acc: 0.6341463327407837)
[2025-02-05 13:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:09][root][INFO] - Training Epoch: 2/2, step 19922/23838 completed (loss: 1.428091049194336, acc: 0.6173912882804871)
[2025-02-05 13:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:09][root][INFO] - Training Epoch: 2/2, step 19923/23838 completed (loss: 1.1991474628448486, acc: 0.6424580812454224)
[2025-02-05 13:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:09][root][INFO] - Training Epoch: 2/2, step 19924/23838 completed (loss: 1.4229750633239746, acc: 0.5759999752044678)
[2025-02-05 13:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:10][root][INFO] - Training Epoch: 2/2, step 19925/23838 completed (loss: 1.2519705295562744, acc: 0.6518518328666687)
[2025-02-05 13:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:10][root][INFO] - Training Epoch: 2/2, step 19926/23838 completed (loss: 1.4495171308517456, acc: 0.5625)
[2025-02-05 13:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:11][root][INFO] - Training Epoch: 2/2, step 19927/23838 completed (loss: 1.2745622396469116, acc: 0.6557376980781555)
[2025-02-05 13:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:11][root][INFO] - Training Epoch: 2/2, step 19928/23838 completed (loss: 0.9655612707138062, acc: 0.737500011920929)
[2025-02-05 13:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:11][root][INFO] - Training Epoch: 2/2, step 19929/23838 completed (loss: 1.2166876792907715, acc: 0.640625)
[2025-02-05 13:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:12][root][INFO] - Training Epoch: 2/2, step 19930/23838 completed (loss: 1.2141588926315308, acc: 0.6847826242446899)
[2025-02-05 13:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:12][root][INFO] - Training Epoch: 2/2, step 19931/23838 completed (loss: 0.8564485311508179, acc: 0.7599999904632568)
[2025-02-05 13:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:13][root][INFO] - Training Epoch: 2/2, step 19932/23838 completed (loss: 1.0827350616455078, acc: 0.6849315166473389)
[2025-02-05 13:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:13][root][INFO] - Training Epoch: 2/2, step 19933/23838 completed (loss: 1.0692919492721558, acc: 0.6666666865348816)
[2025-02-05 13:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:13][root][INFO] - Training Epoch: 2/2, step 19934/23838 completed (loss: 1.082156777381897, acc: 0.6931818127632141)
[2025-02-05 13:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:14][root][INFO] - Training Epoch: 2/2, step 19935/23838 completed (loss: 1.2028995752334595, acc: 0.6271186470985413)
[2025-02-05 13:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:14][root][INFO] - Training Epoch: 2/2, step 19936/23838 completed (loss: 1.2254459857940674, acc: 0.6000000238418579)
[2025-02-05 13:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:14][root][INFO] - Training Epoch: 2/2, step 19937/23838 completed (loss: 0.9850224256515503, acc: 0.6845637559890747)
[2025-02-05 13:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:15][root][INFO] - Training Epoch: 2/2, step 19938/23838 completed (loss: 0.8207589983940125, acc: 0.762499988079071)
[2025-02-05 13:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:15][root][INFO] - Training Epoch: 2/2, step 19939/23838 completed (loss: 0.9833923578262329, acc: 0.6941176652908325)
[2025-02-05 13:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:15][root][INFO] - Training Epoch: 2/2, step 19940/23838 completed (loss: 1.1049007177352905, acc: 0.6455696225166321)
[2025-02-05 13:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:16][root][INFO] - Training Epoch: 2/2, step 19941/23838 completed (loss: 1.0866079330444336, acc: 0.7171717286109924)
[2025-02-05 13:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:16][root][INFO] - Training Epoch: 2/2, step 19942/23838 completed (loss: 1.1678507328033447, acc: 0.6390977501869202)
[2025-02-05 13:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:17][root][INFO] - Training Epoch: 2/2, step 19943/23838 completed (loss: 1.1236649751663208, acc: 0.675000011920929)
[2025-02-05 13:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:17][root][INFO] - Training Epoch: 2/2, step 19944/23838 completed (loss: 1.0591813325881958, acc: 0.6796875)
[2025-02-05 13:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:17][root][INFO] - Training Epoch: 2/2, step 19945/23838 completed (loss: 0.9251209497451782, acc: 0.7534246444702148)
[2025-02-05 13:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:18][root][INFO] - Training Epoch: 2/2, step 19946/23838 completed (loss: 1.069006085395813, acc: 0.6818181872367859)
[2025-02-05 13:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:18][root][INFO] - Training Epoch: 2/2, step 19947/23838 completed (loss: 1.1747397184371948, acc: 0.6708074808120728)
[2025-02-05 13:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:18][root][INFO] - Training Epoch: 2/2, step 19948/23838 completed (loss: 0.837466835975647, acc: 0.7281553149223328)
[2025-02-05 13:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:19][root][INFO] - Training Epoch: 2/2, step 19949/23838 completed (loss: 1.2671308517456055, acc: 0.5798319578170776)
[2025-02-05 13:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:19][root][INFO] - Training Epoch: 2/2, step 19950/23838 completed (loss: 1.233961820602417, acc: 0.7121211886405945)
[2025-02-05 13:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:20][root][INFO] - Training Epoch: 2/2, step 19951/23838 completed (loss: 0.8035145401954651, acc: 0.7526881694793701)
[2025-02-05 13:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:20][root][INFO] - Training Epoch: 2/2, step 19952/23838 completed (loss: 0.45540696382522583, acc: 0.8526315689086914)
[2025-02-05 13:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:20][root][INFO] - Training Epoch: 2/2, step 19953/23838 completed (loss: 0.6230913996696472, acc: 0.8395061492919922)
[2025-02-05 13:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:21][root][INFO] - Training Epoch: 2/2, step 19954/23838 completed (loss: 0.910284698009491, acc: 0.7454545497894287)
[2025-02-05 13:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:21][root][INFO] - Training Epoch: 2/2, step 19955/23838 completed (loss: 0.6230874061584473, acc: 0.8510638475418091)
[2025-02-05 13:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:22][root][INFO] - Training Epoch: 2/2, step 19956/23838 completed (loss: 1.2305302619934082, acc: 0.6625000238418579)
[2025-02-05 13:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:22][root][INFO] - Training Epoch: 2/2, step 19957/23838 completed (loss: 0.8900600671768188, acc: 0.7272727489471436)
[2025-02-05 13:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:22][root][INFO] - Training Epoch: 2/2, step 19958/23838 completed (loss: 1.4611799716949463, acc: 0.6382978558540344)
[2025-02-05 13:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:23][root][INFO] - Training Epoch: 2/2, step 19959/23838 completed (loss: 1.4143986701965332, acc: 0.5128205418586731)
[2025-02-05 13:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:23][root][INFO] - Training Epoch: 2/2, step 19960/23838 completed (loss: 1.2929545640945435, acc: 0.6136363744735718)
[2025-02-05 13:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:24][root][INFO] - Training Epoch: 2/2, step 19961/23838 completed (loss: 1.8134175539016724, acc: 0.488095223903656)
[2025-02-05 13:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:24][root][INFO] - Training Epoch: 2/2, step 19962/23838 completed (loss: 1.4762510061264038, acc: 0.6000000238418579)
[2025-02-05 13:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:24][root][INFO] - Training Epoch: 2/2, step 19963/23838 completed (loss: 1.3261958360671997, acc: 0.5952380895614624)
[2025-02-05 13:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:25][root][INFO] - Training Epoch: 2/2, step 19964/23838 completed (loss: 1.7915705442428589, acc: 0.5677965879440308)
[2025-02-05 13:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:25][root][INFO] - Training Epoch: 2/2, step 19965/23838 completed (loss: 1.6737537384033203, acc: 0.5632184147834778)
[2025-02-05 13:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:26][root][INFO] - Training Epoch: 2/2, step 19966/23838 completed (loss: 1.8297914266586304, acc: 0.44285714626312256)
[2025-02-05 13:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:26][root][INFO] - Training Epoch: 2/2, step 19967/23838 completed (loss: 1.107906460762024, acc: 0.6794871687889099)
[2025-02-05 13:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:26][root][INFO] - Training Epoch: 2/2, step 19968/23838 completed (loss: 1.5042072534561157, acc: 0.59375)
[2025-02-05 13:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:27][root][INFO] - Training Epoch: 2/2, step 19969/23838 completed (loss: 1.7584606409072876, acc: 0.5104166865348816)
[2025-02-05 13:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:27][root][INFO] - Training Epoch: 2/2, step 19970/23838 completed (loss: 1.5162441730499268, acc: 0.5375000238418579)
[2025-02-05 13:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:27][root][INFO] - Training Epoch: 2/2, step 19971/23838 completed (loss: 1.5497967004776, acc: 0.6056337952613831)
[2025-02-05 13:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:28][root][INFO] - Training Epoch: 2/2, step 19972/23838 completed (loss: 1.4382984638214111, acc: 0.6000000238418579)
[2025-02-05 13:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:28][root][INFO] - Training Epoch: 2/2, step 19973/23838 completed (loss: 1.6217962503433228, acc: 0.5600000023841858)
[2025-02-05 13:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:29][root][INFO] - Training Epoch: 2/2, step 19974/23838 completed (loss: 1.42686128616333, acc: 0.5423728823661804)
[2025-02-05 13:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:29][root][INFO] - Training Epoch: 2/2, step 19975/23838 completed (loss: 1.1504766941070557, acc: 0.6984127163887024)
[2025-02-05 13:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:29][root][INFO] - Training Epoch: 2/2, step 19976/23838 completed (loss: 1.3336974382400513, acc: 0.5555555820465088)
[2025-02-05 13:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:30][root][INFO] - Training Epoch: 2/2, step 19977/23838 completed (loss: 1.2618299722671509, acc: 0.5929203629493713)
[2025-02-05 13:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:30][root][INFO] - Training Epoch: 2/2, step 19978/23838 completed (loss: 1.491878867149353, acc: 0.52173912525177)
[2025-02-05 13:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:31][root][INFO] - Training Epoch: 2/2, step 19979/23838 completed (loss: 1.3434935808181763, acc: 0.6607142686843872)
[2025-02-05 13:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:31][root][INFO] - Training Epoch: 2/2, step 19980/23838 completed (loss: 1.3216705322265625, acc: 0.5979381203651428)
[2025-02-05 13:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:31][root][INFO] - Training Epoch: 2/2, step 19981/23838 completed (loss: 1.2189931869506836, acc: 0.5882353186607361)
[2025-02-05 13:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:32][root][INFO] - Training Epoch: 2/2, step 19982/23838 completed (loss: 1.2897114753723145, acc: 0.6428571343421936)
[2025-02-05 13:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:32][root][INFO] - Training Epoch: 2/2, step 19983/23838 completed (loss: 1.6631793975830078, acc: 0.5193798542022705)
[2025-02-05 13:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:32][root][INFO] - Training Epoch: 2/2, step 19984/23838 completed (loss: 1.3549156188964844, acc: 0.6168224215507507)
[2025-02-05 13:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:33][root][INFO] - Training Epoch: 2/2, step 19985/23838 completed (loss: 1.5935301780700684, acc: 0.5277777910232544)
[2025-02-05 13:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:33][root][INFO] - Training Epoch: 2/2, step 19986/23838 completed (loss: 1.0928527116775513, acc: 0.6595744490623474)
[2025-02-05 13:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:34][root][INFO] - Training Epoch: 2/2, step 19987/23838 completed (loss: 1.2471139430999756, acc: 0.6610169410705566)
[2025-02-05 13:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:34][root][INFO] - Training Epoch: 2/2, step 19988/23838 completed (loss: 1.1656540632247925, acc: 0.6111111044883728)
[2025-02-05 13:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:35][root][INFO] - Training Epoch: 2/2, step 19989/23838 completed (loss: 1.2788654565811157, acc: 0.6614173054695129)
[2025-02-05 13:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:35][root][INFO] - Training Epoch: 2/2, step 19990/23838 completed (loss: 1.1766973733901978, acc: 0.6697247624397278)
[2025-02-05 13:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:36][root][INFO] - Training Epoch: 2/2, step 19991/23838 completed (loss: 1.46573805809021, acc: 0.5894039869308472)
[2025-02-05 13:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:36][root][INFO] - Training Epoch: 2/2, step 19992/23838 completed (loss: 1.3040064573287964, acc: 0.6262626051902771)
[2025-02-05 13:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:37][root][INFO] - Training Epoch: 2/2, step 19993/23838 completed (loss: 1.5006086826324463, acc: 0.6478873491287231)
[2025-02-05 13:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:37][root][INFO] - Training Epoch: 2/2, step 19994/23838 completed (loss: 1.0292770862579346, acc: 0.6938775777816772)
[2025-02-05 13:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:37][root][INFO] - Training Epoch: 2/2, step 19995/23838 completed (loss: 1.4859038591384888, acc: 0.5724637508392334)
[2025-02-05 13:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:38][root][INFO] - Training Epoch: 2/2, step 19996/23838 completed (loss: 1.4261584281921387, acc: 0.5405405163764954)
[2025-02-05 13:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:38][root][INFO] - Training Epoch: 2/2, step 19997/23838 completed (loss: 1.6266494989395142, acc: 0.5283018946647644)
[2025-02-05 13:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:38][root][INFO] - Training Epoch: 2/2, step 19998/23838 completed (loss: 1.5558960437774658, acc: 0.5)
[2025-02-05 13:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:39][root][INFO] - Training Epoch: 2/2, step 19999/23838 completed (loss: 1.2316700220108032, acc: 0.5744680762290955)
[2025-02-05 13:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:39][root][INFO] - Training Epoch: 2/2, step 20000/23838 completed (loss: 1.3153156042099, acc: 0.6078431606292725)
[2025-02-05 13:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:40][root][INFO] - Training Epoch: 2/2, step 20001/23838 completed (loss: 1.301127552986145, acc: 0.6071428656578064)
[2025-02-05 13:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:40][root][INFO] - Training Epoch: 2/2, step 20002/23838 completed (loss: 0.9850212931632996, acc: 0.6666666865348816)
[2025-02-05 13:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:41][root][INFO] - Training Epoch: 2/2, step 20003/23838 completed (loss: 1.0488988161087036, acc: 0.7096773982048035)
[2025-02-05 13:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:41][root][INFO] - Training Epoch: 2/2, step 20004/23838 completed (loss: 0.37938398122787476, acc: 0.9122806787490845)
[2025-02-05 13:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:41][root][INFO] - Training Epoch: 2/2, step 20005/23838 completed (loss: 1.144119143486023, acc: 0.6857143044471741)
[2025-02-05 13:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:42][root][INFO] - Training Epoch: 2/2, step 20006/23838 completed (loss: 1.010374903678894, acc: 0.7058823704719543)
[2025-02-05 13:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:42][root][INFO] - Training Epoch: 2/2, step 20007/23838 completed (loss: 1.3749853372573853, acc: 0.6168224215507507)
[2025-02-05 13:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:43][root][INFO] - Training Epoch: 2/2, step 20008/23838 completed (loss: 1.5695496797561646, acc: 0.565891444683075)
[2025-02-05 13:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:43][root][INFO] - Training Epoch: 2/2, step 20009/23838 completed (loss: 1.580615520477295, acc: 0.5702479481697083)
[2025-02-05 13:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:44][root][INFO] - Training Epoch: 2/2, step 20010/23838 completed (loss: 1.2704713344573975, acc: 0.6451612710952759)
[2025-02-05 13:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:44][root][INFO] - Training Epoch: 2/2, step 20011/23838 completed (loss: 0.9786224365234375, acc: 0.7047619223594666)
[2025-02-05 13:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:44][root][INFO] - Training Epoch: 2/2, step 20012/23838 completed (loss: 1.540561318397522, acc: 0.5841584205627441)
[2025-02-05 13:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:45][root][INFO] - Training Epoch: 2/2, step 20013/23838 completed (loss: 0.7519580721855164, acc: 0.7945205569267273)
[2025-02-05 13:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:45][root][INFO] - Training Epoch: 2/2, step 20014/23838 completed (loss: 0.9973223209381104, acc: 0.7076923251152039)
[2025-02-05 13:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:45][root][INFO] - Training Epoch: 2/2, step 20015/23838 completed (loss: 0.9737316370010376, acc: 0.6727272868156433)
[2025-02-05 13:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:46][root][INFO] - Training Epoch: 2/2, step 20016/23838 completed (loss: 1.1854424476623535, acc: 0.6574074029922485)
[2025-02-05 13:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:46][root][INFO] - Training Epoch: 2/2, step 20017/23838 completed (loss: 1.0677504539489746, acc: 0.6727272868156433)
[2025-02-05 13:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:47][root][INFO] - Training Epoch: 2/2, step 20018/23838 completed (loss: 0.8613559007644653, acc: 0.7758620977401733)
[2025-02-05 13:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:47][root][INFO] - Training Epoch: 2/2, step 20019/23838 completed (loss: 0.33971676230430603, acc: 0.9042553305625916)
[2025-02-05 13:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:47][root][INFO] - Training Epoch: 2/2, step 20020/23838 completed (loss: 0.4677123427391052, acc: 0.8561151027679443)
[2025-02-05 13:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:48][root][INFO] - Training Epoch: 2/2, step 20021/23838 completed (loss: 0.5380439162254333, acc: 0.8695651888847351)
[2025-02-05 13:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:48][root][INFO] - Training Epoch: 2/2, step 20022/23838 completed (loss: 0.614604115486145, acc: 0.8348624110221863)
[2025-02-05 13:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:49][root][INFO] - Training Epoch: 2/2, step 20023/23838 completed (loss: 1.1395256519317627, acc: 0.762499988079071)
[2025-02-05 13:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:49][root][INFO] - Training Epoch: 2/2, step 20024/23838 completed (loss: 1.671866536140442, acc: 0.5555555820465088)
[2025-02-05 13:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:49][root][INFO] - Training Epoch: 2/2, step 20025/23838 completed (loss: 1.6069543361663818, acc: 0.5263158082962036)
[2025-02-05 13:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:50][root][INFO] - Training Epoch: 2/2, step 20026/23838 completed (loss: 1.8689910173416138, acc: 0.5)
[2025-02-05 13:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:50][root][INFO] - Training Epoch: 2/2, step 20027/23838 completed (loss: 2.166761636734009, acc: 0.3636363744735718)
[2025-02-05 13:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:51][root][INFO] - Training Epoch: 2/2, step 20028/23838 completed (loss: 1.9985846281051636, acc: 0.4583333432674408)
[2025-02-05 13:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:51][root][INFO] - Training Epoch: 2/2, step 20029/23838 completed (loss: 2.1292316913604736, acc: 0.3478260934352875)
[2025-02-05 13:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:51][root][INFO] - Training Epoch: 2/2, step 20030/23838 completed (loss: 1.5127973556518555, acc: 0.4166666567325592)
[2025-02-05 13:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:52][root][INFO] - Training Epoch: 2/2, step 20031/23838 completed (loss: 1.730193853378296, acc: 0.4736842215061188)
[2025-02-05 13:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:52][root][INFO] - Training Epoch: 2/2, step 20032/23838 completed (loss: 1.796531319618225, acc: 0.5)
[2025-02-05 13:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:52][root][INFO] - Training Epoch: 2/2, step 20033/23838 completed (loss: 1.531442403793335, acc: 0.4285714328289032)
[2025-02-05 13:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:53][root][INFO] - Training Epoch: 2/2, step 20034/23838 completed (loss: 1.8111423254013062, acc: 0.5)
[2025-02-05 13:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:53][root][INFO] - Training Epoch: 2/2, step 20035/23838 completed (loss: 1.4254599809646606, acc: 0.5161290168762207)
[2025-02-05 13:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:53][root][INFO] - Training Epoch: 2/2, step 20036/23838 completed (loss: 1.6446055173873901, acc: 0.4000000059604645)
[2025-02-05 13:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:54][root][INFO] - Training Epoch: 2/2, step 20037/23838 completed (loss: 1.7282477617263794, acc: 0.47826087474823)
[2025-02-05 13:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:54][root][INFO] - Training Epoch: 2/2, step 20038/23838 completed (loss: 1.2149012088775635, acc: 0.523809552192688)
[2025-02-05 13:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:55][root][INFO] - Training Epoch: 2/2, step 20039/23838 completed (loss: 1.4346263408660889, acc: 0.6000000238418579)
[2025-02-05 13:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:55][root][INFO] - Training Epoch: 2/2, step 20040/23838 completed (loss: 1.9508979320526123, acc: 0.30434781312942505)
[2025-02-05 13:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:55][root][INFO] - Training Epoch: 2/2, step 20041/23838 completed (loss: 1.4269455671310425, acc: 0.5479452013969421)
[2025-02-05 13:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:56][root][INFO] - Training Epoch: 2/2, step 20042/23838 completed (loss: 1.330250859260559, acc: 0.6379310488700867)
[2025-02-05 13:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:56][root][INFO] - Training Epoch: 2/2, step 20043/23838 completed (loss: 1.693307638168335, acc: 0.49295774102211)
[2025-02-05 13:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:56][root][INFO] - Training Epoch: 2/2, step 20044/23838 completed (loss: 1.5293527841567993, acc: 0.5545454621315002)
[2025-02-05 13:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:57][root][INFO] - Training Epoch: 2/2, step 20045/23838 completed (loss: 1.1349880695343018, acc: 0.7291666865348816)
[2025-02-05 13:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:57][root][INFO] - Training Epoch: 2/2, step 20046/23838 completed (loss: 1.2405413389205933, acc: 0.6800000071525574)
[2025-02-05 13:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:57][root][INFO] - Training Epoch: 2/2, step 20047/23838 completed (loss: 1.2567168474197388, acc: 0.671875)
[2025-02-05 13:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:58][root][INFO] - Training Epoch: 2/2, step 20048/23838 completed (loss: 1.2326291799545288, acc: 0.6631578803062439)
[2025-02-05 13:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:58][root][INFO] - Training Epoch: 2/2, step 20049/23838 completed (loss: 1.1579006910324097, acc: 0.6428571343421936)
[2025-02-05 13:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:59][root][INFO] - Training Epoch: 2/2, step 20050/23838 completed (loss: 1.3662703037261963, acc: 0.6428571343421936)
[2025-02-05 13:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:59][root][INFO] - Training Epoch: 2/2, step 20051/23838 completed (loss: 1.393433928489685, acc: 0.6108108162879944)
[2025-02-05 13:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:53:59][root][INFO] - Training Epoch: 2/2, step 20052/23838 completed (loss: 1.1094588041305542, acc: 0.675000011920929)
[2025-02-05 13:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:00][root][INFO] - Training Epoch: 2/2, step 20053/23838 completed (loss: 1.265446662902832, acc: 0.6486486196517944)
[2025-02-05 13:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:00][root][INFO] - Training Epoch: 2/2, step 20054/23838 completed (loss: 1.2361884117126465, acc: 0.6510066986083984)
[2025-02-05 13:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:01][root][INFO] - Training Epoch: 2/2, step 20055/23838 completed (loss: 1.4080668687820435, acc: 0.5608108043670654)
[2025-02-05 13:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:01][root][INFO] - Training Epoch: 2/2, step 20056/23838 completed (loss: 1.5176936388015747, acc: 0.5918367505073547)
[2025-02-05 13:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:01][root][INFO] - Training Epoch: 2/2, step 20057/23838 completed (loss: 1.241002082824707, acc: 0.6407766938209534)
[2025-02-05 13:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:02][root][INFO] - Training Epoch: 2/2, step 20058/23838 completed (loss: 1.4709447622299194, acc: 0.5649350881576538)
[2025-02-05 13:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:02][root][INFO] - Training Epoch: 2/2, step 20059/23838 completed (loss: 1.4300426244735718, acc: 0.6336633563041687)
[2025-02-05 13:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:03][root][INFO] - Training Epoch: 2/2, step 20060/23838 completed (loss: 1.1586960554122925, acc: 0.6349206566810608)
[2025-02-05 13:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:03][root][INFO] - Training Epoch: 2/2, step 20061/23838 completed (loss: 1.114542841911316, acc: 0.7016128897666931)
[2025-02-05 13:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:03][root][INFO] - Training Epoch: 2/2, step 20062/23838 completed (loss: 1.2875492572784424, acc: 0.6190476417541504)
[2025-02-05 13:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:04][root][INFO] - Training Epoch: 2/2, step 20063/23838 completed (loss: 1.065169334411621, acc: 0.7111111283302307)
[2025-02-05 13:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:04][root][INFO] - Training Epoch: 2/2, step 20064/23838 completed (loss: 1.3651070594787598, acc: 0.5850340127944946)
[2025-02-05 13:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:04][root][INFO] - Training Epoch: 2/2, step 20065/23838 completed (loss: 1.1282294988632202, acc: 0.6966292262077332)
[2025-02-05 13:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:05][root][INFO] - Training Epoch: 2/2, step 20066/23838 completed (loss: 1.2758588790893555, acc: 0.6606060862541199)
[2025-02-05 13:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:05][root][INFO] - Training Epoch: 2/2, step 20067/23838 completed (loss: 1.1189666986465454, acc: 0.6666666865348816)
[2025-02-05 13:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:06][root][INFO] - Training Epoch: 2/2, step 20068/23838 completed (loss: 1.3035457134246826, acc: 0.6233766078948975)
[2025-02-05 13:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:06][root][INFO] - Training Epoch: 2/2, step 20069/23838 completed (loss: 1.4016711711883545, acc: 0.5714285969734192)
[2025-02-05 13:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:07][root][INFO] - Training Epoch: 2/2, step 20070/23838 completed (loss: 1.1786913871765137, acc: 0.6612903475761414)
[2025-02-05 13:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:07][root][INFO] - Training Epoch: 2/2, step 20071/23838 completed (loss: 0.9030484557151794, acc: 0.7804877758026123)
[2025-02-05 13:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:07][root][INFO] - Training Epoch: 2/2, step 20072/23838 completed (loss: 1.1178512573242188, acc: 0.6274510025978088)
[2025-02-05 13:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:08][root][INFO] - Training Epoch: 2/2, step 20073/23838 completed (loss: 1.3669919967651367, acc: 0.5454545617103577)
[2025-02-05 13:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:08][root][INFO] - Training Epoch: 2/2, step 20074/23838 completed (loss: 1.354891300201416, acc: 0.5306122303009033)
[2025-02-05 13:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:09][root][INFO] - Training Epoch: 2/2, step 20075/23838 completed (loss: 0.7184633612632751, acc: 0.8181818127632141)
[2025-02-05 13:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:09][root][INFO] - Training Epoch: 2/2, step 20076/23838 completed (loss: 1.344272255897522, acc: 0.631147563457489)
[2025-02-05 13:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:09][root][INFO] - Training Epoch: 2/2, step 20077/23838 completed (loss: 0.9764212965965271, acc: 0.6842105388641357)
[2025-02-05 13:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:10][root][INFO] - Training Epoch: 2/2, step 20078/23838 completed (loss: 1.2341036796569824, acc: 0.597484290599823)
[2025-02-05 13:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:10][root][INFO] - Training Epoch: 2/2, step 20079/23838 completed (loss: 1.1873337030410767, acc: 0.641791045665741)
[2025-02-05 13:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:11][root][INFO] - Training Epoch: 2/2, step 20080/23838 completed (loss: 1.1477638483047485, acc: 0.6951219439506531)
[2025-02-05 13:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:11][root][INFO] - Training Epoch: 2/2, step 20081/23838 completed (loss: 1.2342231273651123, acc: 0.6392405033111572)
[2025-02-05 13:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:11][root][INFO] - Training Epoch: 2/2, step 20082/23838 completed (loss: 1.1766382455825806, acc: 0.6402438879013062)
[2025-02-05 13:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:12][root][INFO] - Training Epoch: 2/2, step 20083/23838 completed (loss: 1.3922678232192993, acc: 0.6016949415206909)
[2025-02-05 13:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:12][root][INFO] - Training Epoch: 2/2, step 20084/23838 completed (loss: 1.121752381324768, acc: 0.6666666865348816)
[2025-02-05 13:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:13][root][INFO] - Training Epoch: 2/2, step 20085/23838 completed (loss: 1.2051345109939575, acc: 0.6355932354927063)
[2025-02-05 13:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:13][root][INFO] - Training Epoch: 2/2, step 20086/23838 completed (loss: 1.012971043586731, acc: 0.6847826242446899)
[2025-02-05 13:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:13][root][INFO] - Training Epoch: 2/2, step 20087/23838 completed (loss: 1.478379726409912, acc: 0.5792682766914368)
[2025-02-05 13:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:14][root][INFO] - Training Epoch: 2/2, step 20088/23838 completed (loss: 1.3523831367492676, acc: 0.5942857265472412)
[2025-02-05 13:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:14][root][INFO] - Training Epoch: 2/2, step 20089/23838 completed (loss: 1.3365764617919922, acc: 0.6083915829658508)
[2025-02-05 13:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:15][root][INFO] - Training Epoch: 2/2, step 20090/23838 completed (loss: 1.2282023429870605, acc: 0.5886076092720032)
[2025-02-05 13:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:15][root][INFO] - Training Epoch: 2/2, step 20091/23838 completed (loss: 1.1270689964294434, acc: 0.6833333373069763)
[2025-02-05 13:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:16][root][INFO] - Training Epoch: 2/2, step 20092/23838 completed (loss: 1.200721025466919, acc: 0.605042040348053)
[2025-02-05 13:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:16][root][INFO] - Training Epoch: 2/2, step 20093/23838 completed (loss: 1.3419731855392456, acc: 0.5696969628334045)
[2025-02-05 13:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:16][root][INFO] - Training Epoch: 2/2, step 20094/23838 completed (loss: 1.1792974472045898, acc: 0.6605504751205444)
[2025-02-05 13:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:17][root][INFO] - Training Epoch: 2/2, step 20095/23838 completed (loss: 1.2178609371185303, acc: 0.6133333444595337)
[2025-02-05 13:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:17][root][INFO] - Training Epoch: 2/2, step 20096/23838 completed (loss: 1.2477613687515259, acc: 0.71875)
[2025-02-05 13:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:17][root][INFO] - Training Epoch: 2/2, step 20097/23838 completed (loss: 1.3767694234848022, acc: 0.6240000128746033)
[2025-02-05 13:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:18][root][INFO] - Training Epoch: 2/2, step 20098/23838 completed (loss: 1.2885016202926636, acc: 0.6277372241020203)
[2025-02-05 13:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:18][root][INFO] - Training Epoch: 2/2, step 20099/23838 completed (loss: 1.4505276679992676, acc: 0.5862069129943848)
[2025-02-05 13:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:19][root][INFO] - Training Epoch: 2/2, step 20100/23838 completed (loss: 1.3898591995239258, acc: 0.6168224215507507)
[2025-02-05 13:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:19][root][INFO] - Training Epoch: 2/2, step 20101/23838 completed (loss: 1.0669538974761963, acc: 0.6605504751205444)
[2025-02-05 13:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:20][root][INFO] - Training Epoch: 2/2, step 20102/23838 completed (loss: 0.7442071437835693, acc: 0.782608687877655)
[2025-02-05 13:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:20][root][INFO] - Training Epoch: 2/2, step 20103/23838 completed (loss: 1.2222013473510742, acc: 0.6381579041481018)
[2025-02-05 13:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:20][root][INFO] - Training Epoch: 2/2, step 20104/23838 completed (loss: 0.9233008623123169, acc: 0.7659574747085571)
[2025-02-05 13:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:21][root][INFO] - Training Epoch: 2/2, step 20105/23838 completed (loss: 1.1477546691894531, acc: 0.6516854166984558)
[2025-02-05 13:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:21][root][INFO] - Training Epoch: 2/2, step 20106/23838 completed (loss: 1.4055631160736084, acc: 0.6019900441169739)
[2025-02-05 13:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:22][root][INFO] - Training Epoch: 2/2, step 20107/23838 completed (loss: 1.1016418933868408, acc: 0.6641790866851807)
[2025-02-05 13:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:22][root][INFO] - Training Epoch: 2/2, step 20108/23838 completed (loss: 1.3740416765213013, acc: 0.6176470518112183)
[2025-02-05 13:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:23][root][INFO] - Training Epoch: 2/2, step 20109/23838 completed (loss: 1.2098058462142944, acc: 0.6415094137191772)
[2025-02-05 13:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:23][root][INFO] - Training Epoch: 2/2, step 20110/23838 completed (loss: 1.2581210136413574, acc: 0.6691729426383972)
[2025-02-05 13:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:23][root][INFO] - Training Epoch: 2/2, step 20111/23838 completed (loss: 0.9201714396476746, acc: 0.75)
[2025-02-05 13:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:24][root][INFO] - Training Epoch: 2/2, step 20112/23838 completed (loss: 1.0999646186828613, acc: 0.6867470145225525)
[2025-02-05 13:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:24][root][INFO] - Training Epoch: 2/2, step 20113/23838 completed (loss: 0.9431578516960144, acc: 0.7124999761581421)
[2025-02-05 13:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:25][root][INFO] - Training Epoch: 2/2, step 20114/23838 completed (loss: 0.9454214572906494, acc: 0.7229729890823364)
[2025-02-05 13:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:25][root][INFO] - Training Epoch: 2/2, step 20115/23838 completed (loss: 1.3077642917633057, acc: 0.6564885377883911)
[2025-02-05 13:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:26][root][INFO] - Training Epoch: 2/2, step 20116/23838 completed (loss: 1.2983318567276, acc: 0.5545454621315002)
[2025-02-05 13:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:26][root][INFO] - Training Epoch: 2/2, step 20117/23838 completed (loss: 1.1082981824874878, acc: 0.7200000286102295)
[2025-02-05 13:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:26][root][INFO] - Training Epoch: 2/2, step 20118/23838 completed (loss: 1.097184419631958, acc: 0.6590909361839294)
[2025-02-05 13:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:27][root][INFO] - Training Epoch: 2/2, step 20119/23838 completed (loss: 1.2393351793289185, acc: 0.6346153616905212)
[2025-02-05 13:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:27][root][INFO] - Training Epoch: 2/2, step 20120/23838 completed (loss: 1.281232237815857, acc: 0.6716417670249939)
[2025-02-05 13:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:28][root][INFO] - Training Epoch: 2/2, step 20121/23838 completed (loss: 1.142383337020874, acc: 0.6515151262283325)
[2025-02-05 13:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:28][root][INFO] - Training Epoch: 2/2, step 20122/23838 completed (loss: 1.0598223209381104, acc: 0.699999988079071)
[2025-02-05 13:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:29][root][INFO] - Training Epoch: 2/2, step 20123/23838 completed (loss: 1.194849967956543, acc: 0.6509804129600525)
[2025-02-05 13:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:29][root][INFO] - Training Epoch: 2/2, step 20124/23838 completed (loss: 1.2284976243972778, acc: 0.6558441519737244)
[2025-02-05 13:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:29][root][INFO] - Training Epoch: 2/2, step 20125/23838 completed (loss: 1.1217905282974243, acc: 0.6283186078071594)
[2025-02-05 13:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:30][root][INFO] - Training Epoch: 2/2, step 20126/23838 completed (loss: 1.4499855041503906, acc: 0.5461538434028625)
[2025-02-05 13:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:30][root][INFO] - Training Epoch: 2/2, step 20127/23838 completed (loss: 1.1639769077301025, acc: 0.6535947918891907)
[2025-02-05 13:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:31][root][INFO] - Training Epoch: 2/2, step 20128/23838 completed (loss: 1.4252651929855347, acc: 0.5767441987991333)
[2025-02-05 13:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:31][root][INFO] - Training Epoch: 2/2, step 20129/23838 completed (loss: 1.0427616834640503, acc: 0.6917293071746826)
[2025-02-05 13:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:32][root][INFO] - Training Epoch: 2/2, step 20130/23838 completed (loss: 0.8776125311851501, acc: 0.7111111283302307)
[2025-02-05 13:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:32][root][INFO] - Training Epoch: 2/2, step 20131/23838 completed (loss: 1.0857908725738525, acc: 0.671999990940094)
[2025-02-05 13:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:33][root][INFO] - Training Epoch: 2/2, step 20132/23838 completed (loss: 1.4204635620117188, acc: 0.6078431606292725)
[2025-02-05 13:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:33][root][INFO] - Training Epoch: 2/2, step 20133/23838 completed (loss: 1.414524793624878, acc: 0.6339285969734192)
[2025-02-05 13:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:33][root][INFO] - Training Epoch: 2/2, step 20134/23838 completed (loss: 1.278245210647583, acc: 0.6126482486724854)
[2025-02-05 13:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:34][root][INFO] - Training Epoch: 2/2, step 20135/23838 completed (loss: 0.8426834344863892, acc: 0.7553191781044006)
[2025-02-05 13:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:34][root][INFO] - Training Epoch: 2/2, step 20136/23838 completed (loss: 1.5146567821502686, acc: 0.5321100950241089)
[2025-02-05 13:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:35][root][INFO] - Training Epoch: 2/2, step 20137/23838 completed (loss: 1.2731637954711914, acc: 0.6136363744735718)
[2025-02-05 13:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:35][root][INFO] - Training Epoch: 2/2, step 20138/23838 completed (loss: 1.1179418563842773, acc: 0.7033898234367371)
[2025-02-05 13:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:36][root][INFO] - Training Epoch: 2/2, step 20139/23838 completed (loss: 0.9511475563049316, acc: 0.7037037014961243)
[2025-02-05 13:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:36][root][INFO] - Training Epoch: 2/2, step 20140/23838 completed (loss: 1.140465497970581, acc: 0.6666666865348816)
[2025-02-05 13:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:36][root][INFO] - Training Epoch: 2/2, step 20141/23838 completed (loss: 0.9252482652664185, acc: 0.7777777910232544)
[2025-02-05 13:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:37][root][INFO] - Training Epoch: 2/2, step 20142/23838 completed (loss: 0.8025209903717041, acc: 0.75)
[2025-02-05 13:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:37][root][INFO] - Training Epoch: 2/2, step 20143/23838 completed (loss: 0.9223619103431702, acc: 0.7777777910232544)
[2025-02-05 13:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:38][root][INFO] - Training Epoch: 2/2, step 20144/23838 completed (loss: 1.3156299591064453, acc: 0.6666666865348816)
[2025-02-05 13:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:38][root][INFO] - Training Epoch: 2/2, step 20145/23838 completed (loss: 1.191623568534851, acc: 0.6296296119689941)
[2025-02-05 13:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:39][root][INFO] - Training Epoch: 2/2, step 20146/23838 completed (loss: 1.448723554611206, acc: 0.5494505763053894)
[2025-02-05 13:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:39][root][INFO] - Training Epoch: 2/2, step 20147/23838 completed (loss: 1.585248589515686, acc: 0.5675675868988037)
[2025-02-05 13:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:40][root][INFO] - Training Epoch: 2/2, step 20148/23838 completed (loss: 1.203710913658142, acc: 0.6860465407371521)
[2025-02-05 13:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:40][root][INFO] - Training Epoch: 2/2, step 20149/23838 completed (loss: 1.1417547464370728, acc: 0.6086956262588501)
[2025-02-05 13:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:41][root][INFO] - Training Epoch: 2/2, step 20150/23838 completed (loss: 1.6147551536560059, acc: 0.5128205418586731)
[2025-02-05 13:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:41][root][INFO] - Training Epoch: 2/2, step 20151/23838 completed (loss: 0.7450122237205505, acc: 0.7714285850524902)
[2025-02-05 13:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:41][root][INFO] - Training Epoch: 2/2, step 20152/23838 completed (loss: 0.9300211668014526, acc: 0.6938775777816772)
[2025-02-05 13:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:42][root][INFO] - Training Epoch: 2/2, step 20153/23838 completed (loss: 1.1112568378448486, acc: 0.5714285969734192)
[2025-02-05 13:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:42][root][INFO] - Training Epoch: 2/2, step 20154/23838 completed (loss: 1.2215938568115234, acc: 0.6428571343421936)
[2025-02-05 13:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:43][root][INFO] - Training Epoch: 2/2, step 20155/23838 completed (loss: 1.0298641920089722, acc: 0.6265060305595398)
[2025-02-05 13:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:43][root][INFO] - Training Epoch: 2/2, step 20156/23838 completed (loss: 1.1064038276672363, acc: 0.6790123581886292)
[2025-02-05 13:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:44][root][INFO] - Training Epoch: 2/2, step 20157/23838 completed (loss: 0.8906246423721313, acc: 0.7555555701255798)
[2025-02-05 13:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:44][root][INFO] - Training Epoch: 2/2, step 20158/23838 completed (loss: 1.0829432010650635, acc: 0.6796116232872009)
[2025-02-05 13:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:44][root][INFO] - Training Epoch: 2/2, step 20159/23838 completed (loss: 0.8884415030479431, acc: 0.7586206793785095)
[2025-02-05 13:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:45][root][INFO] - Training Epoch: 2/2, step 20160/23838 completed (loss: 0.745009183883667, acc: 0.7719298005104065)
[2025-02-05 13:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:45][root][INFO] - Training Epoch: 2/2, step 20161/23838 completed (loss: 0.6951640844345093, acc: 0.761904776096344)
[2025-02-05 13:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:46][root][INFO] - Training Epoch: 2/2, step 20162/23838 completed (loss: 1.1423841714859009, acc: 0.6595744490623474)
[2025-02-05 13:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:46][root][INFO] - Training Epoch: 2/2, step 20163/23838 completed (loss: 0.9890020489692688, acc: 0.7124999761581421)
[2025-02-05 13:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:46][root][INFO] - Training Epoch: 2/2, step 20164/23838 completed (loss: 0.7133598923683167, acc: 0.7368420958518982)
[2025-02-05 13:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:47][root][INFO] - Training Epoch: 2/2, step 20165/23838 completed (loss: 0.6149283647537231, acc: 0.8181818127632141)
[2025-02-05 13:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:47][root][INFO] - Training Epoch: 2/2, step 20166/23838 completed (loss: 0.9540732502937317, acc: 0.7246376872062683)
[2025-02-05 13:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:48][root][INFO] - Training Epoch: 2/2, step 20167/23838 completed (loss: 0.5381761193275452, acc: 0.8723404407501221)
[2025-02-05 13:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:48][root][INFO] - Training Epoch: 2/2, step 20168/23838 completed (loss: 0.9535378217697144, acc: 0.7289719581604004)
[2025-02-05 13:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:49][root][INFO] - Training Epoch: 2/2, step 20169/23838 completed (loss: 0.6498920917510986, acc: 0.8169013857841492)
[2025-02-05 13:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:49][root][INFO] - Training Epoch: 2/2, step 20170/23838 completed (loss: 0.7941698431968689, acc: 0.7096773982048035)
[2025-02-05 13:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:50][root][INFO] - Training Epoch: 2/2, step 20171/23838 completed (loss: 1.0918209552764893, acc: 0.6666666865348816)
[2025-02-05 13:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:50][root][INFO] - Training Epoch: 2/2, step 20172/23838 completed (loss: 1.2590423822402954, acc: 0.6470588445663452)
[2025-02-05 13:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:50][root][INFO] - Training Epoch: 2/2, step 20173/23838 completed (loss: 1.1167211532592773, acc: 0.6666666865348816)
[2025-02-05 13:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:51][root][INFO] - Training Epoch: 2/2, step 20174/23838 completed (loss: 1.1550981998443604, acc: 0.694915235042572)
[2025-02-05 13:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:51][root][INFO] - Training Epoch: 2/2, step 20175/23838 completed (loss: 1.344931721687317, acc: 0.6000000238418579)
[2025-02-05 13:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:52][root][INFO] - Training Epoch: 2/2, step 20176/23838 completed (loss: 1.4891918897628784, acc: 0.6000000238418579)
[2025-02-05 13:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:52][root][INFO] - Training Epoch: 2/2, step 20177/23838 completed (loss: 0.7747055292129517, acc: 0.8194444179534912)
[2025-02-05 13:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:53][root][INFO] - Training Epoch: 2/2, step 20178/23838 completed (loss: 0.79701167345047, acc: 0.8064516186714172)
[2025-02-05 13:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:53][root][INFO] - Training Epoch: 2/2, step 20179/23838 completed (loss: 0.4115208089351654, acc: 0.8571428656578064)
[2025-02-05 13:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:53][root][INFO] - Training Epoch: 2/2, step 20180/23838 completed (loss: 1.0196070671081543, acc: 0.7547169923782349)
[2025-02-05 13:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:54][root][INFO] - Training Epoch: 2/2, step 20181/23838 completed (loss: 1.447974681854248, acc: 0.546875)
[2025-02-05 13:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:54][root][INFO] - Training Epoch: 2/2, step 20182/23838 completed (loss: 0.7904809713363647, acc: 0.8461538553237915)
[2025-02-05 13:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:55][root][INFO] - Training Epoch: 2/2, step 20183/23838 completed (loss: 0.6913147568702698, acc: 0.7752808928489685)
[2025-02-05 13:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:55][root][INFO] - Training Epoch: 2/2, step 20184/23838 completed (loss: 1.2250635623931885, acc: 0.675000011920929)
[2025-02-05 13:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:55][root][INFO] - Training Epoch: 2/2, step 20185/23838 completed (loss: 1.2470651865005493, acc: 0.654411792755127)
[2025-02-05 13:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:56][root][INFO] - Training Epoch: 2/2, step 20186/23838 completed (loss: 1.4004113674163818, acc: 0.6111111044883728)
[2025-02-05 13:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:57][root][INFO] - Training Epoch: 2/2, step 20187/23838 completed (loss: 1.6206389665603638, acc: 0.5520833134651184)
[2025-02-05 13:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:57][root][INFO] - Training Epoch: 2/2, step 20188/23838 completed (loss: 1.0562469959259033, acc: 0.7113401889801025)
[2025-02-05 13:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:57][root][INFO] - Training Epoch: 2/2, step 20189/23838 completed (loss: 1.0219348669052124, acc: 0.6725663542747498)
[2025-02-05 13:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:58][root][INFO] - Training Epoch: 2/2, step 20190/23838 completed (loss: 1.4888747930526733, acc: 0.6304348111152649)
[2025-02-05 13:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:58][root][INFO] - Training Epoch: 2/2, step 20191/23838 completed (loss: 1.0178998708724976, acc: 0.6777777671813965)
[2025-02-05 13:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:59][root][INFO] - Training Epoch: 2/2, step 20192/23838 completed (loss: 1.1457499265670776, acc: 0.6847826242446899)
[2025-02-05 13:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:54:59][root][INFO] - Training Epoch: 2/2, step 20193/23838 completed (loss: 1.1979466676712036, acc: 0.6618704795837402)
[2025-02-05 13:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:00][root][INFO] - Training Epoch: 2/2, step 20194/23838 completed (loss: 1.0379630327224731, acc: 0.7164179086685181)
[2025-02-05 13:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:00][root][INFO] - Training Epoch: 2/2, step 20195/23838 completed (loss: 1.0662449598312378, acc: 0.692307710647583)
[2025-02-05 13:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:00][root][INFO] - Training Epoch: 2/2, step 20196/23838 completed (loss: 1.2628055810928345, acc: 0.6235294342041016)
[2025-02-05 13:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:01][root][INFO] - Training Epoch: 2/2, step 20197/23838 completed (loss: 1.391784906387329, acc: 0.6233766078948975)
[2025-02-05 13:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:02][root][INFO] - Training Epoch: 2/2, step 20198/23838 completed (loss: 1.1142030954360962, acc: 0.675000011920929)
[2025-02-05 13:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:02][root][INFO] - Training Epoch: 2/2, step 20199/23838 completed (loss: 1.1468974351882935, acc: 0.6781609058380127)
[2025-02-05 13:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:03][root][INFO] - Training Epoch: 2/2, step 20200/23838 completed (loss: 0.8533592820167542, acc: 0.7704917788505554)
[2025-02-05 13:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:03][root][INFO] - Training Epoch: 2/2, step 20201/23838 completed (loss: 1.0098294019699097, acc: 0.707317054271698)
[2025-02-05 13:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:04][root][INFO] - Training Epoch: 2/2, step 20202/23838 completed (loss: 0.8995565176010132, acc: 0.767123281955719)
[2025-02-05 13:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:04][root][INFO] - Training Epoch: 2/2, step 20203/23838 completed (loss: 1.1068730354309082, acc: 0.6666666865348816)
[2025-02-05 13:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:04][root][INFO] - Training Epoch: 2/2, step 20204/23838 completed (loss: 0.7870294451713562, acc: 0.8529411554336548)
[2025-02-05 13:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:05][root][INFO] - Training Epoch: 2/2, step 20205/23838 completed (loss: 1.574641466140747, acc: 0.5675675868988037)
[2025-02-05 13:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:05][root][INFO] - Training Epoch: 2/2, step 20206/23838 completed (loss: 1.5564721822738647, acc: 0.59375)
[2025-02-05 13:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:05][root][INFO] - Training Epoch: 2/2, step 20207/23838 completed (loss: 1.0805104970932007, acc: 0.6739130616188049)
[2025-02-05 13:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:06][root][INFO] - Training Epoch: 2/2, step 20208/23838 completed (loss: 1.2906090021133423, acc: 0.5531914830207825)
[2025-02-05 13:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:06][root][INFO] - Training Epoch: 2/2, step 20209/23838 completed (loss: 1.1363184452056885, acc: 0.6911764740943909)
[2025-02-05 13:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:06][root][INFO] - Training Epoch: 2/2, step 20210/23838 completed (loss: 1.2760859727859497, acc: 0.5955055952072144)
[2025-02-05 13:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:07][root][INFO] - Training Epoch: 2/2, step 20211/23838 completed (loss: 1.1984913349151611, acc: 0.639053225517273)
[2025-02-05 13:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:07][root][INFO] - Training Epoch: 2/2, step 20212/23838 completed (loss: 1.2326730489730835, acc: 0.6065573692321777)
[2025-02-05 13:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:08][root][INFO] - Training Epoch: 2/2, step 20213/23838 completed (loss: 0.7889788746833801, acc: 0.738095223903656)
[2025-02-05 13:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:08][root][INFO] - Training Epoch: 2/2, step 20214/23838 completed (loss: 1.387479543685913, acc: 0.5977011322975159)
[2025-02-05 13:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:09][root][INFO] - Training Epoch: 2/2, step 20215/23838 completed (loss: 0.9990246295928955, acc: 0.699999988079071)
[2025-02-05 13:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:09][root][INFO] - Training Epoch: 2/2, step 20216/23838 completed (loss: 1.0774813890457153, acc: 0.7058823704719543)
[2025-02-05 13:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:09][root][INFO] - Training Epoch: 2/2, step 20217/23838 completed (loss: 0.9770037531852722, acc: 0.698630154132843)
[2025-02-05 13:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:10][root][INFO] - Training Epoch: 2/2, step 20218/23838 completed (loss: 1.4814949035644531, acc: 0.5394737124443054)
[2025-02-05 13:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:10][root][INFO] - Training Epoch: 2/2, step 20219/23838 completed (loss: 1.0714701414108276, acc: 0.6440678238868713)
[2025-02-05 13:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:11][root][INFO] - Training Epoch: 2/2, step 20220/23838 completed (loss: 1.3175771236419678, acc: 0.6224489808082581)
[2025-02-05 13:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:11][root][INFO] - Training Epoch: 2/2, step 20221/23838 completed (loss: 0.837472677230835, acc: 0.7179487347602844)
[2025-02-05 13:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:11][root][INFO] - Training Epoch: 2/2, step 20222/23838 completed (loss: 1.117965579032898, acc: 0.6764705777168274)
[2025-02-05 13:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:12][root][INFO] - Training Epoch: 2/2, step 20223/23838 completed (loss: 1.2219570875167847, acc: 0.6341463327407837)
[2025-02-05 13:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:12][root][INFO] - Training Epoch: 2/2, step 20224/23838 completed (loss: 1.236831784248352, acc: 0.6357616186141968)
[2025-02-05 13:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:13][root][INFO] - Training Epoch: 2/2, step 20225/23838 completed (loss: 0.8680577278137207, acc: 0.7857142686843872)
[2025-02-05 13:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:13][root][INFO] - Training Epoch: 2/2, step 20226/23838 completed (loss: 1.1244227886199951, acc: 0.6917293071746826)
[2025-02-05 13:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:13][root][INFO] - Training Epoch: 2/2, step 20227/23838 completed (loss: 1.0737974643707275, acc: 0.6605504751205444)
[2025-02-05 13:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:14][root][INFO] - Training Epoch: 2/2, step 20228/23838 completed (loss: 1.1565544605255127, acc: 0.6612021923065186)
[2025-02-05 13:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:14][root][INFO] - Training Epoch: 2/2, step 20229/23838 completed (loss: 1.1405255794525146, acc: 0.6388888955116272)
[2025-02-05 13:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:15][root][INFO] - Training Epoch: 2/2, step 20230/23838 completed (loss: 0.8699794411659241, acc: 0.717391312122345)
[2025-02-05 13:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:15][root][INFO] - Training Epoch: 2/2, step 20231/23838 completed (loss: 1.1993882656097412, acc: 0.6397058963775635)
[2025-02-05 13:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:15][root][INFO] - Training Epoch: 2/2, step 20232/23838 completed (loss: 1.172180414199829, acc: 0.6666666865348816)
[2025-02-05 13:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:16][root][INFO] - Training Epoch: 2/2, step 20233/23838 completed (loss: 1.118302822113037, acc: 0.654321014881134)
[2025-02-05 13:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:16][root][INFO] - Training Epoch: 2/2, step 20234/23838 completed (loss: 1.370681643486023, acc: 0.5728155374526978)
[2025-02-05 13:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:17][root][INFO] - Training Epoch: 2/2, step 20235/23838 completed (loss: 1.1398568153381348, acc: 0.6153846383094788)
[2025-02-05 13:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:17][root][INFO] - Training Epoch: 2/2, step 20236/23838 completed (loss: 0.5394255518913269, acc: 0.8611111044883728)
[2025-02-05 13:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:17][root][INFO] - Training Epoch: 2/2, step 20237/23838 completed (loss: 0.8018310070037842, acc: 0.7096773982048035)
[2025-02-05 13:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:18][root][INFO] - Training Epoch: 2/2, step 20238/23838 completed (loss: 1.2665938138961792, acc: 0.6477272510528564)
[2025-02-05 13:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:18][root][INFO] - Training Epoch: 2/2, step 20239/23838 completed (loss: 1.4589719772338867, acc: 0.5693780183792114)
[2025-02-05 13:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:19][root][INFO] - Training Epoch: 2/2, step 20240/23838 completed (loss: 1.1493037939071655, acc: 0.6710526347160339)
[2025-02-05 13:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:19][root][INFO] - Training Epoch: 2/2, step 20241/23838 completed (loss: 1.2341892719268799, acc: 0.6612903475761414)
[2025-02-05 13:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:19][root][INFO] - Training Epoch: 2/2, step 20242/23838 completed (loss: 1.18766450881958, acc: 0.6515151262283325)
[2025-02-05 13:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:20][root][INFO] - Training Epoch: 2/2, step 20243/23838 completed (loss: 1.1755014657974243, acc: 0.6477987170219421)
[2025-02-05 13:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:20][root][INFO] - Training Epoch: 2/2, step 20244/23838 completed (loss: 1.2612258195877075, acc: 0.6666666865348816)
[2025-02-05 13:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:21][root][INFO] - Training Epoch: 2/2, step 20245/23838 completed (loss: 1.044334053993225, acc: 0.6708860993385315)
[2025-02-05 13:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:21][root][INFO] - Training Epoch: 2/2, step 20246/23838 completed (loss: 0.9267339110374451, acc: 0.7539682388305664)
[2025-02-05 13:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:21][root][INFO] - Training Epoch: 2/2, step 20247/23838 completed (loss: 1.0516023635864258, acc: 0.6666666865348816)
[2025-02-05 13:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:22][root][INFO] - Training Epoch: 2/2, step 20248/23838 completed (loss: 1.3818517923355103, acc: 0.5840708017349243)
[2025-02-05 13:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:22][root][INFO] - Training Epoch: 2/2, step 20249/23838 completed (loss: 1.1064722537994385, acc: 0.7179487347602844)
[2025-02-05 13:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:23][root][INFO] - Training Epoch: 2/2, step 20250/23838 completed (loss: 1.2188717126846313, acc: 0.6343283653259277)
[2025-02-05 13:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:23][root][INFO] - Training Epoch: 2/2, step 20251/23838 completed (loss: 1.1654820442199707, acc: 0.6296296119689941)
[2025-02-05 13:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:23][root][INFO] - Training Epoch: 2/2, step 20252/23838 completed (loss: 0.8400614261627197, acc: 0.7356321811676025)
[2025-02-05 13:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:24][root][INFO] - Training Epoch: 2/2, step 20253/23838 completed (loss: 1.328969120979309, acc: 0.6284152865409851)
[2025-02-05 13:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:24][root][INFO] - Training Epoch: 2/2, step 20254/23838 completed (loss: 0.889768660068512, acc: 0.7678571343421936)
[2025-02-05 13:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:25][root][INFO] - Training Epoch: 2/2, step 20255/23838 completed (loss: 1.6302019357681274, acc: 0.4615384638309479)
[2025-02-05 13:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:25][root][INFO] - Training Epoch: 2/2, step 20256/23838 completed (loss: 1.1601210832595825, acc: 0.6857143044471741)
[2025-02-05 13:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:25][root][INFO] - Training Epoch: 2/2, step 20257/23838 completed (loss: 1.2217711210250854, acc: 0.7111111283302307)
[2025-02-05 13:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:26][root][INFO] - Training Epoch: 2/2, step 20258/23838 completed (loss: 0.8449977040290833, acc: 0.8088235259056091)
[2025-02-05 13:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:26][root][INFO] - Training Epoch: 2/2, step 20259/23838 completed (loss: 1.1912838220596313, acc: 0.6216216087341309)
[2025-02-05 13:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:27][root][INFO] - Training Epoch: 2/2, step 20260/23838 completed (loss: 1.325239896774292, acc: 0.6399999856948853)
[2025-02-05 13:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:27][root][INFO] - Training Epoch: 2/2, step 20261/23838 completed (loss: 1.0053772926330566, acc: 0.6744186282157898)
[2025-02-05 13:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:27][root][INFO] - Training Epoch: 2/2, step 20262/23838 completed (loss: 1.0984587669372559, acc: 0.6363636255264282)
[2025-02-05 13:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:28][root][INFO] - Training Epoch: 2/2, step 20263/23838 completed (loss: 1.2763571739196777, acc: 0.6399999856948853)
[2025-02-05 13:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:28][root][INFO] - Training Epoch: 2/2, step 20264/23838 completed (loss: 0.9436542391777039, acc: 0.7666666507720947)
[2025-02-05 13:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:28][root][INFO] - Training Epoch: 2/2, step 20265/23838 completed (loss: 0.8774914741516113, acc: 0.7090908885002136)
[2025-02-05 13:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:29][root][INFO] - Training Epoch: 2/2, step 20266/23838 completed (loss: 1.1148254871368408, acc: 0.6176470518112183)
[2025-02-05 13:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:29][root][INFO] - Training Epoch: 2/2, step 20267/23838 completed (loss: 1.1965620517730713, acc: 0.6363636255264282)
[2025-02-05 13:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:30][root][INFO] - Training Epoch: 2/2, step 20268/23838 completed (loss: 0.9489461183547974, acc: 0.7105262875556946)
[2025-02-05 13:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:30][root][INFO] - Training Epoch: 2/2, step 20269/23838 completed (loss: 1.4405202865600586, acc: 0.6078431606292725)
[2025-02-05 13:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:30][root][INFO] - Training Epoch: 2/2, step 20270/23838 completed (loss: 0.8634070754051208, acc: 0.7101449370384216)
[2025-02-05 13:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:31][root][INFO] - Training Epoch: 2/2, step 20271/23838 completed (loss: 1.4485526084899902, acc: 0.6200000047683716)
[2025-02-05 13:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:31][root][INFO] - Training Epoch: 2/2, step 20272/23838 completed (loss: 0.8847052454948425, acc: 0.7777777910232544)
[2025-02-05 13:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:32][root][INFO] - Training Epoch: 2/2, step 20273/23838 completed (loss: 1.2261805534362793, acc: 0.7166666388511658)
[2025-02-05 13:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:32][root][INFO] - Training Epoch: 2/2, step 20274/23838 completed (loss: 1.247918725013733, acc: 0.6111111044883728)
[2025-02-05 13:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:32][root][INFO] - Training Epoch: 2/2, step 20275/23838 completed (loss: 1.3100521564483643, acc: 0.6043956279754639)
[2025-02-05 13:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:33][root][INFO] - Training Epoch: 2/2, step 20276/23838 completed (loss: 1.2176443338394165, acc: 0.6486486196517944)
[2025-02-05 13:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:33][root][INFO] - Training Epoch: 2/2, step 20277/23838 completed (loss: 0.9098431468009949, acc: 0.7037037014961243)
[2025-02-05 13:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:34][root][INFO] - Training Epoch: 2/2, step 20278/23838 completed (loss: 1.436829924583435, acc: 0.5833333134651184)
[2025-02-05 13:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:34][root][INFO] - Training Epoch: 2/2, step 20279/23838 completed (loss: 0.7641359567642212, acc: 0.7777777910232544)
[2025-02-05 13:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:34][root][INFO] - Training Epoch: 2/2, step 20280/23838 completed (loss: 1.0370218753814697, acc: 0.692307710647583)
[2025-02-05 13:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:35][root][INFO] - Training Epoch: 2/2, step 20281/23838 completed (loss: 0.8039590120315552, acc: 0.7714285850524902)
[2025-02-05 13:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:35][root][INFO] - Training Epoch: 2/2, step 20282/23838 completed (loss: 2.5336756706237793, acc: 0.0)
[2025-02-05 13:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:35][root][INFO] - Training Epoch: 2/2, step 20283/23838 completed (loss: 1.7069092988967896, acc: 0.5)
[2025-02-05 13:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:36][root][INFO] - Training Epoch: 2/2, step 20284/23838 completed (loss: 1.4261375665664673, acc: 0.5833333134651184)
[2025-02-05 13:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:36][root][INFO] - Training Epoch: 2/2, step 20285/23838 completed (loss: 0.8317237496376038, acc: 0.7586206793785095)
[2025-02-05 13:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:36][root][INFO] - Training Epoch: 2/2, step 20286/23838 completed (loss: 0.4354446530342102, acc: 0.8500000238418579)
[2025-02-05 13:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:37][root][INFO] - Training Epoch: 2/2, step 20287/23838 completed (loss: 0.48383909463882446, acc: 0.8902438879013062)
[2025-02-05 13:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:37][root][INFO] - Training Epoch: 2/2, step 20288/23838 completed (loss: 0.8143301606178284, acc: 0.7906976938247681)
[2025-02-05 13:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:38][root][INFO] - Training Epoch: 2/2, step 20289/23838 completed (loss: 0.953748881816864, acc: 0.8095238208770752)
[2025-02-05 13:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:38][root][INFO] - Training Epoch: 2/2, step 20290/23838 completed (loss: 0.6629469394683838, acc: 0.7954545617103577)
[2025-02-05 13:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:38][root][INFO] - Training Epoch: 2/2, step 20291/23838 completed (loss: 1.2468323707580566, acc: 0.6521739363670349)
[2025-02-05 13:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:39][root][INFO] - Training Epoch: 2/2, step 20292/23838 completed (loss: 1.132673740386963, acc: 0.7777777910232544)
[2025-02-05 13:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:39][root][INFO] - Training Epoch: 2/2, step 20293/23838 completed (loss: 1.7199114561080933, acc: 0.6363636255264282)
[2025-02-05 13:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:39][root][INFO] - Training Epoch: 2/2, step 20294/23838 completed (loss: 0.9493110775947571, acc: 0.7358490824699402)
[2025-02-05 13:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:40][root][INFO] - Training Epoch: 2/2, step 20295/23838 completed (loss: 1.2438517808914185, acc: 0.6268656849861145)
[2025-02-05 13:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:40][root][INFO] - Training Epoch: 2/2, step 20296/23838 completed (loss: 0.9372459053993225, acc: 0.7536231875419617)
[2025-02-05 13:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:41][root][INFO] - Training Epoch: 2/2, step 20297/23838 completed (loss: 0.7133990526199341, acc: 0.75)
[2025-02-05 13:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:41][root][INFO] - Training Epoch: 2/2, step 20298/23838 completed (loss: 1.1198434829711914, acc: 0.7058823704719543)
[2025-02-05 13:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:41][root][INFO] - Training Epoch: 2/2, step 20299/23838 completed (loss: 1.416557788848877, acc: 0.6000000238418579)
[2025-02-05 13:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:42][root][INFO] - Training Epoch: 2/2, step 20300/23838 completed (loss: 1.9576880931854248, acc: 0.4117647111415863)
[2025-02-05 13:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:42][root][INFO] - Training Epoch: 2/2, step 20301/23838 completed (loss: 1.1220881938934326, acc: 0.6470588445663452)
[2025-02-05 13:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:43][root][INFO] - Training Epoch: 2/2, step 20302/23838 completed (loss: 1.4003996849060059, acc: 0.5744680762290955)
[2025-02-05 13:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:43][root][INFO] - Training Epoch: 2/2, step 20303/23838 completed (loss: 1.2089433670043945, acc: 0.686274528503418)
[2025-02-05 13:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:44][root][INFO] - Training Epoch: 2/2, step 20304/23838 completed (loss: 1.4166926145553589, acc: 0.6216216087341309)
[2025-02-05 13:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:44][root][INFO] - Training Epoch: 2/2, step 20305/23838 completed (loss: 1.221027135848999, acc: 0.5853658318519592)
[2025-02-05 13:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:44][root][INFO] - Training Epoch: 2/2, step 20306/23838 completed (loss: 0.9365631937980652, acc: 0.699999988079071)
[2025-02-05 13:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:45][root][INFO] - Training Epoch: 2/2, step 20307/23838 completed (loss: 0.8826995491981506, acc: 0.7777777910232544)
[2025-02-05 13:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:45][root][INFO] - Training Epoch: 2/2, step 20308/23838 completed (loss: 1.1242777109146118, acc: 0.6499999761581421)
[2025-02-05 13:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:46][root][INFO] - Training Epoch: 2/2, step 20309/23838 completed (loss: 1.363490104675293, acc: 0.6200000047683716)
[2025-02-05 13:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:46][root][INFO] - Training Epoch: 2/2, step 20310/23838 completed (loss: 1.203839659690857, acc: 0.6666666865348816)
[2025-02-05 13:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:46][root][INFO] - Training Epoch: 2/2, step 20311/23838 completed (loss: 1.1844254732131958, acc: 0.65625)
[2025-02-05 13:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:47][root][INFO] - Training Epoch: 2/2, step 20312/23838 completed (loss: 0.9439506530761719, acc: 0.7142857313156128)
[2025-02-05 13:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:47][root][INFO] - Training Epoch: 2/2, step 20313/23838 completed (loss: 1.382947564125061, acc: 0.5517241358757019)
[2025-02-05 13:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:47][root][INFO] - Training Epoch: 2/2, step 20314/23838 completed (loss: 0.8065111637115479, acc: 0.75)
[2025-02-05 13:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:48][root][INFO] - Training Epoch: 2/2, step 20315/23838 completed (loss: 1.374197244644165, acc: 0.6000000238418579)
[2025-02-05 13:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:48][root][INFO] - Training Epoch: 2/2, step 20316/23838 completed (loss: 1.485946536064148, acc: 0.48148149251937866)
[2025-02-05 13:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:49][root][INFO] - Training Epoch: 2/2, step 20317/23838 completed (loss: 1.0412095785140991, acc: 0.6857143044471741)
[2025-02-05 13:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:49][root][INFO] - Training Epoch: 2/2, step 20318/23838 completed (loss: 0.856846809387207, acc: 0.7446808218955994)
[2025-02-05 13:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:50][root][INFO] - Training Epoch: 2/2, step 20319/23838 completed (loss: 1.196023941040039, acc: 0.6933333277702332)
[2025-02-05 13:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:50][root][INFO] - Training Epoch: 2/2, step 20320/23838 completed (loss: 1.2306621074676514, acc: 0.6507936716079712)
[2025-02-05 13:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:50][root][INFO] - Training Epoch: 2/2, step 20321/23838 completed (loss: 0.6515812277793884, acc: 0.8571428656578064)
[2025-02-05 13:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:51][root][INFO] - Training Epoch: 2/2, step 20322/23838 completed (loss: 0.7930676341056824, acc: 0.7708333134651184)
[2025-02-05 13:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:51][root][INFO] - Training Epoch: 2/2, step 20323/23838 completed (loss: 0.6386229395866394, acc: 0.8205128312110901)
[2025-02-05 13:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:51][root][INFO] - Training Epoch: 2/2, step 20324/23838 completed (loss: 0.9922511577606201, acc: 0.7307692170143127)
[2025-02-05 13:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:52][root][INFO] - Training Epoch: 2/2, step 20325/23838 completed (loss: 1.3367536067962646, acc: 0.6385542154312134)
[2025-02-05 13:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:52][root][INFO] - Training Epoch: 2/2, step 20326/23838 completed (loss: 1.2873464822769165, acc: 0.6071428656578064)
[2025-02-05 13:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:53][root][INFO] - Training Epoch: 2/2, step 20327/23838 completed (loss: 1.1490304470062256, acc: 0.6666666865348816)
[2025-02-05 13:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:53][root][INFO] - Training Epoch: 2/2, step 20328/23838 completed (loss: 1.035183072090149, acc: 0.6578947305679321)
[2025-02-05 13:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:54][root][INFO] - Training Epoch: 2/2, step 20329/23838 completed (loss: 1.193537712097168, acc: 0.6382978558540344)
[2025-02-05 13:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:54][root][INFO] - Training Epoch: 2/2, step 20330/23838 completed (loss: 0.6798757314682007, acc: 0.800000011920929)
[2025-02-05 13:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:54][root][INFO] - Training Epoch: 2/2, step 20331/23838 completed (loss: 1.4429912567138672, acc: 0.5566037893295288)
[2025-02-05 13:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:55][root][INFO] - Training Epoch: 2/2, step 20332/23838 completed (loss: 1.216314673423767, acc: 0.6233766078948975)
[2025-02-05 13:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:55][root][INFO] - Training Epoch: 2/2, step 20333/23838 completed (loss: 1.3400964736938477, acc: 0.5887850522994995)
[2025-02-05 13:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:55][root][INFO] - Training Epoch: 2/2, step 20334/23838 completed (loss: 1.358904242515564, acc: 0.6283186078071594)
[2025-02-05 13:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:56][root][INFO] - Training Epoch: 2/2, step 20335/23838 completed (loss: 1.5596262216567993, acc: 0.5555555820465088)
[2025-02-05 13:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:56][root][INFO] - Training Epoch: 2/2, step 20336/23838 completed (loss: 1.5580939054489136, acc: 0.5568181872367859)
[2025-02-05 13:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:57][root][INFO] - Training Epoch: 2/2, step 20337/23838 completed (loss: 1.3955254554748535, acc: 0.6216216087341309)
[2025-02-05 13:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:57][root][INFO] - Training Epoch: 2/2, step 20338/23838 completed (loss: 1.250284194946289, acc: 0.6091954112052917)
[2025-02-05 13:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:57][root][INFO] - Training Epoch: 2/2, step 20339/23838 completed (loss: 1.643980622291565, acc: 0.5051546096801758)
[2025-02-05 13:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:58][root][INFO] - Training Epoch: 2/2, step 20340/23838 completed (loss: 2.093883752822876, acc: 0.3333333432674408)
[2025-02-05 13:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:58][root][INFO] - Training Epoch: 2/2, step 20341/23838 completed (loss: 1.607309103012085, acc: 0.5652173757553101)
[2025-02-05 13:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:59][root][INFO] - Training Epoch: 2/2, step 20342/23838 completed (loss: 1.4972012042999268, acc: 0.604651153087616)
[2025-02-05 13:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:59][root][INFO] - Training Epoch: 2/2, step 20343/23838 completed (loss: 1.5680620670318604, acc: 0.5121951103210449)
[2025-02-05 13:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:55:59][root][INFO] - Training Epoch: 2/2, step 20344/23838 completed (loss: 1.1969590187072754, acc: 0.6857143044471741)
[2025-02-05 13:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:00][root][INFO] - Training Epoch: 2/2, step 20345/23838 completed (loss: 1.454312801361084, acc: 0.6162790656089783)
[2025-02-05 13:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:00][root][INFO] - Training Epoch: 2/2, step 20346/23838 completed (loss: 1.5032974481582642, acc: 0.5571428537368774)
[2025-02-05 13:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:01][root][INFO] - Training Epoch: 2/2, step 20347/23838 completed (loss: 1.8145372867584229, acc: 0.5116279125213623)
[2025-02-05 13:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:01][root][INFO] - Training Epoch: 2/2, step 20348/23838 completed (loss: 1.3344963788986206, acc: 0.6666666865348816)
[2025-02-05 13:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:01][root][INFO] - Training Epoch: 2/2, step 20349/23838 completed (loss: 0.9642184972763062, acc: 0.6935483813285828)
[2025-02-05 13:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:02][root][INFO] - Training Epoch: 2/2, step 20350/23838 completed (loss: 1.068517804145813, acc: 0.7068965435028076)
[2025-02-05 13:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:02][root][INFO] - Training Epoch: 2/2, step 20351/23838 completed (loss: 0.9796676635742188, acc: 0.7325581312179565)
[2025-02-05 13:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:03][root][INFO] - Training Epoch: 2/2, step 20352/23838 completed (loss: 1.2694923877716064, acc: 0.625)
[2025-02-05 13:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:03][root][INFO] - Training Epoch: 2/2, step 20353/23838 completed (loss: 1.5523710250854492, acc: 0.5777778029441833)
[2025-02-05 13:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:04][root][INFO] - Training Epoch: 2/2, step 20354/23838 completed (loss: 1.4260252714157104, acc: 0.6081081032752991)
[2025-02-05 13:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:04][root][INFO] - Training Epoch: 2/2, step 20355/23838 completed (loss: 1.448634147644043, acc: 0.5476190447807312)
[2025-02-05 13:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:04][root][INFO] - Training Epoch: 2/2, step 20356/23838 completed (loss: 0.9819233417510986, acc: 0.7142857313156128)
[2025-02-05 13:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:05][root][INFO] - Training Epoch: 2/2, step 20357/23838 completed (loss: 1.1065620183944702, acc: 0.7124999761581421)
[2025-02-05 13:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:05][root][INFO] - Training Epoch: 2/2, step 20358/23838 completed (loss: 1.3911373615264893, acc: 0.5952380895614624)
[2025-02-05 13:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:06][root][INFO] - Training Epoch: 2/2, step 20359/23838 completed (loss: 1.3827377557754517, acc: 0.5894736647605896)
[2025-02-05 13:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:06][root][INFO] - Training Epoch: 2/2, step 20360/23838 completed (loss: 1.3095853328704834, acc: 0.6285714507102966)
[2025-02-05 13:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:06][root][INFO] - Training Epoch: 2/2, step 20361/23838 completed (loss: 0.9646603465080261, acc: 0.7777777910232544)
[2025-02-05 13:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:07][root][INFO] - Training Epoch: 2/2, step 20362/23838 completed (loss: 1.4372652769088745, acc: 0.5827814340591431)
[2025-02-05 13:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:07][root][INFO] - Training Epoch: 2/2, step 20363/23838 completed (loss: 1.4640802145004272, acc: 0.5742574334144592)
[2025-02-05 13:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:08][root][INFO] - Training Epoch: 2/2, step 20364/23838 completed (loss: 1.403647541999817, acc: 0.5809524059295654)
[2025-02-05 13:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:08][root][INFO] - Training Epoch: 2/2, step 20365/23838 completed (loss: 1.3142434358596802, acc: 0.5798319578170776)
[2025-02-05 13:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:08][root][INFO] - Training Epoch: 2/2, step 20366/23838 completed (loss: 0.9943946003913879, acc: 0.7142857313156128)
[2025-02-05 13:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:09][root][INFO] - Training Epoch: 2/2, step 20367/23838 completed (loss: 1.1868335008621216, acc: 0.6274510025978088)
[2025-02-05 13:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:09][root][INFO] - Training Epoch: 2/2, step 20368/23838 completed (loss: 1.2424803972244263, acc: 0.5918367505073547)
[2025-02-05 13:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:10][root][INFO] - Training Epoch: 2/2, step 20369/23838 completed (loss: 1.253240942955017, acc: 0.6184210777282715)
[2025-02-05 13:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:10][root][INFO] - Training Epoch: 2/2, step 20370/23838 completed (loss: 1.1590092182159424, acc: 0.5581395626068115)
[2025-02-05 13:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:11][root][INFO] - Training Epoch: 2/2, step 20371/23838 completed (loss: 1.0833977460861206, acc: 0.6838235259056091)
[2025-02-05 13:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:11][root][INFO] - Training Epoch: 2/2, step 20372/23838 completed (loss: 1.0763856172561646, acc: 0.6867470145225525)
[2025-02-05 13:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:11][root][INFO] - Training Epoch: 2/2, step 20373/23838 completed (loss: 1.4602503776550293, acc: 0.5199999809265137)
[2025-02-05 13:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:12][root][INFO] - Training Epoch: 2/2, step 20374/23838 completed (loss: 1.2503989934921265, acc: 0.5813953280448914)
[2025-02-05 13:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:12][root][INFO] - Training Epoch: 2/2, step 20375/23838 completed (loss: 0.9856207370758057, acc: 0.7078651785850525)
[2025-02-05 13:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:12][root][INFO] - Training Epoch: 2/2, step 20376/23838 completed (loss: 1.4597491025924683, acc: 0.6166666746139526)
[2025-02-05 13:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:13][root][INFO] - Training Epoch: 2/2, step 20377/23838 completed (loss: 1.0498594045639038, acc: 0.7155963182449341)
[2025-02-05 13:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:13][root][INFO] - Training Epoch: 2/2, step 20378/23838 completed (loss: 1.4244003295898438, acc: 0.5517241358757019)
[2025-02-05 13:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:14][root][INFO] - Training Epoch: 2/2, step 20379/23838 completed (loss: 1.1686689853668213, acc: 0.6666666865348816)
[2025-02-05 13:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:14][root][INFO] - Training Epoch: 2/2, step 20380/23838 completed (loss: 1.4078553915023804, acc: 0.5833333134651184)
[2025-02-05 13:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:14][root][INFO] - Training Epoch: 2/2, step 20381/23838 completed (loss: 1.194024682044983, acc: 0.6101694703102112)
[2025-02-05 13:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:15][root][INFO] - Training Epoch: 2/2, step 20382/23838 completed (loss: 1.0777051448822021, acc: 0.6868686676025391)
[2025-02-05 13:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:15][root][INFO] - Training Epoch: 2/2, step 20383/23838 completed (loss: 1.080820918083191, acc: 0.6774193644523621)
[2025-02-05 13:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:16][root][INFO] - Training Epoch: 2/2, step 20384/23838 completed (loss: 1.0651153326034546, acc: 0.6774193644523621)
[2025-02-05 13:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:16][root][INFO] - Training Epoch: 2/2, step 20385/23838 completed (loss: 1.3927326202392578, acc: 0.591549277305603)
[2025-02-05 13:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:17][root][INFO] - Training Epoch: 2/2, step 20386/23838 completed (loss: 1.6467552185058594, acc: 0.5230769515037537)
[2025-02-05 13:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:17][root][INFO] - Training Epoch: 2/2, step 20387/23838 completed (loss: 1.4870425462722778, acc: 0.5617977380752563)
[2025-02-05 13:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:18][root][INFO] - Training Epoch: 2/2, step 20388/23838 completed (loss: 1.8043551445007324, acc: 0.4693877696990967)
[2025-02-05 13:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:18][root][INFO] - Training Epoch: 2/2, step 20389/23838 completed (loss: 1.4945920705795288, acc: 0.5979381203651428)
[2025-02-05 13:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:18][root][INFO] - Training Epoch: 2/2, step 20390/23838 completed (loss: 1.5485188961029053, acc: 0.5448275804519653)
[2025-02-05 13:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:19][root][INFO] - Training Epoch: 2/2, step 20391/23838 completed (loss: 1.1629338264465332, acc: 0.6969696879386902)
[2025-02-05 13:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:19][root][INFO] - Training Epoch: 2/2, step 20392/23838 completed (loss: 1.2275221347808838, acc: 0.6592592597007751)
[2025-02-05 13:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:20][root][INFO] - Training Epoch: 2/2, step 20393/23838 completed (loss: 1.3087362051010132, acc: 0.6494845151901245)
[2025-02-05 13:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:20][root][INFO] - Training Epoch: 2/2, step 20394/23838 completed (loss: 1.390234351158142, acc: 0.5643564462661743)
[2025-02-05 13:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:20][root][INFO] - Training Epoch: 2/2, step 20395/23838 completed (loss: 1.3691496849060059, acc: 0.5734265446662903)
[2025-02-05 13:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:21][root][INFO] - Training Epoch: 2/2, step 20396/23838 completed (loss: 1.251590609550476, acc: 0.6141079068183899)
[2025-02-05 13:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:21][root][INFO] - Training Epoch: 2/2, step 20397/23838 completed (loss: 1.8763834238052368, acc: 0.4932432472705841)
[2025-02-05 13:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:22][root][INFO] - Training Epoch: 2/2, step 20398/23838 completed (loss: 1.3463256359100342, acc: 0.6305732727050781)
[2025-02-05 13:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:22][root][INFO] - Training Epoch: 2/2, step 20399/23838 completed (loss: 1.1710280179977417, acc: 0.6465517282485962)
[2025-02-05 13:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:23][root][INFO] - Training Epoch: 2/2, step 20400/23838 completed (loss: 1.2040669918060303, acc: 0.6339869499206543)
[2025-02-05 13:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:23][root][INFO] - Training Epoch: 2/2, step 20401/23838 completed (loss: 1.4023051261901855, acc: 0.640350878238678)
[2025-02-05 13:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:24][root][INFO] - Training Epoch: 2/2, step 20402/23838 completed (loss: 1.021254062652588, acc: 0.6746031641960144)
[2025-02-05 13:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:24][root][INFO] - Training Epoch: 2/2, step 20403/23838 completed (loss: 1.3873528242111206, acc: 0.5955055952072144)
[2025-02-05 13:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:24][root][INFO] - Training Epoch: 2/2, step 20404/23838 completed (loss: 1.2571206092834473, acc: 0.6592592597007751)
[2025-02-05 13:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:25][root][INFO] - Training Epoch: 2/2, step 20405/23838 completed (loss: 1.1194376945495605, acc: 0.6585366129875183)
[2025-02-05 13:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:25][root][INFO] - Training Epoch: 2/2, step 20406/23838 completed (loss: 0.8293419480323792, acc: 0.7916666865348816)
[2025-02-05 13:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:25][root][INFO] - Training Epoch: 2/2, step 20407/23838 completed (loss: 1.5710629224777222, acc: 0.5166666507720947)
[2025-02-05 13:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:26][root][INFO] - Training Epoch: 2/2, step 20408/23838 completed (loss: 1.3720415830612183, acc: 0.5593220591545105)
[2025-02-05 13:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:26][root][INFO] - Training Epoch: 2/2, step 20409/23838 completed (loss: 1.3270885944366455, acc: 0.5636363625526428)
[2025-02-05 13:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:27][root][INFO] - Training Epoch: 2/2, step 20410/23838 completed (loss: 1.424381971359253, acc: 0.6379310488700867)
[2025-02-05 13:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:27][root][INFO] - Training Epoch: 2/2, step 20411/23838 completed (loss: 1.1132532358169556, acc: 0.6721311211585999)
[2025-02-05 13:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:28][root][INFO] - Training Epoch: 2/2, step 20412/23838 completed (loss: 1.7401221990585327, acc: 0.5249999761581421)
[2025-02-05 13:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:28][root][INFO] - Training Epoch: 2/2, step 20413/23838 completed (loss: 1.273976445198059, acc: 0.6363636255264282)
[2025-02-05 13:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:29][root][INFO] - Training Epoch: 2/2, step 20414/23838 completed (loss: 0.857650101184845, acc: 0.774193525314331)
[2025-02-05 13:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:29][root][INFO] - Training Epoch: 2/2, step 20415/23838 completed (loss: 1.8054287433624268, acc: 0.4767441749572754)
[2025-02-05 13:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:29][root][INFO] - Training Epoch: 2/2, step 20416/23838 completed (loss: 1.043725848197937, acc: 0.6511628031730652)
[2025-02-05 13:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:30][root][INFO] - Training Epoch: 2/2, step 20417/23838 completed (loss: 0.8502978086471558, acc: 0.7837837934494019)
[2025-02-05 13:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:30][root][INFO] - Training Epoch: 2/2, step 20418/23838 completed (loss: 1.0426541566848755, acc: 0.6911764740943909)
[2025-02-05 13:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:30][root][INFO] - Training Epoch: 2/2, step 20419/23838 completed (loss: 1.3029654026031494, acc: 0.6307692527770996)
[2025-02-05 13:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:31][root][INFO] - Training Epoch: 2/2, step 20420/23838 completed (loss: 0.977572500705719, acc: 0.7228915691375732)
[2025-02-05 13:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:31][root][INFO] - Training Epoch: 2/2, step 20421/23838 completed (loss: 1.316754937171936, acc: 0.6478873491287231)
[2025-02-05 13:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:32][root][INFO] - Training Epoch: 2/2, step 20422/23838 completed (loss: 1.139201283454895, acc: 0.6603773832321167)
[2025-02-05 13:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:32][root][INFO] - Training Epoch: 2/2, step 20423/23838 completed (loss: 1.0132534503936768, acc: 0.75)
[2025-02-05 13:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:33][root][INFO] - Training Epoch: 2/2, step 20424/23838 completed (loss: 1.5948004722595215, acc: 0.5370370149612427)
[2025-02-05 13:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:33][root][INFO] - Training Epoch: 2/2, step 20425/23838 completed (loss: 1.3263270854949951, acc: 0.6341463327407837)
[2025-02-05 13:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:33][root][INFO] - Training Epoch: 2/2, step 20426/23838 completed (loss: 1.2894831895828247, acc: 0.6346153616905212)
[2025-02-05 13:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:34][root][INFO] - Training Epoch: 2/2, step 20427/23838 completed (loss: 1.2699997425079346, acc: 0.6515151262283325)
[2025-02-05 13:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:34][root][INFO] - Training Epoch: 2/2, step 20428/23838 completed (loss: 1.0432848930358887, acc: 0.6481481194496155)
[2025-02-05 13:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:35][root][INFO] - Training Epoch: 2/2, step 20429/23838 completed (loss: 1.3865725994110107, acc: 0.5909090638160706)
[2025-02-05 13:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:35][root][INFO] - Training Epoch: 2/2, step 20430/23838 completed (loss: 1.3945509195327759, acc: 0.5955055952072144)
[2025-02-05 13:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:35][root][INFO] - Training Epoch: 2/2, step 20431/23838 completed (loss: 0.9566359519958496, acc: 0.7124999761581421)
[2025-02-05 13:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:36][root][INFO] - Training Epoch: 2/2, step 20432/23838 completed (loss: 1.0750408172607422, acc: 0.6976743936538696)
[2025-02-05 13:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:36][root][INFO] - Training Epoch: 2/2, step 20433/23838 completed (loss: 1.3965703248977661, acc: 0.6296296119689941)
[2025-02-05 13:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:36][root][INFO] - Training Epoch: 2/2, step 20434/23838 completed (loss: 1.498362421989441, acc: 0.6153846383094788)
[2025-02-05 13:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:37][root][INFO] - Training Epoch: 2/2, step 20435/23838 completed (loss: 0.8503649234771729, acc: 0.725806474685669)
[2025-02-05 13:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:37][root][INFO] - Training Epoch: 2/2, step 20436/23838 completed (loss: 1.162324070930481, acc: 0.6913580298423767)
[2025-02-05 13:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:38][root][INFO] - Training Epoch: 2/2, step 20437/23838 completed (loss: 1.0427112579345703, acc: 0.7291666865348816)
[2025-02-05 13:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:38][root][INFO] - Training Epoch: 2/2, step 20438/23838 completed (loss: 1.2912075519561768, acc: 0.6451612710952759)
[2025-02-05 13:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:39][root][INFO] - Training Epoch: 2/2, step 20439/23838 completed (loss: 1.5817246437072754, acc: 0.5287356376647949)
[2025-02-05 13:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:39][root][INFO] - Training Epoch: 2/2, step 20440/23838 completed (loss: 0.8876446485519409, acc: 0.7857142686843872)
[2025-02-05 13:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:39][root][INFO] - Training Epoch: 2/2, step 20441/23838 completed (loss: 1.3537932634353638, acc: 0.5909090638160706)
[2025-02-05 13:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:40][root][INFO] - Training Epoch: 2/2, step 20442/23838 completed (loss: 1.3392598628997803, acc: 0.7017543911933899)
[2025-02-05 13:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:40][root][INFO] - Training Epoch: 2/2, step 20443/23838 completed (loss: 1.3332743644714355, acc: 0.5882353186607361)
[2025-02-05 13:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:40][root][INFO] - Training Epoch: 2/2, step 20444/23838 completed (loss: 1.216886043548584, acc: 0.6363636255264282)
[2025-02-05 13:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:41][root][INFO] - Training Epoch: 2/2, step 20445/23838 completed (loss: 0.9912562966346741, acc: 0.6935483813285828)
[2025-02-05 13:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:41][root][INFO] - Training Epoch: 2/2, step 20446/23838 completed (loss: 0.9160459041595459, acc: 0.767123281955719)
[2025-02-05 13:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:41][root][INFO] - Training Epoch: 2/2, step 20447/23838 completed (loss: 0.9538986682891846, acc: 0.7419354915618896)
[2025-02-05 13:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:42][root][INFO] - Training Epoch: 2/2, step 20448/23838 completed (loss: 1.5986039638519287, acc: 0.5686274766921997)
[2025-02-05 13:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:42][root][INFO] - Training Epoch: 2/2, step 20449/23838 completed (loss: 1.7329678535461426, acc: 0.5384615659713745)
[2025-02-05 13:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:43][root][INFO] - Training Epoch: 2/2, step 20450/23838 completed (loss: 1.2775471210479736, acc: 0.7058823704719543)
[2025-02-05 13:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:43][root][INFO] - Training Epoch: 2/2, step 20451/23838 completed (loss: 1.4434279203414917, acc: 0.5405405163764954)
[2025-02-05 13:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:43][root][INFO] - Training Epoch: 2/2, step 20452/23838 completed (loss: 1.607405185699463, acc: 0.5510203838348389)
[2025-02-05 13:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:44][root][INFO] - Training Epoch: 2/2, step 20453/23838 completed (loss: 1.1726783514022827, acc: 0.6582278609275818)
[2025-02-05 13:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:44][root][INFO] - Training Epoch: 2/2, step 20454/23838 completed (loss: 1.2077745199203491, acc: 0.6507936716079712)
[2025-02-05 13:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:45][root][INFO] - Training Epoch: 2/2, step 20455/23838 completed (loss: 1.2171630859375, acc: 0.699999988079071)
[2025-02-05 13:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:45][root][INFO] - Training Epoch: 2/2, step 20456/23838 completed (loss: 1.6486772298812866, acc: 0.5333333611488342)
[2025-02-05 13:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:45][root][INFO] - Training Epoch: 2/2, step 20457/23838 completed (loss: 1.392637014389038, acc: 0.6000000238418579)
[2025-02-05 13:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:46][root][INFO] - Training Epoch: 2/2, step 20458/23838 completed (loss: 1.3208720684051514, acc: 0.6136363744735718)
[2025-02-05 13:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:46][root][INFO] - Training Epoch: 2/2, step 20459/23838 completed (loss: 1.4401683807373047, acc: 0.523809552192688)
[2025-02-05 13:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:47][root][INFO] - Training Epoch: 2/2, step 20460/23838 completed (loss: 1.1877487897872925, acc: 0.6857143044471741)
[2025-02-05 13:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:47][root][INFO] - Training Epoch: 2/2, step 20461/23838 completed (loss: 1.196443796157837, acc: 0.641791045665741)
[2025-02-05 13:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:48][root][INFO] - Training Epoch: 2/2, step 20462/23838 completed (loss: 1.1693947315216064, acc: 0.644444465637207)
[2025-02-05 13:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:48][root][INFO] - Training Epoch: 2/2, step 20463/23838 completed (loss: 0.9246138334274292, acc: 0.692307710647583)
[2025-02-05 13:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:48][root][INFO] - Training Epoch: 2/2, step 20464/23838 completed (loss: 0.8974124193191528, acc: 0.7234042286872864)
[2025-02-05 13:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:49][root][INFO] - Training Epoch: 2/2, step 20465/23838 completed (loss: 1.2920361757278442, acc: 0.5925925970077515)
[2025-02-05 13:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:49][root][INFO] - Training Epoch: 2/2, step 20466/23838 completed (loss: 1.33041512966156, acc: 0.6323529481887817)
[2025-02-05 13:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:50][root][INFO] - Training Epoch: 2/2, step 20467/23838 completed (loss: 1.0955893993377686, acc: 0.686274528503418)
[2025-02-05 13:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:50][root][INFO] - Training Epoch: 2/2, step 20468/23838 completed (loss: 0.894280195236206, acc: 0.7045454382896423)
[2025-02-05 13:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:50][root][INFO] - Training Epoch: 2/2, step 20469/23838 completed (loss: 2.599069595336914, acc: 0.44999998807907104)
[2025-02-05 13:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:51][root][INFO] - Training Epoch: 2/2, step 20470/23838 completed (loss: 1.3020182847976685, acc: 0.703125)
[2025-02-05 13:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:51][root][INFO] - Training Epoch: 2/2, step 20471/23838 completed (loss: 1.0339831113815308, acc: 0.6739130616188049)
[2025-02-05 13:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:52][root][INFO] - Training Epoch: 2/2, step 20472/23838 completed (loss: 1.2794744968414307, acc: 0.6399999856948853)
[2025-02-05 13:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:52][root][INFO] - Training Epoch: 2/2, step 20473/23838 completed (loss: 1.508249282836914, acc: 0.5588235259056091)
[2025-02-05 13:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:53][root][INFO] - Training Epoch: 2/2, step 20474/23838 completed (loss: 1.2875702381134033, acc: 0.6086956262588501)
[2025-02-05 13:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:53][root][INFO] - Training Epoch: 2/2, step 20475/23838 completed (loss: 1.3242506980895996, acc: 0.5454545617103577)
[2025-02-05 13:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:53][root][INFO] - Training Epoch: 2/2, step 20476/23838 completed (loss: 0.7571208477020264, acc: 0.8222222328186035)
[2025-02-05 13:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:54][root][INFO] - Training Epoch: 2/2, step 20477/23838 completed (loss: 1.0390816926956177, acc: 0.6764705777168274)
[2025-02-05 13:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:54][root][INFO] - Training Epoch: 2/2, step 20478/23838 completed (loss: 1.5029135942459106, acc: 0.5517241358757019)
[2025-02-05 13:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:55][root][INFO] - Training Epoch: 2/2, step 20479/23838 completed (loss: 0.9285272359848022, acc: 0.7702702879905701)
[2025-02-05 13:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:55][root][INFO] - Training Epoch: 2/2, step 20480/23838 completed (loss: 1.2275433540344238, acc: 0.5675675868988037)
[2025-02-05 13:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:56][root][INFO] - Training Epoch: 2/2, step 20481/23838 completed (loss: 0.868912398815155, acc: 0.699999988079071)
[2025-02-05 13:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:56][root][INFO] - Training Epoch: 2/2, step 20482/23838 completed (loss: 0.7322653532028198, acc: 0.8333333134651184)
[2025-02-05 13:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:56][root][INFO] - Training Epoch: 2/2, step 20483/23838 completed (loss: 0.9150698184967041, acc: 0.6734693646430969)
[2025-02-05 13:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:57][root][INFO] - Training Epoch: 2/2, step 20484/23838 completed (loss: 1.3694168329238892, acc: 0.5568181872367859)
[2025-02-05 13:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:57][root][INFO] - Training Epoch: 2/2, step 20485/23838 completed (loss: 1.2246509790420532, acc: 0.739130437374115)
[2025-02-05 13:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:58][root][INFO] - Training Epoch: 2/2, step 20486/23838 completed (loss: 0.9581179022789001, acc: 0.738095223903656)
[2025-02-05 13:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:58][root][INFO] - Training Epoch: 2/2, step 20487/23838 completed (loss: 1.5160753726959229, acc: 0.5913978219032288)
[2025-02-05 13:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:58][root][INFO] - Training Epoch: 2/2, step 20488/23838 completed (loss: 1.3537917137145996, acc: 0.649350643157959)
[2025-02-05 13:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:59][root][INFO] - Training Epoch: 2/2, step 20489/23838 completed (loss: 1.3774259090423584, acc: 0.5882353186607361)
[2025-02-05 13:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:56:59][root][INFO] - Training Epoch: 2/2, step 20490/23838 completed (loss: 1.4375194311141968, acc: 0.6101694703102112)
[2025-02-05 13:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:00][root][INFO] - Training Epoch: 2/2, step 20491/23838 completed (loss: 0.9354220032691956, acc: 0.6800000071525574)
[2025-02-05 13:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:00][root][INFO] - Training Epoch: 2/2, step 20492/23838 completed (loss: 1.46012544631958, acc: 0.4878048896789551)
[2025-02-05 13:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:00][root][INFO] - Training Epoch: 2/2, step 20493/23838 completed (loss: 1.0528801679611206, acc: 0.7179487347602844)
[2025-02-05 13:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:01][root][INFO] - Training Epoch: 2/2, step 20494/23838 completed (loss: 1.3506273031234741, acc: 0.6938775777816772)
[2025-02-05 13:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:01][root][INFO] - Training Epoch: 2/2, step 20495/23838 completed (loss: 1.3250850439071655, acc: 0.6111111044883728)
[2025-02-05 13:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:02][root][INFO] - Training Epoch: 2/2, step 20496/23838 completed (loss: 1.3961354494094849, acc: 0.6399999856948853)
[2025-02-05 13:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:02][root][INFO] - Training Epoch: 2/2, step 20497/23838 completed (loss: 1.281759262084961, acc: 0.6393442749977112)
[2025-02-05 13:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:02][root][INFO] - Training Epoch: 2/2, step 20498/23838 completed (loss: 1.3456752300262451, acc: 0.6435643434524536)
[2025-02-05 13:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:03][root][INFO] - Training Epoch: 2/2, step 20499/23838 completed (loss: 1.4090520143508911, acc: 0.6020408272743225)
[2025-02-05 13:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:03][root][INFO] - Training Epoch: 2/2, step 20500/23838 completed (loss: 1.261304259300232, acc: 0.6335877776145935)
[2025-02-05 13:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:04][root][INFO] - Training Epoch: 2/2, step 20501/23838 completed (loss: 1.1719530820846558, acc: 0.6752136945724487)
[2025-02-05 13:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:04][root][INFO] - Training Epoch: 2/2, step 20502/23838 completed (loss: 1.100468635559082, acc: 0.6527777910232544)
[2025-02-05 13:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:04][root][INFO] - Training Epoch: 2/2, step 20503/23838 completed (loss: 1.2650786638259888, acc: 0.6034482717514038)
[2025-02-05 13:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:05][root][INFO] - Training Epoch: 2/2, step 20504/23838 completed (loss: 1.238135576248169, acc: 0.6343283653259277)
[2025-02-05 13:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:05][root][INFO] - Training Epoch: 2/2, step 20505/23838 completed (loss: 1.2724459171295166, acc: 0.6357616186141968)
[2025-02-05 13:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:05][root][INFO] - Training Epoch: 2/2, step 20506/23838 completed (loss: 1.2257477045059204, acc: 0.6453900933265686)
[2025-02-05 13:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:06][root][INFO] - Training Epoch: 2/2, step 20507/23838 completed (loss: 1.2993358373641968, acc: 0.6086956262588501)
[2025-02-05 13:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:06][root][INFO] - Training Epoch: 2/2, step 20508/23838 completed (loss: 1.297024130821228, acc: 0.5722891688346863)
[2025-02-05 13:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:07][root][INFO] - Training Epoch: 2/2, step 20509/23838 completed (loss: 1.1211469173431396, acc: 0.6585366129875183)
[2025-02-05 13:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:07][root][INFO] - Training Epoch: 2/2, step 20510/23838 completed (loss: 1.286469578742981, acc: 0.6690647602081299)
[2025-02-05 13:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:07][root][INFO] - Training Epoch: 2/2, step 20511/23838 completed (loss: 1.2449405193328857, acc: 0.6612903475761414)
[2025-02-05 13:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:08][root][INFO] - Training Epoch: 2/2, step 20512/23838 completed (loss: 1.145817756652832, acc: 0.6341463327407837)
[2025-02-05 13:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:08][root][INFO] - Training Epoch: 2/2, step 20513/23838 completed (loss: 1.3533778190612793, acc: 0.5898876190185547)
[2025-02-05 13:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:09][root][INFO] - Training Epoch: 2/2, step 20514/23838 completed (loss: 1.3635178804397583, acc: 0.6118420958518982)
[2025-02-05 13:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:09][root][INFO] - Training Epoch: 2/2, step 20515/23838 completed (loss: 1.1313656568527222, acc: 0.6994219422340393)
[2025-02-05 13:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:09][root][INFO] - Training Epoch: 2/2, step 20516/23838 completed (loss: 1.356863260269165, acc: 0.6268656849861145)
[2025-02-05 13:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:10][root][INFO] - Training Epoch: 2/2, step 20517/23838 completed (loss: 1.3352017402648926, acc: 0.5476190447807312)
[2025-02-05 13:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:10][root][INFO] - Training Epoch: 2/2, step 20518/23838 completed (loss: 1.3965901136398315, acc: 0.6060606241226196)
[2025-02-05 13:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:11][root][INFO] - Training Epoch: 2/2, step 20519/23838 completed (loss: 1.1774977445602417, acc: 0.656000018119812)
[2025-02-05 13:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:11][root][INFO] - Training Epoch: 2/2, step 20520/23838 completed (loss: 1.3755009174346924, acc: 0.5684210658073425)
[2025-02-05 13:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:11][root][INFO] - Training Epoch: 2/2, step 20521/23838 completed (loss: 0.8174449801445007, acc: 0.7749999761581421)
[2025-02-05 13:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:12][root][INFO] - Training Epoch: 2/2, step 20522/23838 completed (loss: 0.9941992163658142, acc: 0.7549669146537781)
[2025-02-05 13:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:12][root][INFO] - Training Epoch: 2/2, step 20523/23838 completed (loss: 1.186126470565796, acc: 0.6222222447395325)
[2025-02-05 13:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:12][root][INFO] - Training Epoch: 2/2, step 20524/23838 completed (loss: 1.2585079669952393, acc: 0.6565656661987305)
[2025-02-05 13:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:13][root][INFO] - Training Epoch: 2/2, step 20525/23838 completed (loss: 1.237718939781189, acc: 0.6173912882804871)
[2025-02-05 13:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:13][root][INFO] - Training Epoch: 2/2, step 20526/23838 completed (loss: 0.9821259379386902, acc: 0.6705882549285889)
[2025-02-05 13:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:14][root][INFO] - Training Epoch: 2/2, step 20527/23838 completed (loss: 1.1504833698272705, acc: 0.6603773832321167)
[2025-02-05 13:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:14][root][INFO] - Training Epoch: 2/2, step 20528/23838 completed (loss: 1.5073920488357544, acc: 0.5634920597076416)
[2025-02-05 13:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:14][root][INFO] - Training Epoch: 2/2, step 20529/23838 completed (loss: 1.367478609085083, acc: 0.5892857313156128)
[2025-02-05 13:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:15][root][INFO] - Training Epoch: 2/2, step 20530/23838 completed (loss: 0.9942854642868042, acc: 0.6973684430122375)
[2025-02-05 13:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:15][root][INFO] - Training Epoch: 2/2, step 20531/23838 completed (loss: 1.2321572303771973, acc: 0.658823549747467)
[2025-02-05 13:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:16][root][INFO] - Training Epoch: 2/2, step 20532/23838 completed (loss: 1.5163148641586304, acc: 0.5652173757553101)
[2025-02-05 13:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:16][root][INFO] - Training Epoch: 2/2, step 20533/23838 completed (loss: 1.27601158618927, acc: 0.5757575631141663)
[2025-02-05 13:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:16][root][INFO] - Training Epoch: 2/2, step 20534/23838 completed (loss: 1.1396125555038452, acc: 0.7142857313156128)
[2025-02-05 13:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:17][root][INFO] - Training Epoch: 2/2, step 20535/23838 completed (loss: 1.215065598487854, acc: 0.61654132604599)
[2025-02-05 13:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:17][root][INFO] - Training Epoch: 2/2, step 20536/23838 completed (loss: 1.2232215404510498, acc: 0.6578947305679321)
[2025-02-05 13:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:18][root][INFO] - Training Epoch: 2/2, step 20537/23838 completed (loss: 1.1127054691314697, acc: 0.6842105388641357)
[2025-02-05 13:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:18][root][INFO] - Training Epoch: 2/2, step 20538/23838 completed (loss: 1.0988938808441162, acc: 0.704081654548645)
[2025-02-05 13:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:19][root][INFO] - Training Epoch: 2/2, step 20539/23838 completed (loss: 1.2458595037460327, acc: 0.6769230961799622)
[2025-02-05 13:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:19][root][INFO] - Training Epoch: 2/2, step 20540/23838 completed (loss: 1.3840911388397217, acc: 0.6363636255264282)
[2025-02-05 13:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:19][root][INFO] - Training Epoch: 2/2, step 20541/23838 completed (loss: 1.4705809354782104, acc: 0.5507246255874634)
[2025-02-05 13:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:20][root][INFO] - Training Epoch: 2/2, step 20542/23838 completed (loss: 1.222340703010559, acc: 0.65625)
[2025-02-05 13:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:20][root][INFO] - Training Epoch: 2/2, step 20543/23838 completed (loss: 1.429154396057129, acc: 0.6122449040412903)
[2025-02-05 13:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:20][root][INFO] - Training Epoch: 2/2, step 20544/23838 completed (loss: 1.1823368072509766, acc: 0.6065573692321777)
[2025-02-05 13:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:21][root][INFO] - Training Epoch: 2/2, step 20545/23838 completed (loss: 1.1674346923828125, acc: 0.6293103694915771)
[2025-02-05 13:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:21][root][INFO] - Training Epoch: 2/2, step 20546/23838 completed (loss: 0.9920599460601807, acc: 0.7115384340286255)
[2025-02-05 13:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:22][root][INFO] - Training Epoch: 2/2, step 20547/23838 completed (loss: 1.128873586654663, acc: 0.681034505367279)
[2025-02-05 13:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:22][root][INFO] - Training Epoch: 2/2, step 20548/23838 completed (loss: 1.1164172887802124, acc: 0.6868686676025391)
[2025-02-05 13:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:22][root][INFO] - Training Epoch: 2/2, step 20549/23838 completed (loss: 1.381565809249878, acc: 0.6495726704597473)
[2025-02-05 13:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:23][root][INFO] - Training Epoch: 2/2, step 20550/23838 completed (loss: 1.201590895652771, acc: 0.644859790802002)
[2025-02-05 13:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:23][root][INFO] - Training Epoch: 2/2, step 20551/23838 completed (loss: 1.3612990379333496, acc: 0.5833333134651184)
[2025-02-05 13:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:24][root][INFO] - Training Epoch: 2/2, step 20552/23838 completed (loss: 1.4394241571426392, acc: 0.5948718190193176)
[2025-02-05 13:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:24][root][INFO] - Training Epoch: 2/2, step 20553/23838 completed (loss: 1.3313087224960327, acc: 0.582608699798584)
[2025-02-05 13:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:24][root][INFO] - Training Epoch: 2/2, step 20554/23838 completed (loss: 1.5063576698303223, acc: 0.6184210777282715)
[2025-02-05 13:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:25][root][INFO] - Training Epoch: 2/2, step 20555/23838 completed (loss: 1.5244297981262207, acc: 0.5813953280448914)
[2025-02-05 13:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:25][root][INFO] - Training Epoch: 2/2, step 20556/23838 completed (loss: 1.1146605014801025, acc: 0.6634615659713745)
[2025-02-05 13:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:25][root][INFO] - Training Epoch: 2/2, step 20557/23838 completed (loss: 1.2594627141952515, acc: 0.6168224215507507)
[2025-02-05 13:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:26][root][INFO] - Training Epoch: 2/2, step 20558/23838 completed (loss: 1.1267797946929932, acc: 0.6721311211585999)
[2025-02-05 13:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:26][root][INFO] - Training Epoch: 2/2, step 20559/23838 completed (loss: 1.22829270362854, acc: 0.6171875)
[2025-02-05 13:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:26][root][INFO] - Training Epoch: 2/2, step 20560/23838 completed (loss: 1.0886999368667603, acc: 0.6666666865348816)
[2025-02-05 13:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:27][root][INFO] - Training Epoch: 2/2, step 20561/23838 completed (loss: 1.3448421955108643, acc: 0.5849056839942932)
[2025-02-05 13:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:27][root][INFO] - Training Epoch: 2/2, step 20562/23838 completed (loss: 1.219678282737732, acc: 0.6095238327980042)
[2025-02-05 13:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:27][root][INFO] - Training Epoch: 2/2, step 20563/23838 completed (loss: 1.1870955228805542, acc: 0.6354166865348816)
[2025-02-05 13:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:28][root][INFO] - Training Epoch: 2/2, step 20564/23838 completed (loss: 1.114923119544983, acc: 0.6415094137191772)
[2025-02-05 13:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:28][root][INFO] - Training Epoch: 2/2, step 20565/23838 completed (loss: 1.3151452541351318, acc: 0.5833333134651184)
[2025-02-05 13:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:29][root][INFO] - Training Epoch: 2/2, step 20566/23838 completed (loss: 1.0427387952804565, acc: 0.7083333134651184)
[2025-02-05 13:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:29][root][INFO] - Training Epoch: 2/2, step 20567/23838 completed (loss: 1.2441121339797974, acc: 0.6492537260055542)
[2025-02-05 13:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:29][root][INFO] - Training Epoch: 2/2, step 20568/23838 completed (loss: 1.3587725162506104, acc: 0.6153846383094788)
[2025-02-05 13:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:30][root][INFO] - Training Epoch: 2/2, step 20569/23838 completed (loss: 1.0481054782867432, acc: 0.6836734414100647)
[2025-02-05 13:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:30][root][INFO] - Training Epoch: 2/2, step 20570/23838 completed (loss: 1.194393277168274, acc: 0.6641221642494202)
[2025-02-05 13:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:31][root][INFO] - Training Epoch: 2/2, step 20571/23838 completed (loss: 0.9196668267250061, acc: 0.75)
[2025-02-05 13:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:31][root][INFO] - Training Epoch: 2/2, step 20572/23838 completed (loss: 1.375238060951233, acc: 0.6000000238418579)
[2025-02-05 13:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:31][root][INFO] - Training Epoch: 2/2, step 20573/23838 completed (loss: 1.0815051794052124, acc: 0.6551724076271057)
[2025-02-05 13:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:32][root][INFO] - Training Epoch: 2/2, step 20574/23838 completed (loss: 0.7700104713439941, acc: 0.7971014380455017)
[2025-02-05 13:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:32][root][INFO] - Training Epoch: 2/2, step 20575/23838 completed (loss: 1.0223196744918823, acc: 0.6761904954910278)
[2025-02-05 13:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:32][root][INFO] - Training Epoch: 2/2, step 20576/23838 completed (loss: 1.2593659162521362, acc: 0.6202531456947327)
[2025-02-05 13:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:33][root][INFO] - Training Epoch: 2/2, step 20577/23838 completed (loss: 1.3029192686080933, acc: 0.6597937941551208)
[2025-02-05 13:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:33][root][INFO] - Training Epoch: 2/2, step 20578/23838 completed (loss: 1.2680017948150635, acc: 0.6309523582458496)
[2025-02-05 13:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:34][root][INFO] - Training Epoch: 2/2, step 20579/23838 completed (loss: 1.648175597190857, acc: 0.5583333373069763)
[2025-02-05 13:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:34][root][INFO] - Training Epoch: 2/2, step 20580/23838 completed (loss: 1.5142923593521118, acc: 0.6172839403152466)
[2025-02-05 13:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:34][root][INFO] - Training Epoch: 2/2, step 20581/23838 completed (loss: 1.1017652750015259, acc: 0.7252747416496277)
[2025-02-05 13:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:35][root][INFO] - Training Epoch: 2/2, step 20582/23838 completed (loss: 1.202672004699707, acc: 0.6741573214530945)
[2025-02-05 13:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:35][root][INFO] - Training Epoch: 2/2, step 20583/23838 completed (loss: 1.3291008472442627, acc: 0.6451612710952759)
[2025-02-05 13:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:36][root][INFO] - Training Epoch: 2/2, step 20584/23838 completed (loss: 1.0443828105926514, acc: 0.70652174949646)
[2025-02-05 13:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:36][root][INFO] - Training Epoch: 2/2, step 20585/23838 completed (loss: 1.2211884260177612, acc: 0.6822429895401001)
[2025-02-05 13:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:36][root][INFO] - Training Epoch: 2/2, step 20586/23838 completed (loss: 1.1866673231124878, acc: 0.6382978558540344)
[2025-02-05 13:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:37][root][INFO] - Training Epoch: 2/2, step 20587/23838 completed (loss: 1.4930835962295532, acc: 0.54666668176651)
[2025-02-05 13:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:37][root][INFO] - Training Epoch: 2/2, step 20588/23838 completed (loss: 1.1401140689849854, acc: 0.6571428775787354)
[2025-02-05 13:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:37][root][INFO] - Training Epoch: 2/2, step 20589/23838 completed (loss: 1.0257302522659302, acc: 0.6933333277702332)
[2025-02-05 13:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:38][root][INFO] - Training Epoch: 2/2, step 20590/23838 completed (loss: 1.2826827764511108, acc: 0.634482741355896)
[2025-02-05 13:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:38][root][INFO] - Training Epoch: 2/2, step 20591/23838 completed (loss: 1.0645617246627808, acc: 0.6896551847457886)
[2025-02-05 13:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:39][root][INFO] - Training Epoch: 2/2, step 20592/23838 completed (loss: 1.089971899986267, acc: 0.6768292784690857)
[2025-02-05 13:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:39][root][INFO] - Training Epoch: 2/2, step 20593/23838 completed (loss: 0.9758377075195312, acc: 0.7128713130950928)
[2025-02-05 13:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:39][root][INFO] - Training Epoch: 2/2, step 20594/23838 completed (loss: 1.223073959350586, acc: 0.640350878238678)
[2025-02-05 13:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:40][root][INFO] - Training Epoch: 2/2, step 20595/23838 completed (loss: 1.1747573614120483, acc: 0.6344085931777954)
[2025-02-05 13:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:40][root][INFO] - Training Epoch: 2/2, step 20596/23838 completed (loss: 0.9232587814331055, acc: 0.719298243522644)
[2025-02-05 13:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:40][root][INFO] - Training Epoch: 2/2, step 20597/23838 completed (loss: 1.322988510131836, acc: 0.6020408272743225)
[2025-02-05 13:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:41][root][INFO] - Training Epoch: 2/2, step 20598/23838 completed (loss: 1.1943575143814087, acc: 0.6697247624397278)
[2025-02-05 13:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:41][root][INFO] - Training Epoch: 2/2, step 20599/23838 completed (loss: 1.0225175619125366, acc: 0.671999990940094)
[2025-02-05 13:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:42][root][INFO] - Training Epoch: 2/2, step 20600/23838 completed (loss: 1.3268241882324219, acc: 0.6106870174407959)
[2025-02-05 13:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:42][root][INFO] - Training Epoch: 2/2, step 20601/23838 completed (loss: 1.2911635637283325, acc: 0.6263736486434937)
[2025-02-05 13:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:43][root][INFO] - Training Epoch: 2/2, step 20602/23838 completed (loss: 1.21604323387146, acc: 0.6637167930603027)
[2025-02-05 13:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:43][root][INFO] - Training Epoch: 2/2, step 20603/23838 completed (loss: 1.4732184410095215, acc: 0.5725806355476379)
[2025-02-05 13:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:43][root][INFO] - Training Epoch: 2/2, step 20604/23838 completed (loss: 1.3000162839889526, acc: 0.6172839403152466)
[2025-02-05 13:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:44][root][INFO] - Training Epoch: 2/2, step 20605/23838 completed (loss: 1.3294049501419067, acc: 0.6339285969734192)
[2025-02-05 13:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:44][root][INFO] - Training Epoch: 2/2, step 20606/23838 completed (loss: 0.9538507461547852, acc: 0.7575757503509521)
[2025-02-05 13:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:45][root][INFO] - Training Epoch: 2/2, step 20607/23838 completed (loss: 1.3840937614440918, acc: 0.6083333492279053)
[2025-02-05 13:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:45][root][INFO] - Training Epoch: 2/2, step 20608/23838 completed (loss: 1.4008835554122925, acc: 0.634782612323761)
[2025-02-05 13:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:45][root][INFO] - Training Epoch: 2/2, step 20609/23838 completed (loss: 1.5578176975250244, acc: 0.6025640964508057)
[2025-02-05 13:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:46][root][INFO] - Training Epoch: 2/2, step 20610/23838 completed (loss: 1.3280234336853027, acc: 0.640625)
[2025-02-05 13:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:46][root][INFO] - Training Epoch: 2/2, step 20611/23838 completed (loss: 0.8865690231323242, acc: 0.7254902124404907)
[2025-02-05 13:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:46][root][INFO] - Training Epoch: 2/2, step 20612/23838 completed (loss: 1.6631138324737549, acc: 0.5106382966041565)
[2025-02-05 13:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:47][root][INFO] - Training Epoch: 2/2, step 20613/23838 completed (loss: 1.3283017873764038, acc: 0.6071428656578064)
[2025-02-05 13:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:47][root][INFO] - Training Epoch: 2/2, step 20614/23838 completed (loss: 1.4734220504760742, acc: 0.568965494632721)
[2025-02-05 13:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:48][root][INFO] - Training Epoch: 2/2, step 20615/23838 completed (loss: 1.5867177248001099, acc: 0.6086956262588501)
[2025-02-05 13:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:48][root][INFO] - Training Epoch: 2/2, step 20616/23838 completed (loss: 1.1838507652282715, acc: 0.6515151262283325)
[2025-02-05 13:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:49][root][INFO] - Training Epoch: 2/2, step 20617/23838 completed (loss: 1.5369880199432373, acc: 0.5245901346206665)
[2025-02-05 13:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:49][root][INFO] - Training Epoch: 2/2, step 20618/23838 completed (loss: 1.5704716444015503, acc: 0.560606062412262)
[2025-02-05 13:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:49][root][INFO] - Training Epoch: 2/2, step 20619/23838 completed (loss: 1.2522114515304565, acc: 0.6585366129875183)
[2025-02-05 13:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:50][root][INFO] - Training Epoch: 2/2, step 20620/23838 completed (loss: 1.4907808303833008, acc: 0.5925925970077515)
[2025-02-05 13:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:50][root][INFO] - Training Epoch: 2/2, step 20621/23838 completed (loss: 1.5416240692138672, acc: 0.5396825671195984)
[2025-02-05 13:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:51][root][INFO] - Training Epoch: 2/2, step 20622/23838 completed (loss: 1.3103333711624146, acc: 0.5799999833106995)
[2025-02-05 13:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:51][root][INFO] - Training Epoch: 2/2, step 20623/23838 completed (loss: 1.338578701019287, acc: 0.5675675868988037)
[2025-02-05 13:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:52][root][INFO] - Training Epoch: 2/2, step 20624/23838 completed (loss: 1.0302228927612305, acc: 0.7142857313156128)
[2025-02-05 13:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:52][root][INFO] - Training Epoch: 2/2, step 20625/23838 completed (loss: 1.2358901500701904, acc: 0.6140350699424744)
[2025-02-05 13:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:52][root][INFO] - Training Epoch: 2/2, step 20626/23838 completed (loss: 1.380659580230713, acc: 0.6162790656089783)
[2025-02-05 13:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:53][root][INFO] - Training Epoch: 2/2, step 20627/23838 completed (loss: 1.1018441915512085, acc: 0.6458333134651184)
[2025-02-05 13:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:53][root][INFO] - Training Epoch: 2/2, step 20628/23838 completed (loss: 1.3418387174606323, acc: 0.6842105388641357)
[2025-02-05 13:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:54][root][INFO] - Training Epoch: 2/2, step 20629/23838 completed (loss: 1.1442960500717163, acc: 0.7349397540092468)
[2025-02-05 13:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:54][root][INFO] - Training Epoch: 2/2, step 20630/23838 completed (loss: 1.1835271120071411, acc: 0.6774193644523621)
[2025-02-05 13:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:54][root][INFO] - Training Epoch: 2/2, step 20631/23838 completed (loss: 1.4207239151000977, acc: 0.5918367505073547)
[2025-02-05 13:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:55][root][INFO] - Training Epoch: 2/2, step 20632/23838 completed (loss: 1.6831282377243042, acc: 0.5094339847564697)
[2025-02-05 13:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:55][root][INFO] - Training Epoch: 2/2, step 20633/23838 completed (loss: 1.322334885597229, acc: 0.6707317233085632)
[2025-02-05 13:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:56][root][INFO] - Training Epoch: 2/2, step 20634/23838 completed (loss: 1.5278748273849487, acc: 0.5128205418586731)
[2025-02-05 13:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:56][root][INFO] - Training Epoch: 2/2, step 20635/23838 completed (loss: 1.0923534631729126, acc: 0.6888889074325562)
[2025-02-05 13:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:56][root][INFO] - Training Epoch: 2/2, step 20636/23838 completed (loss: 1.2118442058563232, acc: 0.654321014881134)
[2025-02-05 13:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:57][root][INFO] - Training Epoch: 2/2, step 20637/23838 completed (loss: 1.2256944179534912, acc: 0.6814159154891968)
[2025-02-05 13:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:57][root][INFO] - Training Epoch: 2/2, step 20638/23838 completed (loss: 1.3878264427185059, acc: 0.5789473652839661)
[2025-02-05 13:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:58][root][INFO] - Training Epoch: 2/2, step 20639/23838 completed (loss: 1.127245545387268, acc: 0.6060606241226196)
[2025-02-05 13:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:58][root][INFO] - Training Epoch: 2/2, step 20640/23838 completed (loss: 0.8565791249275208, acc: 0.7903226017951965)
[2025-02-05 13:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:58][root][INFO] - Training Epoch: 2/2, step 20641/23838 completed (loss: 1.1655831336975098, acc: 0.654321014881134)
[2025-02-05 13:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:59][root][INFO] - Training Epoch: 2/2, step 20642/23838 completed (loss: 1.642628788948059, acc: 0.5666666626930237)
[2025-02-05 13:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:57:59][root][INFO] - Training Epoch: 2/2, step 20643/23838 completed (loss: 1.202520728111267, acc: 0.6027397513389587)
[2025-02-05 13:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:00][root][INFO] - Training Epoch: 2/2, step 20644/23838 completed (loss: 0.9377560019493103, acc: 0.6976743936538696)
[2025-02-05 13:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:00][root][INFO] - Training Epoch: 2/2, step 20645/23838 completed (loss: 0.8543221950531006, acc: 0.8235294222831726)
[2025-02-05 13:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:01][root][INFO] - Training Epoch: 2/2, step 20646/23838 completed (loss: 1.161676287651062, acc: 0.6935483813285828)
[2025-02-05 13:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:01][root][INFO] - Training Epoch: 2/2, step 20647/23838 completed (loss: 1.5442293882369995, acc: 0.5645161271095276)
[2025-02-05 13:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:02][root][INFO] - Training Epoch: 2/2, step 20648/23838 completed (loss: 1.2798291444778442, acc: 0.6666666865348816)
[2025-02-05 13:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:02][root][INFO] - Training Epoch: 2/2, step 20649/23838 completed (loss: 1.2244571447372437, acc: 0.6346153616905212)
[2025-02-05 13:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:03][root][INFO] - Training Epoch: 2/2, step 20650/23838 completed (loss: 1.4451746940612793, acc: 0.5820895433425903)
[2025-02-05 13:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:03][root][INFO] - Training Epoch: 2/2, step 20651/23838 completed (loss: 1.422226905822754, acc: 0.5494505763053894)
[2025-02-05 13:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:03][root][INFO] - Training Epoch: 2/2, step 20652/23838 completed (loss: 1.6684129238128662, acc: 0.5373134613037109)
[2025-02-05 13:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:04][root][INFO] - Training Epoch: 2/2, step 20653/23838 completed (loss: 1.5726479291915894, acc: 0.5978260636329651)
[2025-02-05 13:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:04][root][INFO] - Training Epoch: 2/2, step 20654/23838 completed (loss: 1.4003114700317383, acc: 0.637499988079071)
[2025-02-05 13:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:05][root][INFO] - Training Epoch: 2/2, step 20655/23838 completed (loss: 1.5359073877334595, acc: 0.5909090638160706)
[2025-02-05 13:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:05][root][INFO] - Training Epoch: 2/2, step 20656/23838 completed (loss: 1.228025197982788, acc: 0.6666666865348816)
[2025-02-05 13:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:06][root][INFO] - Training Epoch: 2/2, step 20657/23838 completed (loss: 1.7923932075500488, acc: 0.48275861144065857)
[2025-02-05 13:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:06][root][INFO] - Training Epoch: 2/2, step 20658/23838 completed (loss: 1.2388246059417725, acc: 0.6818181872367859)
[2025-02-05 13:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:06][root][INFO] - Training Epoch: 2/2, step 20659/23838 completed (loss: 1.7530207633972168, acc: 0.5384615659713745)
[2025-02-05 13:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:07][root][INFO] - Training Epoch: 2/2, step 20660/23838 completed (loss: 1.3096097707748413, acc: 0.6206896305084229)
[2025-02-05 13:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:07][root][INFO] - Training Epoch: 2/2, step 20661/23838 completed (loss: 1.7013612985610962, acc: 0.5)
[2025-02-05 13:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:08][root][INFO] - Training Epoch: 2/2, step 20662/23838 completed (loss: 1.2115503549575806, acc: 0.6666666865348816)
[2025-02-05 13:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:08][root][INFO] - Training Epoch: 2/2, step 20663/23838 completed (loss: 1.4858547449111938, acc: 0.5555555820465088)
[2025-02-05 13:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:08][root][INFO] - Training Epoch: 2/2, step 20664/23838 completed (loss: 1.287266492843628, acc: 0.6352941393852234)
[2025-02-05 13:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:09][root][INFO] - Training Epoch: 2/2, step 20665/23838 completed (loss: 1.4349621534347534, acc: 0.5853658318519592)
[2025-02-05 13:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:09][root][INFO] - Training Epoch: 2/2, step 20666/23838 completed (loss: 1.5108274221420288, acc: 0.5365853905677795)
[2025-02-05 13:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:10][root][INFO] - Training Epoch: 2/2, step 20667/23838 completed (loss: 1.520438551902771, acc: 0.5744680762290955)
[2025-02-05 13:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:10][root][INFO] - Training Epoch: 2/2, step 20668/23838 completed (loss: 1.3968876600265503, acc: 0.5967742204666138)
[2025-02-05 13:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:10][root][INFO] - Training Epoch: 2/2, step 20669/23838 completed (loss: 1.0567930936813354, acc: 0.7400000095367432)
[2025-02-05 13:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:11][root][INFO] - Training Epoch: 2/2, step 20670/23838 completed (loss: 1.0786882638931274, acc: 0.6891891956329346)
[2025-02-05 13:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:11][root][INFO] - Training Epoch: 2/2, step 20671/23838 completed (loss: 1.3875535726547241, acc: 0.5573770403862)
[2025-02-05 13:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:12][root][INFO] - Training Epoch: 2/2, step 20672/23838 completed (loss: 1.462174415588379, acc: 0.5942028760910034)
[2025-02-05 13:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:12][root][INFO] - Training Epoch: 2/2, step 20673/23838 completed (loss: 1.5152150392532349, acc: 0.5955055952072144)
[2025-02-05 13:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:13][root][INFO] - Training Epoch: 2/2, step 20674/23838 completed (loss: 1.7571661472320557, acc: 0.5360000133514404)
[2025-02-05 13:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:13][root][INFO] - Training Epoch: 2/2, step 20675/23838 completed (loss: 1.1826773881912231, acc: 0.6453900933265686)
[2025-02-05 13:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:14][root][INFO] - Training Epoch: 2/2, step 20676/23838 completed (loss: 1.2755032777786255, acc: 0.6000000238418579)
[2025-02-05 13:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:14][root][INFO] - Training Epoch: 2/2, step 20677/23838 completed (loss: 1.4799267053604126, acc: 0.6126126050949097)
[2025-02-05 13:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:14][root][INFO] - Training Epoch: 2/2, step 20678/23838 completed (loss: 1.059030294418335, acc: 0.6754385828971863)
[2025-02-05 13:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:15][root][INFO] - Training Epoch: 2/2, step 20679/23838 completed (loss: 1.2825117111206055, acc: 0.6428571343421936)
[2025-02-05 13:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:15][root][INFO] - Training Epoch: 2/2, step 20680/23838 completed (loss: 1.4827481508255005, acc: 0.6086956262588501)
[2025-02-05 13:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:15][root][INFO] - Training Epoch: 2/2, step 20681/23838 completed (loss: 1.2020878791809082, acc: 0.7160493731498718)
[2025-02-05 13:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:16][root][INFO] - Training Epoch: 2/2, step 20682/23838 completed (loss: 1.2221271991729736, acc: 0.6352941393852234)
[2025-02-05 13:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:16][root][INFO] - Training Epoch: 2/2, step 20683/23838 completed (loss: 1.236202359199524, acc: 0.6785714030265808)
[2025-02-05 13:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:17][root][INFO] - Training Epoch: 2/2, step 20684/23838 completed (loss: 1.6565186977386475, acc: 0.4954128563404083)
[2025-02-05 13:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:17][root][INFO] - Training Epoch: 2/2, step 20685/23838 completed (loss: 1.0934041738510132, acc: 0.6959999799728394)
[2025-02-05 13:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:17][root][INFO] - Training Epoch: 2/2, step 20686/23838 completed (loss: 1.1096299886703491, acc: 0.6774193644523621)
[2025-02-05 13:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:18][root][INFO] - Training Epoch: 2/2, step 20687/23838 completed (loss: 1.395451545715332, acc: 0.5731707215309143)
[2025-02-05 13:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:18][root][INFO] - Training Epoch: 2/2, step 20688/23838 completed (loss: 1.4227607250213623, acc: 0.6399999856948853)
[2025-02-05 13:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:19][root][INFO] - Training Epoch: 2/2, step 20689/23838 completed (loss: 1.3370757102966309, acc: 0.6435643434524536)
[2025-02-05 13:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:19][root][INFO] - Training Epoch: 2/2, step 20690/23838 completed (loss: 1.2416033744812012, acc: 0.6106870174407959)
[2025-02-05 13:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:20][root][INFO] - Training Epoch: 2/2, step 20691/23838 completed (loss: 1.2332030534744263, acc: 0.6242038011550903)
[2025-02-05 13:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:20][root][INFO] - Training Epoch: 2/2, step 20692/23838 completed (loss: 1.1867929697036743, acc: 0.6380952596664429)
[2025-02-05 13:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:20][root][INFO] - Training Epoch: 2/2, step 20693/23838 completed (loss: 1.4159715175628662, acc: 0.6079999804496765)
[2025-02-05 13:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:21][root][INFO] - Training Epoch: 2/2, step 20694/23838 completed (loss: 1.202210545539856, acc: 0.6861313581466675)
[2025-02-05 13:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:21][root][INFO] - Training Epoch: 2/2, step 20695/23838 completed (loss: 1.4298536777496338, acc: 0.5769230723381042)
[2025-02-05 13:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:22][root][INFO] - Training Epoch: 2/2, step 20696/23838 completed (loss: 1.40779709815979, acc: 0.5846994519233704)
[2025-02-05 13:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:22][root][INFO] - Training Epoch: 2/2, step 20697/23838 completed (loss: 1.4228166341781616, acc: 0.6184210777282715)
[2025-02-05 13:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:22][root][INFO] - Training Epoch: 2/2, step 20698/23838 completed (loss: 1.3310022354125977, acc: 0.5882353186607361)
[2025-02-05 13:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:23][root][INFO] - Training Epoch: 2/2, step 20699/23838 completed (loss: 1.3146542310714722, acc: 0.6594203114509583)
[2025-02-05 13:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:23][root][INFO] - Training Epoch: 2/2, step 20700/23838 completed (loss: 1.5791079998016357, acc: 0.5486725568771362)
[2025-02-05 13:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:24][root][INFO] - Training Epoch: 2/2, step 20701/23838 completed (loss: 1.3841099739074707, acc: 0.6145833134651184)
[2025-02-05 13:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:24][root][INFO] - Training Epoch: 2/2, step 20702/23838 completed (loss: 1.0604406595230103, acc: 0.7364341020584106)
[2025-02-05 13:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:24][root][INFO] - Training Epoch: 2/2, step 20703/23838 completed (loss: 1.4812641143798828, acc: 0.5744680762290955)
[2025-02-05 13:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:25][root][INFO] - Training Epoch: 2/2, step 20704/23838 completed (loss: 1.4900009632110596, acc: 0.5487805008888245)
[2025-02-05 13:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:25][root][INFO] - Training Epoch: 2/2, step 20705/23838 completed (loss: 1.2441390752792358, acc: 0.6694915294647217)
[2025-02-05 13:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:26][root][INFO] - Training Epoch: 2/2, step 20706/23838 completed (loss: 1.1123753786087036, acc: 0.7121211886405945)
[2025-02-05 13:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:26][root][INFO] - Training Epoch: 2/2, step 20707/23838 completed (loss: 1.2380880117416382, acc: 0.6470588445663452)
[2025-02-05 13:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:26][root][INFO] - Training Epoch: 2/2, step 20708/23838 completed (loss: 1.0766782760620117, acc: 0.7032967209815979)
[2025-02-05 13:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:27][root][INFO] - Training Epoch: 2/2, step 20709/23838 completed (loss: 1.2027660608291626, acc: 0.7076923251152039)
[2025-02-05 13:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:27][root][INFO] - Training Epoch: 2/2, step 20710/23838 completed (loss: 1.30880606174469, acc: 0.6744186282157898)
[2025-02-05 13:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:28][root][INFO] - Training Epoch: 2/2, step 20711/23838 completed (loss: 1.248996615409851, acc: 0.5732483863830566)
[2025-02-05 13:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:28][root][INFO] - Training Epoch: 2/2, step 20712/23838 completed (loss: 1.0760037899017334, acc: 0.6785714030265808)
[2025-02-05 13:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:29][root][INFO] - Training Epoch: 2/2, step 20713/23838 completed (loss: 1.386654019355774, acc: 0.6222222447395325)
[2025-02-05 13:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:29][root][INFO] - Training Epoch: 2/2, step 20714/23838 completed (loss: 1.5003793239593506, acc: 0.5873016119003296)
[2025-02-05 13:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:29][root][INFO] - Training Epoch: 2/2, step 20715/23838 completed (loss: 1.6459053754806519, acc: 0.49152541160583496)
[2025-02-05 13:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:30][root][INFO] - Training Epoch: 2/2, step 20716/23838 completed (loss: 0.8401855230331421, acc: 0.7627118825912476)
[2025-02-05 13:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:30][root][INFO] - Training Epoch: 2/2, step 20717/23838 completed (loss: 1.2379854917526245, acc: 0.6746987700462341)
[2025-02-05 13:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:31][root][INFO] - Training Epoch: 2/2, step 20718/23838 completed (loss: 1.3112196922302246, acc: 0.6173912882804871)
[2025-02-05 13:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:31][root][INFO] - Training Epoch: 2/2, step 20719/23838 completed (loss: 1.1538389921188354, acc: 0.7017543911933899)
[2025-02-05 13:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:32][root][INFO] - Training Epoch: 2/2, step 20720/23838 completed (loss: 1.4014031887054443, acc: 0.6477987170219421)
[2025-02-05 13:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:32][root][INFO] - Training Epoch: 2/2, step 20721/23838 completed (loss: 0.9546986222267151, acc: 0.7555555701255798)
[2025-02-05 13:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:32][root][INFO] - Training Epoch: 2/2, step 20722/23838 completed (loss: 1.1519819498062134, acc: 0.6525423526763916)
[2025-02-05 13:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:33][root][INFO] - Training Epoch: 2/2, step 20723/23838 completed (loss: 0.8548212051391602, acc: 0.7631579041481018)
[2025-02-05 13:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:33][root][INFO] - Training Epoch: 2/2, step 20724/23838 completed (loss: 0.9641106724739075, acc: 0.695652186870575)
[2025-02-05 13:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:33][root][INFO] - Training Epoch: 2/2, step 20725/23838 completed (loss: 1.222523808479309, acc: 0.6271186470985413)
[2025-02-05 13:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:34][root][INFO] - Training Epoch: 2/2, step 20726/23838 completed (loss: 0.7199456691741943, acc: 0.8266666531562805)
[2025-02-05 13:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:34][root][INFO] - Training Epoch: 2/2, step 20727/23838 completed (loss: 0.9723212122917175, acc: 0.6875)
[2025-02-05 13:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:35][root][INFO] - Training Epoch: 2/2, step 20728/23838 completed (loss: 0.9954476952552795, acc: 0.6451612710952759)
[2025-02-05 13:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:35][root][INFO] - Training Epoch: 2/2, step 20729/23838 completed (loss: 1.055894136428833, acc: 0.650602400302887)
[2025-02-05 13:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:35][root][INFO] - Training Epoch: 2/2, step 20730/23838 completed (loss: 1.195592999458313, acc: 0.6380952596664429)
[2025-02-05 13:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:36][root][INFO] - Training Epoch: 2/2, step 20731/23838 completed (loss: 1.1123616695404053, acc: 0.6333333253860474)
[2025-02-05 13:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:36][root][INFO] - Training Epoch: 2/2, step 20732/23838 completed (loss: 0.8029329180717468, acc: 0.7659574747085571)
[2025-02-05 13:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:37][root][INFO] - Training Epoch: 2/2, step 20733/23838 completed (loss: 1.0485492944717407, acc: 0.6823529601097107)
[2025-02-05 13:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:37][root][INFO] - Training Epoch: 2/2, step 20734/23838 completed (loss: 0.841511607170105, acc: 0.7058823704719543)
[2025-02-05 13:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:37][root][INFO] - Training Epoch: 2/2, step 20735/23838 completed (loss: 0.8732350468635559, acc: 0.7295597195625305)
[2025-02-05 13:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:38][root][INFO] - Training Epoch: 2/2, step 20736/23838 completed (loss: 1.0345127582550049, acc: 0.7014925479888916)
[2025-02-05 13:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:38][root][INFO] - Training Epoch: 2/2, step 20737/23838 completed (loss: 1.3113032579421997, acc: 0.6147540807723999)
[2025-02-05 13:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:39][root][INFO] - Training Epoch: 2/2, step 20738/23838 completed (loss: 1.2276335954666138, acc: 0.6402877569198608)
[2025-02-05 13:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:39][root][INFO] - Training Epoch: 2/2, step 20739/23838 completed (loss: 1.3465150594711304, acc: 0.5894736647605896)
[2025-02-05 13:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:39][root][INFO] - Training Epoch: 2/2, step 20740/23838 completed (loss: 1.037557601928711, acc: 0.7126436829566956)
[2025-02-05 13:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:40][root][INFO] - Training Epoch: 2/2, step 20741/23838 completed (loss: 1.277811050415039, acc: 0.6233766078948975)
[2025-02-05 13:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:40][root][INFO] - Training Epoch: 2/2, step 20742/23838 completed (loss: 1.3066457509994507, acc: 0.602150559425354)
[2025-02-05 13:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:40][root][INFO] - Training Epoch: 2/2, step 20743/23838 completed (loss: 1.2998523712158203, acc: 0.604651153087616)
[2025-02-05 13:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:41][root][INFO] - Training Epoch: 2/2, step 20744/23838 completed (loss: 1.420029878616333, acc: 0.6126126050949097)
[2025-02-05 13:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:41][root][INFO] - Training Epoch: 2/2, step 20745/23838 completed (loss: 1.3278592824935913, acc: 0.5877193212509155)
[2025-02-05 13:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:42][root][INFO] - Training Epoch: 2/2, step 20746/23838 completed (loss: 1.2060585021972656, acc: 0.643478274345398)
[2025-02-05 13:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:42][root][INFO] - Training Epoch: 2/2, step 20747/23838 completed (loss: 1.246809720993042, acc: 0.6206896305084229)
[2025-02-05 13:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:43][root][INFO] - Training Epoch: 2/2, step 20748/23838 completed (loss: 1.5173407793045044, acc: 0.6019417643547058)
[2025-02-05 13:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:43][root][INFO] - Training Epoch: 2/2, step 20749/23838 completed (loss: 1.120614767074585, acc: 0.6953125)
[2025-02-05 13:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:43][root][INFO] - Training Epoch: 2/2, step 20750/23838 completed (loss: 1.2512309551239014, acc: 0.6158940196037292)
[2025-02-05 13:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:44][root][INFO] - Training Epoch: 2/2, step 20751/23838 completed (loss: 1.038744568824768, acc: 0.6721311211585999)
[2025-02-05 13:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:44][root][INFO] - Training Epoch: 2/2, step 20752/23838 completed (loss: 1.200461745262146, acc: 0.6102941036224365)
[2025-02-05 13:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:44][root][INFO] - Training Epoch: 2/2, step 20753/23838 completed (loss: 1.0269256830215454, acc: 0.6940298676490784)
[2025-02-05 13:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:45][root][INFO] - Training Epoch: 2/2, step 20754/23838 completed (loss: 0.9584820866584778, acc: 0.6875)
[2025-02-05 13:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:45][root][INFO] - Training Epoch: 2/2, step 20755/23838 completed (loss: 0.9761858582496643, acc: 0.681034505367279)
[2025-02-05 13:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:46][root][INFO] - Training Epoch: 2/2, step 20756/23838 completed (loss: 1.4945220947265625, acc: 0.5460526347160339)
[2025-02-05 13:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:46][root][INFO] - Training Epoch: 2/2, step 20757/23838 completed (loss: 1.035434603691101, acc: 0.7014925479888916)
[2025-02-05 13:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:47][root][INFO] - Training Epoch: 2/2, step 20758/23838 completed (loss: 1.4628357887268066, acc: 0.6385542154312134)
[2025-02-05 13:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:47][root][INFO] - Training Epoch: 2/2, step 20759/23838 completed (loss: 1.0150748491287231, acc: 0.6812499761581421)
[2025-02-05 13:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:47][root][INFO] - Training Epoch: 2/2, step 20760/23838 completed (loss: 0.9319648742675781, acc: 0.7213114500045776)
[2025-02-05 13:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:48][root][INFO] - Training Epoch: 2/2, step 20761/23838 completed (loss: 0.914230227470398, acc: 0.7215189933776855)
[2025-02-05 13:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:48][root][INFO] - Training Epoch: 2/2, step 20762/23838 completed (loss: 1.2309728860855103, acc: 0.6518518328666687)
[2025-02-05 13:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:49][root][INFO] - Training Epoch: 2/2, step 20763/23838 completed (loss: 0.8663089871406555, acc: 0.7445651888847351)
[2025-02-05 13:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:49][root][INFO] - Training Epoch: 2/2, step 20764/23838 completed (loss: 1.0047107934951782, acc: 0.678260862827301)
[2025-02-05 13:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:49][root][INFO] - Training Epoch: 2/2, step 20765/23838 completed (loss: 1.4150742292404175, acc: 0.6000000238418579)
[2025-02-05 13:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:50][root][INFO] - Training Epoch: 2/2, step 20766/23838 completed (loss: 1.3411754369735718, acc: 0.609375)
[2025-02-05 13:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:50][root][INFO] - Training Epoch: 2/2, step 20767/23838 completed (loss: 1.568599820137024, acc: 0.5827338099479675)
[2025-02-05 13:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:51][root][INFO] - Training Epoch: 2/2, step 20768/23838 completed (loss: 1.1813952922821045, acc: 0.6343283653259277)
[2025-02-05 13:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:51][root][INFO] - Training Epoch: 2/2, step 20769/23838 completed (loss: 1.161178469657898, acc: 0.6304348111152649)
[2025-02-05 13:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:51][root][INFO] - Training Epoch: 2/2, step 20770/23838 completed (loss: 0.9539494514465332, acc: 0.7472527623176575)
[2025-02-05 13:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:52][root][INFO] - Training Epoch: 2/2, step 20771/23838 completed (loss: 1.4348279237747192, acc: 0.5890411138534546)
[2025-02-05 13:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:52][root][INFO] - Training Epoch: 2/2, step 20772/23838 completed (loss: 1.4673939943313599, acc: 0.5885714292526245)
[2025-02-05 13:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:53][root][INFO] - Training Epoch: 2/2, step 20773/23838 completed (loss: 0.9909107089042664, acc: 0.7191011309623718)
[2025-02-05 13:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:53][root][INFO] - Training Epoch: 2/2, step 20774/23838 completed (loss: 1.5779132843017578, acc: 0.5769230723381042)
[2025-02-05 13:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:54][root][INFO] - Training Epoch: 2/2, step 20775/23838 completed (loss: 1.451464056968689, acc: 0.5507246255874634)
[2025-02-05 13:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:54][root][INFO] - Training Epoch: 2/2, step 20776/23838 completed (loss: 1.1481446027755737, acc: 0.6603773832321167)
[2025-02-05 13:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:54][root][INFO] - Training Epoch: 2/2, step 20777/23838 completed (loss: 1.484142780303955, acc: 0.5714285969734192)
[2025-02-05 13:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:55][root][INFO] - Training Epoch: 2/2, step 20778/23838 completed (loss: 1.3945037126541138, acc: 0.5730336904525757)
[2025-02-05 13:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:55][root][INFO] - Training Epoch: 2/2, step 20779/23838 completed (loss: 1.4922150373458862, acc: 0.6052631735801697)
[2025-02-05 13:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:56][root][INFO] - Training Epoch: 2/2, step 20780/23838 completed (loss: 1.1221362352371216, acc: 0.6666666865348816)
[2025-02-05 13:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:56][root][INFO] - Training Epoch: 2/2, step 20781/23838 completed (loss: 1.36371910572052, acc: 0.625)
[2025-02-05 13:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:56][root][INFO] - Training Epoch: 2/2, step 20782/23838 completed (loss: 0.9829989075660706, acc: 0.7272727489471436)
[2025-02-05 13:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:57][root][INFO] - Training Epoch: 2/2, step 20783/23838 completed (loss: 1.5804966688156128, acc: 0.5555555820465088)
[2025-02-05 13:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:57][root][INFO] - Training Epoch: 2/2, step 20784/23838 completed (loss: 1.349976658821106, acc: 0.6133333444595337)
[2025-02-05 13:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:57][root][INFO] - Training Epoch: 2/2, step 20785/23838 completed (loss: 1.425756812095642, acc: 0.5909090638160706)
[2025-02-05 13:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:58][root][INFO] - Training Epoch: 2/2, step 20786/23838 completed (loss: 0.4713311195373535, acc: 0.8333333134651184)
[2025-02-05 13:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:58][root][INFO] - Training Epoch: 2/2, step 20787/23838 completed (loss: 1.0159454345703125, acc: 0.7021276354789734)
[2025-02-05 13:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:59][root][INFO] - Training Epoch: 2/2, step 20788/23838 completed (loss: 1.092065691947937, acc: 0.6788991093635559)
[2025-02-05 13:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:58:59][root][INFO] - Training Epoch: 2/2, step 20789/23838 completed (loss: 1.1116478443145752, acc: 0.6638655662536621)
[2025-02-05 13:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:00][root][INFO] - Training Epoch: 2/2, step 20790/23838 completed (loss: 1.0779080390930176, acc: 0.6582278609275818)
[2025-02-05 13:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:00][root][INFO] - Training Epoch: 2/2, step 20791/23838 completed (loss: 0.9111809134483337, acc: 0.7093023061752319)
[2025-02-05 13:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:00][root][INFO] - Training Epoch: 2/2, step 20792/23838 completed (loss: 1.0578643083572388, acc: 0.6875)
[2025-02-05 13:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:01][root][INFO] - Training Epoch: 2/2, step 20793/23838 completed (loss: 1.104154109954834, acc: 0.6829268336296082)
[2025-02-05 13:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:01][root][INFO] - Training Epoch: 2/2, step 20794/23838 completed (loss: 1.0719022750854492, acc: 0.640625)
[2025-02-05 13:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:01][root][INFO] - Training Epoch: 2/2, step 20795/23838 completed (loss: 1.137040138244629, acc: 0.6585366129875183)
[2025-02-05 13:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:02][root][INFO] - Training Epoch: 2/2, step 20796/23838 completed (loss: 1.1495164632797241, acc: 0.6666666865348816)
[2025-02-05 13:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:02][root][INFO] - Training Epoch: 2/2, step 20797/23838 completed (loss: 1.0848158597946167, acc: 0.6666666865348816)
[2025-02-05 13:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:03][root][INFO] - Training Epoch: 2/2, step 20798/23838 completed (loss: 1.0714489221572876, acc: 0.6630434989929199)
[2025-02-05 13:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:03][root][INFO] - Training Epoch: 2/2, step 20799/23838 completed (loss: 1.1026933193206787, acc: 0.692307710647583)
[2025-02-05 13:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:03][root][INFO] - Training Epoch: 2/2, step 20800/23838 completed (loss: 1.17314612865448, acc: 0.6571428775787354)
[2025-02-05 13:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:04][root][INFO] - Training Epoch: 2/2, step 20801/23838 completed (loss: 1.4926996231079102, acc: 0.6206896305084229)
[2025-02-05 13:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:04][root][INFO] - Training Epoch: 2/2, step 20802/23838 completed (loss: 1.4855772256851196, acc: 0.5655737519264221)
[2025-02-05 13:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:05][root][INFO] - Training Epoch: 2/2, step 20803/23838 completed (loss: 1.138614296913147, acc: 0.64462810754776)
[2025-02-05 13:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:05][root][INFO] - Training Epoch: 2/2, step 20804/23838 completed (loss: 1.5488779544830322, acc: 0.5365853905677795)
[2025-02-05 13:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:05][root][INFO] - Training Epoch: 2/2, step 20805/23838 completed (loss: 1.2885931730270386, acc: 0.5888888835906982)
[2025-02-05 13:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:06][root][INFO] - Training Epoch: 2/2, step 20806/23838 completed (loss: 1.130279541015625, acc: 0.6461538672447205)
[2025-02-05 13:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:06][root][INFO] - Training Epoch: 2/2, step 20807/23838 completed (loss: 1.2182546854019165, acc: 0.6428571343421936)
[2025-02-05 13:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:07][root][INFO] - Training Epoch: 2/2, step 20808/23838 completed (loss: 1.1240432262420654, acc: 0.6538461446762085)
[2025-02-05 13:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:07][root][INFO] - Training Epoch: 2/2, step 20809/23838 completed (loss: 1.032409906387329, acc: 0.7252747416496277)
[2025-02-05 13:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:08][root][INFO] - Training Epoch: 2/2, step 20810/23838 completed (loss: 1.0583477020263672, acc: 0.6692307591438293)
[2025-02-05 13:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:08][root][INFO] - Training Epoch: 2/2, step 20811/23838 completed (loss: 1.2178525924682617, acc: 0.6320754885673523)
[2025-02-05 13:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:08][root][INFO] - Training Epoch: 2/2, step 20812/23838 completed (loss: 1.0452631711959839, acc: 0.7092198729515076)
[2025-02-05 13:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:09][root][INFO] - Training Epoch: 2/2, step 20813/23838 completed (loss: 1.1431242227554321, acc: 0.6698113083839417)
[2025-02-05 13:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:09][root][INFO] - Training Epoch: 2/2, step 20814/23838 completed (loss: 1.038107991218567, acc: 0.6592592597007751)
[2025-02-05 13:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:10][root][INFO] - Training Epoch: 2/2, step 20815/23838 completed (loss: 1.388061761856079, acc: 0.6097561120986938)
[2025-02-05 13:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:10][root][INFO] - Training Epoch: 2/2, step 20816/23838 completed (loss: 0.846011757850647, acc: 0.7580645084381104)
[2025-02-05 13:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:10][root][INFO] - Training Epoch: 2/2, step 20817/23838 completed (loss: 0.7875653505325317, acc: 0.7692307829856873)
[2025-02-05 13:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:11][root][INFO] - Training Epoch: 2/2, step 20818/23838 completed (loss: 1.0487900972366333, acc: 0.6779661178588867)
[2025-02-05 13:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:11][root][INFO] - Training Epoch: 2/2, step 20819/23838 completed (loss: 0.8086695075035095, acc: 0.7564102411270142)
[2025-02-05 13:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:12][root][INFO] - Training Epoch: 2/2, step 20820/23838 completed (loss: 1.4643467664718628, acc: 0.6341463327407837)
[2025-02-05 13:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:12][root][INFO] - Training Epoch: 2/2, step 20821/23838 completed (loss: 1.3901526927947998, acc: 0.5227272510528564)
[2025-02-05 13:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:13][root][INFO] - Training Epoch: 2/2, step 20822/23838 completed (loss: 0.9482006430625916, acc: 0.734375)
[2025-02-05 13:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:13][root][INFO] - Training Epoch: 2/2, step 20823/23838 completed (loss: 0.894824206829071, acc: 0.7160493731498718)
[2025-02-05 13:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:13][root][INFO] - Training Epoch: 2/2, step 20824/23838 completed (loss: 0.8727187514305115, acc: 0.7471264600753784)
[2025-02-05 13:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:14][root][INFO] - Training Epoch: 2/2, step 20825/23838 completed (loss: 1.4645057916641235, acc: 0.5799999833106995)
[2025-02-05 13:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:14][root][INFO] - Training Epoch: 2/2, step 20826/23838 completed (loss: 1.0770248174667358, acc: 0.6710526347160339)
[2025-02-05 13:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:15][root][INFO] - Training Epoch: 2/2, step 20827/23838 completed (loss: 1.2415497303009033, acc: 0.6000000238418579)
[2025-02-05 13:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:15][root][INFO] - Training Epoch: 2/2, step 20828/23838 completed (loss: 1.4816179275512695, acc: 0.5364238619804382)
[2025-02-05 13:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:15][root][INFO] - Training Epoch: 2/2, step 20829/23838 completed (loss: 0.935226321220398, acc: 0.6829268336296082)
[2025-02-05 13:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:16][root][INFO] - Training Epoch: 2/2, step 20830/23838 completed (loss: 1.2137962579727173, acc: 0.6399999856948853)
[2025-02-05 13:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:16][root][INFO] - Training Epoch: 2/2, step 20831/23838 completed (loss: 0.7654989361763, acc: 0.7200000286102295)
[2025-02-05 13:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:16][root][INFO] - Training Epoch: 2/2, step 20832/23838 completed (loss: 0.8835729360580444, acc: 0.7260273694992065)
[2025-02-05 13:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:17][root][INFO] - Training Epoch: 2/2, step 20833/23838 completed (loss: 1.6395784616470337, acc: 0.5769230723381042)
[2025-02-05 13:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:17][root][INFO] - Training Epoch: 2/2, step 20834/23838 completed (loss: 1.4754666090011597, acc: 0.5869565010070801)
[2025-02-05 13:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:18][root][INFO] - Training Epoch: 2/2, step 20835/23838 completed (loss: 1.3132743835449219, acc: 0.6388888955116272)
[2025-02-05 13:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:18][root][INFO] - Training Epoch: 2/2, step 20836/23838 completed (loss: 1.2663878202438354, acc: 0.6000000238418579)
[2025-02-05 13:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:18][root][INFO] - Training Epoch: 2/2, step 20837/23838 completed (loss: 1.0198308229446411, acc: 0.75)
[2025-02-05 13:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:19][root][INFO] - Training Epoch: 2/2, step 20838/23838 completed (loss: 1.1364452838897705, acc: 0.6875)
[2025-02-05 13:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:19][root][INFO] - Training Epoch: 2/2, step 20839/23838 completed (loss: 1.272818684577942, acc: 0.6399999856948853)
[2025-02-05 13:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:19][root][INFO] - Training Epoch: 2/2, step 20840/23838 completed (loss: 1.284682273864746, acc: 0.574999988079071)
[2025-02-05 13:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:20][root][INFO] - Training Epoch: 2/2, step 20841/23838 completed (loss: 1.1512231826782227, acc: 0.698113203048706)
[2025-02-05 13:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:20][root][INFO] - Training Epoch: 2/2, step 20842/23838 completed (loss: 0.5015426874160767, acc: 0.8695651888847351)
[2025-02-05 13:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:21][root][INFO] - Training Epoch: 2/2, step 20843/23838 completed (loss: 1.8361848592758179, acc: 0.5)
[2025-02-05 13:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:21][root][INFO] - Training Epoch: 2/2, step 20844/23838 completed (loss: 1.5573129653930664, acc: 0.49180328845977783)
[2025-02-05 13:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:21][root][INFO] - Training Epoch: 2/2, step 20845/23838 completed (loss: 1.0575464963912964, acc: 0.6849315166473389)
[2025-02-05 13:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:22][root][INFO] - Training Epoch: 2/2, step 20846/23838 completed (loss: 1.2811357975006104, acc: 0.5)
[2025-02-05 13:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:22][root][INFO] - Training Epoch: 2/2, step 20847/23838 completed (loss: 1.025313377380371, acc: 0.6727272868156433)
[2025-02-05 13:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:22][root][INFO] - Training Epoch: 2/2, step 20848/23838 completed (loss: 1.6119909286499023, acc: 0.6129032373428345)
[2025-02-05 13:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:23][root][INFO] - Training Epoch: 2/2, step 20849/23838 completed (loss: 0.7269675135612488, acc: 0.8166666626930237)
[2025-02-05 13:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:23][root][INFO] - Training Epoch: 2/2, step 20850/23838 completed (loss: 1.5624785423278809, acc: 0.42553192377090454)
[2025-02-05 13:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:24][root][INFO] - Training Epoch: 2/2, step 20851/23838 completed (loss: 1.0799133777618408, acc: 0.7111111283302307)
[2025-02-05 13:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:24][root][INFO] - Training Epoch: 2/2, step 20852/23838 completed (loss: 1.1713981628417969, acc: 0.6499999761581421)
[2025-02-05 13:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:25][root][INFO] - Training Epoch: 2/2, step 20853/23838 completed (loss: 1.1694508790969849, acc: 0.6176470518112183)
[2025-02-05 13:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:25][root][INFO] - Training Epoch: 2/2, step 20854/23838 completed (loss: 0.9950456023216248, acc: 0.6399999856948853)
[2025-02-05 13:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:26][root][INFO] - Training Epoch: 2/2, step 20855/23838 completed (loss: 1.0005121231079102, acc: 0.5833333134651184)
[2025-02-05 13:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:26][root][INFO] - Training Epoch: 2/2, step 20856/23838 completed (loss: 1.087078332901001, acc: 0.707317054271698)
[2025-02-05 13:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:26][root][INFO] - Training Epoch: 2/2, step 20857/23838 completed (loss: 1.4039504528045654, acc: 0.6666666865348816)
[2025-02-05 13:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:27][root][INFO] - Training Epoch: 2/2, step 20858/23838 completed (loss: 1.5236537456512451, acc: 0.5783132314682007)
[2025-02-05 13:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:27][root][INFO] - Training Epoch: 2/2, step 20859/23838 completed (loss: 1.2700893878936768, acc: 0.6185566782951355)
[2025-02-05 13:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:27][root][INFO] - Training Epoch: 2/2, step 20860/23838 completed (loss: 1.2127511501312256, acc: 0.6585366129875183)
[2025-02-05 13:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:28][root][INFO] - Training Epoch: 2/2, step 20861/23838 completed (loss: 1.4131146669387817, acc: 0.5362318754196167)
[2025-02-05 13:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:28][root][INFO] - Training Epoch: 2/2, step 20862/23838 completed (loss: 1.4539614915847778, acc: 0.5090909004211426)
[2025-02-05 13:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:29][root][INFO] - Training Epoch: 2/2, step 20863/23838 completed (loss: 1.2491883039474487, acc: 0.5679012537002563)
[2025-02-05 13:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:29][root][INFO] - Training Epoch: 2/2, step 20864/23838 completed (loss: 1.4880295991897583, acc: 0.5686274766921997)
[2025-02-05 13:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:29][root][INFO] - Training Epoch: 2/2, step 20865/23838 completed (loss: 1.309762716293335, acc: 0.612500011920929)
[2025-02-05 13:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:30][root][INFO] - Training Epoch: 2/2, step 20866/23838 completed (loss: 1.3370046615600586, acc: 0.6056337952613831)
[2025-02-05 13:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:30][root][INFO] - Training Epoch: 2/2, step 20867/23838 completed (loss: 1.0221573114395142, acc: 0.695652186870575)
[2025-02-05 13:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:31][root][INFO] - Training Epoch: 2/2, step 20868/23838 completed (loss: 1.3605198860168457, acc: 0.7045454382896423)
[2025-02-05 13:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:31][root][INFO] - Training Epoch: 2/2, step 20869/23838 completed (loss: 1.2221571207046509, acc: 0.774193525314331)
[2025-02-05 13:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:31][root][INFO] - Training Epoch: 2/2, step 20870/23838 completed (loss: 0.9765334725379944, acc: 0.692307710647583)
[2025-02-05 13:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:32][root][INFO] - Training Epoch: 2/2, step 20871/23838 completed (loss: 0.748189389705658, acc: 0.9047619104385376)
[2025-02-05 13:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:32][root][INFO] - Training Epoch: 2/2, step 20872/23838 completed (loss: 0.6042086482048035, acc: 0.8666666746139526)
[2025-02-05 13:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:33][root][INFO] - Training Epoch: 2/2, step 20873/23838 completed (loss: 1.5033881664276123, acc: 0.5600000023841858)
[2025-02-05 13:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:33][root][INFO] - Training Epoch: 2/2, step 20874/23838 completed (loss: 0.3648451566696167, acc: 0.95652174949646)
[2025-02-05 13:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:33][root][INFO] - Training Epoch: 2/2, step 20875/23838 completed (loss: 0.565126895904541, acc: 0.7777777910232544)
[2025-02-05 13:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:34][root][INFO] - Training Epoch: 2/2, step 20876/23838 completed (loss: 0.7264966368675232, acc: 0.7714285850524902)
[2025-02-05 13:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:34][root][INFO] - Training Epoch: 2/2, step 20877/23838 completed (loss: 1.011215090751648, acc: 0.6000000238418579)
[2025-02-05 13:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:35][root][INFO] - Training Epoch: 2/2, step 20878/23838 completed (loss: 1.2710983753204346, acc: 0.6451612710952759)
[2025-02-05 13:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:35][root][INFO] - Training Epoch: 2/2, step 20879/23838 completed (loss: 1.1720353364944458, acc: 0.6428571343421936)
[2025-02-05 13:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:35][root][INFO] - Training Epoch: 2/2, step 20880/23838 completed (loss: 0.913118839263916, acc: 0.7916666865348816)
[2025-02-05 13:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:36][root][INFO] - Training Epoch: 2/2, step 20881/23838 completed (loss: 0.8250292539596558, acc: 0.6190476417541504)
[2025-02-05 13:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:36][root][INFO] - Training Epoch: 2/2, step 20882/23838 completed (loss: 1.2665660381317139, acc: 0.5161290168762207)
[2025-02-05 13:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:37][root][INFO] - Training Epoch: 2/2, step 20883/23838 completed (loss: 0.46249741315841675, acc: 0.875)
[2025-02-05 13:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:37][root][INFO] - Training Epoch: 2/2, step 20884/23838 completed (loss: 0.7280219793319702, acc: 0.7272727489471436)
[2025-02-05 13:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:37][root][INFO] - Training Epoch: 2/2, step 20885/23838 completed (loss: 1.2060513496398926, acc: 0.625)
[2025-02-05 13:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:38][root][INFO] - Training Epoch: 2/2, step 20886/23838 completed (loss: 1.6850560903549194, acc: 0.4166666567325592)
[2025-02-05 13:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:38][root][INFO] - Training Epoch: 2/2, step 20887/23838 completed (loss: 0.6678426861763, acc: 0.8620689511299133)
[2025-02-05 13:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:38][root][INFO] - Training Epoch: 2/2, step 20888/23838 completed (loss: 0.9091235399246216, acc: 0.75)
[2025-02-05 13:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:39][root][INFO] - Training Epoch: 2/2, step 20889/23838 completed (loss: 1.0651642084121704, acc: 0.6153846383094788)
[2025-02-05 13:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:39][root][INFO] - Training Epoch: 2/2, step 20890/23838 completed (loss: 0.5739396810531616, acc: 0.7058823704719543)
[2025-02-05 13:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:40][root][INFO] - Training Epoch: 2/2, step 20891/23838 completed (loss: 1.5184656381607056, acc: 0.6111111044883728)
[2025-02-05 13:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:40][root][INFO] - Training Epoch: 2/2, step 20892/23838 completed (loss: 1.6050838232040405, acc: 0.47058823704719543)
[2025-02-05 13:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:40][root][INFO] - Training Epoch: 2/2, step 20893/23838 completed (loss: 1.1100510358810425, acc: 0.7058823704719543)
[2025-02-05 13:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:41][root][INFO] - Training Epoch: 2/2, step 20894/23838 completed (loss: 1.0399636030197144, acc: 0.6666666865348816)
[2025-02-05 13:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:41][root][INFO] - Training Epoch: 2/2, step 20895/23838 completed (loss: 0.8660354614257812, acc: 0.6842105388641357)
[2025-02-05 13:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:42][root][INFO] - Training Epoch: 2/2, step 20896/23838 completed (loss: 0.7794805765151978, acc: 0.7272727489471436)
[2025-02-05 13:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:42][root][INFO] - Training Epoch: 2/2, step 20897/23838 completed (loss: 0.6305060386657715, acc: 0.7083333134651184)
[2025-02-05 13:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:42][root][INFO] - Training Epoch: 2/2, step 20898/23838 completed (loss: 0.8326993584632874, acc: 0.7777777910232544)
[2025-02-05 13:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:43][root][INFO] - Training Epoch: 2/2, step 20899/23838 completed (loss: 0.7396786212921143, acc: 0.7222222089767456)
[2025-02-05 13:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:43][root][INFO] - Training Epoch: 2/2, step 20900/23838 completed (loss: 0.48448100686073303, acc: 0.800000011920929)
[2025-02-05 13:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:44][root][INFO] - Training Epoch: 2/2, step 20901/23838 completed (loss: 0.6274242401123047, acc: 0.8181818127632141)
[2025-02-05 13:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:44][root][INFO] - Training Epoch: 2/2, step 20902/23838 completed (loss: 1.024479866027832, acc: 0.7142857313156128)
[2025-02-05 13:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:44][root][INFO] - Training Epoch: 2/2, step 20903/23838 completed (loss: 0.9978575706481934, acc: 0.6857143044471741)
[2025-02-05 13:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:45][root][INFO] - Training Epoch: 2/2, step 20904/23838 completed (loss: 0.8410210609436035, acc: 0.7916666865348816)
[2025-02-05 13:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:45][root][INFO] - Training Epoch: 2/2, step 20905/23838 completed (loss: 1.2455546855926514, acc: 0.7368420958518982)
[2025-02-05 13:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:45][root][INFO] - Training Epoch: 2/2, step 20906/23838 completed (loss: 0.7768552899360657, acc: 0.695652186870575)
[2025-02-05 13:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:46][root][INFO] - Training Epoch: 2/2, step 20907/23838 completed (loss: 1.6411195993423462, acc: 0.557692289352417)
[2025-02-05 13:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:46][root][INFO] - Training Epoch: 2/2, step 20908/23838 completed (loss: 1.3100206851959229, acc: 0.7037037014961243)
[2025-02-05 13:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:47][root][INFO] - Training Epoch: 2/2, step 20909/23838 completed (loss: 1.0537855625152588, acc: 0.5)
[2025-02-05 13:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:47][root][INFO] - Training Epoch: 2/2, step 20910/23838 completed (loss: 0.964972734451294, acc: 0.65625)
[2025-02-05 13:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:47][root][INFO] - Training Epoch: 2/2, step 20911/23838 completed (loss: 0.9357937574386597, acc: 0.7037037014961243)
[2025-02-05 13:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:48][root][INFO] - Training Epoch: 2/2, step 20912/23838 completed (loss: 1.0747721195220947, acc: 0.6857143044471741)
[2025-02-05 13:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:48][root][INFO] - Training Epoch: 2/2, step 20913/23838 completed (loss: 1.6434260606765747, acc: 0.6363636255264282)
[2025-02-05 13:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:49][root][INFO] - Training Epoch: 2/2, step 20914/23838 completed (loss: 1.1487953662872314, acc: 0.6086956262588501)
[2025-02-05 13:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:49][root][INFO] - Training Epoch: 2/2, step 20915/23838 completed (loss: 0.743377149105072, acc: 0.7083333134651184)
[2025-02-05 13:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:49][root][INFO] - Training Epoch: 2/2, step 20916/23838 completed (loss: 1.0211042165756226, acc: 0.8095238208770752)
[2025-02-05 13:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:50][root][INFO] - Training Epoch: 2/2, step 20917/23838 completed (loss: 1.0367603302001953, acc: 0.6875)
[2025-02-05 13:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:50][root][INFO] - Training Epoch: 2/2, step 20918/23838 completed (loss: 1.3959301710128784, acc: 0.7142857313156128)
[2025-02-05 13:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:51][root][INFO] - Training Epoch: 2/2, step 20919/23838 completed (loss: 0.6416872143745422, acc: 0.837837815284729)
[2025-02-05 13:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:51][root][INFO] - Training Epoch: 2/2, step 20920/23838 completed (loss: 0.4203786253929138, acc: 0.8484848737716675)
[2025-02-05 13:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:51][root][INFO] - Training Epoch: 2/2, step 20921/23838 completed (loss: 1.010413646697998, acc: 0.7037037014961243)
[2025-02-05 13:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:52][root][INFO] - Training Epoch: 2/2, step 20922/23838 completed (loss: 0.5196495652198792, acc: 0.875)
[2025-02-05 13:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:52][root][INFO] - Training Epoch: 2/2, step 20923/23838 completed (loss: 1.1010304689407349, acc: 0.707317054271698)
[2025-02-05 13:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:53][root][INFO] - Training Epoch: 2/2, step 20924/23838 completed (loss: 1.2323997020721436, acc: 0.6976743936538696)
[2025-02-05 13:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:53][root][INFO] - Training Epoch: 2/2, step 20925/23838 completed (loss: 1.200391173362732, acc: 0.7021276354789734)
[2025-02-05 13:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:53][root][INFO] - Training Epoch: 2/2, step 20926/23838 completed (loss: 0.962314248085022, acc: 0.7209302186965942)
[2025-02-05 13:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:54][root][INFO] - Training Epoch: 2/2, step 20927/23838 completed (loss: 0.7525724172592163, acc: 0.8139534592628479)
[2025-02-05 13:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:54][root][INFO] - Training Epoch: 2/2, step 20928/23838 completed (loss: 1.3118244409561157, acc: 0.5833333134651184)
[2025-02-05 13:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:55][root][INFO] - Training Epoch: 2/2, step 20929/23838 completed (loss: 1.0052841901779175, acc: 0.7096773982048035)
[2025-02-05 13:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:55][root][INFO] - Training Epoch: 2/2, step 20930/23838 completed (loss: 1.5669395923614502, acc: 0.604651153087616)
[2025-02-05 13:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:55][root][INFO] - Training Epoch: 2/2, step 20931/23838 completed (loss: 1.1361207962036133, acc: 0.7358490824699402)
[2025-02-05 13:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:56][root][INFO] - Training Epoch: 2/2, step 20932/23838 completed (loss: 1.24030339717865, acc: 0.6666666865348816)
[2025-02-05 13:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:56][root][INFO] - Training Epoch: 2/2, step 20933/23838 completed (loss: 1.2123792171478271, acc: 0.6585366129875183)
[2025-02-05 13:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:57][root][INFO] - Training Epoch: 2/2, step 20934/23838 completed (loss: 1.5184054374694824, acc: 0.5625)
[2025-02-05 13:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:57][root][INFO] - Training Epoch: 2/2, step 20935/23838 completed (loss: 1.2344741821289062, acc: 0.7037037014961243)
[2025-02-05 13:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:57][root][INFO] - Training Epoch: 2/2, step 20936/23838 completed (loss: 0.5034657120704651, acc: 0.8888888955116272)
[2025-02-05 13:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:58][root][INFO] - Training Epoch: 2/2, step 20937/23838 completed (loss: 1.221989631652832, acc: 0.6399999856948853)
[2025-02-05 13:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:58][root][INFO] - Training Epoch: 2/2, step 20938/23838 completed (loss: 1.0892165899276733, acc: 0.6744186282157898)
[2025-02-05 13:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:58][root][INFO] - Training Epoch: 2/2, step 20939/23838 completed (loss: 0.8345397710800171, acc: 0.6785714030265808)
[2025-02-05 13:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:59][root][INFO] - Training Epoch: 2/2, step 20940/23838 completed (loss: 1.1134600639343262, acc: 0.6451612710952759)
[2025-02-05 13:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 13:59:59][root][INFO] - Training Epoch: 2/2, step 20941/23838 completed (loss: 1.448981761932373, acc: 0.6200000047683716)
[2025-02-05 13:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:00][root][INFO] - Training Epoch: 2/2, step 20942/23838 completed (loss: 1.2933183908462524, acc: 0.6842105388641357)
[2025-02-05 14:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:00][root][INFO] - Training Epoch: 2/2, step 20943/23838 completed (loss: 0.7441824078559875, acc: 0.7368420958518982)
[2025-02-05 14:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:00][root][INFO] - Training Epoch: 2/2, step 20944/23838 completed (loss: 0.9949529767036438, acc: 0.760869562625885)
[2025-02-05 14:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:01][root][INFO] - Training Epoch: 2/2, step 20945/23838 completed (loss: 1.5017977952957153, acc: 0.6129032373428345)
[2025-02-05 14:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:01][root][INFO] - Training Epoch: 2/2, step 20946/23838 completed (loss: 1.6355259418487549, acc: 0.6000000238418579)
[2025-02-05 14:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:01][root][INFO] - Training Epoch: 2/2, step 20947/23838 completed (loss: 0.7937853932380676, acc: 0.7250000238418579)
[2025-02-05 14:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:02][root][INFO] - Training Epoch: 2/2, step 20948/23838 completed (loss: 1.5414031744003296, acc: 0.5199999809265137)
[2025-02-05 14:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:02][root][INFO] - Training Epoch: 2/2, step 20949/23838 completed (loss: 0.7670969367027283, acc: 0.7857142686843872)
[2025-02-05 14:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:03][root][INFO] - Training Epoch: 2/2, step 20950/23838 completed (loss: 1.5798728466033936, acc: 0.48571428656578064)
[2025-02-05 14:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:03][root][INFO] - Training Epoch: 2/2, step 20951/23838 completed (loss: 1.103869915008545, acc: 0.7096773982048035)
[2025-02-05 14:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:03][root][INFO] - Training Epoch: 2/2, step 20952/23838 completed (loss: 1.3342026472091675, acc: 0.5757575631141663)
[2025-02-05 14:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:04][root][INFO] - Training Epoch: 2/2, step 20953/23838 completed (loss: 1.1358429193496704, acc: 0.6470588445663452)
[2025-02-05 14:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:04][root][INFO] - Training Epoch: 2/2, step 20954/23838 completed (loss: 0.4742637872695923, acc: 0.8064516186714172)
[2025-02-05 14:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:04][root][INFO] - Training Epoch: 2/2, step 20955/23838 completed (loss: 0.6701163053512573, acc: 0.8095238208770752)
[2025-02-05 14:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:05][root][INFO] - Training Epoch: 2/2, step 20956/23838 completed (loss: 1.189105749130249, acc: 0.6666666865348816)
[2025-02-05 14:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:05][root][INFO] - Training Epoch: 2/2, step 20957/23838 completed (loss: 0.39496877789497375, acc: 0.8888888955116272)
[2025-02-05 14:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:06][root][INFO] - Training Epoch: 2/2, step 20958/23838 completed (loss: 1.1448115110397339, acc: 0.6097561120986938)
[2025-02-05 14:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:06][root][INFO] - Training Epoch: 2/2, step 20959/23838 completed (loss: 1.4893549680709839, acc: 0.5)
[2025-02-05 14:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:06][root][INFO] - Training Epoch: 2/2, step 20960/23838 completed (loss: 1.4374438524246216, acc: 0.5892857313156128)
[2025-02-05 14:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:07][root][INFO] - Training Epoch: 2/2, step 20961/23838 completed (loss: 1.1595834493637085, acc: 0.6571428775787354)
[2025-02-05 14:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:07][root][INFO] - Training Epoch: 2/2, step 20962/23838 completed (loss: 1.696755290031433, acc: 0.5277777910232544)
[2025-02-05 14:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:08][root][INFO] - Training Epoch: 2/2, step 20963/23838 completed (loss: 1.0997925996780396, acc: 0.65625)
[2025-02-05 14:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:08][root][INFO] - Training Epoch: 2/2, step 20964/23838 completed (loss: 1.5829838514328003, acc: 0.5423728823661804)
[2025-02-05 14:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:08][root][INFO] - Training Epoch: 2/2, step 20965/23838 completed (loss: 1.3929632902145386, acc: 0.578125)
[2025-02-05 14:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:09][root][INFO] - Training Epoch: 2/2, step 20966/23838 completed (loss: 0.8633695840835571, acc: 0.841269850730896)
[2025-02-05 14:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:09][root][INFO] - Training Epoch: 2/2, step 20967/23838 completed (loss: 1.4045770168304443, acc: 0.5625)
[2025-02-05 14:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:09][root][INFO] - Training Epoch: 2/2, step 20968/23838 completed (loss: 1.3267674446105957, acc: 0.584269642829895)
[2025-02-05 14:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:10][root][INFO] - Training Epoch: 2/2, step 20969/23838 completed (loss: 1.535888671875, acc: 0.5531914830207825)
[2025-02-05 14:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:10][root][INFO] - Training Epoch: 2/2, step 20970/23838 completed (loss: 1.2573630809783936, acc: 0.6000000238418579)
[2025-02-05 14:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:11][root][INFO] - Training Epoch: 2/2, step 20971/23838 completed (loss: 0.8734817504882812, acc: 0.7179487347602844)
[2025-02-05 14:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:11][root][INFO] - Training Epoch: 2/2, step 20972/23838 completed (loss: 1.132521152496338, acc: 0.6101694703102112)
[2025-02-05 14:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:11][root][INFO] - Training Epoch: 2/2, step 20973/23838 completed (loss: 1.1971925497055054, acc: 0.698113203048706)
[2025-02-05 14:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:12][root][INFO] - Training Epoch: 2/2, step 20974/23838 completed (loss: 1.1195956468582153, acc: 0.6451612710952759)
[2025-02-05 14:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:12][root][INFO] - Training Epoch: 2/2, step 20975/23838 completed (loss: 1.1832876205444336, acc: 0.6617646813392639)
[2025-02-05 14:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:13][root][INFO] - Training Epoch: 2/2, step 20976/23838 completed (loss: 1.0306488275527954, acc: 0.6716417670249939)
[2025-02-05 14:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:13][root][INFO] - Training Epoch: 2/2, step 20977/23838 completed (loss: 1.2811253070831299, acc: 0.6352941393852234)
[2025-02-05 14:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:13][root][INFO] - Training Epoch: 2/2, step 20978/23838 completed (loss: 1.3188508749008179, acc: 0.6024096608161926)
[2025-02-05 14:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:14][root][INFO] - Training Epoch: 2/2, step 20979/23838 completed (loss: 1.3348456621170044, acc: 0.6206896305084229)
[2025-02-05 14:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:14][root][INFO] - Training Epoch: 2/2, step 20980/23838 completed (loss: 1.2345281839370728, acc: 0.699999988079071)
[2025-02-05 14:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:15][root][INFO] - Training Epoch: 2/2, step 20981/23838 completed (loss: 1.2421616315841675, acc: 0.641791045665741)
[2025-02-05 14:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:15][root][INFO] - Training Epoch: 2/2, step 20982/23838 completed (loss: 1.378606915473938, acc: 0.5897436141967773)
[2025-02-05 14:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:16][root][INFO] - Training Epoch: 2/2, step 20983/23838 completed (loss: 1.0237929821014404, acc: 0.6752136945724487)
[2025-02-05 14:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:16][root][INFO] - Training Epoch: 2/2, step 20984/23838 completed (loss: 1.1201841831207275, acc: 0.6886792182922363)
[2025-02-05 14:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:17][root][INFO] - Training Epoch: 2/2, step 20985/23838 completed (loss: 1.1752594709396362, acc: 0.6705882549285889)
[2025-02-05 14:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:17][root][INFO] - Training Epoch: 2/2, step 20986/23838 completed (loss: 1.2718666791915894, acc: 0.6057692170143127)
[2025-02-05 14:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:17][root][INFO] - Training Epoch: 2/2, step 20987/23838 completed (loss: 0.8775600790977478, acc: 0.7475728392601013)
[2025-02-05 14:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:18][root][INFO] - Training Epoch: 2/2, step 20988/23838 completed (loss: 0.9828948974609375, acc: 0.7011494040489197)
[2025-02-05 14:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:18][root][INFO] - Training Epoch: 2/2, step 20989/23838 completed (loss: 1.2608734369277954, acc: 0.584269642829895)
[2025-02-05 14:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:19][root][INFO] - Training Epoch: 2/2, step 20990/23838 completed (loss: 0.7050804495811462, acc: 0.78125)
[2025-02-05 14:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:19][root][INFO] - Training Epoch: 2/2, step 20991/23838 completed (loss: 0.9373405575752258, acc: 0.7200000286102295)
[2025-02-05 14:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:19][root][INFO] - Training Epoch: 2/2, step 20992/23838 completed (loss: 1.2856383323669434, acc: 0.6071428656578064)
[2025-02-05 14:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:20][root][INFO] - Training Epoch: 2/2, step 20993/23838 completed (loss: 0.9779640436172485, acc: 0.7272727489471436)
[2025-02-05 14:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:20][root][INFO] - Training Epoch: 2/2, step 20994/23838 completed (loss: 1.1822260618209839, acc: 0.7209302186965942)
[2025-02-05 14:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:20][root][INFO] - Training Epoch: 2/2, step 20995/23838 completed (loss: 0.7596223950386047, acc: 0.8205128312110901)
[2025-02-05 14:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:21][root][INFO] - Training Epoch: 2/2, step 20996/23838 completed (loss: 1.1110652685165405, acc: 0.6760563254356384)
[2025-02-05 14:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:21][root][INFO] - Training Epoch: 2/2, step 20997/23838 completed (loss: 1.0303441286087036, acc: 0.6724137663841248)
[2025-02-05 14:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:22][root][INFO] - Training Epoch: 2/2, step 20998/23838 completed (loss: 1.6663562059402466, acc: 0.5454545617103577)
[2025-02-05 14:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:22][root][INFO] - Training Epoch: 2/2, step 20999/23838 completed (loss: 1.0852559804916382, acc: 0.7857142686843872)
[2025-02-05 14:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:22][root][INFO] - Training Epoch: 2/2, step 21000/23838 completed (loss: 1.2162712812423706, acc: 0.6794871687889099)
[2025-02-05 14:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:23][root][INFO] - Training Epoch: 2/2, step 21001/23838 completed (loss: 1.015242576599121, acc: 0.6976743936538696)
[2025-02-05 14:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:23][root][INFO] - Training Epoch: 2/2, step 21002/23838 completed (loss: 1.188178300857544, acc: 0.6593406796455383)
[2025-02-05 14:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:24][root][INFO] - Training Epoch: 2/2, step 21003/23838 completed (loss: 0.9988977909088135, acc: 0.7142857313156128)
[2025-02-05 14:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:24][root][INFO] - Training Epoch: 2/2, step 21004/23838 completed (loss: 1.0389339923858643, acc: 0.7419354915618896)
[2025-02-05 14:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:24][root][INFO] - Training Epoch: 2/2, step 21005/23838 completed (loss: 1.2624237537384033, acc: 0.6428571343421936)
[2025-02-05 14:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:25][root][INFO] - Training Epoch: 2/2, step 21006/23838 completed (loss: 1.0410478115081787, acc: 0.6769230961799622)
[2025-02-05 14:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:25][root][INFO] - Training Epoch: 2/2, step 21007/23838 completed (loss: 1.2182648181915283, acc: 0.6513761281967163)
[2025-02-05 14:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:25][root][INFO] - Training Epoch: 2/2, step 21008/23838 completed (loss: 1.2421656847000122, acc: 0.625)
[2025-02-05 14:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:26][root][INFO] - Training Epoch: 2/2, step 21009/23838 completed (loss: 1.0071475505828857, acc: 0.7352941036224365)
[2025-02-05 14:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:26][root][INFO] - Training Epoch: 2/2, step 21010/23838 completed (loss: 1.0304522514343262, acc: 0.6896551847457886)
[2025-02-05 14:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:27][root][INFO] - Training Epoch: 2/2, step 21011/23838 completed (loss: 0.5382353663444519, acc: 0.8679245114326477)
[2025-02-05 14:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:27][root][INFO] - Training Epoch: 2/2, step 21012/23838 completed (loss: 0.8395803570747375, acc: 0.78125)
[2025-02-05 14:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:28][root][INFO] - Training Epoch: 2/2, step 21013/23838 completed (loss: 0.7005468010902405, acc: 0.7708333134651184)
[2025-02-05 14:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:28][root][INFO] - Training Epoch: 2/2, step 21014/23838 completed (loss: 1.2696207761764526, acc: 0.5681818127632141)
[2025-02-05 14:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:28][root][INFO] - Training Epoch: 2/2, step 21015/23838 completed (loss: 1.1657946109771729, acc: 0.6438356041908264)
[2025-02-05 14:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:29][root][INFO] - Training Epoch: 2/2, step 21016/23838 completed (loss: 0.8649013042449951, acc: 0.7222222089767456)
[2025-02-05 14:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:29][root][INFO] - Training Epoch: 2/2, step 21017/23838 completed (loss: 1.0116956233978271, acc: 0.6865671873092651)
[2025-02-05 14:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:30][root][INFO] - Training Epoch: 2/2, step 21018/23838 completed (loss: 0.8368349671363831, acc: 0.7972972989082336)
[2025-02-05 14:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:30][root][INFO] - Training Epoch: 2/2, step 21019/23838 completed (loss: 1.1295539140701294, acc: 0.6891891956329346)
[2025-02-05 14:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:30][root][INFO] - Training Epoch: 2/2, step 21020/23838 completed (loss: 0.9296282529830933, acc: 0.7142857313156128)
[2025-02-05 14:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:31][root][INFO] - Training Epoch: 2/2, step 21021/23838 completed (loss: 0.9540168046951294, acc: 0.7633587718009949)
[2025-02-05 14:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:31][root][INFO] - Training Epoch: 2/2, step 21022/23838 completed (loss: 0.9797387719154358, acc: 0.7300000190734863)
[2025-02-05 14:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:32][root][INFO] - Training Epoch: 2/2, step 21023/23838 completed (loss: 1.3634237051010132, acc: 0.6428571343421936)
[2025-02-05 14:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:32][root][INFO] - Training Epoch: 2/2, step 21024/23838 completed (loss: 1.1833993196487427, acc: 0.6902654767036438)
[2025-02-05 14:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:32][root][INFO] - Training Epoch: 2/2, step 21025/23838 completed (loss: 0.771921694278717, acc: 0.7368420958518982)
[2025-02-05 14:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:33][root][INFO] - Training Epoch: 2/2, step 21026/23838 completed (loss: 0.7798076272010803, acc: 0.7733333110809326)
[2025-02-05 14:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:33][root][INFO] - Training Epoch: 2/2, step 21027/23838 completed (loss: 1.0540850162506104, acc: 0.6951219439506531)
[2025-02-05 14:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:34][root][INFO] - Training Epoch: 2/2, step 21028/23838 completed (loss: 1.127921223640442, acc: 0.70652174949646)
[2025-02-05 14:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:34][root][INFO] - Training Epoch: 2/2, step 21029/23838 completed (loss: 1.274559736251831, acc: 0.6419752836227417)
[2025-02-05 14:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:34][root][INFO] - Training Epoch: 2/2, step 21030/23838 completed (loss: 1.0258902311325073, acc: 0.7307692170143127)
[2025-02-05 14:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:35][root][INFO] - Training Epoch: 2/2, step 21031/23838 completed (loss: 1.0797237157821655, acc: 0.7123287916183472)
[2025-02-05 14:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:35][root][INFO] - Training Epoch: 2/2, step 21032/23838 completed (loss: 1.4179346561431885, acc: 0.5692307949066162)
[2025-02-05 14:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:36][root][INFO] - Training Epoch: 2/2, step 21033/23838 completed (loss: 0.9449040293693542, acc: 0.7078651785850525)
[2025-02-05 14:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:36][root][INFO] - Training Epoch: 2/2, step 21034/23838 completed (loss: 0.9096241593360901, acc: 0.6790123581886292)
[2025-02-05 14:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:36][root][INFO] - Training Epoch: 2/2, step 21035/23838 completed (loss: 0.9194353222846985, acc: 0.6891891956329346)
[2025-02-05 14:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:37][root][INFO] - Training Epoch: 2/2, step 21036/23838 completed (loss: 1.1853318214416504, acc: 0.6363636255264282)
[2025-02-05 14:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:37][root][INFO] - Training Epoch: 2/2, step 21037/23838 completed (loss: 1.221479058265686, acc: 0.6555555462837219)
[2025-02-05 14:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:38][root][INFO] - Training Epoch: 2/2, step 21038/23838 completed (loss: 1.1371718645095825, acc: 0.644444465637207)
[2025-02-05 14:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:38][root][INFO] - Training Epoch: 2/2, step 21039/23838 completed (loss: 1.5100388526916504, acc: 0.5632184147834778)
[2025-02-05 14:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:39][root][INFO] - Training Epoch: 2/2, step 21040/23838 completed (loss: 1.182318925857544, acc: 0.699999988079071)
[2025-02-05 14:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:39][root][INFO] - Training Epoch: 2/2, step 21041/23838 completed (loss: 0.9866202473640442, acc: 0.6363636255264282)
[2025-02-05 14:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:40][root][INFO] - Training Epoch: 2/2, step 21042/23838 completed (loss: 1.0484336614608765, acc: 0.71875)
[2025-02-05 14:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:40][root][INFO] - Training Epoch: 2/2, step 21043/23838 completed (loss: 1.4645828008651733, acc: 0.6451612710952759)
[2025-02-05 14:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:40][root][INFO] - Training Epoch: 2/2, step 21044/23838 completed (loss: 0.9669938683509827, acc: 0.7058823704719543)
[2025-02-05 14:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:41][root][INFO] - Training Epoch: 2/2, step 21045/23838 completed (loss: 1.2259209156036377, acc: 0.6279069781303406)
[2025-02-05 14:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:41][root][INFO] - Training Epoch: 2/2, step 21046/23838 completed (loss: 1.5760972499847412, acc: 0.4761904776096344)
[2025-02-05 14:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:41][root][INFO] - Training Epoch: 2/2, step 21047/23838 completed (loss: 1.6481902599334717, acc: 0.560606062412262)
[2025-02-05 14:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:42][root][INFO] - Training Epoch: 2/2, step 21048/23838 completed (loss: 1.0384736061096191, acc: 0.6756756901741028)
[2025-02-05 14:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:42][root][INFO] - Training Epoch: 2/2, step 21049/23838 completed (loss: 1.605064034461975, acc: 0.5384615659713745)
[2025-02-05 14:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:42][root][INFO] - Training Epoch: 2/2, step 21050/23838 completed (loss: 1.3866392374038696, acc: 0.6363636255264282)
[2025-02-05 14:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:43][root][INFO] - Training Epoch: 2/2, step 21051/23838 completed (loss: 1.1822477579116821, acc: 0.6346153616905212)
[2025-02-05 14:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:43][root][INFO] - Training Epoch: 2/2, step 21052/23838 completed (loss: 1.2291467189788818, acc: 0.6153846383094788)
[2025-02-05 14:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:44][root][INFO] - Training Epoch: 2/2, step 21053/23838 completed (loss: 0.8271206617355347, acc: 0.7319587469100952)
[2025-02-05 14:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:44][root][INFO] - Training Epoch: 2/2, step 21054/23838 completed (loss: 1.1517966985702515, acc: 0.6551724076271057)
[2025-02-05 14:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:44][root][INFO] - Training Epoch: 2/2, step 21055/23838 completed (loss: 1.208562970161438, acc: 0.6666666865348816)
[2025-02-05 14:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:45][root][INFO] - Training Epoch: 2/2, step 21056/23838 completed (loss: 0.930770993232727, acc: 0.6666666865348816)
[2025-02-05 14:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:45][root][INFO] - Training Epoch: 2/2, step 21057/23838 completed (loss: 0.9339430928230286, acc: 0.7195122241973877)
[2025-02-05 14:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:46][root][INFO] - Training Epoch: 2/2, step 21058/23838 completed (loss: 1.1038322448730469, acc: 0.6344085931777954)
[2025-02-05 14:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:46][root][INFO] - Training Epoch: 2/2, step 21059/23838 completed (loss: 0.9539998769760132, acc: 0.7142857313156128)
[2025-02-05 14:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:46][root][INFO] - Training Epoch: 2/2, step 21060/23838 completed (loss: 1.1114988327026367, acc: 0.6623376607894897)
[2025-02-05 14:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:47][root][INFO] - Training Epoch: 2/2, step 21061/23838 completed (loss: 1.1478381156921387, acc: 0.692307710647583)
[2025-02-05 14:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:47][root][INFO] - Training Epoch: 2/2, step 21062/23838 completed (loss: 1.3806582689285278, acc: 0.6376811861991882)
[2025-02-05 14:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:48][root][INFO] - Training Epoch: 2/2, step 21063/23838 completed (loss: 1.1527197360992432, acc: 0.6595744490623474)
[2025-02-05 14:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:48][root][INFO] - Training Epoch: 2/2, step 21064/23838 completed (loss: 0.8665978312492371, acc: 0.7471264600753784)
[2025-02-05 14:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:48][root][INFO] - Training Epoch: 2/2, step 21065/23838 completed (loss: 0.9579879641532898, acc: 0.6727272868156433)
[2025-02-05 14:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:49][root][INFO] - Training Epoch: 2/2, step 21066/23838 completed (loss: 1.1862467527389526, acc: 0.5909090638160706)
[2025-02-05 14:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:49][root][INFO] - Training Epoch: 2/2, step 21067/23838 completed (loss: 1.0954123735427856, acc: 0.6000000238418579)
[2025-02-05 14:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:49][root][INFO] - Training Epoch: 2/2, step 21068/23838 completed (loss: 0.9706037044525146, acc: 0.7083333134651184)
[2025-02-05 14:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:50][root][INFO] - Training Epoch: 2/2, step 21069/23838 completed (loss: 0.8762186169624329, acc: 0.7333333492279053)
[2025-02-05 14:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:50][root][INFO] - Training Epoch: 2/2, step 21070/23838 completed (loss: 1.0381423234939575, acc: 0.7142857313156128)
[2025-02-05 14:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:51][root][INFO] - Training Epoch: 2/2, step 21071/23838 completed (loss: 1.0221227407455444, acc: 0.7226890921592712)
[2025-02-05 14:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:51][root][INFO] - Training Epoch: 2/2, step 21072/23838 completed (loss: 1.0434987545013428, acc: 0.6875)
[2025-02-05 14:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:51][root][INFO] - Training Epoch: 2/2, step 21073/23838 completed (loss: 0.7133145928382874, acc: 0.800000011920929)
[2025-02-05 14:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:52][root][INFO] - Training Epoch: 2/2, step 21074/23838 completed (loss: 0.816916286945343, acc: 0.7105262875556946)
[2025-02-05 14:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:52][root][INFO] - Training Epoch: 2/2, step 21075/23838 completed (loss: 1.3332183361053467, acc: 0.6103895902633667)
[2025-02-05 14:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:53][root][INFO] - Training Epoch: 2/2, step 21076/23838 completed (loss: 0.8481719493865967, acc: 0.7160493731498718)
[2025-02-05 14:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:53][root][INFO] - Training Epoch: 2/2, step 21077/23838 completed (loss: 1.1492940187454224, acc: 0.680672287940979)
[2025-02-05 14:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:53][root][INFO] - Training Epoch: 2/2, step 21078/23838 completed (loss: 1.2385188341140747, acc: 0.6419752836227417)
[2025-02-05 14:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:54][root][INFO] - Training Epoch: 2/2, step 21079/23838 completed (loss: 0.9228392839431763, acc: 0.7303370833396912)
[2025-02-05 14:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:54][root][INFO] - Training Epoch: 2/2, step 21080/23838 completed (loss: 0.9927188754081726, acc: 0.7070707082748413)
[2025-02-05 14:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:54][root][INFO] - Training Epoch: 2/2, step 21081/23838 completed (loss: 0.8083745837211609, acc: 0.7777777910232544)
[2025-02-05 14:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:55][root][INFO] - Training Epoch: 2/2, step 21082/23838 completed (loss: 1.2937326431274414, acc: 0.6129032373428345)
[2025-02-05 14:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:55][root][INFO] - Training Epoch: 2/2, step 21083/23838 completed (loss: 0.9297100305557251, acc: 0.7441860437393188)
[2025-02-05 14:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:56][root][INFO] - Training Epoch: 2/2, step 21084/23838 completed (loss: 0.9571792483329773, acc: 0.7352941036224365)
[2025-02-05 14:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:56][root][INFO] - Training Epoch: 2/2, step 21085/23838 completed (loss: 1.134395956993103, acc: 0.6626505851745605)
[2025-02-05 14:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:56][root][INFO] - Training Epoch: 2/2, step 21086/23838 completed (loss: 1.3010326623916626, acc: 0.640625)
[2025-02-05 14:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:57][root][INFO] - Training Epoch: 2/2, step 21087/23838 completed (loss: 0.9542652368545532, acc: 0.699999988079071)
[2025-02-05 14:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:57][root][INFO] - Training Epoch: 2/2, step 21088/23838 completed (loss: 1.1241228580474854, acc: 0.6117647290229797)
[2025-02-05 14:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:58][root][INFO] - Training Epoch: 2/2, step 21089/23838 completed (loss: 1.2620099782943726, acc: 0.6029411554336548)
[2025-02-05 14:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:58][root][INFO] - Training Epoch: 2/2, step 21090/23838 completed (loss: 0.9446470737457275, acc: 0.71875)
[2025-02-05 14:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:58][root][INFO] - Training Epoch: 2/2, step 21091/23838 completed (loss: 0.7830332517623901, acc: 0.7714285850524902)
[2025-02-05 14:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:59][root][INFO] - Training Epoch: 2/2, step 21092/23838 completed (loss: 0.8851485848426819, acc: 0.7605633735656738)
[2025-02-05 14:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:59][root][INFO] - Training Epoch: 2/2, step 21093/23838 completed (loss: 0.675115704536438, acc: 0.7419354915618896)
[2025-02-05 14:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:00:59][root][INFO] - Training Epoch: 2/2, step 21094/23838 completed (loss: 1.018310308456421, acc: 0.6499999761581421)
[2025-02-05 14:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:00][root][INFO] - Training Epoch: 2/2, step 21095/23838 completed (loss: 1.4650367498397827, acc: 0.6333333253860474)
[2025-02-05 14:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:00][root][INFO] - Training Epoch: 2/2, step 21096/23838 completed (loss: 1.0224997997283936, acc: 0.7435897588729858)
[2025-02-05 14:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:01][root][INFO] - Training Epoch: 2/2, step 21097/23838 completed (loss: 0.9241534471511841, acc: 0.7777777910232544)
[2025-02-05 14:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:01][root][INFO] - Training Epoch: 2/2, step 21098/23838 completed (loss: 1.0399737358093262, acc: 0.7307692170143127)
[2025-02-05 14:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:01][root][INFO] - Training Epoch: 2/2, step 21099/23838 completed (loss: 1.1641643047332764, acc: 0.6944444179534912)
[2025-02-05 14:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:02][root][INFO] - Training Epoch: 2/2, step 21100/23838 completed (loss: 0.9468433856964111, acc: 0.7272727489471436)
[2025-02-05 14:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:02][root][INFO] - Training Epoch: 2/2, step 21101/23838 completed (loss: 0.9111567735671997, acc: 0.7599999904632568)
[2025-02-05 14:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:03][root][INFO] - Training Epoch: 2/2, step 21102/23838 completed (loss: 0.7799680829048157, acc: 0.84375)
[2025-02-05 14:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:03][root][INFO] - Training Epoch: 2/2, step 21103/23838 completed (loss: 0.587113618850708, acc: 0.7857142686843872)
[2025-02-05 14:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:03][root][INFO] - Training Epoch: 2/2, step 21104/23838 completed (loss: 0.7153730392456055, acc: 0.8181818127632141)
[2025-02-05 14:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:04][root][INFO] - Training Epoch: 2/2, step 21105/23838 completed (loss: 0.9702956080436707, acc: 0.7555555701255798)
[2025-02-05 14:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:04][root][INFO] - Training Epoch: 2/2, step 21106/23838 completed (loss: 0.5055643320083618, acc: 0.9230769276618958)
[2025-02-05 14:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:05][root][INFO] - Training Epoch: 2/2, step 21107/23838 completed (loss: 0.6872311234474182, acc: 0.7666666507720947)
[2025-02-05 14:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:05][root][INFO] - Training Epoch: 2/2, step 21108/23838 completed (loss: 1.4494396448135376, acc: 0.6315789222717285)
[2025-02-05 14:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:05][root][INFO] - Training Epoch: 2/2, step 21109/23838 completed (loss: 1.2109078168869019, acc: 0.6333333253860474)
[2025-02-05 14:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:06][root][INFO] - Training Epoch: 2/2, step 21110/23838 completed (loss: 1.1890977621078491, acc: 0.6571428775787354)
[2025-02-05 14:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:06][root][INFO] - Training Epoch: 2/2, step 21111/23838 completed (loss: 1.6775903701782227, acc: 0.5737704634666443)
[2025-02-05 14:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:06][root][INFO] - Training Epoch: 2/2, step 21112/23838 completed (loss: 1.500203251838684, acc: 0.5625)
[2025-02-05 14:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:07][root][INFO] - Training Epoch: 2/2, step 21113/23838 completed (loss: 1.1586288213729858, acc: 0.6315789222717285)
[2025-02-05 14:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:07][root][INFO] - Training Epoch: 2/2, step 21114/23838 completed (loss: 1.0700318813323975, acc: 0.6666666865348816)
[2025-02-05 14:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:08][root][INFO] - Training Epoch: 2/2, step 21115/23838 completed (loss: 1.694942593574524, acc: 0.4920634925365448)
[2025-02-05 14:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:08][root][INFO] - Training Epoch: 2/2, step 21116/23838 completed (loss: 1.6470648050308228, acc: 0.5185185074806213)
[2025-02-05 14:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:08][root][INFO] - Training Epoch: 2/2, step 21117/23838 completed (loss: 1.339171290397644, acc: 0.5909090638160706)
[2025-02-05 14:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:09][root][INFO] - Training Epoch: 2/2, step 21118/23838 completed (loss: 1.191354751586914, acc: 0.6507936716079712)
[2025-02-05 14:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:09][root][INFO] - Training Epoch: 2/2, step 21119/23838 completed (loss: 1.67320716381073, acc: 0.5454545617103577)
[2025-02-05 14:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:10][root][INFO] - Training Epoch: 2/2, step 21120/23838 completed (loss: 0.8771939277648926, acc: 0.739130437374115)
[2025-02-05 14:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:10][root][INFO] - Training Epoch: 2/2, step 21121/23838 completed (loss: 1.7298431396484375, acc: 0.5094339847564697)
[2025-02-05 14:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:10][root][INFO] - Training Epoch: 2/2, step 21122/23838 completed (loss: 1.553362488746643, acc: 0.5416666865348816)
[2025-02-05 14:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:11][root][INFO] - Training Epoch: 2/2, step 21123/23838 completed (loss: 1.358153223991394, acc: 0.6458333134651184)
[2025-02-05 14:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:11][root][INFO] - Training Epoch: 2/2, step 21124/23838 completed (loss: 1.40668785572052, acc: 0.5609756112098694)
[2025-02-05 14:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:12][root][INFO] - Training Epoch: 2/2, step 21125/23838 completed (loss: 1.9467215538024902, acc: 0.43877550959587097)
[2025-02-05 14:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:12][root][INFO] - Training Epoch: 2/2, step 21126/23838 completed (loss: 1.1438113451004028, acc: 0.7333333492279053)
[2025-02-05 14:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:13][root][INFO] - Training Epoch: 2/2, step 21127/23838 completed (loss: 1.985935926437378, acc: 0.40425533056259155)
[2025-02-05 14:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:13][root][INFO] - Training Epoch: 2/2, step 21128/23838 completed (loss: 1.2965021133422852, acc: 0.6595744490623474)
[2025-02-05 14:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:13][root][INFO] - Training Epoch: 2/2, step 21129/23838 completed (loss: 1.4386342763900757, acc: 0.6025640964508057)
[2025-02-05 14:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:14][root][INFO] - Training Epoch: 2/2, step 21130/23838 completed (loss: 1.4682129621505737, acc: 0.5666666626930237)
[2025-02-05 14:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:14][root][INFO] - Training Epoch: 2/2, step 21131/23838 completed (loss: 1.5072402954101562, acc: 0.6000000238418579)
[2025-02-05 14:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:15][root][INFO] - Training Epoch: 2/2, step 21132/23838 completed (loss: 1.2411752939224243, acc: 0.6486486196517944)
[2025-02-05 14:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:15][root][INFO] - Training Epoch: 2/2, step 21133/23838 completed (loss: 1.1822538375854492, acc: 0.6428571343421936)
[2025-02-05 14:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:15][root][INFO] - Training Epoch: 2/2, step 21134/23838 completed (loss: 1.3125349283218384, acc: 0.5698924660682678)
[2025-02-05 14:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:16][root][INFO] - Training Epoch: 2/2, step 21135/23838 completed (loss: 0.9586275815963745, acc: 0.7115384340286255)
[2025-02-05 14:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:16][root][INFO] - Training Epoch: 2/2, step 21136/23838 completed (loss: 1.5769767761230469, acc: 0.5333333611488342)
[2025-02-05 14:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:17][root][INFO] - Training Epoch: 2/2, step 21137/23838 completed (loss: 1.1012498140335083, acc: 0.6111111044883728)
[2025-02-05 14:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:17][root][INFO] - Training Epoch: 2/2, step 21138/23838 completed (loss: 1.6536954641342163, acc: 0.5277777910232544)
[2025-02-05 14:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:17][root][INFO] - Training Epoch: 2/2, step 21139/23838 completed (loss: 1.147334098815918, acc: 0.7285714149475098)
[2025-02-05 14:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:18][root][INFO] - Training Epoch: 2/2, step 21140/23838 completed (loss: 1.1279585361480713, acc: 0.6623376607894897)
[2025-02-05 14:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:18][root][INFO] - Training Epoch: 2/2, step 21141/23838 completed (loss: 1.088878870010376, acc: 0.688524603843689)
[2025-02-05 14:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:19][root][INFO] - Training Epoch: 2/2, step 21142/23838 completed (loss: 0.9409469962120056, acc: 0.7460317611694336)
[2025-02-05 14:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:19][root][INFO] - Training Epoch: 2/2, step 21143/23838 completed (loss: 1.285645842552185, acc: 0.6111111044883728)
[2025-02-05 14:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:20][root][INFO] - Training Epoch: 2/2, step 21144/23838 completed (loss: 1.2588597536087036, acc: 0.6195651888847351)
[2025-02-05 14:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:20][root][INFO] - Training Epoch: 2/2, step 21145/23838 completed (loss: 1.2882617712020874, acc: 0.6176470518112183)
[2025-02-05 14:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:20][root][INFO] - Training Epoch: 2/2, step 21146/23838 completed (loss: 1.6377404928207397, acc: 0.4941176474094391)
[2025-02-05 14:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:21][root][INFO] - Training Epoch: 2/2, step 21147/23838 completed (loss: 1.1713670492172241, acc: 0.7166666388511658)
[2025-02-05 14:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:21][root][INFO] - Training Epoch: 2/2, step 21148/23838 completed (loss: 1.3950886726379395, acc: 0.561904788017273)
[2025-02-05 14:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:22][root][INFO] - Training Epoch: 2/2, step 21149/23838 completed (loss: 0.6657187938690186, acc: 0.7931034564971924)
[2025-02-05 14:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:22][root][INFO] - Training Epoch: 2/2, step 21150/23838 completed (loss: 1.7321668863296509, acc: 0.4590163826942444)
[2025-02-05 14:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:23][root][INFO] - Training Epoch: 2/2, step 21151/23838 completed (loss: 1.2873462438583374, acc: 0.6938775777816772)
[2025-02-05 14:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:23][root][INFO] - Training Epoch: 2/2, step 21152/23838 completed (loss: 1.2253856658935547, acc: 0.625)
[2025-02-05 14:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:24][root][INFO] - Training Epoch: 2/2, step 21153/23838 completed (loss: 1.5090479850769043, acc: 0.5547945499420166)
[2025-02-05 14:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:24][root][INFO] - Training Epoch: 2/2, step 21154/23838 completed (loss: 1.2281901836395264, acc: 0.6547619104385376)
[2025-02-05 14:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:25][root][INFO] - Training Epoch: 2/2, step 21155/23838 completed (loss: 1.3309025764465332, acc: 0.5666666626930237)
[2025-02-05 14:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:25][root][INFO] - Training Epoch: 2/2, step 21156/23838 completed (loss: 1.189881682395935, acc: 0.6603773832321167)
[2025-02-05 14:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:25][root][INFO] - Training Epoch: 2/2, step 21157/23838 completed (loss: 1.3065214157104492, acc: 0.6440678238868713)
[2025-02-05 14:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:26][root][INFO] - Training Epoch: 2/2, step 21158/23838 completed (loss: 1.1639609336853027, acc: 0.6477272510528564)
[2025-02-05 14:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:26][root][INFO] - Training Epoch: 2/2, step 21159/23838 completed (loss: 1.4509705305099487, acc: 0.6301369667053223)
[2025-02-05 14:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:27][root][INFO] - Training Epoch: 2/2, step 21160/23838 completed (loss: 1.2156264781951904, acc: 0.6129032373428345)
[2025-02-05 14:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:27][root][INFO] - Training Epoch: 2/2, step 21161/23838 completed (loss: 1.1524016857147217, acc: 0.6875)
[2025-02-05 14:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:27][root][INFO] - Training Epoch: 2/2, step 21162/23838 completed (loss: 1.241721272468567, acc: 0.6162790656089783)
[2025-02-05 14:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:28][root][INFO] - Training Epoch: 2/2, step 21163/23838 completed (loss: 1.3561756610870361, acc: 0.6024096608161926)
[2025-02-05 14:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:28][root][INFO] - Training Epoch: 2/2, step 21164/23838 completed (loss: 1.5469690561294556, acc: 0.5408163070678711)
[2025-02-05 14:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:29][root][INFO] - Training Epoch: 2/2, step 21165/23838 completed (loss: 0.7585901021957397, acc: 0.7924528121948242)
[2025-02-05 14:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:29][root][INFO] - Training Epoch: 2/2, step 21166/23838 completed (loss: 1.2136447429656982, acc: 0.6438356041908264)
[2025-02-05 14:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:29][root][INFO] - Training Epoch: 2/2, step 21167/23838 completed (loss: 1.4881194829940796, acc: 0.5555555820465088)
[2025-02-05 14:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:30][root][INFO] - Training Epoch: 2/2, step 21168/23838 completed (loss: 1.3530091047286987, acc: 0.6571428775787354)
[2025-02-05 14:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:30][root][INFO] - Training Epoch: 2/2, step 21169/23838 completed (loss: 1.2838572263717651, acc: 0.6800000071525574)
[2025-02-05 14:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:31][root][INFO] - Training Epoch: 2/2, step 21170/23838 completed (loss: 1.0117703676223755, acc: 0.6739130616188049)
[2025-02-05 14:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:31][root][INFO] - Training Epoch: 2/2, step 21171/23838 completed (loss: 1.0632498264312744, acc: 0.6567164063453674)
[2025-02-05 14:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:32][root][INFO] - Training Epoch: 2/2, step 21172/23838 completed (loss: 0.6135995388031006, acc: 0.5882353186607361)
[2025-02-05 14:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:32][root][INFO] - Training Epoch: 2/2, step 21173/23838 completed (loss: 1.2625806331634521, acc: 0.6896551847457886)
[2025-02-05 14:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:32][root][INFO] - Training Epoch: 2/2, step 21174/23838 completed (loss: 0.7872905135154724, acc: 0.7666666507720947)
[2025-02-05 14:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:33][root][INFO] - Training Epoch: 2/2, step 21175/23838 completed (loss: 0.48296815156936646, acc: 0.9090909361839294)
[2025-02-05 14:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:33][root][INFO] - Training Epoch: 2/2, step 21176/23838 completed (loss: 0.7196873426437378, acc: 0.75)
[2025-02-05 14:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:34][root][INFO] - Training Epoch: 2/2, step 21177/23838 completed (loss: 1.5248303413391113, acc: 0.640625)
[2025-02-05 14:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:34][root][INFO] - Training Epoch: 2/2, step 21178/23838 completed (loss: 0.3100452125072479, acc: 0.8500000238418579)
[2025-02-05 14:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:35][root][INFO] - Training Epoch: 2/2, step 21179/23838 completed (loss: 1.2684704065322876, acc: 0.5901639461517334)
[2025-02-05 14:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:35][root][INFO] - Training Epoch: 2/2, step 21180/23838 completed (loss: 1.0628666877746582, acc: 0.6730769276618958)
[2025-02-05 14:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:35][root][INFO] - Training Epoch: 2/2, step 21181/23838 completed (loss: 0.3364076018333435, acc: 0.9285714030265808)
[2025-02-05 14:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:36][root][INFO] - Training Epoch: 2/2, step 21182/23838 completed (loss: 1.1232554912567139, acc: 0.699999988079071)
[2025-02-05 14:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:36][root][INFO] - Training Epoch: 2/2, step 21183/23838 completed (loss: 1.2212531566619873, acc: 0.6419752836227417)
[2025-02-05 14:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:37][root][INFO] - Training Epoch: 2/2, step 21184/23838 completed (loss: 1.4013880491256714, acc: 0.6301369667053223)
[2025-02-05 14:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:37][root][INFO] - Training Epoch: 2/2, step 21185/23838 completed (loss: 1.247968316078186, acc: 0.6315789222717285)
[2025-02-05 14:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:37][root][INFO] - Training Epoch: 2/2, step 21186/23838 completed (loss: 0.25194090604782104, acc: 0.931034505367279)
[2025-02-05 14:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:38][root][INFO] - Training Epoch: 2/2, step 21187/23838 completed (loss: 1.2107418775558472, acc: 0.5769230723381042)
[2025-02-05 14:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:38][root][INFO] - Training Epoch: 2/2, step 21188/23838 completed (loss: 0.7202343344688416, acc: 0.7272727489471436)
[2025-02-05 14:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:39][root][INFO] - Training Epoch: 2/2, step 21189/23838 completed (loss: 1.310890555381775, acc: 0.6052631735801697)
[2025-02-05 14:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:39][root][INFO] - Training Epoch: 2/2, step 21190/23838 completed (loss: 1.1130036115646362, acc: 0.6818181872367859)
[2025-02-05 14:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:39][root][INFO] - Training Epoch: 2/2, step 21191/23838 completed (loss: 1.65095055103302, acc: 0.557692289352417)
[2025-02-05 14:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:40][root][INFO] - Training Epoch: 2/2, step 21192/23838 completed (loss: 1.4131473302841187, acc: 0.574999988079071)
[2025-02-05 14:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:40][root][INFO] - Training Epoch: 2/2, step 21193/23838 completed (loss: 1.114278793334961, acc: 0.6590909361839294)
[2025-02-05 14:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:41][root][INFO] - Training Epoch: 2/2, step 21194/23838 completed (loss: 0.5014985203742981, acc: 0.8235294222831726)
[2025-02-05 14:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:41][root][INFO] - Training Epoch: 2/2, step 21195/23838 completed (loss: 1.5190342664718628, acc: 0.5199999809265137)
[2025-02-05 14:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:41][root][INFO] - Training Epoch: 2/2, step 21196/23838 completed (loss: 0.5082681775093079, acc: 0.8636363744735718)
[2025-02-05 14:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:42][root][INFO] - Training Epoch: 2/2, step 21197/23838 completed (loss: 1.3645546436309814, acc: 0.6086956262588501)
[2025-02-05 14:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:42][root][INFO] - Training Epoch: 2/2, step 21198/23838 completed (loss: 0.6023078560829163, acc: 0.7567567825317383)
[2025-02-05 14:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:43][root][INFO] - Training Epoch: 2/2, step 21199/23838 completed (loss: 1.0609469413757324, acc: 0.6875)
[2025-02-05 14:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:43][root][INFO] - Training Epoch: 2/2, step 21200/23838 completed (loss: 0.6195139288902283, acc: 0.8333333134651184)
[2025-02-05 14:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:43][root][INFO] - Training Epoch: 2/2, step 21201/23838 completed (loss: 0.497732549905777, acc: 0.8965517282485962)
[2025-02-05 14:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:44][root][INFO] - Training Epoch: 2/2, step 21202/23838 completed (loss: 0.6473028063774109, acc: 0.8333333134651184)
[2025-02-05 14:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:44][root][INFO] - Training Epoch: 2/2, step 21203/23838 completed (loss: 1.5409015417099, acc: 0.5660377144813538)
[2025-02-05 14:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:45][root][INFO] - Training Epoch: 2/2, step 21204/23838 completed (loss: 0.9494094252586365, acc: 0.7142857313156128)
[2025-02-05 14:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:45][root][INFO] - Training Epoch: 2/2, step 21205/23838 completed (loss: 1.0225495100021362, acc: 0.6666666865348816)
[2025-02-05 14:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:46][root][INFO] - Training Epoch: 2/2, step 21206/23838 completed (loss: 0.8619719743728638, acc: 0.7096773982048035)
[2025-02-05 14:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:46][root][INFO] - Training Epoch: 2/2, step 21207/23838 completed (loss: 0.6829837560653687, acc: 0.7843137383460999)
[2025-02-05 14:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:47][root][INFO] - Training Epoch: 2/2, step 21208/23838 completed (loss: 0.4576863646507263, acc: 0.8947368264198303)
[2025-02-05 14:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:47][root][INFO] - Training Epoch: 2/2, step 21209/23838 completed (loss: 1.5382115840911865, acc: 0.5423728823661804)
[2025-02-05 14:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:47][root][INFO] - Training Epoch: 2/2, step 21210/23838 completed (loss: 0.769134521484375, acc: 0.8048780560493469)
[2025-02-05 14:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:48][root][INFO] - Training Epoch: 2/2, step 21211/23838 completed (loss: 1.1115846633911133, acc: 0.6829268336296082)
[2025-02-05 14:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:48][root][INFO] - Training Epoch: 2/2, step 21212/23838 completed (loss: 1.1277259588241577, acc: 0.6153846383094788)
[2025-02-05 14:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:49][root][INFO] - Training Epoch: 2/2, step 21213/23838 completed (loss: 1.4814507961273193, acc: 0.5853658318519592)
[2025-02-05 14:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:49][root][INFO] - Training Epoch: 2/2, step 21214/23838 completed (loss: 0.9052855372428894, acc: 0.6428571343421936)
[2025-02-05 14:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:49][root][INFO] - Training Epoch: 2/2, step 21215/23838 completed (loss: 0.6417326331138611, acc: 0.8125)
[2025-02-05 14:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:50][root][INFO] - Training Epoch: 2/2, step 21216/23838 completed (loss: 0.6289744973182678, acc: 0.7872340679168701)
[2025-02-05 14:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:50][root][INFO] - Training Epoch: 2/2, step 21217/23838 completed (loss: 1.448189377784729, acc: 0.5696202516555786)
[2025-02-05 14:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:51][root][INFO] - Training Epoch: 2/2, step 21218/23838 completed (loss: 1.2970383167266846, acc: 0.6190476417541504)
[2025-02-05 14:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:51][root][INFO] - Training Epoch: 2/2, step 21219/23838 completed (loss: 1.2737585306167603, acc: 0.6666666865348816)
[2025-02-05 14:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:51][root][INFO] - Training Epoch: 2/2, step 21220/23838 completed (loss: 0.988331139087677, acc: 0.707317054271698)
[2025-02-05 14:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:52][root][INFO] - Training Epoch: 2/2, step 21221/23838 completed (loss: 1.4198392629623413, acc: 0.6339285969734192)
[2025-02-05 14:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:52][root][INFO] - Training Epoch: 2/2, step 21222/23838 completed (loss: 1.2456591129302979, acc: 0.6451612710952759)
[2025-02-05 14:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:53][root][INFO] - Training Epoch: 2/2, step 21223/23838 completed (loss: 0.9334292411804199, acc: 0.7021276354789734)
[2025-02-05 14:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:53][root][INFO] - Training Epoch: 2/2, step 21224/23838 completed (loss: 1.0054110288619995, acc: 0.6972476840019226)
[2025-02-05 14:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:53][root][INFO] - Training Epoch: 2/2, step 21225/23838 completed (loss: 1.4063026905059814, acc: 0.6159999966621399)
[2025-02-05 14:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:54][root][INFO] - Training Epoch: 2/2, step 21226/23838 completed (loss: 1.488942265510559, acc: 0.568965494632721)
[2025-02-05 14:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:54][root][INFO] - Training Epoch: 2/2, step 21227/23838 completed (loss: 1.3170310258865356, acc: 0.6269230842590332)
[2025-02-05 14:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:55][root][INFO] - Training Epoch: 2/2, step 21228/23838 completed (loss: 1.47134530544281, acc: 0.5688073635101318)
[2025-02-05 14:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:55][root][INFO] - Training Epoch: 2/2, step 21229/23838 completed (loss: 1.1234158277511597, acc: 0.662162184715271)
[2025-02-05 14:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:55][root][INFO] - Training Epoch: 2/2, step 21230/23838 completed (loss: 1.0332847833633423, acc: 0.6982758641242981)
[2025-02-05 14:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:56][root][INFO] - Training Epoch: 2/2, step 21231/23838 completed (loss: 0.8127678036689758, acc: 0.7647058963775635)
[2025-02-05 14:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:56][root][INFO] - Training Epoch: 2/2, step 21232/23838 completed (loss: 0.8671904802322388, acc: 0.6883116960525513)
[2025-02-05 14:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:56][root][INFO] - Training Epoch: 2/2, step 21233/23838 completed (loss: 0.8189704418182373, acc: 0.7545454502105713)
[2025-02-05 14:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:57][root][INFO] - Training Epoch: 2/2, step 21234/23838 completed (loss: 0.9153625965118408, acc: 0.7388059496879578)
[2025-02-05 14:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:57][root][INFO] - Training Epoch: 2/2, step 21235/23838 completed (loss: 0.9595323204994202, acc: 0.7252747416496277)
[2025-02-05 14:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:57][root][INFO] - Training Epoch: 2/2, step 21236/23838 completed (loss: 1.2349873781204224, acc: 0.6979166865348816)
[2025-02-05 14:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:58][root][INFO] - Training Epoch: 2/2, step 21237/23838 completed (loss: 1.1474705934524536, acc: 0.6424242258071899)
[2025-02-05 14:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:58][root][INFO] - Training Epoch: 2/2, step 21238/23838 completed (loss: 1.135642409324646, acc: 0.6371681690216064)
[2025-02-05 14:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:59][root][INFO] - Training Epoch: 2/2, step 21239/23838 completed (loss: 1.4908034801483154, acc: 0.5)
[2025-02-05 14:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:01:59][root][INFO] - Training Epoch: 2/2, step 21240/23838 completed (loss: 1.0595165491104126, acc: 0.6697247624397278)
[2025-02-05 14:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:00][root][INFO] - Training Epoch: 2/2, step 21241/23838 completed (loss: 1.1444143056869507, acc: 0.692307710647583)
[2025-02-05 14:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:00][root][INFO] - Training Epoch: 2/2, step 21242/23838 completed (loss: 1.1032217741012573, acc: 0.6870229244232178)
[2025-02-05 14:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:00][root][INFO] - Training Epoch: 2/2, step 21243/23838 completed (loss: 0.8851553797721863, acc: 0.7222222089767456)
[2025-02-05 14:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:01][root][INFO] - Training Epoch: 2/2, step 21244/23838 completed (loss: 0.9261447787284851, acc: 0.7191011309623718)
[2025-02-05 14:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:01][root][INFO] - Training Epoch: 2/2, step 21245/23838 completed (loss: 1.1270027160644531, acc: 0.671875)
[2025-02-05 14:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:01][root][INFO] - Training Epoch: 2/2, step 21246/23838 completed (loss: 0.9984358549118042, acc: 0.7384615540504456)
[2025-02-05 14:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:02][root][INFO] - Training Epoch: 2/2, step 21247/23838 completed (loss: 0.9626125693321228, acc: 0.7207207083702087)
[2025-02-05 14:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:02][root][INFO] - Training Epoch: 2/2, step 21248/23838 completed (loss: 1.161271333694458, acc: 0.6037735939025879)
[2025-02-05 14:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:03][root][INFO] - Training Epoch: 2/2, step 21249/23838 completed (loss: 0.8412556052207947, acc: 0.7631579041481018)
[2025-02-05 14:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:03][root][INFO] - Training Epoch: 2/2, step 21250/23838 completed (loss: 1.304382562637329, acc: 0.6419752836227417)
[2025-02-05 14:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:03][root][INFO] - Training Epoch: 2/2, step 21251/23838 completed (loss: 0.9937583208084106, acc: 0.75)
[2025-02-05 14:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:04][root][INFO] - Training Epoch: 2/2, step 21252/23838 completed (loss: 0.8654952645301819, acc: 0.7627118825912476)
[2025-02-05 14:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:04][root][INFO] - Training Epoch: 2/2, step 21253/23838 completed (loss: 0.9131850004196167, acc: 0.7042253613471985)
[2025-02-05 14:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:05][root][INFO] - Training Epoch: 2/2, step 21254/23838 completed (loss: 1.0302151441574097, acc: 0.7120000123977661)
[2025-02-05 14:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:05][root][INFO] - Training Epoch: 2/2, step 21255/23838 completed (loss: 0.9904627203941345, acc: 0.7160493731498718)
[2025-02-05 14:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:06][root][INFO] - Training Epoch: 2/2, step 21256/23838 completed (loss: 1.4481115341186523, acc: 0.6339285969734192)
[2025-02-05 14:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:06][root][INFO] - Training Epoch: 2/2, step 21257/23838 completed (loss: 0.9029392600059509, acc: 0.7472527623176575)
[2025-02-05 14:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:07][root][INFO] - Training Epoch: 2/2, step 21258/23838 completed (loss: 1.0704312324523926, acc: 0.6499999761581421)
[2025-02-05 14:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:07][root][INFO] - Training Epoch: 2/2, step 21259/23838 completed (loss: 0.8703264594078064, acc: 0.7164179086685181)
[2025-02-05 14:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:07][root][INFO] - Training Epoch: 2/2, step 21260/23838 completed (loss: 1.0133204460144043, acc: 0.7017543911933899)
[2025-02-05 14:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:08][root][INFO] - Training Epoch: 2/2, step 21261/23838 completed (loss: 1.0227693319320679, acc: 0.6818181872367859)
[2025-02-05 14:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:08][root][INFO] - Training Epoch: 2/2, step 21262/23838 completed (loss: 0.7182257771492004, acc: 0.8085106611251831)
[2025-02-05 14:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:09][root][INFO] - Training Epoch: 2/2, step 21263/23838 completed (loss: 0.9128401279449463, acc: 0.7226277589797974)
[2025-02-05 14:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:09][root][INFO] - Training Epoch: 2/2, step 21264/23838 completed (loss: 0.772628664970398, acc: 0.7761194109916687)
[2025-02-05 14:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:10][root][INFO] - Training Epoch: 2/2, step 21265/23838 completed (loss: 0.7947295904159546, acc: 0.7979797720909119)
[2025-02-05 14:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:10][root][INFO] - Training Epoch: 2/2, step 21266/23838 completed (loss: 1.0387262105941772, acc: 0.739130437374115)
[2025-02-05 14:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:11][root][INFO] - Training Epoch: 2/2, step 21267/23838 completed (loss: 1.0469971895217896, acc: 0.6976743936538696)
[2025-02-05 14:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:12][root][INFO] - Training Epoch: 2/2, step 21268/23838 completed (loss: 1.095515251159668, acc: 0.7008547186851501)
[2025-02-05 14:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:12][root][INFO] - Training Epoch: 2/2, step 21269/23838 completed (loss: 1.1303999423980713, acc: 0.6491228342056274)
[2025-02-05 14:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:13][root][INFO] - Training Epoch: 2/2, step 21270/23838 completed (loss: 0.9052538275718689, acc: 0.7321428656578064)
[2025-02-05 14:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:14][root][INFO] - Training Epoch: 2/2, step 21271/23838 completed (loss: 1.6472182273864746, acc: 0.5344827771186829)
[2025-02-05 14:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:14][root][INFO] - Training Epoch: 2/2, step 21272/23838 completed (loss: 1.1548243761062622, acc: 0.644444465637207)
[2025-02-05 14:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:14][root][INFO] - Training Epoch: 2/2, step 21273/23838 completed (loss: 1.2803161144256592, acc: 0.6086956262588501)
[2025-02-05 14:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:15][root][INFO] - Training Epoch: 2/2, step 21274/23838 completed (loss: 1.2782152891159058, acc: 0.6805555820465088)
[2025-02-05 14:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:15][root][INFO] - Training Epoch: 2/2, step 21275/23838 completed (loss: 0.9352413415908813, acc: 0.6835442781448364)
[2025-02-05 14:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:16][root][INFO] - Training Epoch: 2/2, step 21276/23838 completed (loss: 1.161972165107727, acc: 0.6481481194496155)
[2025-02-05 14:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:16][root][INFO] - Training Epoch: 2/2, step 21277/23838 completed (loss: 1.5157760381698608, acc: 0.5272727012634277)
[2025-02-05 14:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:16][root][INFO] - Training Epoch: 2/2, step 21278/23838 completed (loss: 0.6707549095153809, acc: 0.625)
[2025-02-05 14:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:17][root][INFO] - Training Epoch: 2/2, step 21279/23838 completed (loss: 2.522688150405884, acc: 0.0)
[2025-02-05 14:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:17][root][INFO] - Training Epoch: 2/2, step 21280/23838 completed (loss: 0.4747411906719208, acc: 0.8888888955116272)
[2025-02-05 14:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:17][root][INFO] - Training Epoch: 2/2, step 21281/23838 completed (loss: 0.3113960027694702, acc: 1.0)
[2025-02-05 14:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:18][root][INFO] - Training Epoch: 2/2, step 21282/23838 completed (loss: 0.5867717266082764, acc: 0.8055555820465088)
[2025-02-05 14:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:18][root][INFO] - Training Epoch: 2/2, step 21283/23838 completed (loss: 0.9759461879730225, acc: 0.7407407164573669)
[2025-02-05 14:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:19][root][INFO] - Training Epoch: 2/2, step 21284/23838 completed (loss: 0.6875334978103638, acc: 0.8235294222831726)
[2025-02-05 14:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:19][root][INFO] - Training Epoch: 2/2, step 21285/23838 completed (loss: 0.6805985569953918, acc: 0.7982456088066101)
[2025-02-05 14:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:19][root][INFO] - Training Epoch: 2/2, step 21286/23838 completed (loss: 0.6246429681777954, acc: 0.8271604776382446)
[2025-02-05 14:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:20][root][INFO] - Training Epoch: 2/2, step 21287/23838 completed (loss: 0.5616747736930847, acc: 0.8468468189239502)
[2025-02-05 14:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:20][root][INFO] - Training Epoch: 2/2, step 21288/23838 completed (loss: 0.8747723698616028, acc: 0.717391312122345)
[2025-02-05 14:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:21][root][INFO] - Training Epoch: 2/2, step 21289/23838 completed (loss: 0.8030399084091187, acc: 0.746268630027771)
[2025-02-05 14:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:21][root][INFO] - Training Epoch: 2/2, step 21290/23838 completed (loss: 0.5650731921195984, acc: 0.8615384697914124)
[2025-02-05 14:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:21][root][INFO] - Training Epoch: 2/2, step 21291/23838 completed (loss: 1.0000544786453247, acc: 0.7532467246055603)
[2025-02-05 14:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:22][root][INFO] - Training Epoch: 2/2, step 21292/23838 completed (loss: 0.9722605347633362, acc: 0.6753246784210205)
[2025-02-05 14:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:22][root][INFO] - Training Epoch: 2/2, step 21293/23838 completed (loss: 1.0534942150115967, acc: 0.7066666483879089)
[2025-02-05 14:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:23][root][INFO] - Training Epoch: 2/2, step 21294/23838 completed (loss: 0.6591991186141968, acc: 0.800000011920929)
[2025-02-05 14:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:23][root][INFO] - Training Epoch: 2/2, step 21295/23838 completed (loss: 1.009780764579773, acc: 0.6769230961799622)
[2025-02-05 14:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:23][root][INFO] - Training Epoch: 2/2, step 21296/23838 completed (loss: 1.149865984916687, acc: 0.6842105388641357)
[2025-02-05 14:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:24][root][INFO] - Training Epoch: 2/2, step 21297/23838 completed (loss: 0.7325978875160217, acc: 0.7551020383834839)
[2025-02-05 14:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:24][root][INFO] - Training Epoch: 2/2, step 21298/23838 completed (loss: 1.267817735671997, acc: 0.6904761791229248)
[2025-02-05 14:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:25][root][INFO] - Training Epoch: 2/2, step 21299/23838 completed (loss: 0.9562243819236755, acc: 0.7441860437393188)
[2025-02-05 14:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:25][root][INFO] - Training Epoch: 2/2, step 21300/23838 completed (loss: 1.1503437757492065, acc: 0.6333333253860474)
[2025-02-05 14:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:25][root][INFO] - Training Epoch: 2/2, step 21301/23838 completed (loss: 0.9550058841705322, acc: 0.6764705777168274)
[2025-02-05 14:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:26][root][INFO] - Training Epoch: 2/2, step 21302/23838 completed (loss: 1.1185519695281982, acc: 0.7105262875556946)
[2025-02-05 14:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:26][root][INFO] - Training Epoch: 2/2, step 21303/23838 completed (loss: 1.2247295379638672, acc: 0.6351351141929626)
[2025-02-05 14:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:26][root][INFO] - Training Epoch: 2/2, step 21304/23838 completed (loss: 1.422661304473877, acc: 0.6233766078948975)
[2025-02-05 14:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:27][root][INFO] - Training Epoch: 2/2, step 21305/23838 completed (loss: 0.7692782878875732, acc: 0.7846153974533081)
[2025-02-05 14:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:27][root][INFO] - Training Epoch: 2/2, step 21306/23838 completed (loss: 0.796851634979248, acc: 0.7631579041481018)
[2025-02-05 14:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:27][root][INFO] - Training Epoch: 2/2, step 21307/23838 completed (loss: 1.372692584991455, acc: 0.649350643157959)
[2025-02-05 14:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:28][root][INFO] - Training Epoch: 2/2, step 21308/23838 completed (loss: 0.9304060935974121, acc: 0.7264150977134705)
[2025-02-05 14:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:28][root][INFO] - Training Epoch: 2/2, step 21309/23838 completed (loss: 0.9751865863800049, acc: 0.7555555701255798)
[2025-02-05 14:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:29][root][INFO] - Training Epoch: 2/2, step 21310/23838 completed (loss: 1.5690028667449951, acc: 0.6172839403152466)
[2025-02-05 14:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:29][root][INFO] - Training Epoch: 2/2, step 21311/23838 completed (loss: 0.8453958630561829, acc: 0.78125)
[2025-02-05 14:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:29][root][INFO] - Training Epoch: 2/2, step 21312/23838 completed (loss: 0.8760814070701599, acc: 0.7058823704719543)
[2025-02-05 14:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:30][root][INFO] - Training Epoch: 2/2, step 21313/23838 completed (loss: 1.0511040687561035, acc: 0.699999988079071)
[2025-02-05 14:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:30][root][INFO] - Training Epoch: 2/2, step 21314/23838 completed (loss: 0.8737758994102478, acc: 0.7631579041481018)
[2025-02-05 14:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:30][root][INFO] - Training Epoch: 2/2, step 21315/23838 completed (loss: 1.161355972290039, acc: 0.7111111283302307)
[2025-02-05 14:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:31][root][INFO] - Training Epoch: 2/2, step 21316/23838 completed (loss: 0.8076925277709961, acc: 0.7272727489471436)
[2025-02-05 14:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:31][root][INFO] - Training Epoch: 2/2, step 21317/23838 completed (loss: 1.0630302429199219, acc: 0.6212121248245239)
[2025-02-05 14:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:32][root][INFO] - Training Epoch: 2/2, step 21318/23838 completed (loss: 0.9995458722114563, acc: 0.7017543911933899)
[2025-02-05 14:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:32][root][INFO] - Training Epoch: 2/2, step 21319/23838 completed (loss: 1.091400384902954, acc: 0.6557376980781555)
[2025-02-05 14:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:32][root][INFO] - Training Epoch: 2/2, step 21320/23838 completed (loss: 1.163484811782837, acc: 0.6000000238418579)
[2025-02-05 14:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:33][root][INFO] - Training Epoch: 2/2, step 21321/23838 completed (loss: 1.091310977935791, acc: 0.6951219439506531)
[2025-02-05 14:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:33][root][INFO] - Training Epoch: 2/2, step 21322/23838 completed (loss: 1.3360731601715088, acc: 0.6363636255264282)
[2025-02-05 14:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:33][root][INFO] - Training Epoch: 2/2, step 21323/23838 completed (loss: 0.9462690353393555, acc: 0.7659574747085571)
[2025-02-05 14:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:34][root][INFO] - Training Epoch: 2/2, step 21324/23838 completed (loss: 1.0998018980026245, acc: 0.6307692527770996)
[2025-02-05 14:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:34][root][INFO] - Training Epoch: 2/2, step 21325/23838 completed (loss: 1.2697336673736572, acc: 0.6000000238418579)
[2025-02-05 14:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:35][root][INFO] - Training Epoch: 2/2, step 21326/23838 completed (loss: 0.7866882681846619, acc: 0.6774193644523621)
[2025-02-05 14:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:35][root][INFO] - Training Epoch: 2/2, step 21327/23838 completed (loss: 1.1488007307052612, acc: 0.6190476417541504)
[2025-02-05 14:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:35][root][INFO] - Training Epoch: 2/2, step 21328/23838 completed (loss: 0.9032767415046692, acc: 0.8095238208770752)
[2025-02-05 14:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:36][root][INFO] - Training Epoch: 2/2, step 21329/23838 completed (loss: 1.11455500125885, acc: 0.6808510422706604)
[2025-02-05 14:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:36][root][INFO] - Training Epoch: 2/2, step 21330/23838 completed (loss: 0.9264941215515137, acc: 0.699999988079071)
[2025-02-05 14:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:36][root][INFO] - Training Epoch: 2/2, step 21331/23838 completed (loss: 0.9011117219924927, acc: 0.7538461685180664)
[2025-02-05 14:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:37][root][INFO] - Training Epoch: 2/2, step 21332/23838 completed (loss: 1.0641769170761108, acc: 0.6307692527770996)
[2025-02-05 14:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:37][root][INFO] - Training Epoch: 2/2, step 21333/23838 completed (loss: 1.4044471979141235, acc: 0.6279069781303406)
[2025-02-05 14:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:37][root][INFO] - Training Epoch: 2/2, step 21334/23838 completed (loss: 1.0958309173583984, acc: 0.7333333492279053)
[2025-02-05 14:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:38][root][INFO] - Training Epoch: 2/2, step 21335/23838 completed (loss: 0.87507164478302, acc: 0.782608687877655)
[2025-02-05 14:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:38][root][INFO] - Training Epoch: 2/2, step 21336/23838 completed (loss: 1.0023397207260132, acc: 0.6888889074325562)
[2025-02-05 14:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:39][root][INFO] - Training Epoch: 2/2, step 21337/23838 completed (loss: 1.360666036605835, acc: 0.5833333134651184)
[2025-02-05 14:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:39][root][INFO] - Training Epoch: 2/2, step 21338/23838 completed (loss: 1.490261197090149, acc: 0.6212121248245239)
[2025-02-05 14:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:39][root][INFO] - Training Epoch: 2/2, step 21339/23838 completed (loss: 1.4844902753829956, acc: 0.5869565010070801)
[2025-02-05 14:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:40][root][INFO] - Training Epoch: 2/2, step 21340/23838 completed (loss: 1.2928112745285034, acc: 0.5945945978164673)
[2025-02-05 14:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:40][root][INFO] - Training Epoch: 2/2, step 21341/23838 completed (loss: 1.2226171493530273, acc: 0.625)
[2025-02-05 14:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:41][root][INFO] - Training Epoch: 2/2, step 21342/23838 completed (loss: 1.4187506437301636, acc: 0.6282051205635071)
[2025-02-05 14:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:41][root][INFO] - Training Epoch: 2/2, step 21343/23838 completed (loss: 1.387399435043335, acc: 0.6428571343421936)
[2025-02-05 14:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:42][root][INFO] - Training Epoch: 2/2, step 21344/23838 completed (loss: 1.5884387493133545, acc: 0.53125)
[2025-02-05 14:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:42][root][INFO] - Training Epoch: 2/2, step 21345/23838 completed (loss: 1.1756714582443237, acc: 0.699999988079071)
[2025-02-05 14:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:42][root][INFO] - Training Epoch: 2/2, step 21346/23838 completed (loss: 1.376189112663269, acc: 0.6412213444709778)
[2025-02-05 14:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:43][root][INFO] - Training Epoch: 2/2, step 21347/23838 completed (loss: 1.517534852027893, acc: 0.5942028760910034)
[2025-02-05 14:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:43][root][INFO] - Training Epoch: 2/2, step 21348/23838 completed (loss: 1.4789979457855225, acc: 0.6086956262588501)
[2025-02-05 14:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:43][root][INFO] - Training Epoch: 2/2, step 21349/23838 completed (loss: 1.1715959310531616, acc: 0.6960784196853638)
[2025-02-05 14:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:44][root][INFO] - Training Epoch: 2/2, step 21350/23838 completed (loss: 1.1756317615509033, acc: 0.6666666865348816)
[2025-02-05 14:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:44][root][INFO] - Training Epoch: 2/2, step 21351/23838 completed (loss: 1.1898399591445923, acc: 0.6666666865348816)
[2025-02-05 14:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:44][root][INFO] - Training Epoch: 2/2, step 21352/23838 completed (loss: 0.9790602922439575, acc: 0.761904776096344)
[2025-02-05 14:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:45][root][INFO] - Training Epoch: 2/2, step 21353/23838 completed (loss: 0.8647257685661316, acc: 0.8064516186714172)
[2025-02-05 14:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:45][root][INFO] - Training Epoch: 2/2, step 21354/23838 completed (loss: 1.5243734121322632, acc: 0.578125)
[2025-02-05 14:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:46][root][INFO] - Training Epoch: 2/2, step 21355/23838 completed (loss: 1.190476894378662, acc: 0.6363636255264282)
[2025-02-05 14:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:46][root][INFO] - Training Epoch: 2/2, step 21356/23838 completed (loss: 1.381546139717102, acc: 0.6091954112052917)
[2025-02-05 14:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:46][root][INFO] - Training Epoch: 2/2, step 21357/23838 completed (loss: 1.039197325706482, acc: 0.7142857313156128)
[2025-02-05 14:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:47][root][INFO] - Training Epoch: 2/2, step 21358/23838 completed (loss: 0.5989403128623962, acc: 0.8518518805503845)
[2025-02-05 14:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:47][root][INFO] - Training Epoch: 2/2, step 21359/23838 completed (loss: 1.0017836093902588, acc: 0.7692307829856873)
[2025-02-05 14:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:48][root][INFO] - Training Epoch: 2/2, step 21360/23838 completed (loss: 0.8927452564239502, acc: 0.8108108043670654)
[2025-02-05 14:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:48][root][INFO] - Training Epoch: 2/2, step 21361/23838 completed (loss: 0.9062511920928955, acc: 0.7115384340286255)
[2025-02-05 14:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:48][root][INFO] - Training Epoch: 2/2, step 21362/23838 completed (loss: 0.8954570293426514, acc: 0.7368420958518982)
[2025-02-05 14:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:49][root][INFO] - Training Epoch: 2/2, step 21363/23838 completed (loss: 1.2100497484207153, acc: 0.71875)
[2025-02-05 14:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:49][root][INFO] - Training Epoch: 2/2, step 21364/23838 completed (loss: 0.710672914981842, acc: 0.8461538553237915)
[2025-02-05 14:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:49][root][INFO] - Training Epoch: 2/2, step 21365/23838 completed (loss: 0.9540112614631653, acc: 0.782608687877655)
[2025-02-05 14:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:50][root][INFO] - Training Epoch: 2/2, step 21366/23838 completed (loss: 0.9745004773139954, acc: 0.7250000238418579)
[2025-02-05 14:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:50][root][INFO] - Training Epoch: 2/2, step 21367/23838 completed (loss: 1.089058756828308, acc: 0.6935483813285828)
[2025-02-05 14:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:51][root][INFO] - Training Epoch: 2/2, step 21368/23838 completed (loss: 1.3323938846588135, acc: 0.6363636255264282)
[2025-02-05 14:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:51][root][INFO] - Training Epoch: 2/2, step 21369/23838 completed (loss: 1.3224267959594727, acc: 0.5909090638160706)
[2025-02-05 14:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:51][root][INFO] - Training Epoch: 2/2, step 21370/23838 completed (loss: 1.0584172010421753, acc: 0.7121211886405945)
[2025-02-05 14:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:52][root][INFO] - Training Epoch: 2/2, step 21371/23838 completed (loss: 1.2710775136947632, acc: 0.6052631735801697)
[2025-02-05 14:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:52][root][INFO] - Training Epoch: 2/2, step 21372/23838 completed (loss: 0.9996197819709778, acc: 0.7111111283302307)
[2025-02-05 14:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:53][root][INFO] - Training Epoch: 2/2, step 21373/23838 completed (loss: 0.8180603384971619, acc: 0.7234042286872864)
[2025-02-05 14:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:53][root][INFO] - Training Epoch: 2/2, step 21374/23838 completed (loss: 1.2006374597549438, acc: 0.6428571343421936)
[2025-02-05 14:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:53][root][INFO] - Training Epoch: 2/2, step 21375/23838 completed (loss: 0.8514915108680725, acc: 0.7708333134651184)
[2025-02-05 14:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:54][root][INFO] - Training Epoch: 2/2, step 21376/23838 completed (loss: 0.6317577958106995, acc: 0.8461538553237915)
[2025-02-05 14:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:54][root][INFO] - Training Epoch: 2/2, step 21377/23838 completed (loss: 1.3016071319580078, acc: 0.5744680762290955)
[2025-02-05 14:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:55][root][INFO] - Training Epoch: 2/2, step 21378/23838 completed (loss: 1.4194401502609253, acc: 0.621052622795105)
[2025-02-05 14:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:55][root][INFO] - Training Epoch: 2/2, step 21379/23838 completed (loss: 0.9006446599960327, acc: 0.8108108043670654)
[2025-02-05 14:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:55][root][INFO] - Training Epoch: 2/2, step 21380/23838 completed (loss: 1.0544788837432861, acc: 0.7142857313156128)
[2025-02-05 14:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:56][root][INFO] - Training Epoch: 2/2, step 21381/23838 completed (loss: 0.6809602975845337, acc: 0.8307692408561707)
[2025-02-05 14:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:56][root][INFO] - Training Epoch: 2/2, step 21382/23838 completed (loss: 0.8352167010307312, acc: 0.7727272510528564)
[2025-02-05 14:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:57][root][INFO] - Training Epoch: 2/2, step 21383/23838 completed (loss: 1.135580062866211, acc: 0.7346938848495483)
[2025-02-05 14:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:57][root][INFO] - Training Epoch: 2/2, step 21384/23838 completed (loss: 0.7479106783866882, acc: 0.8500000238418579)
[2025-02-05 14:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:57][root][INFO] - Training Epoch: 2/2, step 21385/23838 completed (loss: 0.8411300182342529, acc: 0.774193525314331)
[2025-02-05 14:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:58][root][INFO] - Training Epoch: 2/2, step 21386/23838 completed (loss: 0.39220812916755676, acc: 0.8695651888847351)
[2025-02-05 14:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:58][root][INFO] - Training Epoch: 2/2, step 21387/23838 completed (loss: 1.4151010513305664, acc: 0.6521739363670349)
[2025-02-05 14:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:59][root][INFO] - Training Epoch: 2/2, step 21388/23838 completed (loss: 0.6596085429191589, acc: 0.8372092843055725)
[2025-02-05 14:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:59][root][INFO] - Training Epoch: 2/2, step 21389/23838 completed (loss: 0.6925750970840454, acc: 0.800000011920929)
[2025-02-05 14:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:02:59][root][INFO] - Training Epoch: 2/2, step 21390/23838 completed (loss: 0.6187894344329834, acc: 0.8333333134651184)
[2025-02-05 14:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:00][root][INFO] - Training Epoch: 2/2, step 21391/23838 completed (loss: 0.8931911587715149, acc: 0.7631579041481018)
[2025-02-05 14:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:00][root][INFO] - Training Epoch: 2/2, step 21392/23838 completed (loss: 0.6916752457618713, acc: 0.841269850730896)
[2025-02-05 14:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:01][root][INFO] - Training Epoch: 2/2, step 21393/23838 completed (loss: 1.2263950109481812, acc: 0.6808510422706604)
[2025-02-05 14:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:01][root][INFO] - Training Epoch: 2/2, step 21394/23838 completed (loss: 0.22716571390628815, acc: 0.931034505367279)
[2025-02-05 14:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:01][root][INFO] - Training Epoch: 2/2, step 21395/23838 completed (loss: 0.9381499290466309, acc: 0.75)
[2025-02-05 14:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:02][root][INFO] - Training Epoch: 2/2, step 21396/23838 completed (loss: 1.1919387578964233, acc: 0.6896551847457886)
[2025-02-05 14:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:02][root][INFO] - Training Epoch: 2/2, step 21397/23838 completed (loss: 1.7327004671096802, acc: 0.529411792755127)
[2025-02-05 14:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:02][root][INFO] - Training Epoch: 2/2, step 21398/23838 completed (loss: 1.1379647254943848, acc: 0.6730769276618958)
[2025-02-05 14:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:03][root][INFO] - Training Epoch: 2/2, step 21399/23838 completed (loss: 0.6954971551895142, acc: 0.7441860437393188)
[2025-02-05 14:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:03][root][INFO] - Training Epoch: 2/2, step 21400/23838 completed (loss: 1.072955846786499, acc: 0.6774193644523621)
[2025-02-05 14:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:04][root][INFO] - Training Epoch: 2/2, step 21401/23838 completed (loss: 0.9640486836433411, acc: 0.7346938848495483)
[2025-02-05 14:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:04][root][INFO] - Training Epoch: 2/2, step 21402/23838 completed (loss: 1.1264694929122925, acc: 0.625)
[2025-02-05 14:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:04][root][INFO] - Training Epoch: 2/2, step 21403/23838 completed (loss: 1.3730520009994507, acc: 0.6101694703102112)
[2025-02-05 14:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:05][root][INFO] - Training Epoch: 2/2, step 21404/23838 completed (loss: 2.2140212059020996, acc: 0.3529411852359772)
[2025-02-05 14:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:05][root][INFO] - Training Epoch: 2/2, step 21405/23838 completed (loss: 1.3093392848968506, acc: 0.6000000238418579)
[2025-02-05 14:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:06][root][INFO] - Training Epoch: 2/2, step 21406/23838 completed (loss: 1.0677332878112793, acc: 0.692307710647583)
[2025-02-05 14:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:06][root][INFO] - Training Epoch: 2/2, step 21407/23838 completed (loss: 1.5748677253723145, acc: 0.5204081535339355)
[2025-02-05 14:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:07][root][INFO] - Training Epoch: 2/2, step 21408/23838 completed (loss: 1.187878966331482, acc: 0.7083333134651184)
[2025-02-05 14:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:07][root][INFO] - Training Epoch: 2/2, step 21409/23838 completed (loss: 1.1966137886047363, acc: 0.65625)
[2025-02-05 14:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:07][root][INFO] - Training Epoch: 2/2, step 21410/23838 completed (loss: 1.3019707202911377, acc: 0.6666666865348816)
[2025-02-05 14:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:08][root][INFO] - Training Epoch: 2/2, step 21411/23838 completed (loss: 1.2837562561035156, acc: 0.5588235259056091)
[2025-02-05 14:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:08][root][INFO] - Training Epoch: 2/2, step 21412/23838 completed (loss: 0.8823272585868835, acc: 0.75)
[2025-02-05 14:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:08][root][INFO] - Training Epoch: 2/2, step 21413/23838 completed (loss: 1.8169612884521484, acc: 0.4583333432674408)
[2025-02-05 14:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:09][root][INFO] - Training Epoch: 2/2, step 21414/23838 completed (loss: 1.131588101387024, acc: 0.7291666865348816)
[2025-02-05 14:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:09][root][INFO] - Training Epoch: 2/2, step 21415/23838 completed (loss: 1.1166764497756958, acc: 0.7209302186965942)
[2025-02-05 14:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:10][root][INFO] - Training Epoch: 2/2, step 21416/23838 completed (loss: 1.41294264793396, acc: 0.6129032373428345)
[2025-02-05 14:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:10][root][INFO] - Training Epoch: 2/2, step 21417/23838 completed (loss: 1.269679069519043, acc: 0.6865671873092651)
[2025-02-05 14:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:10][root][INFO] - Training Epoch: 2/2, step 21418/23838 completed (loss: 1.565834879875183, acc: 0.581818163394928)
[2025-02-05 14:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:11][root][INFO] - Training Epoch: 2/2, step 21419/23838 completed (loss: 1.2123441696166992, acc: 0.7017543911933899)
[2025-02-05 14:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:11][root][INFO] - Training Epoch: 2/2, step 21420/23838 completed (loss: 1.4457894563674927, acc: 0.6785714030265808)
[2025-02-05 14:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:11][root][INFO] - Training Epoch: 2/2, step 21421/23838 completed (loss: 0.8913472294807434, acc: 0.7777777910232544)
[2025-02-05 14:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:12][root][INFO] - Training Epoch: 2/2, step 21422/23838 completed (loss: 1.5910497903823853, acc: 0.6176470518112183)
[2025-02-05 14:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:12][root][INFO] - Training Epoch: 2/2, step 21423/23838 completed (loss: 1.4721487760543823, acc: 0.6041666865348816)
[2025-02-05 14:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:13][root][INFO] - Training Epoch: 2/2, step 21424/23838 completed (loss: 1.1988590955734253, acc: 0.6805555820465088)
[2025-02-05 14:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:13][root][INFO] - Training Epoch: 2/2, step 21425/23838 completed (loss: 1.952755331993103, acc: 0.469696968793869)
[2025-02-05 14:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:13][root][INFO] - Training Epoch: 2/2, step 21426/23838 completed (loss: 1.5832535028457642, acc: 0.5517241358757019)
[2025-02-05 14:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:14][root][INFO] - Training Epoch: 2/2, step 21427/23838 completed (loss: 1.9161077737808228, acc: 0.43396225571632385)
[2025-02-05 14:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:14][root][INFO] - Training Epoch: 2/2, step 21428/23838 completed (loss: 1.4054734706878662, acc: 0.5714285969734192)
[2025-02-05 14:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:15][root][INFO] - Training Epoch: 2/2, step 21429/23838 completed (loss: 1.5537278652191162, acc: 0.5789473652839661)
[2025-02-05 14:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:15][root][INFO] - Training Epoch: 2/2, step 21430/23838 completed (loss: 1.4890367984771729, acc: 0.6153846383094788)
[2025-02-05 14:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:15][root][INFO] - Training Epoch: 2/2, step 21431/23838 completed (loss: 1.397554874420166, acc: 0.6216216087341309)
[2025-02-05 14:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:16][root][INFO] - Training Epoch: 2/2, step 21432/23838 completed (loss: 1.5442619323730469, acc: 0.5588235259056091)
[2025-02-05 14:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:16][root][INFO] - Training Epoch: 2/2, step 21433/23838 completed (loss: 1.214953899383545, acc: 0.6421052813529968)
[2025-02-05 14:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:17][root][INFO] - Training Epoch: 2/2, step 21434/23838 completed (loss: 2.03731107711792, acc: 0.5098039507865906)
[2025-02-05 14:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:17][root][INFO] - Training Epoch: 2/2, step 21435/23838 completed (loss: 1.7274904251098633, acc: 0.5400000214576721)
[2025-02-05 14:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:17][root][INFO] - Training Epoch: 2/2, step 21436/23838 completed (loss: 1.6872514486312866, acc: 0.5365853905677795)
[2025-02-05 14:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:18][root][INFO] - Training Epoch: 2/2, step 21437/23838 completed (loss: 1.4906014204025269, acc: 0.5625)
[2025-02-05 14:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:18][root][INFO] - Training Epoch: 2/2, step 21438/23838 completed (loss: 1.23096764087677, acc: 0.6363636255264282)
[2025-02-05 14:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:18][root][INFO] - Training Epoch: 2/2, step 21439/23838 completed (loss: 1.3102635145187378, acc: 0.6666666865348816)
[2025-02-05 14:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:19][root][INFO] - Training Epoch: 2/2, step 21440/23838 completed (loss: 1.605074405670166, acc: 0.5714285969734192)
[2025-02-05 14:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:19][root][INFO] - Training Epoch: 2/2, step 21441/23838 completed (loss: 1.4502893686294556, acc: 0.6612903475761414)
[2025-02-05 14:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:20][root][INFO] - Training Epoch: 2/2, step 21442/23838 completed (loss: 1.3049113750457764, acc: 0.675000011920929)
[2025-02-05 14:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:20][root][INFO] - Training Epoch: 2/2, step 21443/23838 completed (loss: 1.2213307619094849, acc: 0.625)
[2025-02-05 14:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:20][root][INFO] - Training Epoch: 2/2, step 21444/23838 completed (loss: 1.0706814527511597, acc: 0.6666666865348816)
[2025-02-05 14:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:21][root][INFO] - Training Epoch: 2/2, step 21445/23838 completed (loss: 1.1080682277679443, acc: 0.699999988079071)
[2025-02-05 14:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:21][root][INFO] - Training Epoch: 2/2, step 21446/23838 completed (loss: 1.1390365362167358, acc: 0.7068965435028076)
[2025-02-05 14:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:22][root][INFO] - Training Epoch: 2/2, step 21447/23838 completed (loss: 1.7881873846054077, acc: 0.4736842215061188)
[2025-02-05 14:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:22][root][INFO] - Training Epoch: 2/2, step 21448/23838 completed (loss: 1.4367722272872925, acc: 0.6025640964508057)
[2025-02-05 14:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:22][root][INFO] - Training Epoch: 2/2, step 21449/23838 completed (loss: 1.0944994688034058, acc: 0.7066666483879089)
[2025-02-05 14:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:23][root][INFO] - Training Epoch: 2/2, step 21450/23838 completed (loss: 1.3324400186538696, acc: 0.6341463327407837)
[2025-02-05 14:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:23][root][INFO] - Training Epoch: 2/2, step 21451/23838 completed (loss: 1.2128392457962036, acc: 0.6875)
[2025-02-05 14:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:24][root][INFO] - Training Epoch: 2/2, step 21452/23838 completed (loss: 1.3884297609329224, acc: 0.6231883764266968)
[2025-02-05 14:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:24][root][INFO] - Training Epoch: 2/2, step 21453/23838 completed (loss: 1.1391983032226562, acc: 0.6831682920455933)
[2025-02-05 14:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:24][root][INFO] - Training Epoch: 2/2, step 21454/23838 completed (loss: 1.3502659797668457, acc: 0.6388888955116272)
[2025-02-05 14:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:25][root][INFO] - Training Epoch: 2/2, step 21455/23838 completed (loss: 0.6929120421409607, acc: 0.837837815284729)
[2025-02-05 14:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:25][root][INFO] - Training Epoch: 2/2, step 21456/23838 completed (loss: 1.283715009689331, acc: 0.6875)
[2025-02-05 14:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:26][root][INFO] - Training Epoch: 2/2, step 21457/23838 completed (loss: 0.9814996123313904, acc: 0.6800000071525574)
[2025-02-05 14:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:26][root][INFO] - Training Epoch: 2/2, step 21458/23838 completed (loss: 1.1675843000411987, acc: 0.6896551847457886)
[2025-02-05 14:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:26][root][INFO] - Training Epoch: 2/2, step 21459/23838 completed (loss: 1.288435459136963, acc: 0.6280992031097412)
[2025-02-05 14:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:27][root][INFO] - Training Epoch: 2/2, step 21460/23838 completed (loss: 1.307729959487915, acc: 0.6079999804496765)
[2025-02-05 14:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:27][root][INFO] - Training Epoch: 2/2, step 21461/23838 completed (loss: 1.3289885520935059, acc: 0.6451612710952759)
[2025-02-05 14:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:28][root][INFO] - Training Epoch: 2/2, step 21462/23838 completed (loss: 1.4634780883789062, acc: 0.567251443862915)
[2025-02-05 14:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:28][root][INFO] - Training Epoch: 2/2, step 21463/23838 completed (loss: 1.0285351276397705, acc: 0.6896551847457886)
[2025-02-05 14:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:29][root][INFO] - Training Epoch: 2/2, step 21464/23838 completed (loss: 1.2786171436309814, acc: 0.6301369667053223)
[2025-02-05 14:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:29][root][INFO] - Training Epoch: 2/2, step 21465/23838 completed (loss: 1.5298750400543213, acc: 0.5945945978164673)
[2025-02-05 14:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:30][root][INFO] - Training Epoch: 2/2, step 21466/23838 completed (loss: 1.1649844646453857, acc: 0.6597937941551208)
[2025-02-05 14:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:30][root][INFO] - Training Epoch: 2/2, step 21467/23838 completed (loss: 1.1973330974578857, acc: 0.6478873491287231)
[2025-02-05 14:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:30][root][INFO] - Training Epoch: 2/2, step 21468/23838 completed (loss: 1.1847882270812988, acc: 0.6909090876579285)
[2025-02-05 14:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:31][root][INFO] - Training Epoch: 2/2, step 21469/23838 completed (loss: 1.245124101638794, acc: 0.6911764740943909)
[2025-02-05 14:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:31][root][INFO] - Training Epoch: 2/2, step 21470/23838 completed (loss: 1.4136701822280884, acc: 0.6262626051902771)
[2025-02-05 14:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:32][root][INFO] - Training Epoch: 2/2, step 21471/23838 completed (loss: 1.0465842485427856, acc: 0.692307710647583)
[2025-02-05 14:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:32][root][INFO] - Training Epoch: 2/2, step 21472/23838 completed (loss: 1.3548346757888794, acc: 0.625)
[2025-02-05 14:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:32][root][INFO] - Training Epoch: 2/2, step 21473/23838 completed (loss: 1.0807603597640991, acc: 0.701298713684082)
[2025-02-05 14:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:33][root][INFO] - Training Epoch: 2/2, step 21474/23838 completed (loss: 1.0102545022964478, acc: 0.6504064798355103)
[2025-02-05 14:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:33][root][INFO] - Training Epoch: 2/2, step 21475/23838 completed (loss: 0.8509268164634705, acc: 0.7183098793029785)
[2025-02-05 14:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:34][root][INFO] - Training Epoch: 2/2, step 21476/23838 completed (loss: 0.8018023371696472, acc: 0.739130437374115)
[2025-02-05 14:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:34][root][INFO] - Training Epoch: 2/2, step 21477/23838 completed (loss: 1.0309914350509644, acc: 0.7317073345184326)
[2025-02-05 14:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:34][root][INFO] - Training Epoch: 2/2, step 21478/23838 completed (loss: 1.0127733945846558, acc: 0.6912751793861389)
[2025-02-05 14:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:35][root][INFO] - Training Epoch: 2/2, step 21479/23838 completed (loss: 1.3405183553695679, acc: 0.6124030947685242)
[2025-02-05 14:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:35][root][INFO] - Training Epoch: 2/2, step 21480/23838 completed (loss: 1.5113029479980469, acc: 0.5840708017349243)
[2025-02-05 14:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:36][root][INFO] - Training Epoch: 2/2, step 21481/23838 completed (loss: 1.0612143278121948, acc: 0.699999988079071)
[2025-02-05 14:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:36][root][INFO] - Training Epoch: 2/2, step 21482/23838 completed (loss: 0.7414669394493103, acc: 0.7692307829856873)
[2025-02-05 14:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:36][root][INFO] - Training Epoch: 2/2, step 21483/23838 completed (loss: 1.192358136177063, acc: 0.6551724076271057)
[2025-02-05 14:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:37][root][INFO] - Training Epoch: 2/2, step 21484/23838 completed (loss: 0.8420671820640564, acc: 0.7341772317886353)
[2025-02-05 14:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:37][root][INFO] - Training Epoch: 2/2, step 21485/23838 completed (loss: 0.8734583258628845, acc: 0.7121211886405945)
[2025-02-05 14:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:38][root][INFO] - Training Epoch: 2/2, step 21486/23838 completed (loss: 0.8711475729942322, acc: 0.75)
[2025-02-05 14:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:38][root][INFO] - Training Epoch: 2/2, step 21487/23838 completed (loss: 1.211758017539978, acc: 0.6756756901741028)
[2025-02-05 14:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:39][root][INFO] - Training Epoch: 2/2, step 21488/23838 completed (loss: 1.3069380521774292, acc: 0.6315789222717285)
[2025-02-05 14:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:39][root][INFO] - Training Epoch: 2/2, step 21489/23838 completed (loss: 1.1588609218597412, acc: 0.6756756901741028)
[2025-02-05 14:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:39][root][INFO] - Training Epoch: 2/2, step 21490/23838 completed (loss: 1.2192422151565552, acc: 0.6590909361839294)
[2025-02-05 14:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:40][root][INFO] - Training Epoch: 2/2, step 21491/23838 completed (loss: 1.0688594579696655, acc: 0.7441860437393188)
[2025-02-05 14:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:40][root][INFO] - Training Epoch: 2/2, step 21492/23838 completed (loss: 0.8566957116127014, acc: 0.7307692170143127)
[2025-02-05 14:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:41][root][INFO] - Training Epoch: 2/2, step 21493/23838 completed (loss: 1.0902589559555054, acc: 0.6000000238418579)
[2025-02-05 14:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:41][root][INFO] - Training Epoch: 2/2, step 21494/23838 completed (loss: 1.4323251247406006, acc: 0.6000000238418579)
[2025-02-05 14:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:42][root][INFO] - Training Epoch: 2/2, step 21495/23838 completed (loss: 0.8166629076004028, acc: 0.7424242496490479)
[2025-02-05 14:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:42][root][INFO] - Training Epoch: 2/2, step 21496/23838 completed (loss: 0.881500244140625, acc: 0.7213114500045776)
[2025-02-05 14:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:42][root][INFO] - Training Epoch: 2/2, step 21497/23838 completed (loss: 1.2707027196884155, acc: 0.6000000238418579)
[2025-02-05 14:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:43][root][INFO] - Training Epoch: 2/2, step 21498/23838 completed (loss: 1.4426066875457764, acc: 0.5428571701049805)
[2025-02-05 14:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:43][root][INFO] - Training Epoch: 2/2, step 21499/23838 completed (loss: 1.1996781826019287, acc: 0.707317054271698)
[2025-02-05 14:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:44][root][INFO] - Training Epoch: 2/2, step 21500/23838 completed (loss: 0.7059400081634521, acc: 0.7727272510528564)
[2025-02-05 14:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:44][root][INFO] - Training Epoch: 2/2, step 21501/23838 completed (loss: 1.0918173789978027, acc: 0.6481481194496155)
[2025-02-05 14:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:44][root][INFO] - Training Epoch: 2/2, step 21502/23838 completed (loss: 0.6465979218482971, acc: 0.8372092843055725)
[2025-02-05 14:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:45][root][INFO] - Training Epoch: 2/2, step 21503/23838 completed (loss: 1.0688010454177856, acc: 0.6595744490623474)
[2025-02-05 14:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:45][root][INFO] - Training Epoch: 2/2, step 21504/23838 completed (loss: 0.9001157283782959, acc: 0.7714285850524902)
[2025-02-05 14:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:45][root][INFO] - Training Epoch: 2/2, step 21505/23838 completed (loss: 1.0524296760559082, acc: 0.692307710647583)
[2025-02-05 14:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:46][root][INFO] - Training Epoch: 2/2, step 21506/23838 completed (loss: 0.8468998074531555, acc: 0.6315789222717285)
[2025-02-05 14:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:46][root][INFO] - Training Epoch: 2/2, step 21507/23838 completed (loss: 0.6867634654045105, acc: 0.824999988079071)
[2025-02-05 14:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:47][root][INFO] - Training Epoch: 2/2, step 21508/23838 completed (loss: 0.6966791749000549, acc: 0.7954545617103577)
[2025-02-05 14:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:47][root][INFO] - Training Epoch: 2/2, step 21509/23838 completed (loss: 1.214507818222046, acc: 0.6111111044883728)
[2025-02-05 14:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:47][root][INFO] - Training Epoch: 2/2, step 21510/23838 completed (loss: 0.9222998023033142, acc: 0.6792452931404114)
[2025-02-05 14:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:48][root][INFO] - Training Epoch: 2/2, step 21511/23838 completed (loss: 1.1101250648498535, acc: 0.7333333492279053)
[2025-02-05 14:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:48][root][INFO] - Training Epoch: 2/2, step 21512/23838 completed (loss: 1.3568503856658936, acc: 0.6226415038108826)
[2025-02-05 14:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:49][root][INFO] - Training Epoch: 2/2, step 21513/23838 completed (loss: 0.6721183061599731, acc: 0.8235294222831726)
[2025-02-05 14:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:49][root][INFO] - Training Epoch: 2/2, step 21514/23838 completed (loss: 0.656241238117218, acc: 0.7647058963775635)
[2025-02-05 14:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:50][root][INFO] - Training Epoch: 2/2, step 21515/23838 completed (loss: 0.8301877379417419, acc: 0.7605633735656738)
[2025-02-05 14:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:50][root][INFO] - Training Epoch: 2/2, step 21516/23838 completed (loss: 1.165166974067688, acc: 0.5833333134651184)
[2025-02-05 14:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:51][root][INFO] - Training Epoch: 2/2, step 21517/23838 completed (loss: 1.1726912260055542, acc: 0.640625)
[2025-02-05 14:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:51][root][INFO] - Training Epoch: 2/2, step 21518/23838 completed (loss: 1.174450397491455, acc: 0.6415094137191772)
[2025-02-05 14:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:51][root][INFO] - Training Epoch: 2/2, step 21519/23838 completed (loss: 0.6495174169540405, acc: 0.8611111044883728)
[2025-02-05 14:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:52][root][INFO] - Training Epoch: 2/2, step 21520/23838 completed (loss: 1.0069319009780884, acc: 0.6842105388641357)
[2025-02-05 14:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:52][root][INFO] - Training Epoch: 2/2, step 21521/23838 completed (loss: 0.5653002262115479, acc: 0.8279569745063782)
[2025-02-05 14:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:53][root][INFO] - Training Epoch: 2/2, step 21522/23838 completed (loss: 0.8877161741256714, acc: 0.75)
[2025-02-05 14:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:53][root][INFO] - Training Epoch: 2/2, step 21523/23838 completed (loss: 0.7108222842216492, acc: 0.8169013857841492)
[2025-02-05 14:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:54][root][INFO] - Training Epoch: 2/2, step 21524/23838 completed (loss: 0.4905395805835724, acc: 0.8444444537162781)
[2025-02-05 14:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:54][root][INFO] - Training Epoch: 2/2, step 21525/23838 completed (loss: 0.60797119140625, acc: 0.7857142686843872)
[2025-02-05 14:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:54][root][INFO] - Training Epoch: 2/2, step 21526/23838 completed (loss: 2.270904064178467, acc: 0.4444444477558136)
[2025-02-05 14:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:55][root][INFO] - Training Epoch: 2/2, step 21527/23838 completed (loss: 1.1589890718460083, acc: 0.6111111044883728)
[2025-02-05 14:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:55][root][INFO] - Training Epoch: 2/2, step 21528/23838 completed (loss: 1.3695212602615356, acc: 0.6086956262588501)
[2025-02-05 14:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:56][root][INFO] - Training Epoch: 2/2, step 21529/23838 completed (loss: 0.8587138652801514, acc: 0.7435897588729858)
[2025-02-05 14:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:56][root][INFO] - Training Epoch: 2/2, step 21530/23838 completed (loss: 0.8642242550849915, acc: 0.8421052694320679)
[2025-02-05 14:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:56][root][INFO] - Training Epoch: 2/2, step 21531/23838 completed (loss: 1.2601675987243652, acc: 0.6976743936538696)
[2025-02-05 14:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:57][root][INFO] - Training Epoch: 2/2, step 21532/23838 completed (loss: 1.180305004119873, acc: 0.6666666865348816)
[2025-02-05 14:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:57][root][INFO] - Training Epoch: 2/2, step 21533/23838 completed (loss: 1.0129005908966064, acc: 0.7586206793785095)
[2025-02-05 14:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:58][root][INFO] - Training Epoch: 2/2, step 21534/23838 completed (loss: 1.1334295272827148, acc: 0.6785714030265808)
[2025-02-05 14:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:58][root][INFO] - Training Epoch: 2/2, step 21535/23838 completed (loss: 0.6368590593338013, acc: 0.8536585569381714)
[2025-02-05 14:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:58][root][INFO] - Training Epoch: 2/2, step 21536/23838 completed (loss: 0.9279685020446777, acc: 0.75)
[2025-02-05 14:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:59][root][INFO] - Training Epoch: 2/2, step 21537/23838 completed (loss: 1.3950730562210083, acc: 0.6000000238418579)
[2025-02-05 14:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:59][root][INFO] - Training Epoch: 2/2, step 21538/23838 completed (loss: 1.2248170375823975, acc: 0.5952380895614624)
[2025-02-05 14:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:03:59][root][INFO] - Training Epoch: 2/2, step 21539/23838 completed (loss: 0.9322972297668457, acc: 0.625)
[2025-02-05 14:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:00][root][INFO] - Training Epoch: 2/2, step 21540/23838 completed (loss: 1.1861594915390015, acc: 0.6829268336296082)
[2025-02-05 14:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:00][root][INFO] - Training Epoch: 2/2, step 21541/23838 completed (loss: 1.2657414674758911, acc: 0.6200000047683716)
[2025-02-05 14:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:01][root][INFO] - Training Epoch: 2/2, step 21542/23838 completed (loss: 1.609455943107605, acc: 0.5348837375640869)
[2025-02-05 14:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:01][root][INFO] - Training Epoch: 2/2, step 21543/23838 completed (loss: 1.045845866203308, acc: 0.6551724076271057)
[2025-02-05 14:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:01][root][INFO] - Training Epoch: 2/2, step 21544/23838 completed (loss: 1.6807483434677124, acc: 0.4761904776096344)
[2025-02-05 14:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:02][root][INFO] - Training Epoch: 2/2, step 21545/23838 completed (loss: 1.0524653196334839, acc: 0.7058823704719543)
[2025-02-05 14:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:02][root][INFO] - Training Epoch: 2/2, step 21546/23838 completed (loss: 0.9478135704994202, acc: 0.746835470199585)
[2025-02-05 14:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:02][root][INFO] - Training Epoch: 2/2, step 21547/23838 completed (loss: 1.1500701904296875, acc: 0.6805555820465088)
[2025-02-05 14:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:03][root][INFO] - Training Epoch: 2/2, step 21548/23838 completed (loss: 1.0156292915344238, acc: 0.7076923251152039)
[2025-02-05 14:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:03][root][INFO] - Training Epoch: 2/2, step 21549/23838 completed (loss: 1.445853352546692, acc: 0.6279069781303406)
[2025-02-05 14:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:04][root][INFO] - Training Epoch: 2/2, step 21550/23838 completed (loss: 0.8863027095794678, acc: 0.797468364238739)
[2025-02-05 14:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:04][root][INFO] - Training Epoch: 2/2, step 21551/23838 completed (loss: 0.903098464012146, acc: 0.7333333492279053)
[2025-02-05 14:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:05][root][INFO] - Training Epoch: 2/2, step 21552/23838 completed (loss: 0.6512811183929443, acc: 0.7777777910232544)
[2025-02-05 14:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:05][root][INFO] - Training Epoch: 2/2, step 21553/23838 completed (loss: 1.608760118484497, acc: 0.5614035129547119)
[2025-02-05 14:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:06][root][INFO] - Training Epoch: 2/2, step 21554/23838 completed (loss: 1.5839040279388428, acc: 0.5769230723381042)
[2025-02-05 14:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:06][root][INFO] - Training Epoch: 2/2, step 21555/23838 completed (loss: 1.1968239545822144, acc: 0.6818181872367859)
[2025-02-05 14:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:07][root][INFO] - Training Epoch: 2/2, step 21556/23838 completed (loss: 0.9843081831932068, acc: 0.7346938848495483)
[2025-02-05 14:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:07][root][INFO] - Training Epoch: 2/2, step 21557/23838 completed (loss: 0.8839259147644043, acc: 0.734375)
[2025-02-05 14:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:07][root][INFO] - Training Epoch: 2/2, step 21558/23838 completed (loss: 0.8247433304786682, acc: 0.7115384340286255)
[2025-02-05 14:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:08][root][INFO] - Training Epoch: 2/2, step 21559/23838 completed (loss: 0.980323314666748, acc: 0.7179487347602844)
[2025-02-05 14:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:08][root][INFO] - Training Epoch: 2/2, step 21560/23838 completed (loss: 1.4475843906402588, acc: 0.5053763389587402)
[2025-02-05 14:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:09][root][INFO] - Training Epoch: 2/2, step 21561/23838 completed (loss: 1.1029030084609985, acc: 0.7200000286102295)
[2025-02-05 14:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:09][root][INFO] - Training Epoch: 2/2, step 21562/23838 completed (loss: 0.9687793850898743, acc: 0.7090908885002136)
[2025-02-05 14:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:10][root][INFO] - Training Epoch: 2/2, step 21563/23838 completed (loss: 1.082025170326233, acc: 0.6582278609275818)
[2025-02-05 14:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:10][root][INFO] - Training Epoch: 2/2, step 21564/23838 completed (loss: 0.5668314695358276, acc: 0.8269230723381042)
[2025-02-05 14:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:10][root][INFO] - Training Epoch: 2/2, step 21565/23838 completed (loss: 0.8841120600700378, acc: 0.7027027010917664)
[2025-02-05 14:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:11][root][INFO] - Training Epoch: 2/2, step 21566/23838 completed (loss: 1.0636441707611084, acc: 0.7200000286102295)
[2025-02-05 14:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:11][root][INFO] - Training Epoch: 2/2, step 21567/23838 completed (loss: 1.002740502357483, acc: 0.7209302186965942)
[2025-02-05 14:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:12][root][INFO] - Training Epoch: 2/2, step 21568/23838 completed (loss: 1.330891489982605, acc: 0.6285714507102966)
[2025-02-05 14:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:12][root][INFO] - Training Epoch: 2/2, step 21569/23838 completed (loss: 0.7429513335227966, acc: 0.7682926654815674)
[2025-02-05 14:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:13][root][INFO] - Training Epoch: 2/2, step 21570/23838 completed (loss: 0.7122862339019775, acc: 0.835616409778595)
[2025-02-05 14:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:13][root][INFO] - Training Epoch: 2/2, step 21571/23838 completed (loss: 0.6165778636932373, acc: 0.8135592937469482)
[2025-02-05 14:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:14][root][INFO] - Training Epoch: 2/2, step 21572/23838 completed (loss: 0.8225658535957336, acc: 0.7714285850524902)
[2025-02-05 14:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:14][root][INFO] - Training Epoch: 2/2, step 21573/23838 completed (loss: 1.0493896007537842, acc: 0.6947368383407593)
[2025-02-05 14:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:15][root][INFO] - Training Epoch: 2/2, step 21574/23838 completed (loss: 0.7602860331535339, acc: 0.7941176295280457)
[2025-02-05 14:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:15][root][INFO] - Training Epoch: 2/2, step 21575/23838 completed (loss: 0.8402923941612244, acc: 0.7439024448394775)
[2025-02-05 14:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:16][root][INFO] - Training Epoch: 2/2, step 21576/23838 completed (loss: 0.7964994311332703, acc: 0.7387387156486511)
[2025-02-05 14:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:16][root][INFO] - Training Epoch: 2/2, step 21577/23838 completed (loss: 1.1586848497390747, acc: 0.7285714149475098)
[2025-02-05 14:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:17][root][INFO] - Training Epoch: 2/2, step 21578/23838 completed (loss: 0.958512544631958, acc: 0.7291666865348816)
[2025-02-05 14:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:17][root][INFO] - Training Epoch: 2/2, step 21579/23838 completed (loss: 1.1954567432403564, acc: 0.6666666865348816)
[2025-02-05 14:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:18][root][INFO] - Training Epoch: 2/2, step 21580/23838 completed (loss: 0.9962730407714844, acc: 0.7777777910232544)
[2025-02-05 14:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:18][root][INFO] - Training Epoch: 2/2, step 21581/23838 completed (loss: 1.1883524656295776, acc: 0.6666666865348816)
[2025-02-05 14:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:18][root][INFO] - Training Epoch: 2/2, step 21582/23838 completed (loss: 1.2852189540863037, acc: 0.6363636255264282)
[2025-02-05 14:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:19][root][INFO] - Training Epoch: 2/2, step 21583/23838 completed (loss: 1.2418402433395386, acc: 0.6435643434524536)
[2025-02-05 14:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:19][root][INFO] - Training Epoch: 2/2, step 21584/23838 completed (loss: 1.4050538539886475, acc: 0.6030534505844116)
[2025-02-05 14:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:20][root][INFO] - Training Epoch: 2/2, step 21585/23838 completed (loss: 1.2360038757324219, acc: 0.5967742204666138)
[2025-02-05 14:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:20][root][INFO] - Training Epoch: 2/2, step 21586/23838 completed (loss: 1.2113374471664429, acc: 0.6800000071525574)
[2025-02-05 14:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:20][root][INFO] - Training Epoch: 2/2, step 21587/23838 completed (loss: 1.2951231002807617, acc: 0.6185566782951355)
[2025-02-05 14:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:21][root][INFO] - Training Epoch: 2/2, step 21588/23838 completed (loss: 1.562440037727356, acc: 0.560606062412262)
[2025-02-05 14:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:21][root][INFO] - Training Epoch: 2/2, step 21589/23838 completed (loss: 1.4253324270248413, acc: 0.634482741355896)
[2025-02-05 14:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:22][root][INFO] - Training Epoch: 2/2, step 21590/23838 completed (loss: 1.0240960121154785, acc: 0.6666666865348816)
[2025-02-05 14:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:22][root][INFO] - Training Epoch: 2/2, step 21591/23838 completed (loss: 1.1791309118270874, acc: 0.650602400302887)
[2025-02-05 14:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:22][root][INFO] - Training Epoch: 2/2, step 21592/23838 completed (loss: 1.2930134534835815, acc: 0.6499999761581421)
[2025-02-05 14:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:23][root][INFO] - Training Epoch: 2/2, step 21593/23838 completed (loss: 1.475616455078125, acc: 0.523809552192688)
[2025-02-05 14:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:23][root][INFO] - Training Epoch: 2/2, step 21594/23838 completed (loss: 0.8930919170379639, acc: 0.8399999737739563)
[2025-02-05 14:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:23][root][INFO] - Training Epoch: 2/2, step 21595/23838 completed (loss: 1.2439035177230835, acc: 0.6610169410705566)
[2025-02-05 14:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:24][root][INFO] - Training Epoch: 2/2, step 21596/23838 completed (loss: 1.3240303993225098, acc: 0.5662650465965271)
[2025-02-05 14:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:24][root][INFO] - Training Epoch: 2/2, step 21597/23838 completed (loss: 1.5801020860671997, acc: 0.5477706789970398)
[2025-02-05 14:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:25][root][INFO] - Training Epoch: 2/2, step 21598/23838 completed (loss: 1.1756945848464966, acc: 0.656862735748291)
[2025-02-05 14:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:25][root][INFO] - Training Epoch: 2/2, step 21599/23838 completed (loss: 1.4503706693649292, acc: 0.5740740895271301)
[2025-02-05 14:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:25][root][INFO] - Training Epoch: 2/2, step 21600/23838 completed (loss: 1.0525027513504028, acc: 0.6774193644523621)
[2025-02-05 14:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:26][root][INFO] - Training Epoch: 2/2, step 21601/23838 completed (loss: 1.235977053642273, acc: 0.5918367505073547)
[2025-02-05 14:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:26][root][INFO] - Training Epoch: 2/2, step 21602/23838 completed (loss: 0.8685446977615356, acc: 0.8125)
[2025-02-05 14:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:27][root][INFO] - Training Epoch: 2/2, step 21603/23838 completed (loss: 1.1134922504425049, acc: 0.6853932738304138)
[2025-02-05 14:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:27][root][INFO] - Training Epoch: 2/2, step 21604/23838 completed (loss: 1.2471990585327148, acc: 0.6756756901741028)
[2025-02-05 14:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:27][root][INFO] - Training Epoch: 2/2, step 21605/23838 completed (loss: 1.0612071752548218, acc: 0.698924720287323)
[2025-02-05 14:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:28][root][INFO] - Training Epoch: 2/2, step 21606/23838 completed (loss: 1.301699161529541, acc: 0.604938268661499)
[2025-02-05 14:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:28][root][INFO] - Training Epoch: 2/2, step 21607/23838 completed (loss: 0.9339859485626221, acc: 0.7323943376541138)
[2025-02-05 14:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:29][root][INFO] - Training Epoch: 2/2, step 21608/23838 completed (loss: 1.525648832321167, acc: 0.5833333134651184)
[2025-02-05 14:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:29][root][INFO] - Training Epoch: 2/2, step 21609/23838 completed (loss: 1.3619102239608765, acc: 0.5922330021858215)
[2025-02-05 14:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:29][root][INFO] - Training Epoch: 2/2, step 21610/23838 completed (loss: 1.3198370933532715, acc: 0.6557376980781555)
[2025-02-05 14:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:30][root][INFO] - Training Epoch: 2/2, step 21611/23838 completed (loss: 1.335194706916809, acc: 0.5970149040222168)
[2025-02-05 14:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:30][root][INFO] - Training Epoch: 2/2, step 21612/23838 completed (loss: 1.2827008962631226, acc: 0.6404494643211365)
[2025-02-05 14:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:30][root][INFO] - Training Epoch: 2/2, step 21613/23838 completed (loss: 1.280123233795166, acc: 0.6705882549285889)
[2025-02-05 14:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:31][root][INFO] - Training Epoch: 2/2, step 21614/23838 completed (loss: 0.5352198481559753, acc: 0.875)
[2025-02-05 14:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:31][root][INFO] - Training Epoch: 2/2, step 21615/23838 completed (loss: 1.0582164525985718, acc: 0.7142857313156128)
[2025-02-05 14:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:31][root][INFO] - Training Epoch: 2/2, step 21616/23838 completed (loss: 0.9566308856010437, acc: 0.6349206566810608)
[2025-02-05 14:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:32][root][INFO] - Training Epoch: 2/2, step 21617/23838 completed (loss: 1.216495156288147, acc: 0.6153846383094788)
[2025-02-05 14:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:32][root][INFO] - Training Epoch: 2/2, step 21618/23838 completed (loss: 1.169601559638977, acc: 0.6363636255264282)
[2025-02-05 14:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:33][root][INFO] - Training Epoch: 2/2, step 21619/23838 completed (loss: 0.9985151290893555, acc: 0.7118644118309021)
[2025-02-05 14:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:33][root][INFO] - Training Epoch: 2/2, step 21620/23838 completed (loss: 1.3324811458587646, acc: 0.6000000238418579)
[2025-02-05 14:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:33][root][INFO] - Training Epoch: 2/2, step 21621/23838 completed (loss: 0.9238567352294922, acc: 0.7300000190734863)
[2025-02-05 14:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:34][root][INFO] - Training Epoch: 2/2, step 21622/23838 completed (loss: 1.1431219577789307, acc: 0.6435643434524536)
[2025-02-05 14:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:34][root][INFO] - Training Epoch: 2/2, step 21623/23838 completed (loss: 1.50874924659729, acc: 0.577464759349823)
[2025-02-05 14:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:34][root][INFO] - Training Epoch: 2/2, step 21624/23838 completed (loss: 0.8273723721504211, acc: 0.7647058963775635)
[2025-02-05 14:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:35][root][INFO] - Training Epoch: 2/2, step 21625/23838 completed (loss: 1.238917350769043, acc: 0.6292135119438171)
[2025-02-05 14:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:35][root][INFO] - Training Epoch: 2/2, step 21626/23838 completed (loss: 0.870694637298584, acc: 0.75)
[2025-02-05 14:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:36][root][INFO] - Training Epoch: 2/2, step 21627/23838 completed (loss: 0.7519387602806091, acc: 0.7631579041481018)
[2025-02-05 14:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:36][root][INFO] - Training Epoch: 2/2, step 21628/23838 completed (loss: 0.8314011096954346, acc: 0.7872340679168701)
[2025-02-05 14:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:36][root][INFO] - Training Epoch: 2/2, step 21629/23838 completed (loss: 1.0542854070663452, acc: 0.6637167930603027)
[2025-02-05 14:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:37][root][INFO] - Training Epoch: 2/2, step 21630/23838 completed (loss: 1.0770336389541626, acc: 0.7142857313156128)
[2025-02-05 14:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:37][root][INFO] - Training Epoch: 2/2, step 21631/23838 completed (loss: 1.2552862167358398, acc: 0.6589147448539734)
[2025-02-05 14:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:38][root][INFO] - Training Epoch: 2/2, step 21632/23838 completed (loss: 1.1085100173950195, acc: 0.6494845151901245)
[2025-02-05 14:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:38][root][INFO] - Training Epoch: 2/2, step 21633/23838 completed (loss: 1.170394778251648, acc: 0.6753246784210205)
[2025-02-05 14:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:38][root][INFO] - Training Epoch: 2/2, step 21634/23838 completed (loss: 1.0414888858795166, acc: 0.7627118825912476)
[2025-02-05 14:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:39][root][INFO] - Training Epoch: 2/2, step 21635/23838 completed (loss: 0.878376841545105, acc: 0.761904776096344)
[2025-02-05 14:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:39][root][INFO] - Training Epoch: 2/2, step 21636/23838 completed (loss: 1.2571779489517212, acc: 0.5855855941772461)
[2025-02-05 14:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:39][root][INFO] - Training Epoch: 2/2, step 21637/23838 completed (loss: 1.1455671787261963, acc: 0.6078431606292725)
[2025-02-05 14:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:40][root][INFO] - Training Epoch: 2/2, step 21638/23838 completed (loss: 1.0654994249343872, acc: 0.6933333277702332)
[2025-02-05 14:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:40][root][INFO] - Training Epoch: 2/2, step 21639/23838 completed (loss: 0.7727951407432556, acc: 0.7604166865348816)
[2025-02-05 14:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:41][root][INFO] - Training Epoch: 2/2, step 21640/23838 completed (loss: 1.079797387123108, acc: 0.686274528503418)
[2025-02-05 14:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:41][root][INFO] - Training Epoch: 2/2, step 21641/23838 completed (loss: 1.2886335849761963, acc: 0.6355140209197998)
[2025-02-05 14:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:41][root][INFO] - Training Epoch: 2/2, step 21642/23838 completed (loss: 1.0652594566345215, acc: 0.6976743936538696)
[2025-02-05 14:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:42][root][INFO] - Training Epoch: 2/2, step 21643/23838 completed (loss: 1.6161611080169678, acc: 0.5614035129547119)
[2025-02-05 14:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:42][root][INFO] - Training Epoch: 2/2, step 21644/23838 completed (loss: 1.3563264608383179, acc: 0.5891472697257996)
[2025-02-05 14:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:43][root][INFO] - Training Epoch: 2/2, step 21645/23838 completed (loss: 0.7508881688117981, acc: 0.7647058963775635)
[2025-02-05 14:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:43][root][INFO] - Training Epoch: 2/2, step 21646/23838 completed (loss: 0.7984424829483032, acc: 0.7397260069847107)
[2025-02-05 14:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:44][root][INFO] - Training Epoch: 2/2, step 21647/23838 completed (loss: 1.2009038925170898, acc: 0.6481481194496155)
[2025-02-05 14:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:44][root][INFO] - Training Epoch: 2/2, step 21648/23838 completed (loss: 0.9484586715698242, acc: 0.7010309100151062)
[2025-02-05 14:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:44][root][INFO] - Training Epoch: 2/2, step 21649/23838 completed (loss: 1.1554408073425293, acc: 0.6764705777168274)
[2025-02-05 14:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:45][root][INFO] - Training Epoch: 2/2, step 21650/23838 completed (loss: 1.0368092060089111, acc: 0.7128713130950928)
[2025-02-05 14:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:45][root][INFO] - Training Epoch: 2/2, step 21651/23838 completed (loss: 1.3424330949783325, acc: 0.5604395866394043)
[2025-02-05 14:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:45][root][INFO] - Training Epoch: 2/2, step 21652/23838 completed (loss: 1.0665323734283447, acc: 0.692307710647583)
[2025-02-05 14:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:46][root][INFO] - Training Epoch: 2/2, step 21653/23838 completed (loss: 1.370986819267273, acc: 0.6404494643211365)
[2025-02-05 14:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:46][root][INFO] - Training Epoch: 2/2, step 21654/23838 completed (loss: 1.515842080116272, acc: 0.59375)
[2025-02-05 14:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:47][root][INFO] - Training Epoch: 2/2, step 21655/23838 completed (loss: 1.321282982826233, acc: 0.6181818246841431)
[2025-02-05 14:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:47][root][INFO] - Training Epoch: 2/2, step 21656/23838 completed (loss: 1.1064234972000122, acc: 0.6774193644523621)
[2025-02-05 14:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:47][root][INFO] - Training Epoch: 2/2, step 21657/23838 completed (loss: 1.2362349033355713, acc: 0.6333333253860474)
[2025-02-05 14:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:48][root][INFO] - Training Epoch: 2/2, step 21658/23838 completed (loss: 0.8875261545181274, acc: 0.7345132827758789)
[2025-02-05 14:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:48][root][INFO] - Training Epoch: 2/2, step 21659/23838 completed (loss: 1.1140180826187134, acc: 0.6666666865348816)
[2025-02-05 14:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:49][root][INFO] - Training Epoch: 2/2, step 21660/23838 completed (loss: 0.8163681626319885, acc: 0.7611940503120422)
[2025-02-05 14:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:49][root][INFO] - Training Epoch: 2/2, step 21661/23838 completed (loss: 1.027733325958252, acc: 0.6933333277702332)
[2025-02-05 14:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:50][root][INFO] - Training Epoch: 2/2, step 21662/23838 completed (loss: 1.2993903160095215, acc: 0.6790123581886292)
[2025-02-05 14:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:50][root][INFO] - Training Epoch: 2/2, step 21663/23838 completed (loss: 1.0837604999542236, acc: 0.686274528503418)
[2025-02-05 14:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:50][root][INFO] - Training Epoch: 2/2, step 21664/23838 completed (loss: 1.2375608682632446, acc: 0.5681818127632141)
[2025-02-05 14:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:51][root][INFO] - Training Epoch: 2/2, step 21665/23838 completed (loss: 1.230417013168335, acc: 0.5744680762290955)
[2025-02-05 14:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:51][root][INFO] - Training Epoch: 2/2, step 21666/23838 completed (loss: 0.7620652318000793, acc: 0.7837837934494019)
[2025-02-05 14:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:52][root][INFO] - Training Epoch: 2/2, step 21667/23838 completed (loss: 1.246686339378357, acc: 0.6440678238868713)
[2025-02-05 14:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:52][root][INFO] - Training Epoch: 2/2, step 21668/23838 completed (loss: 1.0779520273208618, acc: 0.6730769276618958)
[2025-02-05 14:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:52][root][INFO] - Training Epoch: 2/2, step 21669/23838 completed (loss: 0.7549287676811218, acc: 0.7878788113594055)
[2025-02-05 14:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:53][root][INFO] - Training Epoch: 2/2, step 21670/23838 completed (loss: 1.370034098625183, acc: 0.5873016119003296)
[2025-02-05 14:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:53][root][INFO] - Training Epoch: 2/2, step 21671/23838 completed (loss: 1.7121553421020508, acc: 0.5357142686843872)
[2025-02-05 14:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:53][root][INFO] - Training Epoch: 2/2, step 21672/23838 completed (loss: 1.456916332244873, acc: 0.6774193644523621)
[2025-02-05 14:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:54][root][INFO] - Training Epoch: 2/2, step 21673/23838 completed (loss: 0.7889599800109863, acc: 0.7894737124443054)
[2025-02-05 14:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:54][root][INFO] - Training Epoch: 2/2, step 21674/23838 completed (loss: 0.8252835869789124, acc: 0.7115384340286255)
[2025-02-05 14:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:54][root][INFO] - Training Epoch: 2/2, step 21675/23838 completed (loss: 1.0160231590270996, acc: 0.6666666865348816)
[2025-02-05 14:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:55][root][INFO] - Training Epoch: 2/2, step 21676/23838 completed (loss: 1.2255825996398926, acc: 0.6000000238418579)
[2025-02-05 14:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:55][root][INFO] - Training Epoch: 2/2, step 21677/23838 completed (loss: 1.060436487197876, acc: 0.6730769276618958)
[2025-02-05 14:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:55][root][INFO] - Training Epoch: 2/2, step 21678/23838 completed (loss: 0.9203963875770569, acc: 0.774193525314331)
[2025-02-05 14:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:56][root][INFO] - Training Epoch: 2/2, step 21679/23838 completed (loss: 0.9492558836936951, acc: 0.695652186870575)
[2025-02-05 14:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:56][root][INFO] - Training Epoch: 2/2, step 21680/23838 completed (loss: 1.0139644145965576, acc: 0.78125)
[2025-02-05 14:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:57][root][INFO] - Training Epoch: 2/2, step 21681/23838 completed (loss: 1.2961704730987549, acc: 0.5666666626930237)
[2025-02-05 14:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:57][root][INFO] - Training Epoch: 2/2, step 21682/23838 completed (loss: 0.7642938494682312, acc: 0.75)
[2025-02-05 14:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:57][root][INFO] - Training Epoch: 2/2, step 21683/23838 completed (loss: 0.7072446942329407, acc: 0.875)
[2025-02-05 14:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:58][root][INFO] - Training Epoch: 2/2, step 21684/23838 completed (loss: 0.7971657514572144, acc: 0.7777777910232544)
[2025-02-05 14:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:58][root][INFO] - Training Epoch: 2/2, step 21685/23838 completed (loss: 1.5053833723068237, acc: 0.625)
[2025-02-05 14:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:59][root][INFO] - Training Epoch: 2/2, step 21686/23838 completed (loss: 0.9199844002723694, acc: 0.6666666865348816)
[2025-02-05 14:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:59][root][INFO] - Training Epoch: 2/2, step 21687/23838 completed (loss: 1.8449792861938477, acc: 0.4285714328289032)
[2025-02-05 14:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:04:59][root][INFO] - Training Epoch: 2/2, step 21688/23838 completed (loss: 0.8755560517311096, acc: 0.7333333492279053)
[2025-02-05 14:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:00][root][INFO] - Training Epoch: 2/2, step 21689/23838 completed (loss: 0.8638156056404114, acc: 0.7450980544090271)
[2025-02-05 14:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:00][root][INFO] - Training Epoch: 2/2, step 21690/23838 completed (loss: 0.9256806969642639, acc: 0.6896551847457886)
[2025-02-05 14:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:00][root][INFO] - Training Epoch: 2/2, step 21691/23838 completed (loss: 1.1608248949050903, acc: 0.6136363744735718)
[2025-02-05 14:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:01][root][INFO] - Training Epoch: 2/2, step 21692/23838 completed (loss: 0.6689987778663635, acc: 0.8125)
[2025-02-05 14:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:01][root][INFO] - Training Epoch: 2/2, step 21693/23838 completed (loss: 0.7889262437820435, acc: 0.692307710647583)
[2025-02-05 14:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:01][root][INFO] - Training Epoch: 2/2, step 21694/23838 completed (loss: 0.5678439140319824, acc: 0.8260869383811951)
[2025-02-05 14:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:02][root][INFO] - Training Epoch: 2/2, step 21695/23838 completed (loss: 0.8819547891616821, acc: 0.7307692170143127)
[2025-02-05 14:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:02][root][INFO] - Training Epoch: 2/2, step 21696/23838 completed (loss: 1.0059845447540283, acc: 0.7142857313156128)
[2025-02-05 14:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:03][root][INFO] - Training Epoch: 2/2, step 21697/23838 completed (loss: 1.4012399911880493, acc: 0.5652173757553101)
[2025-02-05 14:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:03][root][INFO] - Training Epoch: 2/2, step 21698/23838 completed (loss: 0.5764267444610596, acc: 0.7647058963775635)
[2025-02-05 14:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:03][root][INFO] - Training Epoch: 2/2, step 21699/23838 completed (loss: 1.0538525581359863, acc: 0.7222222089767456)
[2025-02-05 14:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:04][root][INFO] - Training Epoch: 2/2, step 21700/23838 completed (loss: 0.9259309768676758, acc: 0.6499999761581421)
[2025-02-05 14:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:04][root][INFO] - Training Epoch: 2/2, step 21701/23838 completed (loss: 1.1095997095108032, acc: 0.6567164063453674)
[2025-02-05 14:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:04][root][INFO] - Training Epoch: 2/2, step 21702/23838 completed (loss: 1.311639428138733, acc: 0.695652186870575)
[2025-02-05 14:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:05][root][INFO] - Training Epoch: 2/2, step 21703/23838 completed (loss: 1.157810926437378, acc: 0.6857143044471741)
[2025-02-05 14:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:05][root][INFO] - Training Epoch: 2/2, step 21704/23838 completed (loss: 0.9583287835121155, acc: 0.7083333134651184)
[2025-02-05 14:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:06][root][INFO] - Training Epoch: 2/2, step 21705/23838 completed (loss: 1.1871576309204102, acc: 0.6904761791229248)
[2025-02-05 14:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:06][root][INFO] - Training Epoch: 2/2, step 21706/23838 completed (loss: 0.815491795539856, acc: 0.6666666865348816)
[2025-02-05 14:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:06][root][INFO] - Training Epoch: 2/2, step 21707/23838 completed (loss: 1.062135934829712, acc: 0.6428571343421936)
[2025-02-05 14:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:07][root][INFO] - Training Epoch: 2/2, step 21708/23838 completed (loss: 1.2287921905517578, acc: 0.7142857313156128)
[2025-02-05 14:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:07][root][INFO] - Training Epoch: 2/2, step 21709/23838 completed (loss: 1.4113454818725586, acc: 0.6451612710952759)
[2025-02-05 14:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:08][root][INFO] - Training Epoch: 2/2, step 21710/23838 completed (loss: 1.2233306169509888, acc: 0.6734693646430969)
[2025-02-05 14:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:08][root][INFO] - Training Epoch: 2/2, step 21711/23838 completed (loss: 1.2552417516708374, acc: 0.7142857313156128)
[2025-02-05 14:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:08][root][INFO] - Training Epoch: 2/2, step 21712/23838 completed (loss: 0.9131330847740173, acc: 0.7142857313156128)
[2025-02-05 14:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:09][root][INFO] - Training Epoch: 2/2, step 21713/23838 completed (loss: 0.9588263034820557, acc: 0.7580645084381104)
[2025-02-05 14:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:09][root][INFO] - Training Epoch: 2/2, step 21714/23838 completed (loss: 0.954988420009613, acc: 0.7543859481811523)
[2025-02-05 14:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:09][root][INFO] - Training Epoch: 2/2, step 21715/23838 completed (loss: 1.1420161724090576, acc: 0.6973684430122375)
[2025-02-05 14:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:10][root][INFO] - Training Epoch: 2/2, step 21716/23838 completed (loss: 1.1011064052581787, acc: 0.6744186282157898)
[2025-02-05 14:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:10][root][INFO] - Training Epoch: 2/2, step 21717/23838 completed (loss: 1.2362738847732544, acc: 0.625)
[2025-02-05 14:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:11][root][INFO] - Training Epoch: 2/2, step 21718/23838 completed (loss: 1.1205161809921265, acc: 0.5789473652839661)
[2025-02-05 14:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:11][root][INFO] - Training Epoch: 2/2, step 21719/23838 completed (loss: 0.9196702837944031, acc: 0.7027027010917664)
[2025-02-05 14:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:11][root][INFO] - Training Epoch: 2/2, step 21720/23838 completed (loss: 1.306472659111023, acc: 0.6000000238418579)
[2025-02-05 14:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:12][root][INFO] - Training Epoch: 2/2, step 21721/23838 completed (loss: 1.1875090599060059, acc: 0.7096773982048035)
[2025-02-05 14:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:12][root][INFO] - Training Epoch: 2/2, step 21722/23838 completed (loss: 1.2132269144058228, acc: 0.6111111044883728)
[2025-02-05 14:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:13][root][INFO] - Training Epoch: 2/2, step 21723/23838 completed (loss: 0.5495340824127197, acc: 0.8181818127632141)
[2025-02-05 14:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:13][root][INFO] - Training Epoch: 2/2, step 21724/23838 completed (loss: 1.1324172019958496, acc: 0.7222222089767456)
[2025-02-05 14:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:13][root][INFO] - Training Epoch: 2/2, step 21725/23838 completed (loss: 0.8004834651947021, acc: 0.7288135886192322)
[2025-02-05 14:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:14][root][INFO] - Training Epoch: 2/2, step 21726/23838 completed (loss: 1.0594805479049683, acc: 0.707317054271698)
[2025-02-05 14:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:14][root][INFO] - Training Epoch: 2/2, step 21727/23838 completed (loss: 1.0907306671142578, acc: 0.7307692170143127)
[2025-02-05 14:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:14][root][INFO] - Training Epoch: 2/2, step 21728/23838 completed (loss: 1.348565936088562, acc: 0.6666666865348816)
[2025-02-05 14:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:15][root][INFO] - Training Epoch: 2/2, step 21729/23838 completed (loss: 1.6348364353179932, acc: 0.6086956262588501)
[2025-02-05 14:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:15][root][INFO] - Training Epoch: 2/2, step 21730/23838 completed (loss: 1.6665828227996826, acc: 0.5151515007019043)
[2025-02-05 14:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:16][root][INFO] - Training Epoch: 2/2, step 21731/23838 completed (loss: 1.2252845764160156, acc: 0.6103895902633667)
[2025-02-05 14:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:16][root][INFO] - Training Epoch: 2/2, step 21732/23838 completed (loss: 1.4717830419540405, acc: 0.6170212626457214)
[2025-02-05 14:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:16][root][INFO] - Training Epoch: 2/2, step 21733/23838 completed (loss: 1.5893951654434204, acc: 0.5164835453033447)
[2025-02-05 14:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:17][root][INFO] - Training Epoch: 2/2, step 21734/23838 completed (loss: 1.6838951110839844, acc: 0.5147058963775635)
[2025-02-05 14:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:17][root][INFO] - Training Epoch: 2/2, step 21735/23838 completed (loss: 1.160995364189148, acc: 0.650602400302887)
[2025-02-05 14:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:18][root][INFO] - Training Epoch: 2/2, step 21736/23838 completed (loss: 1.0221670866012573, acc: 0.7058823704719543)
[2025-02-05 14:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:18][root][INFO] - Training Epoch: 2/2, step 21737/23838 completed (loss: 0.8637280464172363, acc: 0.7528089880943298)
[2025-02-05 14:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:19][root][INFO] - Training Epoch: 2/2, step 21738/23838 completed (loss: 1.0539813041687012, acc: 0.6625000238418579)
[2025-02-05 14:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:19][root][INFO] - Training Epoch: 2/2, step 21739/23838 completed (loss: 1.2310917377471924, acc: 0.6688741445541382)
[2025-02-05 14:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:20][root][INFO] - Training Epoch: 2/2, step 21740/23838 completed (loss: 1.1782357692718506, acc: 0.699999988079071)
[2025-02-05 14:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:20][root][INFO] - Training Epoch: 2/2, step 21741/23838 completed (loss: 1.2957673072814941, acc: 0.6476190686225891)
[2025-02-05 14:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:21][root][INFO] - Training Epoch: 2/2, step 21742/23838 completed (loss: 0.923259973526001, acc: 0.7361111044883728)
[2025-02-05 14:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:21][root][INFO] - Training Epoch: 2/2, step 21743/23838 completed (loss: 0.5693408846855164, acc: 0.9090909361839294)
[2025-02-05 14:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:22][root][INFO] - Training Epoch: 2/2, step 21744/23838 completed (loss: 0.7493616342544556, acc: 0.6764705777168274)
[2025-02-05 14:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:22][root][INFO] - Training Epoch: 2/2, step 21745/23838 completed (loss: 1.0738362073898315, acc: 0.6785714030265808)
[2025-02-05 14:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:23][root][INFO] - Training Epoch: 2/2, step 21746/23838 completed (loss: 0.8761471509933472, acc: 0.737864077091217)
[2025-02-05 14:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:23][root][INFO] - Training Epoch: 2/2, step 21747/23838 completed (loss: 1.091152310371399, acc: 0.7007299065589905)
[2025-02-05 14:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:23][root][INFO] - Training Epoch: 2/2, step 21748/23838 completed (loss: 1.0702825784683228, acc: 0.7028985619544983)
[2025-02-05 14:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:24][root][INFO] - Training Epoch: 2/2, step 21749/23838 completed (loss: 1.1969642639160156, acc: 0.6265060305595398)
[2025-02-05 14:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:24][root][INFO] - Training Epoch: 2/2, step 21750/23838 completed (loss: 1.2283629179000854, acc: 0.6521739363670349)
[2025-02-05 14:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:24][root][INFO] - Training Epoch: 2/2, step 21751/23838 completed (loss: 1.1012905836105347, acc: 0.679347813129425)
[2025-02-05 14:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:25][root][INFO] - Training Epoch: 2/2, step 21752/23838 completed (loss: 1.2708048820495605, acc: 0.621052622795105)
[2025-02-05 14:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:25][root][INFO] - Training Epoch: 2/2, step 21753/23838 completed (loss: 1.2696884870529175, acc: 0.6800000071525574)
[2025-02-05 14:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:25][root][INFO] - Training Epoch: 2/2, step 21754/23838 completed (loss: 1.2200570106506348, acc: 0.6341463327407837)
[2025-02-05 14:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:26][root][INFO] - Training Epoch: 2/2, step 21755/23838 completed (loss: 1.459995985031128, acc: 0.508474588394165)
[2025-02-05 14:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:26][root][INFO] - Training Epoch: 2/2, step 21756/23838 completed (loss: 0.8694982528686523, acc: 0.7191011309623718)
[2025-02-05 14:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:26][root][INFO] - Training Epoch: 2/2, step 21757/23838 completed (loss: 0.871863067150116, acc: 0.7093023061752319)
[2025-02-05 14:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:27][root][INFO] - Training Epoch: 2/2, step 21758/23838 completed (loss: 1.0401252508163452, acc: 0.6727272868156433)
[2025-02-05 14:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:27][root][INFO] - Training Epoch: 2/2, step 21759/23838 completed (loss: 0.7408004403114319, acc: 0.791208803653717)
[2025-02-05 14:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:28][root][INFO] - Training Epoch: 2/2, step 21760/23838 completed (loss: 0.856415867805481, acc: 0.7017543911933899)
[2025-02-05 14:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:28][root][INFO] - Training Epoch: 2/2, step 21761/23838 completed (loss: 1.3436915874481201, acc: 0.5909090638160706)
[2025-02-05 14:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:29][root][INFO] - Training Epoch: 2/2, step 21762/23838 completed (loss: 1.3125672340393066, acc: 0.6081081032752991)
[2025-02-05 14:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:29][root][INFO] - Training Epoch: 2/2, step 21763/23838 completed (loss: 1.1585891246795654, acc: 0.7142857313156128)
[2025-02-05 14:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:29][root][INFO] - Training Epoch: 2/2, step 21764/23838 completed (loss: 0.9175692200660706, acc: 0.75)
[2025-02-05 14:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:30][root][INFO] - Training Epoch: 2/2, step 21765/23838 completed (loss: 1.0045104026794434, acc: 0.695652186870575)
[2025-02-05 14:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:30][root][INFO] - Training Epoch: 2/2, step 21766/23838 completed (loss: 1.1322147846221924, acc: 0.6507936716079712)
[2025-02-05 14:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:31][root][INFO] - Training Epoch: 2/2, step 21767/23838 completed (loss: 1.1944509744644165, acc: 0.6486486196517944)
[2025-02-05 14:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:31][root][INFO] - Training Epoch: 2/2, step 21768/23838 completed (loss: 1.2232862710952759, acc: 0.6666666865348816)
[2025-02-05 14:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:31][root][INFO] - Training Epoch: 2/2, step 21769/23838 completed (loss: 0.9534632563591003, acc: 0.7171717286109924)
[2025-02-05 14:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:32][root][INFO] - Training Epoch: 2/2, step 21770/23838 completed (loss: 1.2678991556167603, acc: 0.646616518497467)
[2025-02-05 14:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:32][root][INFO] - Training Epoch: 2/2, step 21771/23838 completed (loss: 1.137007713317871, acc: 0.6555555462837219)
[2025-02-05 14:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:33][root][INFO] - Training Epoch: 2/2, step 21772/23838 completed (loss: 1.1446462869644165, acc: 0.6410256624221802)
[2025-02-05 14:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:33][root][INFO] - Training Epoch: 2/2, step 21773/23838 completed (loss: 1.195603847503662, acc: 0.6811594367027283)
[2025-02-05 14:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:33][root][INFO] - Training Epoch: 2/2, step 21774/23838 completed (loss: 0.989362895488739, acc: 0.7394958138465881)
[2025-02-05 14:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:34][root][INFO] - Training Epoch: 2/2, step 21775/23838 completed (loss: 1.211112141609192, acc: 0.6653061509132385)
[2025-02-05 14:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:34][root][INFO] - Training Epoch: 2/2, step 21776/23838 completed (loss: 1.0845197439193726, acc: 0.6388888955116272)
[2025-02-05 14:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:35][root][INFO] - Training Epoch: 2/2, step 21777/23838 completed (loss: 0.9668140411376953, acc: 0.6930692791938782)
[2025-02-05 14:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:35][root][INFO] - Training Epoch: 2/2, step 21778/23838 completed (loss: 1.1799352169036865, acc: 0.6666666865348816)
[2025-02-05 14:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:35][root][INFO] - Training Epoch: 2/2, step 21779/23838 completed (loss: 1.17295503616333, acc: 0.6515151262283325)
[2025-02-05 14:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:36][root][INFO] - Training Epoch: 2/2, step 21780/23838 completed (loss: 1.340719223022461, acc: 0.6086956262588501)
[2025-02-05 14:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:36][root][INFO] - Training Epoch: 2/2, step 21781/23838 completed (loss: 0.9406728148460388, acc: 0.6666666865348816)
[2025-02-05 14:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:37][root][INFO] - Training Epoch: 2/2, step 21782/23838 completed (loss: 1.5851176977157593, acc: 0.5319148898124695)
[2025-02-05 14:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:37][root][INFO] - Training Epoch: 2/2, step 21783/23838 completed (loss: 0.8184584975242615, acc: 0.7549019455909729)
[2025-02-05 14:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:37][root][INFO] - Training Epoch: 2/2, step 21784/23838 completed (loss: 1.022520899772644, acc: 0.6697247624397278)
[2025-02-05 14:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:38][root][INFO] - Training Epoch: 2/2, step 21785/23838 completed (loss: 1.134629249572754, acc: 0.6666666865348816)
[2025-02-05 14:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:38][root][INFO] - Training Epoch: 2/2, step 21786/23838 completed (loss: 0.7012843489646912, acc: 0.7692307829856873)
[2025-02-05 14:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:39][root][INFO] - Training Epoch: 2/2, step 21787/23838 completed (loss: 1.0165903568267822, acc: 0.6896551847457886)
[2025-02-05 14:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:39][root][INFO] - Training Epoch: 2/2, step 21788/23838 completed (loss: 1.3216397762298584, acc: 0.6132075190544128)
[2025-02-05 14:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:39][root][INFO] - Training Epoch: 2/2, step 21789/23838 completed (loss: 0.662377655506134, acc: 0.8405796885490417)
[2025-02-05 14:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:40][root][INFO] - Training Epoch: 2/2, step 21790/23838 completed (loss: 1.0172271728515625, acc: 0.699999988079071)
[2025-02-05 14:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:40][root][INFO] - Training Epoch: 2/2, step 21791/23838 completed (loss: 1.024272084236145, acc: 0.6976743936538696)
[2025-02-05 14:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:41][root][INFO] - Training Epoch: 2/2, step 21792/23838 completed (loss: 1.0070478916168213, acc: 0.6982758641242981)
[2025-02-05 14:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:41][root][INFO] - Training Epoch: 2/2, step 21793/23838 completed (loss: 1.1455271244049072, acc: 0.6545454263687134)
[2025-02-05 14:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:41][root][INFO] - Training Epoch: 2/2, step 21794/23838 completed (loss: 1.0173314809799194, acc: 0.6666666865348816)
[2025-02-05 14:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:42][root][INFO] - Training Epoch: 2/2, step 21795/23838 completed (loss: 1.1527235507965088, acc: 0.6451612710952759)
[2025-02-05 14:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:42][root][INFO] - Training Epoch: 2/2, step 21796/23838 completed (loss: 1.188300371170044, acc: 0.6304348111152649)
[2025-02-05 14:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:43][root][INFO] - Training Epoch: 2/2, step 21797/23838 completed (loss: 1.3254486322402954, acc: 0.5833333134651184)
[2025-02-05 14:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:43][root][INFO] - Training Epoch: 2/2, step 21798/23838 completed (loss: 1.3919968605041504, acc: 0.5833333134651184)
[2025-02-05 14:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:43][root][INFO] - Training Epoch: 2/2, step 21799/23838 completed (loss: 0.7601851224899292, acc: 0.7976190447807312)
[2025-02-05 14:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:44][root][INFO] - Training Epoch: 2/2, step 21800/23838 completed (loss: 1.147679090499878, acc: 0.6575342416763306)
[2025-02-05 14:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:44][root][INFO] - Training Epoch: 2/2, step 21801/23838 completed (loss: 1.161873698234558, acc: 0.611940324306488)
[2025-02-05 14:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:45][root][INFO] - Training Epoch: 2/2, step 21802/23838 completed (loss: 1.0868116617202759, acc: 0.746666669845581)
[2025-02-05 14:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:45][root][INFO] - Training Epoch: 2/2, step 21803/23838 completed (loss: 0.890820324420929, acc: 0.7441860437393188)
[2025-02-05 14:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:45][root][INFO] - Training Epoch: 2/2, step 21804/23838 completed (loss: 1.202248454093933, acc: 0.6321839094161987)
[2025-02-05 14:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:46][root][INFO] - Training Epoch: 2/2, step 21805/23838 completed (loss: 0.9309645891189575, acc: 0.7272727489471436)
[2025-02-05 14:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:46][root][INFO] - Training Epoch: 2/2, step 21806/23838 completed (loss: 0.778205931186676, acc: 0.7894737124443054)
[2025-02-05 14:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:47][root][INFO] - Training Epoch: 2/2, step 21807/23838 completed (loss: 1.2051948308944702, acc: 0.6499999761581421)
[2025-02-05 14:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:47][root][INFO] - Training Epoch: 2/2, step 21808/23838 completed (loss: 1.3865545988082886, acc: 0.6486486196517944)
[2025-02-05 14:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:47][root][INFO] - Training Epoch: 2/2, step 21809/23838 completed (loss: 1.2924230098724365, acc: 0.5866666436195374)
[2025-02-05 14:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:48][root][INFO] - Training Epoch: 2/2, step 21810/23838 completed (loss: 0.9495383501052856, acc: 0.7156862616539001)
[2025-02-05 14:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:48][root][INFO] - Training Epoch: 2/2, step 21811/23838 completed (loss: 1.3065061569213867, acc: 0.6034482717514038)
[2025-02-05 14:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:49][root][INFO] - Training Epoch: 2/2, step 21812/23838 completed (loss: 1.051526427268982, acc: 0.701298713684082)
[2025-02-05 14:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:49][root][INFO] - Training Epoch: 2/2, step 21813/23838 completed (loss: 0.7429929375648499, acc: 0.7551020383834839)
[2025-02-05 14:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:49][root][INFO] - Training Epoch: 2/2, step 21814/23838 completed (loss: 0.9291085600852966, acc: 0.695652186870575)
[2025-02-05 14:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:50][root][INFO] - Training Epoch: 2/2, step 21815/23838 completed (loss: 1.1472944021224976, acc: 0.6060606241226196)
[2025-02-05 14:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:50][root][INFO] - Training Epoch: 2/2, step 21816/23838 completed (loss: 0.7593205571174622, acc: 0.8160919547080994)
[2025-02-05 14:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:50][root][INFO] - Training Epoch: 2/2, step 21817/23838 completed (loss: 1.292984127998352, acc: 0.6173912882804871)
[2025-02-05 14:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:51][root][INFO] - Training Epoch: 2/2, step 21818/23838 completed (loss: 1.2227537631988525, acc: 0.625)
[2025-02-05 14:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:51][root][INFO] - Training Epoch: 2/2, step 21819/23838 completed (loss: 0.880656361579895, acc: 0.7411764860153198)
[2025-02-05 14:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:51][root][INFO] - Training Epoch: 2/2, step 21820/23838 completed (loss: 1.0641508102416992, acc: 0.6551724076271057)
[2025-02-05 14:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:52][root][INFO] - Training Epoch: 2/2, step 21821/23838 completed (loss: 1.3013333082199097, acc: 0.6349206566810608)
[2025-02-05 14:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:52][root][INFO] - Training Epoch: 2/2, step 21822/23838 completed (loss: 1.2591041326522827, acc: 0.546875)
[2025-02-05 14:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:53][root][INFO] - Training Epoch: 2/2, step 21823/23838 completed (loss: 1.3579161167144775, acc: 0.6052631735801697)
[2025-02-05 14:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:53][root][INFO] - Training Epoch: 2/2, step 21824/23838 completed (loss: 1.5146536827087402, acc: 0.5370370149612427)
[2025-02-05 14:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:54][root][INFO] - Training Epoch: 2/2, step 21825/23838 completed (loss: 1.1604220867156982, acc: 0.6724137663841248)
[2025-02-05 14:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:54][root][INFO] - Training Epoch: 2/2, step 21826/23838 completed (loss: 1.0521618127822876, acc: 0.6122449040412903)
[2025-02-05 14:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:54][root][INFO] - Training Epoch: 2/2, step 21827/23838 completed (loss: 1.013822078704834, acc: 0.7352941036224365)
[2025-02-05 14:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:55][root][INFO] - Training Epoch: 2/2, step 21828/23838 completed (loss: 0.5983572006225586, acc: 0.7400000095367432)
[2025-02-05 14:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:55][root][INFO] - Training Epoch: 2/2, step 21829/23838 completed (loss: 0.9174068570137024, acc: 0.747474730014801)
[2025-02-05 14:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:55][root][INFO] - Training Epoch: 2/2, step 21830/23838 completed (loss: 1.3924602270126343, acc: 0.6666666865348816)
[2025-02-05 14:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:56][root][INFO] - Training Epoch: 2/2, step 21831/23838 completed (loss: 0.6001436710357666, acc: 0.8524590134620667)
[2025-02-05 14:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:56][root][INFO] - Training Epoch: 2/2, step 21832/23838 completed (loss: 1.1207551956176758, acc: 0.6438356041908264)
[2025-02-05 14:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:56][root][INFO] - Training Epoch: 2/2, step 21833/23838 completed (loss: 0.972838819026947, acc: 0.7272727489471436)
[2025-02-05 14:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:57][root][INFO] - Training Epoch: 2/2, step 21834/23838 completed (loss: 1.2979967594146729, acc: 0.65625)
[2025-02-05 14:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:57][root][INFO] - Training Epoch: 2/2, step 21835/23838 completed (loss: 0.9124671816825867, acc: 0.699999988079071)
[2025-02-05 14:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:58][root][INFO] - Training Epoch: 2/2, step 21836/23838 completed (loss: 1.0710557699203491, acc: 0.7457627058029175)
[2025-02-05 14:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:58][root][INFO] - Training Epoch: 2/2, step 21837/23838 completed (loss: 1.1976313591003418, acc: 0.6818181872367859)
[2025-02-05 14:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:58][root][INFO] - Training Epoch: 2/2, step 21838/23838 completed (loss: 1.1925474405288696, acc: 0.6290322542190552)
[2025-02-05 14:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:59][root][INFO] - Training Epoch: 2/2, step 21839/23838 completed (loss: 1.0704444646835327, acc: 0.6931818127632141)
[2025-02-05 14:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:59][root][INFO] - Training Epoch: 2/2, step 21840/23838 completed (loss: 1.0399274826049805, acc: 0.6551724076271057)
[2025-02-05 14:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:05:59][root][INFO] - Training Epoch: 2/2, step 21841/23838 completed (loss: 1.3132853507995605, acc: 0.6170212626457214)
[2025-02-05 14:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:00][root][INFO] - Training Epoch: 2/2, step 21842/23838 completed (loss: 0.9072983264923096, acc: 0.6875)
[2025-02-05 14:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:00][root][INFO] - Training Epoch: 2/2, step 21843/23838 completed (loss: 0.9689525961875916, acc: 0.6875)
[2025-02-05 14:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:00][root][INFO] - Training Epoch: 2/2, step 21844/23838 completed (loss: 0.9441781044006348, acc: 0.7096773982048035)
[2025-02-05 14:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:01][root][INFO] - Training Epoch: 2/2, step 21845/23838 completed (loss: 0.9777452349662781, acc: 0.699999988079071)
[2025-02-05 14:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:01][root][INFO] - Training Epoch: 2/2, step 21846/23838 completed (loss: 1.0429201126098633, acc: 0.6481481194496155)
[2025-02-05 14:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:02][root][INFO] - Training Epoch: 2/2, step 21847/23838 completed (loss: 0.8484998941421509, acc: 0.739130437374115)
[2025-02-05 14:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:02][root][INFO] - Training Epoch: 2/2, step 21848/23838 completed (loss: 0.9755445718765259, acc: 0.7272727489471436)
[2025-02-05 14:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:03][root][INFO] - Training Epoch: 2/2, step 21849/23838 completed (loss: 1.0661680698394775, acc: 0.699999988079071)
[2025-02-05 14:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:03][root][INFO] - Training Epoch: 2/2, step 21850/23838 completed (loss: 1.3233721256256104, acc: 0.5932203531265259)
[2025-02-05 14:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:03][root][INFO] - Training Epoch: 2/2, step 21851/23838 completed (loss: 1.1686773300170898, acc: 0.5789473652839661)
[2025-02-05 14:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:04][root][INFO] - Training Epoch: 2/2, step 21852/23838 completed (loss: 1.0762187242507935, acc: 0.6666666865348816)
[2025-02-05 14:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:04][root][INFO] - Training Epoch: 2/2, step 21853/23838 completed (loss: 0.7579931020736694, acc: 0.800000011920929)
[2025-02-05 14:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:04][root][INFO] - Training Epoch: 2/2, step 21854/23838 completed (loss: 0.9697808027267456, acc: 0.7142857313156128)
[2025-02-05 14:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:05][root][INFO] - Training Epoch: 2/2, step 21855/23838 completed (loss: 1.059985637664795, acc: 0.7049180269241333)
[2025-02-05 14:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:05][root][INFO] - Training Epoch: 2/2, step 21856/23838 completed (loss: 1.2998111248016357, acc: 0.632478654384613)
[2025-02-05 14:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:06][root][INFO] - Training Epoch: 2/2, step 21857/23838 completed (loss: 1.0426816940307617, acc: 0.6875)
[2025-02-05 14:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:06][root][INFO] - Training Epoch: 2/2, step 21858/23838 completed (loss: 1.241615653038025, acc: 0.6727272868156433)
[2025-02-05 14:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:06][root][INFO] - Training Epoch: 2/2, step 21859/23838 completed (loss: 1.3562291860580444, acc: 0.5308641791343689)
[2025-02-05 14:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:07][root][INFO] - Training Epoch: 2/2, step 21860/23838 completed (loss: 1.1133190393447876, acc: 0.7352941036224365)
[2025-02-05 14:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:07][root][INFO] - Training Epoch: 2/2, step 21861/23838 completed (loss: 0.8905666470527649, acc: 0.7291666865348816)
[2025-02-05 14:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:07][root][INFO] - Training Epoch: 2/2, step 21862/23838 completed (loss: 0.9566943645477295, acc: 0.7333333492279053)
[2025-02-05 14:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:08][root][INFO] - Training Epoch: 2/2, step 21863/23838 completed (loss: 0.9964756369590759, acc: 0.7681159377098083)
[2025-02-05 14:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:08][root][INFO] - Training Epoch: 2/2, step 21864/23838 completed (loss: 1.40469229221344, acc: 0.594936728477478)
[2025-02-05 14:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:08][root][INFO] - Training Epoch: 2/2, step 21865/23838 completed (loss: 1.007743239402771, acc: 0.7083333134651184)
[2025-02-05 14:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:09][root][INFO] - Training Epoch: 2/2, step 21866/23838 completed (loss: 1.1765965223312378, acc: 0.6470588445663452)
[2025-02-05 14:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:09][root][INFO] - Training Epoch: 2/2, step 21867/23838 completed (loss: 1.0333852767944336, acc: 0.7155172228813171)
[2025-02-05 14:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:10][root][INFO] - Training Epoch: 2/2, step 21868/23838 completed (loss: 1.4831888675689697, acc: 0.6029411554336548)
[2025-02-05 14:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:10][root][INFO] - Training Epoch: 2/2, step 21869/23838 completed (loss: 1.0550715923309326, acc: 0.6623376607894897)
[2025-02-05 14:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:10][root][INFO] - Training Epoch: 2/2, step 21870/23838 completed (loss: 0.9701340198516846, acc: 0.6938775777816772)
[2025-02-05 14:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:11][root][INFO] - Training Epoch: 2/2, step 21871/23838 completed (loss: 1.2099356651306152, acc: 0.6000000238418579)
[2025-02-05 14:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:11][root][INFO] - Training Epoch: 2/2, step 21872/23838 completed (loss: 0.9007688760757446, acc: 0.7692307829856873)
[2025-02-05 14:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:12][root][INFO] - Training Epoch: 2/2, step 21873/23838 completed (loss: 1.128927230834961, acc: 0.6590909361839294)
[2025-02-05 14:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:12][root][INFO] - Training Epoch: 2/2, step 21874/23838 completed (loss: 0.7414482831954956, acc: 0.7659574747085571)
[2025-02-05 14:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:12][root][INFO] - Training Epoch: 2/2, step 21875/23838 completed (loss: 1.0352706909179688, acc: 0.6714285612106323)
[2025-02-05 14:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:13][root][INFO] - Training Epoch: 2/2, step 21876/23838 completed (loss: 1.387063980102539, acc: 0.5714285969734192)
[2025-02-05 14:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:13][root][INFO] - Training Epoch: 2/2, step 21877/23838 completed (loss: 1.2298033237457275, acc: 0.5974025726318359)
[2025-02-05 14:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:14][root][INFO] - Training Epoch: 2/2, step 21878/23838 completed (loss: 1.1366546154022217, acc: 0.6724137663841248)
[2025-02-05 14:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:14][root][INFO] - Training Epoch: 2/2, step 21879/23838 completed (loss: 1.3072019815444946, acc: 0.6428571343421936)
[2025-02-05 14:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:14][root][INFO] - Training Epoch: 2/2, step 21880/23838 completed (loss: 1.406396746635437, acc: 0.5625)
[2025-02-05 14:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:15][root][INFO] - Training Epoch: 2/2, step 21881/23838 completed (loss: 1.3149131536483765, acc: 0.6451612710952759)
[2025-02-05 14:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:15][root][INFO] - Training Epoch: 2/2, step 21882/23838 completed (loss: 0.9825847744941711, acc: 0.699999988079071)
[2025-02-05 14:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:15][root][INFO] - Training Epoch: 2/2, step 21883/23838 completed (loss: 0.9832040071487427, acc: 0.7272727489471436)
[2025-02-05 14:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:16][root][INFO] - Training Epoch: 2/2, step 21884/23838 completed (loss: 1.2411459684371948, acc: 0.6700000166893005)
[2025-02-05 14:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:16][root][INFO] - Training Epoch: 2/2, step 21885/23838 completed (loss: 1.1333301067352295, acc: 0.6379310488700867)
[2025-02-05 14:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:16][root][INFO] - Training Epoch: 2/2, step 21886/23838 completed (loss: 1.0858622789382935, acc: 0.6701030731201172)
[2025-02-05 14:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:17][root][INFO] - Training Epoch: 2/2, step 21887/23838 completed (loss: 1.494623064994812, acc: 0.5517241358757019)
[2025-02-05 14:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:17][root][INFO] - Training Epoch: 2/2, step 21888/23838 completed (loss: 0.9732062220573425, acc: 0.7021276354789734)
[2025-02-05 14:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:18][root][INFO] - Training Epoch: 2/2, step 21889/23838 completed (loss: 1.1390362977981567, acc: 0.6341463327407837)
[2025-02-05 14:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:18][root][INFO] - Training Epoch: 2/2, step 21890/23838 completed (loss: 1.125114917755127, acc: 0.6976743936538696)
[2025-02-05 14:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:18][root][INFO] - Training Epoch: 2/2, step 21891/23838 completed (loss: 1.1211349964141846, acc: 0.6851851940155029)
[2025-02-05 14:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:19][root][INFO] - Training Epoch: 2/2, step 21892/23838 completed (loss: 1.4936800003051758, acc: 0.5581395626068115)
[2025-02-05 14:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:19][root][INFO] - Training Epoch: 2/2, step 21893/23838 completed (loss: 0.7815870046615601, acc: 0.7837837934494019)
[2025-02-05 14:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:20][root][INFO] - Training Epoch: 2/2, step 21894/23838 completed (loss: 1.161656379699707, acc: 0.6611570119857788)
[2025-02-05 14:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:20][root][INFO] - Training Epoch: 2/2, step 21895/23838 completed (loss: 0.9704654812812805, acc: 0.7088607549667358)
[2025-02-05 14:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:20][root][INFO] - Training Epoch: 2/2, step 21896/23838 completed (loss: 0.8128268718719482, acc: 0.686274528503418)
[2025-02-05 14:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:21][root][INFO] - Training Epoch: 2/2, step 21897/23838 completed (loss: 1.2791475057601929, acc: 0.643478274345398)
[2025-02-05 14:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:21][root][INFO] - Training Epoch: 2/2, step 21898/23838 completed (loss: 0.960949182510376, acc: 0.7441860437393188)
[2025-02-05 14:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:21][root][INFO] - Training Epoch: 2/2, step 21899/23838 completed (loss: 1.2104593515396118, acc: 0.6538461446762085)
[2025-02-05 14:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:22][root][INFO] - Training Epoch: 2/2, step 21900/23838 completed (loss: 1.1016788482666016, acc: 0.653333306312561)
[2025-02-05 14:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:22][root][INFO] - Training Epoch: 2/2, step 21901/23838 completed (loss: 1.3918859958648682, acc: 0.6095238327980042)
[2025-02-05 14:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:23][root][INFO] - Training Epoch: 2/2, step 21902/23838 completed (loss: 1.2114675045013428, acc: 0.6056337952613831)
[2025-02-05 14:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:23][root][INFO] - Training Epoch: 2/2, step 21903/23838 completed (loss: 0.9369615316390991, acc: 0.7068965435028076)
[2025-02-05 14:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:23][root][INFO] - Training Epoch: 2/2, step 21904/23838 completed (loss: 1.337061882019043, acc: 0.6304348111152649)
[2025-02-05 14:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:24][root][INFO] - Training Epoch: 2/2, step 21905/23838 completed (loss: 1.3114420175552368, acc: 0.5873016119003296)
[2025-02-05 14:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:24][root][INFO] - Training Epoch: 2/2, step 21906/23838 completed (loss: 0.7900357842445374, acc: 0.7398374080657959)
[2025-02-05 14:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:25][root][INFO] - Training Epoch: 2/2, step 21907/23838 completed (loss: 1.0841968059539795, acc: 0.6454545259475708)
[2025-02-05 14:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:25][root][INFO] - Training Epoch: 2/2, step 21908/23838 completed (loss: 1.1693295240402222, acc: 0.625)
[2025-02-05 14:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:25][root][INFO] - Training Epoch: 2/2, step 21909/23838 completed (loss: 0.9922851324081421, acc: 0.6867470145225525)
[2025-02-05 14:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:26][root][INFO] - Training Epoch: 2/2, step 21910/23838 completed (loss: 1.197401762008667, acc: 0.6859503984451294)
[2025-02-05 14:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:26][root][INFO] - Training Epoch: 2/2, step 21911/23838 completed (loss: 1.241025686264038, acc: 0.5614035129547119)
[2025-02-05 14:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:27][root][INFO] - Training Epoch: 2/2, step 21912/23838 completed (loss: 1.208540916442871, acc: 0.7099236845970154)
[2025-02-05 14:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:27][root][INFO] - Training Epoch: 2/2, step 21913/23838 completed (loss: 0.5249784588813782, acc: 0.9189189076423645)
[2025-02-05 14:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:27][root][INFO] - Training Epoch: 2/2, step 21914/23838 completed (loss: 1.062601923942566, acc: 0.7435897588729858)
[2025-02-05 14:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:28][root][INFO] - Training Epoch: 2/2, step 21915/23838 completed (loss: 1.348669171333313, acc: 0.5396825671195984)
[2025-02-05 14:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:28][root][INFO] - Training Epoch: 2/2, step 21916/23838 completed (loss: 1.2855826616287231, acc: 0.6734693646430969)
[2025-02-05 14:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:29][root][INFO] - Training Epoch: 2/2, step 21917/23838 completed (loss: 1.2323203086853027, acc: 0.6808510422706604)
[2025-02-05 14:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:29][root][INFO] - Training Epoch: 2/2, step 21918/23838 completed (loss: 1.4778283834457397, acc: 0.5411764979362488)
[2025-02-05 14:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:30][root][INFO] - Training Epoch: 2/2, step 21919/23838 completed (loss: 1.0584876537322998, acc: 0.6451612710952759)
[2025-02-05 14:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:30][root][INFO] - Training Epoch: 2/2, step 21920/23838 completed (loss: 1.4759840965270996, acc: 0.568965494632721)
[2025-02-05 14:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:30][root][INFO] - Training Epoch: 2/2, step 21921/23838 completed (loss: 1.2874144315719604, acc: 0.6384615302085876)
[2025-02-05 14:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:31][root][INFO] - Training Epoch: 2/2, step 21922/23838 completed (loss: 0.5915672183036804, acc: 0.8214285969734192)
[2025-02-05 14:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:31][root][INFO] - Training Epoch: 2/2, step 21923/23838 completed (loss: 1.1597909927368164, acc: 0.65625)
[2025-02-05 14:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:32][root][INFO] - Training Epoch: 2/2, step 21924/23838 completed (loss: 1.04343581199646, acc: 0.7692307829856873)
[2025-02-05 14:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:32][root][INFO] - Training Epoch: 2/2, step 21925/23838 completed (loss: 1.7342479228973389, acc: 0.49056604504585266)
[2025-02-05 14:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:32][root][INFO] - Training Epoch: 2/2, step 21926/23838 completed (loss: 1.2233153581619263, acc: 0.6226415038108826)
[2025-02-05 14:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:33][root][INFO] - Training Epoch: 2/2, step 21927/23838 completed (loss: 1.1489596366882324, acc: 0.644444465637207)
[2025-02-05 14:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:33][root][INFO] - Training Epoch: 2/2, step 21928/23838 completed (loss: 1.7963871955871582, acc: 0.4848484992980957)
[2025-02-05 14:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:34][root][INFO] - Training Epoch: 2/2, step 21929/23838 completed (loss: 1.0901305675506592, acc: 0.695652186870575)
[2025-02-05 14:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:34][root][INFO] - Training Epoch: 2/2, step 21930/23838 completed (loss: 1.191399097442627, acc: 0.6969696879386902)
[2025-02-05 14:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:34][root][INFO] - Training Epoch: 2/2, step 21931/23838 completed (loss: 0.7956479787826538, acc: 0.6969696879386902)
[2025-02-05 14:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:35][root][INFO] - Training Epoch: 2/2, step 21932/23838 completed (loss: 1.259110450744629, acc: 0.6000000238418579)
[2025-02-05 14:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:35][root][INFO] - Training Epoch: 2/2, step 21933/23838 completed (loss: 0.4958077073097229, acc: 0.84375)
[2025-02-05 14:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:36][root][INFO] - Training Epoch: 2/2, step 21934/23838 completed (loss: 1.3310316801071167, acc: 0.6000000238418579)
[2025-02-05 14:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:36][root][INFO] - Training Epoch: 2/2, step 21935/23838 completed (loss: 1.6372545957565308, acc: 0.5897436141967773)
[2025-02-05 14:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:36][root][INFO] - Training Epoch: 2/2, step 21936/23838 completed (loss: 1.4031425714492798, acc: 0.6153846383094788)
[2025-02-05 14:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:37][root][INFO] - Training Epoch: 2/2, step 21937/23838 completed (loss: 0.5541843175888062, acc: 0.8648648858070374)
[2025-02-05 14:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:37][root][INFO] - Training Epoch: 2/2, step 21938/23838 completed (loss: 0.9082362651824951, acc: 0.7428571581840515)
[2025-02-05 14:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:38][root][INFO] - Training Epoch: 2/2, step 21939/23838 completed (loss: 1.3455229997634888, acc: 0.6461538672447205)
[2025-02-05 14:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:38][root][INFO] - Training Epoch: 2/2, step 21940/23838 completed (loss: 1.1326706409454346, acc: 0.7037037014961243)
[2025-02-05 14:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:38][root][INFO] - Training Epoch: 2/2, step 21941/23838 completed (loss: 0.9362666606903076, acc: 0.6842105388641357)
[2025-02-05 14:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:39][root][INFO] - Training Epoch: 2/2, step 21942/23838 completed (loss: 1.56472909450531, acc: 0.5652173757553101)
[2025-02-05 14:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:39][root][INFO] - Training Epoch: 2/2, step 21943/23838 completed (loss: 1.2211824655532837, acc: 0.6296296119689941)
[2025-02-05 14:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:40][root][INFO] - Training Epoch: 2/2, step 21944/23838 completed (loss: 1.559497356414795, acc: 0.5636363625526428)
[2025-02-05 14:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:40][root][INFO] - Training Epoch: 2/2, step 21945/23838 completed (loss: 0.7665412425994873, acc: 0.7857142686843872)
[2025-02-05 14:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:40][root][INFO] - Training Epoch: 2/2, step 21946/23838 completed (loss: 0.9511929750442505, acc: 0.774193525314331)
[2025-02-05 14:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:41][root][INFO] - Training Epoch: 2/2, step 21947/23838 completed (loss: 1.0505788326263428, acc: 0.717391312122345)
[2025-02-05 14:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:41][root][INFO] - Training Epoch: 2/2, step 21948/23838 completed (loss: 1.6230014562606812, acc: 0.550000011920929)
[2025-02-05 14:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:42][root][INFO] - Training Epoch: 2/2, step 21949/23838 completed (loss: 1.0670822858810425, acc: 0.6976743936538696)
[2025-02-05 14:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:42][root][INFO] - Training Epoch: 2/2, step 21950/23838 completed (loss: 1.019525170326233, acc: 0.675000011920929)
[2025-02-05 14:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:42][root][INFO] - Training Epoch: 2/2, step 21951/23838 completed (loss: 1.1430195569992065, acc: 0.6730769276618958)
[2025-02-05 14:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:43][root][INFO] - Training Epoch: 2/2, step 21952/23838 completed (loss: 1.2147499322891235, acc: 0.5526315569877625)
[2025-02-05 14:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:43][root][INFO] - Training Epoch: 2/2, step 21953/23838 completed (loss: 1.2388088703155518, acc: 0.6792452931404114)
[2025-02-05 14:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:44][root][INFO] - Training Epoch: 2/2, step 21954/23838 completed (loss: 1.2124747037887573, acc: 0.688524603843689)
[2025-02-05 14:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:44][root][INFO] - Training Epoch: 2/2, step 21955/23838 completed (loss: 1.199919581413269, acc: 0.6944444179534912)
[2025-02-05 14:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:45][root][INFO] - Training Epoch: 2/2, step 21956/23838 completed (loss: 1.5128250122070312, acc: 0.5641025900840759)
[2025-02-05 14:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:45][root][INFO] - Training Epoch: 2/2, step 21957/23838 completed (loss: 1.0789631605148315, acc: 0.6896551847457886)
[2025-02-05 14:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:45][root][INFO] - Training Epoch: 2/2, step 21958/23838 completed (loss: 1.411448359489441, acc: 0.5819672346115112)
[2025-02-05 14:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:46][root][INFO] - Training Epoch: 2/2, step 21959/23838 completed (loss: 1.2136348485946655, acc: 0.5909090638160706)
[2025-02-05 14:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:46][root][INFO] - Training Epoch: 2/2, step 21960/23838 completed (loss: 1.0606510639190674, acc: 0.6964285969734192)
[2025-02-05 14:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:46][root][INFO] - Training Epoch: 2/2, step 21961/23838 completed (loss: 0.8286867141723633, acc: 0.75)
[2025-02-05 14:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:47][root][INFO] - Training Epoch: 2/2, step 21962/23838 completed (loss: 0.8713709115982056, acc: 0.7361111044883728)
[2025-02-05 14:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:47][root][INFO] - Training Epoch: 2/2, step 21963/23838 completed (loss: 1.1103382110595703, acc: 0.6938775777816772)
[2025-02-05 14:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:48][root][INFO] - Training Epoch: 2/2, step 21964/23838 completed (loss: 1.3329302072525024, acc: 0.5895522236824036)
[2025-02-05 14:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:48][root][INFO] - Training Epoch: 2/2, step 21965/23838 completed (loss: 1.2829729318618774, acc: 0.6428571343421936)
[2025-02-05 14:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:48][root][INFO] - Training Epoch: 2/2, step 21966/23838 completed (loss: 1.205166220664978, acc: 0.5897436141967773)
[2025-02-05 14:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:49][root][INFO] - Training Epoch: 2/2, step 21967/23838 completed (loss: 1.29349946975708, acc: 0.6458333134651184)
[2025-02-05 14:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:49][root][INFO] - Training Epoch: 2/2, step 21968/23838 completed (loss: 1.430007815361023, acc: 0.65625)
[2025-02-05 14:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:50][root][INFO] - Training Epoch: 2/2, step 21969/23838 completed (loss: 1.5092639923095703, acc: 0.4833333194255829)
[2025-02-05 14:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:50][root][INFO] - Training Epoch: 2/2, step 21970/23838 completed (loss: 1.1783767938613892, acc: 0.5675675868988037)
[2025-02-05 14:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:50][root][INFO] - Training Epoch: 2/2, step 21971/23838 completed (loss: 1.4219077825546265, acc: 0.5982906222343445)
[2025-02-05 14:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:51][root][INFO] - Training Epoch: 2/2, step 21972/23838 completed (loss: 1.380782127380371, acc: 0.5909090638160706)
[2025-02-05 14:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:51][root][INFO] - Training Epoch: 2/2, step 21973/23838 completed (loss: 1.1135894060134888, acc: 0.6666666865348816)
[2025-02-05 14:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:52][root][INFO] - Training Epoch: 2/2, step 21974/23838 completed (loss: 1.1753331422805786, acc: 0.6206896305084229)
[2025-02-05 14:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:52][root][INFO] - Training Epoch: 2/2, step 21975/23838 completed (loss: 1.221042513847351, acc: 0.695652186870575)
[2025-02-05 14:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:52][root][INFO] - Training Epoch: 2/2, step 21976/23838 completed (loss: 1.181144118309021, acc: 0.6304348111152649)
[2025-02-05 14:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:53][root][INFO] - Training Epoch: 2/2, step 21977/23838 completed (loss: 1.8491559028625488, acc: 0.5362318754196167)
[2025-02-05 14:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:53][root][INFO] - Training Epoch: 2/2, step 21978/23838 completed (loss: 1.801005482673645, acc: 0.4819277226924896)
[2025-02-05 14:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:53][root][INFO] - Training Epoch: 2/2, step 21979/23838 completed (loss: 0.9014999866485596, acc: 0.7659574747085571)
[2025-02-05 14:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:54][root][INFO] - Training Epoch: 2/2, step 21980/23838 completed (loss: 0.9854423999786377, acc: 0.7397260069847107)
[2025-02-05 14:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:54][root][INFO] - Training Epoch: 2/2, step 21981/23838 completed (loss: 1.0421990156173706, acc: 0.7446808218955994)
[2025-02-05 14:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:55][root][INFO] - Training Epoch: 2/2, step 21982/23838 completed (loss: 0.885044276714325, acc: 0.75)
[2025-02-05 14:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:55][root][INFO] - Training Epoch: 2/2, step 21983/23838 completed (loss: 0.9675795435905457, acc: 0.7575757503509521)
[2025-02-05 14:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:55][root][INFO] - Training Epoch: 2/2, step 21984/23838 completed (loss: 0.8445523381233215, acc: 0.7450980544090271)
[2025-02-05 14:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:56][root][INFO] - Training Epoch: 2/2, step 21985/23838 completed (loss: 0.6619156002998352, acc: 0.800000011920929)
[2025-02-05 14:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:56][root][INFO] - Training Epoch: 2/2, step 21986/23838 completed (loss: 1.1375703811645508, acc: 0.7128713130950928)
[2025-02-05 14:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:56][root][INFO] - Training Epoch: 2/2, step 21987/23838 completed (loss: 1.1099495887756348, acc: 0.681034505367279)
[2025-02-05 14:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:57][root][INFO] - Training Epoch: 2/2, step 21988/23838 completed (loss: 1.1582057476043701, acc: 0.7032967209815979)
[2025-02-05 14:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:57][root][INFO] - Training Epoch: 2/2, step 21989/23838 completed (loss: 1.2717071771621704, acc: 0.5972222089767456)
[2025-02-05 14:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:58][root][INFO] - Training Epoch: 2/2, step 21990/23838 completed (loss: 1.1843135356903076, acc: 0.6530612111091614)
[2025-02-05 14:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:58][root][INFO] - Training Epoch: 2/2, step 21991/23838 completed (loss: 1.1070444583892822, acc: 0.6875)
[2025-02-05 14:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:59][root][INFO] - Training Epoch: 2/2, step 21992/23838 completed (loss: 1.1385482549667358, acc: 0.6849315166473389)
[2025-02-05 14:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:59][root][INFO] - Training Epoch: 2/2, step 21993/23838 completed (loss: 0.9739957451820374, acc: 0.7419354915618896)
[2025-02-05 14:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:06:59][root][INFO] - Training Epoch: 2/2, step 21994/23838 completed (loss: 0.9389572143554688, acc: 0.7457627058029175)
[2025-02-05 14:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:00][root][INFO] - Training Epoch: 2/2, step 21995/23838 completed (loss: 0.9105830192565918, acc: 0.7543859481811523)
[2025-02-05 14:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:00][root][INFO] - Training Epoch: 2/2, step 21996/23838 completed (loss: 1.181097149848938, acc: 0.6279069781303406)
[2025-02-05 14:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:00][root][INFO] - Training Epoch: 2/2, step 21997/23838 completed (loss: 1.2118635177612305, acc: 0.6504064798355103)
[2025-02-05 14:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:01][root][INFO] - Training Epoch: 2/2, step 21998/23838 completed (loss: 1.3868169784545898, acc: 0.6545454263687134)
[2025-02-05 14:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:01][root][INFO] - Training Epoch: 2/2, step 21999/23838 completed (loss: 1.0521401166915894, acc: 0.7234042286872864)
[2025-02-05 14:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:02][root][INFO] - Training Epoch: 2/2, step 22000/23838 completed (loss: 0.9220901131629944, acc: 0.7894737124443054)
[2025-02-05 14:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:02][root][INFO] - Training Epoch: 2/2, step 22001/23838 completed (loss: 1.0607047080993652, acc: 0.6326530575752258)
[2025-02-05 14:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:02][root][INFO] - Training Epoch: 2/2, step 22002/23838 completed (loss: 1.3039615154266357, acc: 0.6230769157409668)
[2025-02-05 14:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:03][root][INFO] - Training Epoch: 2/2, step 22003/23838 completed (loss: 1.0437771081924438, acc: 0.6842105388641357)
[2025-02-05 14:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:03][root][INFO] - Training Epoch: 2/2, step 22004/23838 completed (loss: 1.2772517204284668, acc: 0.609375)
[2025-02-05 14:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:04][root][INFO] - Training Epoch: 2/2, step 22005/23838 completed (loss: 0.5890448689460754, acc: 0.8035714030265808)
[2025-02-05 14:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:04][root][INFO] - Training Epoch: 2/2, step 22006/23838 completed (loss: 1.374232530593872, acc: 0.5873016119003296)
[2025-02-05 14:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:04][root][INFO] - Training Epoch: 2/2, step 22007/23838 completed (loss: 1.3395248651504517, acc: 0.5784313678741455)
[2025-02-05 14:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:05][root][INFO] - Training Epoch: 2/2, step 22008/23838 completed (loss: 1.401473045349121, acc: 0.6382978558540344)
[2025-02-05 14:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:05][root][INFO] - Training Epoch: 2/2, step 22009/23838 completed (loss: 0.9887661933898926, acc: 0.703125)
[2025-02-05 14:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:06][root][INFO] - Training Epoch: 2/2, step 22010/23838 completed (loss: 0.849107027053833, acc: 0.7432432174682617)
[2025-02-05 14:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:06][root][INFO] - Training Epoch: 2/2, step 22011/23838 completed (loss: 1.49546217918396, acc: 0.5444444417953491)
[2025-02-05 14:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:07][root][INFO] - Training Epoch: 2/2, step 22012/23838 completed (loss: 0.8971070051193237, acc: 0.7241379022598267)
[2025-02-05 14:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:07][root][INFO] - Training Epoch: 2/2, step 22013/23838 completed (loss: 1.086978554725647, acc: 0.7096773982048035)
[2025-02-05 14:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:07][root][INFO] - Training Epoch: 2/2, step 22014/23838 completed (loss: 0.5939069986343384, acc: 0.8478260636329651)
[2025-02-05 14:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:08][root][INFO] - Training Epoch: 2/2, step 22015/23838 completed (loss: 0.9127035737037659, acc: 0.7627118825912476)
[2025-02-05 14:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:08][root][INFO] - Training Epoch: 2/2, step 22016/23838 completed (loss: 1.406781792640686, acc: 0.5888888835906982)
[2025-02-05 14:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:09][root][INFO] - Training Epoch: 2/2, step 22017/23838 completed (loss: 1.2751919031143188, acc: 0.6463414430618286)
[2025-02-05 14:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:09][root][INFO] - Training Epoch: 2/2, step 22018/23838 completed (loss: 1.353981375694275, acc: 0.6190476417541504)
[2025-02-05 14:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:09][root][INFO] - Training Epoch: 2/2, step 22019/23838 completed (loss: 0.9802049398422241, acc: 0.7101449370384216)
[2025-02-05 14:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:10][root][INFO] - Training Epoch: 2/2, step 22020/23838 completed (loss: 0.8073617219924927, acc: 0.7560975551605225)
[2025-02-05 14:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:10][root][INFO] - Training Epoch: 2/2, step 22021/23838 completed (loss: 0.758770227432251, acc: 0.7560975551605225)
[2025-02-05 14:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:11][root][INFO] - Training Epoch: 2/2, step 22022/23838 completed (loss: 1.293369174003601, acc: 0.6333333253860474)
[2025-02-05 14:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:11][root][INFO] - Training Epoch: 2/2, step 22023/23838 completed (loss: 0.9508558511734009, acc: 0.6818181872367859)
[2025-02-05 14:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:11][root][INFO] - Training Epoch: 2/2, step 22024/23838 completed (loss: 1.429514765739441, acc: 0.6352941393852234)
[2025-02-05 14:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:12][root][INFO] - Training Epoch: 2/2, step 22025/23838 completed (loss: 1.13887619972229, acc: 0.6753246784210205)
[2025-02-05 14:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:12][root][INFO] - Training Epoch: 2/2, step 22026/23838 completed (loss: 1.007263422012329, acc: 0.7863247990608215)
[2025-02-05 14:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:13][root][INFO] - Training Epoch: 2/2, step 22027/23838 completed (loss: 1.1715617179870605, acc: 0.6875)
[2025-02-05 14:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:13][root][INFO] - Training Epoch: 2/2, step 22028/23838 completed (loss: 1.6478419303894043, acc: 0.5232558250427246)
[2025-02-05 14:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:13][root][INFO] - Training Epoch: 2/2, step 22029/23838 completed (loss: 1.5982543230056763, acc: 0.6219512224197388)
[2025-02-05 14:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:14][root][INFO] - Training Epoch: 2/2, step 22030/23838 completed (loss: 1.3323860168457031, acc: 0.6164383292198181)
[2025-02-05 14:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:14][root][INFO] - Training Epoch: 2/2, step 22031/23838 completed (loss: 1.2880016565322876, acc: 0.644859790802002)
[2025-02-05 14:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:14][root][INFO] - Training Epoch: 2/2, step 22032/23838 completed (loss: 1.1713383197784424, acc: 0.6851851940155029)
[2025-02-05 14:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:15][root][INFO] - Training Epoch: 2/2, step 22033/23838 completed (loss: 1.6169495582580566, acc: 0.5476190447807312)
[2025-02-05 14:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:15][root][INFO] - Training Epoch: 2/2, step 22034/23838 completed (loss: 1.563236117362976, acc: 0.529411792755127)
[2025-02-05 14:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:16][root][INFO] - Training Epoch: 2/2, step 22035/23838 completed (loss: 1.1660046577453613, acc: 0.6666666865348816)
[2025-02-05 14:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:16][root][INFO] - Training Epoch: 2/2, step 22036/23838 completed (loss: 1.4125465154647827, acc: 0.6097561120986938)
[2025-02-05 14:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:16][root][INFO] - Training Epoch: 2/2, step 22037/23838 completed (loss: 1.2717626094818115, acc: 0.6043956279754639)
[2025-02-05 14:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:17][root][INFO] - Training Epoch: 2/2, step 22038/23838 completed (loss: 1.4452934265136719, acc: 0.6266666650772095)
[2025-02-05 14:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:17][root][INFO] - Training Epoch: 2/2, step 22039/23838 completed (loss: 1.855932593345642, acc: 0.49152541160583496)
[2025-02-05 14:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:18][root][INFO] - Training Epoch: 2/2, step 22040/23838 completed (loss: 0.9426848292350769, acc: 0.7191011309623718)
[2025-02-05 14:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:18][root][INFO] - Training Epoch: 2/2, step 22041/23838 completed (loss: 1.5905330181121826, acc: 0.5)
[2025-02-05 14:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:19][root][INFO] - Training Epoch: 2/2, step 22042/23838 completed (loss: 1.0234147310256958, acc: 0.7234042286872864)
[2025-02-05 14:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:19][root][INFO] - Training Epoch: 2/2, step 22043/23838 completed (loss: 1.331009030342102, acc: 0.6511628031730652)
[2025-02-05 14:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:19][root][INFO] - Training Epoch: 2/2, step 22044/23838 completed (loss: 1.0905392169952393, acc: 0.7222222089767456)
[2025-02-05 14:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:20][root][INFO] - Training Epoch: 2/2, step 22045/23838 completed (loss: 1.4241573810577393, acc: 0.5844155550003052)
[2025-02-05 14:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:20][root][INFO] - Training Epoch: 2/2, step 22046/23838 completed (loss: 1.0953104496002197, acc: 0.6842105388641357)
[2025-02-05 14:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:21][root][INFO] - Training Epoch: 2/2, step 22047/23838 completed (loss: 0.9782880544662476, acc: 0.7283950448036194)
[2025-02-05 14:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:21][root][INFO] - Training Epoch: 2/2, step 22048/23838 completed (loss: 0.9531564116477966, acc: 0.7011494040489197)
[2025-02-05 14:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:21][root][INFO] - Training Epoch: 2/2, step 22049/23838 completed (loss: 1.3391624689102173, acc: 0.5882353186607361)
[2025-02-05 14:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:22][root][INFO] - Training Epoch: 2/2, step 22050/23838 completed (loss: 0.9768324494361877, acc: 0.7234042286872864)
[2025-02-05 14:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:22][root][INFO] - Training Epoch: 2/2, step 22051/23838 completed (loss: 1.1961981058120728, acc: 0.6296296119689941)
[2025-02-05 14:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:23][root][INFO] - Training Epoch: 2/2, step 22052/23838 completed (loss: 1.1118550300598145, acc: 0.698630154132843)
[2025-02-05 14:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:23][root][INFO] - Training Epoch: 2/2, step 22053/23838 completed (loss: 1.448327898979187, acc: 0.6136363744735718)
[2025-02-05 14:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:23][root][INFO] - Training Epoch: 2/2, step 22054/23838 completed (loss: 1.0846893787384033, acc: 0.6984127163887024)
[2025-02-05 14:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:24][root][INFO] - Training Epoch: 2/2, step 22055/23838 completed (loss: 1.2869064807891846, acc: 0.6391752362251282)
[2025-02-05 14:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:24][root][INFO] - Training Epoch: 2/2, step 22056/23838 completed (loss: 1.6886835098266602, acc: 0.5428571701049805)
[2025-02-05 14:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:24][root][INFO] - Training Epoch: 2/2, step 22057/23838 completed (loss: 1.2915074825286865, acc: 0.5249999761581421)
[2025-02-05 14:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:25][root][INFO] - Training Epoch: 2/2, step 22058/23838 completed (loss: 1.060537576675415, acc: 0.6883116960525513)
[2025-02-05 14:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:25][root][INFO] - Training Epoch: 2/2, step 22059/23838 completed (loss: 0.8125870823860168, acc: 0.7638888955116272)
[2025-02-05 14:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:25][root][INFO] - Training Epoch: 2/2, step 22060/23838 completed (loss: 1.2304707765579224, acc: 0.6428571343421936)
[2025-02-05 14:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:26][root][INFO] - Training Epoch: 2/2, step 22061/23838 completed (loss: 1.1501818895339966, acc: 0.7049180269241333)
[2025-02-05 14:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:26][root][INFO] - Training Epoch: 2/2, step 22062/23838 completed (loss: 1.2816165685653687, acc: 0.6470588445663452)
[2025-02-05 14:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:27][root][INFO] - Training Epoch: 2/2, step 22063/23838 completed (loss: 1.1069014072418213, acc: 0.6666666865348816)
[2025-02-05 14:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:27][root][INFO] - Training Epoch: 2/2, step 22064/23838 completed (loss: 1.0495221614837646, acc: 0.7160493731498718)
[2025-02-05 14:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:27][root][INFO] - Training Epoch: 2/2, step 22065/23838 completed (loss: 1.5502829551696777, acc: 0.5526315569877625)
[2025-02-05 14:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:28][root][INFO] - Training Epoch: 2/2, step 22066/23838 completed (loss: 1.0842205286026, acc: 0.7160493731498718)
[2025-02-05 14:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:28][root][INFO] - Training Epoch: 2/2, step 22067/23838 completed (loss: 1.1755173206329346, acc: 0.6730769276618958)
[2025-02-05 14:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:28][root][INFO] - Training Epoch: 2/2, step 22068/23838 completed (loss: 1.4658989906311035, acc: 0.607594907283783)
[2025-02-05 14:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:29][root][INFO] - Training Epoch: 2/2, step 22069/23838 completed (loss: 1.1057226657867432, acc: 0.7142857313156128)
[2025-02-05 14:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:29][root][INFO] - Training Epoch: 2/2, step 22070/23838 completed (loss: 1.3347102403640747, acc: 0.5526315569877625)
[2025-02-05 14:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:30][root][INFO] - Training Epoch: 2/2, step 22071/23838 completed (loss: 0.964657187461853, acc: 0.7215189933776855)
[2025-02-05 14:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:30][root][INFO] - Training Epoch: 2/2, step 22072/23838 completed (loss: 1.1943273544311523, acc: 0.6043956279754639)
[2025-02-05 14:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:31][root][INFO] - Training Epoch: 2/2, step 22073/23838 completed (loss: 1.1726199388504028, acc: 0.6478873491287231)
[2025-02-05 14:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:31][root][INFO] - Training Epoch: 2/2, step 22074/23838 completed (loss: 1.3635461330413818, acc: 0.6499999761581421)
[2025-02-05 14:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:31][root][INFO] - Training Epoch: 2/2, step 22075/23838 completed (loss: 1.1370784044265747, acc: 0.7142857313156128)
[2025-02-05 14:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:32][root][INFO] - Training Epoch: 2/2, step 22076/23838 completed (loss: 1.3742344379425049, acc: 0.6086956262588501)
[2025-02-05 14:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:32][root][INFO] - Training Epoch: 2/2, step 22077/23838 completed (loss: 1.2933443784713745, acc: 0.6526315808296204)
[2025-02-05 14:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:33][root][INFO] - Training Epoch: 2/2, step 22078/23838 completed (loss: 1.4875901937484741, acc: 0.5783132314682007)
[2025-02-05 14:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:33][root][INFO] - Training Epoch: 2/2, step 22079/23838 completed (loss: 1.642172932624817, acc: 0.4864864945411682)
[2025-02-05 14:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:33][root][INFO] - Training Epoch: 2/2, step 22080/23838 completed (loss: 1.2857245206832886, acc: 0.6000000238418579)
[2025-02-05 14:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:34][root][INFO] - Training Epoch: 2/2, step 22081/23838 completed (loss: 1.4025694131851196, acc: 0.5462962985038757)
[2025-02-05 14:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:34][root][INFO] - Training Epoch: 2/2, step 22082/23838 completed (loss: 1.7330009937286377, acc: 0.4862385392189026)
[2025-02-05 14:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:34][root][INFO] - Training Epoch: 2/2, step 22083/23838 completed (loss: 1.4844456911087036, acc: 0.5777778029441833)
[2025-02-05 14:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:35][root][INFO] - Training Epoch: 2/2, step 22084/23838 completed (loss: 1.1591006517410278, acc: 0.6714285612106323)
[2025-02-05 14:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:35][root][INFO] - Training Epoch: 2/2, step 22085/23838 completed (loss: 1.1082388162612915, acc: 0.699999988079071)
[2025-02-05 14:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:36][root][INFO] - Training Epoch: 2/2, step 22086/23838 completed (loss: 1.1029469966888428, acc: 0.7045454382896423)
[2025-02-05 14:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:36][root][INFO] - Training Epoch: 2/2, step 22087/23838 completed (loss: 1.1020548343658447, acc: 0.6966292262077332)
[2025-02-05 14:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:36][root][INFO] - Training Epoch: 2/2, step 22088/23838 completed (loss: 1.3021653890609741, acc: 0.6363636255264282)
[2025-02-05 14:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:37][root][INFO] - Training Epoch: 2/2, step 22089/23838 completed (loss: 1.5187889337539673, acc: 0.6578947305679321)
[2025-02-05 14:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:37][root][INFO] - Training Epoch: 2/2, step 22090/23838 completed (loss: 1.7011483907699585, acc: 0.46226415038108826)
[2025-02-05 14:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:37][root][INFO] - Training Epoch: 2/2, step 22091/23838 completed (loss: 1.144848108291626, acc: 0.6235294342041016)
[2025-02-05 14:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:38][root][INFO] - Training Epoch: 2/2, step 22092/23838 completed (loss: 1.2851911783218384, acc: 0.5892857313156128)
[2025-02-05 14:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:38][root][INFO] - Training Epoch: 2/2, step 22093/23838 completed (loss: 1.1379506587982178, acc: 0.6341463327407837)
[2025-02-05 14:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:39][root][INFO] - Training Epoch: 2/2, step 22094/23838 completed (loss: 1.202161192893982, acc: 0.6499999761581421)
[2025-02-05 14:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:39][root][INFO] - Training Epoch: 2/2, step 22095/23838 completed (loss: 0.9155676364898682, acc: 0.7195122241973877)
[2025-02-05 14:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:40][root][INFO] - Training Epoch: 2/2, step 22096/23838 completed (loss: 1.1595203876495361, acc: 0.656862735748291)
[2025-02-05 14:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:40][root][INFO] - Training Epoch: 2/2, step 22097/23838 completed (loss: 1.0801554918289185, acc: 0.6699029207229614)
[2025-02-05 14:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:40][root][INFO] - Training Epoch: 2/2, step 22098/23838 completed (loss: 1.5973665714263916, acc: 0.5185185074806213)
[2025-02-05 14:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:41][root][INFO] - Training Epoch: 2/2, step 22099/23838 completed (loss: 1.286149263381958, acc: 0.6067415475845337)
[2025-02-05 14:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:41][root][INFO] - Training Epoch: 2/2, step 22100/23838 completed (loss: 1.3116650581359863, acc: 0.5699999928474426)
[2025-02-05 14:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:41][root][INFO] - Training Epoch: 2/2, step 22101/23838 completed (loss: 0.7857741117477417, acc: 0.746666669845581)
[2025-02-05 14:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:42][root][INFO] - Training Epoch: 2/2, step 22102/23838 completed (loss: 1.1655914783477783, acc: 0.6769230961799622)
[2025-02-05 14:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:42][root][INFO] - Training Epoch: 2/2, step 22103/23838 completed (loss: 0.6984338164329529, acc: 0.7678571343421936)
[2025-02-05 14:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:43][root][INFO] - Training Epoch: 2/2, step 22104/23838 completed (loss: 0.9910331964492798, acc: 0.7117117047309875)
[2025-02-05 14:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:43][root][INFO] - Training Epoch: 2/2, step 22105/23838 completed (loss: 0.9867176413536072, acc: 0.7166666388511658)
[2025-02-05 14:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:43][root][INFO] - Training Epoch: 2/2, step 22106/23838 completed (loss: 1.1815166473388672, acc: 0.6589147448539734)
[2025-02-05 14:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:44][root][INFO] - Training Epoch: 2/2, step 22107/23838 completed (loss: 1.3898966312408447, acc: 0.5911949872970581)
[2025-02-05 14:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:44][root][INFO] - Training Epoch: 2/2, step 22108/23838 completed (loss: 0.9772400259971619, acc: 0.7160493731498718)
[2025-02-05 14:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:45][root][INFO] - Training Epoch: 2/2, step 22109/23838 completed (loss: 1.0444921255111694, acc: 0.6870748400688171)
[2025-02-05 14:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:45][root][INFO] - Training Epoch: 2/2, step 22110/23838 completed (loss: 1.0438519716262817, acc: 0.699999988079071)
[2025-02-05 14:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:45][root][INFO] - Training Epoch: 2/2, step 22111/23838 completed (loss: 1.0949689149856567, acc: 0.6805555820465088)
[2025-02-05 14:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:46][root][INFO] - Training Epoch: 2/2, step 22112/23838 completed (loss: 1.196118950843811, acc: 0.6976743936538696)
[2025-02-05 14:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:46][root][INFO] - Training Epoch: 2/2, step 22113/23838 completed (loss: 1.211944818496704, acc: 0.6142857074737549)
[2025-02-05 14:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:46][root][INFO] - Training Epoch: 2/2, step 22114/23838 completed (loss: 1.2577828168869019, acc: 0.6233766078948975)
[2025-02-05 14:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:47][root][INFO] - Training Epoch: 2/2, step 22115/23838 completed (loss: 1.0443315505981445, acc: 0.7196261882781982)
[2025-02-05 14:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:48][root][INFO] - Training Epoch: 2/2, step 22116/23838 completed (loss: 1.268402099609375, acc: 0.5978260636329651)
[2025-02-05 14:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:48][root][INFO] - Training Epoch: 2/2, step 22117/23838 completed (loss: 1.1447944641113281, acc: 0.6527777910232544)
[2025-02-05 14:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:49][root][INFO] - Training Epoch: 2/2, step 22118/23838 completed (loss: 1.2259441614151, acc: 0.6451612710952759)
[2025-02-05 14:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:49][root][INFO] - Training Epoch: 2/2, step 22119/23838 completed (loss: 1.3828966617584229, acc: 0.6065573692321777)
[2025-02-05 14:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:50][root][INFO] - Training Epoch: 2/2, step 22120/23838 completed (loss: 1.1505366563796997, acc: 0.644444465637207)
[2025-02-05 14:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:50][root][INFO] - Training Epoch: 2/2, step 22121/23838 completed (loss: 1.1551513671875, acc: 0.6391752362251282)
[2025-02-05 14:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:50][root][INFO] - Training Epoch: 2/2, step 22122/23838 completed (loss: 1.342179298400879, acc: 0.6138613820075989)
[2025-02-05 14:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:51][root][INFO] - Training Epoch: 2/2, step 22123/23838 completed (loss: 1.2041600942611694, acc: 0.6696428656578064)
[2025-02-05 14:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:51][root][INFO] - Training Epoch: 2/2, step 22124/23838 completed (loss: 1.2315231561660767, acc: 0.6666666865348816)
[2025-02-05 14:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:52][root][INFO] - Training Epoch: 2/2, step 22125/23838 completed (loss: 1.3498156070709229, acc: 0.5922330021858215)
[2025-02-05 14:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:52][root][INFO] - Training Epoch: 2/2, step 22126/23838 completed (loss: 1.4033782482147217, acc: 0.5656565427780151)
[2025-02-05 14:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:53][root][INFO] - Training Epoch: 2/2, step 22127/23838 completed (loss: 1.086998462677002, acc: 0.6760563254356384)
[2025-02-05 14:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:53][root][INFO] - Training Epoch: 2/2, step 22128/23838 completed (loss: 1.2391130924224854, acc: 0.6296296119689941)
[2025-02-05 14:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:53][root][INFO] - Training Epoch: 2/2, step 22129/23838 completed (loss: 1.262255311012268, acc: 0.5882353186607361)
[2025-02-05 14:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:54][root][INFO] - Training Epoch: 2/2, step 22130/23838 completed (loss: 1.1996722221374512, acc: 0.6190476417541504)
[2025-02-05 14:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:54][root][INFO] - Training Epoch: 2/2, step 22131/23838 completed (loss: 1.3860136270523071, acc: 0.5764706134796143)
[2025-02-05 14:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:55][root][INFO] - Training Epoch: 2/2, step 22132/23838 completed (loss: 1.3900052309036255, acc: 0.5822784900665283)
[2025-02-05 14:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:55][root][INFO] - Training Epoch: 2/2, step 22133/23838 completed (loss: 1.0244381427764893, acc: 0.6727272868156433)
[2025-02-05 14:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:55][root][INFO] - Training Epoch: 2/2, step 22134/23838 completed (loss: 1.017681360244751, acc: 0.7183098793029785)
[2025-02-05 14:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:56][root][INFO] - Training Epoch: 2/2, step 22135/23838 completed (loss: 1.17714524269104, acc: 0.625)
[2025-02-05 14:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:56][root][INFO] - Training Epoch: 2/2, step 22136/23838 completed (loss: 1.0724388360977173, acc: 0.6915887594223022)
[2025-02-05 14:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:57][root][INFO] - Training Epoch: 2/2, step 22137/23838 completed (loss: 1.0641599893569946, acc: 0.6792452931404114)
[2025-02-05 14:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:58][root][INFO] - Training Epoch: 2/2, step 22138/23838 completed (loss: 1.1262239217758179, acc: 0.6538461446762085)
[2025-02-05 14:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:58][root][INFO] - Training Epoch: 2/2, step 22139/23838 completed (loss: 0.684848427772522, acc: 0.8048780560493469)
[2025-02-05 14:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:58][root][INFO] - Training Epoch: 2/2, step 22140/23838 completed (loss: 1.0818945169448853, acc: 0.7476635575294495)
[2025-02-05 14:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:59][root][INFO] - Training Epoch: 2/2, step 22141/23838 completed (loss: 1.8339391946792603, acc: 0.4523809552192688)
[2025-02-05 14:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:07:59][root][INFO] - Training Epoch: 2/2, step 22142/23838 completed (loss: 1.2706921100616455, acc: 0.6633663177490234)
[2025-02-05 14:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:00][root][INFO] - Training Epoch: 2/2, step 22143/23838 completed (loss: 1.5162885189056396, acc: 0.6153846383094788)
[2025-02-05 14:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:00][root][INFO] - Training Epoch: 2/2, step 22144/23838 completed (loss: 1.0464705228805542, acc: 0.7317073345184326)
[2025-02-05 14:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:00][root][INFO] - Training Epoch: 2/2, step 22145/23838 completed (loss: 1.3516573905944824, acc: 0.6415094137191772)
[2025-02-05 14:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:01][root][INFO] - Training Epoch: 2/2, step 22146/23838 completed (loss: 1.6136754751205444, acc: 0.5111111402511597)
[2025-02-05 14:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:01][root][INFO] - Training Epoch: 2/2, step 22147/23838 completed (loss: 1.2235771417617798, acc: 0.6578947305679321)
[2025-02-05 14:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:02][root][INFO] - Training Epoch: 2/2, step 22148/23838 completed (loss: 1.1299742460250854, acc: 0.7317073345184326)
[2025-02-05 14:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:02][root][INFO] - Training Epoch: 2/2, step 22149/23838 completed (loss: 1.1849877834320068, acc: 0.6571428775787354)
[2025-02-05 14:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:02][root][INFO] - Training Epoch: 2/2, step 22150/23838 completed (loss: 1.048249363899231, acc: 0.6666666865348816)
[2025-02-05 14:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:03][root][INFO] - Training Epoch: 2/2, step 22151/23838 completed (loss: 0.8641035556793213, acc: 0.7708333134651184)
[2025-02-05 14:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:03][root][INFO] - Training Epoch: 2/2, step 22152/23838 completed (loss: 0.8539319634437561, acc: 0.7209302186965942)
[2025-02-05 14:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:03][root][INFO] - Training Epoch: 2/2, step 22153/23838 completed (loss: 1.0409560203552246, acc: 0.6428571343421936)
[2025-02-05 14:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:04][root][INFO] - Training Epoch: 2/2, step 22154/23838 completed (loss: 1.2596962451934814, acc: 0.5769230723381042)
[2025-02-05 14:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:04][root][INFO] - Training Epoch: 2/2, step 22155/23838 completed (loss: 0.8216780424118042, acc: 0.7169811129570007)
[2025-02-05 14:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:05][root][INFO] - Training Epoch: 2/2, step 22156/23838 completed (loss: 0.8148723840713501, acc: 0.7692307829856873)
[2025-02-05 14:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:05][root][INFO] - Training Epoch: 2/2, step 22157/23838 completed (loss: 1.2625213861465454, acc: 0.6744186282157898)
[2025-02-05 14:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:05][root][INFO] - Training Epoch: 2/2, step 22158/23838 completed (loss: 0.8846638202667236, acc: 0.7395833134651184)
[2025-02-05 14:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:06][root][INFO] - Training Epoch: 2/2, step 22159/23838 completed (loss: 1.3846702575683594, acc: 0.609375)
[2025-02-05 14:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:06][root][INFO] - Training Epoch: 2/2, step 22160/23838 completed (loss: 1.2065232992172241, acc: 0.699999988079071)
[2025-02-05 14:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:06][root][INFO] - Training Epoch: 2/2, step 22161/23838 completed (loss: 0.9461221098899841, acc: 0.7599999904632568)
[2025-02-05 14:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:07][root][INFO] - Training Epoch: 2/2, step 22162/23838 completed (loss: 1.4302390813827515, acc: 0.4901960790157318)
[2025-02-05 14:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:07][root][INFO] - Training Epoch: 2/2, step 22163/23838 completed (loss: 0.9791558980941772, acc: 0.6363636255264282)
[2025-02-05 14:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:08][root][INFO] - Training Epoch: 2/2, step 22164/23838 completed (loss: 1.3030812740325928, acc: 0.6785714030265808)
[2025-02-05 14:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:08][root][INFO] - Training Epoch: 2/2, step 22165/23838 completed (loss: 1.3241184949874878, acc: 0.6041666865348816)
[2025-02-05 14:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:08][root][INFO] - Training Epoch: 2/2, step 22166/23838 completed (loss: 0.879428505897522, acc: 0.6666666865348816)
[2025-02-05 14:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:09][root][INFO] - Training Epoch: 2/2, step 22167/23838 completed (loss: 0.8897940516471863, acc: 0.7307692170143127)
[2025-02-05 14:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:09][root][INFO] - Training Epoch: 2/2, step 22168/23838 completed (loss: 1.0601080656051636, acc: 0.774193525314331)
[2025-02-05 14:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:10][root][INFO] - Training Epoch: 2/2, step 22169/23838 completed (loss: 1.0910717248916626, acc: 0.761904776096344)
[2025-02-05 14:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:10][root][INFO] - Training Epoch: 2/2, step 22170/23838 completed (loss: 1.1681767702102661, acc: 0.6583333611488342)
[2025-02-05 14:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:10][root][INFO] - Training Epoch: 2/2, step 22171/23838 completed (loss: 1.0211642980575562, acc: 0.7692307829856873)
[2025-02-05 14:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:11][root][INFO] - Training Epoch: 2/2, step 22172/23838 completed (loss: 0.946097731590271, acc: 0.7333333492279053)
[2025-02-05 14:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:11][root][INFO] - Training Epoch: 2/2, step 22173/23838 completed (loss: 1.098296046257019, acc: 0.6666666865348816)
[2025-02-05 14:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:11][root][INFO] - Training Epoch: 2/2, step 22174/23838 completed (loss: 0.6727727055549622, acc: 0.84112149477005)
[2025-02-05 14:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:12][root][INFO] - Training Epoch: 2/2, step 22175/23838 completed (loss: 1.0742990970611572, acc: 0.650602400302887)
[2025-02-05 14:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:12][root][INFO] - Training Epoch: 2/2, step 22176/23838 completed (loss: 1.2285574674606323, acc: 0.5714285969734192)
[2025-02-05 14:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:13][root][INFO] - Training Epoch: 2/2, step 22177/23838 completed (loss: 0.9801680445671082, acc: 0.7339449524879456)
[2025-02-05 14:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:13][root][INFO] - Training Epoch: 2/2, step 22178/23838 completed (loss: 1.2530550956726074, acc: 0.6600000262260437)
[2025-02-05 14:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:13][root][INFO] - Training Epoch: 2/2, step 22179/23838 completed (loss: 1.1170369386672974, acc: 0.6499999761581421)
[2025-02-05 14:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:14][root][INFO] - Training Epoch: 2/2, step 22180/23838 completed (loss: 1.1237176656723022, acc: 0.7010309100151062)
[2025-02-05 14:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:14][root][INFO] - Training Epoch: 2/2, step 22181/23838 completed (loss: 1.9576103687286377, acc: 0.42222222685813904)
[2025-02-05 14:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:15][root][INFO] - Training Epoch: 2/2, step 22182/23838 completed (loss: 1.6822010278701782, acc: 0.5762711763381958)
[2025-02-05 14:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:15][root][INFO] - Training Epoch: 2/2, step 22183/23838 completed (loss: 1.012915849685669, acc: 0.7222222089767456)
[2025-02-05 14:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:15][root][INFO] - Training Epoch: 2/2, step 22184/23838 completed (loss: 1.6647295951843262, acc: 0.5806451439857483)
[2025-02-05 14:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:16][root][INFO] - Training Epoch: 2/2, step 22185/23838 completed (loss: 1.565948247909546, acc: 0.581818163394928)
[2025-02-05 14:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:16][root][INFO] - Training Epoch: 2/2, step 22186/23838 completed (loss: 1.2061859369277954, acc: 0.6744186282157898)
[2025-02-05 14:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:17][root][INFO] - Training Epoch: 2/2, step 22187/23838 completed (loss: 1.3007681369781494, acc: 0.5833333134651184)
[2025-02-05 14:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:17][root][INFO] - Training Epoch: 2/2, step 22188/23838 completed (loss: 1.2442810535430908, acc: 0.6764705777168274)
[2025-02-05 14:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:18][root][INFO] - Training Epoch: 2/2, step 22189/23838 completed (loss: 1.108029842376709, acc: 0.7164179086685181)
[2025-02-05 14:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:18][root][INFO] - Training Epoch: 2/2, step 22190/23838 completed (loss: 1.2743021249771118, acc: 0.5925925970077515)
[2025-02-05 14:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:18][root][INFO] - Training Epoch: 2/2, step 22191/23838 completed (loss: 0.7584725618362427, acc: 0.7358490824699402)
[2025-02-05 14:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:19][root][INFO] - Training Epoch: 2/2, step 22192/23838 completed (loss: 0.895323634147644, acc: 0.7358490824699402)
[2025-02-05 14:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:19][root][INFO] - Training Epoch: 2/2, step 22193/23838 completed (loss: 0.7194003462791443, acc: 0.7727272510528564)
[2025-02-05 14:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:20][root][INFO] - Training Epoch: 2/2, step 22194/23838 completed (loss: 0.6201352477073669, acc: 0.800000011920929)
[2025-02-05 14:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:20][root][INFO] - Training Epoch: 2/2, step 22195/23838 completed (loss: 1.1079072952270508, acc: 0.6904761791229248)
[2025-02-05 14:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:20][root][INFO] - Training Epoch: 2/2, step 22196/23838 completed (loss: 0.7566505074501038, acc: 0.8214285969734192)
[2025-02-05 14:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:21][root][INFO] - Training Epoch: 2/2, step 22197/23838 completed (loss: 0.9009347558021545, acc: 0.7333333492279053)
[2025-02-05 14:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:21][root][INFO] - Training Epoch: 2/2, step 22198/23838 completed (loss: 0.944415271282196, acc: 0.7058823704719543)
[2025-02-05 14:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:21][root][INFO] - Training Epoch: 2/2, step 22199/23838 completed (loss: 0.6329988241195679, acc: 0.8108108043670654)
[2025-02-05 14:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:22][root][INFO] - Training Epoch: 2/2, step 22200/23838 completed (loss: 1.7710901498794556, acc: 0.5373134613037109)
[2025-02-05 14:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:22][root][INFO] - Training Epoch: 2/2, step 22201/23838 completed (loss: 1.3662846088409424, acc: 0.609375)
[2025-02-05 14:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:23][root][INFO] - Training Epoch: 2/2, step 22202/23838 completed (loss: 1.2645119428634644, acc: 0.6666666865348816)
[2025-02-05 14:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:23][root][INFO] - Training Epoch: 2/2, step 22203/23838 completed (loss: 0.8626177310943604, acc: 0.7543859481811523)
[2025-02-05 14:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:24][root][INFO] - Training Epoch: 2/2, step 22204/23838 completed (loss: 0.960559070110321, acc: 0.7745097875595093)
[2025-02-05 14:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:24][root][INFO] - Training Epoch: 2/2, step 22205/23838 completed (loss: 1.3418635129928589, acc: 0.6041666865348816)
[2025-02-05 14:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:24][root][INFO] - Training Epoch: 2/2, step 22206/23838 completed (loss: 1.332977294921875, acc: 0.5652173757553101)
[2025-02-05 14:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:25][root][INFO] - Training Epoch: 2/2, step 22207/23838 completed (loss: 1.4087934494018555, acc: 0.5909090638160706)
[2025-02-05 14:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:25][root][INFO] - Training Epoch: 2/2, step 22208/23838 completed (loss: 1.2644374370574951, acc: 0.7241379022598267)
[2025-02-05 14:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:26][root][INFO] - Training Epoch: 2/2, step 22209/23838 completed (loss: 1.3966617584228516, acc: 0.6341463327407837)
[2025-02-05 14:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:26][root][INFO] - Training Epoch: 2/2, step 22210/23838 completed (loss: 1.2930735349655151, acc: 0.5652173757553101)
[2025-02-05 14:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:26][root][INFO] - Training Epoch: 2/2, step 22211/23838 completed (loss: 0.8320426940917969, acc: 0.7272727489471436)
[2025-02-05 14:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:27][root][INFO] - Training Epoch: 2/2, step 22212/23838 completed (loss: 0.825934886932373, acc: 0.7068965435028076)
[2025-02-05 14:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:27][root][INFO] - Training Epoch: 2/2, step 22213/23838 completed (loss: 0.5205538868904114, acc: 0.8199999928474426)
[2025-02-05 14:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:28][root][INFO] - Training Epoch: 2/2, step 22214/23838 completed (loss: 1.1122792959213257, acc: 0.6607142686843872)
[2025-02-05 14:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:28][root][INFO] - Training Epoch: 2/2, step 22215/23838 completed (loss: 0.9386600852012634, acc: 0.7702702879905701)
[2025-02-05 14:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:28][root][INFO] - Training Epoch: 2/2, step 22216/23838 completed (loss: 1.8599239587783813, acc: 0.54347825050354)
[2025-02-05 14:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:29][root][INFO] - Training Epoch: 2/2, step 22217/23838 completed (loss: 0.7351473569869995, acc: 0.75)
[2025-02-05 14:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:29][root][INFO] - Training Epoch: 2/2, step 22218/23838 completed (loss: 0.8951460719108582, acc: 0.6764705777168274)
[2025-02-05 14:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:30][root][INFO] - Training Epoch: 2/2, step 22219/23838 completed (loss: 0.9113957285881042, acc: 0.6666666865348816)
[2025-02-05 14:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:30][root][INFO] - Training Epoch: 2/2, step 22220/23838 completed (loss: 1.0184894800186157, acc: 0.6800000071525574)
[2025-02-05 14:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:30][root][INFO] - Training Epoch: 2/2, step 22221/23838 completed (loss: 1.472597599029541, acc: 0.6000000238418579)
[2025-02-05 14:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:31][root][INFO] - Training Epoch: 2/2, step 22222/23838 completed (loss: 1.276384949684143, acc: 0.6545454263687134)
[2025-02-05 14:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:31][root][INFO] - Training Epoch: 2/2, step 22223/23838 completed (loss: 0.8193777799606323, acc: 0.7384615540504456)
[2025-02-05 14:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:32][root][INFO] - Training Epoch: 2/2, step 22224/23838 completed (loss: 1.1420316696166992, acc: 0.6521739363670349)
[2025-02-05 14:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:32][root][INFO] - Training Epoch: 2/2, step 22225/23838 completed (loss: 0.8872710466384888, acc: 0.7307692170143127)
[2025-02-05 14:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:32][root][INFO] - Training Epoch: 2/2, step 22226/23838 completed (loss: 0.550708532333374, acc: 0.849056601524353)
[2025-02-05 14:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:33][root][INFO] - Training Epoch: 2/2, step 22227/23838 completed (loss: 1.066646933555603, acc: 0.671875)
[2025-02-05 14:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:33][root][INFO] - Training Epoch: 2/2, step 22228/23838 completed (loss: 1.2200559377670288, acc: 0.5833333134651184)
[2025-02-05 14:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:34][root][INFO] - Training Epoch: 2/2, step 22229/23838 completed (loss: 1.0703941583633423, acc: 0.7317073345184326)
[2025-02-05 14:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:34][root][INFO] - Training Epoch: 2/2, step 22230/23838 completed (loss: 1.2108136415481567, acc: 0.5454545617103577)
[2025-02-05 14:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:34][root][INFO] - Training Epoch: 2/2, step 22231/23838 completed (loss: 1.0605206489562988, acc: 0.6875)
[2025-02-05 14:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:35][root][INFO] - Training Epoch: 2/2, step 22232/23838 completed (loss: 1.5843392610549927, acc: 0.517241358757019)
[2025-02-05 14:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:35][root][INFO] - Training Epoch: 2/2, step 22233/23838 completed (loss: 0.7940212488174438, acc: 0.75)
[2025-02-05 14:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:35][root][INFO] - Training Epoch: 2/2, step 22234/23838 completed (loss: 0.9052762389183044, acc: 0.6800000071525574)
[2025-02-05 14:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:36][root][INFO] - Training Epoch: 2/2, step 22235/23838 completed (loss: 0.3250649571418762, acc: 0.95652174949646)
[2025-02-05 14:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:36][root][INFO] - Training Epoch: 2/2, step 22236/23838 completed (loss: 1.0348405838012695, acc: 0.7142857313156128)
[2025-02-05 14:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:37][root][INFO] - Training Epoch: 2/2, step 22237/23838 completed (loss: 0.5273661017417908, acc: 0.8235294222831726)
[2025-02-05 14:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:37][root][INFO] - Training Epoch: 2/2, step 22238/23838 completed (loss: 1.3139197826385498, acc: 0.7142857313156128)
[2025-02-05 14:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:37][root][INFO] - Training Epoch: 2/2, step 22239/23838 completed (loss: 1.0229097604751587, acc: 0.6153846383094788)
[2025-02-05 14:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:38][root][INFO] - Training Epoch: 2/2, step 22240/23838 completed (loss: 1.355795979499817, acc: 0.5517241358757019)
[2025-02-05 14:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:38][root][INFO] - Training Epoch: 2/2, step 22241/23838 completed (loss: 0.9320185780525208, acc: 0.7027027010917664)
[2025-02-05 14:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:38][root][INFO] - Training Epoch: 2/2, step 22242/23838 completed (loss: 1.4166202545166016, acc: 0.6071428656578064)
[2025-02-05 14:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:39][root][INFO] - Training Epoch: 2/2, step 22243/23838 completed (loss: 1.3589653968811035, acc: 0.5897436141967773)
[2025-02-05 14:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:39][root][INFO] - Training Epoch: 2/2, step 22244/23838 completed (loss: 0.7200881242752075, acc: 0.7222222089767456)
[2025-02-05 14:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:40][root][INFO] - Training Epoch: 2/2, step 22245/23838 completed (loss: 1.100378394126892, acc: 0.6315789222717285)
[2025-02-05 14:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:40][root][INFO] - Training Epoch: 2/2, step 22246/23838 completed (loss: 1.0550906658172607, acc: 0.625)
[2025-02-05 14:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:40][root][INFO] - Training Epoch: 2/2, step 22247/23838 completed (loss: 0.7096260786056519, acc: 0.7894737124443054)
[2025-02-05 14:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:41][root][INFO] - Training Epoch: 2/2, step 22248/23838 completed (loss: 1.088254690170288, acc: 0.5897436141967773)
[2025-02-05 14:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:41][root][INFO] - Training Epoch: 2/2, step 22249/23838 completed (loss: 1.0380167961120605, acc: 0.6875)
[2025-02-05 14:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:42][root][INFO] - Training Epoch: 2/2, step 22250/23838 completed (loss: 0.7352794408798218, acc: 0.7611940503120422)
[2025-02-05 14:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:42][root][INFO] - Training Epoch: 2/2, step 22251/23838 completed (loss: 0.8663480877876282, acc: 0.71875)
[2025-02-05 14:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:42][root][INFO] - Training Epoch: 2/2, step 22252/23838 completed (loss: 0.9510741829872131, acc: 0.6666666865348816)
[2025-02-05 14:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:43][root][INFO] - Training Epoch: 2/2, step 22253/23838 completed (loss: 1.2176538705825806, acc: 0.5925925970077515)
[2025-02-05 14:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:43][root][INFO] - Training Epoch: 2/2, step 22254/23838 completed (loss: 1.5242347717285156, acc: 0.5454545617103577)
[2025-02-05 14:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:44][root][INFO] - Training Epoch: 2/2, step 22255/23838 completed (loss: 1.696703314781189, acc: 0.4736842215061188)
[2025-02-05 14:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:44][root][INFO] - Training Epoch: 2/2, step 22256/23838 completed (loss: 1.621461033821106, acc: 0.5365853905677795)
[2025-02-05 14:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:44][root][INFO] - Training Epoch: 2/2, step 22257/23838 completed (loss: 1.509512186050415, acc: 0.5625)
[2025-02-05 14:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:45][root][INFO] - Training Epoch: 2/2, step 22258/23838 completed (loss: 1.4200173616409302, acc: 0.6170212626457214)
[2025-02-05 14:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:45][root][INFO] - Training Epoch: 2/2, step 22259/23838 completed (loss: 1.183953046798706, acc: 0.7058823704719543)
[2025-02-05 14:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:45][root][INFO] - Training Epoch: 2/2, step 22260/23838 completed (loss: 0.9564728736877441, acc: 0.699999988079071)
[2025-02-05 14:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:46][root][INFO] - Training Epoch: 2/2, step 22261/23838 completed (loss: 1.7611403465270996, acc: 0.5)
[2025-02-05 14:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:46][root][INFO] - Training Epoch: 2/2, step 22262/23838 completed (loss: 0.8318105936050415, acc: 0.7428571581840515)
[2025-02-05 14:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:47][root][INFO] - Training Epoch: 2/2, step 22263/23838 completed (loss: 0.8814132809638977, acc: 0.8148148059844971)
[2025-02-05 14:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:47][root][INFO] - Training Epoch: 2/2, step 22264/23838 completed (loss: 1.3551239967346191, acc: 0.6341463327407837)
[2025-02-05 14:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:47][root][INFO] - Training Epoch: 2/2, step 22265/23838 completed (loss: 1.171126127243042, acc: 0.6756756901741028)
[2025-02-05 14:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:48][root][INFO] - Training Epoch: 2/2, step 22266/23838 completed (loss: 0.9144793152809143, acc: 0.7555555701255798)
[2025-02-05 14:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:48][root][INFO] - Training Epoch: 2/2, step 22267/23838 completed (loss: 1.6927852630615234, acc: 0.5454545617103577)
[2025-02-05 14:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:49][root][INFO] - Training Epoch: 2/2, step 22268/23838 completed (loss: 0.6053423881530762, acc: 0.8297872543334961)
[2025-02-05 14:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:49][root][INFO] - Training Epoch: 2/2, step 22269/23838 completed (loss: 0.7979439496994019, acc: 0.699999988079071)
[2025-02-05 14:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:49][root][INFO] - Training Epoch: 2/2, step 22270/23838 completed (loss: 1.0400148630142212, acc: 0.7727272510528564)
[2025-02-05 14:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:50][root][INFO] - Training Epoch: 2/2, step 22271/23838 completed (loss: 0.9188482165336609, acc: 0.75)
[2025-02-05 14:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:50][root][INFO] - Training Epoch: 2/2, step 22272/23838 completed (loss: 1.0133557319641113, acc: 0.7222222089767456)
[2025-02-05 14:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:51][root][INFO] - Training Epoch: 2/2, step 22273/23838 completed (loss: 0.5101821422576904, acc: 0.8500000238418579)
[2025-02-05 14:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:51][root][INFO] - Training Epoch: 2/2, step 22274/23838 completed (loss: 1.0153347253799438, acc: 0.6944444179534912)
[2025-02-05 14:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:51][root][INFO] - Training Epoch: 2/2, step 22275/23838 completed (loss: 1.1472818851470947, acc: 0.6785714030265808)
[2025-02-05 14:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:52][root][INFO] - Training Epoch: 2/2, step 22276/23838 completed (loss: 1.2593700885772705, acc: 0.5957446694374084)
[2025-02-05 14:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:52][root][INFO] - Training Epoch: 2/2, step 22277/23838 completed (loss: 0.8079974055290222, acc: 0.692307710647583)
[2025-02-05 14:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:53][root][INFO] - Training Epoch: 2/2, step 22278/23838 completed (loss: 0.8235551118850708, acc: 0.6764705777168274)
[2025-02-05 14:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:53][root][INFO] - Training Epoch: 2/2, step 22279/23838 completed (loss: 1.0918750762939453, acc: 0.6857143044471741)
[2025-02-05 14:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:53][root][INFO] - Training Epoch: 2/2, step 22280/23838 completed (loss: 0.8538621068000793, acc: 0.7692307829856873)
[2025-02-05 14:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:54][root][INFO] - Training Epoch: 2/2, step 22281/23838 completed (loss: 1.351572036743164, acc: 0.6521739363670349)
[2025-02-05 14:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:54][root][INFO] - Training Epoch: 2/2, step 22282/23838 completed (loss: 1.0518440008163452, acc: 0.75)
[2025-02-05 14:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:55][root][INFO] - Training Epoch: 2/2, step 22283/23838 completed (loss: 0.5438930988311768, acc: 0.7272727489471436)
[2025-02-05 14:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:55][root][INFO] - Training Epoch: 2/2, step 22284/23838 completed (loss: 0.8483870625495911, acc: 0.7878788113594055)
[2025-02-05 14:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:55][root][INFO] - Training Epoch: 2/2, step 22285/23838 completed (loss: 0.5516030192375183, acc: 0.807692289352417)
[2025-02-05 14:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:56][root][INFO] - Training Epoch: 2/2, step 22286/23838 completed (loss: 0.8960447311401367, acc: 0.7755101919174194)
[2025-02-05 14:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:56][root][INFO] - Training Epoch: 2/2, step 22287/23838 completed (loss: 1.1796953678131104, acc: 0.675000011920929)
[2025-02-05 14:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:57][root][INFO] - Training Epoch: 2/2, step 22288/23838 completed (loss: 1.364158034324646, acc: 0.5882353186607361)
[2025-02-05 14:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:57][root][INFO] - Training Epoch: 2/2, step 22289/23838 completed (loss: 1.403792142868042, acc: 0.7352941036224365)
[2025-02-05 14:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:57][root][INFO] - Training Epoch: 2/2, step 22290/23838 completed (loss: 1.399108648300171, acc: 0.6216216087341309)
[2025-02-05 14:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:58][root][INFO] - Training Epoch: 2/2, step 22291/23838 completed (loss: 1.4593470096588135, acc: 0.6578947305679321)
[2025-02-05 14:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:58][root][INFO] - Training Epoch: 2/2, step 22292/23838 completed (loss: 1.3931268453598022, acc: 0.6808510422706604)
[2025-02-05 14:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:59][root][INFO] - Training Epoch: 2/2, step 22293/23838 completed (loss: 1.3851501941680908, acc: 0.6190476417541504)
[2025-02-05 14:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:59][root][INFO] - Training Epoch: 2/2, step 22294/23838 completed (loss: 1.001097321510315, acc: 0.6590909361839294)
[2025-02-05 14:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:08:59][root][INFO] - Training Epoch: 2/2, step 22295/23838 completed (loss: 1.8111358880996704, acc: 0.49180328845977783)
[2025-02-05 14:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:00][root][INFO] - Training Epoch: 2/2, step 22296/23838 completed (loss: 0.9897357821464539, acc: 0.6585366129875183)
[2025-02-05 14:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:00][root][INFO] - Training Epoch: 2/2, step 22297/23838 completed (loss: 1.0828880071640015, acc: 0.6875)
[2025-02-05 14:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:01][root][INFO] - Training Epoch: 2/2, step 22298/23838 completed (loss: 0.8283371925354004, acc: 0.7241379022598267)
[2025-02-05 14:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:01][root][INFO] - Training Epoch: 2/2, step 22299/23838 completed (loss: 1.1367253065109253, acc: 0.6315789222717285)
[2025-02-05 14:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:01][root][INFO] - Training Epoch: 2/2, step 22300/23838 completed (loss: 0.9751189351081848, acc: 0.6976743936538696)
[2025-02-05 14:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:02][root][INFO] - Training Epoch: 2/2, step 22301/23838 completed (loss: 1.1125463247299194, acc: 0.6399999856948853)
[2025-02-05 14:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:02][root][INFO] - Training Epoch: 2/2, step 22302/23838 completed (loss: 1.1683712005615234, acc: 0.6052631735801697)
[2025-02-05 14:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:03][root][INFO] - Training Epoch: 2/2, step 22303/23838 completed (loss: 1.1442049741744995, acc: 0.5925925970077515)
[2025-02-05 14:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:03][root][INFO] - Training Epoch: 2/2, step 22304/23838 completed (loss: 1.026792287826538, acc: 0.6333333253860474)
[2025-02-05 14:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:03][root][INFO] - Training Epoch: 2/2, step 22305/23838 completed (loss: 0.9055132269859314, acc: 0.7027027010917664)
[2025-02-05 14:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:04][root][INFO] - Training Epoch: 2/2, step 22306/23838 completed (loss: 0.9223642945289612, acc: 0.7924528121948242)
[2025-02-05 14:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:04][root][INFO] - Training Epoch: 2/2, step 22307/23838 completed (loss: 0.8638729453086853, acc: 0.7647058963775635)
[2025-02-05 14:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:04][root][INFO] - Training Epoch: 2/2, step 22308/23838 completed (loss: 1.3371310234069824, acc: 0.6111111044883728)
[2025-02-05 14:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:05][root][INFO] - Training Epoch: 2/2, step 22309/23838 completed (loss: 1.3504313230514526, acc: 0.6530612111091614)
[2025-02-05 14:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:05][root][INFO] - Training Epoch: 2/2, step 22310/23838 completed (loss: 1.935100793838501, acc: 0.4523809552192688)
[2025-02-05 14:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:05][root][INFO] - Training Epoch: 2/2, step 22311/23838 completed (loss: 1.4600363969802856, acc: 0.6015037298202515)
[2025-02-05 14:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:06][root][INFO] - Training Epoch: 2/2, step 22312/23838 completed (loss: 1.512112021446228, acc: 0.6056337952613831)
[2025-02-05 14:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:06][root][INFO] - Training Epoch: 2/2, step 22313/23838 completed (loss: 1.6090095043182373, acc: 0.5333333611488342)
[2025-02-05 14:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:07][root][INFO] - Training Epoch: 2/2, step 22314/23838 completed (loss: 1.3047927618026733, acc: 0.6428571343421936)
[2025-02-05 14:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:07][root][INFO] - Training Epoch: 2/2, step 22315/23838 completed (loss: 1.3151415586471558, acc: 0.6410256624221802)
[2025-02-05 14:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:08][root][INFO] - Training Epoch: 2/2, step 22316/23838 completed (loss: 1.1706998348236084, acc: 0.6458333134651184)
[2025-02-05 14:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:08][root][INFO] - Training Epoch: 2/2, step 22317/23838 completed (loss: 1.195987343788147, acc: 0.6516128778457642)
[2025-02-05 14:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:08][root][INFO] - Training Epoch: 2/2, step 22318/23838 completed (loss: 1.3283717632293701, acc: 0.6041666865348816)
[2025-02-05 14:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:09][root][INFO] - Training Epoch: 2/2, step 22319/23838 completed (loss: 1.211207389831543, acc: 0.6666666865348816)
[2025-02-05 14:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:09][root][INFO] - Training Epoch: 2/2, step 22320/23838 completed (loss: 1.215036153793335, acc: 0.671875)
[2025-02-05 14:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:10][root][INFO] - Training Epoch: 2/2, step 22321/23838 completed (loss: 1.2644996643066406, acc: 0.6266666650772095)
[2025-02-05 14:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:10][root][INFO] - Training Epoch: 2/2, step 22322/23838 completed (loss: 1.4401098489761353, acc: 0.5725806355476379)
[2025-02-05 14:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:10][root][INFO] - Training Epoch: 2/2, step 22323/23838 completed (loss: 1.449834942817688, acc: 0.592391312122345)
[2025-02-05 14:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:11][root][INFO] - Training Epoch: 2/2, step 22324/23838 completed (loss: 1.2464874982833862, acc: 0.6578947305679321)
[2025-02-05 14:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:11][root][INFO] - Training Epoch: 2/2, step 22325/23838 completed (loss: 0.9911341071128845, acc: 0.7692307829856873)
[2025-02-05 14:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:12][root][INFO] - Training Epoch: 2/2, step 22326/23838 completed (loss: 1.2430436611175537, acc: 0.6615384817123413)
[2025-02-05 14:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:12][root][INFO] - Training Epoch: 2/2, step 22327/23838 completed (loss: 1.3492830991744995, acc: 0.6341463327407837)
[2025-02-05 14:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:12][root][INFO] - Training Epoch: 2/2, step 22328/23838 completed (loss: 1.2392009496688843, acc: 0.6746987700462341)
[2025-02-05 14:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:13][root][INFO] - Training Epoch: 2/2, step 22329/23838 completed (loss: 1.2350184917449951, acc: 0.6527777910232544)
[2025-02-05 14:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:13][root][INFO] - Training Epoch: 2/2, step 22330/23838 completed (loss: 1.3781906366348267, acc: 0.6111111044883728)
[2025-02-05 14:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:14][root][INFO] - Training Epoch: 2/2, step 22331/23838 completed (loss: 1.267946481704712, acc: 0.6433120965957642)
[2025-02-05 14:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:14][root][INFO] - Training Epoch: 2/2, step 22332/23838 completed (loss: 1.1608895063400269, acc: 0.6527777910232544)
[2025-02-05 14:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:14][root][INFO] - Training Epoch: 2/2, step 22333/23838 completed (loss: 1.2400543689727783, acc: 0.6000000238418579)
[2025-02-05 14:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:15][root][INFO] - Training Epoch: 2/2, step 22334/23838 completed (loss: 1.6423920392990112, acc: 0.5504587292671204)
[2025-02-05 14:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:15][root][INFO] - Training Epoch: 2/2, step 22335/23838 completed (loss: 1.142524242401123, acc: 0.6796116232872009)
[2025-02-05 14:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:15][root][INFO] - Training Epoch: 2/2, step 22336/23838 completed (loss: 1.086676836013794, acc: 0.6521739363670349)
[2025-02-05 14:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:16][root][INFO] - Training Epoch: 2/2, step 22337/23838 completed (loss: 1.2816158533096313, acc: 0.6290322542190552)
[2025-02-05 14:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:16][root][INFO] - Training Epoch: 2/2, step 22338/23838 completed (loss: 1.3084427118301392, acc: 0.611940324306488)
[2025-02-05 14:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:17][root][INFO] - Training Epoch: 2/2, step 22339/23838 completed (loss: 0.9663348197937012, acc: 0.6666666865348816)
[2025-02-05 14:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:17][root][INFO] - Training Epoch: 2/2, step 22340/23838 completed (loss: 0.9348980784416199, acc: 0.7064220309257507)
[2025-02-05 14:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:17][root][INFO] - Training Epoch: 2/2, step 22341/23838 completed (loss: 0.9068191647529602, acc: 0.6984127163887024)
[2025-02-05 14:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:18][root][INFO] - Training Epoch: 2/2, step 22342/23838 completed (loss: 1.2428205013275146, acc: 0.6304348111152649)
[2025-02-05 14:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:18][root][INFO] - Training Epoch: 2/2, step 22343/23838 completed (loss: 1.0947906970977783, acc: 0.7042253613471985)
[2025-02-05 14:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:19][root][INFO] - Training Epoch: 2/2, step 22344/23838 completed (loss: 1.3995314836502075, acc: 0.6213017702102661)
[2025-02-05 14:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:19][root][INFO] - Training Epoch: 2/2, step 22345/23838 completed (loss: 1.2694681882858276, acc: 0.6558441519737244)
[2025-02-05 14:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:20][root][INFO] - Training Epoch: 2/2, step 22346/23838 completed (loss: 1.2100543975830078, acc: 0.6315789222717285)
[2025-02-05 14:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:20][root][INFO] - Training Epoch: 2/2, step 22347/23838 completed (loss: 1.1424223184585571, acc: 0.6407766938209534)
[2025-02-05 14:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:20][root][INFO] - Training Epoch: 2/2, step 22348/23838 completed (loss: 1.0068398714065552, acc: 0.6666666865348816)
[2025-02-05 14:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:21][root][INFO] - Training Epoch: 2/2, step 22349/23838 completed (loss: 1.3334031105041504, acc: 0.6363636255264282)
[2025-02-05 14:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:21][root][INFO] - Training Epoch: 2/2, step 22350/23838 completed (loss: 0.9972719550132751, acc: 0.7272727489471436)
[2025-02-05 14:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:21][root][INFO] - Training Epoch: 2/2, step 22351/23838 completed (loss: 1.3437319993972778, acc: 0.6884058117866516)
[2025-02-05 14:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:22][root][INFO] - Training Epoch: 2/2, step 22352/23838 completed (loss: 0.8073089122772217, acc: 0.8028169274330139)
[2025-02-05 14:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:22][root][INFO] - Training Epoch: 2/2, step 22353/23838 completed (loss: 0.9126938581466675, acc: 0.7536231875419617)
[2025-02-05 14:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:23][root][INFO] - Training Epoch: 2/2, step 22354/23838 completed (loss: 1.2619881629943848, acc: 0.6206896305084229)
[2025-02-05 14:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:23][root][INFO] - Training Epoch: 2/2, step 22355/23838 completed (loss: 1.140252709388733, acc: 0.6415094137191772)
[2025-02-05 14:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:23][root][INFO] - Training Epoch: 2/2, step 22356/23838 completed (loss: 1.216537594795227, acc: 0.6373626589775085)
[2025-02-05 14:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:24][root][INFO] - Training Epoch: 2/2, step 22357/23838 completed (loss: 1.410250186920166, acc: 0.6499999761581421)
[2025-02-05 14:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:24][root][INFO] - Training Epoch: 2/2, step 22358/23838 completed (loss: 1.2327481508255005, acc: 0.6101694703102112)
[2025-02-05 14:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:25][root][INFO] - Training Epoch: 2/2, step 22359/23838 completed (loss: 1.1014817953109741, acc: 0.7021276354789734)
[2025-02-05 14:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:25][root][INFO] - Training Epoch: 2/2, step 22360/23838 completed (loss: 1.408746361732483, acc: 0.6274510025978088)
[2025-02-05 14:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:25][root][INFO] - Training Epoch: 2/2, step 22361/23838 completed (loss: 1.3748937845230103, acc: 0.6481481194496155)
[2025-02-05 14:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:26][root][INFO] - Training Epoch: 2/2, step 22362/23838 completed (loss: 1.4072343111038208, acc: 0.5952380895614624)
[2025-02-05 14:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:26][root][INFO] - Training Epoch: 2/2, step 22363/23838 completed (loss: 1.0271204710006714, acc: 0.6666666865348816)
[2025-02-05 14:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:27][root][INFO] - Training Epoch: 2/2, step 22364/23838 completed (loss: 1.2254207134246826, acc: 0.604651153087616)
[2025-02-05 14:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:27][root][INFO] - Training Epoch: 2/2, step 22365/23838 completed (loss: 1.3146244287490845, acc: 0.6455696225166321)
[2025-02-05 14:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:27][root][INFO] - Training Epoch: 2/2, step 22366/23838 completed (loss: 1.1867163181304932, acc: 0.6379310488700867)
[2025-02-05 14:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:28][root][INFO] - Training Epoch: 2/2, step 22367/23838 completed (loss: 1.088467001914978, acc: 0.7288135886192322)
[2025-02-05 14:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:28][root][INFO] - Training Epoch: 2/2, step 22368/23838 completed (loss: 1.369702935218811, acc: 0.5441176295280457)
[2025-02-05 14:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:29][root][INFO] - Training Epoch: 2/2, step 22369/23838 completed (loss: 1.0136351585388184, acc: 0.7021276354789734)
[2025-02-05 14:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:29][root][INFO] - Training Epoch: 2/2, step 22370/23838 completed (loss: 0.8647897839546204, acc: 0.78125)
[2025-02-05 14:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:29][root][INFO] - Training Epoch: 2/2, step 22371/23838 completed (loss: 0.7697691917419434, acc: 0.7631579041481018)
[2025-02-05 14:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:30][root][INFO] - Training Epoch: 2/2, step 22372/23838 completed (loss: 1.2761167287826538, acc: 0.6037735939025879)
[2025-02-05 14:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:30][root][INFO] - Training Epoch: 2/2, step 22373/23838 completed (loss: 1.6131260395050049, acc: 0.5423728823661804)
[2025-02-05 14:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:30][root][INFO] - Training Epoch: 2/2, step 22374/23838 completed (loss: 1.2269994020462036, acc: 0.6896551847457886)
[2025-02-05 14:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:31][root][INFO] - Training Epoch: 2/2, step 22375/23838 completed (loss: 1.3046667575836182, acc: 0.6103895902633667)
[2025-02-05 14:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:31][root][INFO] - Training Epoch: 2/2, step 22376/23838 completed (loss: 0.8012863397598267, acc: 0.760869562625885)
[2025-02-05 14:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:31][root][INFO] - Training Epoch: 2/2, step 22377/23838 completed (loss: 1.0955013036727905, acc: 0.6382978558540344)
[2025-02-05 14:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:32][root][INFO] - Training Epoch: 2/2, step 22378/23838 completed (loss: 1.2171528339385986, acc: 0.607594907283783)
[2025-02-05 14:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:32][root][INFO] - Training Epoch: 2/2, step 22379/23838 completed (loss: 1.4054734706878662, acc: 0.5600000023841858)
[2025-02-05 14:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:33][root][INFO] - Training Epoch: 2/2, step 22380/23838 completed (loss: 1.0949558019638062, acc: 0.6666666865348816)
[2025-02-05 14:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:33][root][INFO] - Training Epoch: 2/2, step 22381/23838 completed (loss: 1.3242720365524292, acc: 0.6000000238418579)
[2025-02-05 14:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:33][root][INFO] - Training Epoch: 2/2, step 22382/23838 completed (loss: 1.4813413619995117, acc: 0.546875)
[2025-02-05 14:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:34][root][INFO] - Training Epoch: 2/2, step 22383/23838 completed (loss: 1.540204644203186, acc: 0.6000000238418579)
[2025-02-05 14:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:34][root][INFO] - Training Epoch: 2/2, step 22384/23838 completed (loss: 1.0885138511657715, acc: 0.7083333134651184)
[2025-02-05 14:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:34][root][INFO] - Training Epoch: 2/2, step 22385/23838 completed (loss: 1.1485484838485718, acc: 0.7014925479888916)
[2025-02-05 14:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:35][root][INFO] - Training Epoch: 2/2, step 22386/23838 completed (loss: 1.4694820642471313, acc: 0.5903614163398743)
[2025-02-05 14:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:35][root][INFO] - Training Epoch: 2/2, step 22387/23838 completed (loss: 1.177127718925476, acc: 0.6181818246841431)
[2025-02-05 14:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:36][root][INFO] - Training Epoch: 2/2, step 22388/23838 completed (loss: 1.1707204580307007, acc: 0.6904761791229248)
[2025-02-05 14:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:36][root][INFO] - Training Epoch: 2/2, step 22389/23838 completed (loss: 1.039636492729187, acc: 0.6756756901741028)
[2025-02-05 14:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:36][root][INFO] - Training Epoch: 2/2, step 22390/23838 completed (loss: 1.244616150856018, acc: 0.640625)
[2025-02-05 14:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:37][root][INFO] - Training Epoch: 2/2, step 22391/23838 completed (loss: 1.0284136533737183, acc: 0.6323529481887817)
[2025-02-05 14:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:37][root][INFO] - Training Epoch: 2/2, step 22392/23838 completed (loss: 1.078994631767273, acc: 0.6530612111091614)
[2025-02-05 14:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:38][root][INFO] - Training Epoch: 2/2, step 22393/23838 completed (loss: 0.9564626812934875, acc: 0.7142857313156128)
[2025-02-05 14:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:38][root][INFO] - Training Epoch: 2/2, step 22394/23838 completed (loss: 0.8680223822593689, acc: 0.7941176295280457)
[2025-02-05 14:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:38][root][INFO] - Training Epoch: 2/2, step 22395/23838 completed (loss: 1.2190150022506714, acc: 0.5694444179534912)
[2025-02-05 14:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:39][root][INFO] - Training Epoch: 2/2, step 22396/23838 completed (loss: 1.2673395872116089, acc: 0.6307692527770996)
[2025-02-05 14:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:39][root][INFO] - Training Epoch: 2/2, step 22397/23838 completed (loss: 1.3043370246887207, acc: 0.6470588445663452)
[2025-02-05 14:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:40][root][INFO] - Training Epoch: 2/2, step 22398/23838 completed (loss: 1.3556702136993408, acc: 0.5483871102333069)
[2025-02-05 14:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:40][root][INFO] - Training Epoch: 2/2, step 22399/23838 completed (loss: 1.215012550354004, acc: 0.5645161271095276)
[2025-02-05 14:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:40][root][INFO] - Training Epoch: 2/2, step 22400/23838 completed (loss: 1.2762386798858643, acc: 0.5967742204666138)
[2025-02-05 14:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:41][root][INFO] - Training Epoch: 2/2, step 22401/23838 completed (loss: 1.326884150505066, acc: 0.6470588445663452)
[2025-02-05 14:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:41][root][INFO] - Training Epoch: 2/2, step 22402/23838 completed (loss: 1.3410764932632446, acc: 0.5806451439857483)
[2025-02-05 14:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:42][root][INFO] - Training Epoch: 2/2, step 22403/23838 completed (loss: 0.9749528169631958, acc: 0.746835470199585)
[2025-02-05 14:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:42][root][INFO] - Training Epoch: 2/2, step 22404/23838 completed (loss: 1.3061845302581787, acc: 0.6632652878761292)
[2025-02-05 14:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:42][root][INFO] - Training Epoch: 2/2, step 22405/23838 completed (loss: 1.1959009170532227, acc: 0.6739130616188049)
[2025-02-05 14:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:43][root][INFO] - Training Epoch: 2/2, step 22406/23838 completed (loss: 1.505129098892212, acc: 0.5862069129943848)
[2025-02-05 14:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:43][root][INFO] - Training Epoch: 2/2, step 22407/23838 completed (loss: 1.1961174011230469, acc: 0.6470588445663452)
[2025-02-05 14:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:43][root][INFO] - Training Epoch: 2/2, step 22408/23838 completed (loss: 1.2087699174880981, acc: 0.6511628031730652)
[2025-02-05 14:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:44][root][INFO] - Training Epoch: 2/2, step 22409/23838 completed (loss: 1.1882624626159668, acc: 0.6666666865348816)
[2025-02-05 14:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:44][root][INFO] - Training Epoch: 2/2, step 22410/23838 completed (loss: 0.5062397718429565, acc: 0.8888888955116272)
[2025-02-05 14:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:44][root][INFO] - Training Epoch: 2/2, step 22411/23838 completed (loss: 1.5079768896102905, acc: 0.5384615659713745)
[2025-02-05 14:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:45][root][INFO] - Training Epoch: 2/2, step 22412/23838 completed (loss: 1.1813023090362549, acc: 0.6440678238868713)
[2025-02-05 14:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:45][root][INFO] - Training Epoch: 2/2, step 22413/23838 completed (loss: 1.449012041091919, acc: 0.5806451439857483)
[2025-02-05 14:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:46][root][INFO] - Training Epoch: 2/2, step 22414/23838 completed (loss: 1.2270041704177856, acc: 0.6470588445663452)
[2025-02-05 14:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:46][root][INFO] - Training Epoch: 2/2, step 22415/23838 completed (loss: 1.4075982570648193, acc: 0.523809552192688)
[2025-02-05 14:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:46][root][INFO] - Training Epoch: 2/2, step 22416/23838 completed (loss: 1.5163776874542236, acc: 0.6363636255264282)
[2025-02-05 14:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:47][root][INFO] - Training Epoch: 2/2, step 22417/23838 completed (loss: 1.812363862991333, acc: 0.489130437374115)
[2025-02-05 14:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:47][root][INFO] - Training Epoch: 2/2, step 22418/23838 completed (loss: 1.3186233043670654, acc: 0.5769230723381042)
[2025-02-05 14:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:48][root][INFO] - Training Epoch: 2/2, step 22419/23838 completed (loss: 1.14146888256073, acc: 0.692307710647583)
[2025-02-05 14:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:48][root][INFO] - Training Epoch: 2/2, step 22420/23838 completed (loss: 1.6397967338562012, acc: 0.5189873576164246)
[2025-02-05 14:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:48][root][INFO] - Training Epoch: 2/2, step 22421/23838 completed (loss: 1.4953669309616089, acc: 0.6000000238418579)
[2025-02-05 14:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:49][root][INFO] - Training Epoch: 2/2, step 22422/23838 completed (loss: 1.2901338338851929, acc: 0.6373626589775085)
[2025-02-05 14:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:49][root][INFO] - Training Epoch: 2/2, step 22423/23838 completed (loss: 1.3493669033050537, acc: 0.550000011920929)
[2025-02-05 14:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:50][root][INFO] - Training Epoch: 2/2, step 22424/23838 completed (loss: 1.456774353981018, acc: 0.5416666865348816)
[2025-02-05 14:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:50][root][INFO] - Training Epoch: 2/2, step 22425/23838 completed (loss: 1.5675673484802246, acc: 0.4864864945411682)
[2025-02-05 14:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:50][root][INFO] - Training Epoch: 2/2, step 22426/23838 completed (loss: 1.5188418626785278, acc: 0.5974025726318359)
[2025-02-05 14:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:51][root][INFO] - Training Epoch: 2/2, step 22427/23838 completed (loss: 1.4286372661590576, acc: 0.5970149040222168)
[2025-02-05 14:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:51][root][INFO] - Training Epoch: 2/2, step 22428/23838 completed (loss: 1.0684435367584229, acc: 0.692307710647583)
[2025-02-05 14:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:52][root][INFO] - Training Epoch: 2/2, step 22429/23838 completed (loss: 1.3446828126907349, acc: 0.6746031641960144)
[2025-02-05 14:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:52][root][INFO] - Training Epoch: 2/2, step 22430/23838 completed (loss: 1.6454349756240845, acc: 0.5591397881507874)
[2025-02-05 14:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:52][root][INFO] - Training Epoch: 2/2, step 22431/23838 completed (loss: 1.0617603063583374, acc: 0.7058823704719543)
[2025-02-05 14:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:53][root][INFO] - Training Epoch: 2/2, step 22432/23838 completed (loss: 1.4097874164581299, acc: 0.5555555820465088)
[2025-02-05 14:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:53][root][INFO] - Training Epoch: 2/2, step 22433/23838 completed (loss: 0.7230632305145264, acc: 0.7419354915618896)
[2025-02-05 14:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:54][root][INFO] - Training Epoch: 2/2, step 22434/23838 completed (loss: 0.9797369837760925, acc: 0.692307710647583)
[2025-02-05 14:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:54][root][INFO] - Training Epoch: 2/2, step 22435/23838 completed (loss: 1.0669115781784058, acc: 0.682539701461792)
[2025-02-05 14:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:54][root][INFO] - Training Epoch: 2/2, step 22436/23838 completed (loss: 1.0510082244873047, acc: 0.739130437374115)
[2025-02-05 14:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:55][root][INFO] - Training Epoch: 2/2, step 22437/23838 completed (loss: 1.0391021966934204, acc: 0.7126436829566956)
[2025-02-05 14:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:55][root][INFO] - Training Epoch: 2/2, step 22438/23838 completed (loss: 1.29251229763031, acc: 0.609375)
[2025-02-05 14:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:56][root][INFO] - Training Epoch: 2/2, step 22439/23838 completed (loss: 0.6884244680404663, acc: 0.7948718070983887)
[2025-02-05 14:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:56][root][INFO] - Training Epoch: 2/2, step 22440/23838 completed (loss: 1.3057286739349365, acc: 0.6282051205635071)
[2025-02-05 14:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:56][root][INFO] - Training Epoch: 2/2, step 22441/23838 completed (loss: 1.751258134841919, acc: 0.47727271914482117)
[2025-02-05 14:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:57][root][INFO] - Training Epoch: 2/2, step 22442/23838 completed (loss: 0.825602114200592, acc: 0.7931034564971924)
[2025-02-05 14:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:57][root][INFO] - Training Epoch: 2/2, step 22443/23838 completed (loss: 1.3479458093643188, acc: 0.5799999833106995)
[2025-02-05 14:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:58][root][INFO] - Training Epoch: 2/2, step 22444/23838 completed (loss: 1.1746370792388916, acc: 0.6296296119689941)
[2025-02-05 14:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:58][root][INFO] - Training Epoch: 2/2, step 22445/23838 completed (loss: 1.4957497119903564, acc: 0.5631067752838135)
[2025-02-05 14:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:58][root][INFO] - Training Epoch: 2/2, step 22446/23838 completed (loss: 1.1099778413772583, acc: 0.6630434989929199)
[2025-02-05 14:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:59][root][INFO] - Training Epoch: 2/2, step 22447/23838 completed (loss: 1.4946552515029907, acc: 0.5208333134651184)
[2025-02-05 14:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:09:59][root][INFO] - Training Epoch: 2/2, step 22448/23838 completed (loss: 1.1951428651809692, acc: 0.6610169410705566)
[2025-02-05 14:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:00][root][INFO] - Training Epoch: 2/2, step 22449/23838 completed (loss: 0.867970883846283, acc: 0.7428571581840515)
[2025-02-05 14:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:00][root][INFO] - Training Epoch: 2/2, step 22450/23838 completed (loss: 1.380892276763916, acc: 0.5277777910232544)
[2025-02-05 14:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:00][root][INFO] - Training Epoch: 2/2, step 22451/23838 completed (loss: 0.8636125922203064, acc: 0.7586206793785095)
[2025-02-05 14:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:01][root][INFO] - Training Epoch: 2/2, step 22452/23838 completed (loss: 0.9808258414268494, acc: 0.7966101765632629)
[2025-02-05 14:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:01][root][INFO] - Training Epoch: 2/2, step 22453/23838 completed (loss: 1.04771089553833, acc: 0.7093023061752319)
[2025-02-05 14:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:02][root][INFO] - Training Epoch: 2/2, step 22454/23838 completed (loss: 1.247694730758667, acc: 0.6309523582458496)
[2025-02-05 14:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:02][root][INFO] - Training Epoch: 2/2, step 22455/23838 completed (loss: 1.3476660251617432, acc: 0.7234042286872864)
[2025-02-05 14:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:02][root][INFO] - Training Epoch: 2/2, step 22456/23838 completed (loss: 1.214766263961792, acc: 0.6136363744735718)
[2025-02-05 14:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:03][root][INFO] - Training Epoch: 2/2, step 22457/23838 completed (loss: 1.4934616088867188, acc: 0.5161290168762207)
[2025-02-05 14:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:03][root][INFO] - Training Epoch: 2/2, step 22458/23838 completed (loss: 1.2423577308654785, acc: 0.6285714507102966)
[2025-02-05 14:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:03][root][INFO] - Training Epoch: 2/2, step 22459/23838 completed (loss: 1.2854856252670288, acc: 0.5961538553237915)
[2025-02-05 14:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:04][root][INFO] - Training Epoch: 2/2, step 22460/23838 completed (loss: 1.0118119716644287, acc: 0.7169811129570007)
[2025-02-05 14:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:04][root][INFO] - Training Epoch: 2/2, step 22461/23838 completed (loss: 1.0243556499481201, acc: 0.7164179086685181)
[2025-02-05 14:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:05][root][INFO] - Training Epoch: 2/2, step 22462/23838 completed (loss: 1.1734960079193115, acc: 0.6842105388641357)
[2025-02-05 14:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:05][root][INFO] - Training Epoch: 2/2, step 22463/23838 completed (loss: 1.1838933229446411, acc: 0.6629213690757751)
[2025-02-05 14:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:05][root][INFO] - Training Epoch: 2/2, step 22464/23838 completed (loss: 1.4590644836425781, acc: 0.6184210777282715)
[2025-02-05 14:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:06][root][INFO] - Training Epoch: 2/2, step 22465/23838 completed (loss: 1.6234129667282104, acc: 0.5394737124443054)
[2025-02-05 14:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:06][root][INFO] - Training Epoch: 2/2, step 22466/23838 completed (loss: 1.242809534072876, acc: 0.6022727489471436)
[2025-02-05 14:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:06][root][INFO] - Training Epoch: 2/2, step 22467/23838 completed (loss: 1.1291781663894653, acc: 0.6964285969734192)
[2025-02-05 14:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:07][root][INFO] - Training Epoch: 2/2, step 22468/23838 completed (loss: 1.648316740989685, acc: 0.529411792755127)
[2025-02-05 14:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:07][root][INFO] - Training Epoch: 2/2, step 22469/23838 completed (loss: 1.625307321548462, acc: 0.48235294222831726)
[2025-02-05 14:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:08][root][INFO] - Training Epoch: 2/2, step 22470/23838 completed (loss: 1.4460605382919312, acc: 0.6379310488700867)
[2025-02-05 14:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:08][root][INFO] - Training Epoch: 2/2, step 22471/23838 completed (loss: 1.4956423044204712, acc: 0.5714285969734192)
[2025-02-05 14:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:08][root][INFO] - Training Epoch: 2/2, step 22472/23838 completed (loss: 1.1729637384414673, acc: 0.6451612710952759)
[2025-02-05 14:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:09][root][INFO] - Training Epoch: 2/2, step 22473/23838 completed (loss: 1.0937297344207764, acc: 0.7297297120094299)
[2025-02-05 14:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:09][root][INFO] - Training Epoch: 2/2, step 22474/23838 completed (loss: 1.3930004835128784, acc: 0.6065573692321777)
[2025-02-05 14:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:10][root][INFO] - Training Epoch: 2/2, step 22475/23838 completed (loss: 0.6300620436668396, acc: 0.8600000143051147)
[2025-02-05 14:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:10][root][INFO] - Training Epoch: 2/2, step 22476/23838 completed (loss: 0.8405953049659729, acc: 0.78125)
[2025-02-05 14:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:10][root][INFO] - Training Epoch: 2/2, step 22477/23838 completed (loss: 1.398309588432312, acc: 0.5799999833106995)
[2025-02-05 14:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:11][root][INFO] - Training Epoch: 2/2, step 22478/23838 completed (loss: 1.2231810092926025, acc: 0.695652186870575)
[2025-02-05 14:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:11][root][INFO] - Training Epoch: 2/2, step 22479/23838 completed (loss: 1.4199620485305786, acc: 0.6756756901741028)
[2025-02-05 14:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:11][root][INFO] - Training Epoch: 2/2, step 22480/23838 completed (loss: 1.287194013595581, acc: 0.6200000047683716)
[2025-02-05 14:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:12][root][INFO] - Training Epoch: 2/2, step 22481/23838 completed (loss: 1.5796352624893188, acc: 0.4871794879436493)
[2025-02-05 14:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:12][root][INFO] - Training Epoch: 2/2, step 22482/23838 completed (loss: 1.4946593046188354, acc: 0.43589743971824646)
[2025-02-05 14:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:13][root][INFO] - Training Epoch: 2/2, step 22483/23838 completed (loss: 1.841353416442871, acc: 0.49180328845977783)
[2025-02-05 14:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:13][root][INFO] - Training Epoch: 2/2, step 22484/23838 completed (loss: 1.0698117017745972, acc: 0.6727272868156433)
[2025-02-05 14:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:13][root][INFO] - Training Epoch: 2/2, step 22485/23838 completed (loss: 1.3976075649261475, acc: 0.5918367505073547)
[2025-02-05 14:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:14][root][INFO] - Training Epoch: 2/2, step 22486/23838 completed (loss: 1.4037894010543823, acc: 0.6274510025978088)
[2025-02-05 14:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:14][root][INFO] - Training Epoch: 2/2, step 22487/23838 completed (loss: 1.615666151046753, acc: 0.40909090638160706)
[2025-02-05 14:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:15][root][INFO] - Training Epoch: 2/2, step 22488/23838 completed (loss: 1.3765925168991089, acc: 0.53125)
[2025-02-05 14:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:15][root][INFO] - Training Epoch: 2/2, step 22489/23838 completed (loss: 1.257557988166809, acc: 0.698113203048706)
[2025-02-05 14:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:15][root][INFO] - Training Epoch: 2/2, step 22490/23838 completed (loss: 1.4526410102844238, acc: 0.5283018946647644)
[2025-02-05 14:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:16][root][INFO] - Training Epoch: 2/2, step 22491/23838 completed (loss: 1.1675846576690674, acc: 0.6521739363670349)
[2025-02-05 14:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:16][root][INFO] - Training Epoch: 2/2, step 22492/23838 completed (loss: 1.4551119804382324, acc: 0.46666666865348816)
[2025-02-05 14:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:17][root][INFO] - Training Epoch: 2/2, step 22493/23838 completed (loss: 1.3599193096160889, acc: 0.6666666865348816)
[2025-02-05 14:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:17][root][INFO] - Training Epoch: 2/2, step 22494/23838 completed (loss: 1.3111953735351562, acc: 0.6129032373428345)
[2025-02-05 14:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:17][root][INFO] - Training Epoch: 2/2, step 22495/23838 completed (loss: 0.9140161275863647, acc: 0.7586206793785095)
[2025-02-05 14:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:18][root][INFO] - Training Epoch: 2/2, step 22496/23838 completed (loss: 1.3974671363830566, acc: 0.5)
[2025-02-05 14:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:18][root][INFO] - Training Epoch: 2/2, step 22497/23838 completed (loss: 1.3993841409683228, acc: 0.5797101259231567)
[2025-02-05 14:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:19][root][INFO] - Training Epoch: 2/2, step 22498/23838 completed (loss: 1.2648859024047852, acc: 0.6710526347160339)
[2025-02-05 14:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:19][root][INFO] - Training Epoch: 2/2, step 22499/23838 completed (loss: 1.065122365951538, acc: 0.6315789222717285)
[2025-02-05 14:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:19][root][INFO] - Training Epoch: 2/2, step 22500/23838 completed (loss: 1.2915687561035156, acc: 0.5517241358757019)
[2025-02-05 14:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:20][root][INFO] - Training Epoch: 2/2, step 22501/23838 completed (loss: 1.236811637878418, acc: 0.5517241358757019)
[2025-02-05 14:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:20][root][INFO] - Training Epoch: 2/2, step 22502/23838 completed (loss: 1.4898303747177124, acc: 0.5593220591545105)
[2025-02-05 14:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:20][root][INFO] - Training Epoch: 2/2, step 22503/23838 completed (loss: 1.3144354820251465, acc: 0.6133333444595337)
[2025-02-05 14:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:21][root][INFO] - Training Epoch: 2/2, step 22504/23838 completed (loss: 1.4437577724456787, acc: 0.6097561120986938)
[2025-02-05 14:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:21][root][INFO] - Training Epoch: 2/2, step 22505/23838 completed (loss: 1.3658791780471802, acc: 0.5901639461517334)
[2025-02-05 14:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:22][root][INFO] - Training Epoch: 2/2, step 22506/23838 completed (loss: 1.160535454750061, acc: 0.625)
[2025-02-05 14:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:22][root][INFO] - Training Epoch: 2/2, step 22507/23838 completed (loss: 1.1301672458648682, acc: 0.6323529481887817)
[2025-02-05 14:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:22][root][INFO] - Training Epoch: 2/2, step 22508/23838 completed (loss: 1.6376197338104248, acc: 0.5)
[2025-02-05 14:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:23][root][INFO] - Training Epoch: 2/2, step 22509/23838 completed (loss: 1.3598705530166626, acc: 0.5844155550003052)
[2025-02-05 14:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:23][root][INFO] - Training Epoch: 2/2, step 22510/23838 completed (loss: 1.363890528678894, acc: 0.6585366129875183)
[2025-02-05 14:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:23][root][INFO] - Training Epoch: 2/2, step 22511/23838 completed (loss: 1.8280274868011475, acc: 0.4590163826942444)
[2025-02-05 14:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:24][root][INFO] - Training Epoch: 2/2, step 22512/23838 completed (loss: 1.441352367401123, acc: 0.5789473652839661)
[2025-02-05 14:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:24][root][INFO] - Training Epoch: 2/2, step 22513/23838 completed (loss: 1.6883314847946167, acc: 0.5584415793418884)
[2025-02-05 14:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:25][root][INFO] - Training Epoch: 2/2, step 22514/23838 completed (loss: 1.6246675252914429, acc: 0.5194805264472961)
[2025-02-05 14:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:25][root][INFO] - Training Epoch: 2/2, step 22515/23838 completed (loss: 1.5796910524368286, acc: 0.5517241358757019)
[2025-02-05 14:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:25][root][INFO] - Training Epoch: 2/2, step 22516/23838 completed (loss: 1.4058451652526855, acc: 0.5882353186607361)
[2025-02-05 14:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:26][root][INFO] - Training Epoch: 2/2, step 22517/23838 completed (loss: 1.5436673164367676, acc: 0.5277777910232544)
[2025-02-05 14:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:26][root][INFO] - Training Epoch: 2/2, step 22518/23838 completed (loss: 2.244920492172241, acc: 0.3896103799343109)
[2025-02-05 14:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:27][root][INFO] - Training Epoch: 2/2, step 22519/23838 completed (loss: 2.408602714538574, acc: 0.28260868787765503)
[2025-02-05 14:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:27][root][INFO] - Training Epoch: 2/2, step 22520/23838 completed (loss: 1.6852495670318604, acc: 0.49425286054611206)
[2025-02-05 14:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:28][root][INFO] - Training Epoch: 2/2, step 22521/23838 completed (loss: 1.3294756412506104, acc: 0.604938268661499)
[2025-02-05 14:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:28][root][INFO] - Training Epoch: 2/2, step 22522/23838 completed (loss: 1.2719720602035522, acc: 0.6595744490623474)
[2025-02-05 14:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:28][root][INFO] - Training Epoch: 2/2, step 22523/23838 completed (loss: 1.7176178693771362, acc: 0.5)
[2025-02-05 14:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:29][root][INFO] - Training Epoch: 2/2, step 22524/23838 completed (loss: 1.2887558937072754, acc: 0.625)
[2025-02-05 14:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:29][root][INFO] - Training Epoch: 2/2, step 22525/23838 completed (loss: 1.183716058731079, acc: 0.6822429895401001)
[2025-02-05 14:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:30][root][INFO] - Training Epoch: 2/2, step 22526/23838 completed (loss: 0.9579778909683228, acc: 0.7301587462425232)
[2025-02-05 14:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:30][root][INFO] - Training Epoch: 2/2, step 22527/23838 completed (loss: 1.3651695251464844, acc: 0.6057692170143127)
[2025-02-05 14:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:30][root][INFO] - Training Epoch: 2/2, step 22528/23838 completed (loss: 0.9991154074668884, acc: 0.7297297120094299)
[2025-02-05 14:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:31][root][INFO] - Training Epoch: 2/2, step 22529/23838 completed (loss: 0.894882082939148, acc: 0.7962962985038757)
[2025-02-05 14:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:31][root][INFO] - Training Epoch: 2/2, step 22530/23838 completed (loss: 1.0793529748916626, acc: 0.675000011920929)
[2025-02-05 14:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:31][root][INFO] - Training Epoch: 2/2, step 22531/23838 completed (loss: 1.6172252893447876, acc: 0.5178571343421936)
[2025-02-05 14:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:32][root][INFO] - Training Epoch: 2/2, step 22532/23838 completed (loss: 1.5426502227783203, acc: 0.5686274766921997)
[2025-02-05 14:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:32][root][INFO] - Training Epoch: 2/2, step 22533/23838 completed (loss: 1.37325918674469, acc: 0.6304348111152649)
[2025-02-05 14:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:33][root][INFO] - Training Epoch: 2/2, step 22534/23838 completed (loss: 1.0151140689849854, acc: 0.6829268336296082)
[2025-02-05 14:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:33][root][INFO] - Training Epoch: 2/2, step 22535/23838 completed (loss: 1.4964016675949097, acc: 0.6428571343421936)
[2025-02-05 14:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:34][root][INFO] - Training Epoch: 2/2, step 22536/23838 completed (loss: 1.109614372253418, acc: 0.6666666865348816)
[2025-02-05 14:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:34][root][INFO] - Training Epoch: 2/2, step 22537/23838 completed (loss: 1.0559452772140503, acc: 0.6333333253860474)
[2025-02-05 14:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:34][root][INFO] - Training Epoch: 2/2, step 22538/23838 completed (loss: 0.8497681617736816, acc: 0.7647058963775635)
[2025-02-05 14:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:35][root][INFO] - Training Epoch: 2/2, step 22539/23838 completed (loss: 1.2270848751068115, acc: 0.662162184715271)
[2025-02-05 14:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:35][root][INFO] - Training Epoch: 2/2, step 22540/23838 completed (loss: 1.5118138790130615, acc: 0.5263158082962036)
[2025-02-05 14:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:36][root][INFO] - Training Epoch: 2/2, step 22541/23838 completed (loss: 1.457550287246704, acc: 0.6370967626571655)
[2025-02-05 14:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:36][root][INFO] - Training Epoch: 2/2, step 22542/23838 completed (loss: 1.5053409337997437, acc: 0.6043956279754639)
[2025-02-05 14:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:36][root][INFO] - Training Epoch: 2/2, step 22543/23838 completed (loss: 1.5579394102096558, acc: 0.625)
[2025-02-05 14:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:37][root][INFO] - Training Epoch: 2/2, step 22544/23838 completed (loss: 1.9173738956451416, acc: 0.4642857015132904)
[2025-02-05 14:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:37][root][INFO] - Training Epoch: 2/2, step 22545/23838 completed (loss: 1.9794820547103882, acc: 0.4793388545513153)
[2025-02-05 14:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:37][root][INFO] - Training Epoch: 2/2, step 22546/23838 completed (loss: 1.1815836429595947, acc: 0.6206896305084229)
[2025-02-05 14:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:38][root][INFO] - Training Epoch: 2/2, step 22547/23838 completed (loss: 0.9494034051895142, acc: 0.6803278923034668)
[2025-02-05 14:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:38][root][INFO] - Training Epoch: 2/2, step 22548/23838 completed (loss: 0.8813121318817139, acc: 0.7439024448394775)
[2025-02-05 14:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:39][root][INFO] - Training Epoch: 2/2, step 22549/23838 completed (loss: 1.2370307445526123, acc: 0.663551390171051)
[2025-02-05 14:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:39][root][INFO] - Training Epoch: 2/2, step 22550/23838 completed (loss: 1.0184998512268066, acc: 0.7113401889801025)
[2025-02-05 14:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:40][root][INFO] - Training Epoch: 2/2, step 22551/23838 completed (loss: 1.0702922344207764, acc: 0.7099999785423279)
[2025-02-05 14:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:40][root][INFO] - Training Epoch: 2/2, step 22552/23838 completed (loss: 1.1461554765701294, acc: 0.6944444179534912)
[2025-02-05 14:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:41][root][INFO] - Training Epoch: 2/2, step 22553/23838 completed (loss: 1.2852935791015625, acc: 0.6639344096183777)
[2025-02-05 14:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:41][root][INFO] - Training Epoch: 2/2, step 22554/23838 completed (loss: 0.958193302154541, acc: 0.71875)
[2025-02-05 14:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:41][root][INFO] - Training Epoch: 2/2, step 22555/23838 completed (loss: 0.5667780637741089, acc: 0.8292682766914368)
[2025-02-05 14:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:42][root][INFO] - Training Epoch: 2/2, step 22556/23838 completed (loss: 1.5132328271865845, acc: 0.5454545617103577)
[2025-02-05 14:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:42][root][INFO] - Training Epoch: 2/2, step 22557/23838 completed (loss: 1.9764230251312256, acc: 0.5128205418586731)
[2025-02-05 14:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:43][root][INFO] - Training Epoch: 2/2, step 22558/23838 completed (loss: 0.6008467078208923, acc: 0.800000011920929)
[2025-02-05 14:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:43][root][INFO] - Training Epoch: 2/2, step 22559/23838 completed (loss: 1.2393224239349365, acc: 0.6499999761581421)
[2025-02-05 14:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:43][root][INFO] - Training Epoch: 2/2, step 22560/23838 completed (loss: 0.9441841244697571, acc: 0.7368420958518982)
[2025-02-05 14:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:44][root][INFO] - Training Epoch: 2/2, step 22561/23838 completed (loss: 1.7975640296936035, acc: 0.6052631735801697)
[2025-02-05 14:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:44][root][INFO] - Training Epoch: 2/2, step 22562/23838 completed (loss: 0.40925610065460205, acc: 0.8695651888847351)
[2025-02-05 14:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:44][root][INFO] - Training Epoch: 2/2, step 22563/23838 completed (loss: 0.7854242920875549, acc: 0.6666666865348816)
[2025-02-05 14:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:45][root][INFO] - Training Epoch: 2/2, step 22564/23838 completed (loss: 1.3464921712875366, acc: 0.6153846383094788)
[2025-02-05 14:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:45][root][INFO] - Training Epoch: 2/2, step 22565/23838 completed (loss: 0.5510441064834595, acc: 0.7272727489471436)
[2025-02-05 14:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:46][root][INFO] - Training Epoch: 2/2, step 22566/23838 completed (loss: 0.8607641458511353, acc: 0.75)
[2025-02-05 14:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:46][root][INFO] - Training Epoch: 2/2, step 22567/23838 completed (loss: 1.1952766180038452, acc: 0.6752136945724487)
[2025-02-05 14:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:46][root][INFO] - Training Epoch: 2/2, step 22568/23838 completed (loss: 1.4288960695266724, acc: 0.5566037893295288)
[2025-02-05 14:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:47][root][INFO] - Training Epoch: 2/2, step 22569/23838 completed (loss: 1.1353931427001953, acc: 0.6603773832321167)
[2025-02-05 14:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:47][root][INFO] - Training Epoch: 2/2, step 22570/23838 completed (loss: 1.013654112815857, acc: 0.7111111283302307)
[2025-02-05 14:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:47][root][INFO] - Training Epoch: 2/2, step 22571/23838 completed (loss: 1.2338284254074097, acc: 0.6721311211585999)
[2025-02-05 14:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:48][root][INFO] - Training Epoch: 2/2, step 22572/23838 completed (loss: 1.0858508348464966, acc: 0.6857143044471741)
[2025-02-05 14:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:48][root][INFO] - Training Epoch: 2/2, step 22573/23838 completed (loss: 1.2361366748809814, acc: 0.6578947305679321)
[2025-02-05 14:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:49][root][INFO] - Training Epoch: 2/2, step 22574/23838 completed (loss: 1.3996466398239136, acc: 0.6083915829658508)
[2025-02-05 14:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:49][root][INFO] - Training Epoch: 2/2, step 22575/23838 completed (loss: 1.291810154914856, acc: 0.6342856884002686)
[2025-02-05 14:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:49][root][INFO] - Training Epoch: 2/2, step 22576/23838 completed (loss: 1.0648635625839233, acc: 0.6842105388641357)
[2025-02-05 14:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:50][root][INFO] - Training Epoch: 2/2, step 22577/23838 completed (loss: 1.3642866611480713, acc: 0.5476190447807312)
[2025-02-05 14:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:50][root][INFO] - Training Epoch: 2/2, step 22578/23838 completed (loss: 0.9160791635513306, acc: 0.7105262875556946)
[2025-02-05 14:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:50][root][INFO] - Training Epoch: 2/2, step 22579/23838 completed (loss: 1.0192339420318604, acc: 0.6969696879386902)
[2025-02-05 14:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:51][root][INFO] - Training Epoch: 2/2, step 22580/23838 completed (loss: 1.0726842880249023, acc: 0.686956524848938)
[2025-02-05 14:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:51][root][INFO] - Training Epoch: 2/2, step 22581/23838 completed (loss: 1.3261725902557373, acc: 0.5912408828735352)
[2025-02-05 14:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:52][root][INFO] - Training Epoch: 2/2, step 22582/23838 completed (loss: 1.1032968759536743, acc: 0.6969696879386902)
[2025-02-05 14:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:52][root][INFO] - Training Epoch: 2/2, step 22583/23838 completed (loss: 1.2943552732467651, acc: 0.5916666388511658)
[2025-02-05 14:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:52][root][INFO] - Training Epoch: 2/2, step 22584/23838 completed (loss: 1.0293998718261719, acc: 0.6785714030265808)
[2025-02-05 14:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:53][root][INFO] - Training Epoch: 2/2, step 22585/23838 completed (loss: 1.2698285579681396, acc: 0.6355140209197998)
[2025-02-05 14:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:53][root][INFO] - Training Epoch: 2/2, step 22586/23838 completed (loss: 1.0404212474822998, acc: 0.707317054271698)
[2025-02-05 14:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:54][root][INFO] - Training Epoch: 2/2, step 22587/23838 completed (loss: 0.8656423091888428, acc: 0.765625)
[2025-02-05 14:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:54][root][INFO] - Training Epoch: 2/2, step 22588/23838 completed (loss: 0.9109532237052917, acc: 0.7164179086685181)
[2025-02-05 14:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:54][root][INFO] - Training Epoch: 2/2, step 22589/23838 completed (loss: 0.7157936692237854, acc: 0.76106196641922)
[2025-02-05 14:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:55][root][INFO] - Training Epoch: 2/2, step 22590/23838 completed (loss: 0.9406143426895142, acc: 0.708737850189209)
[2025-02-05 14:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:55][root][INFO] - Training Epoch: 2/2, step 22591/23838 completed (loss: 1.1119383573532104, acc: 0.7079645991325378)
[2025-02-05 14:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:56][root][INFO] - Training Epoch: 2/2, step 22592/23838 completed (loss: 1.0875316858291626, acc: 0.6685393452644348)
[2025-02-05 14:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:56][root][INFO] - Training Epoch: 2/2, step 22593/23838 completed (loss: 1.312821388244629, acc: 0.616216242313385)
[2025-02-05 14:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:56][root][INFO] - Training Epoch: 2/2, step 22594/23838 completed (loss: 1.1794835329055786, acc: 0.6636363863945007)
[2025-02-05 14:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:57][root][INFO] - Training Epoch: 2/2, step 22595/23838 completed (loss: 1.0084102153778076, acc: 0.7222222089767456)
[2025-02-05 14:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:57][root][INFO] - Training Epoch: 2/2, step 22596/23838 completed (loss: 0.8064007759094238, acc: 0.7415730357170105)
[2025-02-05 14:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:57][root][INFO] - Training Epoch: 2/2, step 22597/23838 completed (loss: 1.0871433019638062, acc: 0.6811594367027283)
[2025-02-05 14:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:58][root][INFO] - Training Epoch: 2/2, step 22598/23838 completed (loss: 1.1343729496002197, acc: 0.6755555272102356)
[2025-02-05 14:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:58][root][INFO] - Training Epoch: 2/2, step 22599/23838 completed (loss: 0.9036264419555664, acc: 0.71074378490448)
[2025-02-05 14:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:59][root][INFO] - Training Epoch: 2/2, step 22600/23838 completed (loss: 0.8243797421455383, acc: 0.7323943376541138)
[2025-02-05 14:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:59][root][INFO] - Training Epoch: 2/2, step 22601/23838 completed (loss: 1.2390339374542236, acc: 0.6114285588264465)
[2025-02-05 14:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:10:59][root][INFO] - Training Epoch: 2/2, step 22602/23838 completed (loss: 0.868621289730072, acc: 0.7272727489471436)
[2025-02-05 14:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:00][root][INFO] - Training Epoch: 2/2, step 22603/23838 completed (loss: 1.0169364213943481, acc: 0.6857143044471741)
[2025-02-05 14:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:00][root][INFO] - Training Epoch: 2/2, step 22604/23838 completed (loss: 1.1155469417572021, acc: 0.6708860993385315)
[2025-02-05 14:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:01][root][INFO] - Training Epoch: 2/2, step 22605/23838 completed (loss: 0.5645630955696106, acc: 0.8117647171020508)
[2025-02-05 14:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:01][root][INFO] - Training Epoch: 2/2, step 22606/23838 completed (loss: 0.8256713151931763, acc: 0.7846153974533081)
[2025-02-05 14:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:01][root][INFO] - Training Epoch: 2/2, step 22607/23838 completed (loss: 1.00017249584198, acc: 0.6111111044883728)
[2025-02-05 14:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:02][root][INFO] - Training Epoch: 2/2, step 22608/23838 completed (loss: 1.0219770669937134, acc: 0.625)
[2025-02-05 14:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:02][root][INFO] - Training Epoch: 2/2, step 22609/23838 completed (loss: 1.1746275424957275, acc: 0.5789473652839661)
[2025-02-05 14:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:03][root][INFO] - Training Epoch: 2/2, step 22610/23838 completed (loss: 1.1680017709732056, acc: 0.7083333134651184)
[2025-02-05 14:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:03][root][INFO] - Training Epoch: 2/2, step 22611/23838 completed (loss: 1.1290203332901, acc: 0.75)
[2025-02-05 14:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:03][root][INFO] - Training Epoch: 2/2, step 22612/23838 completed (loss: 1.1649969816207886, acc: 0.675000011920929)
[2025-02-05 14:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:04][root][INFO] - Training Epoch: 2/2, step 22613/23838 completed (loss: 1.4729923009872437, acc: 0.6304348111152649)
[2025-02-05 14:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:04][root][INFO] - Training Epoch: 2/2, step 22614/23838 completed (loss: 1.4756799936294556, acc: 0.5333333611488342)
[2025-02-05 14:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:04][root][INFO] - Training Epoch: 2/2, step 22615/23838 completed (loss: 1.538424015045166, acc: 0.5106382966041565)
[2025-02-05 14:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:05][root][INFO] - Training Epoch: 2/2, step 22616/23838 completed (loss: 1.384034276008606, acc: 0.59375)
[2025-02-05 14:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:05][root][INFO] - Training Epoch: 2/2, step 22617/23838 completed (loss: 1.4972575902938843, acc: 0.6190476417541504)
[2025-02-05 14:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:06][root][INFO] - Training Epoch: 2/2, step 22618/23838 completed (loss: 1.3772317171096802, acc: 0.6060606241226196)
[2025-02-05 14:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:06][root][INFO] - Training Epoch: 2/2, step 22619/23838 completed (loss: 1.131787896156311, acc: 0.6739130616188049)
[2025-02-05 14:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:06][root][INFO] - Training Epoch: 2/2, step 22620/23838 completed (loss: 1.3468281030654907, acc: 0.6486486196517944)
[2025-02-05 14:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:07][root][INFO] - Training Epoch: 2/2, step 22621/23838 completed (loss: 1.2840827703475952, acc: 0.6363636255264282)
[2025-02-05 14:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:07][root][INFO] - Training Epoch: 2/2, step 22622/23838 completed (loss: 1.1788815259933472, acc: 0.75)
[2025-02-05 14:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:08][root][INFO] - Training Epoch: 2/2, step 22623/23838 completed (loss: 0.9313652515411377, acc: 0.761904776096344)
[2025-02-05 14:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:08][root][INFO] - Training Epoch: 2/2, step 22624/23838 completed (loss: 0.5789282321929932, acc: 0.8421052694320679)
[2025-02-05 14:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:09][root][INFO] - Training Epoch: 2/2, step 22625/23838 completed (loss: 0.8149290084838867, acc: 0.7288135886192322)
[2025-02-05 14:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:09][root][INFO] - Training Epoch: 2/2, step 22626/23838 completed (loss: 1.5386757850646973, acc: 0.6428571343421936)
[2025-02-05 14:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:09][root][INFO] - Training Epoch: 2/2, step 22627/23838 completed (loss: 1.1758110523223877, acc: 0.6480000019073486)
[2025-02-05 14:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:10][root][INFO] - Training Epoch: 2/2, step 22628/23838 completed (loss: 1.1624658107757568, acc: 0.6891891956329346)
[2025-02-05 14:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:10][root][INFO] - Training Epoch: 2/2, step 22629/23838 completed (loss: 0.9483975172042847, acc: 0.759036123752594)
[2025-02-05 14:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:10][root][INFO] - Training Epoch: 2/2, step 22630/23838 completed (loss: 1.1784415245056152, acc: 0.6666666865348816)
[2025-02-05 14:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:11][root][INFO] - Training Epoch: 2/2, step 22631/23838 completed (loss: 1.2575334310531616, acc: 0.6222222447395325)
[2025-02-05 14:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:11][root][INFO] - Training Epoch: 2/2, step 22632/23838 completed (loss: 1.1010541915893555, acc: 0.6875)
[2025-02-05 14:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:12][root][INFO] - Training Epoch: 2/2, step 22633/23838 completed (loss: 1.468627691268921, acc: 0.5680000185966492)
[2025-02-05 14:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:12][root][INFO] - Training Epoch: 2/2, step 22634/23838 completed (loss: 1.1306850910186768, acc: 0.6853147149085999)
[2025-02-05 14:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:13][root][INFO] - Training Epoch: 2/2, step 22635/23838 completed (loss: 1.1060807704925537, acc: 0.6606060862541199)
[2025-02-05 14:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:13][root][INFO] - Training Epoch: 2/2, step 22636/23838 completed (loss: 1.2245904207229614, acc: 0.6595744490623474)
[2025-02-05 14:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:13][root][INFO] - Training Epoch: 2/2, step 22637/23838 completed (loss: 1.5000606775283813, acc: 0.550000011920929)
[2025-02-05 14:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:14][root][INFO] - Training Epoch: 2/2, step 22638/23838 completed (loss: 1.145361304283142, acc: 0.6507936716079712)
[2025-02-05 14:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:14][root][INFO] - Training Epoch: 2/2, step 22639/23838 completed (loss: 1.39913809299469, acc: 0.5671641826629639)
[2025-02-05 14:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:15][root][INFO] - Training Epoch: 2/2, step 22640/23838 completed (loss: 1.3361698389053345, acc: 0.5777778029441833)
[2025-02-05 14:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:15][root][INFO] - Training Epoch: 2/2, step 22641/23838 completed (loss: 1.380866527557373, acc: 0.6276595592498779)
[2025-02-05 14:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:15][root][INFO] - Training Epoch: 2/2, step 22642/23838 completed (loss: 1.4368661642074585, acc: 0.6214285492897034)
[2025-02-05 14:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:16][root][INFO] - Training Epoch: 2/2, step 22643/23838 completed (loss: 1.1415959596633911, acc: 0.6027397513389587)
[2025-02-05 14:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:16][root][INFO] - Training Epoch: 2/2, step 22644/23838 completed (loss: 1.2052685022354126, acc: 0.5958904027938843)
[2025-02-05 14:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:17][root][INFO] - Training Epoch: 2/2, step 22645/23838 completed (loss: 1.228760838508606, acc: 0.6551724076271057)
[2025-02-05 14:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:17][root][INFO] - Training Epoch: 2/2, step 22646/23838 completed (loss: 1.4165302515029907, acc: 0.5555555820465088)
[2025-02-05 14:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:17][root][INFO] - Training Epoch: 2/2, step 22647/23838 completed (loss: 1.2603470087051392, acc: 0.5794392228126526)
[2025-02-05 14:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:18][root][INFO] - Training Epoch: 2/2, step 22648/23838 completed (loss: 1.1685727834701538, acc: 0.6564416885375977)
[2025-02-05 14:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:18][root][INFO] - Training Epoch: 2/2, step 22649/23838 completed (loss: 1.4437593221664429, acc: 0.5612903237342834)
[2025-02-05 14:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:19][root][INFO] - Training Epoch: 2/2, step 22650/23838 completed (loss: 1.2219748497009277, acc: 0.6355932354927063)
[2025-02-05 14:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:19][root][INFO] - Training Epoch: 2/2, step 22651/23838 completed (loss: 1.3101003170013428, acc: 0.529411792755127)
[2025-02-05 14:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:19][root][INFO] - Training Epoch: 2/2, step 22652/23838 completed (loss: 1.1474909782409668, acc: 0.6903225779533386)
[2025-02-05 14:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:20][root][INFO] - Training Epoch: 2/2, step 22653/23838 completed (loss: 1.207778811454773, acc: 0.6105263233184814)
[2025-02-05 14:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:20][root][INFO] - Training Epoch: 2/2, step 22654/23838 completed (loss: 1.386794090270996, acc: 0.6153846383094788)
[2025-02-05 14:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:21][root][INFO] - Training Epoch: 2/2, step 22655/23838 completed (loss: 1.2834415435791016, acc: 0.5714285969734192)
[2025-02-05 14:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:21][root][INFO] - Training Epoch: 2/2, step 22656/23838 completed (loss: 1.1809372901916504, acc: 0.7014925479888916)
[2025-02-05 14:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:21][root][INFO] - Training Epoch: 2/2, step 22657/23838 completed (loss: 1.2786126136779785, acc: 0.6494845151901245)
[2025-02-05 14:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:22][root][INFO] - Training Epoch: 2/2, step 22658/23838 completed (loss: 0.7451937198638916, acc: 0.75)
[2025-02-05 14:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:22][root][INFO] - Training Epoch: 2/2, step 22659/23838 completed (loss: 1.3185218572616577, acc: 0.6416666507720947)
[2025-02-05 14:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:22][root][INFO] - Training Epoch: 2/2, step 22660/23838 completed (loss: 1.3430688381195068, acc: 0.6481481194496155)
[2025-02-05 14:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:23][root][INFO] - Training Epoch: 2/2, step 22661/23838 completed (loss: 1.3942980766296387, acc: 0.5465116500854492)
[2025-02-05 14:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:23][root][INFO] - Training Epoch: 2/2, step 22662/23838 completed (loss: 1.1713593006134033, acc: 0.6774193644523621)
[2025-02-05 14:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:24][root][INFO] - Training Epoch: 2/2, step 22663/23838 completed (loss: 1.5326932668685913, acc: 0.5925925970077515)
[2025-02-05 14:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:24][root][INFO] - Training Epoch: 2/2, step 22664/23838 completed (loss: 1.221058964729309, acc: 0.6592592597007751)
[2025-02-05 14:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:24][root][INFO] - Training Epoch: 2/2, step 22665/23838 completed (loss: 1.1877381801605225, acc: 0.625)
[2025-02-05 14:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:25][root][INFO] - Training Epoch: 2/2, step 22666/23838 completed (loss: 1.1020219326019287, acc: 0.6603773832321167)
[2025-02-05 14:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:25][root][INFO] - Training Epoch: 2/2, step 22667/23838 completed (loss: 0.9674138426780701, acc: 0.7428571581840515)
[2025-02-05 14:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:25][root][INFO] - Training Epoch: 2/2, step 22668/23838 completed (loss: 0.7293896675109863, acc: 0.7977527976036072)
[2025-02-05 14:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:26][root][INFO] - Training Epoch: 2/2, step 22669/23838 completed (loss: 0.7992343902587891, acc: 0.7846153974533081)
[2025-02-05 14:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:26][root][INFO] - Training Epoch: 2/2, step 22670/23838 completed (loss: 1.2771352529525757, acc: 0.5968992114067078)
[2025-02-05 14:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:27][root][INFO] - Training Epoch: 2/2, step 22671/23838 completed (loss: 1.1730035543441772, acc: 0.6646706461906433)
[2025-02-05 14:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:27][root][INFO] - Training Epoch: 2/2, step 22672/23838 completed (loss: 1.4111417531967163, acc: 0.61654132604599)
[2025-02-05 14:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:27][root][INFO] - Training Epoch: 2/2, step 22673/23838 completed (loss: 1.1333091259002686, acc: 0.6629213690757751)
[2025-02-05 14:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:28][root][INFO] - Training Epoch: 2/2, step 22674/23838 completed (loss: 1.0359596014022827, acc: 0.7021276354789734)
[2025-02-05 14:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:28][root][INFO] - Training Epoch: 2/2, step 22675/23838 completed (loss: 1.0427699089050293, acc: 0.6857143044471741)
[2025-02-05 14:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:29][root][INFO] - Training Epoch: 2/2, step 22676/23838 completed (loss: 0.730859637260437, acc: 0.800000011920929)
[2025-02-05 14:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:29][root][INFO] - Training Epoch: 2/2, step 22677/23838 completed (loss: 1.2832173109054565, acc: 0.6399999856948853)
[2025-02-05 14:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:29][root][INFO] - Training Epoch: 2/2, step 22678/23838 completed (loss: 1.1004459857940674, acc: 0.6732673048973083)
[2025-02-05 14:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:30][root][INFO] - Training Epoch: 2/2, step 22679/23838 completed (loss: 1.1473689079284668, acc: 0.6485148668289185)
[2025-02-05 14:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:30][root][INFO] - Training Epoch: 2/2, step 22680/23838 completed (loss: 1.2286226749420166, acc: 0.6428571343421936)
[2025-02-05 14:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:31][root][INFO] - Training Epoch: 2/2, step 22681/23838 completed (loss: 1.4580333232879639, acc: 0.5714285969734192)
[2025-02-05 14:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:31][root][INFO] - Training Epoch: 2/2, step 22682/23838 completed (loss: 0.969950258731842, acc: 0.7234042286872864)
[2025-02-05 14:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:32][root][INFO] - Training Epoch: 2/2, step 22683/23838 completed (loss: 1.1740518808364868, acc: 0.6666666865348816)
[2025-02-05 14:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:32][root][INFO] - Training Epoch: 2/2, step 22684/23838 completed (loss: 1.256081461906433, acc: 0.6126760840415955)
[2025-02-05 14:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:32][root][INFO] - Training Epoch: 2/2, step 22685/23838 completed (loss: 1.2123059034347534, acc: 0.625)
[2025-02-05 14:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:33][root][INFO] - Training Epoch: 2/2, step 22686/23838 completed (loss: 1.3690249919891357, acc: 0.6224489808082581)
[2025-02-05 14:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:33][root][INFO] - Training Epoch: 2/2, step 22687/23838 completed (loss: 1.104012131690979, acc: 0.6583333611488342)
[2025-02-05 14:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:34][root][INFO] - Training Epoch: 2/2, step 22688/23838 completed (loss: 1.1596759557724, acc: 0.6495726704597473)
[2025-02-05 14:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:34][root][INFO] - Training Epoch: 2/2, step 22689/23838 completed (loss: 0.9481284022331238, acc: 0.7324841022491455)
[2025-02-05 14:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:35][root][INFO] - Training Epoch: 2/2, step 22690/23838 completed (loss: 1.3877395391464233, acc: 0.6197183132171631)
[2025-02-05 14:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:35][root][INFO] - Training Epoch: 2/2, step 22691/23838 completed (loss: 1.05116868019104, acc: 0.6969696879386902)
[2025-02-05 14:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:36][root][INFO] - Training Epoch: 2/2, step 22692/23838 completed (loss: 1.0440375804901123, acc: 0.6721311211585999)
[2025-02-05 14:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:36][root][INFO] - Training Epoch: 2/2, step 22693/23838 completed (loss: 1.25017249584198, acc: 0.6282051205635071)
[2025-02-05 14:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:36][root][INFO] - Training Epoch: 2/2, step 22694/23838 completed (loss: 1.0732169151306152, acc: 0.7244094610214233)
[2025-02-05 14:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:37][root][INFO] - Training Epoch: 2/2, step 22695/23838 completed (loss: 1.2260985374450684, acc: 0.6666666865348816)
[2025-02-05 14:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:37][root][INFO] - Training Epoch: 2/2, step 22696/23838 completed (loss: 1.1622228622436523, acc: 0.7042253613471985)
[2025-02-05 14:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:38][root][INFO] - Training Epoch: 2/2, step 22697/23838 completed (loss: 0.8178579211235046, acc: 0.7722772359848022)
[2025-02-05 14:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:38][root][INFO] - Training Epoch: 2/2, step 22698/23838 completed (loss: 0.8557048439979553, acc: 0.746666669845581)
[2025-02-05 14:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:38][root][INFO] - Training Epoch: 2/2, step 22699/23838 completed (loss: 1.0728851556777954, acc: 0.7052631378173828)
[2025-02-05 14:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:39][root][INFO] - Training Epoch: 2/2, step 22700/23838 completed (loss: 1.0920919179916382, acc: 0.6727272868156433)
[2025-02-05 14:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:39][root][INFO] - Training Epoch: 2/2, step 22701/23838 completed (loss: 0.9486597180366516, acc: 0.738095223903656)
[2025-02-05 14:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:40][root][INFO] - Training Epoch: 2/2, step 22702/23838 completed (loss: 0.9414694905281067, acc: 0.6829268336296082)
[2025-02-05 14:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:40][root][INFO] - Training Epoch: 2/2, step 22703/23838 completed (loss: 1.3509751558303833, acc: 0.6037735939025879)
[2025-02-05 14:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:40][root][INFO] - Training Epoch: 2/2, step 22704/23838 completed (loss: 1.0897191762924194, acc: 0.6818181872367859)
[2025-02-05 14:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:41][root][INFO] - Training Epoch: 2/2, step 22705/23838 completed (loss: 1.1315650939941406, acc: 0.7142857313156128)
[2025-02-05 14:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:41][root][INFO] - Training Epoch: 2/2, step 22706/23838 completed (loss: 1.4300786256790161, acc: 0.5)
[2025-02-05 14:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:41][root][INFO] - Training Epoch: 2/2, step 22707/23838 completed (loss: 1.047838807106018, acc: 0.6888889074325562)
[2025-02-05 14:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:42][root][INFO] - Training Epoch: 2/2, step 22708/23838 completed (loss: 1.0624027252197266, acc: 0.6571428775787354)
[2025-02-05 14:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:42][root][INFO] - Training Epoch: 2/2, step 22709/23838 completed (loss: 1.3459632396697998, acc: 0.6341463327407837)
[2025-02-05 14:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:43][root][INFO] - Training Epoch: 2/2, step 22710/23838 completed (loss: 1.1378531455993652, acc: 0.6301369667053223)
[2025-02-05 14:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:43][root][INFO] - Training Epoch: 2/2, step 22711/23838 completed (loss: 0.8708623647689819, acc: 0.75)
[2025-02-05 14:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:43][root][INFO] - Training Epoch: 2/2, step 22712/23838 completed (loss: 1.0217255353927612, acc: 0.6393442749977112)
[2025-02-05 14:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:44][root][INFO] - Training Epoch: 2/2, step 22713/23838 completed (loss: 1.2279938459396362, acc: 0.641791045665741)
[2025-02-05 14:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:44][root][INFO] - Training Epoch: 2/2, step 22714/23838 completed (loss: 1.3553975820541382, acc: 0.6000000238418579)
[2025-02-05 14:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:44][root][INFO] - Training Epoch: 2/2, step 22715/23838 completed (loss: 1.4846935272216797, acc: 0.603960394859314)
[2025-02-05 14:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:45][root][INFO] - Training Epoch: 2/2, step 22716/23838 completed (loss: 1.1926854848861694, acc: 0.6619718074798584)
[2025-02-05 14:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:45][root][INFO] - Training Epoch: 2/2, step 22717/23838 completed (loss: 1.0152254104614258, acc: 0.686274528503418)
[2025-02-05 14:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:46][root][INFO] - Training Epoch: 2/2, step 22718/23838 completed (loss: 0.8663668036460876, acc: 0.7777777910232544)
[2025-02-05 14:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:46][root][INFO] - Training Epoch: 2/2, step 22719/23838 completed (loss: 1.211878776550293, acc: 0.6458333134651184)
[2025-02-05 14:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:46][root][INFO] - Training Epoch: 2/2, step 22720/23838 completed (loss: 0.9932370185852051, acc: 0.6451612710952759)
[2025-02-05 14:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:47][root][INFO] - Training Epoch: 2/2, step 22721/23838 completed (loss: 0.6193315386772156, acc: 0.782608687877655)
[2025-02-05 14:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:47][root][INFO] - Training Epoch: 2/2, step 22722/23838 completed (loss: 1.395229458808899, acc: 0.6538461446762085)
[2025-02-05 14:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:47][root][INFO] - Training Epoch: 2/2, step 22723/23838 completed (loss: 1.1550042629241943, acc: 0.6808510422706604)
[2025-02-05 14:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:48][root][INFO] - Training Epoch: 2/2, step 22724/23838 completed (loss: 1.3532837629318237, acc: 0.6329113841056824)
[2025-02-05 14:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:48][root][INFO] - Training Epoch: 2/2, step 22725/23838 completed (loss: 0.8226001858711243, acc: 0.7540983557701111)
[2025-02-05 14:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:49][root][INFO] - Training Epoch: 2/2, step 22726/23838 completed (loss: 0.8275104761123657, acc: 0.7142857313156128)
[2025-02-05 14:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:49][root][INFO] - Training Epoch: 2/2, step 22727/23838 completed (loss: 1.1423395872116089, acc: 0.5757575631141663)
[2025-02-05 14:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:49][root][INFO] - Training Epoch: 2/2, step 22728/23838 completed (loss: 0.9588204622268677, acc: 0.7575757503509521)
[2025-02-05 14:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:50][root][INFO] - Training Epoch: 2/2, step 22729/23838 completed (loss: 0.9174902439117432, acc: 0.7647058963775635)
[2025-02-05 14:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:50][root][INFO] - Training Epoch: 2/2, step 22730/23838 completed (loss: 1.354737639427185, acc: 0.5882353186607361)
[2025-02-05 14:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:51][root][INFO] - Training Epoch: 2/2, step 22731/23838 completed (loss: 1.1852903366088867, acc: 0.6590909361839294)
[2025-02-05 14:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:51][root][INFO] - Training Epoch: 2/2, step 22732/23838 completed (loss: 0.8159605264663696, acc: 0.7428571581840515)
[2025-02-05 14:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:51][root][INFO] - Training Epoch: 2/2, step 22733/23838 completed (loss: 1.2757468223571777, acc: 0.5517241358757019)
[2025-02-05 14:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:52][root][INFO] - Training Epoch: 2/2, step 22734/23838 completed (loss: 1.0890992879867554, acc: 0.6769230961799622)
[2025-02-05 14:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:52][root][INFO] - Training Epoch: 2/2, step 22735/23838 completed (loss: 1.5160878896713257, acc: 0.568965494632721)
[2025-02-05 14:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:53][root][INFO] - Training Epoch: 2/2, step 22736/23838 completed (loss: 1.2420817613601685, acc: 0.6499999761581421)
[2025-02-05 14:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:53][root][INFO] - Training Epoch: 2/2, step 22737/23838 completed (loss: 1.267823338508606, acc: 0.5641025900840759)
[2025-02-05 14:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:53][root][INFO] - Training Epoch: 2/2, step 22738/23838 completed (loss: 0.6432905793190002, acc: 0.8333333134651184)
[2025-02-05 14:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:54][root][INFO] - Training Epoch: 2/2, step 22739/23838 completed (loss: 1.0344690084457397, acc: 0.6805555820465088)
[2025-02-05 14:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:54][root][INFO] - Training Epoch: 2/2, step 22740/23838 completed (loss: 1.3316258192062378, acc: 0.6065573692321777)
[2025-02-05 14:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:55][root][INFO] - Training Epoch: 2/2, step 22741/23838 completed (loss: 1.3235000371932983, acc: 0.5892857313156128)
[2025-02-05 14:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:55][root][INFO] - Training Epoch: 2/2, step 22742/23838 completed (loss: 1.1768083572387695, acc: 0.625)
[2025-02-05 14:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:55][root][INFO] - Training Epoch: 2/2, step 22743/23838 completed (loss: 1.338891625404358, acc: 0.6491228342056274)
[2025-02-05 14:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:56][root][INFO] - Training Epoch: 2/2, step 22744/23838 completed (loss: 1.4369196891784668, acc: 0.6296296119689941)
[2025-02-05 14:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:56][root][INFO] - Training Epoch: 2/2, step 22745/23838 completed (loss: 1.6051613092422485, acc: 0.5555555820465088)
[2025-02-05 14:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:57][root][INFO] - Training Epoch: 2/2, step 22746/23838 completed (loss: 0.9349159598350525, acc: 0.75)
[2025-02-05 14:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:57][root][INFO] - Training Epoch: 2/2, step 22747/23838 completed (loss: 0.9763138890266418, acc: 0.7586206793785095)
[2025-02-05 14:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:57][root][INFO] - Training Epoch: 2/2, step 22748/23838 completed (loss: 1.1283339262008667, acc: 0.6499999761581421)
[2025-02-05 14:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:58][root][INFO] - Training Epoch: 2/2, step 22749/23838 completed (loss: 1.0962129831314087, acc: 0.6774193644523621)
[2025-02-05 14:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:58][root][INFO] - Training Epoch: 2/2, step 22750/23838 completed (loss: 0.9140653610229492, acc: 0.7441860437393188)
[2025-02-05 14:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:59][root][INFO] - Training Epoch: 2/2, step 22751/23838 completed (loss: 1.7643704414367676, acc: 0.5492957830429077)
[2025-02-05 14:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:59][root][INFO] - Training Epoch: 2/2, step 22752/23838 completed (loss: 1.4603676795959473, acc: 0.625)
[2025-02-05 14:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:11:59][root][INFO] - Training Epoch: 2/2, step 22753/23838 completed (loss: 1.235235333442688, acc: 0.6666666865348816)
[2025-02-05 14:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:00][root][INFO] - Training Epoch: 2/2, step 22754/23838 completed (loss: 0.9537307620048523, acc: 0.7454545497894287)
[2025-02-05 14:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:00][root][INFO] - Training Epoch: 2/2, step 22755/23838 completed (loss: 1.1258162260055542, acc: 0.686274528503418)
[2025-02-05 14:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:01][root][INFO] - Training Epoch: 2/2, step 22756/23838 completed (loss: 1.2231827974319458, acc: 0.6507936716079712)
[2025-02-05 14:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:01][root][INFO] - Training Epoch: 2/2, step 22757/23838 completed (loss: 0.870171844959259, acc: 0.738095223903656)
[2025-02-05 14:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:01][root][INFO] - Training Epoch: 2/2, step 22758/23838 completed (loss: 1.0967397689819336, acc: 0.7234042286872864)
[2025-02-05 14:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:02][root][INFO] - Training Epoch: 2/2, step 22759/23838 completed (loss: 0.4646017253398895, acc: 0.8936170339584351)
[2025-02-05 14:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:02][root][INFO] - Training Epoch: 2/2, step 22760/23838 completed (loss: 0.9189574718475342, acc: 0.75)
[2025-02-05 14:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:03][root][INFO] - Training Epoch: 2/2, step 22761/23838 completed (loss: 1.3845586776733398, acc: 0.5957446694374084)
[2025-02-05 14:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:03][root][INFO] - Training Epoch: 2/2, step 22762/23838 completed (loss: 1.1028114557266235, acc: 0.7631579041481018)
[2025-02-05 14:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:03][root][INFO] - Training Epoch: 2/2, step 22763/23838 completed (loss: 1.2557127475738525, acc: 0.6666666865348816)
[2025-02-05 14:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:04][root][INFO] - Training Epoch: 2/2, step 22764/23838 completed (loss: 1.5720206499099731, acc: 0.5864197611808777)
[2025-02-05 14:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:04][root][INFO] - Training Epoch: 2/2, step 22765/23838 completed (loss: 1.2949579954147339, acc: 0.6159999966621399)
[2025-02-05 14:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:05][root][INFO] - Training Epoch: 2/2, step 22766/23838 completed (loss: 1.2767767906188965, acc: 0.6666666865348816)
[2025-02-05 14:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:05][root][INFO] - Training Epoch: 2/2, step 22767/23838 completed (loss: 1.8011778593063354, acc: 0.4651162922382355)
[2025-02-05 14:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:05][root][INFO] - Training Epoch: 2/2, step 22768/23838 completed (loss: 1.0475374460220337, acc: 0.692307710647583)
[2025-02-05 14:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:06][root][INFO] - Training Epoch: 2/2, step 22769/23838 completed (loss: 1.1883811950683594, acc: 0.6071428656578064)
[2025-02-05 14:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:06][root][INFO] - Training Epoch: 2/2, step 22770/23838 completed (loss: 1.437792181968689, acc: 0.5528455376625061)
[2025-02-05 14:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:06][root][INFO] - Training Epoch: 2/2, step 22771/23838 completed (loss: 1.1727187633514404, acc: 0.649350643157959)
[2025-02-05 14:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:07][root][INFO] - Training Epoch: 2/2, step 22772/23838 completed (loss: 1.442179560661316, acc: 0.5634920597076416)
[2025-02-05 14:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:07][root][INFO] - Training Epoch: 2/2, step 22773/23838 completed (loss: 1.3628848791122437, acc: 0.612500011920929)
[2025-02-05 14:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:08][root][INFO] - Training Epoch: 2/2, step 22774/23838 completed (loss: 1.3421411514282227, acc: 0.5630252361297607)
[2025-02-05 14:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:08][root][INFO] - Training Epoch: 2/2, step 22775/23838 completed (loss: 1.4212008714675903, acc: 0.5930232405662537)
[2025-02-05 14:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:08][root][INFO] - Training Epoch: 2/2, step 22776/23838 completed (loss: 1.0437968969345093, acc: 0.6973684430122375)
[2025-02-05 14:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:09][root][INFO] - Training Epoch: 2/2, step 22777/23838 completed (loss: 1.13556969165802, acc: 0.6631578803062439)
[2025-02-05 14:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:09][root][INFO] - Training Epoch: 2/2, step 22778/23838 completed (loss: 1.4205557107925415, acc: 0.5243902206420898)
[2025-02-05 14:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:09][root][INFO] - Training Epoch: 2/2, step 22779/23838 completed (loss: 1.1061832904815674, acc: 0.6229507923126221)
[2025-02-05 14:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:10][root][INFO] - Training Epoch: 2/2, step 22780/23838 completed (loss: 1.2160953283309937, acc: 0.6216216087341309)
[2025-02-05 14:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:10][root][INFO] - Training Epoch: 2/2, step 22781/23838 completed (loss: 1.0909901857376099, acc: 0.5862069129943848)
[2025-02-05 14:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:11][root][INFO] - Training Epoch: 2/2, step 22782/23838 completed (loss: 1.2973946332931519, acc: 0.6542056202888489)
[2025-02-05 14:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:11][root][INFO] - Training Epoch: 2/2, step 22783/23838 completed (loss: 1.136025309562683, acc: 0.6713286638259888)
[2025-02-05 14:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:11][root][INFO] - Training Epoch: 2/2, step 22784/23838 completed (loss: 1.2669302225112915, acc: 0.6010638475418091)
[2025-02-05 14:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:12][root][INFO] - Training Epoch: 2/2, step 22785/23838 completed (loss: 1.1901440620422363, acc: 0.5977011322975159)
[2025-02-05 14:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:12][root][INFO] - Training Epoch: 2/2, step 22786/23838 completed (loss: 1.0358954668045044, acc: 0.6893203854560852)
[2025-02-05 14:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:13][root][INFO] - Training Epoch: 2/2, step 22787/23838 completed (loss: 0.9319531321525574, acc: 0.7010309100151062)
[2025-02-05 14:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:13][root][INFO] - Training Epoch: 2/2, step 22788/23838 completed (loss: 1.4698671102523804, acc: 0.5346534848213196)
[2025-02-05 14:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:13][root][INFO] - Training Epoch: 2/2, step 22789/23838 completed (loss: 1.3355967998504639, acc: 0.658730149269104)
[2025-02-05 14:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:14][root][INFO] - Training Epoch: 2/2, step 22790/23838 completed (loss: 1.0663303136825562, acc: 0.7102803587913513)
[2025-02-05 14:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:14][root][INFO] - Training Epoch: 2/2, step 22791/23838 completed (loss: 1.257890224456787, acc: 0.6279069781303406)
[2025-02-05 14:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:14][root][INFO] - Training Epoch: 2/2, step 22792/23838 completed (loss: 0.9717684388160706, acc: 0.7676767706871033)
[2025-02-05 14:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:15][root][INFO] - Training Epoch: 2/2, step 22793/23838 completed (loss: 1.063352346420288, acc: 0.6823529601097107)
[2025-02-05 14:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:15][root][INFO] - Training Epoch: 2/2, step 22794/23838 completed (loss: 1.2449111938476562, acc: 0.6666666865348816)
[2025-02-05 14:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:16][root][INFO] - Training Epoch: 2/2, step 22795/23838 completed (loss: 1.2542800903320312, acc: 0.6091954112052917)
[2025-02-05 14:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:16][root][INFO] - Training Epoch: 2/2, step 22796/23838 completed (loss: 1.1400055885314941, acc: 0.6666666865348816)
[2025-02-05 14:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:16][root][INFO] - Training Epoch: 2/2, step 22797/23838 completed (loss: 1.3472093343734741, acc: 0.6186440587043762)
[2025-02-05 14:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:17][root][INFO] - Training Epoch: 2/2, step 22798/23838 completed (loss: 1.2390998601913452, acc: 0.6325300931930542)
[2025-02-05 14:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:17][root][INFO] - Training Epoch: 2/2, step 22799/23838 completed (loss: 1.1122045516967773, acc: 0.6632652878761292)
[2025-02-05 14:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:18][root][INFO] - Training Epoch: 2/2, step 22800/23838 completed (loss: 1.3153342008590698, acc: 0.557692289352417)
[2025-02-05 14:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:18][root][INFO] - Training Epoch: 2/2, step 22801/23838 completed (loss: 1.259178876876831, acc: 0.6293103694915771)
[2025-02-05 14:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:18][root][INFO] - Training Epoch: 2/2, step 22802/23838 completed (loss: 1.1630975008010864, acc: 0.585106372833252)
[2025-02-05 14:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:19][root][INFO] - Training Epoch: 2/2, step 22803/23838 completed (loss: 1.1187036037445068, acc: 0.654321014881134)
[2025-02-05 14:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:19][root][INFO] - Training Epoch: 2/2, step 22804/23838 completed (loss: 1.1243585348129272, acc: 0.668789803981781)
[2025-02-05 14:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:20][root][INFO] - Training Epoch: 2/2, step 22805/23838 completed (loss: 1.2231820821762085, acc: 0.6428571343421936)
[2025-02-05 14:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:20][root][INFO] - Training Epoch: 2/2, step 22806/23838 completed (loss: 1.1276894807815552, acc: 0.6936936974525452)
[2025-02-05 14:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:20][root][INFO] - Training Epoch: 2/2, step 22807/23838 completed (loss: 1.0665873289108276, acc: 0.6730769276618958)
[2025-02-05 14:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:21][root][INFO] - Training Epoch: 2/2, step 22808/23838 completed (loss: 1.3428268432617188, acc: 0.6022727489471436)
[2025-02-05 14:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:21][root][INFO] - Training Epoch: 2/2, step 22809/23838 completed (loss: 1.2881888151168823, acc: 0.6023392081260681)
[2025-02-05 14:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:22][root][INFO] - Training Epoch: 2/2, step 22810/23838 completed (loss: 1.0596336126327515, acc: 0.7191011309623718)
[2025-02-05 14:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:22][root][INFO] - Training Epoch: 2/2, step 22811/23838 completed (loss: 1.0065189599990845, acc: 0.6666666865348816)
[2025-02-05 14:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:22][root][INFO] - Training Epoch: 2/2, step 22812/23838 completed (loss: 1.3536274433135986, acc: 0.6190476417541504)
[2025-02-05 14:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:23][root][INFO] - Training Epoch: 2/2, step 22813/23838 completed (loss: 1.1131092309951782, acc: 0.6399999856948853)
[2025-02-05 14:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:23][root][INFO] - Training Epoch: 2/2, step 22814/23838 completed (loss: 1.2599422931671143, acc: 0.6372548937797546)
[2025-02-05 14:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:24][root][INFO] - Training Epoch: 2/2, step 22815/23838 completed (loss: 1.4385734796524048, acc: 0.5283018946647644)
[2025-02-05 14:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:24][root][INFO] - Training Epoch: 2/2, step 22816/23838 completed (loss: 1.2326021194458008, acc: 0.6268656849861145)
[2025-02-05 14:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:24][root][INFO] - Training Epoch: 2/2, step 22817/23838 completed (loss: 1.065379023551941, acc: 0.6323529481887817)
[2025-02-05 14:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:25][root][INFO] - Training Epoch: 2/2, step 22818/23838 completed (loss: 0.6847009062767029, acc: 0.8181818127632141)
[2025-02-05 14:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:25][root][INFO] - Training Epoch: 2/2, step 22819/23838 completed (loss: 0.9366132020950317, acc: 0.7238805890083313)
[2025-02-05 14:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:25][root][INFO] - Training Epoch: 2/2, step 22820/23838 completed (loss: 1.0221636295318604, acc: 0.7058823704719543)
[2025-02-05 14:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:26][root][INFO] - Training Epoch: 2/2, step 22821/23838 completed (loss: 1.4777604341506958, acc: 0.5733333230018616)
[2025-02-05 14:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:26][root][INFO] - Training Epoch: 2/2, step 22822/23838 completed (loss: 1.2943603992462158, acc: 0.6610169410705566)
[2025-02-05 14:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:27][root][INFO] - Training Epoch: 2/2, step 22823/23838 completed (loss: 0.9678810834884644, acc: 0.7439024448394775)
[2025-02-05 14:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:27][root][INFO] - Training Epoch: 2/2, step 22824/23838 completed (loss: 0.9020224213600159, acc: 0.7101449370384216)
[2025-02-05 14:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:27][root][INFO] - Training Epoch: 2/2, step 22825/23838 completed (loss: 0.6725379228591919, acc: 0.8727272748947144)
[2025-02-05 14:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:28][root][INFO] - Training Epoch: 2/2, step 22826/23838 completed (loss: 1.247942328453064, acc: 0.641791045665741)
[2025-02-05 14:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:28][root][INFO] - Training Epoch: 2/2, step 22827/23838 completed (loss: 1.4636543989181519, acc: 0.5732483863830566)
[2025-02-05 14:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:28][root][INFO] - Training Epoch: 2/2, step 22828/23838 completed (loss: 1.3408676385879517, acc: 0.5746268630027771)
[2025-02-05 14:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:29][root][INFO] - Training Epoch: 2/2, step 22829/23838 completed (loss: 1.4217993021011353, acc: 0.5670102834701538)
[2025-02-05 14:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:29][root][INFO] - Training Epoch: 2/2, step 22830/23838 completed (loss: 1.5418132543563843, acc: 0.5274725556373596)
[2025-02-05 14:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:30][root][INFO] - Training Epoch: 2/2, step 22831/23838 completed (loss: 1.421942114830017, acc: 0.6060606241226196)
[2025-02-05 14:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:30][root][INFO] - Training Epoch: 2/2, step 22832/23838 completed (loss: 1.1774801015853882, acc: 0.6415094137191772)
[2025-02-05 14:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:30][root][INFO] - Training Epoch: 2/2, step 22833/23838 completed (loss: 1.5085381269454956, acc: 0.604938268661499)
[2025-02-05 14:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:31][root][INFO] - Training Epoch: 2/2, step 22834/23838 completed (loss: 1.2181615829467773, acc: 0.604651153087616)
[2025-02-05 14:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:31][root][INFO] - Training Epoch: 2/2, step 22835/23838 completed (loss: 1.2063310146331787, acc: 0.6764705777168274)
[2025-02-05 14:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:32][root][INFO] - Training Epoch: 2/2, step 22836/23838 completed (loss: 1.6187719106674194, acc: 0.569343090057373)
[2025-02-05 14:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:32][root][INFO] - Training Epoch: 2/2, step 22837/23838 completed (loss: 1.3044143915176392, acc: 0.6299212574958801)
[2025-02-05 14:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:32][root][INFO] - Training Epoch: 2/2, step 22838/23838 completed (loss: 1.4695584774017334, acc: 0.582608699798584)
[2025-02-05 14:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:33][root][INFO] - Training Epoch: 2/2, step 22839/23838 completed (loss: 1.0936189889907837, acc: 0.6594203114509583)
[2025-02-05 14:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:33][root][INFO] - Training Epoch: 2/2, step 22840/23838 completed (loss: 1.618147373199463, acc: 0.477477490901947)
[2025-02-05 14:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:34][root][INFO] - Training Epoch: 2/2, step 22841/23838 completed (loss: 1.0888373851776123, acc: 0.6153846383094788)
[2025-02-05 14:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:34][root][INFO] - Training Epoch: 2/2, step 22842/23838 completed (loss: 1.172408103942871, acc: 0.6436781883239746)
[2025-02-05 14:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:34][root][INFO] - Training Epoch: 2/2, step 22843/23838 completed (loss: 1.2107237577438354, acc: 0.6781609058380127)
[2025-02-05 14:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:35][root][INFO] - Training Epoch: 2/2, step 22844/23838 completed (loss: 1.06309175491333, acc: 0.6779661178588867)
[2025-02-05 14:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:35][root][INFO] - Training Epoch: 2/2, step 22845/23838 completed (loss: 1.2390294075012207, acc: 0.6527777910232544)
[2025-02-05 14:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:36][root][INFO] - Training Epoch: 2/2, step 22846/23838 completed (loss: 1.1325535774230957, acc: 0.6564416885375977)
[2025-02-05 14:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:36][root][INFO] - Training Epoch: 2/2, step 22847/23838 completed (loss: 0.8463166356086731, acc: 0.7457627058029175)
[2025-02-05 14:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:36][root][INFO] - Training Epoch: 2/2, step 22848/23838 completed (loss: 1.066697120666504, acc: 0.7076923251152039)
[2025-02-05 14:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:37][root][INFO] - Training Epoch: 2/2, step 22849/23838 completed (loss: 1.0871593952178955, acc: 0.6847826242446899)
[2025-02-05 14:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:37][root][INFO] - Training Epoch: 2/2, step 22850/23838 completed (loss: 0.9998780488967896, acc: 0.6969696879386902)
[2025-02-05 14:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:37][root][INFO] - Training Epoch: 2/2, step 22851/23838 completed (loss: 1.326194405555725, acc: 0.5982906222343445)
[2025-02-05 14:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:38][root][INFO] - Training Epoch: 2/2, step 22852/23838 completed (loss: 1.246085524559021, acc: 0.6528925895690918)
[2025-02-05 14:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:38][root][INFO] - Training Epoch: 2/2, step 22853/23838 completed (loss: 1.1529613733291626, acc: 0.6486486196517944)
[2025-02-05 14:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:39][root][INFO] - Training Epoch: 2/2, step 22854/23838 completed (loss: 1.234655737876892, acc: 0.6639344096183777)
[2025-02-05 14:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:39][root][INFO] - Training Epoch: 2/2, step 22855/23838 completed (loss: 1.2560396194458008, acc: 0.6326530575752258)
[2025-02-05 14:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:39][root][INFO] - Training Epoch: 2/2, step 22856/23838 completed (loss: 0.9908403158187866, acc: 0.75)
[2025-02-05 14:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:40][root][INFO] - Training Epoch: 2/2, step 22857/23838 completed (loss: 0.9233943819999695, acc: 0.6764705777168274)
[2025-02-05 14:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:40][root][INFO] - Training Epoch: 2/2, step 22858/23838 completed (loss: 1.3776612281799316, acc: 0.5802469253540039)
[2025-02-05 14:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:40][root][INFO] - Training Epoch: 2/2, step 22859/23838 completed (loss: 1.2184301614761353, acc: 0.6564885377883911)
[2025-02-05 14:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:41][root][INFO] - Training Epoch: 2/2, step 22860/23838 completed (loss: 0.9545601606369019, acc: 0.7448979616165161)
[2025-02-05 14:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:41][root][INFO] - Training Epoch: 2/2, step 22861/23838 completed (loss: 1.1826292276382446, acc: 0.6666666865348816)
[2025-02-05 14:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:42][root][INFO] - Training Epoch: 2/2, step 22862/23838 completed (loss: 1.1477837562561035, acc: 0.7129629850387573)
[2025-02-05 14:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:42][root][INFO] - Training Epoch: 2/2, step 22863/23838 completed (loss: 1.3466663360595703, acc: 0.5785714387893677)
[2025-02-05 14:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:42][root][INFO] - Training Epoch: 2/2, step 22864/23838 completed (loss: 0.8806199431419373, acc: 0.7372881174087524)
[2025-02-05 14:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:43][root][INFO] - Training Epoch: 2/2, step 22865/23838 completed (loss: 0.9698846936225891, acc: 0.75)
[2025-02-05 14:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:43][root][INFO] - Training Epoch: 2/2, step 22866/23838 completed (loss: 1.0418390035629272, acc: 0.6931818127632141)
[2025-02-05 14:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:44][root][INFO] - Training Epoch: 2/2, step 22867/23838 completed (loss: 1.156028389930725, acc: 0.7045454382896423)
[2025-02-05 14:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:44][root][INFO] - Training Epoch: 2/2, step 22868/23838 completed (loss: 1.226237177848816, acc: 0.6363636255264282)
[2025-02-05 14:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:44][root][INFO] - Training Epoch: 2/2, step 22869/23838 completed (loss: 1.6918872594833374, acc: 0.5833333134651184)
[2025-02-05 14:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:45][root][INFO] - Training Epoch: 2/2, step 22870/23838 completed (loss: 1.1118320226669312, acc: 0.6647727489471436)
[2025-02-05 14:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:45][root][INFO] - Training Epoch: 2/2, step 22871/23838 completed (loss: 0.7315891981124878, acc: 0.78125)
[2025-02-05 14:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:45][root][INFO] - Training Epoch: 2/2, step 22872/23838 completed (loss: 1.2521275281906128, acc: 0.5882353186607361)
[2025-02-05 14:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:46][root][INFO] - Training Epoch: 2/2, step 22873/23838 completed (loss: 1.2037116289138794, acc: 0.692307710647583)
[2025-02-05 14:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:46][root][INFO] - Training Epoch: 2/2, step 22874/23838 completed (loss: 1.0911340713500977, acc: 0.6666666865348816)
[2025-02-05 14:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:47][root][INFO] - Training Epoch: 2/2, step 22875/23838 completed (loss: 1.4289218187332153, acc: 0.5740740895271301)
[2025-02-05 14:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:47][root][INFO] - Training Epoch: 2/2, step 22876/23838 completed (loss: 1.1587384939193726, acc: 0.625)
[2025-02-05 14:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:47][root][INFO] - Training Epoch: 2/2, step 22877/23838 completed (loss: 1.677404761314392, acc: 0.5147058963775635)
[2025-02-05 14:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:48][root][INFO] - Training Epoch: 2/2, step 22878/23838 completed (loss: 1.304832100868225, acc: 0.6166666746139526)
[2025-02-05 14:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:48][root][INFO] - Training Epoch: 2/2, step 22879/23838 completed (loss: 1.1157338619232178, acc: 0.6666666865348816)
[2025-02-05 14:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:49][root][INFO] - Training Epoch: 2/2, step 22880/23838 completed (loss: 1.0605863332748413, acc: 0.7164179086685181)
[2025-02-05 14:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:49][root][INFO] - Training Epoch: 2/2, step 22881/23838 completed (loss: 1.2409435510635376, acc: 0.6268656849861145)
[2025-02-05 14:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:49][root][INFO] - Training Epoch: 2/2, step 22882/23838 completed (loss: 1.1113916635513306, acc: 0.625)
[2025-02-05 14:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:50][root][INFO] - Training Epoch: 2/2, step 22883/23838 completed (loss: 1.2755459547042847, acc: 0.6268656849861145)
[2025-02-05 14:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:50][root][INFO] - Training Epoch: 2/2, step 22884/23838 completed (loss: 1.485088586807251, acc: 0.6000000238418579)
[2025-02-05 14:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:51][root][INFO] - Training Epoch: 2/2, step 22885/23838 completed (loss: 1.035061240196228, acc: 0.6666666865348816)
[2025-02-05 14:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:51][root][INFO] - Training Epoch: 2/2, step 22886/23838 completed (loss: 1.556363582611084, acc: 0.4912280738353729)
[2025-02-05 14:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:51][root][INFO] - Training Epoch: 2/2, step 22887/23838 completed (loss: 1.5845744609832764, acc: 0.5)
[2025-02-05 14:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:52][root][INFO] - Training Epoch: 2/2, step 22888/23838 completed (loss: 1.0165637731552124, acc: 0.6222222447395325)
[2025-02-05 14:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:52][root][INFO] - Training Epoch: 2/2, step 22889/23838 completed (loss: 1.0804532766342163, acc: 0.6976743936538696)
[2025-02-05 14:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:53][root][INFO] - Training Epoch: 2/2, step 22890/23838 completed (loss: 0.8296484351158142, acc: 0.7288135886192322)
[2025-02-05 14:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:53][root][INFO] - Training Epoch: 2/2, step 22891/23838 completed (loss: 0.9605877995491028, acc: 0.7121211886405945)
[2025-02-05 14:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:53][root][INFO] - Training Epoch: 2/2, step 22892/23838 completed (loss: 0.7714726328849792, acc: 0.746268630027771)
[2025-02-05 14:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:54][root][INFO] - Training Epoch: 2/2, step 22893/23838 completed (loss: 0.8575075268745422, acc: 0.7200000286102295)
[2025-02-05 14:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:54][root][INFO] - Training Epoch: 2/2, step 22894/23838 completed (loss: 1.0301233530044556, acc: 0.7419354915618896)
[2025-02-05 14:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:55][root][INFO] - Training Epoch: 2/2, step 22895/23838 completed (loss: 1.0684677362442017, acc: 0.6859503984451294)
[2025-02-05 14:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:55][root][INFO] - Training Epoch: 2/2, step 22896/23838 completed (loss: 1.2136116027832031, acc: 0.64462810754776)
[2025-02-05 14:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:56][root][INFO] - Training Epoch: 2/2, step 22897/23838 completed (loss: 1.490155577659607, acc: 0.5704697966575623)
[2025-02-05 14:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:56][root][INFO] - Training Epoch: 2/2, step 22898/23838 completed (loss: 1.435282826423645, acc: 0.5523809790611267)
[2025-02-05 14:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:56][root][INFO] - Training Epoch: 2/2, step 22899/23838 completed (loss: 1.2032095193862915, acc: 0.6590909361839294)
[2025-02-05 14:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:57][root][INFO] - Training Epoch: 2/2, step 22900/23838 completed (loss: 1.428702712059021, acc: 0.6235294342041016)
[2025-02-05 14:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:57][root][INFO] - Training Epoch: 2/2, step 22901/23838 completed (loss: 0.9738460779190063, acc: 0.6774193644523621)
[2025-02-05 14:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:57][root][INFO] - Training Epoch: 2/2, step 22902/23838 completed (loss: 1.5215932130813599, acc: 0.5441176295280457)
[2025-02-05 14:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:58][root][INFO] - Training Epoch: 2/2, step 22903/23838 completed (loss: 1.2938005924224854, acc: 0.6168224215507507)
[2025-02-05 14:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:58][root][INFO] - Training Epoch: 2/2, step 22904/23838 completed (loss: 1.2848503589630127, acc: 0.6329113841056824)
[2025-02-05 14:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:59][root][INFO] - Training Epoch: 2/2, step 22905/23838 completed (loss: 0.9119510054588318, acc: 0.7162162065505981)
[2025-02-05 14:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:59][root][INFO] - Training Epoch: 2/2, step 22906/23838 completed (loss: 1.0370945930480957, acc: 0.6951219439506531)
[2025-02-05 14:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:12:59][root][INFO] - Training Epoch: 2/2, step 22907/23838 completed (loss: 0.9009732604026794, acc: 0.7027027010917664)
[2025-02-05 14:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:00][root][INFO] - Training Epoch: 2/2, step 22908/23838 completed (loss: 1.1913586854934692, acc: 0.6041666865348816)
[2025-02-05 14:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:00][root][INFO] - Training Epoch: 2/2, step 22909/23838 completed (loss: 1.001325249671936, acc: 0.6423357725143433)
[2025-02-05 14:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:01][root][INFO] - Training Epoch: 2/2, step 22910/23838 completed (loss: 1.0554522275924683, acc: 0.6875)
[2025-02-05 14:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:01][root][INFO] - Training Epoch: 2/2, step 22911/23838 completed (loss: 0.9246465563774109, acc: 0.7090908885002136)
[2025-02-05 14:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:01][root][INFO] - Training Epoch: 2/2, step 22912/23838 completed (loss: 1.0627171993255615, acc: 0.6470588445663452)
[2025-02-05 14:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:02][root][INFO] - Training Epoch: 2/2, step 22913/23838 completed (loss: 1.1249542236328125, acc: 0.6551724076271057)
[2025-02-05 14:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:02][root][INFO] - Training Epoch: 2/2, step 22914/23838 completed (loss: 1.0092147588729858, acc: 0.6666666865348816)
[2025-02-05 14:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:02][root][INFO] - Training Epoch: 2/2, step 22915/23838 completed (loss: 1.024168848991394, acc: 0.692307710647583)
[2025-02-05 14:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:03][root][INFO] - Training Epoch: 2/2, step 22916/23838 completed (loss: 0.9956914186477661, acc: 0.7310924530029297)
[2025-02-05 14:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:03][root][INFO] - Training Epoch: 2/2, step 22917/23838 completed (loss: 0.9209408760070801, acc: 0.7872340679168701)
[2025-02-05 14:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:04][root][INFO] - Training Epoch: 2/2, step 22918/23838 completed (loss: 1.1903191804885864, acc: 0.6632652878761292)
[2025-02-05 14:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:04][root][INFO] - Training Epoch: 2/2, step 22919/23838 completed (loss: 1.1695141792297363, acc: 0.6494845151901245)
[2025-02-05 14:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:04][root][INFO] - Training Epoch: 2/2, step 22920/23838 completed (loss: 1.0946284532546997, acc: 0.7023809552192688)
[2025-02-05 14:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:05][root][INFO] - Training Epoch: 2/2, step 22921/23838 completed (loss: 1.1296751499176025, acc: 0.6338028311729431)
[2025-02-05 14:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:05][root][INFO] - Training Epoch: 2/2, step 22922/23838 completed (loss: 1.0723868608474731, acc: 0.6851851940155029)
[2025-02-05 14:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:05][root][INFO] - Training Epoch: 2/2, step 22923/23838 completed (loss: 0.8706282377243042, acc: 0.6941176652908325)
[2025-02-05 14:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:06][root][INFO] - Training Epoch: 2/2, step 22924/23838 completed (loss: 1.3902925252914429, acc: 0.6111111044883728)
[2025-02-05 14:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:06][root][INFO] - Training Epoch: 2/2, step 22925/23838 completed (loss: 0.8219740986824036, acc: 0.7560975551605225)
[2025-02-05 14:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:07][root][INFO] - Training Epoch: 2/2, step 22926/23838 completed (loss: 0.8817422986030579, acc: 0.7419354915618896)
[2025-02-05 14:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:07][root][INFO] - Training Epoch: 2/2, step 22927/23838 completed (loss: 0.9284104704856873, acc: 0.6712328791618347)
[2025-02-05 14:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:08][root][INFO] - Training Epoch: 2/2, step 22928/23838 completed (loss: 0.7673666477203369, acc: 0.8139534592628479)
[2025-02-05 14:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:08][root][INFO] - Training Epoch: 2/2, step 22929/23838 completed (loss: 1.205310344696045, acc: 0.6666666865348816)
[2025-02-05 14:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:08][root][INFO] - Training Epoch: 2/2, step 22930/23838 completed (loss: 0.8011954426765442, acc: 0.7599999904632568)
[2025-02-05 14:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:09][root][INFO] - Training Epoch: 2/2, step 22931/23838 completed (loss: 1.1560333967208862, acc: 0.6976743936538696)
[2025-02-05 14:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:09][root][INFO] - Training Epoch: 2/2, step 22932/23838 completed (loss: 0.8283205032348633, acc: 0.7333333492279053)
[2025-02-05 14:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:09][root][INFO] - Training Epoch: 2/2, step 22933/23838 completed (loss: 0.8855154514312744, acc: 0.6976743936538696)
[2025-02-05 14:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:10][root][INFO] - Training Epoch: 2/2, step 22934/23838 completed (loss: 1.339140772819519, acc: 0.6000000238418579)
[2025-02-05 14:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:10][root][INFO] - Training Epoch: 2/2, step 22935/23838 completed (loss: 1.5073070526123047, acc: 0.4909090995788574)
[2025-02-05 14:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:11][root][INFO] - Training Epoch: 2/2, step 22936/23838 completed (loss: 1.1523860692977905, acc: 0.692307710647583)
[2025-02-05 14:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:11][root][INFO] - Training Epoch: 2/2, step 22937/23838 completed (loss: 0.9534364938735962, acc: 0.756302535533905)
[2025-02-05 14:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:11][root][INFO] - Training Epoch: 2/2, step 22938/23838 completed (loss: 1.0165127515792847, acc: 0.7419354915618896)
[2025-02-05 14:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:12][root][INFO] - Training Epoch: 2/2, step 22939/23838 completed (loss: 1.075574517250061, acc: 0.6280487775802612)
[2025-02-05 14:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:12][root][INFO] - Training Epoch: 2/2, step 22940/23838 completed (loss: 0.9189659357070923, acc: 0.7111111283302307)
[2025-02-05 14:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:13][root][INFO] - Training Epoch: 2/2, step 22941/23838 completed (loss: 0.949643611907959, acc: 0.7432432174682617)
[2025-02-05 14:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:13][root][INFO] - Training Epoch: 2/2, step 22942/23838 completed (loss: 1.0954207181930542, acc: 0.699999988079071)
[2025-02-05 14:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:13][root][INFO] - Training Epoch: 2/2, step 22943/23838 completed (loss: 1.0098004341125488, acc: 0.7301587462425232)
[2025-02-05 14:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:14][root][INFO] - Training Epoch: 2/2, step 22944/23838 completed (loss: 1.0695728063583374, acc: 0.7236841917037964)
[2025-02-05 14:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:14][root][INFO] - Training Epoch: 2/2, step 22945/23838 completed (loss: 0.9974645972251892, acc: 0.6708860993385315)
[2025-02-05 14:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:15][root][INFO] - Training Epoch: 2/2, step 22946/23838 completed (loss: 1.1311523914337158, acc: 0.6732673048973083)
[2025-02-05 14:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:15][root][INFO] - Training Epoch: 2/2, step 22947/23838 completed (loss: 0.9139940738677979, acc: 0.7083333134651184)
[2025-02-05 14:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:15][root][INFO] - Training Epoch: 2/2, step 22948/23838 completed (loss: 1.0398685932159424, acc: 0.6554622054100037)
[2025-02-05 14:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:16][root][INFO] - Training Epoch: 2/2, step 22949/23838 completed (loss: 1.1700330972671509, acc: 0.6666666865348816)
[2025-02-05 14:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:16][root][INFO] - Training Epoch: 2/2, step 22950/23838 completed (loss: 1.0952224731445312, acc: 0.6407766938209534)
[2025-02-05 14:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:17][root][INFO] - Training Epoch: 2/2, step 22951/23838 completed (loss: 1.024401307106018, acc: 0.7169811129570007)
[2025-02-05 14:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:17][root][INFO] - Training Epoch: 2/2, step 22952/23838 completed (loss: 0.7580799460411072, acc: 0.75)
[2025-02-05 14:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:17][root][INFO] - Training Epoch: 2/2, step 22953/23838 completed (loss: 0.7396501302719116, acc: 0.7849462628364563)
[2025-02-05 14:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:18][root][INFO] - Training Epoch: 2/2, step 22954/23838 completed (loss: 0.7799156308174133, acc: 0.7848101258277893)
[2025-02-05 14:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:18][root][INFO] - Training Epoch: 2/2, step 22955/23838 completed (loss: 1.1454304456710815, acc: 0.7008547186851501)
[2025-02-05 14:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:18][root][INFO] - Training Epoch: 2/2, step 22956/23838 completed (loss: 1.169475793838501, acc: 0.6376811861991882)
[2025-02-05 14:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:19][root][INFO] - Training Epoch: 2/2, step 22957/23838 completed (loss: 1.2290139198303223, acc: 0.6279069781303406)
[2025-02-05 14:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:19][root][INFO] - Training Epoch: 2/2, step 22958/23838 completed (loss: 1.3647836446762085, acc: 0.5568181872367859)
[2025-02-05 14:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:19][root][INFO] - Training Epoch: 2/2, step 22959/23838 completed (loss: 1.161476492881775, acc: 0.7058823704719543)
[2025-02-05 14:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:20][root][INFO] - Training Epoch: 2/2, step 22960/23838 completed (loss: 0.9977476000785828, acc: 0.698113203048706)
[2025-02-05 14:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:20][root][INFO] - Training Epoch: 2/2, step 22961/23838 completed (loss: 1.4129191637039185, acc: 0.6082473993301392)
[2025-02-05 14:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:21][root][INFO] - Training Epoch: 2/2, step 22962/23838 completed (loss: 1.1214320659637451, acc: 0.6696428656578064)
[2025-02-05 14:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:21][root][INFO] - Training Epoch: 2/2, step 22963/23838 completed (loss: 1.4613018035888672, acc: 0.563829779624939)
[2025-02-05 14:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:21][root][INFO] - Training Epoch: 2/2, step 22964/23838 completed (loss: 1.4916465282440186, acc: 0.6022727489471436)
[2025-02-05 14:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:22][root][INFO] - Training Epoch: 2/2, step 22965/23838 completed (loss: 1.3505516052246094, acc: 0.5967742204666138)
[2025-02-05 14:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:22][root][INFO] - Training Epoch: 2/2, step 22966/23838 completed (loss: 1.0763673782348633, acc: 0.7211538553237915)
[2025-02-05 14:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:23][root][INFO] - Training Epoch: 2/2, step 22967/23838 completed (loss: 1.0541932582855225, acc: 0.6964285969734192)
[2025-02-05 14:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:23][root][INFO] - Training Epoch: 2/2, step 22968/23838 completed (loss: 1.7583260536193848, acc: 0.4583333432674408)
[2025-02-05 14:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:23][root][INFO] - Training Epoch: 2/2, step 22969/23838 completed (loss: 1.725312352180481, acc: 0.5686274766921997)
[2025-02-05 14:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:24][root][INFO] - Training Epoch: 2/2, step 22970/23838 completed (loss: 1.482798457145691, acc: 0.5492957830429077)
[2025-02-05 14:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:24][root][INFO] - Training Epoch: 2/2, step 22971/23838 completed (loss: 1.321340560913086, acc: 0.6146789193153381)
[2025-02-05 14:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:24][root][INFO] - Training Epoch: 2/2, step 22972/23838 completed (loss: 1.2250781059265137, acc: 0.6363636255264282)
[2025-02-05 14:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:25][root][INFO] - Training Epoch: 2/2, step 22973/23838 completed (loss: 1.0667740106582642, acc: 0.6428571343421936)
[2025-02-05 14:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:25][root][INFO] - Training Epoch: 2/2, step 22974/23838 completed (loss: 1.2521400451660156, acc: 0.611940324306488)
[2025-02-05 14:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:26][root][INFO] - Training Epoch: 2/2, step 22975/23838 completed (loss: 0.8621059656143188, acc: 0.7321428656578064)
[2025-02-05 14:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:26][root][INFO] - Training Epoch: 2/2, step 22976/23838 completed (loss: 1.063126802444458, acc: 0.6716417670249939)
[2025-02-05 14:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:26][root][INFO] - Training Epoch: 2/2, step 22977/23838 completed (loss: 1.1851770877838135, acc: 0.671875)
[2025-02-05 14:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:27][root][INFO] - Training Epoch: 2/2, step 22978/23838 completed (loss: 1.0343035459518433, acc: 0.6746987700462341)
[2025-02-05 14:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:27][root][INFO] - Training Epoch: 2/2, step 22979/23838 completed (loss: 1.2602373361587524, acc: 0.5802469253540039)
[2025-02-05 14:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:28][root][INFO] - Training Epoch: 2/2, step 22980/23838 completed (loss: 1.415934681892395, acc: 0.5575221180915833)
[2025-02-05 14:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:28][root][INFO] - Training Epoch: 2/2, step 22981/23838 completed (loss: 1.1847857236862183, acc: 0.6363636255264282)
[2025-02-05 14:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:28][root][INFO] - Training Epoch: 2/2, step 22982/23838 completed (loss: 0.8426835536956787, acc: 0.75)
[2025-02-05 14:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:29][root][INFO] - Training Epoch: 2/2, step 22983/23838 completed (loss: 1.1149784326553345, acc: 0.6153846383094788)
[2025-02-05 14:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:29][root][INFO] - Training Epoch: 2/2, step 22984/23838 completed (loss: 1.3097119331359863, acc: 0.5384615659713745)
[2025-02-05 14:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:30][root][INFO] - Training Epoch: 2/2, step 22985/23838 completed (loss: 1.0028531551361084, acc: 0.6893203854560852)
[2025-02-05 14:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:30][root][INFO] - Training Epoch: 2/2, step 22986/23838 completed (loss: 1.2941898107528687, acc: 0.6162790656089783)
[2025-02-05 14:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:30][root][INFO] - Training Epoch: 2/2, step 22987/23838 completed (loss: 1.1657869815826416, acc: 0.658823549747467)
[2025-02-05 14:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:31][root][INFO] - Training Epoch: 2/2, step 22988/23838 completed (loss: 0.8716301321983337, acc: 0.7457627058029175)
[2025-02-05 14:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:31][root][INFO] - Training Epoch: 2/2, step 22989/23838 completed (loss: 1.3361551761627197, acc: 0.5511810779571533)
[2025-02-05 14:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:32][root][INFO] - Training Epoch: 2/2, step 22990/23838 completed (loss: 1.1089612245559692, acc: 0.6929824352264404)
[2025-02-05 14:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:32][root][INFO] - Training Epoch: 2/2, step 22991/23838 completed (loss: 1.1726192235946655, acc: 0.6818181872367859)
[2025-02-05 14:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:33][root][INFO] - Training Epoch: 2/2, step 22992/23838 completed (loss: 1.1125454902648926, acc: 0.7594936490058899)
[2025-02-05 14:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:33][root][INFO] - Training Epoch: 2/2, step 22993/23838 completed (loss: 1.1751092672348022, acc: 0.6697247624397278)
[2025-02-05 14:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:33][root][INFO] - Training Epoch: 2/2, step 22994/23838 completed (loss: 1.4291192293167114, acc: 0.5948275923728943)
[2025-02-05 14:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:34][root][INFO] - Training Epoch: 2/2, step 22995/23838 completed (loss: 1.5537956953048706, acc: 0.573913037776947)
[2025-02-05 14:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:34][root][INFO] - Training Epoch: 2/2, step 22996/23838 completed (loss: 1.1499769687652588, acc: 0.6399999856948853)
[2025-02-05 14:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:35][root][INFO] - Training Epoch: 2/2, step 22997/23838 completed (loss: 1.1543481349945068, acc: 0.6535947918891907)
[2025-02-05 14:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:35][root][INFO] - Training Epoch: 2/2, step 22998/23838 completed (loss: 1.3050658702850342, acc: 0.6708860993385315)
[2025-02-05 14:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:36][root][INFO] - Training Epoch: 2/2, step 22999/23838 completed (loss: 1.4871550798416138, acc: 0.5857142806053162)
[2025-02-05 14:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:36][root][INFO] - Training Epoch: 2/2, step 23000/23838 completed (loss: 1.3714654445648193, acc: 0.6159999966621399)
[2025-02-05 14:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:37][root][INFO] - Training Epoch: 2/2, step 23001/23838 completed (loss: 1.401476263999939, acc: 0.5967742204666138)
[2025-02-05 14:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:37][root][INFO] - Training Epoch: 2/2, step 23002/23838 completed (loss: 0.7076549530029297, acc: 0.8148148059844971)
[2025-02-05 14:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:37][root][INFO] - Training Epoch: 2/2, step 23003/23838 completed (loss: 1.1105172634124756, acc: 0.7068965435028076)
[2025-02-05 14:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:38][root][INFO] - Training Epoch: 2/2, step 23004/23838 completed (loss: 1.0616661310195923, acc: 0.6554622054100037)
[2025-02-05 14:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:38][root][INFO] - Training Epoch: 2/2, step 23005/23838 completed (loss: 1.092308759689331, acc: 0.6627907156944275)
[2025-02-05 14:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:39][root][INFO] - Training Epoch: 2/2, step 23006/23838 completed (loss: 0.9832305908203125, acc: 0.734375)
[2025-02-05 14:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:39][root][INFO] - Training Epoch: 2/2, step 23007/23838 completed (loss: 1.0473417043685913, acc: 0.6875)
[2025-02-05 14:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:39][root][INFO] - Training Epoch: 2/2, step 23008/23838 completed (loss: 1.0229957103729248, acc: 0.7108433842658997)
[2025-02-05 14:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:40][root][INFO] - Training Epoch: 2/2, step 23009/23838 completed (loss: 0.9736177921295166, acc: 0.7111111283302307)
[2025-02-05 14:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:40][root][INFO] - Training Epoch: 2/2, step 23010/23838 completed (loss: 1.1306556463241577, acc: 0.6697247624397278)
[2025-02-05 14:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:41][root][INFO] - Training Epoch: 2/2, step 23011/23838 completed (loss: 0.8521586060523987, acc: 0.7368420958518982)
[2025-02-05 14:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:41][root][INFO] - Training Epoch: 2/2, step 23012/23838 completed (loss: 0.5601401925086975, acc: 0.8399999737739563)
[2025-02-05 14:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:42][root][INFO] - Training Epoch: 2/2, step 23013/23838 completed (loss: 1.1484533548355103, acc: 0.7009345889091492)
[2025-02-05 14:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:42][root][INFO] - Training Epoch: 2/2, step 23014/23838 completed (loss: 1.231529951095581, acc: 0.6666666865348816)
[2025-02-05 14:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:43][root][INFO] - Training Epoch: 2/2, step 23015/23838 completed (loss: 1.114349126815796, acc: 0.7235772609710693)
[2025-02-05 14:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:44][root][INFO] - Training Epoch: 2/2, step 23016/23838 completed (loss: 1.183585524559021, acc: 0.6571428775787354)
[2025-02-05 14:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:44][root][INFO] - Training Epoch: 2/2, step 23017/23838 completed (loss: 1.213639259338379, acc: 0.6271186470985413)
[2025-02-05 14:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:44][root][INFO] - Training Epoch: 2/2, step 23018/23838 completed (loss: 1.0963914394378662, acc: 0.6567164063453674)
[2025-02-05 14:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:45][root][INFO] - Training Epoch: 2/2, step 23019/23838 completed (loss: 1.1978522539138794, acc: 0.717391312122345)
[2025-02-05 14:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:45][root][INFO] - Training Epoch: 2/2, step 23020/23838 completed (loss: 1.282617211341858, acc: 0.6404494643211365)
[2025-02-05 14:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:46][root][INFO] - Training Epoch: 2/2, step 23021/23838 completed (loss: 1.1172101497650146, acc: 0.6499999761581421)
[2025-02-05 14:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:46][root][INFO] - Training Epoch: 2/2, step 23022/23838 completed (loss: 1.1162652969360352, acc: 0.6515151262283325)
[2025-02-05 14:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:46][root][INFO] - Training Epoch: 2/2, step 23023/23838 completed (loss: 1.4557362794876099, acc: 0.5454545617103577)
[2025-02-05 14:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:47][root][INFO] - Training Epoch: 2/2, step 23024/23838 completed (loss: 1.2068864107131958, acc: 0.6271186470985413)
[2025-02-05 14:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:47][root][INFO] - Training Epoch: 2/2, step 23025/23838 completed (loss: 1.0231602191925049, acc: 0.698630154132843)
[2025-02-05 14:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:47][root][INFO] - Training Epoch: 2/2, step 23026/23838 completed (loss: 1.2966959476470947, acc: 0.6081081032752991)
[2025-02-05 14:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:48][root][INFO] - Training Epoch: 2/2, step 23027/23838 completed (loss: 1.252341389656067, acc: 0.6363636255264282)
[2025-02-05 14:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:48][root][INFO] - Training Epoch: 2/2, step 23028/23838 completed (loss: 1.3397042751312256, acc: 0.6265060305595398)
[2025-02-05 14:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:49][root][INFO] - Training Epoch: 2/2, step 23029/23838 completed (loss: 1.1522190570831299, acc: 0.640625)
[2025-02-05 14:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:49][root][INFO] - Training Epoch: 2/2, step 23030/23838 completed (loss: 1.289935827255249, acc: 0.5454545617103577)
[2025-02-05 14:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:49][root][INFO] - Training Epoch: 2/2, step 23031/23838 completed (loss: 1.3288733959197998, acc: 0.5471698045730591)
[2025-02-05 14:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:50][root][INFO] - Training Epoch: 2/2, step 23032/23838 completed (loss: 0.9543043375015259, acc: 0.6666666865348816)
[2025-02-05 14:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:50][root][INFO] - Training Epoch: 2/2, step 23033/23838 completed (loss: 0.9037409424781799, acc: 0.6470588445663452)
[2025-02-05 14:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:50][root][INFO] - Training Epoch: 2/2, step 23034/23838 completed (loss: 1.1378965377807617, acc: 0.6666666865348816)
[2025-02-05 14:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:51][root][INFO] - Training Epoch: 2/2, step 23035/23838 completed (loss: 0.8609012365341187, acc: 0.7250000238418579)
[2025-02-05 14:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:51][root][INFO] - Training Epoch: 2/2, step 23036/23838 completed (loss: 1.046247124671936, acc: 0.6290322542190552)
[2025-02-05 14:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:52][root][INFO] - Training Epoch: 2/2, step 23037/23838 completed (loss: 0.7757911682128906, acc: 0.7142857313156128)
[2025-02-05 14:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:52][root][INFO] - Training Epoch: 2/2, step 23038/23838 completed (loss: 1.1848489046096802, acc: 0.6034482717514038)
[2025-02-05 14:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:52][root][INFO] - Training Epoch: 2/2, step 23039/23838 completed (loss: 1.0677906274795532, acc: 0.7037037014961243)
[2025-02-05 14:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:53][root][INFO] - Training Epoch: 2/2, step 23040/23838 completed (loss: 0.8445723056793213, acc: 0.7555555701255798)
[2025-02-05 14:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:53][root][INFO] - Training Epoch: 2/2, step 23041/23838 completed (loss: 1.0116513967514038, acc: 0.7088607549667358)
[2025-02-05 14:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:54][root][INFO] - Training Epoch: 2/2, step 23042/23838 completed (loss: 0.7758121490478516, acc: 0.774193525314331)
[2025-02-05 14:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:54][root][INFO] - Training Epoch: 2/2, step 23043/23838 completed (loss: 0.7847498655319214, acc: 0.7878788113594055)
[2025-02-05 14:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:54][root][INFO] - Training Epoch: 2/2, step 23044/23838 completed (loss: 0.9110811948776245, acc: 0.738095223903656)
[2025-02-05 14:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:55][root][INFO] - Training Epoch: 2/2, step 23045/23838 completed (loss: 1.386262059211731, acc: 0.5617977380752563)
[2025-02-05 14:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:55][root][INFO] - Training Epoch: 2/2, step 23046/23838 completed (loss: 1.1520248651504517, acc: 0.6724137663841248)
[2025-02-05 14:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:56][root][INFO] - Training Epoch: 2/2, step 23047/23838 completed (loss: 1.1625851392745972, acc: 0.6304348111152649)
[2025-02-05 14:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:56][root][INFO] - Training Epoch: 2/2, step 23048/23838 completed (loss: 0.7858132123947144, acc: 0.8095238208770752)
[2025-02-05 14:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:57][root][INFO] - Training Epoch: 2/2, step 23049/23838 completed (loss: 1.3987466096878052, acc: 0.6153846383094788)
[2025-02-05 14:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:57][root][INFO] - Training Epoch: 2/2, step 23050/23838 completed (loss: 0.9238512516021729, acc: 0.7200000286102295)
[2025-02-05 14:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:57][root][INFO] - Training Epoch: 2/2, step 23051/23838 completed (loss: 1.0517772436141968, acc: 0.6849315166473389)
[2025-02-05 14:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:58][root][INFO] - Training Epoch: 2/2, step 23052/23838 completed (loss: 0.967534065246582, acc: 0.6410256624221802)
[2025-02-05 14:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:58][root][INFO] - Training Epoch: 2/2, step 23053/23838 completed (loss: 1.133381962776184, acc: 0.6407766938209534)
[2025-02-05 14:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:58][root][INFO] - Training Epoch: 2/2, step 23054/23838 completed (loss: 0.9638202786445618, acc: 0.6984127163887024)
[2025-02-05 14:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:59][root][INFO] - Training Epoch: 2/2, step 23055/23838 completed (loss: 1.2544405460357666, acc: 0.6551724076271057)
[2025-02-05 14:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:59][root][INFO] - Training Epoch: 2/2, step 23056/23838 completed (loss: 1.1590238809585571, acc: 0.644444465637207)
[2025-02-05 14:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:13:59][root][INFO] - Training Epoch: 2/2, step 23057/23838 completed (loss: 0.9981979131698608, acc: 0.7368420958518982)
[2025-02-05 14:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:00][root][INFO] - Training Epoch: 2/2, step 23058/23838 completed (loss: 1.521450161933899, acc: 0.625)
[2025-02-05 14:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:00][root][INFO] - Training Epoch: 2/2, step 23059/23838 completed (loss: 1.4771867990493774, acc: 0.6071428656578064)
[2025-02-05 14:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:01][root][INFO] - Training Epoch: 2/2, step 23060/23838 completed (loss: 1.0898613929748535, acc: 0.6973684430122375)
[2025-02-05 14:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:01][root][INFO] - Training Epoch: 2/2, step 23061/23838 completed (loss: 1.112478256225586, acc: 0.6976743936538696)
[2025-02-05 14:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:01][root][INFO] - Training Epoch: 2/2, step 23062/23838 completed (loss: 1.14994215965271, acc: 0.6875)
[2025-02-05 14:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:02][root][INFO] - Training Epoch: 2/2, step 23063/23838 completed (loss: 1.0215486288070679, acc: 0.694915235042572)
[2025-02-05 14:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:02][root][INFO] - Training Epoch: 2/2, step 23064/23838 completed (loss: 1.2101080417633057, acc: 0.640625)
[2025-02-05 14:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:02][root][INFO] - Training Epoch: 2/2, step 23065/23838 completed (loss: 1.3454147577285767, acc: 0.5657894611358643)
[2025-02-05 14:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:03][root][INFO] - Training Epoch: 2/2, step 23066/23838 completed (loss: 1.0629725456237793, acc: 0.6994535326957703)
[2025-02-05 14:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:03][root][INFO] - Training Epoch: 2/2, step 23067/23838 completed (loss: 1.1955163478851318, acc: 0.6712328791618347)
[2025-02-05 14:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:04][root][INFO] - Training Epoch: 2/2, step 23068/23838 completed (loss: 0.9724646806716919, acc: 0.6800000071525574)
[2025-02-05 14:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:04][root][INFO] - Training Epoch: 2/2, step 23069/23838 completed (loss: 1.3656686544418335, acc: 0.6190476417541504)
[2025-02-05 14:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:04][root][INFO] - Training Epoch: 2/2, step 23070/23838 completed (loss: 1.2277272939682007, acc: 0.6299999952316284)
[2025-02-05 14:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:05][root][INFO] - Training Epoch: 2/2, step 23071/23838 completed (loss: 1.1395823955535889, acc: 0.6891891956329346)
[2025-02-05 14:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:05][root][INFO] - Training Epoch: 2/2, step 23072/23838 completed (loss: 1.2832343578338623, acc: 0.5714285969734192)
[2025-02-05 14:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:06][root][INFO] - Training Epoch: 2/2, step 23073/23838 completed (loss: 1.1109164953231812, acc: 0.6463414430618286)
[2025-02-05 14:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:06][root][INFO] - Training Epoch: 2/2, step 23074/23838 completed (loss: 0.7900974750518799, acc: 0.8352941274642944)
[2025-02-05 14:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:06][root][INFO] - Training Epoch: 2/2, step 23075/23838 completed (loss: 1.094870686531067, acc: 0.6785714030265808)
[2025-02-05 14:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:07][root][INFO] - Training Epoch: 2/2, step 23076/23838 completed (loss: 0.982920229434967, acc: 0.6962025165557861)
[2025-02-05 14:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:07][root][INFO] - Training Epoch: 2/2, step 23077/23838 completed (loss: 1.0728322267532349, acc: 0.699999988079071)
[2025-02-05 14:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:08][root][INFO] - Training Epoch: 2/2, step 23078/23838 completed (loss: 1.0064424276351929, acc: 0.7043478488922119)
[2025-02-05 14:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:08][root][INFO] - Training Epoch: 2/2, step 23079/23838 completed (loss: 1.4790103435516357, acc: 0.5762711763381958)
[2025-02-05 14:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:08][root][INFO] - Training Epoch: 2/2, step 23080/23838 completed (loss: 0.5094836950302124, acc: 0.828125)
[2025-02-05 14:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:09][root][INFO] - Training Epoch: 2/2, step 23081/23838 completed (loss: 1.1446151733398438, acc: 0.7575757503509521)
[2025-02-05 14:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:09][root][INFO] - Training Epoch: 2/2, step 23082/23838 completed (loss: 1.7746021747589111, acc: 0.5483871102333069)
[2025-02-05 14:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:10][root][INFO] - Training Epoch: 2/2, step 23083/23838 completed (loss: 0.9637447595596313, acc: 0.6904761791229248)
[2025-02-05 14:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:10][root][INFO] - Training Epoch: 2/2, step 23084/23838 completed (loss: 1.113162875175476, acc: 0.6603773832321167)
[2025-02-05 14:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:10][root][INFO] - Training Epoch: 2/2, step 23085/23838 completed (loss: 1.6910157203674316, acc: 0.5)
[2025-02-05 14:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:11][root][INFO] - Training Epoch: 2/2, step 23086/23838 completed (loss: 1.0350921154022217, acc: 0.7058823704719543)
[2025-02-05 14:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:11][root][INFO] - Training Epoch: 2/2, step 23087/23838 completed (loss: 1.109960675239563, acc: 0.7068965435028076)
[2025-02-05 14:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:12][root][INFO] - Training Epoch: 2/2, step 23088/23838 completed (loss: 0.8423814177513123, acc: 0.7407407164573669)
[2025-02-05 14:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:12][root][INFO] - Training Epoch: 2/2, step 23089/23838 completed (loss: 1.1868125200271606, acc: 0.6341463327407837)
[2025-02-05 14:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:13][root][INFO] - Training Epoch: 2/2, step 23090/23838 completed (loss: 1.207727313041687, acc: 0.6842105388641357)
[2025-02-05 14:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:13][root][INFO] - Training Epoch: 2/2, step 23091/23838 completed (loss: 1.2236244678497314, acc: 0.653333306312561)
[2025-02-05 14:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:14][root][INFO] - Training Epoch: 2/2, step 23092/23838 completed (loss: 1.3789774179458618, acc: 0.644444465637207)
[2025-02-05 14:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:14][root][INFO] - Training Epoch: 2/2, step 23093/23838 completed (loss: 1.3590646982192993, acc: 0.6393442749977112)
[2025-02-05 14:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:14][root][INFO] - Training Epoch: 2/2, step 23094/23838 completed (loss: 1.1943713426589966, acc: 0.6612903475761414)
[2025-02-05 14:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:15][root][INFO] - Training Epoch: 2/2, step 23095/23838 completed (loss: 1.1058788299560547, acc: 0.6626505851745605)
[2025-02-05 14:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:15][root][INFO] - Training Epoch: 2/2, step 23096/23838 completed (loss: 1.092691421508789, acc: 0.5853658318519592)
[2025-02-05 14:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:16][root][INFO] - Training Epoch: 2/2, step 23097/23838 completed (loss: 1.1598814725875854, acc: 0.6176470518112183)
[2025-02-05 14:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:16][root][INFO] - Training Epoch: 2/2, step 23098/23838 completed (loss: 1.0007297992706299, acc: 0.703125)
[2025-02-05 14:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:16][root][INFO] - Training Epoch: 2/2, step 23099/23838 completed (loss: 0.8663787841796875, acc: 0.7200000286102295)
[2025-02-05 14:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:17][root][INFO] - Training Epoch: 2/2, step 23100/23838 completed (loss: 1.1191906929016113, acc: 0.686274528503418)
[2025-02-05 14:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:17][root][INFO] - Training Epoch: 2/2, step 23101/23838 completed (loss: 1.1677730083465576, acc: 0.6603773832321167)
[2025-02-05 14:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:17][root][INFO] - Training Epoch: 2/2, step 23102/23838 completed (loss: 1.00557279586792, acc: 0.682539701461792)
[2025-02-05 14:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:18][root][INFO] - Training Epoch: 2/2, step 23103/23838 completed (loss: 1.26814866065979, acc: 0.644444465637207)
[2025-02-05 14:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:18][root][INFO] - Training Epoch: 2/2, step 23104/23838 completed (loss: 1.2760125398635864, acc: 0.625)
[2025-02-05 14:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:19][root][INFO] - Training Epoch: 2/2, step 23105/23838 completed (loss: 1.4229618310928345, acc: 0.5263158082962036)
[2025-02-05 14:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:19][root][INFO] - Training Epoch: 2/2, step 23106/23838 completed (loss: 0.9733933210372925, acc: 0.6944444179534912)
[2025-02-05 14:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:20][root][INFO] - Training Epoch: 2/2, step 23107/23838 completed (loss: 1.4365949630737305, acc: 0.5740740895271301)
[2025-02-05 14:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:20][root][INFO] - Training Epoch: 2/2, step 23108/23838 completed (loss: 1.0896692276000977, acc: 0.7078651785850525)
[2025-02-05 14:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:20][root][INFO] - Training Epoch: 2/2, step 23109/23838 completed (loss: 0.7842052578926086, acc: 0.739130437374115)
[2025-02-05 14:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:21][root][INFO] - Training Epoch: 2/2, step 23110/23838 completed (loss: 1.4497162103652954, acc: 0.6136363744735718)
[2025-02-05 14:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:21][root][INFO] - Training Epoch: 2/2, step 23111/23838 completed (loss: 1.37086820602417, acc: 0.6216216087341309)
[2025-02-05 14:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:21][root][INFO] - Training Epoch: 2/2, step 23112/23838 completed (loss: 1.5594366788864136, acc: 0.5263158082962036)
[2025-02-05 14:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:22][root][INFO] - Training Epoch: 2/2, step 23113/23838 completed (loss: 1.3347290754318237, acc: 0.5925925970077515)
[2025-02-05 14:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:22][root][INFO] - Training Epoch: 2/2, step 23114/23838 completed (loss: 1.182889461517334, acc: 0.625)
[2025-02-05 14:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:23][root][INFO] - Training Epoch: 2/2, step 23115/23838 completed (loss: 1.1580381393432617, acc: 0.630630612373352)
[2025-02-05 14:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:23][root][INFO] - Training Epoch: 2/2, step 23116/23838 completed (loss: 1.0514023303985596, acc: 0.7200000286102295)
[2025-02-05 14:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:23][root][INFO] - Training Epoch: 2/2, step 23117/23838 completed (loss: 0.8568752408027649, acc: 0.682539701461792)
[2025-02-05 14:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:24][root][INFO] - Training Epoch: 2/2, step 23118/23838 completed (loss: 0.9659488797187805, acc: 0.6666666865348816)
[2025-02-05 14:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:24][root][INFO] - Training Epoch: 2/2, step 23119/23838 completed (loss: 0.6821500062942505, acc: 0.8055555820465088)
[2025-02-05 14:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:24][root][INFO] - Training Epoch: 2/2, step 23120/23838 completed (loss: 0.9510444402694702, acc: 0.774193525314331)
[2025-02-05 14:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:25][root][INFO] - Training Epoch: 2/2, step 23121/23838 completed (loss: 1.3475213050842285, acc: 0.5897436141967773)
[2025-02-05 14:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:25][root][INFO] - Training Epoch: 2/2, step 23122/23838 completed (loss: 1.0499153137207031, acc: 0.6938775777816772)
[2025-02-05 14:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:26][root][INFO] - Training Epoch: 2/2, step 23123/23838 completed (loss: 1.1692708730697632, acc: 0.6666666865348816)
[2025-02-05 14:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:26][root][INFO] - Training Epoch: 2/2, step 23124/23838 completed (loss: 1.1624614000320435, acc: 0.7250000238418579)
[2025-02-05 14:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:26][root][INFO] - Training Epoch: 2/2, step 23125/23838 completed (loss: 1.144190788269043, acc: 0.6774193644523621)
[2025-02-05 14:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:27][root][INFO] - Training Epoch: 2/2, step 23126/23838 completed (loss: 1.2368741035461426, acc: 0.6744186282157898)
[2025-02-05 14:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:27][root][INFO] - Training Epoch: 2/2, step 23127/23838 completed (loss: 1.0410419702529907, acc: 0.6504854559898376)
[2025-02-05 14:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:28][root][INFO] - Training Epoch: 2/2, step 23128/23838 completed (loss: 1.1577671766281128, acc: 0.6666666865348816)
[2025-02-05 14:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:28][root][INFO] - Training Epoch: 2/2, step 23129/23838 completed (loss: 1.3640906810760498, acc: 0.6209150552749634)
[2025-02-05 14:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:28][root][INFO] - Training Epoch: 2/2, step 23130/23838 completed (loss: 1.5179650783538818, acc: 0.5487805008888245)
[2025-02-05 14:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:29][root][INFO] - Training Epoch: 2/2, step 23131/23838 completed (loss: 1.5478365421295166, acc: 0.550000011920929)
[2025-02-05 14:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:29][root][INFO] - Training Epoch: 2/2, step 23132/23838 completed (loss: 1.242409348487854, acc: 0.6272727251052856)
[2025-02-05 14:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:29][root][INFO] - Training Epoch: 2/2, step 23133/23838 completed (loss: 1.4822484254837036, acc: 0.5894039869308472)
[2025-02-05 14:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:30][root][INFO] - Training Epoch: 2/2, step 23134/23838 completed (loss: 1.1815625429153442, acc: 0.6666666865348816)
[2025-02-05 14:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:30][root][INFO] - Training Epoch: 2/2, step 23135/23838 completed (loss: 1.244529366493225, acc: 0.6753246784210205)
[2025-02-05 14:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:30][root][INFO] - Training Epoch: 2/2, step 23136/23838 completed (loss: 1.2155425548553467, acc: 0.6438356041908264)
[2025-02-05 14:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:31][root][INFO] - Training Epoch: 2/2, step 23137/23838 completed (loss: 1.0228135585784912, acc: 0.7301587462425232)
[2025-02-05 14:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:31][root][INFO] - Training Epoch: 2/2, step 23138/23838 completed (loss: 1.1103841066360474, acc: 0.7019867300987244)
[2025-02-05 14:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:32][root][INFO] - Training Epoch: 2/2, step 23139/23838 completed (loss: 1.2061477899551392, acc: 0.6666666865348816)
[2025-02-05 14:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:32][root][INFO] - Training Epoch: 2/2, step 23140/23838 completed (loss: 1.2294986248016357, acc: 0.5913978219032288)
[2025-02-05 14:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:32][root][INFO] - Training Epoch: 2/2, step 23141/23838 completed (loss: 1.0915230512619019, acc: 0.6829268336296082)
[2025-02-05 14:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:33][root][INFO] - Training Epoch: 2/2, step 23142/23838 completed (loss: 1.1720333099365234, acc: 0.6941176652908325)
[2025-02-05 14:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:33][root][INFO] - Training Epoch: 2/2, step 23143/23838 completed (loss: 1.1004254817962646, acc: 0.6774193644523621)
[2025-02-05 14:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:33][root][INFO] - Training Epoch: 2/2, step 23144/23838 completed (loss: 0.971224308013916, acc: 0.75)
[2025-02-05 14:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:34][root][INFO] - Training Epoch: 2/2, step 23145/23838 completed (loss: 1.2504100799560547, acc: 0.6976743936538696)
[2025-02-05 14:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:34][root][INFO] - Training Epoch: 2/2, step 23146/23838 completed (loss: 1.2053316831588745, acc: 0.6407766938209534)
[2025-02-05 14:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:35][root][INFO] - Training Epoch: 2/2, step 23147/23838 completed (loss: 1.1452159881591797, acc: 0.6614173054695129)
[2025-02-05 14:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:35][root][INFO] - Training Epoch: 2/2, step 23148/23838 completed (loss: 1.0072237253189087, acc: 0.6976743936538696)
[2025-02-05 14:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:35][root][INFO] - Training Epoch: 2/2, step 23149/23838 completed (loss: 1.0788378715515137, acc: 0.6607142686843872)
[2025-02-05 14:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:36][root][INFO] - Training Epoch: 2/2, step 23150/23838 completed (loss: 1.675568699836731, acc: 0.48148149251937866)
[2025-02-05 14:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:36][root][INFO] - Training Epoch: 2/2, step 23151/23838 completed (loss: 1.1597750186920166, acc: 0.6162790656089783)
[2025-02-05 14:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:36][root][INFO] - Training Epoch: 2/2, step 23152/23838 completed (loss: 1.4832566976547241, acc: 0.5874999761581421)
[2025-02-05 14:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:37][root][INFO] - Training Epoch: 2/2, step 23153/23838 completed (loss: 1.181809663772583, acc: 0.6692913174629211)
[2025-02-05 14:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:37][root][INFO] - Training Epoch: 2/2, step 23154/23838 completed (loss: 1.1049515008926392, acc: 0.6521739363670349)
[2025-02-05 14:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:38][root][INFO] - Training Epoch: 2/2, step 23155/23838 completed (loss: 1.0152103900909424, acc: 0.6818181872367859)
[2025-02-05 14:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:38][root][INFO] - Training Epoch: 2/2, step 23156/23838 completed (loss: 1.0543138980865479, acc: 0.738095223903656)
[2025-02-05 14:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:38][root][INFO] - Training Epoch: 2/2, step 23157/23838 completed (loss: 1.2452058792114258, acc: 0.6236559152603149)
[2025-02-05 14:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:39][root][INFO] - Training Epoch: 2/2, step 23158/23838 completed (loss: 1.1545941829681396, acc: 0.699999988079071)
[2025-02-05 14:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:39][root][INFO] - Training Epoch: 2/2, step 23159/23838 completed (loss: 1.241627812385559, acc: 0.6206896305084229)
[2025-02-05 14:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:39][root][INFO] - Training Epoch: 2/2, step 23160/23838 completed (loss: 0.8699617385864258, acc: 0.7307692170143127)
[2025-02-05 14:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:40][root][INFO] - Training Epoch: 2/2, step 23161/23838 completed (loss: 1.004990577697754, acc: 0.7397260069847107)
[2025-02-05 14:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:40][root][INFO] - Training Epoch: 2/2, step 23162/23838 completed (loss: 1.1080458164215088, acc: 0.6565656661987305)
[2025-02-05 14:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:41][root][INFO] - Training Epoch: 2/2, step 23163/23838 completed (loss: 0.715518593788147, acc: 0.719298243522644)
[2025-02-05 14:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:41][root][INFO] - Training Epoch: 2/2, step 23164/23838 completed (loss: 1.375817894935608, acc: 0.5892857313156128)
[2025-02-05 14:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:41][root][INFO] - Training Epoch: 2/2, step 23165/23838 completed (loss: 0.9479394555091858, acc: 0.747474730014801)
[2025-02-05 14:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:42][root][INFO] - Training Epoch: 2/2, step 23166/23838 completed (loss: 1.1466671228408813, acc: 0.7307692170143127)
[2025-02-05 14:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:42][root][INFO] - Training Epoch: 2/2, step 23167/23838 completed (loss: 1.293234944343567, acc: 0.617977499961853)
[2025-02-05 14:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:42][root][INFO] - Training Epoch: 2/2, step 23168/23838 completed (loss: 0.8353769779205322, acc: 0.7692307829856873)
[2025-02-05 14:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:43][root][INFO] - Training Epoch: 2/2, step 23169/23838 completed (loss: 0.9955676198005676, acc: 0.6886792182922363)
[2025-02-05 14:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:43][root][INFO] - Training Epoch: 2/2, step 23170/23838 completed (loss: 0.9670910835266113, acc: 0.7058823704719543)
[2025-02-05 14:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:43][root][INFO] - Training Epoch: 2/2, step 23171/23838 completed (loss: 0.9907551407814026, acc: 0.6707317233085632)
[2025-02-05 14:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:44][root][INFO] - Training Epoch: 2/2, step 23172/23838 completed (loss: 1.083295226097107, acc: 0.6774193644523621)
[2025-02-05 14:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:44][root][INFO] - Training Epoch: 2/2, step 23173/23838 completed (loss: 0.9346814751625061, acc: 0.7297297120094299)
[2025-02-05 14:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:45][root][INFO] - Training Epoch: 2/2, step 23174/23838 completed (loss: 0.7182344794273376, acc: 0.7956989407539368)
[2025-02-05 14:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:45][root][INFO] - Training Epoch: 2/2, step 23175/23838 completed (loss: 1.1294209957122803, acc: 0.6666666865348816)
[2025-02-05 14:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:45][root][INFO] - Training Epoch: 2/2, step 23176/23838 completed (loss: 1.0826053619384766, acc: 0.6770833134651184)
[2025-02-05 14:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:46][root][INFO] - Training Epoch: 2/2, step 23177/23838 completed (loss: 0.9468650817871094, acc: 0.6666666865348816)
[2025-02-05 14:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:46][root][INFO] - Training Epoch: 2/2, step 23178/23838 completed (loss: 1.2277979850769043, acc: 0.6037735939025879)
[2025-02-05 14:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:47][root][INFO] - Training Epoch: 2/2, step 23179/23838 completed (loss: 1.1664186716079712, acc: 0.625)
[2025-02-05 14:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:47][root][INFO] - Training Epoch: 2/2, step 23180/23838 completed (loss: 0.5739017724990845, acc: 0.800000011920929)
[2025-02-05 14:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:47][root][INFO] - Training Epoch: 2/2, step 23181/23838 completed (loss: 0.9722561836242676, acc: 0.6349206566810608)
[2025-02-05 14:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:48][root][INFO] - Training Epoch: 2/2, step 23182/23838 completed (loss: 1.3329253196716309, acc: 0.6358381509780884)
[2025-02-05 14:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:48][root][INFO] - Training Epoch: 2/2, step 23183/23838 completed (loss: 1.0130395889282227, acc: 0.739130437374115)
[2025-02-05 14:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:49][root][INFO] - Training Epoch: 2/2, step 23184/23838 completed (loss: 1.2826523780822754, acc: 0.6124030947685242)
[2025-02-05 14:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:49][root][INFO] - Training Epoch: 2/2, step 23185/23838 completed (loss: 1.1714677810668945, acc: 0.6701570749282837)
[2025-02-05 14:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:49][root][INFO] - Training Epoch: 2/2, step 23186/23838 completed (loss: 1.0511735677719116, acc: 0.6805555820465088)
[2025-02-05 14:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:50][root][INFO] - Training Epoch: 2/2, step 23187/23838 completed (loss: 1.3845051527023315, acc: 0.6214285492897034)
[2025-02-05 14:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:50][root][INFO] - Training Epoch: 2/2, step 23188/23838 completed (loss: 1.1102310419082642, acc: 0.6315789222717285)
[2025-02-05 14:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:51][root][INFO] - Training Epoch: 2/2, step 23189/23838 completed (loss: 0.8252200484275818, acc: 0.6818181872367859)
[2025-02-05 14:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:51][root][INFO] - Training Epoch: 2/2, step 23190/23838 completed (loss: 1.2321116924285889, acc: 0.653333306312561)
[2025-02-05 14:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:51][root][INFO] - Training Epoch: 2/2, step 23191/23838 completed (loss: 0.9213632345199585, acc: 0.7171717286109924)
[2025-02-05 14:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:52][root][INFO] - Training Epoch: 2/2, step 23192/23838 completed (loss: 1.16404128074646, acc: 0.6666666865348816)
[2025-02-05 14:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:52][root][INFO] - Training Epoch: 2/2, step 23193/23838 completed (loss: 1.1306021213531494, acc: 0.7160493731498718)
[2025-02-05 14:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:52][root][INFO] - Training Epoch: 2/2, step 23194/23838 completed (loss: 1.2510697841644287, acc: 0.6666666865348816)
[2025-02-05 14:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:53][root][INFO] - Training Epoch: 2/2, step 23195/23838 completed (loss: 0.9077042937278748, acc: 0.70652174949646)
[2025-02-05 14:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:53][root][INFO] - Training Epoch: 2/2, step 23196/23838 completed (loss: 1.12903892993927, acc: 0.6666666865348816)
[2025-02-05 14:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:54][root][INFO] - Training Epoch: 2/2, step 23197/23838 completed (loss: 0.9488791227340698, acc: 0.6842105388641357)
[2025-02-05 14:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:54][root][INFO] - Training Epoch: 2/2, step 23198/23838 completed (loss: 1.2887800931930542, acc: 0.6534653306007385)
[2025-02-05 14:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:54][root][INFO] - Training Epoch: 2/2, step 23199/23838 completed (loss: 1.1581635475158691, acc: 0.6483516693115234)
[2025-02-05 14:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:55][root][INFO] - Training Epoch: 2/2, step 23200/23838 completed (loss: 1.2206337451934814, acc: 0.6694214940071106)
[2025-02-05 14:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:55][root][INFO] - Training Epoch: 2/2, step 23201/23838 completed (loss: 1.0877494812011719, acc: 0.6666666865348816)
[2025-02-05 14:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:55][root][INFO] - Training Epoch: 2/2, step 23202/23838 completed (loss: 0.9732276201248169, acc: 0.6616541147232056)
[2025-02-05 14:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:56][root][INFO] - Training Epoch: 2/2, step 23203/23838 completed (loss: 1.1795955896377563, acc: 0.6727272868156433)
[2025-02-05 14:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:56][root][INFO] - Training Epoch: 2/2, step 23204/23838 completed (loss: 1.066737174987793, acc: 0.7037037014961243)
[2025-02-05 14:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:57][root][INFO] - Training Epoch: 2/2, step 23205/23838 completed (loss: 1.0186368227005005, acc: 0.6349206566810608)
[2025-02-05 14:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:57][root][INFO] - Training Epoch: 2/2, step 23206/23838 completed (loss: 0.9995535016059875, acc: 0.75)
[2025-02-05 14:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:57][root][INFO] - Training Epoch: 2/2, step 23207/23838 completed (loss: 0.9724850654602051, acc: 0.7042253613471985)
[2025-02-05 14:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:58][root][INFO] - Training Epoch: 2/2, step 23208/23838 completed (loss: 1.0960129499435425, acc: 0.7083333134651184)
[2025-02-05 14:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:58][root][INFO] - Training Epoch: 2/2, step 23209/23838 completed (loss: 0.8059309124946594, acc: 0.7333333492279053)
[2025-02-05 14:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:58][root][INFO] - Training Epoch: 2/2, step 23210/23838 completed (loss: 0.9332624077796936, acc: 0.707317054271698)
[2025-02-05 14:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:59][root][INFO] - Training Epoch: 2/2, step 23211/23838 completed (loss: 1.254908800125122, acc: 0.6428571343421936)
[2025-02-05 14:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:14:59][root][INFO] - Training Epoch: 2/2, step 23212/23838 completed (loss: 1.5325541496276855, acc: 0.469696968793869)
[2025-02-05 14:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:00][root][INFO] - Training Epoch: 2/2, step 23213/23838 completed (loss: 1.1140245199203491, acc: 0.662162184715271)
[2025-02-05 14:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:00][root][INFO] - Training Epoch: 2/2, step 23214/23838 completed (loss: 1.0939629077911377, acc: 0.6263736486434937)
[2025-02-05 14:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:00][root][INFO] - Training Epoch: 2/2, step 23215/23838 completed (loss: 1.1164989471435547, acc: 0.6969696879386902)
[2025-02-05 14:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:01][root][INFO] - Training Epoch: 2/2, step 23216/23838 completed (loss: 1.168614387512207, acc: 0.6694915294647217)
[2025-02-05 14:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:01][root][INFO] - Training Epoch: 2/2, step 23217/23838 completed (loss: 1.0572199821472168, acc: 0.663551390171051)
[2025-02-05 14:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:02][root][INFO] - Training Epoch: 2/2, step 23218/23838 completed (loss: 1.252629041671753, acc: 0.6776859760284424)
[2025-02-05 14:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:02][root][INFO] - Training Epoch: 2/2, step 23219/23838 completed (loss: 1.3024693727493286, acc: 0.6272727251052856)
[2025-02-05 14:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:02][root][INFO] - Training Epoch: 2/2, step 23220/23838 completed (loss: 1.0531072616577148, acc: 0.7063491940498352)
[2025-02-05 14:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:03][root][INFO] - Training Epoch: 2/2, step 23221/23838 completed (loss: 1.1676124334335327, acc: 0.6499999761581421)
[2025-02-05 14:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:03][root][INFO] - Training Epoch: 2/2, step 23222/23838 completed (loss: 1.2628173828125, acc: 0.6285714507102966)
[2025-02-05 14:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:03][root][INFO] - Training Epoch: 2/2, step 23223/23838 completed (loss: 0.9445788264274597, acc: 0.746268630027771)
[2025-02-05 14:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:04][root][INFO] - Training Epoch: 2/2, step 23224/23838 completed (loss: 1.039998173713684, acc: 0.6744186282157898)
[2025-02-05 14:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:04][root][INFO] - Training Epoch: 2/2, step 23225/23838 completed (loss: 1.2967901229858398, acc: 0.6190476417541504)
[2025-02-05 14:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:04][root][INFO] - Training Epoch: 2/2, step 23226/23838 completed (loss: 1.2971891164779663, acc: 0.5760869383811951)
[2025-02-05 14:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:05][root][INFO] - Training Epoch: 2/2, step 23227/23838 completed (loss: 1.2093615531921387, acc: 0.6551724076271057)
[2025-02-05 14:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:05][root][INFO] - Training Epoch: 2/2, step 23228/23838 completed (loss: 1.047241449356079, acc: 0.6666666865348816)
[2025-02-05 14:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:06][root][INFO] - Training Epoch: 2/2, step 23229/23838 completed (loss: 1.1402785778045654, acc: 0.6063829660415649)
[2025-02-05 14:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:06][root][INFO] - Training Epoch: 2/2, step 23230/23838 completed (loss: 1.1233378648757935, acc: 0.6299212574958801)
[2025-02-05 14:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:06][root][INFO] - Training Epoch: 2/2, step 23231/23838 completed (loss: 1.3370345830917358, acc: 0.6480000019073486)
[2025-02-05 14:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:07][root][INFO] - Training Epoch: 2/2, step 23232/23838 completed (loss: 1.2924821376800537, acc: 0.6212121248245239)
[2025-02-05 14:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:07][root][INFO] - Training Epoch: 2/2, step 23233/23838 completed (loss: 0.8047300577163696, acc: 0.7777777910232544)
[2025-02-05 14:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:08][root][INFO] - Training Epoch: 2/2, step 23234/23838 completed (loss: 1.0071141719818115, acc: 0.7029703259468079)
[2025-02-05 14:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:08][root][INFO] - Training Epoch: 2/2, step 23235/23838 completed (loss: 0.8459832668304443, acc: 0.7439024448394775)
[2025-02-05 14:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:08][root][INFO] - Training Epoch: 2/2, step 23236/23838 completed (loss: 1.1773356199264526, acc: 0.6818181872367859)
[2025-02-05 14:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:09][root][INFO] - Training Epoch: 2/2, step 23237/23838 completed (loss: 0.8696465492248535, acc: 0.7364341020584106)
[2025-02-05 14:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:09][root][INFO] - Training Epoch: 2/2, step 23238/23838 completed (loss: 1.2625672817230225, acc: 0.6640625)
[2025-02-05 14:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:10][root][INFO] - Training Epoch: 2/2, step 23239/23838 completed (loss: 1.1278469562530518, acc: 0.6692913174629211)
[2025-02-05 14:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:10][root][INFO] - Training Epoch: 2/2, step 23240/23838 completed (loss: 1.2505296468734741, acc: 0.6557376980781555)
[2025-02-05 14:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:10][root][INFO] - Training Epoch: 2/2, step 23241/23838 completed (loss: 0.922249972820282, acc: 0.7333333492279053)
[2025-02-05 14:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:11][root][INFO] - Training Epoch: 2/2, step 23242/23838 completed (loss: 1.189520001411438, acc: 0.6481481194496155)
[2025-02-05 14:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:11][root][INFO] - Training Epoch: 2/2, step 23243/23838 completed (loss: 1.058807134628296, acc: 0.7094017267227173)
[2025-02-05 14:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:11][root][INFO] - Training Epoch: 2/2, step 23244/23838 completed (loss: 0.9630181789398193, acc: 0.7007874250411987)
[2025-02-05 14:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:12][root][INFO] - Training Epoch: 2/2, step 23245/23838 completed (loss: 1.1354320049285889, acc: 0.6524063944816589)
[2025-02-05 14:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:12][root][INFO] - Training Epoch: 2/2, step 23246/23838 completed (loss: 1.0752373933792114, acc: 0.6711409687995911)
[2025-02-05 14:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:13][root][INFO] - Training Epoch: 2/2, step 23247/23838 completed (loss: 1.2347162961959839, acc: 0.632478654384613)
[2025-02-05 14:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:13][root][INFO] - Training Epoch: 2/2, step 23248/23838 completed (loss: 1.5948896408081055, acc: 0.5094339847564697)
[2025-02-05 14:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:13][root][INFO] - Training Epoch: 2/2, step 23249/23838 completed (loss: 1.2461806535720825, acc: 0.6293103694915771)
[2025-02-05 14:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:14][root][INFO] - Training Epoch: 2/2, step 23250/23838 completed (loss: 0.9277459383010864, acc: 0.7387387156486511)
[2025-02-05 14:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:14][root][INFO] - Training Epoch: 2/2, step 23251/23838 completed (loss: 1.1797033548355103, acc: 0.6666666865348816)
[2025-02-05 14:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:14][root][INFO] - Training Epoch: 2/2, step 23252/23838 completed (loss: 1.3644070625305176, acc: 0.5913978219032288)
[2025-02-05 14:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:15][root][INFO] - Training Epoch: 2/2, step 23253/23838 completed (loss: 0.8815317749977112, acc: 0.7582417726516724)
[2025-02-05 14:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:15][root][INFO] - Training Epoch: 2/2, step 23254/23838 completed (loss: 1.160650372505188, acc: 0.6419752836227417)
[2025-02-05 14:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:16][root][INFO] - Training Epoch: 2/2, step 23255/23838 completed (loss: 0.9798758029937744, acc: 0.6909090876579285)
[2025-02-05 14:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:16][root][INFO] - Training Epoch: 2/2, step 23256/23838 completed (loss: 1.0398756265640259, acc: 0.6521739363670349)
[2025-02-05 14:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:16][root][INFO] - Training Epoch: 2/2, step 23257/23838 completed (loss: 1.255010724067688, acc: 0.6833333373069763)
[2025-02-05 14:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:17][root][INFO] - Training Epoch: 2/2, step 23258/23838 completed (loss: 1.168138861656189, acc: 0.6022727489471436)
[2025-02-05 14:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:17][root][INFO] - Training Epoch: 2/2, step 23259/23838 completed (loss: 0.6410784125328064, acc: 0.8461538553237915)
[2025-02-05 14:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:18][root][INFO] - Training Epoch: 2/2, step 23260/23838 completed (loss: 0.9460654258728027, acc: 0.7096773982048035)
[2025-02-05 14:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:18][root][INFO] - Training Epoch: 2/2, step 23261/23838 completed (loss: 0.9482081532478333, acc: 0.6915887594223022)
[2025-02-05 14:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:18][root][INFO] - Training Epoch: 2/2, step 23262/23838 completed (loss: 1.2950382232666016, acc: 0.6287878751754761)
[2025-02-05 14:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:19][root][INFO] - Training Epoch: 2/2, step 23263/23838 completed (loss: 1.4623408317565918, acc: 0.6000000238418579)
[2025-02-05 14:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:19][root][INFO] - Training Epoch: 2/2, step 23264/23838 completed (loss: 0.8977748155593872, acc: 0.75)
[2025-02-05 14:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:19][root][INFO] - Training Epoch: 2/2, step 23265/23838 completed (loss: 1.0969316959381104, acc: 0.6712328791618347)
[2025-02-05 14:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:20][root][INFO] - Training Epoch: 2/2, step 23266/23838 completed (loss: 1.0752253532409668, acc: 0.6860465407371521)
[2025-02-05 14:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:20][root][INFO] - Training Epoch: 2/2, step 23267/23838 completed (loss: 1.2101292610168457, acc: 0.6000000238418579)
[2025-02-05 14:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:21][root][INFO] - Training Epoch: 2/2, step 23268/23838 completed (loss: 0.9259995222091675, acc: 0.75)
[2025-02-05 14:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:21][root][INFO] - Training Epoch: 2/2, step 23269/23838 completed (loss: 1.1137986183166504, acc: 0.6666666865348816)
[2025-02-05 14:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:21][root][INFO] - Training Epoch: 2/2, step 23270/23838 completed (loss: 1.4901487827301025, acc: 0.6116504669189453)
[2025-02-05 14:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:22][root][INFO] - Training Epoch: 2/2, step 23271/23838 completed (loss: 1.1200928688049316, acc: 0.6463414430618286)
[2025-02-05 14:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:22][root][INFO] - Training Epoch: 2/2, step 23272/23838 completed (loss: 1.2767767906188965, acc: 0.6428571343421936)
[2025-02-05 14:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:22][root][INFO] - Training Epoch: 2/2, step 23273/23838 completed (loss: 0.912361204624176, acc: 0.7628865838050842)
[2025-02-05 14:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:23][root][INFO] - Training Epoch: 2/2, step 23274/23838 completed (loss: 1.0619312524795532, acc: 0.7413793206214905)
[2025-02-05 14:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:23][root][INFO] - Training Epoch: 2/2, step 23275/23838 completed (loss: 1.2091283798217773, acc: 0.6571428775787354)
[2025-02-05 14:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:23][root][INFO] - Training Epoch: 2/2, step 23276/23838 completed (loss: 1.2452424764633179, acc: 0.625)
[2025-02-05 14:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:24][root][INFO] - Training Epoch: 2/2, step 23277/23838 completed (loss: 1.3327100276947021, acc: 0.5862069129943848)
[2025-02-05 14:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:24][root][INFO] - Training Epoch: 2/2, step 23278/23838 completed (loss: 0.9574885964393616, acc: 0.698630154132843)
[2025-02-05 14:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:25][root][INFO] - Training Epoch: 2/2, step 23279/23838 completed (loss: 1.1636472940444946, acc: 0.6969696879386902)
[2025-02-05 14:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:25][root][INFO] - Training Epoch: 2/2, step 23280/23838 completed (loss: 0.9462391138076782, acc: 0.6511628031730652)
[2025-02-05 14:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:25][root][INFO] - Training Epoch: 2/2, step 23281/23838 completed (loss: 1.2431509494781494, acc: 0.6547619104385376)
[2025-02-05 14:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:26][root][INFO] - Training Epoch: 2/2, step 23282/23838 completed (loss: 1.0129462480545044, acc: 0.695652186870575)
[2025-02-05 14:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:26][root][INFO] - Training Epoch: 2/2, step 23283/23838 completed (loss: 1.0934617519378662, acc: 0.7096773982048035)
[2025-02-05 14:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:27][root][INFO] - Training Epoch: 2/2, step 23284/23838 completed (loss: 1.4545766115188599, acc: 0.5858585834503174)
[2025-02-05 14:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:27][root][INFO] - Training Epoch: 2/2, step 23285/23838 completed (loss: 1.0563291311264038, acc: 0.6930692791938782)
[2025-02-05 14:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:27][root][INFO] - Training Epoch: 2/2, step 23286/23838 completed (loss: 1.1073747873306274, acc: 0.6888889074325562)
[2025-02-05 14:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:28][root][INFO] - Training Epoch: 2/2, step 23287/23838 completed (loss: 0.8785136342048645, acc: 0.7153846025466919)
[2025-02-05 14:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:28][root][INFO] - Training Epoch: 2/2, step 23288/23838 completed (loss: 0.9667010307312012, acc: 0.6962962746620178)
[2025-02-05 14:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:28][root][INFO] - Training Epoch: 2/2, step 23289/23838 completed (loss: 1.0071181058883667, acc: 0.6704545617103577)
[2025-02-05 14:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:29][root][INFO] - Training Epoch: 2/2, step 23290/23838 completed (loss: 1.0131126642227173, acc: 0.75)
[2025-02-05 14:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:29][root][INFO] - Training Epoch: 2/2, step 23291/23838 completed (loss: 0.8758926391601562, acc: 0.7422680258750916)
[2025-02-05 14:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:30][root][INFO] - Training Epoch: 2/2, step 23292/23838 completed (loss: 0.9896620512008667, acc: 0.6707317233085632)
[2025-02-05 14:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:30][root][INFO] - Training Epoch: 2/2, step 23293/23838 completed (loss: 1.1801565885543823, acc: 0.604651153087616)
[2025-02-05 14:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:30][root][INFO] - Training Epoch: 2/2, step 23294/23838 completed (loss: 1.0654319524765015, acc: 0.672897219657898)
[2025-02-05 14:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:31][root][INFO] - Training Epoch: 2/2, step 23295/23838 completed (loss: 1.1450295448303223, acc: 0.6571428775787354)
[2025-02-05 14:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:31][root][INFO] - Training Epoch: 2/2, step 23296/23838 completed (loss: 1.38569176197052, acc: 0.5877193212509155)
[2025-02-05 14:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:32][root][INFO] - Training Epoch: 2/2, step 23297/23838 completed (loss: 0.8920751810073853, acc: 0.7297297120094299)
[2025-02-05 14:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:32][root][INFO] - Training Epoch: 2/2, step 23298/23838 completed (loss: 1.118155598640442, acc: 0.6932515501976013)
[2025-02-05 14:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:32][root][INFO] - Training Epoch: 2/2, step 23299/23838 completed (loss: 0.8786279559135437, acc: 0.72826087474823)
[2025-02-05 14:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:33][root][INFO] - Training Epoch: 2/2, step 23300/23838 completed (loss: 1.0902857780456543, acc: 0.6352941393852234)
[2025-02-05 14:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:33][root][INFO] - Training Epoch: 2/2, step 23301/23838 completed (loss: 1.1281447410583496, acc: 0.6959999799728394)
[2025-02-05 14:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:33][root][INFO] - Training Epoch: 2/2, step 23302/23838 completed (loss: 1.1256898641586304, acc: 0.6145833134651184)
[2025-02-05 14:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:34][root][INFO] - Training Epoch: 2/2, step 23303/23838 completed (loss: 1.1697713136672974, acc: 0.6698113083839417)
[2025-02-05 14:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:34][root][INFO] - Training Epoch: 2/2, step 23304/23838 completed (loss: 1.3093136548995972, acc: 0.5873016119003296)
[2025-02-05 14:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:35][root][INFO] - Training Epoch: 2/2, step 23305/23838 completed (loss: 1.1269726753234863, acc: 0.6736842393875122)
[2025-02-05 14:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:35][root][INFO] - Training Epoch: 2/2, step 23306/23838 completed (loss: 1.2060829401016235, acc: 0.6071428656578064)
[2025-02-05 14:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:35][root][INFO] - Training Epoch: 2/2, step 23307/23838 completed (loss: 1.320021152496338, acc: 0.6538461446762085)
[2025-02-05 14:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:36][root][INFO] - Training Epoch: 2/2, step 23308/23838 completed (loss: 1.0083869695663452, acc: 0.6952381134033203)
[2025-02-05 14:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:36][root][INFO] - Training Epoch: 2/2, step 23309/23838 completed (loss: 0.9582525491714478, acc: 0.682539701461792)
[2025-02-05 14:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:37][root][INFO] - Training Epoch: 2/2, step 23310/23838 completed (loss: 1.1717780828475952, acc: 0.7028985619544983)
[2025-02-05 14:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:37][root][INFO] - Training Epoch: 2/2, step 23311/23838 completed (loss: 1.0677039623260498, acc: 0.6964285969734192)
[2025-02-05 14:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:37][root][INFO] - Training Epoch: 2/2, step 23312/23838 completed (loss: 1.155890703201294, acc: 0.6868686676025391)
[2025-02-05 14:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:38][root][INFO] - Training Epoch: 2/2, step 23313/23838 completed (loss: 1.1302818059921265, acc: 0.6637930870056152)
[2025-02-05 14:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:38][root][INFO] - Training Epoch: 2/2, step 23314/23838 completed (loss: 0.9230948090553284, acc: 0.7096773982048035)
[2025-02-05 14:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:39][root][INFO] - Training Epoch: 2/2, step 23315/23838 completed (loss: 0.9756283760070801, acc: 0.7017543911933899)
[2025-02-05 14:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:39][root][INFO] - Training Epoch: 2/2, step 23316/23838 completed (loss: 0.9631931781768799, acc: 0.7289719581604004)
[2025-02-05 14:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:39][root][INFO] - Training Epoch: 2/2, step 23317/23838 completed (loss: 1.1298532485961914, acc: 0.6236559152603149)
[2025-02-05 14:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:40][root][INFO] - Training Epoch: 2/2, step 23318/23838 completed (loss: 1.3485080003738403, acc: 0.5871559381484985)
[2025-02-05 14:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:40][root][INFO] - Training Epoch: 2/2, step 23319/23838 completed (loss: 1.0154348611831665, acc: 0.7096773982048035)
[2025-02-05 14:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:41][root][INFO] - Training Epoch: 2/2, step 23320/23838 completed (loss: 1.0487385988235474, acc: 0.7179487347602844)
[2025-02-05 14:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:41][root][INFO] - Training Epoch: 2/2, step 23321/23838 completed (loss: 1.1251280307769775, acc: 0.6101694703102112)
[2025-02-05 14:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:41][root][INFO] - Training Epoch: 2/2, step 23322/23838 completed (loss: 1.1238120794296265, acc: 0.6438356041908264)
[2025-02-05 14:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:42][root][INFO] - Training Epoch: 2/2, step 23323/23838 completed (loss: 1.015543818473816, acc: 0.7450980544090271)
[2025-02-05 14:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:42][root][INFO] - Training Epoch: 2/2, step 23324/23838 completed (loss: 0.8521096706390381, acc: 0.7906976938247681)
[2025-02-05 14:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:42][root][INFO] - Training Epoch: 2/2, step 23325/23838 completed (loss: 0.7800601720809937, acc: 0.7272727489471436)
[2025-02-05 14:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:43][root][INFO] - Training Epoch: 2/2, step 23326/23838 completed (loss: 0.9974474310874939, acc: 0.7285714149475098)
[2025-02-05 14:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:43][root][INFO] - Training Epoch: 2/2, step 23327/23838 completed (loss: 0.9293755888938904, acc: 0.694915235042572)
[2025-02-05 14:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:44][root][INFO] - Training Epoch: 2/2, step 23328/23838 completed (loss: 0.8425190448760986, acc: 0.7559055089950562)
[2025-02-05 14:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:44][root][INFO] - Training Epoch: 2/2, step 23329/23838 completed (loss: 1.0570242404937744, acc: 0.7241379022598267)
[2025-02-05 14:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:44][root][INFO] - Training Epoch: 2/2, step 23330/23838 completed (loss: 1.0785161256790161, acc: 0.6796116232872009)
[2025-02-05 14:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:45][root][INFO] - Training Epoch: 2/2, step 23331/23838 completed (loss: 0.9952853322029114, acc: 0.6666666865348816)
[2025-02-05 14:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:45][root][INFO] - Training Epoch: 2/2, step 23332/23838 completed (loss: 1.077681541442871, acc: 0.698924720287323)
[2025-02-05 14:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:46][root][INFO] - Training Epoch: 2/2, step 23333/23838 completed (loss: 1.0488367080688477, acc: 0.7131147384643555)
[2025-02-05 14:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:46][root][INFO] - Training Epoch: 2/2, step 23334/23838 completed (loss: 0.8461753726005554, acc: 0.75)
[2025-02-05 14:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:46][root][INFO] - Training Epoch: 2/2, step 23335/23838 completed (loss: 0.790275514125824, acc: 0.7623762488365173)
[2025-02-05 14:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:47][root][INFO] - Training Epoch: 2/2, step 23336/23838 completed (loss: 1.4067676067352295, acc: 0.5849056839942932)
[2025-02-05 14:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:47][root][INFO] - Training Epoch: 2/2, step 23337/23838 completed (loss: 1.458483099937439, acc: 0.5833333134651184)
[2025-02-05 14:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:47][root][INFO] - Training Epoch: 2/2, step 23338/23838 completed (loss: 1.3503055572509766, acc: 0.6052631735801697)
[2025-02-05 14:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:48][root][INFO] - Training Epoch: 2/2, step 23339/23838 completed (loss: 0.800254762172699, acc: 0.765625)
[2025-02-05 14:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:48][root][INFO] - Training Epoch: 2/2, step 23340/23838 completed (loss: 1.0361113548278809, acc: 0.7419354915618896)
[2025-02-05 14:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:48][root][INFO] - Training Epoch: 2/2, step 23341/23838 completed (loss: 1.021714687347412, acc: 0.6081081032752991)
[2025-02-05 14:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:49][root][INFO] - Training Epoch: 2/2, step 23342/23838 completed (loss: 1.4992740154266357, acc: 0.5918367505073547)
[2025-02-05 14:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:49][root][INFO] - Training Epoch: 2/2, step 23343/23838 completed (loss: 1.1460978984832764, acc: 0.6805555820465088)
[2025-02-05 14:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:50][root][INFO] - Training Epoch: 2/2, step 23344/23838 completed (loss: 1.0904604196548462, acc: 0.6344085931777954)
[2025-02-05 14:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:50][root][INFO] - Training Epoch: 2/2, step 23345/23838 completed (loss: 1.3248854875564575, acc: 0.6137930750846863)
[2025-02-05 14:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:50][root][INFO] - Training Epoch: 2/2, step 23346/23838 completed (loss: 1.2387207746505737, acc: 0.6299999952316284)
[2025-02-05 14:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:51][root][INFO] - Training Epoch: 2/2, step 23347/23838 completed (loss: 1.0994999408721924, acc: 0.7142857313156128)
[2025-02-05 14:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:51][root][INFO] - Training Epoch: 2/2, step 23348/23838 completed (loss: 1.073509693145752, acc: 0.6942148804664612)
[2025-02-05 14:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:51][root][INFO] - Training Epoch: 2/2, step 23349/23838 completed (loss: 1.3892407417297363, acc: 0.5887096524238586)
[2025-02-05 14:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:52][root][INFO] - Training Epoch: 2/2, step 23350/23838 completed (loss: 1.3687191009521484, acc: 0.6116504669189453)
[2025-02-05 14:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:52][root][INFO] - Training Epoch: 2/2, step 23351/23838 completed (loss: 0.9150124192237854, acc: 0.7142857313156128)
[2025-02-05 14:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:53][root][INFO] - Training Epoch: 2/2, step 23352/23838 completed (loss: 1.262884497642517, acc: 0.6222222447395325)
[2025-02-05 14:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:53][root][INFO] - Training Epoch: 2/2, step 23353/23838 completed (loss: 1.488966703414917, acc: 0.5762711763381958)
[2025-02-05 14:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:53][root][INFO] - Training Epoch: 2/2, step 23354/23838 completed (loss: 1.2057504653930664, acc: 0.7058823704719543)
[2025-02-05 14:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:54][root][INFO] - Training Epoch: 2/2, step 23355/23838 completed (loss: 0.9209203124046326, acc: 0.7058823704719543)
[2025-02-05 14:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:54][root][INFO] - Training Epoch: 2/2, step 23356/23838 completed (loss: 0.9208223819732666, acc: 0.7547169923782349)
[2025-02-05 14:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:55][root][INFO] - Training Epoch: 2/2, step 23357/23838 completed (loss: 1.2892900705337524, acc: 0.617977499961853)
[2025-02-05 14:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:55][root][INFO] - Training Epoch: 2/2, step 23358/23838 completed (loss: 1.1961833238601685, acc: 0.6315789222717285)
[2025-02-05 14:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:55][root][INFO] - Training Epoch: 2/2, step 23359/23838 completed (loss: 1.0156798362731934, acc: 0.701298713684082)
[2025-02-05 14:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:56][root][INFO] - Training Epoch: 2/2, step 23360/23838 completed (loss: 1.1831170320510864, acc: 0.6279069781303406)
[2025-02-05 14:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:56][root][INFO] - Training Epoch: 2/2, step 23361/23838 completed (loss: 1.1699398756027222, acc: 0.6519337296485901)
[2025-02-05 14:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:56][root][INFO] - Training Epoch: 2/2, step 23362/23838 completed (loss: 0.7473099231719971, acc: 0.7848101258277893)
[2025-02-05 14:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:57][root][INFO] - Training Epoch: 2/2, step 23363/23838 completed (loss: 1.2366089820861816, acc: 0.6614173054695129)
[2025-02-05 14:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:57][root][INFO] - Training Epoch: 2/2, step 23364/23838 completed (loss: 1.1051898002624512, acc: 0.6521739363670349)
[2025-02-05 14:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:58][root][INFO] - Training Epoch: 2/2, step 23365/23838 completed (loss: 1.2376694679260254, acc: 0.6491228342056274)
[2025-02-05 14:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:58][root][INFO] - Training Epoch: 2/2, step 23366/23838 completed (loss: 1.159852147102356, acc: 0.6388888955116272)
[2025-02-05 14:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:58][root][INFO] - Training Epoch: 2/2, step 23367/23838 completed (loss: 1.066828966140747, acc: 0.6357616186141968)
[2025-02-05 14:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:59][root][INFO] - Training Epoch: 2/2, step 23368/23838 completed (loss: 0.8942633271217346, acc: 0.7088607549667358)
[2025-02-05 14:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:59][root][INFO] - Training Epoch: 2/2, step 23369/23838 completed (loss: 1.0860087871551514, acc: 0.7053571343421936)
[2025-02-05 14:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:15:59][root][INFO] - Training Epoch: 2/2, step 23370/23838 completed (loss: 0.9293683767318726, acc: 0.7039473652839661)
[2025-02-05 14:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:00][root][INFO] - Training Epoch: 2/2, step 23371/23838 completed (loss: 1.2918357849121094, acc: 0.5625)
[2025-02-05 14:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:00][root][INFO] - Training Epoch: 2/2, step 23372/23838 completed (loss: 1.2158904075622559, acc: 0.6266666650772095)
[2025-02-05 14:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:01][root][INFO] - Training Epoch: 2/2, step 23373/23838 completed (loss: 0.819134533405304, acc: 0.7894737124443054)
[2025-02-05 14:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:01][root][INFO] - Training Epoch: 2/2, step 23374/23838 completed (loss: 0.9596660733222961, acc: 0.7183098793029785)
[2025-02-05 14:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:01][root][INFO] - Training Epoch: 2/2, step 23375/23838 completed (loss: 1.0023494958877563, acc: 0.7142857313156128)
[2025-02-05 14:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:02][root][INFO] - Training Epoch: 2/2, step 23376/23838 completed (loss: 1.1248464584350586, acc: 0.7099999785423279)
[2025-02-05 14:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:02][root][INFO] - Training Epoch: 2/2, step 23377/23838 completed (loss: 0.991328239440918, acc: 0.6880000233650208)
[2025-02-05 14:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:03][root][INFO] - Training Epoch: 2/2, step 23378/23838 completed (loss: 0.9120728373527527, acc: 0.75)
[2025-02-05 14:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:03][root][INFO] - Training Epoch: 2/2, step 23379/23838 completed (loss: 1.07393217086792, acc: 0.6875)
[2025-02-05 14:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:03][root][INFO] - Training Epoch: 2/2, step 23380/23838 completed (loss: 0.9948850274085999, acc: 0.6785714030265808)
[2025-02-05 14:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:04][root][INFO] - Training Epoch: 2/2, step 23381/23838 completed (loss: 0.8488307595252991, acc: 0.7559055089950562)
[2025-02-05 14:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:04][root][INFO] - Training Epoch: 2/2, step 23382/23838 completed (loss: 1.0554600954055786, acc: 0.6764705777168274)
[2025-02-05 14:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:05][root][INFO] - Training Epoch: 2/2, step 23383/23838 completed (loss: 0.8556358814239502, acc: 0.7191011309623718)
[2025-02-05 14:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:05][root][INFO] - Training Epoch: 2/2, step 23384/23838 completed (loss: 1.0963265895843506, acc: 0.6495726704597473)
[2025-02-05 14:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:05][root][INFO] - Training Epoch: 2/2, step 23385/23838 completed (loss: 1.0016628503799438, acc: 0.6868686676025391)
[2025-02-05 14:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:06][root][INFO] - Training Epoch: 2/2, step 23386/23838 completed (loss: 1.4512498378753662, acc: 0.5625)
[2025-02-05 14:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:06][root][INFO] - Training Epoch: 2/2, step 23387/23838 completed (loss: 0.933640718460083, acc: 0.6407766938209534)
[2025-02-05 14:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:06][root][INFO] - Training Epoch: 2/2, step 23388/23838 completed (loss: 1.0134766101837158, acc: 0.7168141603469849)
[2025-02-05 14:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:07][root][INFO] - Training Epoch: 2/2, step 23389/23838 completed (loss: 0.9351274371147156, acc: 0.737500011920929)
[2025-02-05 14:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:07][root][INFO] - Training Epoch: 2/2, step 23390/23838 completed (loss: 1.0351582765579224, acc: 0.7402597665786743)
[2025-02-05 14:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:07][root][INFO] - Training Epoch: 2/2, step 23391/23838 completed (loss: 0.6890231966972351, acc: 0.77173912525177)
[2025-02-05 14:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:08][root][INFO] - Training Epoch: 2/2, step 23392/23838 completed (loss: 1.0930559635162354, acc: 0.6625000238418579)
[2025-02-05 14:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:08][root][INFO] - Training Epoch: 2/2, step 23393/23838 completed (loss: 1.3096410036087036, acc: 0.5853658318519592)
[2025-02-05 14:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:09][root][INFO] - Training Epoch: 2/2, step 23394/23838 completed (loss: 1.44621741771698, acc: 0.5396825671195984)
[2025-02-05 14:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:09][root][INFO] - Training Epoch: 2/2, step 23395/23838 completed (loss: 0.8941972851753235, acc: 0.7421875)
[2025-02-05 14:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:09][root][INFO] - Training Epoch: 2/2, step 23396/23838 completed (loss: 1.2427864074707031, acc: 0.6153846383094788)
[2025-02-05 14:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:10][root][INFO] - Training Epoch: 2/2, step 23397/23838 completed (loss: 0.9439127445220947, acc: 0.7117117047309875)
[2025-02-05 14:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:10][root][INFO] - Training Epoch: 2/2, step 23398/23838 completed (loss: 1.066110610961914, acc: 0.6857143044471741)
[2025-02-05 14:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:11][root][INFO] - Training Epoch: 2/2, step 23399/23838 completed (loss: 0.9028664231300354, acc: 0.7428571581840515)
[2025-02-05 14:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:11][root][INFO] - Training Epoch: 2/2, step 23400/23838 completed (loss: 0.9253052473068237, acc: 0.7190082669258118)
[2025-02-05 14:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:11][root][INFO] - Training Epoch: 2/2, step 23401/23838 completed (loss: 0.9782311916351318, acc: 0.7118644118309021)
[2025-02-05 14:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:12][root][INFO] - Training Epoch: 2/2, step 23402/23838 completed (loss: 1.1670432090759277, acc: 0.6410256624221802)
[2025-02-05 14:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:12][root][INFO] - Training Epoch: 2/2, step 23403/23838 completed (loss: 0.7366520166397095, acc: 0.7411764860153198)
[2025-02-05 14:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:13][root][INFO] - Training Epoch: 2/2, step 23404/23838 completed (loss: 1.0328588485717773, acc: 0.6902654767036438)
[2025-02-05 14:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:13][root][INFO] - Training Epoch: 2/2, step 23405/23838 completed (loss: 0.7393586039543152, acc: 0.7540983557701111)
[2025-02-05 14:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:13][root][INFO] - Training Epoch: 2/2, step 23406/23838 completed (loss: 1.0817323923110962, acc: 0.6766917109489441)
[2025-02-05 14:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:14][root][INFO] - Training Epoch: 2/2, step 23407/23838 completed (loss: 1.2526845932006836, acc: 0.5975610017776489)
[2025-02-05 14:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:14][root][INFO] - Training Epoch: 2/2, step 23408/23838 completed (loss: 1.0302767753601074, acc: 0.7083333134651184)
[2025-02-05 14:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:14][root][INFO] - Training Epoch: 2/2, step 23409/23838 completed (loss: 1.27936589717865, acc: 0.5901639461517334)
[2025-02-05 14:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:15][root][INFO] - Training Epoch: 2/2, step 23410/23838 completed (loss: 0.9310029149055481, acc: 0.6853932738304138)
[2025-02-05 14:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:15][root][INFO] - Training Epoch: 2/2, step 23411/23838 completed (loss: 1.0037320852279663, acc: 0.6694915294647217)
[2025-02-05 14:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:15][root][INFO] - Training Epoch: 2/2, step 23412/23838 completed (loss: 1.0840495824813843, acc: 0.6904761791229248)
[2025-02-05 14:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:16][root][INFO] - Training Epoch: 2/2, step 23413/23838 completed (loss: 1.1003929376602173, acc: 0.7086614370346069)
[2025-02-05 14:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:16][root][INFO] - Training Epoch: 2/2, step 23414/23838 completed (loss: 1.0164791345596313, acc: 0.6699029207229614)
[2025-02-05 14:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:17][root][INFO] - Training Epoch: 2/2, step 23415/23838 completed (loss: 0.9512535929679871, acc: 0.699999988079071)
[2025-02-05 14:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:17][root][INFO] - Training Epoch: 2/2, step 23416/23838 completed (loss: 1.0482118129730225, acc: 0.6666666865348816)
[2025-02-05 14:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:17][root][INFO] - Training Epoch: 2/2, step 23417/23838 completed (loss: 1.0247324705123901, acc: 0.6666666865348816)
[2025-02-05 14:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:18][root][INFO] - Training Epoch: 2/2, step 23418/23838 completed (loss: 1.0864455699920654, acc: 0.6703296899795532)
[2025-02-05 14:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:18][root][INFO] - Training Epoch: 2/2, step 23419/23838 completed (loss: 1.241447925567627, acc: 0.6338028311729431)
[2025-02-05 14:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:18][root][INFO] - Training Epoch: 2/2, step 23420/23838 completed (loss: 0.896914005279541, acc: 0.720588207244873)
[2025-02-05 14:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:19][root][INFO] - Training Epoch: 2/2, step 23421/23838 completed (loss: 0.9417892694473267, acc: 0.7272727489471436)
[2025-02-05 14:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:19][root][INFO] - Training Epoch: 2/2, step 23422/23838 completed (loss: 1.1499536037445068, acc: 0.6333333253860474)
[2025-02-05 14:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:20][root][INFO] - Training Epoch: 2/2, step 23423/23838 completed (loss: 0.9272300004959106, acc: 0.7425742745399475)
[2025-02-05 14:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:20][root][INFO] - Training Epoch: 2/2, step 23424/23838 completed (loss: 0.8597134947776794, acc: 0.779411792755127)
[2025-02-05 14:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:20][root][INFO] - Training Epoch: 2/2, step 23425/23838 completed (loss: 0.9011254906654358, acc: 0.7219251394271851)
[2025-02-05 14:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:21][root][INFO] - Training Epoch: 2/2, step 23426/23838 completed (loss: 0.9687491059303284, acc: 0.7142857313156128)
[2025-02-05 14:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:21][root][INFO] - Training Epoch: 2/2, step 23427/23838 completed (loss: 0.8560659885406494, acc: 0.71875)
[2025-02-05 14:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:22][root][INFO] - Training Epoch: 2/2, step 23428/23838 completed (loss: 1.095529317855835, acc: 0.6569343209266663)
[2025-02-05 14:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:22][root][INFO] - Training Epoch: 2/2, step 23429/23838 completed (loss: 0.8188841938972473, acc: 0.7528089880943298)
[2025-02-05 14:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:22][root][INFO] - Training Epoch: 2/2, step 23430/23838 completed (loss: 1.080082893371582, acc: 0.6346153616905212)
[2025-02-05 14:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:23][root][INFO] - Training Epoch: 2/2, step 23431/23838 completed (loss: 1.1728883981704712, acc: 0.6756756901741028)
[2025-02-05 14:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:23][root][INFO] - Training Epoch: 2/2, step 23432/23838 completed (loss: 1.1347061395645142, acc: 0.6625000238418579)
[2025-02-05 14:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:23][root][INFO] - Training Epoch: 2/2, step 23433/23838 completed (loss: 0.990389883518219, acc: 0.6974790096282959)
[2025-02-05 14:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:24][root][INFO] - Training Epoch: 2/2, step 23434/23838 completed (loss: 1.2734166383743286, acc: 0.6058394312858582)
[2025-02-05 14:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:24][root][INFO] - Training Epoch: 2/2, step 23435/23838 completed (loss: 1.225043773651123, acc: 0.6700000166893005)
[2025-02-05 14:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:25][root][INFO] - Training Epoch: 2/2, step 23436/23838 completed (loss: 0.9593446850776672, acc: 0.72826087474823)
[2025-02-05 14:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:25][root][INFO] - Training Epoch: 2/2, step 23437/23838 completed (loss: 0.8558933138847351, acc: 0.7311828136444092)
[2025-02-05 14:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:25][root][INFO] - Training Epoch: 2/2, step 23438/23838 completed (loss: 0.9596518874168396, acc: 0.7068965435028076)
[2025-02-05 14:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:26][root][INFO] - Training Epoch: 2/2, step 23439/23838 completed (loss: 0.933098316192627, acc: 0.7070707082748413)
[2025-02-05 14:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:26][root][INFO] - Training Epoch: 2/2, step 23440/23838 completed (loss: 1.0191686153411865, acc: 0.6744186282157898)
[2025-02-05 14:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:26][root][INFO] - Training Epoch: 2/2, step 23441/23838 completed (loss: 0.8152281641960144, acc: 0.7615384459495544)
[2025-02-05 14:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:27][root][INFO] - Training Epoch: 2/2, step 23442/23838 completed (loss: 1.0547089576721191, acc: 0.7045454382896423)
[2025-02-05 14:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:27][root][INFO] - Training Epoch: 2/2, step 23443/23838 completed (loss: 0.7498642802238464, acc: 0.7739130258560181)
[2025-02-05 14:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:27][root][INFO] - Training Epoch: 2/2, step 23444/23838 completed (loss: 0.8282626867294312, acc: 0.7209302186965942)
[2025-02-05 14:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:28][root][INFO] - Training Epoch: 2/2, step 23445/23838 completed (loss: 1.07563054561615, acc: 0.6634615659713745)
[2025-02-05 14:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:28][root][INFO] - Training Epoch: 2/2, step 23446/23838 completed (loss: 1.1381028890609741, acc: 0.625)
[2025-02-05 14:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:29][root][INFO] - Training Epoch: 2/2, step 23447/23838 completed (loss: 1.2798982858657837, acc: 0.65625)
[2025-02-05 14:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:29][root][INFO] - Training Epoch: 2/2, step 23448/23838 completed (loss: 1.0232064723968506, acc: 0.6707317233085632)
[2025-02-05 14:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:30][root][INFO] - Training Epoch: 2/2, step 23449/23838 completed (loss: 1.1371684074401855, acc: 0.6238532066345215)
[2025-02-05 14:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:30][root][INFO] - Training Epoch: 2/2, step 23450/23838 completed (loss: 1.1094633340835571, acc: 0.6380952596664429)
[2025-02-05 14:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:30][root][INFO] - Training Epoch: 2/2, step 23451/23838 completed (loss: 0.9189046025276184, acc: 0.725806474685669)
[2025-02-05 14:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:31][root][INFO] - Training Epoch: 2/2, step 23452/23838 completed (loss: 1.0658769607543945, acc: 0.6764705777168274)
[2025-02-05 14:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:31][root][INFO] - Training Epoch: 2/2, step 23453/23838 completed (loss: 1.1428428888320923, acc: 0.6380952596664429)
[2025-02-05 14:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:31][root][INFO] - Training Epoch: 2/2, step 23454/23838 completed (loss: 1.3213719129562378, acc: 0.5862069129943848)
[2025-02-05 14:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:32][root][INFO] - Training Epoch: 2/2, step 23455/23838 completed (loss: 0.9747816324234009, acc: 0.6705882549285889)
[2025-02-05 14:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:32][root][INFO] - Training Epoch: 2/2, step 23456/23838 completed (loss: 1.2384759187698364, acc: 0.6213592290878296)
[2025-02-05 14:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:33][root][INFO] - Training Epoch: 2/2, step 23457/23838 completed (loss: 0.6698207855224609, acc: 0.8095238208770752)
[2025-02-05 14:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:33][root][INFO] - Training Epoch: 2/2, step 23458/23838 completed (loss: 1.6505937576293945, acc: 0.5522388219833374)
[2025-02-05 14:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:33][root][INFO] - Training Epoch: 2/2, step 23459/23838 completed (loss: 1.0999525785446167, acc: 0.7586206793785095)
[2025-02-05 14:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:34][root][INFO] - Training Epoch: 2/2, step 23460/23838 completed (loss: 1.1963740587234497, acc: 0.6766917109489441)
[2025-02-05 14:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:34][root][INFO] - Training Epoch: 2/2, step 23461/23838 completed (loss: 1.1365786790847778, acc: 0.6611570119857788)
[2025-02-05 14:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:34][root][INFO] - Training Epoch: 2/2, step 23462/23838 completed (loss: 1.1748474836349487, acc: 0.6620689630508423)
[2025-02-05 14:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:35][root][INFO] - Training Epoch: 2/2, step 23463/23838 completed (loss: 0.9586890935897827, acc: 0.7272727489471436)
[2025-02-05 14:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:35][root][INFO] - Training Epoch: 2/2, step 23464/23838 completed (loss: 1.1273974180221558, acc: 0.6265060305595398)
[2025-02-05 14:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:36][root][INFO] - Training Epoch: 2/2, step 23465/23838 completed (loss: 0.8974246382713318, acc: 0.7333333492279053)
[2025-02-05 14:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:36][root][INFO] - Training Epoch: 2/2, step 23466/23838 completed (loss: 0.8253764510154724, acc: 0.7916666865348816)
[2025-02-05 14:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:36][root][INFO] - Training Epoch: 2/2, step 23467/23838 completed (loss: 1.2669289112091064, acc: 0.5824176073074341)
[2025-02-05 14:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:37][root][INFO] - Training Epoch: 2/2, step 23468/23838 completed (loss: 1.321626901626587, acc: 0.6388888955116272)
[2025-02-05 14:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:37][root][INFO] - Training Epoch: 2/2, step 23469/23838 completed (loss: 1.073441505432129, acc: 0.7058823704719543)
[2025-02-05 14:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:38][root][INFO] - Training Epoch: 2/2, step 23470/23838 completed (loss: 1.0144238471984863, acc: 0.7118644118309021)
[2025-02-05 14:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:38][root][INFO] - Training Epoch: 2/2, step 23471/23838 completed (loss: 0.9131697416305542, acc: 0.6615384817123413)
[2025-02-05 14:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:38][root][INFO] - Training Epoch: 2/2, step 23472/23838 completed (loss: 1.0547800064086914, acc: 0.699999988079071)
[2025-02-05 14:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:39][root][INFO] - Training Epoch: 2/2, step 23473/23838 completed (loss: 0.9893654584884644, acc: 0.662162184715271)
[2025-02-05 14:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:39][root][INFO] - Training Epoch: 2/2, step 23474/23838 completed (loss: 0.9412026405334473, acc: 0.6857143044471741)
[2025-02-05 14:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:39][root][INFO] - Training Epoch: 2/2, step 23475/23838 completed (loss: 1.252180814743042, acc: 0.606249988079071)
[2025-02-05 14:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:40][root][INFO] - Training Epoch: 2/2, step 23476/23838 completed (loss: 1.0608470439910889, acc: 0.7142857313156128)
[2025-02-05 14:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:40][root][INFO] - Training Epoch: 2/2, step 23477/23838 completed (loss: 1.109795093536377, acc: 0.6578947305679321)
[2025-02-05 14:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:41][root][INFO] - Training Epoch: 2/2, step 23478/23838 completed (loss: 1.084234595298767, acc: 0.6582278609275818)
[2025-02-05 14:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:41][root][INFO] - Training Epoch: 2/2, step 23479/23838 completed (loss: 0.8986149430274963, acc: 0.7666666507720947)
[2025-02-05 14:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:41][root][INFO] - Training Epoch: 2/2, step 23480/23838 completed (loss: 1.270784616470337, acc: 0.6299999952316284)
[2025-02-05 14:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:42][root][INFO] - Training Epoch: 2/2, step 23481/23838 completed (loss: 1.201526165008545, acc: 0.6574074029922485)
[2025-02-05 14:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:42][root][INFO] - Training Epoch: 2/2, step 23482/23838 completed (loss: 0.8584376573562622, acc: 0.7777777910232544)
[2025-02-05 14:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:43][root][INFO] - Training Epoch: 2/2, step 23483/23838 completed (loss: 1.1134867668151855, acc: 0.6952381134033203)
[2025-02-05 14:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:43][root][INFO] - Training Epoch: 2/2, step 23484/23838 completed (loss: 1.145668387413025, acc: 0.6762589812278748)
[2025-02-05 14:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:43][root][INFO] - Training Epoch: 2/2, step 23485/23838 completed (loss: 1.0581380128860474, acc: 0.6917808055877686)
[2025-02-05 14:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:44][root][INFO] - Training Epoch: 2/2, step 23486/23838 completed (loss: 0.939175009727478, acc: 0.7019867300987244)
[2025-02-05 14:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:44][root][INFO] - Training Epoch: 2/2, step 23487/23838 completed (loss: 1.1284828186035156, acc: 0.7050359845161438)
[2025-02-05 14:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:44][root][INFO] - Training Epoch: 2/2, step 23488/23838 completed (loss: 0.9389249086380005, acc: 0.7402597665786743)
[2025-02-05 14:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:45][root][INFO] - Training Epoch: 2/2, step 23489/23838 completed (loss: 1.04654860496521, acc: 0.6666666865348816)
[2025-02-05 14:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:45][root][INFO] - Training Epoch: 2/2, step 23490/23838 completed (loss: 1.0590883493423462, acc: 0.7142857313156128)
[2025-02-05 14:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:46][root][INFO] - Training Epoch: 2/2, step 23491/23838 completed (loss: 1.3376373052597046, acc: 0.6707317233085632)
[2025-02-05 14:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:46][root][INFO] - Training Epoch: 2/2, step 23492/23838 completed (loss: 1.081421971321106, acc: 0.692307710647583)
[2025-02-05 14:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:46][root][INFO] - Training Epoch: 2/2, step 23493/23838 completed (loss: 0.6391006708145142, acc: 0.8222222328186035)
[2025-02-05 14:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:47][root][INFO] - Training Epoch: 2/2, step 23494/23838 completed (loss: 0.9830445647239685, acc: 0.6962025165557861)
[2025-02-05 14:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:47][root][INFO] - Training Epoch: 2/2, step 23495/23838 completed (loss: 0.8998838067054749, acc: 0.75)
[2025-02-05 14:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:47][root][INFO] - Training Epoch: 2/2, step 23496/23838 completed (loss: 0.8712524771690369, acc: 0.7011494040489197)
[2025-02-05 14:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:48][root][INFO] - Training Epoch: 2/2, step 23497/23838 completed (loss: 1.022109031677246, acc: 0.6614173054695129)
[2025-02-05 14:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:48][root][INFO] - Training Epoch: 2/2, step 23498/23838 completed (loss: 0.8093262910842896, acc: 0.746268630027771)
[2025-02-05 14:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:49][root][INFO] - Training Epoch: 2/2, step 23499/23838 completed (loss: 1.0905147790908813, acc: 0.6962025165557861)
[2025-02-05 14:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:49][root][INFO] - Training Epoch: 2/2, step 23500/23838 completed (loss: 1.0526559352874756, acc: 0.6935483813285828)
[2025-02-05 14:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:50][root][INFO] - Training Epoch: 2/2, step 23501/23838 completed (loss: 1.1718846559524536, acc: 0.6470588445663452)
[2025-02-05 14:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:50][root][INFO] - Training Epoch: 2/2, step 23502/23838 completed (loss: 1.1326323747634888, acc: 0.6226415038108826)
[2025-02-05 14:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:50][root][INFO] - Training Epoch: 2/2, step 23503/23838 completed (loss: 1.002461314201355, acc: 0.6379310488700867)
[2025-02-05 14:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:51][root][INFO] - Training Epoch: 2/2, step 23504/23838 completed (loss: 1.3021368980407715, acc: 0.6428571343421936)
[2025-02-05 14:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:51][root][INFO] - Training Epoch: 2/2, step 23505/23838 completed (loss: 1.2577683925628662, acc: 0.6133333444595337)
[2025-02-05 14:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:52][root][INFO] - Training Epoch: 2/2, step 23506/23838 completed (loss: 0.9918839931488037, acc: 0.75)
[2025-02-05 14:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:52][root][INFO] - Training Epoch: 2/2, step 23507/23838 completed (loss: 0.7945144176483154, acc: 0.7438016533851624)
[2025-02-05 14:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:52][root][INFO] - Training Epoch: 2/2, step 23508/23838 completed (loss: 0.9626434445381165, acc: 0.675000011920929)
[2025-02-05 14:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:53][root][INFO] - Training Epoch: 2/2, step 23509/23838 completed (loss: 0.9993907809257507, acc: 0.6575342416763306)
[2025-02-05 14:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:53][root][INFO] - Training Epoch: 2/2, step 23510/23838 completed (loss: 1.1094436645507812, acc: 0.7017543911933899)
[2025-02-05 14:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:53][root][INFO] - Training Epoch: 2/2, step 23511/23838 completed (loss: 0.8600281476974487, acc: 0.7142857313156128)
[2025-02-05 14:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:54][root][INFO] - Training Epoch: 2/2, step 23512/23838 completed (loss: 0.9400461912155151, acc: 0.7286821603775024)
[2025-02-05 14:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:54][root][INFO] - Training Epoch: 2/2, step 23513/23838 completed (loss: 1.0832457542419434, acc: 0.6951219439506531)
[2025-02-05 14:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:54][root][INFO] - Training Epoch: 2/2, step 23514/23838 completed (loss: 1.1963269710540771, acc: 0.7164179086685181)
[2025-02-05 14:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:55][root][INFO] - Training Epoch: 2/2, step 23515/23838 completed (loss: 1.0051565170288086, acc: 0.7066666483879089)
[2025-02-05 14:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:55][root][INFO] - Training Epoch: 2/2, step 23516/23838 completed (loss: 1.068909764289856, acc: 0.7075471878051758)
[2025-02-05 14:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:56][root][INFO] - Training Epoch: 2/2, step 23517/23838 completed (loss: 1.201672911643982, acc: 0.6349206566810608)
[2025-02-05 14:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:56][root][INFO] - Training Epoch: 2/2, step 23518/23838 completed (loss: 1.2079920768737793, acc: 0.6666666865348816)
[2025-02-05 14:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:56][root][INFO] - Training Epoch: 2/2, step 23519/23838 completed (loss: 1.3605380058288574, acc: 0.5617977380752563)
[2025-02-05 14:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:57][root][INFO] - Training Epoch: 2/2, step 23520/23838 completed (loss: 1.4881500005722046, acc: 0.5882353186607361)
[2025-02-05 14:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:57][root][INFO] - Training Epoch: 2/2, step 23521/23838 completed (loss: 1.3809648752212524, acc: 0.5357142686843872)
[2025-02-05 14:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:58][root][INFO] - Training Epoch: 2/2, step 23522/23838 completed (loss: 1.3126475811004639, acc: 0.625)
[2025-02-05 14:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:58][root][INFO] - Training Epoch: 2/2, step 23523/23838 completed (loss: 1.3944263458251953, acc: 0.641791045665741)
[2025-02-05 14:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:58][root][INFO] - Training Epoch: 2/2, step 23524/23838 completed (loss: 1.3441754579544067, acc: 0.5394737124443054)
[2025-02-05 14:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:59][root][INFO] - Training Epoch: 2/2, step 23525/23838 completed (loss: 1.4303302764892578, acc: 0.6106194853782654)
[2025-02-05 14:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:59][root][INFO] - Training Epoch: 2/2, step 23526/23838 completed (loss: 1.0514159202575684, acc: 0.6785714030265808)
[2025-02-05 14:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:16:59][root][INFO] - Training Epoch: 2/2, step 23527/23838 completed (loss: 1.3782641887664795, acc: 0.6000000238418579)
[2025-02-05 14:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:00][root][INFO] - Training Epoch: 2/2, step 23528/23838 completed (loss: 1.2130742073059082, acc: 0.6209677457809448)
[2025-02-05 14:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:00][root][INFO] - Training Epoch: 2/2, step 23529/23838 completed (loss: 0.9490509033203125, acc: 0.7029703259468079)
[2025-02-05 14:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:01][root][INFO] - Training Epoch: 2/2, step 23530/23838 completed (loss: 0.7450356483459473, acc: 0.7951807379722595)
[2025-02-05 14:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:01][root][INFO] - Training Epoch: 2/2, step 23531/23838 completed (loss: 1.1507209539413452, acc: 0.6184210777282715)
[2025-02-05 14:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:01][root][INFO] - Training Epoch: 2/2, step 23532/23838 completed (loss: 1.067506194114685, acc: 0.6666666865348816)
[2025-02-05 14:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:02][root][INFO] - Training Epoch: 2/2, step 23533/23838 completed (loss: 1.1190868616104126, acc: 0.6206896305084229)
[2025-02-05 14:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:02][root][INFO] - Training Epoch: 2/2, step 23534/23838 completed (loss: 1.137209177017212, acc: 0.6666666865348816)
[2025-02-05 14:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:02][root][INFO] - Training Epoch: 2/2, step 23535/23838 completed (loss: 1.0354481935501099, acc: 0.675000011920929)
[2025-02-05 14:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:03][root][INFO] - Training Epoch: 2/2, step 23536/23838 completed (loss: 1.0016096830368042, acc: 0.6860465407371521)
[2025-02-05 14:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:03][root][INFO] - Training Epoch: 2/2, step 23537/23838 completed (loss: 0.9147997498512268, acc: 0.738095223903656)
[2025-02-05 14:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:03][root][INFO] - Training Epoch: 2/2, step 23538/23838 completed (loss: 0.7515813708305359, acc: 0.7857142686843872)
[2025-02-05 14:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:04][root][INFO] - Training Epoch: 2/2, step 23539/23838 completed (loss: 0.5814263820648193, acc: 0.787401556968689)
[2025-02-05 14:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:04][root][INFO] - Training Epoch: 2/2, step 23540/23838 completed (loss: 1.1423583030700684, acc: 0.6708860993385315)
[2025-02-05 14:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:05][root][INFO] - Training Epoch: 2/2, step 23541/23838 completed (loss: 0.8559008240699768, acc: 0.6791045069694519)
[2025-02-05 14:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:05][root][INFO] - Training Epoch: 2/2, step 23542/23838 completed (loss: 0.7270863652229309, acc: 0.8095238208770752)
[2025-02-05 14:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:05][root][INFO] - Training Epoch: 2/2, step 23543/23838 completed (loss: 1.1594830751419067, acc: 0.6585366129875183)
[2025-02-05 14:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:06][root][INFO] - Training Epoch: 2/2, step 23544/23838 completed (loss: 0.9739272594451904, acc: 0.6216216087341309)
[2025-02-05 14:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:06][root][INFO] - Training Epoch: 2/2, step 23545/23838 completed (loss: 1.3015435934066772, acc: 0.5757575631141663)
[2025-02-05 14:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:06][root][INFO] - Training Epoch: 2/2, step 23546/23838 completed (loss: 1.2296525239944458, acc: 0.6111111044883728)
[2025-02-05 14:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:07][root][INFO] - Training Epoch: 2/2, step 23547/23838 completed (loss: 0.9231892824172974, acc: 0.6785714030265808)
[2025-02-05 14:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:07][root][INFO] - Training Epoch: 2/2, step 23548/23838 completed (loss: 0.908078134059906, acc: 0.7289719581604004)
[2025-02-05 14:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:07][root][INFO] - Training Epoch: 2/2, step 23549/23838 completed (loss: 0.9666379690170288, acc: 0.7575757503509521)
[2025-02-05 14:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:08][root][INFO] - Training Epoch: 2/2, step 23550/23838 completed (loss: 0.7755356431007385, acc: 0.8142856955528259)
[2025-02-05 14:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:08][root][INFO] - Training Epoch: 2/2, step 23551/23838 completed (loss: 1.0505272150039673, acc: 0.6393442749977112)
[2025-02-05 14:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:09][root][INFO] - Training Epoch: 2/2, step 23552/23838 completed (loss: 1.1918742656707764, acc: 0.625)
[2025-02-05 14:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:09][root][INFO] - Training Epoch: 2/2, step 23553/23838 completed (loss: 1.0377192497253418, acc: 0.7055214643478394)
[2025-02-05 14:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:09][root][INFO] - Training Epoch: 2/2, step 23554/23838 completed (loss: 1.1046032905578613, acc: 0.6865671873092651)
[2025-02-05 14:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:10][root][INFO] - Training Epoch: 2/2, step 23555/23838 completed (loss: 1.2446976900100708, acc: 0.6491228342056274)
[2025-02-05 14:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:10][root][INFO] - Training Epoch: 2/2, step 23556/23838 completed (loss: 1.1342966556549072, acc: 0.6615384817123413)
[2025-02-05 14:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:10][root][INFO] - Training Epoch: 2/2, step 23557/23838 completed (loss: 0.903052568435669, acc: 0.7333333492279053)
[2025-02-05 14:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:11][root][INFO] - Training Epoch: 2/2, step 23558/23838 completed (loss: 1.1401121616363525, acc: 0.6517857313156128)
[2025-02-05 14:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:11][root][INFO] - Training Epoch: 2/2, step 23559/23838 completed (loss: 1.14182710647583, acc: 0.6470588445663452)
[2025-02-05 14:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:12][root][INFO] - Training Epoch: 2/2, step 23560/23838 completed (loss: 0.9492106437683105, acc: 0.7586206793785095)
[2025-02-05 14:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:12][root][INFO] - Training Epoch: 2/2, step 23561/23838 completed (loss: 1.0361241102218628, acc: 0.6666666865348816)
[2025-02-05 14:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:12][root][INFO] - Training Epoch: 2/2, step 23562/23838 completed (loss: 0.9700459837913513, acc: 0.7333333492279053)
[2025-02-05 14:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:13][root][INFO] - Training Epoch: 2/2, step 23563/23838 completed (loss: 0.7711137533187866, acc: 0.7297297120094299)
[2025-02-05 14:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:13][root][INFO] - Training Epoch: 2/2, step 23564/23838 completed (loss: 0.8969951868057251, acc: 0.762499988079071)
[2025-02-05 14:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:13][root][INFO] - Training Epoch: 2/2, step 23565/23838 completed (loss: 0.73554927110672, acc: 0.7241379022598267)
[2025-02-05 14:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:14][root][INFO] - Training Epoch: 2/2, step 23566/23838 completed (loss: 0.9618997573852539, acc: 0.6942148804664612)
[2025-02-05 14:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:14][root][INFO] - Training Epoch: 2/2, step 23567/23838 completed (loss: 1.0790437459945679, acc: 0.7142857313156128)
[2025-02-05 14:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:15][root][INFO] - Training Epoch: 2/2, step 23568/23838 completed (loss: 1.008212924003601, acc: 0.6530612111091614)
[2025-02-05 14:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:15][root][INFO] - Training Epoch: 2/2, step 23569/23838 completed (loss: 0.883844792842865, acc: 0.7244094610214233)
[2025-02-05 14:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:15][root][INFO] - Training Epoch: 2/2, step 23570/23838 completed (loss: 1.3028861284255981, acc: 0.6086956262588501)
[2025-02-05 14:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:16][root][INFO] - Training Epoch: 2/2, step 23571/23838 completed (loss: 1.5449104309082031, acc: 0.591549277305603)
[2025-02-05 14:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:16][root][INFO] - Training Epoch: 2/2, step 23572/23838 completed (loss: 1.0891348123550415, acc: 0.692307710647583)
[2025-02-05 14:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:17][root][INFO] - Training Epoch: 2/2, step 23573/23838 completed (loss: 1.097087025642395, acc: 0.6643356680870056)
[2025-02-05 14:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:17][root][INFO] - Training Epoch: 2/2, step 23574/23838 completed (loss: 1.2953091859817505, acc: 0.6220472455024719)
[2025-02-05 14:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:17][root][INFO] - Training Epoch: 2/2, step 23575/23838 completed (loss: 0.7452312111854553, acc: 0.7448979616165161)
[2025-02-05 14:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:18][root][INFO] - Training Epoch: 2/2, step 23576/23838 completed (loss: 1.264992356300354, acc: 0.6451612710952759)
[2025-02-05 14:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:18][root][INFO] - Training Epoch: 2/2, step 23577/23838 completed (loss: 0.6683728098869324, acc: 0.7857142686843872)
[2025-02-05 14:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:19][root][INFO] - Training Epoch: 2/2, step 23578/23838 completed (loss: 0.9690783619880676, acc: 0.7291666865348816)
[2025-02-05 14:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:19][root][INFO] - Training Epoch: 2/2, step 23579/23838 completed (loss: 0.9677096009254456, acc: 0.7167630195617676)
[2025-02-05 14:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:19][root][INFO] - Training Epoch: 2/2, step 23580/23838 completed (loss: 1.0789152383804321, acc: 0.7285714149475098)
[2025-02-05 14:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:20][root][INFO] - Training Epoch: 2/2, step 23581/23838 completed (loss: 0.8293471336364746, acc: 0.7916666865348816)
[2025-02-05 14:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:20][root][INFO] - Training Epoch: 2/2, step 23582/23838 completed (loss: 1.3154140710830688, acc: 0.5789473652839661)
[2025-02-05 14:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:20][root][INFO] - Training Epoch: 2/2, step 23583/23838 completed (loss: 1.4686856269836426, acc: 0.6226415038108826)
[2025-02-05 14:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:21][root][INFO] - Training Epoch: 2/2, step 23584/23838 completed (loss: 0.928033709526062, acc: 0.7846153974533081)
[2025-02-05 14:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:21][root][INFO] - Training Epoch: 2/2, step 23585/23838 completed (loss: 0.8531665802001953, acc: 0.773809552192688)
[2025-02-05 14:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:21][root][INFO] - Training Epoch: 2/2, step 23586/23838 completed (loss: 0.8407634496688843, acc: 0.8088235259056091)
[2025-02-05 14:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:22][root][INFO] - Training Epoch: 2/2, step 23587/23838 completed (loss: 1.4469417333602905, acc: 0.6000000238418579)
[2025-02-05 14:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:22][root][INFO] - Training Epoch: 2/2, step 23588/23838 completed (loss: 0.8621208071708679, acc: 0.70652174949646)
[2025-02-05 14:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:22][root][INFO] - Training Epoch: 2/2, step 23589/23838 completed (loss: 0.8537334203720093, acc: 0.7777777910232544)
[2025-02-05 14:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:23][root][INFO] - Training Epoch: 2/2, step 23590/23838 completed (loss: 1.260979413986206, acc: 0.6000000238418579)
[2025-02-05 14:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:23][root][INFO] - Training Epoch: 2/2, step 23591/23838 completed (loss: 1.1922500133514404, acc: 0.6578947305679321)
[2025-02-05 14:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:24][root][INFO] - Training Epoch: 2/2, step 23592/23838 completed (loss: 1.0779550075531006, acc: 0.6979166865348816)
[2025-02-05 14:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:24][root][INFO] - Training Epoch: 2/2, step 23593/23838 completed (loss: 0.7586801648139954, acc: 0.7599999904632568)
[2025-02-05 14:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:24][root][INFO] - Training Epoch: 2/2, step 23594/23838 completed (loss: 0.9535350799560547, acc: 0.7204301357269287)
[2025-02-05 14:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:25][root][INFO] - Training Epoch: 2/2, step 23595/23838 completed (loss: 0.8421609401702881, acc: 0.6851851940155029)
[2025-02-05 14:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:25][root][INFO] - Training Epoch: 2/2, step 23596/23838 completed (loss: 1.0660521984100342, acc: 0.7118644118309021)
[2025-02-05 14:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:26][root][INFO] - Training Epoch: 2/2, step 23597/23838 completed (loss: 1.352934718132019, acc: 0.5636363625526428)
[2025-02-05 14:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:26][root][INFO] - Training Epoch: 2/2, step 23598/23838 completed (loss: 1.1388905048370361, acc: 0.6304348111152649)
[2025-02-05 14:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:26][root][INFO] - Training Epoch: 2/2, step 23599/23838 completed (loss: 0.9455994367599487, acc: 0.7011494040489197)
[2025-02-05 14:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:27][root][INFO] - Training Epoch: 2/2, step 23600/23838 completed (loss: 0.8990583419799805, acc: 0.7439024448394775)
[2025-02-05 14:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:27][root][INFO] - Training Epoch: 2/2, step 23601/23838 completed (loss: 0.9135680794715881, acc: 0.7870370149612427)
[2025-02-05 14:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:28][root][INFO] - Training Epoch: 2/2, step 23602/23838 completed (loss: 0.9753840565681458, acc: 0.7113401889801025)
[2025-02-05 14:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:28][root][INFO] - Training Epoch: 2/2, step 23603/23838 completed (loss: 1.2094669342041016, acc: 0.6385542154312134)
[2025-02-05 14:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:28][root][INFO] - Training Epoch: 2/2, step 23604/23838 completed (loss: 0.6839228868484497, acc: 0.7638888955116272)
[2025-02-05 14:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:29][root][INFO] - Training Epoch: 2/2, step 23605/23838 completed (loss: 1.2868164777755737, acc: 0.5666666626930237)
[2025-02-05 14:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:29][root][INFO] - Training Epoch: 2/2, step 23606/23838 completed (loss: 1.0337872505187988, acc: 0.7113401889801025)
[2025-02-05 14:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:30][root][INFO] - Training Epoch: 2/2, step 23607/23838 completed (loss: 1.0847315788269043, acc: 0.7099999785423279)
[2025-02-05 14:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:30][root][INFO] - Training Epoch: 2/2, step 23608/23838 completed (loss: 0.8817386031150818, acc: 0.7407407164573669)
[2025-02-05 14:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:30][root][INFO] - Training Epoch: 2/2, step 23609/23838 completed (loss: 0.8338770270347595, acc: 0.7731958627700806)
[2025-02-05 14:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:31][root][INFO] - Training Epoch: 2/2, step 23610/23838 completed (loss: 0.904945969581604, acc: 0.7179487347602844)
[2025-02-05 14:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:31][root][INFO] - Training Epoch: 2/2, step 23611/23838 completed (loss: 1.1735121011734009, acc: 0.6702127456665039)
[2025-02-05 14:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:31][root][INFO] - Training Epoch: 2/2, step 23612/23838 completed (loss: 1.3169809579849243, acc: 0.6285714507102966)
[2025-02-05 14:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:32][root][INFO] - Training Epoch: 2/2, step 23613/23838 completed (loss: 1.0303276777267456, acc: 0.6853932738304138)
[2025-02-05 14:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:32][root][INFO] - Training Epoch: 2/2, step 23614/23838 completed (loss: 0.716864824295044, acc: 0.779411792755127)
[2025-02-05 14:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:33][root][INFO] - Training Epoch: 2/2, step 23615/23838 completed (loss: 0.8918696045875549, acc: 0.72826087474823)
[2025-02-05 14:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:33][root][INFO] - Training Epoch: 2/2, step 23616/23838 completed (loss: 0.5756604671478271, acc: 0.7938144207000732)
[2025-02-05 14:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:34][root][INFO] - Training Epoch: 2/2, step 23617/23838 completed (loss: 1.2099862098693848, acc: 0.6352941393852234)
[2025-02-05 14:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:34][root][INFO] - Training Epoch: 2/2, step 23618/23838 completed (loss: 1.1623153686523438, acc: 0.6477272510528564)
[2025-02-05 14:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:34][root][INFO] - Training Epoch: 2/2, step 23619/23838 completed (loss: 0.8472438454627991, acc: 0.7586206793785095)
[2025-02-05 14:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:35][root][INFO] - Training Epoch: 2/2, step 23620/23838 completed (loss: 1.222090482711792, acc: 0.6212121248245239)
[2025-02-05 14:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:35][root][INFO] - Training Epoch: 2/2, step 23621/23838 completed (loss: 1.1038908958435059, acc: 0.6805555820465088)
[2025-02-05 14:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:35][root][INFO] - Training Epoch: 2/2, step 23622/23838 completed (loss: 0.9455031156539917, acc: 0.75)
[2025-02-05 14:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:36][root][INFO] - Training Epoch: 2/2, step 23623/23838 completed (loss: 1.0391360521316528, acc: 0.7215189933776855)
[2025-02-05 14:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:36][root][INFO] - Training Epoch: 2/2, step 23624/23838 completed (loss: 0.8874785304069519, acc: 0.7179487347602844)
[2025-02-05 14:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:37][root][INFO] - Training Epoch: 2/2, step 23625/23838 completed (loss: 0.9351540207862854, acc: 0.7232142686843872)
[2025-02-05 14:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:37][root][INFO] - Training Epoch: 2/2, step 23626/23838 completed (loss: 1.3163950443267822, acc: 0.6373626589775085)
[2025-02-05 14:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:37][root][INFO] - Training Epoch: 2/2, step 23627/23838 completed (loss: 0.9172435998916626, acc: 0.7519999742507935)
[2025-02-05 14:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:38][root][INFO] - Training Epoch: 2/2, step 23628/23838 completed (loss: 0.8679128289222717, acc: 0.7066666483879089)
[2025-02-05 14:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:38][root][INFO] - Training Epoch: 2/2, step 23629/23838 completed (loss: 0.6564142107963562, acc: 0.8148148059844971)
[2025-02-05 14:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:38][root][INFO] - Training Epoch: 2/2, step 23630/23838 completed (loss: 0.6914176940917969, acc: 0.7763158082962036)
[2025-02-05 14:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:39][root][INFO] - Training Epoch: 2/2, step 23631/23838 completed (loss: 1.0273221731185913, acc: 0.7323943376541138)
[2025-02-05 14:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:39][root][INFO] - Training Epoch: 2/2, step 23632/23838 completed (loss: 1.1378209590911865, acc: 0.6463414430618286)
[2025-02-05 14:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:40][root][INFO] - Training Epoch: 2/2, step 23633/23838 completed (loss: 1.0984439849853516, acc: 0.6578947305679321)
[2025-02-05 14:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:40][root][INFO] - Training Epoch: 2/2, step 23634/23838 completed (loss: 0.9093250632286072, acc: 0.6849315166473389)
[2025-02-05 14:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:40][root][INFO] - Training Epoch: 2/2, step 23635/23838 completed (loss: 1.11613929271698, acc: 0.695652186870575)
[2025-02-05 14:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:41][root][INFO] - Training Epoch: 2/2, step 23636/23838 completed (loss: 1.0440882444381714, acc: 0.7272727489471436)
[2025-02-05 14:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:41][root][INFO] - Training Epoch: 2/2, step 23637/23838 completed (loss: 1.296775460243225, acc: 0.6666666865348816)
[2025-02-05 14:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:42][root][INFO] - Training Epoch: 2/2, step 23638/23838 completed (loss: 1.0363274812698364, acc: 0.6966292262077332)
[2025-02-05 14:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:42][root][INFO] - Training Epoch: 2/2, step 23639/23838 completed (loss: 0.8025552034378052, acc: 0.7789473533630371)
[2025-02-05 14:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:42][root][INFO] - Training Epoch: 2/2, step 23640/23838 completed (loss: 1.1610339879989624, acc: 0.7076923251152039)
[2025-02-05 14:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:43][root][INFO] - Training Epoch: 2/2, step 23641/23838 completed (loss: 0.9037988185882568, acc: 0.7200000286102295)
[2025-02-05 14:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:43][root][INFO] - Training Epoch: 2/2, step 23642/23838 completed (loss: 1.1542868614196777, acc: 0.6363636255264282)
[2025-02-05 14:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:44][root][INFO] - Training Epoch: 2/2, step 23643/23838 completed (loss: 0.879703164100647, acc: 0.7083333134651184)
[2025-02-05 14:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:44][root][INFO] - Training Epoch: 2/2, step 23644/23838 completed (loss: 1.4668715000152588, acc: 0.5972222089767456)
[2025-02-05 14:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:44][root][INFO] - Training Epoch: 2/2, step 23645/23838 completed (loss: 0.9403690695762634, acc: 0.7666666507720947)
[2025-02-05 14:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:45][root][INFO] - Training Epoch: 2/2, step 23646/23838 completed (loss: 1.1598740816116333, acc: 0.6458333134651184)
[2025-02-05 14:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:45][root][INFO] - Training Epoch: 2/2, step 23647/23838 completed (loss: 1.0994863510131836, acc: 0.6506849527359009)
[2025-02-05 14:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:45][root][INFO] - Training Epoch: 2/2, step 23648/23838 completed (loss: 1.1768383979797363, acc: 0.6358381509780884)
[2025-02-05 14:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:46][root][INFO] - Training Epoch: 2/2, step 23649/23838 completed (loss: 1.1141213178634644, acc: 0.6000000238418579)
[2025-02-05 14:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:46][root][INFO] - Training Epoch: 2/2, step 23650/23838 completed (loss: 1.1088297367095947, acc: 0.65625)
[2025-02-05 14:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:46][root][INFO] - Training Epoch: 2/2, step 23651/23838 completed (loss: 0.8105077147483826, acc: 0.7404580116271973)
[2025-02-05 14:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:47][root][INFO] - Training Epoch: 2/2, step 23652/23838 completed (loss: 0.933681845664978, acc: 0.695652186870575)
[2025-02-05 14:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:47][root][INFO] - Training Epoch: 2/2, step 23653/23838 completed (loss: 1.1304999589920044, acc: 0.6881720423698425)
[2025-02-05 14:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:48][root][INFO] - Training Epoch: 2/2, step 23654/23838 completed (loss: 1.113856554031372, acc: 0.699999988079071)
[2025-02-05 14:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:48][root][INFO] - Training Epoch: 2/2, step 23655/23838 completed (loss: 0.8781231641769409, acc: 0.7352941036224365)
[2025-02-05 14:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:48][root][INFO] - Training Epoch: 2/2, step 23656/23838 completed (loss: 1.1402002573013306, acc: 0.6803278923034668)
[2025-02-05 14:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:49][root][INFO] - Training Epoch: 2/2, step 23657/23838 completed (loss: 1.0221995115280151, acc: 0.694915235042572)
[2025-02-05 14:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:49][root][INFO] - Training Epoch: 2/2, step 23658/23838 completed (loss: 0.9911071062088013, acc: 0.7142857313156128)
[2025-02-05 14:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:50][root][INFO] - Training Epoch: 2/2, step 23659/23838 completed (loss: 0.9391367435455322, acc: 0.7014925479888916)
[2025-02-05 14:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:50][root][INFO] - Training Epoch: 2/2, step 23660/23838 completed (loss: 0.9100673794746399, acc: 0.7027027010917664)
[2025-02-05 14:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:50][root][INFO] - Training Epoch: 2/2, step 23661/23838 completed (loss: 0.8851341605186462, acc: 0.7333333492279053)
[2025-02-05 14:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:51][root][INFO] - Training Epoch: 2/2, step 23662/23838 completed (loss: 1.3022762537002563, acc: 0.6559139490127563)
[2025-02-05 14:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:51][root][INFO] - Training Epoch: 2/2, step 23663/23838 completed (loss: 1.1530364751815796, acc: 0.7241379022598267)
[2025-02-05 14:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:51][root][INFO] - Training Epoch: 2/2, step 23664/23838 completed (loss: 0.9430851936340332, acc: 0.6842105388641357)
[2025-02-05 14:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:52][root][INFO] - Training Epoch: 2/2, step 23665/23838 completed (loss: 0.9367036819458008, acc: 0.694656491279602)
[2025-02-05 14:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:52][root][INFO] - Training Epoch: 2/2, step 23666/23838 completed (loss: 0.9276326298713684, acc: 0.7346938848495483)
[2025-02-05 14:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:53][root][INFO] - Training Epoch: 2/2, step 23667/23838 completed (loss: 1.053381323814392, acc: 0.7019867300987244)
[2025-02-05 14:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:53][root][INFO] - Training Epoch: 2/2, step 23668/23838 completed (loss: 0.946479320526123, acc: 0.7155963182449341)
[2025-02-05 14:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:53][root][INFO] - Training Epoch: 2/2, step 23669/23838 completed (loss: 0.9760735630989075, acc: 0.6875)
[2025-02-05 14:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:54][root][INFO] - Training Epoch: 2/2, step 23670/23838 completed (loss: 1.1766979694366455, acc: 0.6393442749977112)
[2025-02-05 14:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:54][root][INFO] - Training Epoch: 2/2, step 23671/23838 completed (loss: 1.2063864469528198, acc: 0.6702127456665039)
[2025-02-05 14:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:55][root][INFO] - Training Epoch: 2/2, step 23672/23838 completed (loss: 1.0509394407272339, acc: 0.6595744490623474)
[2025-02-05 14:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:55][root][INFO] - Training Epoch: 2/2, step 23673/23838 completed (loss: 1.0900840759277344, acc: 0.6796116232872009)
[2025-02-05 14:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:55][root][INFO] - Training Epoch: 2/2, step 23674/23838 completed (loss: 1.0219801664352417, acc: 0.7083333134651184)
[2025-02-05 14:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:56][root][INFO] - Training Epoch: 2/2, step 23675/23838 completed (loss: 1.247665286064148, acc: 0.6062992215156555)
[2025-02-05 14:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:56][root][INFO] - Training Epoch: 2/2, step 23676/23838 completed (loss: 1.0936241149902344, acc: 0.6619718074798584)
[2025-02-05 14:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:56][root][INFO] - Training Epoch: 2/2, step 23677/23838 completed (loss: 1.0648512840270996, acc: 0.7200000286102295)
[2025-02-05 14:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:57][root][INFO] - Training Epoch: 2/2, step 23678/23838 completed (loss: 0.981624960899353, acc: 0.7368420958518982)
[2025-02-05 14:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:57][root][INFO] - Training Epoch: 2/2, step 23679/23838 completed (loss: 0.7813742160797119, acc: 0.800000011920929)
[2025-02-05 14:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:58][root][INFO] - Training Epoch: 2/2, step 23680/23838 completed (loss: 0.9997739195823669, acc: 0.699999988079071)
[2025-02-05 14:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:58][root][INFO] - Training Epoch: 2/2, step 23681/23838 completed (loss: 1.0106263160705566, acc: 0.6991869807243347)
[2025-02-05 14:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:58][root][INFO] - Training Epoch: 2/2, step 23682/23838 completed (loss: 1.2122873067855835, acc: 0.6161616444587708)
[2025-02-05 14:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:59][root][INFO] - Training Epoch: 2/2, step 23683/23838 completed (loss: 1.1953102350234985, acc: 0.6811594367027283)
[2025-02-05 14:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:59][root][INFO] - Training Epoch: 2/2, step 23684/23838 completed (loss: 1.0802099704742432, acc: 0.6575342416763306)
[2025-02-05 14:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:17:59][root][INFO] - Training Epoch: 2/2, step 23685/23838 completed (loss: 1.0033403635025024, acc: 0.7320261597633362)
[2025-02-05 14:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:00][root][INFO] - Training Epoch: 2/2, step 23686/23838 completed (loss: 0.9804732799530029, acc: 0.744966447353363)
[2025-02-05 14:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:00][root][INFO] - Training Epoch: 2/2, step 23687/23838 completed (loss: 0.9808316230773926, acc: 0.6796116232872009)
[2025-02-05 14:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:01][root][INFO] - Training Epoch: 2/2, step 23688/23838 completed (loss: 0.7632713913917542, acc: 0.7419354915618896)
[2025-02-05 14:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:01][root][INFO] - Training Epoch: 2/2, step 23689/23838 completed (loss: 1.1488004922866821, acc: 0.6538461446762085)
[2025-02-05 14:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:01][root][INFO] - Training Epoch: 2/2, step 23690/23838 completed (loss: 1.2633172273635864, acc: 0.6202531456947327)
[2025-02-05 14:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:02][root][INFO] - Training Epoch: 2/2, step 23691/23838 completed (loss: 0.7721996903419495, acc: 0.7543859481811523)
[2025-02-05 14:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:02][root][INFO] - Training Epoch: 2/2, step 23692/23838 completed (loss: 1.1972415447235107, acc: 0.6404494643211365)
[2025-02-05 14:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:03][root][INFO] - Training Epoch: 2/2, step 23693/23838 completed (loss: 1.0990734100341797, acc: 0.6666666865348816)
[2025-02-05 14:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:03][root][INFO] - Training Epoch: 2/2, step 23694/23838 completed (loss: 1.1669710874557495, acc: 0.6666666865348816)
[2025-02-05 14:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:03][root][INFO] - Training Epoch: 2/2, step 23695/23838 completed (loss: 0.9955405592918396, acc: 0.7115384340286255)
[2025-02-05 14:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:04][root][INFO] - Training Epoch: 2/2, step 23696/23838 completed (loss: 1.0571953058242798, acc: 0.6404494643211365)
[2025-02-05 14:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:04][root][INFO] - Training Epoch: 2/2, step 23697/23838 completed (loss: 1.0446298122406006, acc: 0.6470588445663452)
[2025-02-05 14:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:04][root][INFO] - Training Epoch: 2/2, step 23698/23838 completed (loss: 1.0942126512527466, acc: 0.6903225779533386)
[2025-02-05 14:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:05][root][INFO] - Training Epoch: 2/2, step 23699/23838 completed (loss: 1.1751039028167725, acc: 0.6727272868156433)
[2025-02-05 14:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:05][root][INFO] - Training Epoch: 2/2, step 23700/23838 completed (loss: 1.0191214084625244, acc: 0.7037037014961243)
[2025-02-05 14:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:06][root][INFO] - Training Epoch: 2/2, step 23701/23838 completed (loss: 0.6860292553901672, acc: 0.8333333134651184)
[2025-02-05 14:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:06][root][INFO] - Training Epoch: 2/2, step 23702/23838 completed (loss: 0.4089408814907074, acc: 0.875)
[2025-02-05 14:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:06][root][INFO] - Training Epoch: 2/2, step 23703/23838 completed (loss: 0.9904153347015381, acc: 0.692307710647583)
[2025-02-05 14:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:07][root][INFO] - Training Epoch: 2/2, step 23704/23838 completed (loss: 1.5468237400054932, acc: 0.5555555820465088)
[2025-02-05 14:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:07][root][INFO] - Training Epoch: 2/2, step 23705/23838 completed (loss: 1.118825912475586, acc: 0.6333333253860474)
[2025-02-05 14:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:07][root][INFO] - Training Epoch: 2/2, step 23706/23838 completed (loss: 0.8692237734794617, acc: 0.800000011920929)
[2025-02-05 14:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:08][root][INFO] - Training Epoch: 2/2, step 23707/23838 completed (loss: 1.077536702156067, acc: 0.7291666865348816)
[2025-02-05 14:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:08][root][INFO] - Training Epoch: 2/2, step 23708/23838 completed (loss: 1.2400965690612793, acc: 0.6730769276618958)
[2025-02-05 14:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:09][root][INFO] - Training Epoch: 2/2, step 23709/23838 completed (loss: 1.0335582494735718, acc: 0.7200000286102295)
[2025-02-05 14:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:09][root][INFO] - Training Epoch: 2/2, step 23710/23838 completed (loss: 1.0135698318481445, acc: 0.6774193644523621)
[2025-02-05 14:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:09][root][INFO] - Training Epoch: 2/2, step 23711/23838 completed (loss: 1.1617259979248047, acc: 0.6857143044471741)
[2025-02-05 14:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:10][root][INFO] - Training Epoch: 2/2, step 23712/23838 completed (loss: 0.8914509415626526, acc: 0.7142857313156128)
[2025-02-05 14:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:10][root][INFO] - Training Epoch: 2/2, step 23713/23838 completed (loss: 1.0283464193344116, acc: 0.6521739363670349)
[2025-02-05 14:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:10][root][INFO] - Training Epoch: 2/2, step 23714/23838 completed (loss: 1.3496004343032837, acc: 0.5714285969734192)
[2025-02-05 14:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:11][root][INFO] - Training Epoch: 2/2, step 23715/23838 completed (loss: 1.2151861190795898, acc: 0.7719298005104065)
[2025-02-05 14:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:11][root][INFO] - Training Epoch: 2/2, step 23716/23838 completed (loss: 1.3400635719299316, acc: 0.6052631735801697)
[2025-02-05 14:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:12][root][INFO] - Training Epoch: 2/2, step 23717/23838 completed (loss: 1.3067792654037476, acc: 0.6349206566810608)
[2025-02-05 14:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:12][root][INFO] - Training Epoch: 2/2, step 23718/23838 completed (loss: 0.8628637194633484, acc: 0.7647058963775635)
[2025-02-05 14:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:13][root][INFO] - Training Epoch: 2/2, step 23719/23838 completed (loss: 0.7611261606216431, acc: 0.7560975551605225)
[2025-02-05 14:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:13][root][INFO] - Training Epoch: 2/2, step 23720/23838 completed (loss: 1.444454312324524, acc: 0.5)
[2025-02-05 14:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:13][root][INFO] - Training Epoch: 2/2, step 23721/23838 completed (loss: 0.7897418141365051, acc: 0.6875)
[2025-02-05 14:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:14][root][INFO] - Training Epoch: 2/2, step 23722/23838 completed (loss: 0.7592904567718506, acc: 0.787401556968689)
[2025-02-05 14:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:14][root][INFO] - Training Epoch: 2/2, step 23723/23838 completed (loss: 0.6979108452796936, acc: 0.8636363744735718)
[2025-02-05 14:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:14][root][INFO] - Training Epoch: 2/2, step 23724/23838 completed (loss: 1.19955313205719, acc: 0.6086956262588501)
[2025-02-05 14:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:15][root][INFO] - Training Epoch: 2/2, step 23725/23838 completed (loss: 0.8165755271911621, acc: 0.7749999761581421)
[2025-02-05 14:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:15][root][INFO] - Training Epoch: 2/2, step 23726/23838 completed (loss: 1.1793879270553589, acc: 0.7142857313156128)
[2025-02-05 14:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:16][root][INFO] - Training Epoch: 2/2, step 23727/23838 completed (loss: 1.4187400341033936, acc: 0.6764705777168274)
[2025-02-05 14:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:16][root][INFO] - Training Epoch: 2/2, step 23728/23838 completed (loss: 0.9272868633270264, acc: 0.692307710647583)
[2025-02-05 14:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:16][root][INFO] - Training Epoch: 2/2, step 23729/23838 completed (loss: 0.869767427444458, acc: 0.7222222089767456)
[2025-02-05 14:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:17][root][INFO] - Training Epoch: 2/2, step 23730/23838 completed (loss: 1.0463929176330566, acc: 0.738095223903656)
[2025-02-05 14:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:17][root][INFO] - Training Epoch: 2/2, step 23731/23838 completed (loss: 0.8091471195220947, acc: 0.7368420958518982)
[2025-02-05 14:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:18][root][INFO] - Training Epoch: 2/2, step 23732/23838 completed (loss: 0.39379748702049255, acc: 0.8999999761581421)
[2025-02-05 14:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:18][root][INFO] - Training Epoch: 2/2, step 23733/23838 completed (loss: 0.7818118333816528, acc: 0.7586206793785095)
[2025-02-05 14:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:18][root][INFO] - Training Epoch: 2/2, step 23734/23838 completed (loss: 1.4398797750473022, acc: 0.5581395626068115)
[2025-02-05 14:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:19][root][INFO] - Training Epoch: 2/2, step 23735/23838 completed (loss: 0.8849546909332275, acc: 0.7123287916183472)
[2025-02-05 14:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:19][root][INFO] - Training Epoch: 2/2, step 23736/23838 completed (loss: 0.9772838354110718, acc: 0.6875)
[2025-02-05 14:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:19][root][INFO] - Training Epoch: 2/2, step 23737/23838 completed (loss: 0.9398013949394226, acc: 0.7536231875419617)
[2025-02-05 14:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:20][root][INFO] - Training Epoch: 2/2, step 23738/23838 completed (loss: 0.9830173850059509, acc: 0.7735849022865295)
[2025-02-05 14:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:20][root][INFO] - Training Epoch: 2/2, step 23739/23838 completed (loss: 1.0908186435699463, acc: 0.6666666865348816)
[2025-02-05 14:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:21][root][INFO] - Training Epoch: 2/2, step 23740/23838 completed (loss: 0.6637926697731018, acc: 0.800000011920929)
[2025-02-05 14:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:21][root][INFO] - Training Epoch: 2/2, step 23741/23838 completed (loss: 1.0832562446594238, acc: 0.7179487347602844)
[2025-02-05 14:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:21][root][INFO] - Training Epoch: 2/2, step 23742/23838 completed (loss: 1.4243803024291992, acc: 0.5918367505073547)
[2025-02-05 14:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:22][root][INFO] - Training Epoch: 2/2, step 23743/23838 completed (loss: 1.512902021408081, acc: 0.5952380895614624)
[2025-02-05 14:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:22][root][INFO] - Training Epoch: 2/2, step 23744/23838 completed (loss: 0.4938383400440216, acc: 0.9009009003639221)
[2025-02-05 14:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:23][root][INFO] - Training Epoch: 2/2, step 23745/23838 completed (loss: 0.6694507598876953, acc: 0.8190476298332214)
[2025-02-05 14:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:23][root][INFO] - Training Epoch: 2/2, step 23746/23838 completed (loss: 0.9193124175071716, acc: 0.7628865838050842)
[2025-02-05 14:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:24][root][INFO] - Training Epoch: 2/2, step 23747/23838 completed (loss: 1.224340558052063, acc: 0.6666666865348816)
[2025-02-05 14:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:24][root][INFO] - Training Epoch: 2/2, step 23748/23838 completed (loss: 0.7863439321517944, acc: 0.774193525314331)
[2025-02-05 14:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:24][root][INFO] - Training Epoch: 2/2, step 23749/23838 completed (loss: 0.9232746362686157, acc: 0.7749999761581421)
[2025-02-05 14:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:25][root][INFO] - Training Epoch: 2/2, step 23750/23838 completed (loss: 0.45341652631759644, acc: 0.9230769276618958)
[2025-02-05 14:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:25][root][INFO] - Training Epoch: 2/2, step 23751/23838 completed (loss: 1.6179888248443604, acc: 0.6333333253860474)
[2025-02-05 14:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:26][root][INFO] - Training Epoch: 2/2, step 23752/23838 completed (loss: 1.3495043516159058, acc: 0.6222222447395325)
[2025-02-05 14:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:26][root][INFO] - Training Epoch: 2/2, step 23753/23838 completed (loss: 0.5762212872505188, acc: 0.8695651888847351)
[2025-02-05 14:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:26][root][INFO] - Training Epoch: 2/2, step 23754/23838 completed (loss: 0.47997868061065674, acc: 0.8461538553237915)
[2025-02-05 14:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:27][root][INFO] - Training Epoch: 2/2, step 23755/23838 completed (loss: 1.1047550439834595, acc: 0.6938775777816772)
[2025-02-05 14:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:27][root][INFO] - Training Epoch: 2/2, step 23756/23838 completed (loss: 1.043694257736206, acc: 0.7749999761581421)
[2025-02-05 14:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:27][root][INFO] - Training Epoch: 2/2, step 23757/23838 completed (loss: 0.8943964838981628, acc: 0.7307692170143127)
[2025-02-05 14:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:28][root][INFO] - Training Epoch: 2/2, step 23758/23838 completed (loss: 1.4111500978469849, acc: 0.6363636255264282)
[2025-02-05 14:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:28][root][INFO] - Training Epoch: 2/2, step 23759/23838 completed (loss: 0.46328267455101013, acc: 0.8399999737739563)
[2025-02-05 14:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:29][root][INFO] - Training Epoch: 2/2, step 23760/23838 completed (loss: 2.029496192932129, acc: 0.5)
[2025-02-05 14:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:29][root][INFO] - Training Epoch: 2/2, step 23761/23838 completed (loss: 0.5307848453521729, acc: 0.7931034564971924)
[2025-02-05 14:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:30][root][INFO] - Training Epoch: 2/2, step 23762/23838 completed (loss: 1.0217186212539673, acc: 0.739130437374115)
[2025-02-05 14:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:30][root][INFO] - Training Epoch: 2/2, step 23763/23838 completed (loss: 1.1163866519927979, acc: 0.640350878238678)
[2025-02-05 14:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:30][root][INFO] - Training Epoch: 2/2, step 23764/23838 completed (loss: 0.9053512215614319, acc: 0.7285714149475098)
[2025-02-05 14:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:31][root][INFO] - Training Epoch: 2/2, step 23765/23838 completed (loss: 0.9318177700042725, acc: 0.6935483813285828)
[2025-02-05 14:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:31][root][INFO] - Training Epoch: 2/2, step 23766/23838 completed (loss: 0.7375956177711487, acc: 0.7058823704719543)
[2025-02-05 14:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:32][root][INFO] - Training Epoch: 2/2, step 23767/23838 completed (loss: 0.7387679815292358, acc: 0.8070175647735596)
[2025-02-05 14:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:32][root][INFO] - Training Epoch: 2/2, step 23768/23838 completed (loss: 0.6297658085823059, acc: 0.8039215803146362)
[2025-02-05 14:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:33][root][INFO] - Training Epoch: 2/2, step 23769/23838 completed (loss: 0.8913126587867737, acc: 0.7317073345184326)
[2025-02-05 14:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:33][root][INFO] - Training Epoch: 2/2, step 23770/23838 completed (loss: 0.7760500311851501, acc: 0.7692307829856873)
[2025-02-05 14:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:34][root][INFO] - Training Epoch: 2/2, step 23771/23838 completed (loss: 0.4832994043827057, acc: 0.8979591727256775)
[2025-02-05 14:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:34][root][INFO] - Training Epoch: 2/2, step 23772/23838 completed (loss: 0.7057082056999207, acc: 0.7831325531005859)
[2025-02-05 14:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:34][root][INFO] - Training Epoch: 2/2, step 23773/23838 completed (loss: 0.4521615505218506, acc: 0.8333333134651184)
[2025-02-05 14:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:35][root][INFO] - Training Epoch: 2/2, step 23774/23838 completed (loss: 0.8349593877792358, acc: 0.7413793206214905)
[2025-02-05 14:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:35][root][INFO] - Training Epoch: 2/2, step 23775/23838 completed (loss: 0.7764718532562256, acc: 0.75)
[2025-02-05 14:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:36][root][INFO] - Training Epoch: 2/2, step 23776/23838 completed (loss: 0.8972505331039429, acc: 0.699999988079071)
[2025-02-05 14:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:36][root][INFO] - Training Epoch: 2/2, step 23777/23838 completed (loss: 0.6970338225364685, acc: 0.7977527976036072)
[2025-02-05 14:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:36][root][INFO] - Training Epoch: 2/2, step 23778/23838 completed (loss: 0.8966572284698486, acc: 0.7663551568984985)
[2025-02-05 14:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:37][root][INFO] - Training Epoch: 2/2, step 23779/23838 completed (loss: 0.6451255083084106, acc: 0.8311688303947449)
[2025-02-05 14:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:37][root][INFO] - Training Epoch: 2/2, step 23780/23838 completed (loss: 0.8850666880607605, acc: 0.7333333492279053)
[2025-02-05 14:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:38][root][INFO] - Training Epoch: 2/2, step 23781/23838 completed (loss: 0.3226063549518585, acc: 0.8684210777282715)
[2025-02-05 14:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:38][root][INFO] - Training Epoch: 2/2, step 23782/23838 completed (loss: 0.9214662313461304, acc: 0.6499999761581421)
[2025-02-05 14:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:39][root][INFO] - Training Epoch: 2/2, step 23783/23838 completed (loss: 1.2087284326553345, acc: 0.6744186282157898)
[2025-02-05 14:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:39][root][INFO] - Training Epoch: 2/2, step 23784/23838 completed (loss: 0.805388331413269, acc: 0.7111111283302307)
[2025-02-05 14:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:39][root][INFO] - Training Epoch: 2/2, step 23785/23838 completed (loss: 0.8138982653617859, acc: 0.800000011920929)
[2025-02-05 14:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:40][root][INFO] - Training Epoch: 2/2, step 23786/23838 completed (loss: 0.5537518858909607, acc: 0.8253968358039856)
[2025-02-05 14:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:40][root][INFO] - Training Epoch: 2/2, step 23787/23838 completed (loss: 0.691275417804718, acc: 0.8088235259056091)
[2025-02-05 14:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:41][root][INFO] - Training Epoch: 2/2, step 23788/23838 completed (loss: 0.7188733220100403, acc: 0.7358490824699402)
[2025-02-05 14:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:41][root][INFO] - Training Epoch: 2/2, step 23789/23838 completed (loss: 0.6334608793258667, acc: 0.7943925261497498)
[2025-02-05 14:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:41][root][INFO] - Training Epoch: 2/2, step 23790/23838 completed (loss: 0.3496765196323395, acc: 0.9387755393981934)
[2025-02-05 14:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:42][root][INFO] - Training Epoch: 2/2, step 23791/23838 completed (loss: 0.3968406617641449, acc: 0.90625)
[2025-02-05 14:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:42][root][INFO] - Training Epoch: 2/2, step 23792/23838 completed (loss: 0.31093621253967285, acc: 0.936170220375061)
[2025-02-05 14:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:43][root][INFO] - Training Epoch: 2/2, step 23793/23838 completed (loss: 1.447487711906433, acc: 0.5833333134651184)
[2025-02-05 14:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:43][root][INFO] - Training Epoch: 2/2, step 23794/23838 completed (loss: 0.840878963470459, acc: 0.762499988079071)
[2025-02-05 14:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:43][root][INFO] - Training Epoch: 2/2, step 23795/23838 completed (loss: 0.43378764390945435, acc: 0.9277108311653137)
[2025-02-05 14:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:44][root][INFO] - Training Epoch: 2/2, step 23796/23838 completed (loss: 0.288137286901474, acc: 0.9268292784690857)
[2025-02-05 14:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:44][root][INFO] - Training Epoch: 2/2, step 23797/23838 completed (loss: 0.2713758647441864, acc: 0.9318181872367859)
[2025-02-05 14:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:45][root][INFO] - Training Epoch: 2/2, step 23798/23838 completed (loss: 0.7832837700843811, acc: 0.75)
[2025-02-05 14:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:45][root][INFO] - Training Epoch: 2/2, step 23799/23838 completed (loss: 0.9672310948371887, acc: 0.7291666865348816)
[2025-02-05 14:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:45][root][INFO] - Training Epoch: 2/2, step 23800/23838 completed (loss: 1.0416537523269653, acc: 0.698113203048706)
[2025-02-05 14:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:46][root][INFO] - Training Epoch: 2/2, step 23801/23838 completed (loss: 0.3014143407344818, acc: 0.9444444179534912)
[2025-02-05 14:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:46][root][INFO] - Training Epoch: 2/2, step 23802/23838 completed (loss: 1.459071159362793, acc: 0.6307692527770996)
[2025-02-05 14:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:46][root][INFO] - Training Epoch: 2/2, step 23803/23838 completed (loss: 1.3478387594223022, acc: 0.5681818127632141)
[2025-02-05 14:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:47][root][INFO] - Training Epoch: 2/2, step 23804/23838 completed (loss: 0.9942471385002136, acc: 0.6499999761581421)
[2025-02-05 14:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:47][root][INFO] - Training Epoch: 2/2, step 23805/23838 completed (loss: 1.012017846107483, acc: 0.6962025165557861)
[2025-02-05 14:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:48][root][INFO] - Training Epoch: 2/2, step 23806/23838 completed (loss: 0.6349809169769287, acc: 0.8453608155250549)
[2025-02-05 14:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:48][root][INFO] - Training Epoch: 2/2, step 23807/23838 completed (loss: 0.6882885098457336, acc: 0.7631579041481018)
[2025-02-05 14:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:49][root][INFO] - Training Epoch: 2/2, step 23808/23838 completed (loss: 1.1034618616104126, acc: 0.7272727489471436)
[2025-02-05 14:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:49][root][INFO] - Training Epoch: 2/2, step 23809/23838 completed (loss: 0.8509290814399719, acc: 0.7777777910232544)
[2025-02-05 14:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:50][root][INFO] - Training Epoch: 2/2, step 23810/23838 completed (loss: 0.9870488047599792, acc: 0.7042253613471985)
[2025-02-05 14:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:50][root][INFO] - Training Epoch: 2/2, step 23811/23838 completed (loss: 1.146389365196228, acc: 0.7222222089767456)
[2025-02-05 14:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:51][root][INFO] - Training Epoch: 2/2, step 23812/23838 completed (loss: 0.9940785765647888, acc: 0.7297297120094299)
[2025-02-05 14:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:51][root][INFO] - Training Epoch: 2/2, step 23813/23838 completed (loss: 0.8572072386741638, acc: 0.7333333492279053)
[2025-02-05 14:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:51][root][INFO] - Training Epoch: 2/2, step 23814/23838 completed (loss: 1.061935544013977, acc: 0.6831682920455933)
[2025-02-05 14:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:52][root][INFO] - Training Epoch: 2/2, step 23815/23838 completed (loss: 0.8025056719779968, acc: 0.7674418687820435)
[2025-02-05 14:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:52][root][INFO] - Training Epoch: 2/2, step 23816/23838 completed (loss: 1.4416409730911255, acc: 0.585106372833252)
[2025-02-05 14:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:53][root][INFO] - Training Epoch: 2/2, step 23817/23838 completed (loss: 0.9115768074989319, acc: 0.7236841917037964)
[2025-02-05 14:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:53][root][INFO] - Training Epoch: 2/2, step 23818/23838 completed (loss: 1.3341695070266724, acc: 0.6465517282485962)
[2025-02-05 14:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:53][root][INFO] - Training Epoch: 2/2, step 23819/23838 completed (loss: 1.377272605895996, acc: 0.5811966061592102)
[2025-02-05 14:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:54][root][INFO] - Training Epoch: 2/2, step 23820/23838 completed (loss: 1.3233708143234253, acc: 0.6229507923126221)
[2025-02-05 14:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:54][root][INFO] - Training Epoch: 2/2, step 23821/23838 completed (loss: 1.2155570983886719, acc: 0.6105263233184814)
[2025-02-05 14:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:55][root][INFO] - Training Epoch: 2/2, step 23822/23838 completed (loss: 1.3390697240829468, acc: 0.5714285969734192)
[2025-02-05 14:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:55][root][INFO] - Training Epoch: 2/2, step 23823/23838 completed (loss: 1.527046799659729, acc: 0.5797101259231567)
[2025-02-05 14:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:55][root][INFO] - Training Epoch: 2/2, step 23824/23838 completed (loss: 1.5175529718399048, acc: 0.5520833134651184)
[2025-02-05 14:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:56][root][INFO] - Training Epoch: 2/2, step 23825/23838 completed (loss: 1.5095149278640747, acc: 0.5892857313156128)
[2025-02-05 14:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:56][root][INFO] - Training Epoch: 2/2, step 23826/23838 completed (loss: 1.1910063028335571, acc: 0.6111111044883728)
[2025-02-05 14:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:57][root][INFO] - Training Epoch: 2/2, step 23827/23838 completed (loss: 1.4773660898208618, acc: 0.6041666865348816)
[2025-02-05 14:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:57][root][INFO] - Training Epoch: 2/2, step 23828/23838 completed (loss: 1.3455169200897217, acc: 0.6017699241638184)
[2025-02-05 14:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:57][root][INFO] - Training Epoch: 2/2, step 23829/23838 completed (loss: 1.3385207653045654, acc: 0.5490196347236633)
[2025-02-05 14:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:58][root][INFO] - Training Epoch: 2/2, step 23830/23838 completed (loss: 1.0046061277389526, acc: 0.6794871687889099)
[2025-02-05 14:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:58][root][INFO] - Training Epoch: 2/2, step 23831/23838 completed (loss: 1.1804827451705933, acc: 0.6385542154312134)
[2025-02-05 14:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:59][root][INFO] - Training Epoch: 2/2, step 23832/23838 completed (loss: 1.3561476469039917, acc: 0.617977499961853)
[2025-02-05 14:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:18:59][root][INFO] - Training Epoch: 2/2, step 23833/23838 completed (loss: 1.2627140283584595, acc: 0.6593406796455383)
[2025-02-05 14:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:04][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.9139, device='cuda:0') eval_epoch_loss=tensor(1.3645, device='cuda:0') eval_epoch_acc=tensor(0.6137, device='cuda:0')
[2025-02-05 14:40:04][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-05 14:40:04][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-05 14:40:05][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_23834_loss_1.3645373582839966/model.pt
[2025-02-05 14:40:05][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft directory
[2025-02-05 14:40:05][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 1.3645373582839966
[2025-02-05 14:40:05][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.6137121319770813
[2025-02-05 14:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:06][root][INFO] - Training Epoch: 2/2, step 23834/23838 completed (loss: 1.3643993139266968, acc: 0.6111111044883728)
[2025-02-05 14:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:06][root][INFO] - Training Epoch: 2/2, step 23835/23838 completed (loss: 0.9815506935119629, acc: 0.6499999761581421)
[2025-02-05 14:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:06][root][INFO] - Training Epoch: 2/2, step 23836/23838 completed (loss: 1.154233694076538, acc: 0.6481481194496155)
[2025-02-05 14:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:07][root][INFO] - Training Epoch: 2/2, step 23837/23838 completed (loss: 1.498506784439087, acc: 0.5620915293693542)
[2025-02-05 14:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:08][root][INFO] - Training Epoch: 2/2, step 23838/23838 completed (loss: 1.1587516069412231, acc: 0.6914893388748169)
[2025-02-05 14:40:08][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.3325, train_epoch_loss=0.2870, epoch time 3638.550074417144s
[2025-02-05 14:40:08][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 16 GB
[2025-02-05 14:40:08][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 21 GB
[2025-02-05 14:40:08][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 16 GB
[2025-02-05 14:40:08][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-05 14:40:08][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-05 14:40:08][root][INFO] - Key: avg_train_prep, Value: 1.3324906826019287
[2025-02-05 14:40:08][root][INFO] - Key: avg_train_loss, Value: 0.28704994916915894
[2025-02-05 14:40:08][root][INFO] - Key: avg_train_acc, Value: 0.16679032146930695
[2025-02-05 14:40:08][root][INFO] - Key: avg_eval_prep, Value: 3.913911819458008
[2025-02-05 14:40:08][root][INFO] - Key: avg_eval_loss, Value: 1.3645373582839966
[2025-02-05 14:40:08][root][INFO] - Key: avg_eval_acc, Value: 0.6137121319770813
[2025-02-05 14:40:08][root][INFO] - Key: avg_epoch_time, Value: 3638.550074417144
[2025-02-05 14:40:08][root][INFO] - Key: avg_checkpoint_time, Value: 0.7808561101555824
Selected lowest loss checkpoint: asr_epoch_2_step_17875_loss_1.2942701578140259
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_1.2942701578140259/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_1.2942701578140259
[2025-02-05 14:40:32][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-05 14:40:32][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-05 14:40:32][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'q-former', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_phoneme_wavlm_llama32_1b_q-former_peft'}
[2025-02-05 14:40:33][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-05 14:40:38][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-05 14:40:38][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-05 14:40:38][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-05 14:40:38][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-05 14:40:41][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-05 14:40:41][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-05 14:40:41][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-05 14:40:41][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-05 14:40:41][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-05 14:40:42][slam_llm.utils.train_utils][INFO] - --> Module q-former
[2025-02-05 14:40:42][slam_llm.utils.train_utils][INFO] - --> q-former has 69.361152 Million params

[2025-02-05 14:40:42][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/asr_epoch_2_step_17875_loss_1.2942701578140259/model.pt
[2025-02-05 14:40:42][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-05 14:40:42][slam_llm.utils.train_utils][INFO] - --> asr has 74.997248 Million params

[2025-02-05 14:40:44][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/aphasia_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-05 14:40:45][root][INFO] - --> Training Set Length = 12232
[2025-02-05 14:40:45][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/test_config.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-02-05 14:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 14:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-05 15:55:39][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/decode_test_beam4_pred_20250205_144045
[2025-02-05 15:55:39][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/decode_test_beam4_gt_20250205_144045
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/decode_test_beam4_gt_20250205_144045
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_q-former_peft/decode_test_beam4_pred_20250205_144045
Combined WER: 0.8593052391799545

Filtering repeated words...

Found 3 repeated lines in total.
Repeated lines are:
- OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY
- AH N D DH EH N AY W AA Z EY B AH L T UW R IH M EH M B ER DH AH S T R OW K AH N D DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T DH AE T
- OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY OW K EY
Filtered Combined WER: 0.8587583283319026
