/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
train_data_folder: psst_phoneme
test_data_folder: psst_phoneme
use_peft: true
seed: 
debug: 
Is test_run? 
freeze_encoder: true
Is save_embedding? false
projector_transfer_learning: false
transfer_data_folder: 
llm_inference_config: repetition_penalty
eval_ckpt: best
----------
----------
Final identifier: psst_phoneme_wavlm_llama32_1b_dual_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_554_loss_0.9401524662971497
Resume epoch: 10
Resume step: 554
[2025-02-08 19:23:05][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 10, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 554, 'resume_epoch': 10, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 10, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-08 19:23:05][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-08 19:23:05][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme', 'identifier': 'psst_phoneme_wavlm_llama32_1b_dual_peft'}
[2025-02-08 19:23:05][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'psst_phoneme_wavlm_llama32_1b_dual_peft', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-08_19-23-04.txt', 'log_interval': 5}
wandb: Currently logged in as: jindaz (jindaz-work). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log/wandb/run-20250208_192307-x4sxs4iz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run psst_phoneme_wavlm_llama32_1b_dual_peft
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jindaz-work/SLAM-LLM
wandb: üöÄ View run at https://wandb.ai/jindaz-work/SLAM-LLM/runs/x4sxs4iz
[2025-02-08 19:23:26][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
[2025-02-08 19:23:32][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-08 19:23:32][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-08 19:23:32][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-08 19:23:32][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-08 19:23:35][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-08 19:23:35][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2025-02-08 19:23:35][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-08 19:23:35][slam_llm.utils.train_utils][INFO] - --> w2v2 has 0.0 Million params

[2025-02-08 19:23:42][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-08 19:23:42][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-08 19:23:42][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-08 19:23:43][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-08 19:23:43][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-08 19:23:43][slam_llm.utils.train_utils][INFO] - --> Module dual
[2025-02-08 19:23:43][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2025-02-08 19:23:43][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_554_loss_0.9401524662971497/model.pt
[2025-02-08 19:23:43][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-08 19:23:43][slam_llm.utils.train_utils][INFO] - --> asr has 30.806016 Million params

[2025-02-08 19:23:48][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-08 19:23:50][root][INFO] - --> Training Set Length = 2298
[2025-02-08 19:23:50][root][INFO] - --> Validation Set Length = 341
[2025-02-08 19:23:50][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-08 19:23:50][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
Error executing job with overrides: ['++model_config.llm_name=llama32_1b', '++model_config.llm_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', '++model_config.llm_dim=2048', '++model_config.encoder_name=wavlm', '++model_config.normalize=true', '++dataset_config.normalize=true', '++model_config.encoder_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', '++model_config.encoder2_name=w2v2', '++model_config.encoder2_path=vitouphy/wav2vec2-xls-r-300m-timit-phoneme', '++model_config.encoder_dim=1024', '++model_config.encoder_projector=dual', '++model_config.encoder_projector_ds_rate=5', '++model_config.identifier=psst_phoneme_wavlm_llama32_1b_dual_peft', '++dataset_config.dataset=speech_dataset', '++dataset_config.train_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', '++dataset_config.file=src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', '++dataset_config.val_data_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', '++dataset_config.input_type=raw', '++train_config.model_name=asr', '++train_config.num_epochs=2', '++train_config.freeze_encoder=true', '++train_config.freeze_encoder2=true', '++train_config.freeze_llm=false', '++train_config.batching_strategy=custom', '++train_config.warmup_steps=1000', '++train_config.total_steps=100000', '++train_config.lr=1e-4', '++train_config.validation_interval=3000', '++train_config.batch_size_training=10', '++train_config.val_batch_size=10', '++train_config.num_workers_dataloader=1', '++train_config.output_dir=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', '++train_config.use_fp16=true', '++train_config.use_peft=true', '++train_config.resume_epoch=10', '++train_config.resume_step=554', '++log_config.use_wandb=true', '++log_config.wandb_exp_name=psst_phoneme_wavlm_llama32_1b_dual_peft', '++ckpt_path=/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_10_step_554_loss_0.9401524662971497/model.pt', '++dataset_config.input_type=raw']
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/finetune_asr.py", line 56, in <module>
    main_hydra()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 261, in return_value
    raise self._return_value
  File "/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/hydra/core/utils.py", line 187, in run_job
    ret.return_value = task_function(task_cfg)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/finetune_asr.py", line 52, in main_hydra
    train(kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/pipeline/finetune.py", line 271, in main
    results = train(
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/src/slam_llm/utils/train_utils.py", line 368, in train
    avg_epoch_time = sum(epoch_times)/ len(epoch_times)
ZeroDivisionError: division by zero
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.017 MB of 0.039 MB uploaded (0.009 MB deduped)wandb: üöÄ View run psst_phoneme_wavlm_llama32_1b_dual_peft at: https://wandb.ai/jindaz-work/SLAM-LLM/runs/x4sxs4iz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jindaz-work/SLAM-LLM
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./out/log/wandb_log/wandb/run-20250208_192307-x4sxs4iz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Selected lowest loss checkpoint: asr_epoch_4_step_137_loss_0.7482238411903381
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_137_loss_0.7482238411903381/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_137_loss_0.7482238411903381
[2025-02-08 19:24:14][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 10, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-08 19:24:14][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-08 19:24:14][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'dual', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': 'w2v2', 'encoder2_dim': 1024, 'encoder2_path': 'vitouphy/wav2vec2-xls-r-300m-timit-phoneme', 'identifier': 'psst_phoneme_wavlm_llama32_1b_dual_peft'}
[2025-02-08 19:24:16][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
[2025-02-08 19:24:23][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-08 19:24:23][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-08 19:24:23][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-08 19:24:23][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-08 19:24:24][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-08 19:24:24][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2025-02-08 19:24:24][slam_llm.utils.train_utils][INFO] - --> Module w2v2
[2025-02-08 19:24:24][slam_llm.utils.train_utils][INFO] - --> w2v2 has 315.43872 Million params

[2025-02-08 19:24:30][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-08 19:24:30][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-08 19:24:30][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-08 19:24:30][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-08 19:24:30][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-08 19:24:30][slam_llm.utils.train_utils][INFO] - --> Module dual
[2025-02-08 19:24:30][slam_llm.utils.train_utils][INFO] - --> dual has 25.16992 Million params

[2025-02-08 19:24:30][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/asr_epoch_4_step_137_loss_0.7482238411903381/model.pt
[2025-02-08 19:24:30][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-08 19:24:30][slam_llm.utils.train_utils][INFO] - --> asr has 346.244736 Million params

[2025-02-08 19:24:36][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-08 19:24:36][root][INFO] - --> Training Set Length = 652
[2025-02-08 19:24:36][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/repetition_penalty.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
  0%|          | 0/66 [00:00<?, ?it/s]/work/van-speech-nlp/jindaznb/slamenv/lib/python3.10/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
[2025-02-08 19:24:38][slam_llm.models.slam_model][INFO] - modality encoder
  2%|‚ñè         | 1/66 [00:03<03:29,  3.23s/it][2025-02-08 19:24:40][slam_llm.models.slam_model][INFO] - modality encoder
  3%|‚ñé         | 2/66 [00:11<06:42,  6.29s/it][2025-02-08 19:24:50][slam_llm.models.slam_model][INFO] - modality encoder
  5%|‚ñç         | 3/66 [00:25<10:20,  9.85s/it][2025-02-08 19:25:03][slam_llm.models.slam_model][INFO] - modality encoder
  6%|‚ñå         | 4/66 [00:34<09:50,  9.53s/it][2025-02-08 19:25:12][slam_llm.models.slam_model][INFO] - modality encoder
  8%|‚ñä         | 5/66 [00:45<10:02,  9.88s/it][2025-02-08 19:25:22][slam_llm.models.slam_model][INFO] - modality encoder
  9%|‚ñâ         | 6/66 [00:54<09:31,  9.52s/it][2025-02-08 19:25:31][slam_llm.models.slam_model][INFO] - modality encoder
 11%|‚ñà         | 7/66 [01:02<09:07,  9.28s/it][2025-02-08 19:25:40][slam_llm.models.slam_model][INFO] - modality encoder
 12%|‚ñà‚ñè        | 8/66 [01:04<06:35,  6.81s/it][2025-02-08 19:25:41][slam_llm.models.slam_model][INFO] - modality encoder
 14%|‚ñà‚ñé        | 9/66 [01:05<04:42,  4.96s/it][2025-02-08 19:25:42][slam_llm.models.slam_model][INFO] - modality encoder
 15%|‚ñà‚ñå        | 10/66 [01:13<05:39,  6.06s/it][2025-02-08 19:25:51][slam_llm.models.slam_model][INFO] - modality encoder
 17%|‚ñà‚ñã        | 11/66 [01:21<06:06,  6.66s/it][2025-02-08 19:25:59][slam_llm.models.slam_model][INFO] - modality encoder
 18%|‚ñà‚ñä        | 12/66 [01:23<04:32,  5.05s/it][2025-02-08 19:26:00][slam_llm.models.slam_model][INFO] - modality encoder
 20%|‚ñà‚ñâ        | 13/66 [01:24<03:24,  3.86s/it][2025-02-08 19:26:01][slam_llm.models.slam_model][INFO] - modality encoder
 21%|‚ñà‚ñà        | 14/66 [01:25<02:43,  3.15s/it][2025-02-08 19:26:03][slam_llm.models.slam_model][INFO] - modality encoder
 23%|‚ñà‚ñà‚ñé       | 15/66 [01:27<02:17,  2.70s/it][2025-02-08 19:26:04][slam_llm.models.slam_model][INFO] - modality encoder
 24%|‚ñà‚ñà‚ñç       | 16/66 [01:35<03:39,  4.39s/it][2025-02-08 19:26:13][slam_llm.models.slam_model][INFO] - modality encoder
 26%|‚ñà‚ñà‚ñå       | 17/66 [01:37<02:57,  3.62s/it][2025-02-08 19:26:15][slam_llm.models.slam_model][INFO] - modality encoder
 27%|‚ñà‚ñà‚ñã       | 18/66 [01:39<02:21,  2.95s/it][2025-02-08 19:26:17][slam_llm.models.slam_model][INFO] - modality encoder
 29%|‚ñà‚ñà‚ñâ       | 19/66 [01:43<02:46,  3.54s/it][2025-02-08 19:26:22][slam_llm.models.slam_model][INFO] - modality encoder
 30%|‚ñà‚ñà‚ñà       | 20/66 [01:56<04:42,  6.13s/it][2025-02-08 19:26:33][slam_llm.models.slam_model][INFO] - modality encoder
 32%|‚ñà‚ñà‚ñà‚ñè      | 21/66 [01:57<03:25,  4.56s/it][2025-02-08 19:26:34][slam_llm.models.slam_model][INFO] - modality encoder
 33%|‚ñà‚ñà‚ñà‚ñé      | 22/66 [01:59<02:49,  3.86s/it][2025-02-08 19:26:36][slam_llm.models.slam_model][INFO] - modality encoder
 35%|‚ñà‚ñà‚ñà‚ñç      | 23/66 [02:09<04:14,  5.91s/it][2025-02-08 19:26:47][slam_llm.models.slam_model][INFO] - modality encoder
 36%|‚ñà‚ñà‚ñà‚ñã      | 24/66 [02:11<03:19,  4.75s/it][2025-02-08 19:26:49][slam_llm.models.slam_model][INFO] - modality encoder
 38%|‚ñà‚ñà‚ñà‚ñä      | 25/66 [02:22<04:20,  6.34s/it][2025-02-08 19:26:59][slam_llm.models.slam_model][INFO] - modality encoder
 39%|‚ñà‚ñà‚ñà‚ñâ      | 26/66 [02:32<05:06,  7.67s/it][2025-02-08 19:27:10][slam_llm.models.slam_model][INFO] - modality encoder
 41%|‚ñà‚ñà‚ñà‚ñà      | 27/66 [02:34<03:49,  5.90s/it][2025-02-08 19:27:11][slam_llm.models.slam_model][INFO] - modality encoder
 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 28/66 [02:36<02:54,  4.60s/it][2025-02-08 19:27:14][slam_llm.models.slam_model][INFO] - modality encoder
 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 29/66 [02:40<02:42,  4.40s/it][2025-02-08 19:27:17][slam_llm.models.slam_model][INFO] - modality encoder
 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 30/66 [02:47<03:16,  5.45s/it][2025-02-08 19:27:25][slam_llm.models.slam_model][INFO] - modality encoder
 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 31/66 [02:59<04:14,  7.26s/it][2025-02-08 19:27:37][slam_llm.models.slam_model][INFO] - modality encoder
 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 32/66 [03:10<04:47,  8.45s/it][2025-02-08 19:27:48][slam_llm.models.slam_model][INFO] - modality encoder
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 33/66 [03:21<05:05,  9.27s/it][2025-02-08 19:27:59][slam_llm.models.slam_model][INFO] - modality encoder
 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 34/66 [03:22<03:37,  6.80s/it][2025-02-08 19:28:00][slam_llm.models.slam_model][INFO] - modality encoder
 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 35/66 [03:23<02:37,  5.08s/it][2025-02-08 19:28:02][slam_llm.models.slam_model][INFO] - modality encoder
 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 36/66 [03:39<04:02,  8.08s/it][2025-02-08 19:28:17][slam_llm.models.slam_model][INFO] - modality encoder
 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 37/66 [03:54<04:59, 10.31s/it][2025-02-08 19:28:32][slam_llm.models.slam_model][INFO] - modality encoder
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 38/66 [03:59<04:00,  8.61s/it][2025-02-08 19:28:36][slam_llm.models.slam_model][INFO] - modality encoder
 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 39/66 [04:01<03:02,  6.74s/it][2025-02-08 19:28:40][slam_llm.models.slam_model][INFO] - modality encoder
 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 40/66 [04:15<03:49,  8.82s/it][2025-02-08 19:28:52][slam_llm.models.slam_model][INFO] - modality encoder
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 41/66 [04:16<02:40,  6.42s/it][2025-02-08 19:28:53][slam_llm.models.slam_model][INFO] - modality encoder
 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 42/66 [04:16<01:54,  4.76s/it][2025-02-08 19:28:54][slam_llm.models.slam_model][INFO] - modality encoder
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 43/66 [04:19<01:34,  4.09s/it][2025-02-08 19:28:56][slam_llm.models.slam_model][INFO] - modality encoder
 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 44/66 [04:20<01:09,  3.14s/it][2025-02-08 19:28:57][slam_llm.models.slam_model][INFO] - modality encoder
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 45/66 [04:22<00:57,  2.76s/it][2025-02-08 19:28:59][slam_llm.models.slam_model][INFO] - modality encoder
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 46/66 [04:30<01:29,  4.46s/it][2025-02-08 19:29:08][slam_llm.models.slam_model][INFO] - modality encoder
 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 47/66 [04:39<01:47,  5.67s/it][2025-02-08 19:29:16][slam_llm.models.slam_model][INFO] - modality encoder
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 48/66 [04:40<01:17,  4.29s/it][2025-02-08 19:29:17][slam_llm.models.slam_model][INFO] - modality encoder
 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 49/66 [04:41<00:57,  3.35s/it][2025-02-08 19:29:18][slam_llm.models.slam_model][INFO] - modality encoder
 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 50/66 [04:50<01:21,  5.07s/it][2025-02-08 19:29:27][slam_llm.models.slam_model][INFO] - modality encoder
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 51/66 [04:58<01:27,  5.84s/it][2025-02-08 19:29:35][slam_llm.models.slam_model][INFO] - modality encoder
 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 52/66 [04:59<01:01,  4.41s/it][2025-02-08 19:29:36][slam_llm.models.slam_model][INFO] - modality encoder
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 53/66 [05:00<00:45,  3.51s/it][2025-02-08 19:29:38][slam_llm.models.slam_model][INFO] - modality encoder
 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 54/66 [05:03<00:40,  3.35s/it][2025-02-08 19:29:40][slam_llm.models.slam_model][INFO] - modality encoder
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 55/66 [05:11<00:52,  4.75s/it][2025-02-08 19:29:49][slam_llm.models.slam_model][INFO] - modality encoder
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 56/66 [05:13<00:39,  3.91s/it][2025-02-08 19:29:50][slam_llm.models.slam_model][INFO] - modality encoder
 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 57/66 [05:14<00:27,  3.09s/it][2025-02-08 19:29:52][slam_llm.models.slam_model][INFO] - modality encoder
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 58/66 [05:16<00:22,  2.82s/it][2025-02-08 19:29:54][slam_llm.models.slam_model][INFO] - modality encoder
 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 59/66 [05:18<00:16,  2.40s/it][2025-02-08 19:29:55][slam_llm.models.slam_model][INFO] - modality encoder
 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 60/66 [05:19<00:12,  2.07s/it][2025-02-08 19:29:58][slam_llm.models.slam_model][INFO] - modality encoder
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 61/66 [05:36<00:31,  6.36s/it][2025-02-08 19:30:14][slam_llm.models.slam_model][INFO] - modality encoder
 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 62/66 [05:48<00:32,  8.11s/it][2025-02-08 19:30:25][slam_llm.models.slam_model][INFO] - modality encoder
 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 63/66 [05:49<00:17,  5.96s/it][2025-02-08 19:30:26][slam_llm.models.slam_model][INFO] - modality encoder
 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 64/66 [05:51<00:09,  4.78s/it][2025-02-08 19:30:29][slam_llm.models.slam_model][INFO] - modality encoder
 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 65/66 [06:06<00:07,  7.91s/it][2025-02-08 19:30:43][slam_llm.models.slam_model][INFO] - modality encoder
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [06:07<00:00,  5.80s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [06:07<00:00,  5.57s/it]
[2025-02-08 19:30:44][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/decode_test_beam4_pred_20250208_192436
[2025-02-08 19:30:44][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/decode_test_beam4_gt_20250208_192436
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/decode_test_beam4_gt_20250208_192436
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/psst_phoneme_wavlm_llama32_1b_dual_peft/decode_test_beam4_pred_20250208_192436
Combined WER: 1.3853435114503816

Filtering repeated words...

Found 15 repeated lines in total.
Repeated lines are:
- AH M <sil> AE K SH IH N JH EH S AH <sil> K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA
- DH AH DH AH M P EY P ER IH Z AH D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D IH S D
- SH IY Z R IY D IH NG T UW L AH G EY N D F R AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R D AO R
- DH AE T S EH TH AH HH IH Z HH IY Z K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R
- TH R OW W AA N N AE K T D AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R AH D R
- R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D R AY D
- HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z
- SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S SH S
- OW P IH N D IH NG AE N D D AE T D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D
- DH AH B AA L IH Z <sil> HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH IY HH
- AH M <sil> AE N D G R IY SH S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S
- AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M S AH M
- AH Y AE HH IY Z Y AE HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY Z HH IY
- AH M EY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L IY L
- AH N S OW IH NG N OW L AY K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K P UH K
Filtered Combined WER: 1.0748891443586797
